2020.aacl-main.56,Q16-1022,0,0.0273759,"Missing"
2020.aacl-main.56,2020.emnlp-main.618,0,0.176607,"18) is a questionanswering dataset consisting of passages extracted from Wikipedia articles and crowd-sourced questions and answers. In SQuAD version 1.1, each example consists of a context passage and a question, and the answer is a text span from the context. SQuAD version 2.0 includes additional questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences wi"
2020.aacl-main.56,D19-1252,0,0.034384,"Missing"
2020.aacl-main.56,D19-1243,0,0.0512798,"Missing"
2020.aacl-main.56,Q17-1024,0,0.0927489,"Missing"
2020.aacl-main.56,P18-4020,0,0.0202348,"Missing"
2020.aacl-main.56,2020.acl-main.653,0,0.118049,"cted from Wikipedia articles and crowd-sourced questions and answers. In SQuAD version 1.1, each example consists of a context passage and a question, and the answer is a text span from the context. SQuAD version 2.0 includes additional questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are s"
2020.aacl-main.56,P19-1441,0,0.15449,"an additional task for intermediate training, using the same multi-task sampling strategy as above. XLM-R + Translated Intermediate Task We translate intermediate-task training and validation data for three tasks and fine-tune XLM-R on translated intermediate-task data before we train and evaluate on the target tasks. 3.2 Software Experiments were carried out using the jiant (Phang et al., 2020) library (2.0 alpha), based on PyTorch (Paszke et al., 2019) and Transformers (Wolf et al., 2019). 7 XLM-R Large (Conneau et al., 2020) is a 550m-parameter variant of the RoBERTa masked language model (Liu et al., 2019b) trained on a cleaned version of CommonCrawl on 100 languages. Notably, Yoruba is used in the POS and NER XTREME tasks but not is not in the set of 100 languages. Results We train three versions of each intermediate-task model with different random seeds. For each run, we compute the average target-task performance across languages, and report the median performance across the three random seeds. Intermediate-Task Training As shown in Table 2, no single intermediate task yields positive transfer across all target tasks. The target tasks TyDiQA, BUCC and Tatoeba see consistent gains from most"
2020.aacl-main.56,2021.ccl-1.108,0,0.128091,"Missing"
2020.aacl-main.56,D17-1269,0,0.049625,"Missing"
2020.aacl-main.56,D11-1006,0,0.110474,"Missing"
2020.aacl-main.56,2020.acl-main.441,0,0.167403,"r from literature. Pruksachatkun et al. (2020) shows that MNLI (of which ANLI+ is a superset), CommonsenseQA, Cosmos QA and HellaSwag yield positive transfer to a range of downstream English-language tasks in intermediate training. CCG involves token-wise prediction and is similar to the POS and NER target tasks. Both versions of SQuAD are widely-used questionanswering tasks, while QQP is semantically similar to sentence retrieval target tasks (BUCC and Tatoeba) as well as PAWS-X, another paraphrasedetection task. ANLI + MNLI + SNLI (ANLI+ ) The Adversarial Natural Language Inference dataset (Nie et al., 2020) is collected using model-in-the-loop crowdsourcing as an extension of the Stanford Natural Language Inference (SNLI; Bowman et al., 2015) and Multi-Genre Natural Language Inference (MNLI; Williams et al., 2018) corpora. We follow Nie et al. (2020) and use the concatenated ANLI, MNLI and SNLI training sets, which we refer to as ANLI+ . For all three natural language inference tasks, examples consist of premise and hypothesis sentence pairs, and the task is to classify the relationship between the premise and hypothesis as entailment, contradiction, or neutral. CommonsenseQA CommonsenseQA (Talm"
2020.aacl-main.56,P17-1178,0,0.0173271,"experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are sentence retrieval tasks that do not include training sets, and are scored based on the similarity of learned representations (see Appendix A). XQuAD, TyDiQA and Tatoeba do not include development sets separate from the test sets.4 For all XTREME tasks, we follow the training and evaluation protocol described in the benchmark p"
2020.aacl-main.56,2020.emnlp-main.617,0,0.103534,"Missing"
2020.aacl-main.56,2020.acl-demos.15,1,0.91429,"ta before using it on a non-English target task, which can lead to the catastrophic forgetting of other languages acquired during pretraining. We investigate whether continuing to train on the multilingual MLM pretraining objective while fine-tuning on an English intermediate task can prevent catastrophic forgetting of the target languages and improve downstream transfer performance. We construct a multilingual corpus across the 40 languages covered by the XTREME benchmark using Wikipedia dumps from April 14, 2020 for each language and the MLM data creation scripts from the jiant 1.3 library (Phang et al., 2020). In total, we use 2 million sentences sampled across all 40 languages using the sampling ratio from Conneau and Lample (2019) with ↵ = 0.3. 2.4 Translated Intermediate-Task Training Large-scale labeled datasets are rarely available in languages other than English for most languageunderstanding benchmark tasks. Given the availability of increasingly performant machine translation models, we investigate if using machinetranslated intermediate-task data can improve samelanguage transfer performance, compared to using English intermediate task data. We translate training and validation data of th"
2020.aacl-main.56,2020.eamt-1.61,0,0.0114263,"sks. Given the availability of increasingly performant machine translation models, we investigate if using machinetranslated intermediate-task data can improve samelanguage transfer performance, compared to using English intermediate task data. We translate training and validation data of three intermediate tasks: QQP, HellaSwag, and MNLI. We choose these tasks based on the size of the training sets and because their examplelevel (rather than word-level) labels can be easily mapped onto translated data. To translate QQP and HellaSwag, we use pretrained machine translation models from OPUS-MT (Tiedemann and Thottingal, 2020). These models are trained with Marian-NMT (Junczys-Dowmunt et al., 2018) on OPUS data (Tiedemann, 2012), which integrates several resources depending on the available corpora for the language pair. For MNLI, we use the publicly available machine-translated training data of XNLI provided by the XNLI authors.6 We use German, Russian, and Swahili translations of 3 http://data.quora.com/ First-Quora-DatasetRelease-Question-Pairs 4 UDPOS also does not include development sets for Kazakh, Thai, Tagalog or Yoruba. 560 5 https://github.com/google-research/ xtreme 6 According to Conneau et al. (2018),"
2020.aacl-main.56,P19-1439,1,0.784162,"Missing"
2020.aacl-main.56,N18-1101,1,0.808471,"in intermediate training. CCG involves token-wise prediction and is similar to the POS and NER target tasks. Both versions of SQuAD are widely-used questionanswering tasks, while QQP is semantically similar to sentence retrieval target tasks (BUCC and Tatoeba) as well as PAWS-X, another paraphrasedetection task. ANLI + MNLI + SNLI (ANLI+ ) The Adversarial Natural Language Inference dataset (Nie et al., 2020) is collected using model-in-the-loop crowdsourcing as an extension of the Stanford Natural Language Inference (SNLI; Bowman et al., 2015) and Multi-Genre Natural Language Inference (MNLI; Williams et al., 2018) corpora. We follow Nie et al. (2020) and use the concatenated ANLI, MNLI and SNLI training sets, which we refer to as ANLI+ . For all three natural language inference tasks, examples consist of premise and hypothesis sentence pairs, and the task is to classify the relationship between the premise and hypothesis as entailment, contradiction, or neutral. CommonsenseQA CommonsenseQA (Talmor et al., 2019) is a multiple-choice QA dataset generated by crowdworkers based on clusters of concepts from ConceptNet (Speer et al., 2017). Cosmos QA Cosmos QA is multiple-choice commonsense-based reading com"
2020.aacl-main.56,D19-1382,0,0.024611,"l questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments. 2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering (XQuAD; Artetxe et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigenbaum et al., 2017, 2018), which requires identifying parallel sentences from corpora of different languages; and Tatoeba (Artetxe and Schwenk, 2019), which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are sentence retrieval tasks that do not include training sets, and are scored based on the similarity of learned representations (see Appendix A). XQuAD, TyDiQA and Tatoeba do not include development sets separate from the test sets.4 For"
2020.aacl-main.56,D18-1009,0,0.0247261,"-choice QA dataset generated by crowdworkers based on clusters of concepts from ConceptNet (Speer et al., 2017). Cosmos QA Cosmos QA is multiple-choice commonsense-based reading comprehension dataset (Huang et al., 2019b) generated by crowdworkers, with a focus on the causes and effects of events. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning dataset framed as a fourway multiple choice task, where examples consist of an incomplete paragraph and four choices of spans, only one of which is a plausible continuation of the scenario. It is built using adversarial filtering (Zellers et al., 2018; Le Bras et al., 2020) with BERT. 2 If a word is tokenized into sub-word tokens, we use the representation of the first token for the tag prediction for that word as in Devlin et al. (2019a). 559 and their sample implementation.5 Intermediateand target-task statistics are shown in Table 1. MNLI In additional to the full ANLI+ , we also consider the MNLI task as a standalone intermediate task because of its already large and diverse training set. QQP Quora Question Pairs3 is a paraphrase detection dataset. Examples in the dataset consist of two questions, labeled for whether they are semantica"
2020.aacl-main.68,P19-1309,0,0.0123084,"f the others to yield candidate sentence pairs. Since the alignment behind the corpus can be noisy, the resulting sentence pairs range almost continuously from being parallel to being semantically unrelated, potentially fitting any of the three entailment relationships. In the T RANSLATE protocol, we investigate whether we can use such sentence pairs as entailment data. We use WikiMatrix (Schwenk et al., 2019), a collection of 135 million Wikipedia parallel sentences, which was constructed by aligning similar sentences in different languages in a joint sentence embedding space (Schwenk, 2018; Artetxe and Schwenk, 2019). It is a mix of translated sentence pairs and comparable sentences written independently about the same information. We collect parallel sentences where one of the sentences is in English, sE . For the paired non-English languages, we pick 5 languages: German, French, Indonesian, Japanese, and Czech. We then translate the aligned non-English sentence into an English senˆ tence, sE using the OPUS-MT (Tiedemann and Thottingal, 2020) machine translation systems, and ˆ treat (sE , sE ) as a sentence pair. The diverse set of languages allows us to collect a more diverse set Individual == Gold No G"
2020.aacl-main.68,Q17-1010,0,0.0193027,"icate yes, true, or confirmed on items in a list. Table 1: Examples of sentence pairs chosen randomly from each test set, along with their assigned labels. E: entailment, C: contradiction, N: neutral. indexing and retrieval, reranking, and crowdworker labeling. To collect a set of sentence pairs with a reasonable label distribution, for each query, we retrieve top-K matches and rerank the (query, retrieved sentence) pairs using the following features: Indexing and Retrieval Given a raw text, we first split it into sentences.3 We encode each sentence as a 300-dimensional vector using fastText (Bojanowski et al., 2017) and index them using FAISS (Johnson et al., 2019), an open-source library for large-scale similarity search on vectors.4 Since Gigaword and Wikipedia consist of billions of sentences, we perform dimensionality reduction using PCA and cluster the search space to allow efficient index and retrieval. We randomly sample query sentences from the text corpus and retrieve the top 1k most similar sentences for each query. This is done by building an index with type &quot;PCAR64,IVFx,Flat&quot; in FAISS terms, where x varies depending on the corpus size. Details of our indexing and retrieval procedures can be f"
2020.aacl-main.68,D15-1075,1,0.774474,"anguage inference, which lend themselves to non-expert crowdsourcing. These datasets are useful in three settings: evaluation (Williams et al., 2018; Rajpurkar et al., 2018; Zellers et al., 2019); pretraining (Phang et al., 2018; Conneau et al., 2018; Pruksachatkun et al., 2020); and as training data for downstream tasks (Trivedi et al., 2019; Portelli et al., 2020). Natural language inference (NLI), also known as recognizing textual entailment (RTE; Dagan et al., 2005) is the problem of determining whether or not a hypothesis semantically entails a premise. The two largest NLI corpora, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise sentence taken from a preexisting text corpus. While these datasets have been widely used as benchmarks for NLU, there have been no studies evaluating writing-based annotation for collecting NLI data. Moreover, there is growing evidence that human writing can introduce annotation artifacts, which enable models to perform moderately well just by learning spurious statistical patterns in the data (Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018a). This pap"
2020.aacl-main.68,D18-1269,1,0.84731,"orker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes. 1 Introduction Research on natural language understanding has benefited greatly from the availability of largescale, annotated data, especially for tasks like reading comprehension and natural language inference, which lend themselves to non-expert crowdsourcing. These datasets are useful in three settings: evaluation (Williams et al., 2018; Rajpurkar et al., 2018; Zellers et al., 2019); pretraining (Phang et al., 2018; Conneau et al., 2018; Pruksachatkun et al., 2020); and as training data for downstream tasks (Trivedi et al., 2019; Portelli et al., 2020). Natural language inference (NLI), also known as recognizing textual entailment (RTE; Dagan et al., 2005) is the problem of determining whether or not a hypothesis semantically entails a premise. The two largest NLI corpora, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise sentence taken from a preexisting text corpus. While these datasets have been widely used as benchmar"
2020.aacl-main.68,N19-1423,0,0.00951342,"it is used as intermediate-task data (Phang et al., 2018; Pruksachatkun et al., 2020). As our col8 This is consistent with the recent findings of Zhang et al. (2020) and Mosbach et al. (2020) regarding fine-tuning BERTstyle models on small data. lected datasets are fairly small (< 10K examples), we use five data-poor downstream target tasks in the SuperGLUE benchmark (Wang et al., 2019a): COPA (Roemmele et al., 2011); WSC (Levesque et al., 2012); RTE (Dagan et al., 2005, et seq), WiC (Pilehvar and Camacho-Collados, 2019); and MultiRC (Khashabi et al., 2018). We experiment with the BERTLarge (Devlin et al., 2019) and RoBERTaLarge models. We follow Pruksachatkun et al. (2020) for training hyperparameters. We use the Adam optimizer (Kingma and Ba, 2015). We run experiments using the jiant toolkit (Wang et al., 2019b), which is the recommended baseline package for SuperGLUE, and is based on Pytorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2020), and AllenNLP (Gardner et al., 2017). 4.1 NLI Experiments Table 4 reports the model performance on individual test sets. We include a baseline training data, a 3k randomly sampled training examples from MNLI (MNLI-3k). We observe that all the"
2020.aacl-main.68,N15-1072,0,0.0259331,"d’s second-biggest hamburger chain, as well as US frozen foods manufacturer Pillsbury, which produces the luxury ice-cream HaagenDaazs. GrandMet owns Burger King, the world’s second-biggest hamburger chain, as well as US food group Pillsbury, which produces the luxury icecream Haagen-Daazs. E Translation divergence Translate-Wiki Marcus Claudius then abducted her while she was on her way to school. Marcus Claudius then kidnapped him while he was on his way to school. N Syntactic structure Label Table 8: Dataset observations from our new protocols. On the topic of cost-effective crowdsourcing, Gao et al. (2015) develop a method to reduce redundant translations when collecting human translated data. When the annotation budget is fixed, Khetan et al. (2018) suggest that it is better to label collect single label per training example as many as possible, rather than collecting less training examples with multiple labels. 7 that we use state-of-the-art tools including FAISS, RoBERTa, and OPUS, and refine our methods with several rounds of piloting and tuning, we are skeptical that there is low-hanging fruit in the two directions we explored. A more radically different direction might involve generating"
2020.aacl-main.68,N18-2017,1,0.89517,"Missing"
2020.aacl-main.68,N18-1023,0,0.0184021,"each dataset can improve downstream task performance when it is used as intermediate-task data (Phang et al., 2018; Pruksachatkun et al., 2020). As our col8 This is consistent with the recent findings of Zhang et al. (2020) and Mosbach et al. (2020) regarding fine-tuning BERTstyle models on small data. lected datasets are fairly small (< 10K examples), we use five data-poor downstream target tasks in the SuperGLUE benchmark (Wang et al., 2019a): COPA (Roemmele et al., 2011); WSC (Levesque et al., 2012); RTE (Dagan et al., 2005, et seq), WiC (Pilehvar and Camacho-Collados, 2019); and MultiRC (Khashabi et al., 2018). We experiment with the BERTLarge (Devlin et al., 2019) and RoBERTaLarge models. We follow Pruksachatkun et al. (2020) for training hyperparameters. We use the Adam optimizer (Kingma and Ba, 2015). We run experiments using the jiant toolkit (Wang et al., 2019b), which is the recommended baseline package for SuperGLUE, and is based on Pytorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2020), and AllenNLP (Gardner et al., 2017). 4.1 NLI Experiments Table 4 reports the model performance on individual test sets. We include a baseline training data, a 3k randomly sampled trainin"
2020.aacl-main.68,marelli-etal-2014-sick,0,0.0607142,"ight be a side-effect of high word overlap in the data, which prefers similar words in the premise and hypothesis. This is also a well-known artifact for NLI data (McCoy et al., 2019). Related Work There is a large body of work on constructing data for natural language inference. The first test suite for entailment problems, FraCas (Consortium et al., 1996), is a very small set created manually by experts to isolate phenomena of interest. The RTE challenge corpora (Dagan et al., 2005, et seq) were built by asking human annotators to judge whether a text entails a hypothesis. The SICK dataset (Marelli et al., 2014) is constructed by mining existing paraphrase sentence pairs from image and video captions, which annotators then label. Some recent works also use automatic methods for generating sentence pairs for entailment data. Zhang et al. (2017) propose a framework to generate hypotheses based on context from general world knowledge or neural sequence-to-sequence methods. The DNC corpus (Poliak et al., 2018a) is an NLI dataset with ordinal judgments constructed by recasting several NLP datasets to NLI examples and labeling them using custom automatic procedures. QA-NLI (Demszky et al., 2018) is an NLI"
2020.aacl-main.68,P19-1334,0,0.124073,"SLATE have slightly unbalanced distributions compared to BASE. In particular, for S IM, we observe that the entailment class has the lowest distribution in the training and test data. One clear difference between BASE and our new protocols is the hypothesis length. S IM and T RANSLATE tend to create longer hypothesis than BASE. We suspect that this is an artifact of the sentence-similarity method, which prefers identical sentences (both syntax and semantics) over semantically similar sentences. Across domains, we observe that sentences from news texts are longer than Wikipedia. Recent work by McCoy et al. (2019) shows that popular NLI models might learn a simple lexical overlap heuristic for predicting entailment labels. While this heuristic is natural for entailment, it can affect the model’s generalization especially when it is strongly reflected in the data. We calculate word type overlap by using the intersection of premise 7 MNLI used an organized group of crowdworkers hired through Hybrid (gethybrid.io). 3.2 4 Annotation Cost Experiments We aim to test whether our alternative protocols can produce high-quality data that yield models that generalize well within NLI and in transfer learning. For"
2020.aacl-main.68,2020.acl-main.441,0,0.199076,"ongly reflected in the data. We calculate word type overlap by using the intersection of premise 7 MNLI used an organized group of crowdworkers hired through Hybrid (gethybrid.io). 3.2 4 Annotation Cost Experiments We aim to test whether our alternative protocols can produce high-quality data that yield models that generalize well within NLI and in transfer learning. For the NLI evaluation, we evaluate each model on nine test sets: (i) the five new individual test sets, each containing ∼250 examples; (ii) the MNLI development set; and (iii) the three development sets of Adversarial NLI (ANLI; Nie et al., 2020), collected from three rounds of annotation (A1, A2, A3). ANLI is collected using an iterative adversarial approach that follows MNLI but encourages 676 Test Data BN BW SN SW TW MNLI A1 A2 A3 Avg. Base-News Base-Wiki Sim-News Sim-Wiki Translate-Wiki 33.4 34.1 35.4 32.3 37.4 37.8 33.1 35.9 37.2 39.3 32.4 37.9 32.0 52.1 35.4 30.1 35.4 32.3 49.1 35.8 35.8 39.0 37.8 44.6 45.5 35.6 35.6 35.8 36.6 35.4 32.8 33.1 33.1 33.1 33.0 32.8 31.6 32.8 32.4 32.9 33.4 33.2 33.4 32.1 32.8 34.0 34.8 34.3 38.8 36.4 MNLI-3k 79.0 61.3 76.7 57.5 58.1 83.9 33.4 27.0 28.7 56.2 Base-News Base-Wiki Sim-News Sim-Wiki Tran"
2020.aacl-main.68,N19-1128,0,0.0310619,"Missing"
2020.aacl-main.68,W18-5441,0,0.104146,"Missing"
2020.aacl-main.68,S18-2023,0,0.107874,"Missing"
2020.aacl-main.68,2020.fever-1.7,0,0.0179672,"k to focus on improving writing-based annotation processes. 1 Introduction Research on natural language understanding has benefited greatly from the availability of largescale, annotated data, especially for tasks like reading comprehension and natural language inference, which lend themselves to non-expert crowdsourcing. These datasets are useful in three settings: evaluation (Williams et al., 2018; Rajpurkar et al., 2018; Zellers et al., 2019); pretraining (Phang et al., 2018; Conneau et al., 2018; Pruksachatkun et al., 2020); and as training data for downstream tasks (Trivedi et al., 2019; Portelli et al., 2020). Natural language inference (NLI), also known as recognizing textual entailment (RTE; Dagan et al., 2005) is the problem of determining whether or not a hypothesis semantically entails a premise. The two largest NLI corpora, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise sentence taken from a preexisting text corpus. While these datasets have been widely used as benchmarks for NLU, there have been no studies evaluating writing-based annotation for collecting NLI data. Moreover, there is"
2020.aacl-main.68,2020.acl-main.467,1,0.885987,"Missing"
2020.aacl-main.68,P18-2124,0,0.0223705,"ation within NLI or on transfer to outside target tasks. We conclude that crowdworker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes. 1 Introduction Research on natural language understanding has benefited greatly from the availability of largescale, annotated data, especially for tasks like reading comprehension and natural language inference, which lend themselves to non-expert crowdsourcing. These datasets are useful in three settings: evaluation (Williams et al., 2018; Rajpurkar et al., 2018; Zellers et al., 2019); pretraining (Phang et al., 2018; Conneau et al., 2018; Pruksachatkun et al., 2020); and as training data for downstream tasks (Trivedi et al., 2019; Portelli et al., 2020). Natural language inference (NLI), also known as recognizing textual entailment (RTE; Dagan et al., 2005) is the problem of determining whether or not a hypothesis semantically entails a premise. The two largest NLI corpora, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise sentence taken from a p"
2020.aacl-main.68,P18-2037,0,0.0132713,"nguage to one of the others to yield candidate sentence pairs. Since the alignment behind the corpus can be noisy, the resulting sentence pairs range almost continuously from being parallel to being semantically unrelated, potentially fitting any of the three entailment relationships. In the T RANSLATE protocol, we investigate whether we can use such sentence pairs as entailment data. We use WikiMatrix (Schwenk et al., 2019), a collection of 135 million Wikipedia parallel sentences, which was constructed by aligning similar sentences in different languages in a joint sentence embedding space (Schwenk, 2018; Artetxe and Schwenk, 2019). It is a mix of translated sentence pairs and comparable sentences written independently about the same information. We collect parallel sentences where one of the sentences is in English, sE . For the paired non-English languages, we pick 5 languages: German, French, Indonesian, Japanese, and Czech. We then translate the aligned non-English sentence into an English senˆ tence, sE using the OPUS-MT (Tiedemann and Thottingal, 2020) machine translation systems, and ˆ treat (sE , sE ) as a sentence pair. The diverse set of languages allows us to collect a more diverse"
2020.aacl-main.68,2020.eamt-1.61,0,0.018469,"Missing"
2020.aacl-main.68,N19-1302,0,0.0387341,"Missing"
2020.aacl-main.68,L18-1239,0,0.298152,"Missing"
2020.aacl-main.68,N18-1101,1,0.836816,"valuations of generalization within NLI or on transfer to outside target tasks. We conclude that crowdworker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes. 1 Introduction Research on natural language understanding has benefited greatly from the availability of largescale, annotated data, especially for tasks like reading comprehension and natural language inference, which lend themselves to non-expert crowdsourcing. These datasets are useful in three settings: evaluation (Williams et al., 2018; Rajpurkar et al., 2018; Zellers et al., 2019); pretraining (Phang et al., 2018; Conneau et al., 2018; Pruksachatkun et al., 2020); and as training data for downstream tasks (Trivedi et al., 2019; Portelli et al., 2020). Natural language inference (NLI), also known as recognizing textual entailment (RTE; Dagan et al., 2005) is the problem of determining whether or not a hypothesis semantically entails a premise. The two largest NLI corpora, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise"
2020.aacl-main.68,P19-1472,0,0.087992,"ransfer to outside target tasks. We conclude that crowdworker writing still the best known option for entailment data, highlighting the need for further data collection work to focus on improving writing-based annotation processes. 1 Introduction Research on natural language understanding has benefited greatly from the availability of largescale, annotated data, especially for tasks like reading comprehension and natural language inference, which lend themselves to non-expert crowdsourcing. These datasets are useful in three settings: evaluation (Williams et al., 2018; Rajpurkar et al., 2018; Zellers et al., 2019); pretraining (Phang et al., 2018; Conneau et al., 2018; Pruksachatkun et al., 2020); and as training data for downstream tasks (Trivedi et al., 2019; Portelli et al., 2020). Natural language inference (NLI), also known as recognizing textual entailment (RTE; Dagan et al., 2005) is the problem of determining whether or not a hypothesis semantically entails a premise. The two largest NLI corpora, SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise sentence taken from a preexisting text corpus."
2020.acl-demos.15,P19-1356,0,0.019798,"vailability of NLU tasks: jiant should maintain and continue to grow a collection of tasks useful for NLU research, especially popular evaluation tasks and tasks commonly used in pretraining and transfer learning. • Availability of cutting-edge models: jiant should make implementations of state-of-theart models available for experimentation. • Open source: jiant should be free to use, and easy to contribute to. Early versions of jiant have already been used in multiple works, including probing analyses (Tenney et al., 2019b,a; Warstadt et al., 2019; Lin et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019), transfer learning experiments (Wang et al., 2019a; Phang et al., 2018), and dataset and benchmark construction (Wang et al., 2019b, 2018; Kim et al., 2019). 109 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 109–117 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 3.2 jiant Components • Tasks: Tasks have references to task data, methods for processing data, references to classifier heads, and methods for calculating performance metrics, and making predictions. • Sentence Encoder: Sentence encoders map from the indexed ex"
2020.acl-demos.15,S19-1026,1,0.890008,"Missing"
2020.acl-demos.15,W19-4825,0,0.0400825,"Missing"
2020.acl-demos.15,P19-1441,0,0.309115,"task models in jiant’s target training phase. 2 Background Transfer learning is an area of research that uses knowledge from pretrained models to transfer to new tasks. In recent years, Transformer-based models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2019) have yielded state-of-the-art results on the lion’s share of benchmark tasks for language understanding through pretraining and transfer, often paired with some form of multitask learning. jiant enables a variety of complex training pipelines through simple configuration changes, including multi-task training (Caruana, 1993; Liu et al., 2019a) and pretraining, as well as the sequential fine-tuning approach from STILTs (Phang et al., 2018). In STILTs, intermediate task training takes a pretrained model like ELMo or BERT, and applies supplementary training on a set of intermediate tasks, before finally performing single-task training on additional downstream tasks. 3 3.1 • Task-Specific Output Heads: Task-specific output modules map representations from sentence encoders to outputs specific to a task, e.g. entailment/neutral/contradiction for NLI tasks, or tags for part-of-speech tagging. They also include logic for computing the c"
2020.acl-demos.15,D14-1162,0,0.0940934,"n = 1 do_target_task_training = 1 do_full_eval = 1 write_preds = ""val,test"" write_strict_glue_format = 1 input_module = bert-base-cased pretrain_tasks = ""swag,squad"" target_tasks = hellaswag // Task specific configuration commitbank = { val_interval = 60 max_epochs = 40 } • Train a probing classifier over a frozen BERT model, as in Tenney et al. (2019a): input_module = bert-base-cased target_tasks = edges-dpr transfer_paradigm = frozen Figure 3: Example jiant experiment config file. as PRPN (Shen et al., 2018) and ON-LSTM (Shen et al., 2019). jiant also supports word embeddings such as GloVe (Pennington et al., 2014). 3.5 Example jiant Use Cases and Options • Compare performance of GloVe (Pennington et al., 2014) embeddings using a BiLSTM: input_module = glove sent_enc = rnn • Evaluate ALBERT (Lan et al., 2019) on the MNLI (Williams et al., 2018) task: User Interface input_module = albert-large-v2 target_task = mnli jiant experiments can be run with a simple CLI: python -m jiant  --config_file roberta_with_mnli.conf  --overrides ""target_tasks = swag,  run_name = swag_01"" 3.7 jiant provides default config files that allow running many experiments without modifying source code. jiant also provides baseli"
2020.acl-demos.15,N18-1202,0,0.0637641,"Model resources in jiant jiant supports over 50 tasks. Task types include classification, regression, sequence generation, tagging, masked language modeling, and span prediction. jiant focuses on NLU tasks like MNLI (Williams et al., 2018), CommonsenseQA (Talmor et al., 2019), the Winograd Schema Challenge (Levesque et al., 2012), and SQuAD (Rajpurkar et al., 2016). A full inventory of tasks and task variants is available in the jiant/tasks module. jiant provides support for cutting-edge sentence encoder models, including support for Huggingface’s Transformers. Supported models include: ELMo (Peters et al., 2018), GPT (Radford, 2018), BERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019b), and ALBERT (Lan et al., 2019). jiant also supports the from-scratch training of (bidirectional) LSTMs (Hochreiter and Schmidhuber, 1997) and deep bag of words models (Iyyer et al., 2015), as well as syntax-aware models such 111 and evaluation against GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) benchmarks. More advanced configurations can be developed by composing multiple configurations files and overrides. Figu"
2020.acl-demos.15,D16-1264,0,0.437691,"encoder’s weights can optionally be left frozen, or be included in the training procedure. 5 Tasks can be sampled using a variety of sample weighting methods, e.g., uniform or proportional to the tasks’ number of training batches or examples. 3.4 Task and Model resources in jiant jiant supports over 50 tasks. Task types include classification, regression, sequence generation, tagging, masked language modeling, and span prediction. jiant focuses on NLU tasks like MNLI (Williams et al., 2018), CommonsenseQA (Talmor et al., 2019), the Winograd Schema Challenge (Levesque et al., 2012), and SQuAD (Rajpurkar et al., 2016). A full inventory of tasks and task variants is available in the jiant/tasks module. jiant provides support for cutting-edge sentence encoder models, including support for Huggingface’s Transformers. Supported models include: ELMo (Peters et al., 2018), GPT (Radford, 2018), BERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019b), and ALBERT (Lan et al., 2019). jiant also supports the from-scratch training of (bidirectional) LSTMs (Hochreiter and Schmidhuber, 1997) and deep bag of words models (Iyyer et al"
2020.acl-demos.15,D19-1454,0,0.0272747,"Missing"
2020.acl-demos.15,N19-1421,0,0.0799595,"s batches randomly from one or more tasks,5 and trains the shared model. 4 The sentence encoder’s weights can optionally be left frozen, or be included in the training procedure. 5 Tasks can be sampled using a variety of sample weighting methods, e.g., uniform or proportional to the tasks’ number of training batches or examples. 3.4 Task and Model resources in jiant jiant supports over 50 tasks. Task types include classification, regression, sequence generation, tagging, masked language modeling, and span prediction. jiant focuses on NLU tasks like MNLI (Williams et al., 2018), CommonsenseQA (Talmor et al., 2019), the Winograd Schema Challenge (Levesque et al., 2012), and SQuAD (Rajpurkar et al., 2016). A full inventory of tasks and task variants is available in the jiant/tasks module. jiant provides support for cutting-edge sentence encoder models, including support for Huggingface’s Transformers. Supported models include: ELMo (Peters et al., 2018), GPT (Radford, 2018), BERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019b), and ALBERT (Lan et al., 2019). jiant also supports the from-scratch training of (bidire"
2020.acl-demos.15,P19-1452,1,0.896619,"t and reproducible experiments, including logging and saving and restoring model state. • Availability of NLU tasks: jiant should maintain and continue to grow a collection of tasks useful for NLU research, especially popular evaluation tasks and tasks commonly used in pretraining and transfer learning. • Availability of cutting-edge models: jiant should make implementations of state-of-theart models available for experimentation. • Open source: jiant should be free to use, and easy to contribute to. Early versions of jiant have already been used in multiple works, including probing analyses (Tenney et al., 2019b,a; Warstadt et al., 2019; Lin et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019), transfer learning experiments (Wang et al., 2019a; Phang et al., 2018), and dataset and benchmark construction (Wang et al., 2019b, 2018; Kim et al., 2019). 109 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 109–117 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 3.2 jiant Components • Tasks: Tasks have references to task data, methods for processing data, references to classifier heads, and methods for calculating performance m"
2020.acl-demos.15,P19-1439,1,0.902222,"Missing"
2020.acl-demos.15,W18-5446,1,0.877923,"Missing"
2020.acl-demos.15,D19-1286,1,0.875639,"iments, including logging and saving and restoring model state. • Availability of NLU tasks: jiant should maintain and continue to grow a collection of tasks useful for NLU research, especially popular evaluation tasks and tasks commonly used in pretraining and transfer learning. • Availability of cutting-edge models: jiant should make implementations of state-of-theart models available for experimentation. • Open source: jiant should be free to use, and easy to contribute to. Early versions of jiant have already been used in multiple works, including probing analyses (Tenney et al., 2019b,a; Warstadt et al., 2019; Lin et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019), transfer learning experiments (Wang et al., 2019a; Phang et al., 2018), and dataset and benchmark construction (Wang et al., 2019b, 2018; Kim et al., 2019). 109 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 109–117 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics 3.2 jiant Components • Tasks: Tasks have references to task data, methods for processing data, references to classifier heads, and methods for calculating performance metrics, and making predict"
2020.acl-demos.15,N18-1101,1,0.936823,"e intermediate phase the trainer samples batches randomly from one or more tasks,5 and trains the shared model. 4 The sentence encoder’s weights can optionally be left frozen, or be included in the training procedure. 5 Tasks can be sampled using a variety of sample weighting methods, e.g., uniform or proportional to the tasks’ number of training batches or examples. 3.4 Task and Model resources in jiant jiant supports over 50 tasks. Task types include classification, regression, sequence generation, tagging, masked language modeling, and span prediction. jiant focuses on NLU tasks like MNLI (Williams et al., 2018), CommonsenseQA (Talmor et al., 2019), the Winograd Schema Challenge (Levesque et al., 2012), and SQuAD (Rajpurkar et al., 2016). A full inventory of tasks and task variants is available in the jiant/tasks module. jiant provides support for cutting-edge sentence encoder models, including support for Huggingface’s Transformers. Supported models include: ELMo (Peters et al., 2018), GPT (Radford, 2018), BERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019b), and ALBERT (Lan et al., 2019). jiant also supports"
2020.acl-demos.15,D18-1009,0,0.0224427,"ed"" // Data and preprocessing settings max_seq_len = 256 // Model settings input_module = ""bert-large-cased"" transformers_output_mode = ""top"" s2s = { attention = none } sent_enc = ""none"" sep_embs_for_skip = 1 classifier = log_reg // fine-tune entire BERT model transfer_paradigm = finetune // Training settings dropout = 0.1 optimizer = bert_adam batch_size = 4 max_epochs = 10 lr = .00001 min_lr = .0000001 lr_patience = 4 patience = 20 max_vals = 10000 3.6 Here we highlight some example use cases and key corresponding jiant config options required in these experiments: • Fine-tune BERT on SWAG (Zellers et al., 2018) and SQUAD (Rajpurkar et al., 2016), then fine-tune on HellaSwag (Zellers et al., 2019): // Phase configuration do_pretrain = 1 do_target_task_training = 1 do_full_eval = 1 write_preds = ""val,test"" write_strict_glue_format = 1 input_module = bert-base-cased pretrain_tasks = ""swag,squad"" target_tasks = hellaswag // Task specific configuration commitbank = { val_interval = 60 max_epochs = 40 } • Train a probing classifier over a frozen BERT model, as in Tenney et al. (2019a): input_module = bert-base-cased target_tasks = edges-dpr transfer_paradigm = frozen Figure 3: Example jiant experiment con"
2020.acl-demos.15,P19-1472,0,0.0182519,"= ""bert-large-cased"" transformers_output_mode = ""top"" s2s = { attention = none } sent_enc = ""none"" sep_embs_for_skip = 1 classifier = log_reg // fine-tune entire BERT model transfer_paradigm = finetune // Training settings dropout = 0.1 optimizer = bert_adam batch_size = 4 max_epochs = 10 lr = .00001 min_lr = .0000001 lr_patience = 4 patience = 20 max_vals = 10000 3.6 Here we highlight some example use cases and key corresponding jiant config options required in these experiments: • Fine-tune BERT on SWAG (Zellers et al., 2018) and SQUAD (Rajpurkar et al., 2016), then fine-tune on HellaSwag (Zellers et al., 2019): // Phase configuration do_pretrain = 1 do_target_task_training = 1 do_full_eval = 1 write_preds = ""val,test"" write_strict_glue_format = 1 input_module = bert-base-cased pretrain_tasks = ""swag,squad"" target_tasks = hellaswag // Task specific configuration commitbank = { val_interval = 60 max_epochs = 40 } • Train a probing classifier over a frozen BERT model, as in Tenney et al. (2019a): input_module = bert-base-cased target_tasks = edges-dpr transfer_paradigm = frozen Figure 3: Example jiant experiment config file. as PRPN (Shen et al., 2018) and ON-LSTM (Shen et al., 2019). jiant also suppo"
2020.acl-demos.15,J07-3004,0,\N,Missing
2020.acl-demos.15,P15-1162,0,\N,Missing
2020.acl-demos.15,N19-1419,0,\N,Missing
2020.acl-demos.15,N19-1300,0,\N,Missing
2020.acl-demos.15,N19-1423,0,\N,Missing
2020.acl-main.467,N19-1300,0,0.428418,"2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark et al., 2019a; Sap et al., 2019), also referred to as ∗ Equal contribution. • Which linguistic skills does a model learn from intermediate-task training? • Which skills learned from intermediate tasks help the model succeed on which target tasks? The first question is the most straightforward: it can be answered by a sufficiently exhaustive search over possible intermediate–target task pairs. The second and third questions address the why rather than the when, and differ in a crucial detail: A 5231 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231–5247 c J"
2020.acl-main.467,W19-4828,0,0.39453,"2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark et al., 2019a; Sap et al., 2019), also referred to as ∗ Equal contribution. • Which linguistic skills does a model learn from intermediate-task training? • Which skills learned from intermediate tasks help the model succeed on which target tasks? The first question is the most straightforward: it can be answered by a sufficiently exhaustive search over possible intermediate–target task pairs. The second and third questions address the why rather than the when, and differ in a crucial detail: A 5231 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231–5247 c J"
2020.acl-main.467,P18-1198,0,0.363494,"raining, and fine-tuning on a target or probing task. on each target and probing task individually. Target tasks are tasks of interest to the general community, spanning various facets of natural language, domains, and sources. Probing tasks, while potentially similar in data source to target tasks such as with CoLA, are designed to isolate the presence of particular linguistic capabilities or skills. For instance, solving the target task BoolQ (Clark et al., 2019a) may require various skills including coreference and commonsense reasoning, while probing tasks like the SentEval probing suite (Conneau et al., 2018) target specific syntactic and metadatalevel phenomena such as subject-verb agreement and sentence length detection. 2.2 Tasks Table 1 presents an overview of the intermediate and target tasks. 2.2.1 Intermediate Tasks We curate a diverse set of tasks that either represent an especially large annotation effort or that have been shown to yield positive transfer in prior work. The resulting set of tasks cover question answering, commonsense reasoning, and natural language inference. QAMR The Question–Answer Meaning Representations dataset (Michael et al., 2018) is a crowdsourced QA task consisti"
2020.acl-main.467,N19-1423,0,0.358065,"en question. We perform a broad survey of intermediate and target task pairs, following an experimental pipeline similar to Phang et al. (2018) and Wang et al. (2019a). This differs from previous work in that we use a larger and more diverse set of intermediate and target tasks, introduce additional analysis-oriented probing tasks, and use a better-performing base model RoBERTa (Liu et al., 2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark et al., 2019a; Sap et al., 2019), also referred to as ∗ Equal contribution. • Which linguistic skills does a model learn from intermediate-task training? • Which skills learned from intermediate tasks help the model succe"
2020.acl-main.467,D15-1076,0,0.0227138,"at is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentence classification version of the task. QQP The Quora Question Pairs dataset1 is constructed based on questions posted on the community question"
2020.acl-main.467,W09-2415,0,0.124413,"Missing"
2020.acl-main.467,N19-1419,0,0.0393455,"ntermediate task, three-step procedure. Similar work (Lin et al., 2019b) has been done on cross-lingual transfer—the analogous challenge of transferring learned knowledge from a highresource to a low-resource language. Many recent works have attempted to understand the knowledge and linguistic skills BERT learns, for instance by analyzing the language model surprisal for subject–verb agreements (Goldberg, 2018), identifying specific knowledge or phenomena encapsulated in the representations learned by BERT using probing tasks (Tenney et al., 2019b,a; Warstadt et al., 2019a; Lin et al., 2019a; Hewitt and Manning, 2019; Jawahar et al., 2019), analyzing the attention heads of BERT (Clark et al., 2019b; 5238 Coenen et al., 2019; Lin et al., 2019a; Htut et al., 2019), and testing the linguistic generalizations of BERT across runs (McCoy et al., 2019). However, relatively little work has been done to analyze fine-tuned BERT-style models (Wang et al., 2019a; Warstadt et al., 2019a). 6 Conclusion and Future Work This paper presents a large-scale study on when and why intermediate-task training works with pretrained models. We perform experiments on RoBERTa with a total of 110 pairs of intermediate and target task"
2020.acl-main.467,J07-3004,0,0.00896312,"et al., 2019). The questions concern the causes or effects of events that require reasoning not only based on the exact text spans in the context, but also wide-range abstractive commonsense reasoning. It differs from CommonsenseQA in that it focuses on causal and deductive commensense reasoning and that it requires reading comprehension over an auxiliary passage, rather than simply answering a freestanding question. SocialIQA SocialIQA (Sap et al., 2019) is a task for multiple choice QA. It tests for reasoning surrounding emotional and social intelligence in everyday situations. CCG CCGbank (Hockenmaier and Steedman, 2007) is a task that is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling data"
2020.acl-main.467,N06-2015,0,0.0334048,"ich tests a model’s ability to understand how ideas in the various clauses relate to each other. AJ-EOS is a task that tests a model’s ability to identify grammatical sentences without indicators such as punctuation marks and capitalization, and consists of grammatical text that are removed of punctuation. Edge-Probing Tasks The edge probing (EP) tasks are a set of core NLP labeling tasks, collected by Tenney et al. (2019b) and cast into Boolean classification. These tasks focus on the syntactic and semantic relations between spans in a sentence. The first five tasks use the OntoNotes corpus (Hovy et al., 2006): Part-of-Speech tagging (EP-POS) is a task that tests a model’s ability to predict the syntactic category (noun, verb, adjective, etc.) for each word in the sentence. Named entity recognition (EP-NER) is task that tests a model’s abil5234 ity to predict the category of an entity in a given span. Semantic Role Labeling (EP-SRL) is a task that tests a model’s ability to assign a label to a given span of words that indicates its semantic role (agent, goal, etc.) in the sentence. Coreference (EP-Coref) is a task that tests a model’s ability to classify if two spans of tokens refer to the same ent"
2020.acl-main.467,D19-1243,0,0.0571475,"Missing"
2020.acl-main.467,P19-1356,0,0.0177692,"ep procedure. Similar work (Lin et al., 2019b) has been done on cross-lingual transfer—the analogous challenge of transferring learned knowledge from a highresource to a low-resource language. Many recent works have attempted to understand the knowledge and linguistic skills BERT learns, for instance by analyzing the language model surprisal for subject–verb agreements (Goldberg, 2018), identifying specific knowledge or phenomena encapsulated in the representations learned by BERT using probing tasks (Tenney et al., 2019b,a; Warstadt et al., 2019a; Lin et al., 2019a; Hewitt and Manning, 2019; Jawahar et al., 2019), analyzing the attention heads of BERT (Clark et al., 2019b; 5238 Coenen et al., 2019; Lin et al., 2019a; Htut et al., 2019), and testing the linguistic generalizations of BERT across runs (McCoy et al., 2019). However, relatively little work has been done to analyze fine-tuned BERT-style models (Wang et al., 2019a; Warstadt et al., 2019a). 6 Conclusion and Future Work This paper presents a large-scale study on when and why intermediate-task training works with pretrained models. We perform experiments on RoBERTa with a total of 110 pairs of intermediate and target tasks, and perform an analy"
2020.acl-main.467,N18-1023,0,0.0451025,"of texts, a pronoun from each text, and a list of possible noun phrases from each text. The dataset has been designed such that world knowledge is required to determine which of the possible noun phrases is the correct referent to the pronoun. We use the SuperGLUE binary classification cast of the task, where each example consists of a text, a pronoun, and a noun phrase from the text, which models must classify as being coreferent to the pronoun or not. Recognizing Textual Entailment (RTE; Dagan et al., 2005, et seq) is a textual entailment task. Multi-Sentence Reading Comprehension (MultiRC; Khashabi et al., 2018) is a multi-hop QA task that consists of paragraphs, a question on each paragraph, and a list of possible answers, in which models must distinguish which of the possible answers are true and which are false. Word-in-Context (WiC; Pilehvar and Camacho-Collados, 2019) is a binary classification word sense disambiguation task. Examples consist of two text snippets, with a polysemous word that appears in both. Models must determine whether the same sense of the word is used in both contexts. BoolQ (Clark et al., 2019a) is a QA task that consists of passages and a yes/no question associated with ea"
2020.acl-main.467,S19-1026,1,0.807373,"Missing"
2020.acl-main.467,W19-4825,0,0.0643425,"Missing"
2020.acl-main.467,P19-1301,0,0.0622512,"Missing"
2020.acl-main.467,N19-1128,0,0.0351443,"Missing"
2020.acl-main.467,P19-1441,0,0.385592,"robing tasks. STILTs. However, this approach does not always improve target task performance, and it is unclear under what conditions it does. This paper offers a large-scale empirical study aimed at addressing this open question. We perform a broad survey of intermediate and target task pairs, following an experimental pipeline similar to Phang et al. (2018) and Wang et al. (2019a). This differs from previous work in that we use a larger and more diverse set of intermediate and target tasks, introduce additional analysis-oriented probing tasks, and use a better-performing base model RoBERTa (Liu et al., 2019b). We aim to answer the following specific questions: • What kind of tasks tend to make good intermediate tasks across a wide variety of target tasks? Introduction Unsupervised pretraining—e.g., BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b)—has recently pushed the state of the art on many natural language understanding tasks. One method of further improving pretrained models that has been shown to be broadly helpful is to first finetune a pretrained model on an intermediate task, before fine-tuning again on the target task of interest (Phang et al., 2018; Wang et al., 2019a; Clark"
2020.acl-main.467,N18-2089,0,0.0254086,"s like the SentEval probing suite (Conneau et al., 2018) target specific syntactic and metadatalevel phenomena such as subject-verb agreement and sentence length detection. 2.2 Tasks Table 1 presents an overview of the intermediate and target tasks. 2.2.1 Intermediate Tasks We curate a diverse set of tasks that either represent an especially large annotation effort or that have been shown to yield positive transfer in prior work. The resulting set of tasks cover question answering, commonsense reasoning, and natural language inference. QAMR The Question–Answer Meaning Representations dataset (Michael et al., 2018) is a crowdsourced QA task consisting of question–answer pairs that correspond to predicate–argument relationships. It is derived from Wikinews and Wikipedia sentences. For example, if the sentence is “Ada Lovelace was a computer scientist.”, a potential question is “What is Ada’s last name?”, with the answer being “Lovelace.” CommonsenseQA CommonsenseQA (Talmor et al., 2019) is a multiple-choice QA task derived from ConceptNet (Speer et al., 2017) with the help of crowdworkers, that is designed to test a range of commonsense knowledge. Intermediate Task Training We fine-tune RoBERTa on each i"
2020.acl-main.467,D12-1071,0,0.0515117,"ict the fine-grained non-exclusive semantic attributes of a given span. Edge probing uses two datasets for SPR: SPR1 (EP-SPR1) (Teichert et al., 2017), derived from the Penn Treebank, and SPR2 (EP-SPR2) (Rudinger et al., 2018), derived from the English Web Treebank. Relation classification (EP-Rel) is a task that tests a model’s ability to predict the relation between two entities. We use the SemEval 2010 Task 8 dataset (Hendrickx et al., 2009) for this task. For example, the relation between “Yeri” and “Korea” in “Yeri is from Korea” is ENTITY-ORIGIN. The Definite Pronoun Resolution dataset (Rahman and Ng, 2012) (EPDPR) is a task that tests a model’s ability to handle coreference, and differs from OntoNotes in that it focuses on difficult cases of definite pronouns. SentEval Tasks The SentEval probing tasks (SE) (Conneau et al., 2018) are cast in the form of single-sentence classification. Sentence Length (SE-SentLen) is a task that tests a model’s ability to classify the length of a sentence. Word Content (SE-WC) is a task that tests a model’s ability to identify which of a set of 1,000 potential words appear in a given sentence. Tree Depth (SETreeDepth) is a task that tests a model’s ability to est"
2020.acl-main.467,D18-1114,0,0.0844869,"Missing"
2020.acl-main.467,D19-1454,0,0.053626,"Missing"
2020.acl-main.467,silveira-etal-2014-gold,1,0.895585,"Missing"
2020.acl-main.467,D13-1170,0,0.00698795,"he most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentence classification version of the task. QQP The Quora Question Pairs dataset1 is constructed based on questions posted on the community question-answering website Quora. The task is to determine if two questions are semantically equivalent. MNLI The Multi-Genre Natural Language Inference dataset (Williams et al., 2018) is a crowdsourced collection of sentence pairs with textual entailment annotations across a variety of genres. 2.2.2 Target Tasks We use ten target tasks, eight of which are drawn from the SuperGL"
2020.acl-main.467,P19-1485,0,0.0248439,"ate strongly with each other and have similar patterns of probing-task performance. 5 Related Work Within the paradigm of training large pretrained Transformer language representations via intermediate-stage training before fine-tuning on a target task, positive transfer has been shown in both sequential task-to-task (Phang et al., 2018) and multi-task-to-task (Liu et al., 2019a; Raffel et al., 2019) formats. Wang et al. (2019a) perform an extensive study on transfer with BERT, finding language modeling and NLI tasks to be among the most beneficial tasks for improving target-task performance. Talmor and Berant (2019) perform a similar cross-task transfer study on reading comprehension datasets, finding similar positive transfer in most cases, with the biggest gains stemming from a combination of multiple QA datasets. Our work consists of a larger, more diverse, set of intermediate task–target task pairs. We also use probing tasks to shed light on the skills learned by the intermediate tasks. Among the prior work on predicting transfer performance, Bingel and Søgaard (2017) is the most similar to ours. They do a regression analysis that predicts target-task performance on the basis of various features of t"
2020.acl-main.467,N19-1421,0,0.0687043,"Missing"
2020.acl-main.467,D17-3004,0,0.107746,"Missing"
2020.acl-main.467,P19-1452,0,0.174188,"e-choice QA task that consists of news articles. For each article, models are given a question about each article with one entity masked out and a list of possible entities from the article, and the goal is to correctly identify the masked entity out of the list. Additionally, we use CommonsenseQA and Cosmos QA as target tasks, due to their unique combination of small dataset size and high level of difficulty for high-performing models like BERT from our set of intermediate tasks. 2.2.3 Probing Tasks We use well-established datasets for our probing tasks, including the edge-probing suite from Tenney et al. (2019b), function word oriented tasks from Kim et al. (2019), and sentence-level probing datasets (SentEval; Conneau et al., 2018). Acceptability Judgment Tasks This set of binary classifications tasks was designed to investigate if a model can judge the grammatical acceptability of a sentence. We use the following five datasets: AJ-CoLA is a task that tests for a model’s understanding of general grammaticality using the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019b), which is drawn from 22 theoretical linguistics publications. The other tasks concern the behaviors of specific c"
2020.acl-main.467,P19-1439,1,0.81674,"Missing"
2020.acl-main.467,D19-1286,1,0.93361,"ks. 2.2.3 Probing Tasks We use well-established datasets for our probing tasks, including the edge-probing suite from Tenney et al. (2019b), function word oriented tasks from Kim et al. (2019), and sentence-level probing datasets (SentEval; Conneau et al., 2018). Acceptability Judgment Tasks This set of binary classifications tasks was designed to investigate if a model can judge the grammatical acceptability of a sentence. We use the following five datasets: AJ-CoLA is a task that tests for a model’s understanding of general grammaticality using the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019b), which is drawn from 22 theoretical linguistics publications. The other tasks concern the behaviors of specific classes of function words, using the dataset by Kim et al. (2019): AJ-WH is a task that tests a model’s ability to detect if a wh-word in a sentence has been swapped with another wh-word, which tests a model’s ability to identify the antecedent associated with the wh-word. AJ-Def is a task that tests a model’s ability to detect if the definite/indefinite articles in a given sentence have been swapped. AJCoord is a task that tests a model’s ability to detect if a coordinating conju"
2020.acl-main.467,Q19-1040,1,0.936652,"ks. 2.2.3 Probing Tasks We use well-established datasets for our probing tasks, including the edge-probing suite from Tenney et al. (2019b), function word oriented tasks from Kim et al. (2019), and sentence-level probing datasets (SentEval; Conneau et al., 2018). Acceptability Judgment Tasks This set of binary classifications tasks was designed to investigate if a model can judge the grammatical acceptability of a sentence. We use the following five datasets: AJ-CoLA is a task that tests for a model’s understanding of general grammaticality using the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019b), which is drawn from 22 theoretical linguistics publications. The other tasks concern the behaviors of specific classes of function words, using the dataset by Kim et al. (2019): AJ-WH is a task that tests a model’s ability to detect if a wh-word in a sentence has been swapped with another wh-word, which tests a model’s ability to identify the antecedent associated with the wh-word. AJ-Def is a task that tests a model’s ability to detect if the definite/indefinite articles in a given sentence have been swapped. AJCoord is a task that tests a model’s ability to detect if a coordinating conju"
2020.acl-main.467,W17-4413,0,0.0426709,"mmonsense knowledge. Intermediate Task Training We fine-tune RoBERTa on each intermediate task. The training procedure follows the standard procedure of fine-tuning a pretrained model on a target task, as described in Devlin et al. (2019). We opt for single intermediate-task training as opposed to multi-task training (cf. Liu et al., 2019a) to isolate the effect of skills learned from individual intermediate tasks. SciTail SciTail (Khot et al., 2018) is a textual entailment task built from multiple-choice science questions from 4th grade and 8th grade exams, as well as crowdsourced questions (Welbl et al., 2017). The task is to determine whether a hypothesis, which is constructed from a science question and its corresponding answer, is entailed or not (neutral) by the premise. Target and Probing Task Fine-Tuning After intermediate-task training, we fine-tune our models Cosmos QA Cosmos QA is a task for a commonsense-based reading comprehension task 5232 Target Tasks Intermediate Tasks Name CommonsenseQA SciTail Cosmos QA SocialIQA CCG HellaSwag QA-SRL SST-2 QAMR QQP MNLI |Train ||Dev |task 9,741 23,596 25,588 33,410 38,015 39,905 44,837 67,349 73,561 363,846 392,702 metrics genre/source 1,221 1,304 3"
2020.acl-main.467,N18-1101,1,0.826025,"ate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentence classification version of the task. QQP The Quora Question Pairs dataset1 is constructed based on questions posted on the community question-answering website Quora. The task is to determine if two questions are semantically equivalent. MNLI The Multi-Genre Natural Language Inference dataset (Williams et al., 2018) is a crowdsourced collection of sentence pairs with textual entailment annotations across a variety of genres. 2.2.2 Target Tasks We use ten target tasks, eight of which are drawn from the SuperGLUE benchmark (Wang et al., 2019b). The tasks in the SuperGLUE benchmark 1 http://data.quora.com/First-Quora-DatasetReleaseQuestion-Pairs 5233 cover question answering, entailment, word sense disambiguation, and coreference resolution and have been shown to be easy for humans but difficult for models like BERT. Although we offer a brief description of the tasks below, we refer readers to the SuperGLUE"
2020.acl-main.467,D18-1009,0,0.0398946,"tests for reasoning surrounding emotional and social intelligence in everyday situations. CCG CCGbank (Hockenmaier and Steedman, 2007) is a task that is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL is specifically focused on verbs. SST-2 The Stanford sentiment treebank (Socher et al., 2013) is a sentiment classification task based on movie reviews. We use the binary sentenc"
2020.acl-main.467,P19-1472,0,0.0255593,"eading comprehension over an auxiliary passage, rather than simply answering a freestanding question. SocialIQA SocialIQA (Sap et al., 2019) is a task for multiple choice QA. It tests for reasoning surrounding emotional and social intelligence in everyday situations. CCG CCGbank (Hockenmaier and Steedman, 2007) is a task that is a translation of the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations. We use the CCG supertagging task, which is the task of assigning tags to individual word tokens that jointly determine the parse of the sentence. HellaSwag HellaSwag (Zellers et al., 2019) is a commonsense reasoning task that tests a model’s ability to choose the most plausible continuation of a story. It is built using adversarial filtering (Zellers et al., 2018) with BERT to create challenging negative examples. QA-SRL The question-answer driven semantic role labeling dataset (QA-SRL; He et al., 2015) for a QA task that is derived from a semantic role labeling task. Each example, which consists of a set of questions and answers, corresponds to a predicate-argument relationship in the sentence it is derived from. Unlike QAMR, which focuses on all words in the sentence, QA-SRL"
2020.acl-main.467,W18-5446,1,\N,Missing
2020.acl-main.467,E17-2026,0,\N,Missing
2020.emnlp-main.154,D17-1082,0,0.0407788,"Missing"
2020.emnlp-main.154,N19-1063,1,0.851036,"try to follow their recommendations in positioning and explaining our work. Related Work Measuring Bias Bias in natural language processing has gained visibility in recent years. Caliskan et al. (2017) introduce a dataset for evaluating gender bias in word embeddings. They find that GloVe embeddings (Pennington et al., 2014) reflect historical gender biases and they show that the geometric bias aligns well with crowd judgements. Rozado (2020) extend Caliskan et al.’s findings and show that popular pretrained word embeddings also display biases based on age, religion, and socioeconomic status. May et al. (2019) extend Caliskan et al.’s analysis to sentence-level evaluation with the SEAT test set. They evaluate popular sentence encoders like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018) for the angry black woman and double bind stereotypes. However they find no clear patterns in their results. One line of work explores evaluation grounded to specific downstream tasks, such as coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Dinan et al., 2020) and relation extraction (Gaut et al., 2019). Another line of work studies within the language modeling framewor, like 7 Ethical"
2020.emnlp-main.154,P10-1108,0,0.0200595,"Missing"
2020.emnlp-main.154,N16-1098,0,0.0753104,"Missing"
2020.emnlp-main.154,D14-1162,0,0.0836328,"g algorithm and they report lower bias scores on the SEAT while maintaining downstream task performance on the GLUE benchmark (Wang et al., 2018). Discussing Bias Upon surveying 146 NLP papers that analyze or mitigate bias, Blodgett et al. (2020) provide recommendations to guide such research. We try to follow their recommendations in positioning and explaining our work. Related Work Measuring Bias Bias in natural language processing has gained visibility in recent years. Caliskan et al. (2017) introduce a dataset for evaluating gender bias in word embeddings. They find that GloVe embeddings (Pennington et al., 2014) reflect historical gender biases and they show that the geometric bias aligns well with crowd judgements. Rozado (2020) extend Caliskan et al.’s findings and show that popular pretrained word embeddings also display biases based on age, religion, and socioeconomic status. May et al. (2019) extend Caliskan et al.’s analysis to sentence-level evaluation with the SEAT test set. They evaluate popular sentence encoders like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018) for the angry black woman and double bind stereotypes. However they find no clear patterns in their results. One line"
2020.emnlp-main.154,N18-1202,0,0.0270002,"et al. (2017) introduce a dataset for evaluating gender bias in word embeddings. They find that GloVe embeddings (Pennington et al., 2014) reflect historical gender biases and they show that the geometric bias aligns well with crowd judgements. Rozado (2020) extend Caliskan et al.’s findings and show that popular pretrained word embeddings also display biases based on age, religion, and socioeconomic status. May et al. (2019) extend Caliskan et al.’s analysis to sentence-level evaluation with the SEAT test set. They evaluate popular sentence encoders like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018) for the angry black woman and double bind stereotypes. However they find no clear patterns in their results. One line of work explores evaluation grounded to specific downstream tasks, such as coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Dinan et al., 2020) and relation extraction (Gaut et al., 2019). Another line of work studies within the language modeling framewor, like 7 Ethical Considerations The data presented in this paper is of a sensitive nature. We argue that this data should not be used to train a language model on a language modeling, or masked language mod"
2020.emnlp-main.154,P18-2124,0,0.0673665,"Missing"
2020.emnlp-main.154,W17-1609,0,0.116809,"Missing"
2020.emnlp-main.154,N18-2002,0,0.247929,"Missing"
2020.emnlp-main.154,W19-2304,0,0.0334982,"Missing"
2020.emnlp-main.154,W18-5446,1,0.898493,"Missing"
2020.emnlp-main.154,Q18-1042,0,0.0373139,"xtend Caliskan et al.’s findings and show that popular pretrained word embeddings also display biases based on age, religion, and socioeconomic status. May et al. (2019) extend Caliskan et al.’s analysis to sentence-level evaluation with the SEAT test set. They evaluate popular sentence encoders like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018) for the angry black woman and double bind stereotypes. However they find no clear patterns in their results. One line of work explores evaluation grounded to specific downstream tasks, such as coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Dinan et al., 2020) and relation extraction (Gaut et al., 2019). Another line of work studies within the language modeling framewor, like 7 Ethical Considerations The data presented in this paper is of a sensitive nature. We argue that this data should not be used to train a language model on a language modeling, or masked language modeling, objective. The explicit purpose of this work is to measure social biases in these models so that we can make more progress towards debiasing them, and training on this data would defeat this purpose. We recognize that there is a clear risk in publishing"
2020.emnlp-main.154,N18-1101,1,0.880768,"Missing"
2020.emnlp-main.154,N18-2003,0,0.0770766,"is dataset can be used as a benchmark to evaluate progress. 1 Introduction Progress in natural language processing research has recently been driven by the use of large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020). However, these models are trained on minimally-filtered real-world text, and contain ample evidence of their authors’ social biases. These language models, and embeddings extracted from them, have been shown to ∗ Equal contribution. learn and use these biases (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2017; May et al., 2010; Zhao et al., 2018; Rudinger et al., 2017). Models that have learnt representations that are biased against historically disadvantaged groups can cause a great deal of harm when those biases surface in downstream tasks or applications, such as automatic summarization or web search (Bender, 2019). Identifying and quantifying the learnt biases enables us to measure progress as we build less biased, or debias, models that propagate less harm in their myriad downstream applications. Quantifying bias in the language models directly allows us to identify and address the problem at the source, rather than attempting t"
2020.emnlp-main.16,N16-1024,0,0.0530388,"Missing"
2020.emnlp-main.16,D17-1215,0,0.284412,"t encode linguistic features (Gulordava et al., 2019; Tenney et al., 2019; Hewitt and Manning, 2019). However, feature learning is just the first step to acquiring helpful inductive biases. Models must also be able to learn which features matter. The NLU datasets these models are often fine-tuned on are ambiguous and contain artifacts, and often support multiple possible generalizations. Neural networks are not mind readers: Models that have been shown to represent linguistic features sometimes fail to use them during fine-tuning on NLU tasks, instead adopting shallow surface generalizations (Jia and Liang, 2017; McCoy et al., 2019). To this end, recent work in probing pretrained models advocates for shifting the focus of study away from whether they represent linguistic features and in favor of whether they learn useful representations of those features (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). We investigate how RoBERTa (Liu et al., 2019b) acquires language-specific inductive biases during self-supervised pretraining. We track separately 217 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 217–235, c November 16–20, 2020. 2020"
2020.emnlp-main.658,E17-2026,0,0.0247759,"ntermediate training—where a model is pretrained on unlabeled data, then on relatively abundant labeled data (MNLI), and finally scarce task-specific labeled data—by Phang et al. (2018), Clark et al. (2019), Liu et al. (2019a), Yang et al. (2019), and Liu et al. (2019b) across a range of large pretrained models models and target language-understanding tasks. Similar results have been observed with transfer from the other reasoning-oriented datasets (Sap et al., 2019; Bhagavatula et al., 2020), especially to target tasks centered on common sense. Another related body of work (Mou et al., 2016; Bingel and Søgaard, 2017; Wang et al., 2019a; Pruksachatkun et al., 2020) has explored the broader empirical landscape of which supervised NLP tasks can offer effective pretraining for other supervised NLP tasks. 3 Data Collection The annotation interface for our tasks is similar to that used for SNLI and MNLI: We provide a premise from a preexisting text source and ask human annotators to provide three hypothesis sentences: one that says something true about the fact or situation in the prompt (entailment), one that says something that may or may not be true about the fact or situation in the prompt (neutral)—with t"
2020.emnlp-main.658,D15-1075,1,0.795602,"ion Related Work PARAGRAPH Existing NLI datasets have been built using a wide range of strategies: FraCaS (Cooper et al., 1996) and several targeted evaluation sets were constructed manually by experts from scratch. The RTE challenge corpora (Dagan et al., 2006, et seq.) primarily used expert annotations on top of existing premise sentences. SICK (Marelli et al., 2014) was created using a structured pipeline centered on asking crowdworkers to edit sentences in prescribed ways. MPE (Lai et al., 2017) uses a similar strategy, but constructs unordered sets of sentences for use as premises. SNLI (Bowman et al., 2015) introduced the method, also used in MNLI, of asking crowdworkers to compose labeled hypotheses for a given premise. SciTail (Khot et al., 2018) and SWAG (Zellers et al., 2018) used domain-specific resources to pair up existing sentences as potential entailment pairs for annotation, with SWAG additionally using trained models to select the examples most worth annotating. There has been little work directly evaluating and comparing these many methods. In that absence, we focus on the SNLI/MNLI approach, because it has been shown to be effective for the collection of pretraining data and because"
2020.emnlp-main.658,D18-2029,0,0.0420599,"Missing"
2020.emnlp-main.658,N19-1300,0,0.191637,"ning as well, driving much of the recent use of the task: Several recent papers have shown that training large neural network models on natural language inference data, then fine-tuning them for other language understanding tasks often yields substantially better results on those target tasks (Conneau et al., 2017; Subramanian et al., 2018). This result holds even ∗ Work done while visiting Google. Figure 1: The annotation interfaces we evaluate. when starting from large models like BERT (Devlin et al., 2019) that have already been pretrained extensively on unlabeled data (Phang et al., 2018; Clark et al., 2019; Liu et al., 2019b; Wang et al., 2019b). The largest general-purpose corpus for NLI, and the one that has proven most successful in this setting, is the Multi-Genre NLI Corpus (MNLI Williams et al., 2018). MNLI was designed informally for use in a benchmark task (with no consideration of transfer learning), and in any case, 8203 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8203–8214, c November 16–20, 2020. 2020 Association for Computational Linguistics no explicit experimental research went into its design. Further, data collected under MNLI’s"
2020.emnlp-main.658,D17-1070,0,0.149224,"t: contradiction: Introduction The task of natural language inference (NLI; also known as textual entailment) has been widely used as an evaluation task when developing new methods for language understanding tasks, and it has recently become clear that high-quality NLI data can be useful in transfer learning as well, driving much of the recent use of the task: Several recent papers have shown that training large neural network models on natural language inference data, then fine-tuning them for other language understanding tasks often yields substantially better results on those target tasks (Conneau et al., 2017; Subramanian et al., 2018). This result holds even ∗ Work done while visiting Google. Figure 1: The annotation interfaces we evaluate. when starting from large models like BERT (Devlin et al., 2019) that have already been pretrained extensively on unlabeled data (Phang et al., 2018; Clark et al., 2019; Liu et al., 2019b; Wang et al., 2019b). The largest general-purpose corpus for NLI, and the one that has proven most successful in this setting, is the Multi-Genre NLI Corpus (MNLI Williams et al., 2018). MNLI was designed informally for use in a benchmark task (with no consideration of transfe"
2020.emnlp-main.658,N19-1423,0,0.0232252,"standing tasks, and it has recently become clear that high-quality NLI data can be useful in transfer learning as well, driving much of the recent use of the task: Several recent papers have shown that training large neural network models on natural language inference data, then fine-tuning them for other language understanding tasks often yields substantially better results on those target tasks (Conneau et al., 2017; Subramanian et al., 2018). This result holds even ∗ Work done while visiting Google. Figure 1: The annotation interfaces we evaluate. when starting from large models like BERT (Devlin et al., 2019) that have already been pretrained extensively on unlabeled data (Phang et al., 2018; Clark et al., 2019; Liu et al., 2019b; Wang et al., 2019b). The largest general-purpose corpus for NLI, and the one that has proven most successful in this setting, is the Multi-Genre NLI Corpus (MNLI Williams et al., 2018). MNLI was designed informally for use in a benchmark task (with no consideration of transfer learning), and in any case, 8203 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8203–8214, c November 16–20, 2020. 2020 Association for Computational"
2020.emnlp-main.658,W07-1401,0,0.104809,"sk datasets we use. We then measure the aggregate performance of the resulting models across those evaluation datasets. We evaluate on the target tasks in the SuperGLUE benchmark (Wang et al., 2019b): which consists of standardized splits and metrics for the question answering tasks BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018); the entailment and reasoning tasks CommitmentBank (CB; De Marneffe et al., 2019), Choice of Plausible Alternatives (COPA; Roemmele et al., 2011), Recognizing Textual Entailment (RTE; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and the Winograd Schema Challenge (WSC; Levesque et al., 2012); and the word sense disambiguation task WiC (Pilehvar and Camacho-Collados, 2019). These tasks were selected to be difficult for BERT but relatively easy for crowdworkers, and are meant to replace the largely-solved GLUE benchmark (Wang et al., 2019c). SuperGLUE does not include labeled test data, and does not allow for substantial ablation analyses on its test sets. Since we have no single final model whose performance we aim to show off, we do not use the test sets. We train our WSC model in the standa"
2020.emnlp-main.658,N18-2017,1,0.86655,"Missing"
2020.emnlp-main.658,ide-suderman-2006-integrating,0,0.0114927,"other sentence must be false only about the fact or situation in the first premise (and true or neutral with respect to the contrasting premise). This yields an entailment pair and a contradiction pair, both of which use only the first premises, with the contrasting premise serving only as a constraint on the annotation process. We could not find a sufficiently intuitive way to collect neutral sentence pairs under this protocol and opted to use only two classes rather than increase the difficulty of an already unintuitive task. 3.1 MNLI uses the small but stylistically diverse OpenANC corpus (Ide and Suderman, 2006) as its source for premise sentences, but uses nearly every available sentence from its non-technical sections, making it impractical for our use. To avoid re-using premise sentences, We instead draw on English Wikipedia.1 Similarity Search The E DIT OTHER and C ON TRAST protocols require pairs of similar sentences as their inputs. To construct these, we assemble a heuristic sentence-matching system intended to generate pairs of highly similar sentences that can be minimally edited to construct entailments or contradictions: Given a premise, we retrieve its closest 10k nearest neighbors accord"
2020.emnlp-main.658,N18-1023,0,0.0284253,", we use the training sets from our datasets in STILTs-style intermediate training (Phang et al., 2018): We fine-tune a large pretrained model on our collected data using standard fine-tuning procedures, then fine-tune copies of the resulting model again on each of the target task datasets we use. We then measure the aggregate performance of the resulting models across those evaluation datasets. We evaluate on the target tasks in the SuperGLUE benchmark (Wang et al., 2019b): which consists of standardized splits and metrics for the question answering tasks BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018); the entailment and reasoning tasks CommitmentBank (CB; De Marneffe et al., 2019), Choice of Plausible Alternatives (COPA; Roemmele et al., 2011), Recognizing Textual Entailment (RTE; Dagan et al., 2006; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and the Winograd Schema Challenge (WSC; Levesque et al., 2012); and the word sense disambiguation task WiC (Pilehvar and Camacho-Collados, 2019). These tasks were selected to be difficult for BERT but relatively easy for crowdworkers, and are meant to replace the largely-solved GLUE benchma"
2020.emnlp-main.658,P19-1478,0,0.0202731,"nge (WSC; Levesque et al., 2012); and the word sense disambiguation task WiC (Pilehvar and Camacho-Collados, 2019). These tasks were selected to be difficult for BERT but relatively easy for crowdworkers, and are meant to replace the largely-solved GLUE benchmark (Wang et al., 2019c). SuperGLUE does not include labeled test data, and does not allow for substantial ablation analyses on its test sets. Since we have no single final model whose performance we aim to show off, we do not use the test sets. We train our WSC model in the standard way without adding data or modifying the format (as in Kocijan et al., 2019; Liu et al., 2019b). Without these modifications, few of our models exceed chance accuracy. Results are shown in Table 6. Our first observation is that our overall data collection pipeline worked well for our purposes: Our BASE data yields models that transfer substantially better than the plain RoBERTa or XLNet baseline, and at least slightly better than 8.5k-example samples of MNLI, MNLI Government or ANLI. However, all four of our interventions yield worse transfer performance than BASE. The variances across runs are small, and this pattern is consistent across both RoBERTa and XLNet, and"
2020.emnlp-main.658,I17-1011,0,0.0217465,"f financial officer. H: According to the board, the Libyan office should be holding more cash on hand. contradiction Related Work PARAGRAPH Existing NLI datasets have been built using a wide range of strategies: FraCaS (Cooper et al., 1996) and several targeted evaluation sets were constructed manually by experts from scratch. The RTE challenge corpora (Dagan et al., 2006, et seq.) primarily used expert annotations on top of existing premise sentences. SICK (Marelli et al., 2014) was created using a structured pipeline centered on asking crowdworkers to edit sentences in prescribed ways. MPE (Lai et al., 2017) uses a similar strategy, but constructs unordered sets of sentences for use as premises. SNLI (Bowman et al., 2015) introduced the method, also used in MNLI, of asking crowdworkers to compose labeled hypotheses for a given premise. SciTail (Khot et al., 2018) and SWAG (Zellers et al., 2018) used domain-specific resources to pair up existing sentences as potential entailment pairs for annotation, with SWAG additionally using trained models to select the examples most worth annotating. There has been little work directly evaluating and comparing these many methods. In that absence, we focus on"
2020.emnlp-main.658,P19-1441,0,0.226983,"g much of the recent use of the task: Several recent papers have shown that training large neural network models on natural language inference data, then fine-tuning them for other language understanding tasks often yields substantially better results on those target tasks (Conneau et al., 2017; Subramanian et al., 2018). This result holds even ∗ Work done while visiting Google. Figure 1: The annotation interfaces we evaluate. when starting from large models like BERT (Devlin et al., 2019) that have already been pretrained extensively on unlabeled data (Phang et al., 2018; Clark et al., 2019; Liu et al., 2019b; Wang et al., 2019b). The largest general-purpose corpus for NLI, and the one that has proven most successful in this setting, is the Multi-Genre NLI Corpus (MNLI Williams et al., 2018). MNLI was designed informally for use in a benchmark task (with no consideration of transfer learning), and in any case, 8203 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8203–8214, c November 16–20, 2020. 2020 Association for Computational Linguistics no explicit experimental research went into its design. Further, data collected under MNLI’s data collection p"
2020.emnlp-main.658,2021.ccl-1.108,0,0.0598458,"Missing"
2020.emnlp-main.658,marelli-etal-2014-sick,0,0.0370264,"d concerns about the amounts of cash kept by SNC’s Libyan office, at that time approximately $10 million, according to the company’s chief financial officer. H: According to the board, the Libyan office should be holding more cash on hand. contradiction Related Work PARAGRAPH Existing NLI datasets have been built using a wide range of strategies: FraCaS (Cooper et al., 1996) and several targeted evaluation sets were constructed manually by experts from scratch. The RTE challenge corpora (Dagan et al., 2006, et seq.) primarily used expert annotations on top of existing premise sentences. SICK (Marelli et al., 2014) was created using a structured pipeline centered on asking crowdworkers to edit sentences in prescribed ways. MPE (Lai et al., 2017) uses a similar strategy, but constructs unordered sets of sentences for use as premises. SNLI (Bowman et al., 2015) introduced the method, also used in MNLI, of asking crowdworkers to compose labeled hypotheses for a given premise. SciTail (Khot et al., 2018) and SWAG (Zellers et al., 2018) used domain-specific resources to pair up existing sentences as potential entailment pairs for annotation, with SWAG additionally using trained models to select the examples"
2020.emnlp-main.658,D16-1046,0,0.0141307,"o the context of intermediate training—where a model is pretrained on unlabeled data, then on relatively abundant labeled data (MNLI), and finally scarce task-specific labeled data—by Phang et al. (2018), Clark et al. (2019), Liu et al. (2019a), Yang et al. (2019), and Liu et al. (2019b) across a range of large pretrained models models and target language-understanding tasks. Similar results have been observed with transfer from the other reasoning-oriented datasets (Sap et al., 2019; Bhagavatula et al., 2020), especially to target tasks centered on common sense. Another related body of work (Mou et al., 2016; Bingel and Søgaard, 2017; Wang et al., 2019a; Pruksachatkun et al., 2020) has explored the broader empirical landscape of which supervised NLP tasks can offer effective pretraining for other supervised NLP tasks. 3 Data Collection The annotation interface for our tasks is similar to that used for SNLI and MNLI: We provide a premise from a preexisting text source and ask human annotators to provide three hypothesis sentences: one that says something true about the fact or situation in the prompt (entailment), one that says something that may or may not be true about the fact or situation in t"
2020.emnlp-main.658,2020.acl-main.441,0,0.0361207,"resources to pair up existing sentences as potential entailment pairs for annotation, with SWAG additionally using trained models to select the examples most worth annotating. There has been little work directly evaluating and comparing these many methods. In that absence, we focus on the SNLI/MNLI approach, because it has been shown to be effective for the collection of pretraining data and because its reliance on only crowdworkers and unstructured source text makes it simple to scale. Two recent papers have investigated methods that could augment the base MNLI protocol we study here. ANLI (Nie et al., 2020) collects new examples following this protocol, but adds an incentive for crowdworkers to produce sentence pairs on which a baseline system will perform poorly. Kaushik et al. (2020) introduce a method for expanding an already-collected dataset by making minimal edits to existing examples that change their labels, with the intent to better teach models to isolate the factors that are causally responsible for the label assignments. Both of these papers offer P: The paper, along with the ”Washington Blade”, was acquired by Window Media, LLC in 2001, and both were then sold to HX Media in 2007. K"
2020.emnlp-main.658,N19-1128,0,0.0291112,"Missing"
2020.emnlp-main.658,W18-5441,0,0.046137,"Missing"
2020.emnlp-main.658,2020.acl-main.467,1,0.855731,"Missing"
2020.emnlp-main.658,W17-1609,0,0.0611337,"Missing"
2020.emnlp-main.658,D19-1454,0,0.0448112,"Missing"
2020.emnlp-main.658,P19-1644,0,0.024079,"is similar to the premise sentence, and uses that. Our C ONTRAST protocol tests the effect of adding artificial constraints on the kinds of hypothesis sentences annotators can write. Giving annotators difficult and varying constraints could encourage creativity and prevent annotators from falling into patterns in their writing that lead to easier or more repetitive data. However, as with the use of longer contexts in PARAGRAPH, this protocol risks substantially slowing the annotation process. We experiment with a procedure inspired by that used to create the language-and-vision dataset NLVR2 (Suhr et al., 2019), in which annotators must write sentences that show some specified relationship (entailment or contradiction) to a given premise, but do not show that relationship to a second similar distractor premise. Because we see transfer learning as the primary application area for which it would be valuable to collect additional large-scale NLI datasets, we focus our evaluation on this setting, and do not collect or designate test sets for the experimental datasets we collect. In transfer evaluations on the SuperGLUE benchmark (Wang et al., 2019b), our BASE dataset and the datasets collected under our"
2020.emnlp-main.658,L18-1239,0,0.0500552,"Missing"
2020.emnlp-main.658,P19-1439,1,0.895723,"Missing"
2020.emnlp-main.658,N18-1101,1,0.819549,"language understanding tasks often yields substantially better results on those target tasks (Conneau et al., 2017; Subramanian et al., 2018). This result holds even ∗ Work done while visiting Google. Figure 1: The annotation interfaces we evaluate. when starting from large models like BERT (Devlin et al., 2019) that have already been pretrained extensively on unlabeled data (Phang et al., 2018; Clark et al., 2019; Liu et al., 2019b; Wang et al., 2019b). The largest general-purpose corpus for NLI, and the one that has proven most successful in this setting, is the Multi-Genre NLI Corpus (MNLI Williams et al., 2018). MNLI was designed informally for use in a benchmark task (with no consideration of transfer learning), and in any case, 8203 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8203–8214, c November 16–20, 2020. 2020 Association for Computational Linguistics no explicit experimental research went into its design. Further, data collected under MNLI’s data collection protocol has known issues with annotation artifacts which make it possible to perform much better than chance using only one of the two sentences that make up each example (Tsuchiya, 2018;"
2020.emnlp-main.658,D18-1009,0,0.0166919,"ructed manually by experts from scratch. The RTE challenge corpora (Dagan et al., 2006, et seq.) primarily used expert annotations on top of existing premise sentences. SICK (Marelli et al., 2014) was created using a structured pipeline centered on asking crowdworkers to edit sentences in prescribed ways. MPE (Lai et al., 2017) uses a similar strategy, but constructs unordered sets of sentences for use as premises. SNLI (Bowman et al., 2015) introduced the method, also used in MNLI, of asking crowdworkers to compose labeled hypotheses for a given premise. SciTail (Khot et al., 2018) and SWAG (Zellers et al., 2018) used domain-specific resources to pair up existing sentences as potential entailment pairs for annotation, with SWAG additionally using trained models to select the examples most worth annotating. There has been little work directly evaluating and comparing these many methods. In that absence, we focus on the SNLI/MNLI approach, because it has been shown to be effective for the collection of pretraining data and because its reliance on only crowdworkers and unstructured source text makes it simple to scale. Two recent papers have investigated methods that could augment the base MNLI protocol"
2020.emnlp-main.658,D18-1007,0,\N,Missing
2020.emnlp-main.664,N19-1423,0,0.0822362,"Missing"
2020.emnlp-main.664,P19-1477,0,0.112847,"Missing"
2020.emnlp-main.664,P19-1478,0,0.197908,"Missing"
2020.emnlp-main.664,2021.ccl-1.108,0,0.0560335,"Missing"
2020.emnlp-main.664,N19-4009,0,0.0621274,"Missing"
2020.insights-1.13,N18-2017,1,0.933091,"lding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable. 1 Samuel R. Bowman New York University bowman@nyu.edu Introduction While standard crowdsourced benchmarks have helped create significant progress within natural language processing (NLP), a growing body of evidence shows the existence of exploitable annotation artifacts in these datasets (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) and that models can use artifacts to achieve state-of-the-art performance on these benchmarks (McCoy et al., 2019; Naik et al., 2018). The existence of these 82 Proceedings of the First Workshop on Insights from Negative Results in NLP, pages 82–87 c Online, November 19, 2020. 2020 Association for Computational Linguistics ISBN 978-1-952148-66-8 et al., 2018)—a dataset for the same task with examples out-of-domain to SNLI—and two diagnostic sets (Naik et al., 2018; Wang et al., 2019a). We find that RoBERTa trained on CNLI yields similar performance on out"
2020.insights-1.13,2020.emnlp-main.12,0,0.492003,"o test. Recent work has explored using counterfactually-augmented datasets to address annotation artifacts with the intent to build more robust classifiers (Kaushik et al., 2020; Khashabi et al., 2020). These datasets are collected by first sampling a set of seed examples and then creating new examples by minimally editing the seed examples to yield counterfactual labels. This type of data collection has been found to mitigate the presence of artifacts in SNLI (Bowman et al., 2015) and is presented as a way to “elucidate the difference that makes a difference” (Kaushik et al., 2020). Further, Khashabi et al. (2020) present this as an efficient method to collect training data yielding models that are “more robust to minor variations and generalize better” (Khashabi et al., 2020). However, they also find that unaugmented datasets yield better performance than datasets with 50-50 original-to-augmented data when controlling for training set size and annotation cost. In our work, we further study whether training with counterfactually-augmented data collected through standard crowdsourcing methods yields models with better generalization and robustness by focusing on the domain of natural language inference"
2020.insights-1.13,2021.ccl-1.108,0,0.16271,"Missing"
2020.insights-1.13,P19-1334,0,0.0589355,"Missing"
2020.insights-1.13,D15-1075,1,0.909568,"cts makes it difficult to predict out-of-domain generalization and creates uncertainty around the abilities these tasks are designed to test. Recent work has explored using counterfactually-augmented datasets to address annotation artifacts with the intent to build more robust classifiers (Kaushik et al., 2020; Khashabi et al., 2020). These datasets are collected by first sampling a set of seed examples and then creating new examples by minimally editing the seed examples to yield counterfactual labels. This type of data collection has been found to mitigate the presence of artifacts in SNLI (Bowman et al., 2015) and is presented as a way to “elucidate the difference that makes a difference” (Kaushik et al., 2020). Further, Khashabi et al. (2020) present this as an efficient method to collect training data yielding models that are “more robust to minor variations and generalize better” (Khashabi et al., 2020). However, they also find that unaugmented datasets yield better performance than datasets with 50-50 original-to-augmented data when controlling for training set size and annotation cost. In our work, we further study whether training with counterfactually-augmented data collected through standar"
2020.insights-1.13,C18-1198,0,0.165201,"Missing"
2020.insights-1.13,2020.acl-main.441,0,0.0607133,"ifacts. Several studies (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018) show that models trained on hypothesis-only examples manage to perform as much as 35 points higher than chance. Gururangan et al. (2018) also find negation words such as no or never are strongly associated with contradiction predictions. Other works (Naik et al., 2018; McCoy et al., 2019) find that models can exploit premise-hypothesis word overlap to achieve state-of-the-art performance on benchmarks by using associations of high overlap with entailment predictions and low overlap with neutral predictions. Nie et al. (2020) use an adversarial human-andmodel-in-the-loop procedure to address these concerns in Adversarial NLI (ANLI). Using a model in the loop makes ANLI inherently adversarial towards the model used, and we instead focus on naturally collected human-in-the-loop augmented data. Experimental Setup We perform two experiments to study the effects of counterfactually-augmented NLI training data. All experiments use RoBERTa trained on SNLI, CNLI, or CNLI seed examples originally sampled from SNLI and compare performances on various tasks. We first compare MNLI performances to evaluate the impact on model"
2020.insights-1.13,S18-2023,0,0.149458,"te models on various annotated tasks. They show that most datasets require 1-3 minutes per augmented example, taking 17-50 hours to create 1,000 examples. We differ by using crowdsourced counterfactuallyaugmented data and focusing on their use for training instead of evaluation. Related Work 3 Recent works show that several NLI benchmark datasets contain exploitable annotation artifacts. Several studies (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018) show that models trained on hypothesis-only examples manage to perform as much as 35 points higher than chance. Gururangan et al. (2018) also find negation words such as no or never are strongly associated with contradiction predictions. Other works (Naik et al., 2018; McCoy et al., 2019) find that models can exploit premise-hypothesis word overlap to achieve state-of-the-art performance on benchmarks by using associations of high overlap with entailment predictions and low overlap with neutral predictions. Nie et al. (2020) use an adversarial human-andmodel-in-the-loop procedure to address these concerns in Adversarial NLI (ANLI). Using a model in the loop makes ANLI inherently adversarial towards the model used, and we inste"
2020.insights-1.13,L18-1239,0,0.222755,"te models on various annotated tasks. They show that most datasets require 1-3 minutes per augmented example, taking 17-50 hours to create 1,000 examples. We differ by using crowdsourced counterfactuallyaugmented data and focusing on their use for training instead of evaluation. Related Work 3 Recent works show that several NLI benchmark datasets contain exploitable annotation artifacts. Several studies (Poliak et al., 2018; Gururangan et al., 2018; Tsuchiya, 2018) show that models trained on hypothesis-only examples manage to perform as much as 35 points higher than chance. Gururangan et al. (2018) also find negation words such as no or never are strongly associated with contradiction predictions. Other works (Naik et al., 2018; McCoy et al., 2019) find that models can exploit premise-hypothesis word overlap to achieve state-of-the-art performance on benchmarks by using associations of high overlap with entailment predictions and low overlap with neutral predictions. Nie et al. (2020) use an adversarial human-andmodel-in-the-loop procedure to address these concerns in Adversarial NLI (ANLI). Using a model in the loop makes ANLI inherently adversarial towards the model used, and we inste"
2020.insights-1.13,N18-1101,1,0.875869,"Missing"
2020.iwpt-1.11,P06-1109,0,0.0774761,"hitecture is to enable self-training, we further hypothesize that, since language modeling and parsing annotations seem to provide complementary information, UP should aid low-resource supervised parsing. As a proof of concept, we employ SS-PRPN for semisupervised training. In extremely-low-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019)"
2020.iwpt-1.11,W01-0713,0,0.171502,"ics velopment of a semi-supervised architecture is to enable self-training, we further hypothesize that, since language modeling and parsing annotations seem to provide complementary information, UP should aid low-resource supervised parsing. As a proof of concept, we employ SS-PRPN for semisupervised training. In extremely-low-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been"
2020.iwpt-1.11,N19-1116,0,0.0159894,"e concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019) propose an architecture consisting of an LSTM (Hochreiter and Schmidhuber, 1997) with a modified update function for the LSTM cell state, Kim et al. (2019a)—the current state-of-the-art—introduce a model based on a mixture of probabilistic context-free grammars, Kim et al. (2019b) present unsupervised learning of recurrent neural networks grammars, Li et al. (2019) combine PRPN with imitation learning, and Drozdov et al. (2019) employ a recursive autoencoder. Kim et al. (2020) examine tree induction from pretrained models. 2 Model Syntactic Distances In order to parse a sentence, a computational model needs to output some kind of variables representing a unique tree structure. The variables we use are syntactic distances as introduced by Shen et al. (2018a). They represent the syntactic relationships between all successive pairs of words in a sentence. If the distance between two neighboring words is large, they belong to different subtrees, and, thus, their traversal distance in the tree is large. A parse tree can"
2020.iwpt-1.11,P19-1228,0,0.611094,"ituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing. This enables our model to leverage silver-standard annotations obtained via selftraining for supervision. Our approach"
2020.iwpt-1.11,N19-1114,0,0.219698,"ituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing. This enables our model to leverage silver-standard annotations obtained via selftraining for supervision. Our approach"
2020.iwpt-1.11,P02-1017,0,0.23577,"of a semi-supervised architecture is to enable self-training, we further hypothesize that, since language modeling and parsing annotations seem to provide complementary information, UP should aid low-resource supervised parsing. As a proof of concept, we employ SS-PRPN for semisupervised training. In extremely-low-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen e"
2020.iwpt-1.11,N15-1067,0,0.271032,"d Dl can be supervised, but Dl can also be learned in a latent manner. Introduction Unsupervised parsing (UP) models learn to parse sentences into unlabeled constituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly"
2020.iwpt-1.11,P19-1338,0,0.0174027,"ng methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019) propose an architecture consisting of an LSTM (Hochreiter and Schmidhuber, 1997) with a modified update function for the LSTM cell state, Kim et al. (2019a)—the current state-of-the-art—introduce a model based on a mixture of probabilistic context-free grammars, Kim et al. (2019b) present unsupervised learning of recurrent neural networks grammars, Li et al. (2019) combine PRPN with imitation learning, and Drozdov et al. (2019) employ a recursive autoencoder. Kim et al. (2020) examine tree induction from pretrained models. 2 Model Syntactic Distances In order to parse a sentence, a computational model needs to output some kind of variables representing a unique tree structure. The variables we use are syntactic distances as introduced by Shen et al. (2018a). They represent the syntactic relationships between all successive pairs of words in a sentence. If the distance between two neighboring words is large, they belong to different subtrees, and, thus,"
2020.iwpt-1.11,N06-1020,0,0.25874,"Missing"
2020.iwpt-1.11,W03-0404,0,0.131515,"Missing"
2020.iwpt-1.11,N16-1024,0,0.0239796,"PRPN 35 50 100 150 200 250 Figure 3: Low-resource parsing on the PTB. The first and second plots show unlabeled and labeled F1 respectively, plotted against the training data size. Low-Resource Parsing Performance We further investigate how SS-PRPN performs when limited gold parses are available in addition to unlabeled data. To predict constituency labels, we add and train an additional linear output layer after the first convolutional layer. We find that, on the development set, converting Dg into parse trees works better for low-resource parsing than Dl . As supervised baselines, we employ Dyer et al. (2016)’s recurrent neural network grammar (RNNG) and a supervised parser (SP) based on syntactic distances (Shen et al., 2018b). Figure 3 shows results for 50 to 250 annotated examples. The upper part shows the unlabeled parsing performance in comparison to the UP baselines. We outperform all baselines for 50 to 150 examples, while SP performs slightly better with more annotations. When looking at labeled F1 in the lower part of Figure 3, SS-PRPN clearly outperforms SP, which indicates that unlabeled data can be leveraged in the low-resource setting. 5 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Number of"
2020.iwpt-1.11,P18-1108,0,0.164263,"ervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension of the UP architecture PRPN (Shen et al., 2018a), which can be trained jointly on language modeling and supervised parsing. This enables our model to leverage silver-standard annotations obtained via selftraining for supervision. Our approach draws on the idea of syntactic distances, which can be learned both as latent variables (Shen et al., 2018a) and as explicit supervision targets (Shen et al., 2018b). We use both of these, leveraging annotations obtained via UP to supervise the two different outputs of the parser, in addition to standard UP training. SS-PRPN, in combination with self-training, improves over its original version by 8."
2020.iwpt-1.11,W18-5452,1,0.906162,"-data regimes with no more than 250 labeled parses, SS-PRPN outperforms supervised and unsupervised baselines in most settings on unlabeled parsing, and in all settings on labeled constituency parsing. Related Work Following the line of research on non-neural UP models (Clark, 2001; Klein and Manning, 2002; Bod, 2006), early approaches to neural UP (Yogatama et al., 2017; Choi et al., 2018) obtain improved performance on downstream tasks, yet show highly inconsistent behavior in parsing (Williams et al., 2018). Recently, Shen et al. (2018a) introduce the first high performing neural UP model (Htut et al., 2018). Dyer et al. (2019) raise concerns that PRPN’s parsing methodology is biased towards English trees. Though these concerns are serious, they are largely orthogonal to our research question regarding the helpfulness of self-training for UP. Several models have been introduced since: Shen et al. (2019) propose an architecture consisting of an LSTM (Hochreiter and Schmidhuber, 1997) with a modified update function for the LSTM cell state, Kim et al. (2019a)—the current state-of-the-art—introduce a model based on a mixture of probabilistic context-free grammars, Kim et al. (2019b) present unsuperv"
2020.iwpt-1.11,E03-1008,0,0.171205,"ser, represented by the dotted box, outputs syntactic distances Dg and Dl . Both Dg and Dl can be supervised, but Dl can also be learned in a latent manner. Introduction Unsupervised parsing (UP) models learn to parse sentences into unlabeled constituency trees without the need for annotated treebanks. Self-training (Yarowsky, 1995; Riloff et al., 2003) consists of training a model, using it to label new examples and, based on a confidence metric, adding a subset to the training set, before repeating training. For supervised parsing, results with self-training have been mixed (Charniak, 1997; Steedman et al., 2003; McClosky D, 2006). For unsupervised dependency parsing, Le and Zuidema (2015) obtain strong results by training a supervised parser on outputs of unsupervised parsing. UP models show low self-agreement between training runs (Kim et al., 2019a), while obtaining parsing performances far above chance. Supervising one run with confident ∗ † Equal contribution. Now at Electronic Arts. FF LAYER parses from the last could combine their individual strengths. Thus, we ask the question: Can UP benefit from self-training? In order to answer this question, we propose SS-PRPN, a semi-supervised extension"
2020.iwpt-1.11,P95-1026,0,0.883065,"Missing"
2020.scil-1.47,N18-1108,0,0.0293809,"Missing"
2020.scil-1.47,Q16-1037,0,0.0507847,"Missing"
2020.scil-1.47,D18-1151,0,0.0308297,"Missing"
2020.tacl-1.25,D19-1287,0,0.021672,"linguistics such as control and raising, ellipsis, quantification, and countless others. This is likely due to the labor-intensive nature of collecting such targeted minimal pairs. A related line of work evaluates neural networks on acceptability judgments in a more domaingeneral way. Corpora of sentences and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies"
2020.tacl-1.25,2020.scil-1.1,0,0.0883905,"pora of sentences and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most recent and comprehensive corpus is CoLA (Warstadt et al., 2019b), containing 10k sentences covering a wide variety of linguistic phenomena provided as examples in lingui"
2020.tacl-1.25,C18-1152,0,0.0766798,"the LMs. 2.2 Linguistic Knowledge of NNs Many recent studies have searched for evidence that neural networks (NNs) learn representations that implicitly encode grammatical concepts. We refer to the ability to encode these concepts as linguistic knowledge. Some studies evaluate NNs’ linguistic knowledge using probing tasks in which a classifier is trained to directly predict grammatical properties of a sentence (e.g., syntactic tree depth) or part of a sentence (e.g., part-of-speech) using only the NNs’ learned representation as input (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Tenney et al., 2019). We follow a complementary approach that uses acceptability judgments to address the same question without the need for training data labeled with grammatical concepts. Acceptability judgments are the main form of behavioral data used in generative linguistics to measure human linguistic competence (Chomsky, 1965; Sch¨utze, 1996). One branch of this literature uses minimal pairs to infer whether LMs detect specific grammatical contrasts. Table 1 summarizes linguistic phenomena studied in this work. For instance, Linzen et al. (2016) look closely at minimal pairs contrast"
2020.tacl-1.25,C18-1012,0,0.0686793,"gments in a more domaingeneral way. Corpora of sentences and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most recent and comprehensive corpus is CoLA (Warstadt et al., 2019b), containing 10k sentences covering a wide variety of linguistic phenomena provided"
2020.tacl-1.25,P18-1198,0,0.0334476,"is already present in the LMs. 2.2 Linguistic Knowledge of NNs Many recent studies have searched for evidence that neural networks (NNs) learn representations that implicitly encode grammatical concepts. We refer to the ability to encode these concepts as linguistic knowledge. Some studies evaluate NNs’ linguistic knowledge using probing tasks in which a classifier is trained to directly predict grammatical properties of a sentence (e.g., syntactic tree depth) or part of a sentence (e.g., part-of-speech) using only the NNs’ learned representation as input (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Tenney et al., 2019). We follow a complementary approach that uses acceptability judgments to address the same question without the need for training data labeled with grammatical concepts. Acceptability judgments are the main form of behavioral data used in generative linguistics to measure human linguistic competence (Chomsky, 1965; Sch¨utze, 1996). One branch of this literature uses minimal pairs to infer whether LMs detect specific grammatical contrasts. Table 1 summarizes linguistic phenomena studied in this work. For instance, Linzen et al. (2016) look closely at"
2020.tacl-1.25,2020.scil-1.2,0,0.0721817,"and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most recent and comprehensive corpus is CoLA (Warstadt et al., 2019b), containing 10k sentences covering a wide variety of linguistic phenomena provided as examples in linguistics papers and books. CoLA"
2020.tacl-1.25,P19-1285,0,0.0680401,"hortened to BLiMP), a linguistically motivated benchmark for assessing the sensitivity of LMs to acceptability contrasts across a wide range of English phenomena, covering both previously studied and novel contrasts. BLiMP consists of 67 datasets automatically generated from linguist-crafted grammar templates, each containing 1,000 minimal pairs and organized by phenomenon into 12 categories. Validation with crowdworkers shows that BLiMP faithfully represents human preferences. We use BLiMP to study several pretrained LMs: Transformer-based LMs GPT-2 (Radford et al., 2019) and Transformer-XL (Dai et al., 2019), an LSTM LM trained by Gulordava et al. (2019), and an n-gram LM. We evaluate whether the LM assigns a higher probability to the acceptable sentence in each minimal pair to determine which grammatical distinctions LMs are sensitive to. This gives us indirect evidence about each model’s linguistic knowledge and allows us to compare models in a fine-grained way. We conclude that current neural LMs appear to acquire robust knowledge of morphological agreement and some syntactic phenomena such as ellipsis and control/ raising. They show weaker evidence of knowledge about argument structure, negat"
2020.tacl-1.25,P13-2121,0,0.0405725,"Missing"
2020.tacl-1.25,P14-2029,0,0.0622012,"ms Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most recent and comprehensive corpus is CoLA (Warstadt et al., 2019b), containing 10k sentences covering a wide variety of linguistic phenomena provided as examples in linguistics papers and books. CoLA, which is included in the GLUE benchmark (Wang et al., 2018), has been used to track advances in the sensitivity of reusable sentence encoding models to acceptability. Current models like BERT (Devlin et al., 2019) and T5 (Raffel et al., 2019) now learn to give acceptability judgments that approach or even exceed individual human agreement with C"
2020.tacl-1.25,P82-1020,0,0.706245,"Missing"
2020.tacl-1.25,P18-1031,0,0.0350078,"like knowledge, and to bring into focus those areas of grammar that future studies evaluating LMs should investigate in greater depth. 2 Background and Related Work 2.1 Language Models The objective of a language model is to give a probability distribution over the strings of a language. Both neural network and non-neural network architectures are used to build LMs, and neural models can be trained in a self-supervised setting without the need for labeled data. Recently, variants of neural language modeling have been shown to be a strong pretraining task for natural language processing tasks (Howard and Ruder, 2018; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The last decade has seen two major paradigm shifts in the state of the art for language modeling. First, there was a movement from models based on local n-gram statistics (see Chen and Goodman, 1999) to neural sequence models such as LSTMs (Mikolov et al., 2010), which optimize on the task of predicting the next token. Subsequently, Transformer-based architectures employing selfattention (Vaswani et al., 2017) have outperformed LSTMs (e.g., Dai et al., 2019). Although these shifts have resulted in stronger LMs, perplexity 378 o"
2020.tacl-1.25,N19-1423,0,0.0925728,"future studies evaluating LMs should investigate in greater depth. 2 Background and Related Work 2.1 Language Models The objective of a language model is to give a probability distribution over the strings of a language. Both neural network and non-neural network architectures are used to build LMs, and neural models can be trained in a self-supervised setting without the need for labeled data. Recently, variants of neural language modeling have been shown to be a strong pretraining task for natural language processing tasks (Howard and Ruder, 2018; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The last decade has seen two major paradigm shifts in the state of the art for language modeling. First, there was a movement from models based on local n-gram statistics (see Chen and Goodman, 1999) to neural sequence models such as LSTMs (Mikolov et al., 2010), which optimize on the task of predicting the next token. Subsequently, Transformer-based architectures employing selfattention (Vaswani et al., 2017) have outperformed LSTMs (e.g., Dai et al., 2019). Although these shifts have resulted in stronger LMs, perplexity 378 on large benchmark datasets like WikiText-103 (Merity et al., 2016"
2020.tacl-1.25,W18-5424,0,0.0807826,"the labor-intensive nature of collecting such targeted minimal pairs. A related line of work evaluates neural networks on acceptability judgments in a more domaingeneral way. Corpora of sentences and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most rec"
2020.tacl-1.25,W19-0129,1,0.770406,"this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most recent and comprehensive corpus is CoLA (Warstadt et al., 2019b), containing 10k sentences covering a wide variety of linguistic phenomena provided as examples in linguistics papers and books. CoLA, which is included in the GLUE benchm"
2020.tacl-1.25,Q16-1037,0,0.188507,"ment and some syntactic phenomena such as ellipsis and control/ raising. They show weaker evidence of knowledge about argument structure, negative polarity item licensing, and the semantic properties of quantifiers. All models perform at or near chance on extraction islands. Overall, every model we 1 Introduction Current neural networks for sentence processing rely on unsupervised pretraining tasks like language modeling. Still, it is an open question how the linguistic knowledge of state-of-the-art language models (LMs) varies across the linguistic phenomena of English. Recent studies (e.g., Linzen et al., 2016; Marvin and Linzen, 2018; Wilcox et al., 2018) have explored this question by evaluating LMs’ preferences between minimal pairs of sentences differing in grammatical acceptability, as in Example 1. However, each 1 a. The cats annoy Tim. (grammatical) b. *The cats annoys Tim. (ungrammatical) https://github.com/alexwarstadt/blimp. 377 Transactions of the Association for Computational Linguistics, vol. 8, pp. 377–392, 2020. https://doi.org/10.1162/tacl a 00321 Action Editor: Mark Steedman. Submission batch: 1/2020; Revision batch: 3/2020; Published 7/2020. c 2020 Association for Computational Li"
2020.tacl-1.25,D18-1151,0,0.114649,"ic phenomena such as ellipsis and control/ raising. They show weaker evidence of knowledge about argument structure, negative polarity item licensing, and the semantic properties of quantifiers. All models perform at or near chance on extraction islands. Overall, every model we 1 Introduction Current neural networks for sentence processing rely on unsupervised pretraining tasks like language modeling. Still, it is an open question how the linguistic knowledge of state-of-the-art language models (LMs) varies across the linguistic phenomena of English. Recent studies (e.g., Linzen et al., 2016; Marvin and Linzen, 2018; Wilcox et al., 2018) have explored this question by evaluating LMs’ preferences between minimal pairs of sentences differing in grammatical acceptability, as in Example 1. However, each 1 a. The cats annoy Tim. (grammatical) b. *The cats annoys Tim. (ungrammatical) https://github.com/alexwarstadt/blimp. 377 Transactions of the Association for Computational Linguistics, vol. 8, pp. 377–392, 2020. https://doi.org/10.1162/tacl a 00321 Action Editor: Mark Steedman. Submission batch: 1/2020; Revision batch: 3/2020; Published 7/2020. c 2020 Association for Computational Linguistics. Distributed un"
2020.tacl-1.25,D16-1159,0,0.0255823,"Missing"
2020.tacl-1.25,N18-1202,0,0.121126,"ring into focus those areas of grammar that future studies evaluating LMs should investigate in greater depth. 2 Background and Related Work 2.1 Language Models The objective of a language model is to give a probability distribution over the strings of a language. Both neural network and non-neural network architectures are used to build LMs, and neural models can be trained in a self-supervised setting without the need for labeled data. Recently, variants of neural language modeling have been shown to be a strong pretraining task for natural language processing tasks (Howard and Ruder, 2018; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). The last decade has seen two major paradigm shifts in the state of the art for language modeling. First, there was a movement from models based on local n-gram statistics (see Chen and Goodman, 1999) to neural sequence models such as LSTMs (Mikolov et al., 2010), which optimize on the task of predicting the next token. Subsequently, Transformer-based architectures employing selfattention (Vaswani et al., 2017) have outperformed LSTMs (e.g., Dai et al., 2019). Although these shifts have resulted in stronger LMs, perplexity 378 on large benchmark dat"
2020.tacl-1.25,D19-1286,1,0.900149,"and reflexive licensing. However, these and related studies cover a limited set of phenomena, to the exclusion of well-studied phenomena in linguistics such as control and raising, ellipsis, quantification, and countless others. This is likely due to the labor-intensive nature of collecting such targeted minimal pairs. A related line of work evaluates neural networks on acceptability judgments in a more domaingeneral way. Corpora of sentences and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguis"
2020.tacl-1.25,W18-5446,1,0.885538,"am statistics (see Chen and Goodman, 1999) to neural sequence models such as LSTMs (Mikolov et al., 2010), which optimize on the task of predicting the next token. Subsequently, Transformer-based architectures employing selfattention (Vaswani et al., 2017) have outperformed LSTMs (e.g., Dai et al., 2019). Although these shifts have resulted in stronger LMs, perplexity 378 on large benchmark datasets like WikiText-103 (Merity et al., 2016) has remained the primary performance metric, which cannot give detailed insight into these models’ knowledge of grammar. Evaluation on benchmarks like GLUE (Wang et al., 2018, 2019a), which heavily adapt language models to perform downstream tasks, is more informative, but doesn’t offer broad coverage of linguistic phenomena, and doesn’t necessary reflect knowledge that is already present in the LMs. 2.2 Linguistic Knowledge of NNs Many recent studies have searched for evidence that neural networks (NNs) learn representations that implicitly encode grammatical concepts. We refer to the ability to encode these concepts as linguistic knowledge. Some studies evaluate NNs’ linguistic knowledge using probing tasks in which a classifier is trained to directly predict gr"
2020.tacl-1.25,Q19-1040,1,0.867374,"or assessing the sensitivity of LMs to acceptability contrasts across a wide range of English phenomena, covering both previously studied and novel contrasts. BLiMP consists of 67 datasets automatically generated from linguist-crafted grammar templates, each containing 1,000 minimal pairs and organized by phenomenon into 12 categories. Validation with crowdworkers shows that BLiMP faithfully represents human preferences. We use BLiMP to study several pretrained LMs: Transformer-based LMs GPT-2 (Radford et al., 2019) and Transformer-XL (Dai et al., 2019), an LSTM LM trained by Gulordava et al. (2019), and an n-gram LM. We evaluate whether the LM assigns a higher probability to the acceptable sentence in each minimal pair to determine which grammatical distinctions LMs are sensitive to. This gives us indirect evidence about each model’s linguistic knowledge and allows us to compare models in a fine-grained way. We conclude that current neural LMs appear to acquire robust knowledge of morphological agreement and some syntactic phenomena such as ellipsis and control/ raising. They show weaker evidence of knowledge about argument structure, negative polarity item licensing, and the semantic p"
2020.tacl-1.25,W18-5423,0,0.103406,"psis and control/ raising. They show weaker evidence of knowledge about argument structure, negative polarity item licensing, and the semantic properties of quantifiers. All models perform at or near chance on extraction islands. Overall, every model we 1 Introduction Current neural networks for sentence processing rely on unsupervised pretraining tasks like language modeling. Still, it is an open question how the linguistic knowledge of state-of-the-art language models (LMs) varies across the linguistic phenomena of English. Recent studies (e.g., Linzen et al., 2016; Marvin and Linzen, 2018; Wilcox et al., 2018) have explored this question by evaluating LMs’ preferences between minimal pairs of sentences differing in grammatical acceptability, as in Example 1. However, each 1 a. The cats annoy Tim. (grammatical) b. *The cats annoys Tim. (ungrammatical) https://github.com/alexwarstadt/blimp. 377 Transactions of the Association for Computational Linguistics, vol. 8, pp. 377–392, 2020. https://doi.org/10.1162/tacl a 00321 Action Editor: Mark Steedman. Submission batch: 1/2020; Revision batch: 3/2020; Published 7/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 licens"
2020.tacl-1.25,N19-1334,0,0.159977,"of collecting such targeted minimal pairs. A related line of work evaluates neural networks on acceptability judgments in a more domaingeneral way. Corpora of sentences and their grammaticality are collected for this purpose in a Phenomenon Relevant work Anaphora/binding Marvin and Linzen (2018), Futrell et al. (2018), Warstadt et al. (2019b) Subj.-verb agreement Linzen et al. (2016), Futrell et al. (2018), Gulordava et al. (2019), Marvin and Linzen (2018), An et al. (2019), Warstadt et al. (2019b) Neg. polarity items Marvin and Linzen (2018), Futrell et al. (2018), Jumelet and Hupkes (2018), Wilcox et al. (2019), Warstadt et al. (2019a) Filler-gap/Islands Wilcox et al. (2018), Warstadt et al. (2019b), Chowdhury and Zamparelli (2018, 2019), Chaves (2020), Da Costa and Chaves (2020) Argument structure Kann et al. (2019), Warstadt et al. (2019b), Chowdhury and Zamparelli (2019) Table 1: Summary of related work organized by linguistic phenomena tested. All studies analyze neural networks using acceptability judgments on minimal pairs mainly in English. Some studies appear multiple times. number of studies (Heilman et al., 2014; Lau et al., 2017; Warstadt et al., 2019b). The most recent and comprehensive"
2021.acl-long.90,2020.emnlp-main.553,0,0.0264816,"for linguistic features over surface features during fine-tuning on ambiguous classification tasks. Schijndel et al. (2019) find large improvements in knowledge of subject-verb agreement and reflexive binding up to 10M words, and little improvement between 10M and 80M words. Hu et al. (2020) find that GPT-2 trained on 42M words performs roughly as well on a syntax benchmark as a similar model trained on 100 times that amount. Other studies have investigated how one model’s linguistic knowledge changes during the training process, as a function of the number of updates (Saphra and Lopez, 2019; Chiang et al., 2020). Raffel et al. (2020) also investigate how performance on SuperGLUE (and other downstream tasks) improves with pretraining dataset size between about 8M and 34B tokens. In contrast to our findings, they find that models with around 500M tokens of pretraining data can perform similarly on downstream tasks to models with 34B words. However, there are many differences in our settings that may lead to this divergence. For example, they pretrain for a fixed number of iterations (totaling 34B token updates), whereas the MiniBERTas we use were pretrained with early stopping. They also use prefix pro"
2021.acl-long.90,2020.acl-main.420,0,0.0198481,"2020). With the advent of large pretrained Transformers like BERT (Devlin et al., 2019), numerous papers have used classifier probing methods to attempt to locate linguistic features in learned representations with striking positive results (Tenney et al., 2019b; Hewitt and Manning, 2019). However, another thread has found problems with many probing methods: Classifier probes can learn too much from training data (Hewitt and Liang, 2019) and can fail to distinguish features that are extractable from features that are actually used when generalizing on downstream tasks (Voita and Titov, 2020; Pimentel et al., 2020; Elazar et al., 2020). Moreover, different probing methods often yield contradictory results (Warstadt et al., 2019). There have also been a few earlier studies investigating the relationship between pretraining data volume and linguistic knowledge in language models. Studies of unsupervised acceptability judgments find fairly consistent evidence of rapid improvements in linguistic knowledge up to about 10M words of pretraining data, after which improvements slow down for most phenomena. van They measure RoBERTa’s preference for linguistic features over surface features during fine-tuning on"
2021.acl-long.90,2020.acl-main.467,1,0.888037,"Missing"
2021.acl-long.90,D12-1071,0,0.0230411,"e (Tenney et al., 2019b).7 Classifier probing has recently come under scrutiny. Hewitt and Liang (2019) and Voita and Titov (2020) caution that the results depend on the complexity of the probe, and so do not precisely reveal the quality of the representations. However, 7 Task data sources: Part-of-Speech, Constituents, Entities, SRL, and OntoNotes coref. from Weischedel et al. (2013), Dependencies from Silveira et al. (2014), Sem. Proto Role 1 from Teichert et al. (2017), Sem. Proto Role 2 from Rudinger et al. (2018), Relations (SemEval) from Hendrickx et al. (2010), and Winograd coref. from Rahman and Ng (2012); White et al. (2017). we see two advantages to this method: First, the downstream classifier setting and F1 evaluation metric make these experiments easier to interpret in the context of earlier results than results from relatively novel probing metrics like minimum description length. Second, we focus on relative differences between models rather than absolute performance, and include a randomly initialized baseline model in the comparison. When the model representations are random, the probe’s performance reflects the probe’s own ability to solve the target task. Therefore, any improvements"
2021.acl-long.90,D16-1264,0,0.215837,"Missing"
2021.acl-long.90,2020.tacl-1.54,0,0.164117,"e; SuperGLUE is a suite of conventional NLU tasks. Introduction Pretrained language models (LMs) like BERT and RoBERTa have become ubiquitous in NLP. New models require massive datasets of tens or even hundreds of billions of words (Brown et al., 2020) to improve on existing models on language understanding benchmarks like GLUE (Wang et al., 2018). Much recent work has used probing methods to evaluate what these models do and do not *Equal Contribution 0.8 Classifier Probing (Edge Probing) MDL Reflected (Edge Probing) BLiMP LAMA SuperGLUE learn (Belinkov and Glass, 2019; Tenney et al., 2019b; Rogers et al., 2020; Ettinger, 2020). Since most of these works only focus on models pretrained on a fixed data volume (usually billions of words), many interesting questions regarding the effect of the amount of pretraining data remain unanswered: What have data-rich models learned that makes them so effective on downstream tasks? How much pretraining data is required for LMs to learn different grammatical features and linguistic phenomena? Which of these skills do we expect to improve when we scale pretraining past 30 billion words? Which aspects of grammar can be learned from data volumes on par with the inpu"
2021.acl-long.90,D18-1114,0,0.0225649,"Missing"
2021.acl-long.90,2020.acl-main.240,0,0.0281163,"1B 30B None 100 1M 10M 100M 1B 30B None 60 None Accuracy 100 80 Overall Learning Curve Phenomenon Learning Curve Overall Results 1M 10M 100M 1B 30B 1M 10M 100M 1B 30B None None 60 Phenomenon Results RoBERTa-Large Task Performance Human Task Agreement Figure 5: BLiMP results by category. BLiMP has 67 constituent datasets covering 12 linguistic phenomena. For each task the objective is to predict the more grammatically acceptable sentence of a minimal pair in an unsupervised setting. For context, we also plot human accuracy numbers from Warstadt et al. (2020a) and RoBERTaLARGE performance from Salazar et al. (2020). data may be needed for the model to be exposed to relevant factual knowledge. The learning curves for many LAMA tasks do not show clear signs of saturation in the range of 0 to 30B words, suggesting further improvements are likely with much larger data quantities. Among LAMA tasks, ConceptNet most directly tests commonsense knowledge. The steep slope of the ConceptNet curve between 100M and 30B words of pretraining data and the large precision jump (> 0.05) from 1B to 30B show that increasing the pretraining data to over 1B words significantly improve the LM’s commonsense knowledge, which ex"
2021.acl-long.90,N19-1329,0,0.0283436,"re RoBERTa’s preference for linguistic features over surface features during fine-tuning on ambiguous classification tasks. Schijndel et al. (2019) find large improvements in knowledge of subject-verb agreement and reflexive binding up to 10M words, and little improvement between 10M and 80M words. Hu et al. (2020) find that GPT-2 trained on 42M words performs roughly as well on a syntax benchmark as a similar model trained on 100 times that amount. Other studies have investigated how one model’s linguistic knowledge changes during the training process, as a function of the number of updates (Saphra and Lopez, 2019; Chiang et al., 2020). Raffel et al. (2020) also investigate how performance on SuperGLUE (and other downstream tasks) improves with pretraining dataset size between about 8M and 34B tokens. In contrast to our findings, they find that models with around 500M tokens of pretraining data can perform similarly on downstream tasks to models with 34B words. However, there are many differences in our settings that may lead to this divergence. For example, they pretrain for a fixed number of iterations (totaling 34B token updates), whereas the MiniBERTas we use were pretrained with early stopping. Th"
2021.acl-long.90,silveira-etal-2014-gold,1,0.711181,"Missing"
2021.acl-long.90,speer-havasi-2012-representing,0,0.028239,"Missing"
2021.acl-long.90,P19-1355,0,0.0794127,"Missing"
2021.acl-long.90,D17-3004,0,0.038132,"Missing"
2021.acl-long.90,P19-1452,0,0.145713,"tests factual knowledge; SuperGLUE is a suite of conventional NLU tasks. Introduction Pretrained language models (LMs) like BERT and RoBERTa have become ubiquitous in NLP. New models require massive datasets of tens or even hundreds of billions of words (Brown et al., 2020) to improve on existing models on language understanding benchmarks like GLUE (Wang et al., 2018). Much recent work has used probing methods to evaluate what these models do and do not *Equal Contribution 0.8 Classifier Probing (Edge Probing) MDL Reflected (Edge Probing) BLiMP LAMA SuperGLUE learn (Belinkov and Glass, 2019; Tenney et al., 2019b; Rogers et al., 2020; Ettinger, 2020). Since most of these works only focus on models pretrained on a fixed data volume (usually billions of words), many interesting questions regarding the effect of the amount of pretraining data remain unanswered: What have data-rich models learned that makes them so effective on downstream tasks? How much pretraining data is required for LMs to learn different grammatical features and linguistic phenomena? Which of these skills do we expect to improve when we scale pretraining past 30 billion words? Which aspects of grammar can be learned from data volume"
2021.acl-long.90,I17-1100,0,0.0316474,"Missing"
2021.acl-long.92,2020.acl-main.747,0,0.119934,"Missing"
2021.acl-long.92,2020.acl-main.130,0,0.019878,"138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3 100.0 100.0 88.9 75.8 88.1 80.0 92.9 94.9 94.0 Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc"
2021.acl-long.92,D19-1606,0,0.0495168,"Missing"
2021.acl-long.92,D15-1075,1,0.813148,"Missing"
2021.acl-long.92,N19-1423,0,0.0754342,"Missing"
2021.acl-long.92,N19-1300,0,0.0154629,"2020) Sentence-Level Multiple Choice |Train| |Dev ||Test |Cust. Metric RoBERTa Human 2,490 138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3"
2021.acl-long.92,D16-1062,0,0.606927,"robability that a model will correctly handle an example in a test set depends on the model’s latent ability parameter and three example-specific parameters, typically measuring example difficulty (how strong does a model have to be to get it right), discrimination (how effective the example is for differentiating between similar models), and guessing (how likely a weak model is to get the example right for spurious reasons). This paper presents a large-scale IRT analysis of existing English NLU datasets. Unlike previous work which focuses on example-level analysis within individual datasets (Lalor et al., 2016, 2018), here we analyze example characteristics from a larger perspective by comparing individual examples across datasets. We evaluate test sets from 29 datasets in different formats—classification, multiple-choice QA, and span-selection QA. As responses, we use model predictions from 18 Transformer-based models, including some limited-capacity models chosen to expose better the dataset’s ability to discriminate weaker from stronger predictors. We then fit a single IRT model on these responses using a variational inference method.2 2 Our data and code can be found at https://github. com/nyu-"
2021.acl-long.92,P13-1139,0,0.0307078,"and False (Table 2). For SQuAD2.0 and QuAIL, we analyze the context length, the answerability of a question, and the lexical overlap between context and questions. However, we do not find any clear evidence that any of them might indicate the difficulty level of test examples. For BoolQ, we observe that the 20 most discriminating examples are all labeled False while 13 of the 20 least discriminating examples are labeled True. Table 2 shows the hardest and the easiest examples of MNLI and MC-TACO. 5 Related Work Prior work on using IRT to evaluate NLP systems mostly relies on human responses. Hopkins and May (2013) use IRT to estimate the relative ability of a set of machine translation systems using responses from pairwise comparison of system outputs by human judges. Otani et al. (2016) extend this work by including a baseline translation to the pairwise comparison. Lalor et al. (2016, 2018) use IRT to identify hard examples in natural language inference data based on human responses. In a follow-up study, Lalor et al. (2019) compare human versus model responses and find that both are positively correlated and demonstrate the use cases of IRT parameters in training set filtering. Sedoc and Ungar (2020"
2021.acl-long.92,P19-1439,1,0.900182,"Missing"
2021.acl-long.92,W18-5446,1,0.87719,"Missing"
2021.acl-long.92,N19-1421,0,0.132588,"Missing"
2021.acl-long.92,N18-1101,1,0.832608,"t as our training set and the rest as a our validation set while leaving the original test set untouched. 3 https://github.com/mrqa/ MRQA-Shared-Task-2019 1143 Classification COPA (Roemmele et al., 2011) WSC (Levesque et al., 2012) CommonsenseQA (CSQA; Talmor et al., 2019) MC-TACO (Zhou et al., 2019) SocialIQA (Sap et al., 2019) WiC (Pilehvar and Camacho-Collados, 2019) Abductive NLI (AbductNLI; Bhagavatula et al., 2020) PIQA (Bisk et al., 2020) WinoGrande (Sakaguchi et al., 2020) Span Selection Paragraph-Level Multiple Choice RTE (Dagan et al., 2005, et seq.) SNLI (Bowman et al., 2015) MNLI (Williams et al., 2018) CommitmentBank (CB; De Marneffe et al., 2019) ANLI (Nie et al., 2020) Sentence-Level Multiple Choice |Train| |Dev ||Test |Cust. Metric RoBERTa Human 2,490 138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT ("
2021.acl-long.92,W17-2623,0,0.0295077,"5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3 100.0 100.0 88.9 75.8 88.1 80.0 92.9 94.9 94.0 Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. 62.5 37.5 86.7 92.8 85.7 79.4 84.1 87.8 77.9 73.3 – – 79.8 98.2 89.0 94.0 95.6 93.8 93.0 – EM EM EM EM EM 79"
2021.acl-long.92,P19-1472,0,0.0660269,"t. Metric RoBERTa Human 2,490 138 139 550,152 10,000 10,000 392,702 9,823 9,824 250 28 28 1,105,719 3,200 3,200 400 554 9,741 3,026 33,410 5,428 169,654 16,113 40,398 50 52 610 757 977 319 766 919 633 50 52 611 9,442 977 319 766 919 634 2,251 1,119 1,211 14,191 9,427 25,262 39,905 7,088 7,088 10,246 570 299 317 2,020 1,635 1,492 5,021 443 443 2,164 2,376 1,172 445 3,610 1,635 1,493 5,021 443 443 556 ARC-Easy (Clark et al., 2018) ARC-Challenge (Clark et al., 2018) ARCT (Habernal et al., 2018) MCScript (Ostermann et al., 2018) BoolQ (Clark et al., 2019) Cosmos QA (Huang et al., 2019) HellaSwag (Zellers et al., 2019) MuTual (Cui et al., 2020) MuTual+ (Cui et al., 2020) QuAIL (Rogers et al., 2020) QAMR (Michael et al., 2018) NewsQA (Trischler et al., 2017) SQuAD2.0 (Rajpurkar et al., 2018) MRQA-NQ (Kwiatkowski et al., 2019) Quoref (Dasigi et al., 2019) 50,615 18,908 18,770 76,568 4,343 4,293 130,319 5,675 6,198 104,071 6,418 6,418 19,399 1,209 1,209 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 Acc. Acc. Acc. Acc. Acc. 87.6 92.7 89.7 90.5 50.8 93.6 – 92.0 95.8 – Acc. Acc. Acc. EM Acc. Acc. Acc. Acc. Acc. 86.0 78.8 74.6 55.9 79.9 71.5 85.0 77.6 77.3 100.0 100.0 88.9 75.8 88.1 80.0 92.9 94.9 94.0 Acc. Acc. Acc. Ac"
2021.acl-long.92,2021.acl-long.90,1,0.452405,"Missing"
2021.acl-long.98,D18-1241,0,0.0260458,"datasets, more appropriate for training neural networks, are often crowdsourced, though the crowdsourcing methods used vary widely. Popular datasets, such as SQuAD (Rajpurkar et al., 2016) for question answering and SNLI (Bowman et al., 2015) for natural language inference, are collected by providing crowdworkers with a context passage and instructing workers to write an example given the context. Rogers et al. (2020) crowdsource QuAIL, a QA dataset, by using a more constrained data collection protocol where they require workers to write nine specific types of question for each passage. QuAC (Choi et al., 2018) is crowdsourced by pairing crowdworkers, providing one worker with a Wikipedia article, and instructing the second worker to ask questions about the hidden article. Recently, there has been a flurry of corpora collected using adversarial models in the crowdsourcing pipeline. Dua et al. (2019), Nie et al. (2020a), and Bartolo et al. (2020) use models in the loop during data collection, where crowdworkers can only submit examples that cannot be solved by the models. However, such datasets can be biased towards quirks of the model used during data collection (Zellers et al., 2019; Gardner et al."
2021.acl-long.98,2020.acl-main.747,0,0.0851416,"Missing"
2021.acl-long.98,cotterell-callison-burch-2014-multi,0,0.0316391,"ment can be unreliable since poor-performing workers overestimate their ability. Drapeau et al. (2016) test a justifyreconsider strategy: Crowdworkers justify their annotations in a relation extraction task, they are shown a justification written by a different crowdworker, or an expert, and are asked to reconsider their annotation. They find that this method significantly boosts the accuracy of annotations. Another commonly used strategy when crowdsourcing NLP datasets is to only qualify workers who pass an initial quiz or perform well in preliminary crowdsourcing batches (Wang et al., 2013; Cotterell and Callison-Burch, 2014; Ning et al., 2020; Shapira et al., 2020; Roit et al., 2020). In addition to using careful qualifications, Roit et al. (2020) send workers feedback detailing errors they made in their QA-SRL annotation. Writing such feedback is labor-intensive and can become untenable as the number of workers grows. Dow et al. (2011) design a framework of promoting crowdworkers into “shepherding roles” to crowdsource such feedback. We compare expert and crowdsourced feedback in our EXPERT and CROWD protocols. 3 Data Collection Protocols We run our study on Amazon Mechanical Turk.2 At launch, crowdworkers are"
2021.acl-long.98,2020.findings-emnlp.171,0,0.0114994,"ntal pipeline is sketched in Figure 1. To quantify the dataset difficulty, we collect additional label annotations to establish human performance on each dataset and compare these to model performance. We also evaluate the difficulty of the datasets for typical machine learning models using IRT (Baker and Kim, 1993; Lalor et al., 2016). We find that the EXPERT protocol dataset is the most challenging. The human–model gap with RoBERTaLARGE (Liu et al., 2019b) on the unanimous agreement portion of EXPERT is 13.9 percentage point, compared to 7.0 on the BASELINE protocol. The gap with UnifiedQA (Khashabi et al., 2020) is 6.7 on EXPERT, compared to 2.9 on BASE LINE . However, the CROWD evaluation data is far less challenging than EXPERT, suggesting that expert evaluations are more reliable than crowdsourced evaluations for sending feedback and assigning qualifications. We also find that the JUSTIFICATION intervention is ineffective as a stand-alone method for increasing NLU data quality. A substantial proportion of the explanations submitted are duplicates, reused for multiple examples, or give trivial reasoning that is not specific to the example. 1222 Lastly, to evaluate the datasets for serious annotatio"
2021.acl-long.98,W09-1904,0,0.0634661,"eading comprehension (Rajpurkar et al., 2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentences (Hsueh et al., 2009; Liu et al., 2019a; Sun et al., 2020). There are some small-scale studies that use writing tasks, like writing product reviews, to compare crowdsourcing methodologies (Dow et al., 2012). However, we are unaware of any prior work that directly evaluates the effects of crowdsourcing protocol design choices on the quality of the resulting data for NLU tasks. Decisions around methodology and task design used to collect datasets dictate the quality of the data collected. As models become stronger and are able to solve existing NLU datasets, we have an increasing need for difficult, high-quality da"
2021.acl-long.98,D16-1062,0,0.0282262,"ted evaluations in EXPERT, and crowdsourced evaluations in CROWD for generating feedback and assigning qualifications. We use a a standard of high pay and strict qualifications for all protocols. We also validate the data to discard ambiguous and unanswerable examples. The experimental pipeline is sketched in Figure 1. To quantify the dataset difficulty, we collect additional label annotations to establish human performance on each dataset and compare these to model performance. We also evaluate the difficulty of the datasets for typical machine learning models using IRT (Baker and Kim, 1993; Lalor et al., 2016). We find that the EXPERT protocol dataset is the most challenging. The human–model gap with RoBERTaLARGE (Liu et al., 2019b) on the unanimous agreement portion of EXPERT is 13.9 percentage point, compared to 7.0 on the BASELINE protocol. The gap with UnifiedQA (Khashabi et al., 2020) is 6.7 on EXPERT, compared to 2.9 on BASE LINE . However, the CROWD evaluation data is far less challenging than EXPERT, suggesting that expert evaluations are more reliable than crowdsourced evaluations for sending feedback and assigning qualifications. We also find that the JUSTIFICATION intervention is ineffec"
2021.acl-long.98,D19-1434,0,0.0227931,"ints). And model accuracy on BASELINE remains stable, while it increases by 2.7 percentage points on JUSTIFICA TION. A task design with minimal constraints, like ours, does not prompt workers to write an easier question followed by a more difficult one, or vice versa. 5.4 Item Response Theory Individual examples within any dataset can have different levels of difficulty. To better understand the distribution of difficult examples in each protocol, we turn to Item Response Theory (IRT; Baker and Kim, 1993), which has been used to estimate individual example difficulty based on model responses (Lalor et al., 2019; Mart´ınez-Plumed et al., 2019). Specifically, we use the three-parameter logistic (3PL) IRT model, where an example is characterized by discrimination, difficulty, and guessing parameters. Discrimination defines how effective an example is at distinguishing between weak and strong models, difficulty defines the minimum ability of a model needed to obtain high performance, and the guessing parameter defines the probability of a correct answer by random guessing. Following Vania et al. (2021), we use 90 Transformer-based models fine-tuned on RACE, with varying ability levels, and use their pre"
2021.acl-long.98,2021.ccl-1.108,0,0.0671281,"Missing"
2021.acl-long.98,D18-1260,0,0.0239964,"is, on average, twice as large as the gap for the baseline protocol data. 1 Introduction Crowdsourcing is a scalable method for constructing examples for many natural language processing tasks. Platforms like Amazon’s Mechanical Turk give researchers access to a large, diverse pool of people to employ (Howe, 2006; Snow et al., 2008; Callison-Burch, 2009). Given the ease of data collection with crowdsourcing, it has been frequently ∗ † Equal contribution. Work done while at New York University. used for collecting datasets for natural language understanding (NLU) tasks like question answering (Mihaylov et al., 2018), reading comprehension (Rajpurkar et al., 2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentence"
2021.acl-long.98,2020.acl-main.441,0,0.241599,"ng tasks. Platforms like Amazon’s Mechanical Turk give researchers access to a large, diverse pool of people to employ (Howe, 2006; Snow et al., 2008; Callison-Burch, 2009). Given the ease of data collection with crowdsourcing, it has been frequently ∗ † Equal contribution. Work done while at New York University. used for collecting datasets for natural language understanding (NLU) tasks like question answering (Mihaylov et al., 2018), reading comprehension (Rajpurkar et al., 2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentences (Hsueh et al., 2009; Liu et al., 2019a; Sun et al., 2020). There are some small-scale studies that use writing tasks, like writing product reviews, to compare crowdsourcing met"
2021.acl-long.98,2020.emnlp-main.734,0,0.0654963,"Missing"
2021.acl-long.98,2020.emnlp-main.88,0,0.0228509,"rforming workers overestimate their ability. Drapeau et al. (2016) test a justifyreconsider strategy: Crowdworkers justify their annotations in a relation extraction task, they are shown a justification written by a different crowdworker, or an expert, and are asked to reconsider their annotation. They find that this method significantly boosts the accuracy of annotations. Another commonly used strategy when crowdsourcing NLP datasets is to only qualify workers who pass an initial quiz or perform well in preliminary crowdsourcing batches (Wang et al., 2013; Cotterell and Callison-Burch, 2014; Ning et al., 2020; Shapira et al., 2020; Roit et al., 2020). In addition to using careful qualifications, Roit et al. (2020) send workers feedback detailing errors they made in their QA-SRL annotation. Writing such feedback is labor-intensive and can become untenable as the number of workers grows. Dow et al. (2011) design a framework of promoting crowdworkers into “shepherding roles” to crowdsource such feedback. We compare expert and crowdsourced feedback in our EXPERT and CROWD protocols. 3 Data Collection Protocols We run our study on Amazon Mechanical Turk.2 At launch, crowdworkers are randomly assigned t"
2021.acl-long.98,Q19-1043,0,0.0202481,"and collect four small datasets in parallel including a baseline dataset with no interventions. We choose QA as our test-bed over the similarly popular testbed task of natural language inference (NLI) because of our focus on very high human-agreement examples which calls for minimizing label ambiguity. In multiple-choice QA, the correct label is the answer choice that is most likely to be correct, even if there is some ambiguity in whether that choice is genuinely true . In NLI however, if more than one label is plausible, then resolving the disagreement by ranking labels may not be possible (Pavlick and Kwiatkowski, 2019). In the trial, crowdworkers are randomly assigned to one of four protocols: BASELINE, JUSTIFICATION , CROWD , or EXPERT.1 In BASELINE, crowdworkers are simply asked to write question-answering examples. In JUSTIFICA TIONthey are tasked with also writing explanations for their examples, prompting self-assessment. For the EXPERT and CROWD protocols, we train work1 All the data is available at https://github.com/nyumll/crowdsourcing-protocol-comparison. ers using an iterative process of collecting data, sending feedback, and qualifying high performing workers to subsequent rounds. We use expertc"
2021.acl-long.98,S18-2023,0,0.0614876,"Missing"
2021.acl-long.98,D16-1264,0,0.351859,"he baseline protocol data. 1 Introduction Crowdsourcing is a scalable method for constructing examples for many natural language processing tasks. Platforms like Amazon’s Mechanical Turk give researchers access to a large, diverse pool of people to employ (Howe, 2006; Snow et al., 2008; Callison-Burch, 2009). Given the ease of data collection with crowdsourcing, it has been frequently ∗ † Equal contribution. Work done while at New York University. used for collecting datasets for natural language understanding (NLU) tasks like question answering (Mihaylov et al., 2018), reading comprehension (Rajpurkar et al., 2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentences (Hsueh et al., 2009; Liu et al., 2019a; Sun e"
2021.acl-long.98,2020.acl-main.626,0,0.0282392,"ty. Drapeau et al. (2016) test a justifyreconsider strategy: Crowdworkers justify their annotations in a relation extraction task, they are shown a justification written by a different crowdworker, or an expert, and are asked to reconsider their annotation. They find that this method significantly boosts the accuracy of annotations. Another commonly used strategy when crowdsourcing NLP datasets is to only qualify workers who pass an initial quiz or perform well in preliminary crowdsourcing batches (Wang et al., 2013; Cotterell and Callison-Burch, 2014; Ning et al., 2020; Shapira et al., 2020; Roit et al., 2020). In addition to using careful qualifications, Roit et al. (2020) send workers feedback detailing errors they made in their QA-SRL annotation. Writing such feedback is labor-intensive and can become untenable as the number of workers grows. Dow et al. (2011) design a framework of promoting crowdworkers into “shepherding roles” to crowdsource such feedback. We compare expert and crowdsourced feedback in our EXPERT and CROWD protocols. 3 Data Collection Protocols We run our study on Amazon Mechanical Turk.2 At launch, crowdworkers are randomly assigned to one of four data collection protocols, i"
2021.acl-long.98,D08-1027,0,0.392493,"Missing"
2021.acl-long.98,2020.coling-main.316,0,0.0356327,"2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentences (Hsueh et al., 2009; Liu et al., 2019a; Sun et al., 2020). There are some small-scale studies that use writing tasks, like writing product reviews, to compare crowdsourcing methodologies (Dow et al., 2012). However, we are unaware of any prior work that directly evaluates the effects of crowdsourcing protocol design choices on the quality of the resulting data for NLU tasks. Decisions around methodology and task design used to collect datasets dictate the quality of the data collected. As models become stronger and are able to solve existing NLU datasets, we have an increasing need for difficult, high-quality datasets that are still reliably solvabl"
2021.acl-long.98,N19-1270,0,0.0457244,"Missing"
2021.acl-long.98,P19-1472,0,0.0234164,"ch passage. QuAC (Choi et al., 2018) is crowdsourced by pairing crowdworkers, providing one worker with a Wikipedia article, and instructing the second worker to ask questions about the hidden article. Recently, there has been a flurry of corpora collected using adversarial models in the crowdsourcing pipeline. Dua et al. (2019), Nie et al. (2020a), and Bartolo et al. (2020) use models in the loop during data collection, where crowdworkers can only submit examples that cannot be solved by the models. However, such datasets can be biased towards quirks of the model used during data collection (Zellers et al., 2019; Gardner et al., 2020). Crowdsourcing Methods While crowdsourcing makes it easy to collect large datasets quickly, there are some clear pitfalls: Crowdworkers are generally less knowledgeable than field experts about the requirements the data needs to meet, crowdwork can be monotonous resulting in repetitive and noisy data, and crowdsourcing platforms can create a “market for lemons” where fast work is incentivized over careful, creative work because of poor quality requesters (Akerlof, 1978; Chandler et al., 2013). Daniel et al. (2018) give a broad overview of the variables at play when tryi"
2021.acl-long.98,2021.acl-long.90,1,0.815307,"Missing"
2021.acl-long.98,N19-1421,0,0.0154274,"rk give researchers access to a large, diverse pool of people to employ (Howe, 2006; Snow et al., 2008; Callison-Burch, 2009). Given the ease of data collection with crowdsourcing, it has been frequently ∗ † Equal contribution. Work done while at New York University. used for collecting datasets for natural language understanding (NLU) tasks like question answering (Mihaylov et al., 2018), reading comprehension (Rajpurkar et al., 2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentences (Hsueh et al., 2009; Liu et al., 2019a; Sun et al., 2020). There are some small-scale studies that use writing tasks, like writing product reviews, to compare crowdsourcing methodologies (Dow et al., 2012). However, we are unaw"
2021.acl-long.98,2021.acl-long.92,1,0.785895,"and Kim, 1993), which has been used to estimate individual example difficulty based on model responses (Lalor et al., 2019; Mart´ınez-Plumed et al., 2019). Specifically, we use the three-parameter logistic (3PL) IRT model, where an example is characterized by discrimination, difficulty, and guessing parameters. Discrimination defines how effective an example is at distinguishing between weak and strong models, difficulty defines the minimum ability of a model needed to obtain high performance, and the guessing parameter defines the probability of a correct answer by random guessing. Following Vania et al. (2021), we use 90 Transformer-based models fine-tuned on RACE, with varying ability levels, and use their predictions on our four datasets as responses. For comparison, we also use model predictions on QuAIL and CosmosQA. Refer to Appendix F for more details. Figure 4 shows the distribution of example difficulty for each protocol. Also plotted are the difficulty parameters for the intermediate rounds of data that are collected in the iterative feedback protocols.9 We see that EXPERT examples have the highest median and 75th percentile difficulty scores, 9 The IRT parameters for discrimination range"
2021.acl-long.98,N18-1101,1,0.897917,"tural language processing tasks. Platforms like Amazon’s Mechanical Turk give researchers access to a large, diverse pool of people to employ (Howe, 2006; Snow et al., 2008; Callison-Burch, 2009). Given the ease of data collection with crowdsourcing, it has been frequently ∗ † Equal contribution. Work done while at New York University. used for collecting datasets for natural language understanding (NLU) tasks like question answering (Mihaylov et al., 2018), reading comprehension (Rajpurkar et al., 2016; Huang et al., 2019), natural language inference (Dagan et al., 2005; Bowman et al., 2015; Williams et al., 2018; Nie et al., 2020a), and commonsense reasoning (Talmor et al., 2019). There has been substantial research devoted to studying crowdsourcing methods, especially in the human-computer interaction literature (Kittur et al., 2008, 2011; Bernstein et al., 2012). However, most prior research investigates methods for collecting accurate annotations for existing data, for example labeling objects in images or labeling the sentiment of sentences (Hsueh et al., 2009; Liu et al., 2019a; Sun et al., 2020). There are some small-scale studies that use writing tasks, like writing product reviews, to compare"
2021.blackboxnlp-1.42,S17-2001,0,0.0285571,"LP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538 Online, November 11, 2021. ©2021 Association for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avoid taskspecific hyperparameter tuning, we fine-tune on each task for up to 10,000 steps. We use the Adam (Kingma and Ba, 2"
2021.blackboxnlp-1.42,N19-1300,0,0.0161901,"tion for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avoid taskspecific hyperparameter tuning, we fine-tune on each task for up to 10,000 steps. We use the Adam (Kingma and Ba, 2014) optimizer with batch size of 4, a learning rate of 1e-5, and 1,000 warmup optimization steps. We use the jiant (Phang"
2021.blackboxnlp-1.42,I05-5002,0,0.0481121,"resentations for almost all task-tuned RoBERTa 2020). Because of the large number of exper529 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538 Online, November 11, 2021. ©2021 Association for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avo"
2021.blackboxnlp-1.42,D19-1243,0,0.020807,"Missing"
2021.blackboxnlp-1.42,D19-1445,0,0.024533,"cent layers have high similarity scores, only gradually decreasing as more distant layers are compared. FT–ORIG We show layers of the task-tuned model on the Y-axis and untuned model on the X-axis. The CLS representations of the later layers in the task-tuned model appear highly dissimilar to any of the untuned model: In other words, the representations differ starkly from those used for ALBERT’s masked language modeling (MLM) and sentence order prediction (SOP) pretraining. This coheres with prior work showing that representations of later layers are most likely to change during fine-tuning (Kovaleva et al., 2019; Wu et al., 2020). FT–FT Next, we compare layers within a single fine-tuned model. We observe a block-diagonal structure in the representation similarities—two distinct clusters of earlier (approx. first 10) and later (approx. last 14) layers that have high inter-cluster but low intra-cluster similarity. When considered together with FT-ORIG, we can infer that the earlier layer representations resemble those used for pretraining, whereas the later layers encode a representation suitable for tackling the task. The high internal similarity between the top few layers and the sharp block diagonal"
2021.blackboxnlp-1.42,N19-1112,0,0.353756,"ores of CLS (classifier token) representations of ORIG (untuned ALBERT) and FT (fine-tuned) models on RTE, across different layers of the model. FT[1]–FT[2] compares two RTE models with different random restarts. ORIG–ORIG and FT– FT are symmetric by construction. Fine-tuned models exhibit a block-diagonal structure in the representation similarities. The same color scale is used in all plots. Introduction and ALBERT models, where early layer represenFine-tuning pretrained language encoders such as tations and later layer representations form two BERT (Devlin et al., 2019) and its successors (Liu et al., 2019b; Lan et al., 2020; Clark et al., 2020; distinct clusters, with high intra-cluster and low He et al., 2020) has proven to be highly success- inter-cluster similarity. Given the strong representation similarity of later ful, attaining state-of-the-art performance on many model layers, we hypothesize that many of the language tasks, but how do these models internally later layers only marginally contribute to task perrepresent task-specific knowledge? In this work, we study how learned representa- formance. We show in experiments that the later layers of task-tuned RoBERTa and ALBERT can tions"
2021.blackboxnlp-1.42,2020.blackboxnlp-1.4,0,0.0203924,"33 5 Related Work While CKA (Kornblith et al., 2019) was initially proposed as an interpretability method for computer vision models, it has more recently seen application to NLP models. Wu et al. (2020) applied CKA to pretrained Transformers models such as BERT and GPT-2, focusing on cross-model comparison—our analysis builds on their findings, with greater focus on layer-wise comparisons and implications for fine-tuning and discarding layers. Sridhar and Sarah (2020) use CKA to measure the impact of a proposed model architecture change on the learned representations. Voita et al. (2019) and Merchant et al. (2020) apply similar representation similarity analyses to Transformers, with the latter also investigating freezing and dropping layers from models. More broadly, significant work has been done on better understanding and interpreting the capabilities of BERT-type models—Rogers et al. (2020) offers a thorough survey of this line of work. Of particular relevance to our work: Work on model probing (Tenney et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a) has studied the extent to syntactic and semantic features are represented at different layers of BERT-type models. Our results on model trunca"
2021.blackboxnlp-1.42,2020.acl-demos.15,1,0.836138,"2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avoid taskspecific hyperparameter tuning, we fine-tune on each task for up to 10,000 steps. We use the Adam (Kingma and Ba, 2014) optimizer with batch size of 4, a learning rate of 1e-5, and 1,000 warmup optimization steps. We use the jiant (Phang et al., 2020) library, built on Transformers (Wolf et al., 2020) and PyTorch (Paszke et al., 2019), to run our experiments. 3 Representation Similarity with CKA To analyze how learned representations change via fine-tuning, we use centered kernel alignment (CKA; Kornblith et al., 2019) to measure representation similarity. CKA is invariant to both orthogonal transformation and isotropic scaling of the compared representations, making it ideal for measuring the similarity of neural network representations, and has applied to BERT-type models in prior work (Wu et al., 2020; Sridhar and Sarah, 2020). Given tw"
2021.blackboxnlp-1.42,D16-1264,0,0.00925438,"-tuned RoBERTa 2020). Because of the large number of exper529 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538 Online, November 11, 2021. ©2021 Association for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avoid taskspecific hyperparameter"
2021.blackboxnlp-1.42,2020.tacl-1.54,0,0.0182701,"ss-model comparison—our analysis builds on their findings, with greater focus on layer-wise comparisons and implications for fine-tuning and discarding layers. Sridhar and Sarah (2020) use CKA to measure the impact of a proposed model architecture change on the learned representations. Voita et al. (2019) and Merchant et al. (2020) apply similar representation similarity analyses to Transformers, with the latter also investigating freezing and dropping layers from models. More broadly, significant work has been done on better understanding and interpreting the capabilities of BERT-type models—Rogers et al. (2020) offers a thorough survey of this line of work. Of particular relevance to our work: Work on model probing (Tenney et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a) has studied the extent to syntactic and semantic features are represented at different layers of BERT-type models. Our results on model truncation also cohere with existing work on early exit in BERT models(Xin et al., 2020a,b; Zhou et al., 2020), wherein models are explicitly fine-tuned to dynamically skip the later layers of a BERT encoder and directly to the output head, often to reduce inference times of models. Our resul"
2021.blackboxnlp-1.42,D13-1170,0,0.00497302,"oceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538 Online, November 11, 2021. ©2021 Association for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avoid taskspecific hyperparameter tuning, we fine-tune on each task for up to 10,000 steps. We"
2021.blackboxnlp-1.42,D19-1448,0,0.0214848,"impact to performance. 533 5 Related Work While CKA (Kornblith et al., 2019) was initially proposed as an interpretability method for computer vision models, it has more recently seen application to NLP models. Wu et al. (2020) applied CKA to pretrained Transformers models such as BERT and GPT-2, focusing on cross-model comparison—our analysis builds on their findings, with greater focus on layer-wise comparisons and implications for fine-tuning and discarding layers. Sridhar and Sarah (2020) use CKA to measure the impact of a proposed model architecture change on the learned representations. Voita et al. (2019) and Merchant et al. (2020) apply similar representation similarity analyses to Transformers, with the latter also investigating freezing and dropping layers from models. More broadly, significant work has been done on better understanding and interpreting the capabilities of BERT-type models—Rogers et al. (2020) offers a thorough survey of this line of work. Of particular relevance to our work: Work on model probing (Tenney et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a) has studied the extent to syntactic and semantic features are represented at different layers of BERT-type models."
2021.blackboxnlp-1.42,W18-5446,1,0.873975,"Missing"
2021.blackboxnlp-1.42,Q19-1040,1,0.844369,"f learned rep- (Lan et al., 2020) and ELECTRA (Clark et al., resentations for almost all task-tuned RoBERTa 2020). Because of the large number of exper529 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538 Online, November 11, 2021. ©2021 Association for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the nu"
2021.blackboxnlp-1.42,2020.acl-main.204,0,0.0218564,", with the latter also investigating freezing and dropping layers from models. More broadly, significant work has been done on better understanding and interpreting the capabilities of BERT-type models—Rogers et al. (2020) offers a thorough survey of this line of work. Of particular relevance to our work: Work on model probing (Tenney et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a) has studied the extent to syntactic and semantic features are represented at different layers of BERT-type models. Our results on model truncation also cohere with existing work on early exit in BERT models(Xin et al., 2020a,b; Zhou et al., 2020), wherein models are explicitly fine-tuned to dynamically skip the later layers of a BERT encoder and directly to the output head, often to reduce inference times of models. Our results somewhat differ as we show that models can also be truncated or exited early without any explicit tuning. It has also been shown in the computer vision domain that models with residual networks work akin to an ensemble of deep and shallow models (Veit et al., 2016). 6 Conclusion els, which prompts further investigation into how and why these models differ. Acknowledgments We would like to"
2021.blackboxnlp-1.42,P19-1472,0,0.017342,"CTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training. To control for this, and to avoid taskspecific hyperparameter tuning, we fine-tune on each task for up to 10,000 steps. We use the Adam (Kingma and Ba, 2014) optimizer with batch size of 4, a learning rate of 1e-5, and 1,000 warmup optimization steps. We use the jiant (Phang et al., 2020) library, built on Transformers (Wolf et al., 2020) and PyTorch (Paszke et al., 2019), to ru"
2021.blackboxnlp-1.42,N18-1101,1,0.667057,"20) and ELECTRA (Clark et al., resentations for almost all task-tuned RoBERTa 2020). Because of the large number of exper529 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 529–538 Online, November 11, 2021. ©2021 Association for Computational Linguistics iments being performed, we use RoBERTaBASE , ALBERTL ARGE V2 and ELECTRABASE rather than the largest available versions of these models. Tasks We use the tasks included in the GLUE benchmark (Wang et al., 2018) excluding the datapoor WNLI, namely: CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,1 RTE (Dagan et al., 2005), SST-2 (Socher et al., 2013), and STS-B (Cer et al., 2017). We include four additional tasks to cover a more diverse set of task formats and difficulties: BoolQ (Clark et al., 2019) and Yelp Review Polarity (Zhang et al., 2015) classification tasks, and HellaSwag (Zellers et al., 2019) and CosmosQA (Huang et al., 2019) multiple-choice tasks. Optimization The representations learned over the course of training and similarity of representations may be sensitive to the number of steps used in training"
2021.blackboxnlp-1.42,2020.acl-main.422,0,0.21226,"zation steps. We use the jiant (Phang et al., 2020) library, built on Transformers (Wolf et al., 2020) and PyTorch (Paszke et al., 2019), to run our experiments. 3 Representation Similarity with CKA To analyze how learned representations change via fine-tuning, we use centered kernel alignment (CKA; Kornblith et al., 2019) to measure representation similarity. CKA is invariant to both orthogonal transformation and isotropic scaling of the compared representations, making it ideal for measuring the similarity of neural network representations, and has applied to BERT-type models in prior work (Wu et al., 2020; Sridhar and Sarah, 2020). Given two sets of representations X ∈ RN ×d1 and Y ∈ RN ×d1 where N is the number of examples and d1 , d2 the hidden dimensions, CKA computes a similarity score between 0 and 1, where a higher score indicates greater similarity. Further details on CKA are provided in Appendix A. Using CKA, we can compare the similarity of representations between different layers of the same model or even different models. For our analysis, we use the representations of the CLS token, i.e. the token whose final layer representation is fed to the task output head.2 We compute CKA over"
2021.blackboxnlp-1.42,2020.sustainlp-1.11,0,0.023887,", with the latter also investigating freezing and dropping layers from models. More broadly, significant work has been done on better understanding and interpreting the capabilities of BERT-type models—Rogers et al. (2020) offers a thorough survey of this line of work. Of particular relevance to our work: Work on model probing (Tenney et al., 2019b; Liu et al., 2019a; Tenney et al., 2019a) has studied the extent to syntactic and semantic features are represented at different layers of BERT-type models. Our results on model truncation also cohere with existing work on early exit in BERT models(Xin et al., 2020a,b; Zhou et al., 2020), wherein models are explicitly fine-tuned to dynamically skip the later layers of a BERT encoder and directly to the output head, often to reduce inference times of models. Our results somewhat differ as we show that models can also be truncated or exited early without any explicit tuning. It has also been shown in the computer vision domain that models with residual networks work akin to an ensemble of deep and shallow models (Veit et al., 2016). 6 Conclusion els, which prompts further investigation into how and why these models differ. Acknowledgments We would like to"
2021.emnlp-tutorials.1,S18-2023,0,0.0382582,"Missing"
2021.emnlp-tutorials.1,N18-2017,1,0.892347,"Missing"
2021.emnlp-tutorials.1,D16-1264,0,0.0431704,"decisions in complex scenarios and the reasoning behind them. NLP researchers aiming to develop new datasets, tasks and data collection protocols will find the content directly applicable to their own work. A strong understanding of data collection practices and the range of decisions they include will also aid researchers using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principle"
2021.emnlp-tutorials.1,W17-1609,0,0.0615736,"Missing"
2021.emnlp-tutorials.1,marelli-etal-2014-sick,0,0.0252616,"ng environment and acting in the environment. The game rules were explicitly designed with the intent of eliciting rich collaborative interactions across many instructions, for example by allowing a pair of players that is scoring well to continue playing for longer, thereby collecting more data from successful collaborations. The CerealBar data collection process included a development of a community of players, which has demonstrated behavioral and linguistic change over the crowdsourcing process. • The development of a simple crowdworkerwriting protocol for natural language inference data (Marelli et al., 2014; Bowman et al., 2015; Williams et al., 2018) • Known issues with artifacts, social bias, and debatable judgments in data collected under this protocol (Rudinger et al., 2017; Tsuchiya, 2018; Gururangan et al., 2018; Poliak et al., 2018; Pavlick and Kwiatkowski, 2019) • Experiments evaluating data collection feasibility under variants of the base task definition (Chen et al., 2020; Bowman et al., 2020) Case Study IV: QuAC (25 min) Question Answering in Context is a dataset for studying information seeking dialogs between a student and a teacher (Choi et al., 2018). Given a subject heading, a s"
2021.emnlp-tutorials.1,D19-1454,1,0.932917,"s using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. This tutorial exposes NLP researchers to such data collection crowdsourcing methods and principles through a detailed discussion of a diverse set of case studies. Post-tutorial Materials The tutorial videos, slides and other mate"
2021.emnlp-tutorials.1,K17-1004,1,0.826389,"Q A (Sap et al., 2019b) is the first large-scale benchmark to test model emotional and social reasoning through 38k questions about everyday situations. The distributional nature of social commonsense knowledge requires the answer candidates to cover the plausible and likely, as well as the plausible but unlikely, as opposed to right/wrong answer candidates as common in other QA benchmarks. S OCIAL IQ A introduces a question-switching technique for crowdsourcing these unlikely answers, to overcome the possible stylistic artefacts in negative answers (e.g., negations, out-of-context responses; Schwartz et al., 2017). Additionally, to achieve large-scale and broad coverage, S OCIAL IQ A used a multi-stage crowdsourcing pipeline to expand seed events from the ATOMIC (Sap et al., 2019a) commonsense knowledge graph into full-fledged social situations. 5 Reading List We recommend reviewing the 2015 NAACL tutorial on crowdsourcing.3 While we focus on unconstrained and complex case studies, the 2015 tutorial provides an overview of basic terms and methods that is a complementary background to our material. However, we review the required material in the background section, and do not assume a familiarity with t"
2021.emnlp-tutorials.1,D18-1287,1,0.830602,"l find the content directly applicable to their own work. A strong understanding of data collection practices and the range of decisions they include will also aid researchers using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. This tutorial exposes NLP researchers to such data collec"
2021.emnlp-tutorials.1,P17-2034,1,0.934295,"practices and the range of decisions they include will also aid researchers using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. This tutorial exposes NLP researchers to such data collection crowdsourcing methods and principles through a detailed discussion of a diverse set of case s"
2021.emnlp-tutorials.1,2020.acl-main.441,0,0.0428154,"ng, a student questions a teacher, who responds by copying spans from a Wikipedia article. The goal of the pair is to maintain a dialog of sufficient length without encountering too many unanswerable questions. The task is to play the role of the teacher: answering questions of an interested student. The collection protocol is unique in that two unreliable workers had to be coordinated for sufficient time to accomplish a meaningful dialog. QuAC collection relied on several strategies to keep • Studies evaluating the feasibility of collecting data for the same task using alternative protocols (Nie et al., 2020; Kaushik et al., 2019; Bowman et al., 2020; Vania et al., 2020; Parrish et al., 2021) Case Study II: NLVR (25 min) Natural Language for Visual Reasoning comprises two datasets, NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019b), both study natural language sentences grounded in visual context.1 The task is to de1 2 http://lil.nlp.cornell.edu/nlvr/ 2 http://lil.nlp.cornell.edu/cerealbar/ partners from leaving interactions, such as allowing workers to simultaneously participate in multiple related dialogs, a feedback system teachers used to help students formulate questions, and scaling in"
2021.emnlp-tutorials.1,D19-1218,1,0.898706,"Missing"
2021.emnlp-tutorials.1,P19-1644,1,0.943001,"irectly applicable to their own work. A strong understanding of data collection practices and the range of decisions they include will also aid researchers using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. This tutorial exposes NLP researchers to such data collection crowdsourcing"
2021.emnlp-tutorials.1,N19-1421,0,0.0115176,"l also aid researchers using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high quality and diverse data. This tutorial exposes NLP researchers to such data collection crowdsourcing methods and principles through a detailed discussion of a diverse set of case studies. Post-tutorial Materials The tutorial videos, sli"
2021.emnlp-tutorials.1,D19-1514,0,0.0135125,"n includes the basic structure of a Mechanical Turk task (HIT), typical incentive mechanisms, typical communication mechanisms, typical worker qualification and screening mechanisms, as well as relevant results about the demographics and expressed preferences of crowdworkers and the crowdworker community. termine whether a caption is true or false about a paired image. The data was collected to require reasoning about object quantities, comparisons between object properties, and spatial relations between objects. NLVR2 is used as evaluation data for numerous language-and-vision systems (e.g., Tan and Bansal, 2019; Chen et al., 2019c). Both datasets were crowdsourced with a contrastive captioning designed to elicit linguistically complex sentences and to naturally balance the datasets between true and false examples. NLVR2 also uses a tiered system during crowdsourcing including distinct pools of annotation tasks for experienced workers and new workers. Case Study I: MultiNLI (45 min) We discuss the MultiNLI (Williams et al., 2018) corpus, with primary focus on experiments from subsequent papers that extend or evaluate the data collection protocol used to create this dataset. MultiNLI is built around t"
2021.emnlp-tutorials.1,L18-1239,0,0.0500816,"Missing"
2021.emnlp-tutorials.1,2020.aacl-main.68,1,0.771447,"ans from a Wikipedia article. The goal of the pair is to maintain a dialog of sufficient length without encountering too many unanswerable questions. The task is to play the role of the teacher: answering questions of an interested student. The collection protocol is unique in that two unreliable workers had to be coordinated for sufficient time to accomplish a meaningful dialog. QuAC collection relied on several strategies to keep • Studies evaluating the feasibility of collecting data for the same task using alternative protocols (Nie et al., 2020; Kaushik et al., 2019; Bowman et al., 2020; Vania et al., 2020; Parrish et al., 2021) Case Study II: NLVR (25 min) Natural Language for Visual Reasoning comprises two datasets, NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019b), both study natural language sentences grounded in visual context.1 The task is to de1 2 http://lil.nlp.cornell.edu/nlvr/ 2 http://lil.nlp.cornell.edu/cerealbar/ partners from leaving interactions, such as allowing workers to simultaneously participate in multiple related dialogs, a feedback system teachers used to help students formulate questions, and scaling incentives that included punitive elements. all necessary backgro"
2021.emnlp-tutorials.1,N18-1101,1,0.920394,"P researchers aiming to develop new datasets, tasks and data collection protocols will find the content directly applicable to their own work. A strong understanding of data collection practices and the range of decisions they include will also aid researchers using existing dataset to critically assess the data they use, including its limitations. Crowdsourcing from non-experts is one of the most common approaches to collecting data and annotations in NLP. It has been applied to a plethora of tasks, including question answering (Rajpurkar et al., 2016; Choi et al., 2018), textual entailment (Williams et al., 2018; Khot et al., 2018), instruction following (Bisk et al., 2016; Misra et al., 2018; Suhr et al., 2019a; Chen et al., 2019a), visual reasoning (Antol et al., 2015; Suhr et al., 2017, 2019b), and commonsense reasoning (Talmor et al., 2019; Sap et al., 2019b). Even though it is such a fundamental tool, crowdsourcing use is largely guided by common practices and the personal experience of researchers. Developing a theory of crowdsourcing use for practical language problems remains an open challenge. However, there are various principles and practices that have proven effective in generating high q"
2021.naacl-main.385,2020.acl-main.463,0,0.0223799,"success of this shift has indirectly laid the groundwork for the widespread use of poor-quality benchmarks. Blodgett et al. (2020) challenge researchers working on social bias in NLP to focus more precisely on specific types of harm to specific populations of users, a challenge that our broad position piece does not fully meet. NLP has had longstanding debates over the types of tasks that best test substantial language understanding skills. Many task-specific papers contribute to this debate, as does a prominent recent thread advocating for an increased focus on grounding of various kinds by Bender and Koller (2020), Bisk et al. (2020), Zellers et al. (2020), and others. 6 Conclusion Benchmarking for NLU is broken. We lay out four major criteria that benchmarks should fulfill to offer faithful, useful, and responsible measures of language ability. We argue that departing from IID evaluation (as is seen with benchmark datasets collected by adversarial filtering) does not help to address these criteria, but lay out in broad strokes how each criterion might be addressed directly. Nonetheless, important open research questions remain. Most centrally, it is still unclear how best to integrate expert effort in"
2021.naacl-main.385,2020.emnlp-main.703,0,0.0443109,"Missing"
2021.naacl-main.385,N19-1423,0,0.143803,"nding (NLU) has focused on improving results on benchmark datasets that feature roughly independent and identically distributed (IID) training, validation, and testing sections, drawn from data that were collected or annotated by crowdsourcing (Maas et al., 2011; Bowman et al., 2015; Rajpurkar et al., 2016; Wang et al., 2019b). Recent methodological progress combined with longstanding issues in crowdsourced data quality has made it so state-of-the-art systems are nearing the maximum achievable values on most of these benchmarks and thus are unlikely to be able to measure further improvements (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843–4855 June 6–11, 2021. ©2021 Association for Computational Linguistics The DynaBench approach falls into the broader category of adversarial filtering (Paperno et al., 2016; Zellers et al., 2018; Nie et al., 2020; Le B"
2021.naacl-main.385,D19-1224,0,0.0189444,"it freestanding datasheets documenting existing training set (Rudinger et al., 2018; Webster dataset releases of all kinds, with a focus on maket al., 2018; Kiritchenko and Mohammad, 2018; Li ing potential harmful mismatches between data and et al., 2020). Even so, refining these metrics and application visible, and Hutchinson et al. (2021) developing new ones will likely require us to face argue along similar lines for a broader program of many of the same challenges that we highlight in transparency and stakeholder engagement in data this paper for benchmark design more generally. creation. Dodge et al. (2019) lay out a set of best The larger challenge in implementing this ap- practices for results reporting, with a focus on the proach, however, is a matter of community struc- impact of hyperparameter tuning on model comture and incentive design. Methods papers dealing parison. Ethayarajh and Jurafsky (2020) advocate with tasks for which metrics already exist rarely for the inclusion of efficiency considerations in report numbers on these metrics. Even for the Su- leaderboard design. Boyd-Graber and Börschinger perGLUE benchmark, which requires users to com- (2020) describe ways that trivia competi"
2021.naacl-main.385,2020.acl-main.701,0,0.0230359,"h community direct, fine-grained control over the data on which their systems will be evaluated. Intentionally or unintentionally, this can produce data that is oriented toward linguistic phenomena that are widely studied and widely known to be important to the task at hand. While this can be helpful when building diagnostic datasets that focus on specific types of model failure (Cooper et al., 1996; Naik et al., 2018; Wang et al., 2019b), it is counterproductive when our goal is to build a broad-coverage benchmark dataset to set priorities and guide progress toward the solution of some task. Dunietz et al. (2020) and Sugawara et al. (2020a) work around this issue by leaning on taxonomies of required phenomena from outside NLP. This is a direction worth pursuing, but it is not clear that appropriate taxonomies will be available for most NLU tasks of interest, or that these taxonomies will be broad and thorough enough to be straightforwardly implemented as datasets. Crowdsourcing Most recent benchmarks for language understanding have been collected, at least in part, through crowdsourcing example construction, where non-expert annotators are given some freedom to construct examples based on a simple set"
2021.naacl-main.385,2020.emnlp-main.393,0,0.0243172,"application visible, and Hutchinson et al. (2021) developing new ones will likely require us to face argue along similar lines for a broader program of many of the same challenges that we highlight in transparency and stakeholder engagement in data this paper for benchmark design more generally. creation. Dodge et al. (2019) lay out a set of best The larger challenge in implementing this ap- practices for results reporting, with a focus on the proach, however, is a matter of community struc- impact of hyperparameter tuning on model comture and incentive design. Methods papers dealing parison. Ethayarajh and Jurafsky (2020) advocate with tasks for which metrics already exist rarely for the inclusion of efficiency considerations in report numbers on these metrics. Even for the Su- leaderboard design. Boyd-Graber and Börschinger perGLUE benchmark, which requires users to com- (2020) describe ways that trivia competitions can 4850 provide a model for carefully-considered dataset design. Church and Hestness (2019) revisit the arguments that motivated the NLP community’s shift toward quantitative benchmarking in the early 1990s and warn that the overwhelming success of this shift has indirectly laid the groundwork fo"
2021.naacl-main.385,W17-5401,0,0.0286144,"ther cupation terms like lawyer or doctor than typically than fluent knowledge of the language variety unwhite male names like Scott are, then a model using der study. those representations is likely to reinforce harmful One promising direction involves methods that race or gender biases in any downstream content start from relatively high-quality crowdsourced moderation systems or predictive text systems it datasets, then use expert effort to augment them in gets used in. ways that mitigate annotation artifacts. The BuildAdequately enumerating the social attributes for it-Break-it challenge (Ettinger et al., 2017), the which we might want to evaluate bias in some con- Open Reading Benchmark (Dua et al., 2019), and text can be difficult. For example, Indian castes, the Gardner et al. (2020) contrast sets, among their like racial categories in the United States, are of- other features, allow expert annotators to add examten signaled by names and are an axis on which ples to a test set to fill perceived gaps in coverage managers sometimes discriminate in hiring. Caste or correct perceived artifacts in a starting set of is a salient category of social bias in India that is crowdsourced examples. To the ext"
2021.naacl-main.385,W19-3621,0,0.0279061,", and we nonetheless have a clear opportunity to mitigate some of the potential harms caused by applied NLP systems before those systems are even developed. Opting not to test models for some plausible and potentiallyharmful social bias is, intentionally or not, a political choice. While it would be appealing to try to guarantee that our evaluation data does not itself demonstrate evidence of bias, we are aware of no robust strategy for reliably accomplishing this, and work on the closely-related problem of model bias mitigation has been fraught with false starts and overly optimistic claims (Gonen and Goldberg, 2019). A viable alternate approach could involve the expanded use of auxiliary metrics: Rather than trying to fully mitigate bias within a single general dataset and metric for some task, benchmark creators can introduce a family of additional expert-constructed test datasets and metrics that each isolate and measure a specific type of bias. Any time a model is evaluated on the primary task test set in this setting, it would be evaluated in parallel on these additional bias test sets. This would not prevent the primary metric from unintentionally and subtly rewarding biased models, but it would com"
2021.naacl-main.385,2020.findings-emnlp.314,0,0.0264224,"as no such recognition some measurable artifacts or flaws, this compro(Tiku, 2020), and where it could be easily over5 The ACM code of ethics states, “when the interests of looked by non-specialist bias researchers. multiple groups conflict, the needs of those less advantaged Furthermore, building such a list of attributes should be given increased attention and priority.” 4848 mise approach may help to create usable benchmark datasets out of the results. Another approach brings computational linguists directly into the crowdsourcing process. This was recently demonstrated at a small scale by Hu et al. (2020) with OCNLI: They show that it is possible to significantly improve data quality issues by making small interventions during the crowdsourcing process—like offering additional bonus payments for examples that avoid overused words and constructions—without significantly limiting annotators’ freedom to independently construct creative examples. Of course, implementing interventions like these in a way that offers convincing evidence of validity will be difficult. 4.2 Improving Handling of Annotation Errors and Disagreements annotator judgments per evaluation example. 4.3 Improving Statistical Po"
2021.naacl-main.385,D19-1243,0,0.0463062,"Missing"
2021.naacl-main.385,D17-1215,0,0.0431106,"Missing"
2021.naacl-main.385,S18-2005,0,0.0293475,"sues that don’t quite fit our schema. Welty et al. (2019) advocate for the more precise reporting of the focus and abilities of test sets and metrics in ML broadly, with a focus on isFor several tasks, metrics like this already ex- sues surrounding statistical power. Bender and ist, at least for gender in English, in the form of Friedman (2018) and Gebru et al. (2018) advocate auxiliary test sets meant to be combined with a pre- for explicit freestanding datasheets documenting existing training set (Rudinger et al., 2018; Webster dataset releases of all kinds, with a focus on maket al., 2018; Kiritchenko and Mohammad, 2018; Li ing potential harmful mismatches between data and et al., 2020). Even so, refining these metrics and application visible, and Hutchinson et al. (2021) developing new ones will likely require us to face argue along similar lines for a broader program of many of the same challenges that we highlight in transparency and stakeholder engagement in data this paper for benchmark design more generally. creation. Dodge et al. (2019) lay out a set of best The larger challenge in implementing this ap- practices for results reporting, with a focus on the proach, however, is a matter of community stru"
2021.naacl-main.385,Q19-1026,0,0.0209345,"ark datasets based on naturally-occurring data distributions. This minimizes our effort in creating benchmarks and minimizes the risk that the benchmark is somehow skewed in a way that omits important phenomena. However, this is often not viable. For tasks like reading comprehension or natural language inference that require multiple related texts (such as a passage and a question) as input, there is often no natural distribution that efficiently isolates the relevant task behaviors. One can find naturally-occurring distributions over questions, like those used to construct Natural Questions (Kwiatkowski et al., 2019), but these will generally be tied to the use contexts of a specific NLP product and will thus be limited by users’ perceptions of the current abilities of that product. Even for single-input tasks like coreference resolution or Cloze, for which any text corpus can be the basis for a benchmark, naturalistic distributions do nothing to separate skills of interest from factual world knowledge and can be overwhelmingly dominated by the latter, making them poor metrics for incremental progress on NLU. Credible existing NLU-oriented benchmarks for such tasks are generally heavily curated (Paperno e"
2021.naacl-main.385,2020.findings-emnlp.311,0,0.0895897,"Missing"
2021.naacl-main.385,2021.ccl-1.108,0,0.0430524,"Missing"
2021.naacl-main.385,P11-1015,0,0.0380055,"researchers overfit on benchmarks”, and “benchmarks can be deceiving” and use these claims to motivate abandoning the IID paradigm in favor of benchmark data that is collected adversarially by asking a broad population of annotators to try to fool some reference neural network model.1 A large and impactful thread of research on natural language understanding (NLU) has focused on improving results on benchmark datasets that feature roughly independent and identically distributed (IID) training, validation, and testing sections, drawn from data that were collected or annotated by crowdsourcing (Maas et al., 2011; Bowman et al., 2015; Rajpurkar et al., 2016; Wang et al., 2019b). Recent methodological progress combined with longstanding issues in crowdsourced data quality has made it so state-of-the-art systems are nearing the maximum achievable values on most of these benchmarks and thus are unlikely to be able to measure further improvements (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of"
2021.naacl-main.385,P19-1334,0,0.112018,"ance at or above that of a majority vote of human crowdworkers. Of the eight tasks for which BERT did poorly enough to leave clear headroom for further progress, all are now effectively saturated (Raffel et al., 2020; He et al., 2020). State-of-the-art performance on the highly popular SQuAD 2 English reading-comprehension leaderboard (Rajpurkar et al., 2018) has long exceeded that of human annotators. Ample evidence has emerged that the systems that have topped these leaderboards can fail dramatically on simple test cases that are meant to test the very skills that the leaderboards focus on (McCoy et al., 2019; Ribeiro et al., 2020). This result makes it clear that our systems have significant room to improve. However, we have no guarantee that our benchmarks will detect these needed improvements when they’re made. Most were collected by crowdsourcing with relatively limited quality control, such that we have no reason to expect that perfect performance on their metrics is achievable or that the benchmark will meaningfully distinguish between systems with superhuman metric performance. While the true upper bound on performance for any task (Bayes error) is not measurable, the fact that our systems"
2021.naacl-main.385,C18-1198,0,0.0233999,"cases we focus on here. Setting aside the logistical challenges of creating sufficiently large and diverse datasets by expert labor alone, expert authorship generally gives members of the research community direct, fine-grained control over the data on which their systems will be evaluated. Intentionally or unintentionally, this can produce data that is oriented toward linguistic phenomena that are widely studied and widely known to be important to the task at hand. While this can be helpful when building diagnostic datasets that focus on specific types of model failure (Cooper et al., 1996; Naik et al., 2018; Wang et al., 2019b), it is counterproductive when our goal is to build a broad-coverage benchmark dataset to set priorities and guide progress toward the solution of some task. Dunietz et al. (2020) and Sugawara et al. (2020a) work around this issue by leaning on taxonomies of required phenomena from outside NLP. This is a direction worth pursuing, but it is not clear that appropriate taxonomies will be available for most NLU tasks of interest, or that these taxonomies will be broad and thorough enough to be straightforwardly implemented as datasets. Crowdsourcing Most recent benchmarks for"
2021.naacl-main.385,P19-1449,1,0.841262,"errors would be tested will not appear in the evaluation set. One could attempt to do this by, for example, pretraining new models that deliberately avoid any data that was used to pretrain the original adversary model, in order to minimize the degree to which the idiosyncratic mistakes of the new model line up with those of the old one. This incentive can slow progress and contribute to spurious claims of discovery. 2 Background The Problem Performance on popular benchmarks is extremely high, but experts can easily find issues with high-scoring models. The GLUE benchmark (Wang et al., 2019b; Nangia and Bowman, 2019), a compilation of NLU evaluation tasks, has seen performance on its leaderboard approach or exceed human performance on all nine of its tasks. The follow-up SuperGLUE benchmark project (Wang et al., 2019a) solicited dataset submissions from the NLP research community in 2019, but wound up needing to exclude the large majority of the submitted tasks from the leaderboard because the BERT model (Devlin et al., 2019) was already showing performance at or above that of a majority vote of human crowdworkers. Of the eight tasks for which BERT did poorly enough to leave clear headroom for further pro"
2021.naacl-main.385,2020.acl-main.441,0,0.0373234,"s (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843–4855 June 6–11, 2021. ©2021 Association for Computational Linguistics The DynaBench approach falls into the broader category of adversarial filtering (Paperno et al., 2016; Zellers et al., 2018; Nie et al., 2020; Le Bras et al., 2020). Adversarial filtering starts with a pipeline that produces candidate examples for the task, often through crowdsourcing, and then constructs a dataset by selecting those examples from the pipeline where one or more machine learning models fails to predict the correct label. This approach is appealing in that it guarantees that, at least in the short term, existing approaches to dataset construction can be patched to keep producing data that will challenge current systems. However, collecting examples on which current models fail is neither necessary nor sufficient to c"
2021.naacl-main.385,P18-2124,0,0.0291347,"ct (Wang et al., 2019a) solicited dataset submissions from the NLP research community in 2019, but wound up needing to exclude the large majority of the submitted tasks from the leaderboard because the BERT model (Devlin et al., 2019) was already showing performance at or above that of a majority vote of human crowdworkers. Of the eight tasks for which BERT did poorly enough to leave clear headroom for further progress, all are now effectively saturated (Raffel et al., 2020; He et al., 2020). State-of-the-art performance on the highly popular SQuAD 2 English reading-comprehension leaderboard (Rajpurkar et al., 2018) has long exceeded that of human annotators. Ample evidence has emerged that the systems that have topped these leaderboards can fail dramatically on simple test cases that are meant to test the very skills that the leaderboards focus on (McCoy et al., 2019; Ribeiro et al., 2020). This result makes it clear that our systems have significant room to improve. However, we have no guarantee that our benchmarks will detect these needed improvements when they’re made. Most were collected by crowdsourcing with relatively limited quality control, such that we have no reason to expect that perfect perf"
2021.naacl-main.385,P19-1459,0,0.0202403,"ine if a benchmark presents a valid measure of model ability. Minimally, though, it requires the following: 4845 • An evaluation dataset should reflect the full range of linguistic variation—including words and higher-level constructions—that is used in the relevant domain, context, and language variety. • An evaluation dataset should have a plausible means by which it tests all of the languagerelated behaviors that we expect the model to show in the context of the task. • An evaluation dataset should be sufficiently free of annotation artifacts (as in Si et al., 2019; Sugawara et al., 2020b; Niven and Kao, 2019) that a system cannot reach near-human levels of performance by any means other than demonstrating the required language-related behaviors. If a benchmark fully meets this challenge, we should expect any clear improvement on the benchmark to translate to similar improvements on any other valid and reasonable evaluation data for the same task and language domain.4 The rest of this section surveys common paradigms for constructing a benchmark dataset, and points to reasons that none offers a straightforward way to satisfy this criterion: Naturally-Occurring Examples It is intuitively appealing t"
2021.naacl-main.385,D16-1264,0,0.0679954,"“benchmarks can be deceiving” and use these claims to motivate abandoning the IID paradigm in favor of benchmark data that is collected adversarially by asking a broad population of annotators to try to fool some reference neural network model.1 A large and impactful thread of research on natural language understanding (NLU) has focused on improving results on benchmark datasets that feature roughly independent and identically distributed (IID) training, validation, and testing sections, drawn from data that were collected or annotated by crowdsourcing (Maas et al., 2011; Bowman et al., 2015; Rajpurkar et al., 2016; Wang et al., 2019b). Recent methodological progress combined with longstanding issues in crowdsourced data quality has made it so state-of-the-art systems are nearing the maximum achievable values on most of these benchmarks and thus are unlikely to be able to measure further improvements (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of the 2021 Conference of the North American Cha"
2021.naacl-main.385,P16-1144,0,0.140276,"ly to be able to measure further improvements (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843–4855 June 6–11, 2021. ©2021 Association for Computational Linguistics The DynaBench approach falls into the broader category of adversarial filtering (Paperno et al., 2016; Zellers et al., 2018; Nie et al., 2020; Le Bras et al., 2020). Adversarial filtering starts with a pipeline that produces candidate examples for the task, often through crowdsourcing, and then constructs a dataset by selecting those examples from the pipeline where one or more machine learning models fails to predict the correct label. This approach is appealing in that it guarantees that, at least in the short term, existing approaches to dataset construction can be patched to keep producing data that will challenge current systems. However, collecting examples on which current models fail"
2021.naacl-main.385,2020.acl-main.442,0,0.0744864,"data that were collected or annotated by crowdsourcing (Maas et al., 2011; Bowman et al., 2015; Rajpurkar et al., 2016; Wang et al., 2019b). Recent methodological progress combined with longstanding issues in crowdsourced data quality has made it so state-of-the-art systems are nearing the maximum achievable values on most of these benchmarks and thus are unlikely to be able to measure further improvements (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843–4855 June 6–11, 2021. ©2021 Association for Computational Linguistics The DynaBench approach falls into the broader category of adversarial filtering (Paperno et al., 2016; Zellers et al., 2018; Nie et al., 2020; Le Bras et al., 2020). Adversarial filtering starts with a pipeline that produces candidate examples for the task, often through crowdsourcing, and then constructs a dataset by selecting those ex"
2021.naacl-main.385,Q19-1043,0,0.0444812,"Missing"
2021.naacl-main.385,N19-1176,0,0.0280315,"Missing"
2021.naacl-main.385,W18-5441,0,0.0490459,"Missing"
2021.naacl-main.385,2020.acl-main.467,1,0.880003,"Missing"
2021.naacl-main.385,W17-1609,0,0.0690728,"Missing"
2021.naacl-main.385,N18-2002,0,0.0423294,"Missing"
2021.naacl-main.385,2021.eacl-main.137,0,0.0678168,"Missing"
2021.naacl-main.385,L18-1239,0,0.0612476,"Missing"
2021.naacl-main.385,2020.aacl-main.68,1,0.762935,"alidity will be difficult. 4.2 Improving Handling of Annotation Errors and Disagreements annotator judgments per evaluation example. 4.3 Improving Statistical Power In principle, achieving adequate statistical power is straightforward: we simply estimate the number of examples required to reach the desired statistical power for any plausible short-to-medium term system evaluation for the task, and collect that number of examples. In practice, however, costs can become prohibitive. For a relatively simple task like NLI, labeling an existing example likely requires a bare minimum of 45 seconds (Vania et al., 2020), and creating a new example requires at least one minute (Bowman et al., 2020). Even if we use these very optimistic numbers to estimate annotation speed, a ten-wayannotated dataset of 500,000 examples will still cost over $1 million at a $15/hr pay rate.6 Recruiting more experienced annotators or encouraging annotators to work more carefully could increase this figure dramatically. While such an amount of money is not completely out of reach in a wellfunded field like NLP,7 investments of this kind will inevitably be rare enough that they help reinforce the field’s concentration of data and"
2021.naacl-main.385,Q18-1042,0,0.0596604,"Missing"
2021.naacl-main.385,D18-1009,0,0.0248973,"re further improvements (Devlin et al., 2019; Raffel et al., 2020). At the same time, these apparently high-performing systems have serious known issues and have not achieved human-level competence at their tasks (Ribeiro et al., 2020). 4843 1 https://dynabench.org/about Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843–4855 June 6–11, 2021. ©2021 Association for Computational Linguistics The DynaBench approach falls into the broader category of adversarial filtering (Paperno et al., 2016; Zellers et al., 2018; Nie et al., 2020; Le Bras et al., 2020). Adversarial filtering starts with a pipeline that produces candidate examples for the task, often through crowdsourcing, and then constructs a dataset by selecting those examples from the pipeline where one or more machine learning models fails to predict the correct label. This approach is appealing in that it guarantees that, at least in the short term, existing approaches to dataset construction can be patched to keep producing data that will challenge current systems. However, collecting examples on which current models fail is neither necessary n"
D15-1075,H05-1079,0,0.0281669,"rease in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 ‡ Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground 632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. A man inspects the uniform of a figure in some East As"
D15-1075,W15-4002,1,0.42771,"all three neural network models, suggesting that research into significantly higher capacity versions of these models would be productive. Figure 3: The neural network classification architecture: for each sentence embedding model evaluated in Tables 6 and 7, two identical copies of the model are run with the two sentences as input, and their outputs are used as the two 100d inputs shown here. sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level. Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the mode"
D15-1075,S14-2001,0,0.0154894,"ent, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then"
D15-1075,W04-3205,0,0.0156665,"Missing"
D15-1075,W03-0906,0,0.156506,"Missing"
D15-1075,marelli-etal-2014-sick,0,0.145481,"ent, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then"
D15-1075,P08-1118,1,0.178349,"Missing"
D15-1075,P15-2070,0,0.23443,"Missing"
D15-1075,D14-1162,1,0.134204,"e concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sen3.4 Analysis and dis"
D15-1075,W07-1401,0,0.11712,"ts such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007). We report results in Table 4. Each of the models Partition We distribute the corpus with a prespecified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated. Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of"
D15-1075,D13-1170,1,0.072644,"Missing"
D15-1075,P03-1054,1,0.0911655,"Missing"
D15-1075,S14-2055,0,0.0386092,"Missing"
D15-1075,W14-1610,0,0.0526904,"ifiers, and neural network-based models. We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art. 2 plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b)."
D15-1075,P12-2018,1,0.254878,", including models lacking crossbigram features (Feature 6), and lacking all lexical features (Features 4–6). We report results both on the test set and the training set to judge overfitting. for removing all lexicalized features. On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features. The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification. A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis. It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model. Lexicalized Classifier Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these"
D15-1075,W09-3714,1,0.152106,"rs to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 ‡ Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground 632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. A man inspects the uniform of a figure in some East Asian country. contradiction The man is sleeping An o"
D15-1075,W07-1406,0,0.068274,"Missing"
D15-1075,P14-5008,0,0.0402786,"eate the sentence pair. A gold label reflects a consensus of three votes from among the author and the four annotators. fluent, correctly spelled English, with a mix of full sentences and caption-style noun phrase fragments, though punctuation and capitalization are often omitted. The corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions. It can be downloaded at: nlp.stanford.edu/projects/snli/ 3.1 Excitement Open Platform models The first class of models is from the Excitement Open Platform (EOP, Pad´o et al. 2014; Magnini et al. 2014)—an open source platform for RTE research. EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by run"
D15-1075,H89-1033,0,0.539141,"and the lexicalized model show similar performance when trained on the current full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets. We were struck by the speed with which the lexicalized classifier outperforms its unlexicalized 638 73.95 76.78 78.22 Unlexicalized Lexicalized this kind of inference through lexical cues can lead them astray. Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand. Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner"
D15-1075,Q14-1006,0,0.347432,"ese are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool 633 York and A tourist visited the city. Assuming coreference between New York and the city justifies labeling the pair as an entailment, but without that assumption the city could be taken to refer to a specific unknown city, leaving the pair neutral. This kind of indeterminacy of label can"
D18-1035,P17-1176,1,0.850225,"the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pair in the training d"
D18-1035,P16-2007,0,0.0243728,"ves are all well correlated with each other, training with different objectives do not differ dramatically. 5 Figure 4: (a) The effect of beam size on the IWSLT16 De-En validation with Transformer and (b) the effect of the training corpus composition in the same setting. para: parallel corpus; full: all 35 beam search outputs; thd: beam search outputs that score higher than the base model’s greedy decoding output; top1: beam search output with the highest bleu score; comb.: top1+para. 0.0 corresponds to 33.04 BLEU. machine translation (SMT) (Chiang, 2012; Gao and He, 2013; Auli and Gao, 2014; Dakwale and Monz, 2016). Gao and He (2013) integrate a recurrent neural network language model as an additional feature into a trained phrase-based SMT system and train it by maximizing the expected BLEU on k-best list from the underlying model. Our work revisits a similar idea in the context trainable greedy decoding for neural MT. Related Work Data Distillation Our work is directly inspired by work on knowledge distillation, which uses a similar pseudo-parallel corpus strategy, but aims at training a compact model to approximate the function learned by a larger model or an ensemble of models (Hinton et al., 2015)."
D18-1035,D18-1549,0,0.0224809,"onjecture that this gives the decoder more flexibility with which to guide decoding. In cases where model throughput is less important, our method can also be combined with beam search at test time to yield results somewhat better than either could achieve alone. Table 2 shows the result when combining our method with beam search. RNN We use OpenNMT-py (Klein et al., 2017)6 to implement our model. It is composed of an encoder with two-layer bidirectional RNN, and a decoder with another twolayer RNN. We refer to OpenNMT’s default setting (rnn size = 500, word vec size = 500) and the setting in Artetxe et al. (2018) (rnn size = 600, word vec size = 300), and choose similar hyper-parameters: rnn size = 500, word vec size = 300 for IWSLT16 and rnn size = 600, word vec size = 500 for WMT. We use the input-feeding decoder and global attention with the general alignment function (Luong et al., 2015). 7 6 Results and Analysis 8 https://github.com/OpenNMT/OpenNMT-py 384 https://github.com/facebookresearch/fairseq-py https://github.com/salesforce/nonauto-nmt src ref Am Vormittag wollte auch die Arbeitsgruppe Migration und Integration ihre Beratungen fortsetzen . During the morning , the Migration and Integration"
D18-1035,P14-2023,0,0.0547932,"BPE; Sennrich et al., 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx,"
D18-1035,N13-1048,0,0.120542,"te-pair encoding (BPE; Sennrich et al., 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More sp"
D18-1035,D17-1210,1,0.839035,"Missing"
D18-1035,P02-1040,0,0.104736,"represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pair in the training data and Z = {z1 , ..., zk } be the kbest list from beam search on the pretrained NMT model, where k is the beam size. We define the objective score of the translation z w.r.t. the goldstandard translation y according to a target metric such as BLEU (Papineni et al., 2002), NIST 4 https://github.com/jhclark/multeval https://s3.amazonaws.com/fairseqpy/models/wmt14.v2.en-de.fconv-py.tar.bz2 5 383 greedy BLEU↑ beam4 23.57 27.44 27.15 24.90 28.80 28.74 23.59 28.74 28.36 12.45 15.43 13.76 13.22 16.86 14.61 13.02 17.17 14.49 23.08 27.52 26.44 24.62 28.79 27.31 24.54 28.56 26.96 tg greedy BLEU↑ beam4 tg greedy tok/s↑ beam4 tg 32.5 64.0 26.5 45.7 124.0 51.2 31.2 11.7 12.8 43.8 16.9 27.9 22.4 9.1 12.2 32.5 13.6 26.1 En → De 62.8 191.1 63.9 45.0 87.2 31.0 60.4 167.5 59.8 20.05 22.88 23.87 21.11 24.02 25.03 19.88 24.42 25.46 48.1 136.5 57.9 En → Fi 51.5 24.8 31.4 33.1 11."
D18-1035,D17-1227,0,0.0157199,"ecoder—while beam search requires an equivalent of k such runs, as well as substantial additional overhead for data management. However, beam search often leads to substantial improvement over greedy decoding. For example, Ranzato et al. (2015) report that beam search (with k = 10) gives a 2.2 BLEU improvement in translation and a 3.5 ROUGE-2 improvement in summarization over greedy decoding. Various approaches have been explored recently to improve beam search by improving the method by which candidate sequences are scored (Li et al., 2016; Shu and Nakayama, 2017), the termination criterion (Huang et al., 2017), or the search function itself (Li et al., 2017). In contrast, Gu et al. (2017) have tried to directly improve greedy decoding to decode for an arbitrary decoding objective. They add a small actor network to the decoder and train it with a version of policy gradient to optimize sequence objectives like BLEU. However, they report that they are seriously limited by the instability of this approach to training. In this paper, we propose a procedure to modify a trained decoder to allow it to generate text greedily with the level of quality (according to metrics like BLEU) that would otherwise req"
D18-1035,D16-1139,0,0.150165,", 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pai"
D18-1035,D15-1044,0,0.0503676,"like BLEU. Our method is inspired by earlier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system. 1 Introduction Neural network sequence decoders yield stateof-the-art results for many text generation tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Dehghani et al., 2018), text summarization (Rush et al., 2015; Ranzato et al., 2015; See et al., 2017; Paulus et al., 2017) and image captioning (Vinyals et al., 2015; Xu et al., 2015). These decoders generate tokens from left to right, at each step giving a distribution over possible next tokens, conditioned on both the input and all the tokens generated so far. However, since the space of all possible output sequences is infinite and grows exponentially with sequence length, heuristic search methods such as greedy decod380 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 380–390 c Brussels, Belgium, October"
D18-1035,P17-4012,0,0.039023,"ose little or no performance, and doing so yields an increase in decoding efficiency, even accounting for the small overhead added by the actor. Among the three architectures, ConvS2S—the one with the most and largest layers—performs best. We conjecture that this gives the decoder more flexibility with which to guide decoding. In cases where model throughput is less important, our method can also be combined with beam search at test time to yield results somewhat better than either could achieve alone. Table 2 shows the result when combining our method with beam search. RNN We use OpenNMT-py (Klein et al., 2017)6 to implement our model. It is composed of an encoder with two-layer bidirectional RNN, and a decoder with another twolayer RNN. We refer to OpenNMT’s default setting (rnn size = 500, word vec size = 500) and the setting in Artetxe et al. (2018) (rnn size = 600, word vec size = 300), and choose similar hyper-parameters: rnn size = 500, word vec size = 300 for IWSLT16 and rnn size = 600, word vec size = 500 for WMT. We use the input-feeding decoder and global attention with the general alignment function (Luong et al., 2015). 7 6 Results and Analysis 8 https://github.com/OpenNMT/OpenNMT-py 384"
D18-1035,P17-1099,0,0.0398925,"ier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system. 1 Introduction Neural network sequence decoders yield stateof-the-art results for many text generation tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Dehghani et al., 2018), text summarization (Rush et al., 2015; Ranzato et al., 2015; See et al., 2017; Paulus et al., 2017) and image captioning (Vinyals et al., 2015; Xu et al., 2015). These decoders generate tokens from left to right, at each step giving a distribution over possible next tokens, conditioned on both the input and all the tokens generated so far. However, since the space of all possible output sequences is infinite and grows exponentially with sequence length, heuristic search methods such as greedy decod380 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 380–390 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association"
D18-1035,W17-3204,0,0.0211464,"P (y|x; θ) = T Y P (yt |y&lt;t , x; θ), Beam Search Beam search decodes from left to right, and maintains k > 1 hypotheses at each step. At each step t, beam search considers all possible next tokens conditioned on the current hypotheses, the overall highest Q and picks the k with ˆ When all the hyscores tt0 =1 P (yt0 |y&lt;t0 , x; θ). potheses are complete (they end in an end-of-thesentence symbol or reach a predetermined length limit), it returns the hypothesis with the highest likelihood. Tuning to find a roughly optimal beam size k can yield improvements in performance with sizes as high as 30 (Koehn and Knowles, 2017; Britz et al., 2017). However, the complexity of beam search grows linearly in beam size, with high constant terms, making it undesirable in some applications where latency is important, such as in on-device real-time translation. (1) t=1 where θ is the set of model parameters. Given a parallel corpus Dx,y of source–target sentence pairs, the neural machine translation model can be trained by maximizing the loglikelihood: ( ) X θˆ = argmax log P (y|x; θ) . (2) θ 2.2 NPAD Noisy parallel approximate decoding (NPAD; Cho, 2016) is a parallel decoding algorithm that can be used to improve greedy d"
D18-1035,P16-1162,0,0.0524523,"g outputs under greedy or small-beam decoding. 4 Experiments 4.1 Setting We evaluate our approach on IWSLT16 GermanEnglish, WMT15 Finnish-English, and WMT14 De-En translation in both directions with three strong translation model architectures. For IWSLT16, we use tst2013 and tst2014 for validation and testing, respectively. For WMT15, we use newstest2013 and newstest2015 for validation and testing, respectively. For WMT14, we use newstest2013 and newstest2014 for validation and testing, respectively. All the data are tokenized and segmented into subword symbols using byte-pair encoding (BPE; Sennrich et al., 2016) to restrict the size of the vocabulary. Our primary evaluations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and"
D18-1035,P16-1159,0,0.0256975,"Zhang et al. (2017) propose a new strategy to generate a pseudo-corpus, namely, fast sequenceinterpolation based on the greedy output of the teacher model and the parallel corpus. Freitag et al. (2017) extend knowledge distillation on an ensemble and oracle BLEU teacher model. However, all these approaches require the expensive procedure of retraining the full student network. Decoding for Multiple Objectives Several works have proposed to incorporate different decoding objectives into training. Ranzato et al. (2015) and Bahdanau et al. (2016) use reinforcement learning to achieve this goal. Shen et al. (2016) and Norouzi et al. (2016) train the model by defining an objective-dependent loss function. Wiseman and Rush (2016) propose a learning algorithm tailored for beam search. Unlike these works that optimize the entire model, Li et al. (2017) introduce an additional network that predicts an arbitrary decoding objective given a source sentence and a prefix of translation. This prediction is used as an auxiliary score in beam search. All of these methods focus primarily on improving beam search results, rather than those with greedy decoding. Pseudo-Parallel Corpora in Statistical MT Pseudo-paralle"
D18-1035,2006.amta-papers.25,0,0.0353439,"the architecture of the actor network. zt = σ([ht , et ]W i + bi ), at = tanh(zt W o + bo ), (5) the ff2 actor is computed as z1t = σ([ht , et ]W i + bi ), z2t = σ(z1t W z + bz ), at = tanh(z2t W o + bo ), (6) the rnn actor is computed as zt = σ([ht , et ]U z + st−1 W z ), 3 Methods rt = σ([ht , et ]U r + st−1 W r ),  ˜ st = tanh [ht , et ]U h + (st−1 ◦ rt )W h , We propose a method for training a small actor neural network, following the trainable greedy decoding approach of Gu et al. (2017). This actor st = (1 − zt ) ◦ ˜ st + zt ◦ st−1 , at = st U, 382 (7) (Doddington, 2002), negative TER (Snover et al., 2006), or METEOR (Lavie and Denkowski, 2009) ˜ that as O(z, y). Then we choose the sentence z has the highest score to become our new target sentence: and the gate actor is computed as zt = σ([ht , et ]U z ), at = zt ◦ tanh([ht , et ]U ). (8) Once the action at has been computed, the hidden state ht is simply replaced with the updated ˜t : state h ˜ t = f (ht , et ) + at . h (9) ˜ = argmax O(z, y). z (10) z=z1 ,..,zk Figure 1 shows a single step of the actor interacting with the underlying neural decoder of each of the three NMT architectures we use: the RNNbased model of Luong et al. (2015), ConvS"
D18-1035,D16-1137,0,0.0578165,"Missing"
D18-1035,D17-1154,0,0.0616288,"valuations use tokenized and cased BLEU. For METEOR and TER evaluations, we use multeval4 with tokenized and case-insensitive scoring. All the underlying models are trained from scratch, except for ConvS2S WMT14 EnglishGerman translation, for which we use the trained model (as well as training data) provided by Gehring et al. (2017).5 Training To overcome the severe instability reported by Gu et al. (2017), we introduce the use of a pseudo-parallel corpus generated from the underlying NMT model (Gao and He, 2013; Auli and Gao, 2014; Kim and Rush, 2016; Chen et al., 2017; Freitag et al., 2017; Zhang et al., 2017) for actor training. This corpus includes pairs that both (i) have a high model likelihood, so that we can coerce the model to generate them without much additional training or many new parameters and, (ii) represent high-quality translations, measured according to a target metric like BLEU. We do this by generating sentences from the original unaugmented model with large-beam beam search and selecting the best sentence from the resulting kbest list according to the decoding objective. More specifically, let hx, yi be a sentence pair in the training data and Z = {z1 , ..., zk } be the kbest li"
D18-1035,D17-1151,0,\N,Missing
D18-1269,L18-1548,0,0.03349,"dation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 and 2018 corpora (Tiedemann, 2012) and LDC2010T21, LDC2010T23 LDC corpora, and obtained a total of 64k parallel sentences. For Swahili, we were 2481 en fr es de el bg ru tr ar vi th z"
D18-1269,P17-1042,0,0.0297003,"in language understanding has been at the word level. Several approaches have been proposed to learn cross-lingual word representations, i.e., word representations where translations are close in the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al., 2018). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-words (CBOW). Although naïve, this method often provides a strong baseline. More sophisticated approaches—such as the unsupervised SkipThought model of Kiros et al. (2015) that extends the skip-gram mo"
D18-1269,D18-1549,1,0.811153,"ings. 4.2.1 Aligning Word Embeddings Multilingual word embeddings are an efficient way to transfer knowledge from one language to another. For instance, Zhang et al. (2016) show that cross-lingual embeddings can be used to extend an English part-of-speech tagger to the cross-lingual setting, and Xiao and Guo (2014) achieve similar results in dependency parsing. Cross-lingual embeddings also provide an efficient mechanism to bootstrap neural machine translation (NMT) systems for low-resource language pairs, which is critical in the case of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018). In that case, the use cross-lingual embeddings directly helps the alignment of sentence-level encoders. Cross-lingual embeddings can be generated efficiently using a very small amount of supervision. By using a small parallel dictionary with n = 5000 word pairs, it is possible to learn a linear mapping to minimize W ? = argmin kW X − Y kF = U V T , W ∈Od (R) where d is the dimension of the embeddings, and X and Y are two matrices of shape (d, n) that correspond to the aligned word embeddings that appear in the parallel dictionary, Od (R) is the group of orthogonal matrices of dimension d, an"
D18-1269,D15-1075,1,0.919597,"ross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference. Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examp"
D18-1269,D18-2029,0,0.0977534,"ding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs. These languages span several language families, and with the inclusion of Swahili and Urdu, include two lower-resource languages as well. Because of its"
D18-1269,L18-1269,1,0.779888,"ng data (X - BILSTM). The former evaluates transfer learning while the latter evaluates NLIspecific encoders trained on in-domain data. Both approaches use the same alignment loss for aligning sentence embedding spaces from multiple languages which is present below. We consider two ways of extracting feature vectors from the BiLSTM: either using the initial and final hidden states (Sutskever et al., 2014), or using the element-wise max over all states (Collobert and Weston, 2008). The first approach is commonly used as a strong baseline for monolingual sentence embeddings (Arora et al., 2017; Conneau and Kiela, 2018; Gouews et al., 2014). Concretely, we consider the English fastText word embedding space as being fixed, and fine-tune embeddings in other languages so that the average of the word vectors in a sentence is close to the average of the word vectors in its English translation. The second approach consists in learning an English sentence encoder on the MultiNLI training data along with an encoder on the target language, with the objective that the representations of two translations are nearby in the embedding space. In both approaches, an English encoder is fixed, and we train target language en"
D18-1269,D17-1070,1,0.920978,"ractical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs. These languages span several language families, and with the inclusion of Swahili and Urdu, include two lowe"
D18-1269,E14-1049,0,0.060128,"ur baselines in Section 4, and finally present and discuss results in Section 5. 2 Related Work Multilingual Word Embeddings Much of the work on multilinguality in language understanding has been at the word level. Several approaches have been proposed to learn cross-lingual word representations, i.e., word representations where translations are close in the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al., 2018). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-words (CBOW). Although naïve, this method"
D18-1269,P11-1134,0,0.0297861,"e are many ways to aggregate sentence embeddings, the comparison between different sentence embeddings is difficult. Moreover, the distribution of classes in the Reuters corpus is highly unbalanced, and the dataset does not provide a development set in the target language, further complicating experimental comparisons. In addition to the Reuters corpus, Cer et al. (2017) propose sentence-level multilingual training and evaluation datasets for semantic textual similarity in four languages. There have also been efforts to build multilingual RTE datasets, either through translating English data (Mehdad et al., 2011), or annotating sentences from a parallel corpora (Negri et al., 2011). More recently, Agic and Schluter (2017) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Among all these benchmarks, XNLI is the first large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their E"
D18-1269,L18-1550,0,0.0260448,"r mapping to minimize W ? = argmin kW X − Y kF = U V T , W ∈Od (R) where d is the dimension of the embeddings, and X and Y are two matrices of shape (d, n) that correspond to the aligned word embeddings that appear in the parallel dictionary, Od (R) is the group of orthogonal matrices of dimension d, and U and V are obtained from the singular value decomposition (SVD) of Y X T : U ΣV T = SVD(Y X T ). Xing et al. (2015) show that enforcing the orthogonality constraint on the linear mapping leads to better results on the word translation task. In this paper, we use common-crawl word embeddings (Grave et al., 2018) aligned with the MUSE library of Conneau et al. (2018). 4.2.2 Universal Multilingual Sentence Embeddings Most of the successful recent approaches for learning universal sentence representations have relied on English (Kiros et al., 2015; Arora et al., 2017; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). While notable recent approaches have considered building a shared sentence encoder for multiple languages using publicly available parallel corpora (Johnson et al., 2016; Schwenk et al., 2017; España-Bonet et al., 2017), the lack of a large-scale, sentence-level semantic ev"
D18-1269,N18-2017,1,0.86915,"Missing"
D18-1269,P14-1006,0,0.0217936,"the model are used as contextualized word vectors (Peters et al., 2018), or when the full model is finetuned on transfer tasks (Radford et al.; Howard and Ruder, 2018). Multilingual Sentence Representations There has been some effort on developing multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt at compositionality. Zhou et al. (2016) learn bilingual document representations by minimizing the Euclidean distance between document representations and their translations. Cross-lingual Evaluation Benchmarks The lack of evaluation benchmark has hindered the development of such multilingual representations. Most previous approaches use the Reuters crosslingual document classification corpus Klem"
D18-1269,C12-1089,0,0.222208,"e) and the resulting system can perform the task only in the training language. In practice, however, systems used in major international products need to handle inputs in many languages. In these settings, it is nearly impossible to annotate data in all languages that a system might encounter during operation. A scalable way to build multilingual systems is through cross-lingual language understanding (XLU), in which a system is trained primarily on data in one language and evaluated on data in others. While XLU shows promising results for tasks such as cross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference. Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a mil"
D18-1269,P14-2037,0,0.0157812,"ction 3. We describe our baselines in Section 4, and finally present and discuss results in Section 5. 2 Related Work Multilingual Word Embeddings Much of the work on multilinguality in language understanding has been at the word level. Several approaches have been proposed to learn cross-lingual word representations, i.e., word representations where translations are close in the embedding space. Many of these methods require some form of supervision (typically in the form of a small bilingual lexicon) to align two sets of source and target embeddings to the same space (Mikolov et al., 2013a; Kociský et al., 2014; Faruqui and Dyer, 2014; Ammar et al., 2016). More recent studies have showed that cross-lingual word embeddings can be generated with no supervision whatsoever (Artetxe et al., 2017; Conneau et al., 2018). Sentence Representation Learning Many approaches have been proposed to extend word embeddings to sentence or paragraph representations (Le and Mikolov, 2014; Wieting et al., 2016; Arora et al., 2017). The most straightforward way to generate sentence embeddings is to consider an average or weighted average of word representations, usually referred to as continuous bag-of-words (CBOW). Alth"
D18-1269,2005.mtsummit-papers.11,0,0.0515074,"a rate of 0.1. For X-BiLSTMs, we perform model selection on the XNLI validation set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 and 2018 corpora (Tiedemann, 2012) and LD"
D18-1269,D11-1062,0,0.0230347,"n different sentence embeddings is difficult. Moreover, the distribution of classes in the Reuters corpus is highly unbalanced, and the dataset does not provide a development set in the target language, further complicating experimental comparisons. In addition to the Reuters corpus, Cer et al. (2017) propose sentence-level multilingual training and evaluation datasets for semantic textual similarity in four languages. There have also been efforts to build multilingual RTE datasets, either through translating English data (Mehdad et al., 2011), or annotating sentences from a parallel corpora (Negri et al., 2011). More recently, Agic and Schluter (2017) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Among all these benchmarks, XNLI is the first large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their English translations, and conclude that most of them come from cultural"
D18-1269,N18-1202,0,0.465016,"le natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and these have been widely used to evaluate neural network architectures and training strategies (Rocktäschel et al., 2016; Gong et al., 2018; Peters et al., 2018; Wang et al., 2018), as well as to train effective, reusable sentence representations (Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). In this work, we introduce a benchmark that we call the Cross-lingual Natural Language Inference corpus, or XNLI, by extending these NLI corpora to 15 languages. XNLI consists of 7500 human-annotated development and test examples in NLI three-way classification format in English, French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu, making a total of 112,500 annotated pairs."
D18-1269,W15-1512,0,0.0419802,"Missing"
D18-1269,S18-2023,0,0.0414974,"Missing"
D18-1269,L18-1560,1,0.795078,"em can perform the task only in the training language. In practice, however, systems used in major international products need to handle inputs in many languages. In these settings, it is nearly impossible to annotate data in all languages that a system might encounter during operation. A scalable way to build multilingual systems is through cross-lingual language understanding (XLU), in which a system is trained primarily on data in one language and evaluated on data in others. While XLU shows promising results for tasks such as cross-lingual document classification (Klementiev et al., 2012; Schwenk and Li, 2018), there are very few, if any, XLU benchmarks for more difficult language understanding tasks like natural language inference. Large-scale natural language inference (NLI), also known as recognizing textual entailment (RTE), has emerged as a practical test bed for work on sentence understanding. In NLI, a system is tasked with reading two sentences and determining whether one entails the other, contradicts it, or neither (neutral). Recent crowdsourced annotation efforts have yielded datasets for NLI in English (Bowman et al., 2015; Williams et al., 2017) with nearly a million examples, and thes"
D18-1269,W17-2619,1,0.925242,"nneau et al., 2017; Subramanian et al., 2018), some recent developments have shown that pretrained language models can also transfer very well, either when the hidden states of the model are used as contextualized word vectors (Peters et al., 2018), or when the full model is finetuned on transfer tasks (Radford et al.; Howard and Ruder, 2018). Multilingual Sentence Representations There has been some effort on developing multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt at compositionality. Zhou et al. (2016) learn bilingual document representations by minimizing the Euclidean distance between document representations and their translations. Cross-lingual Evaluation Benchmarks The lac"
D18-1269,D16-1217,0,0.0166198,"Schluter (2017) provide a corpus, that is very complementary to our work, of human translations for 1332 pairs of the SNLI data into Arabic, French, Russian, and Spanish. Among all these benchmarks, XNLI is the first large-scale corpus for evaluating sentence-level representations on that many languages. In practice, cross-lingual sentence understanding goes beyond translation. For instance, Mohammad et al. (2016) analyze the differences in human sentiment annotations of Arabic sentences and their English translations, and conclude that most of them come from cultural differences. Similarly, Smith et al. (2016) show that most of the degradation in performance when applying a classification model trained in English to Spanish data translated to English is due to cultural differences. One of the limitations of the XNLI corpus is that it does not capture these differences, since it was obtained by translation. We see the XNLI evaluation as a necessary step for multilingual NLP before tackling the even more complex problem of domain-adaptation that occurs when handling this the change in style from one language to another. 3 The XNLI Corpus Because the test portion of the Multi-Genre NLI data is private"
D18-1269,L16-1561,0,0.025266,"er of 128 hidden units, regularized with dropout (Srivastava et al., 2014) at a rate of 0.1. For X-BiLSTMs, we perform model selection on the XNLI validation set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 20"
D18-1269,tiedemann-2012-parallel,0,0.0411264,"set in each target language. For X-CBOW, we keep a validation set of parallel sentences to evaluate our alignment loss. The alignment loss requires a parallel dataset of sentences for each pair of languages, which we describe next. 5.2 Parallel Datasets We use publicly available parallel datasets to learn the alignment between English and target encoders. For French, Spanish, Russian, Arabic and Chinese, we use the United Nation corpora (Ziemski et al., 2016), for German, Greek and Bulgarian, the Europarl corpora (Koehn, 2005), for Turkish, Vietnamese and Thai, the OpenSubtitles 2018 corpus (Tiedemann, 2012), and for Hindi, the IIT Bombay corpus (Anoop et al., 2018). For all the above language pairs, we were able to gather more than 500,000 parallel sentences, and we set the maximum number of parallel sentences to 2 million. For the lower-resource languages Urdu and Swahili, the number of parallel sentences is an order of magnitude smaller than for the other languages we consider. For Urdu, we used the Bible and Quran transcriptions (Tiedemann, 2012), the OpenSubtitles 2016 and 2018 corpora (Tiedemann, 2012) and LDC2010T21, LDC2010T23 LDC corpora, and obtained a total of 64k parallel sentences. F"
D18-1269,L18-1239,0,0.0441002,"Missing"
D18-1269,W18-5446,1,0.847931,"Missing"
D18-1269,W14-1613,0,0.0296834,"ut a vector of fixed size as a sentence representation. While previous work shows that performance on the NLI task can be improved by using cross-sentence attention between the premise and 2479 hypothesis (Rocktäschel et al., 2016; Gong et al., 2018), we focus on methods with fixed-size sentence embeddings. 4.2.1 Aligning Word Embeddings Multilingual word embeddings are an efficient way to transfer knowledge from one language to another. For instance, Zhang et al. (2016) show that cross-lingual embeddings can be used to extend an English part-of-speech tagger to the cross-lingual setting, and Xiao and Guo (2014) achieve similar results in dependency parsing. Cross-lingual embeddings also provide an efficient mechanism to bootstrap neural machine translation (NMT) systems for low-resource language pairs, which is critical in the case of unsupervised machine translation (Lample et al., 2018; Artetxe et al., 2018). In that case, the use cross-lingual embeddings directly helps the alignment of sentence-level encoders. Cross-lingual embeddings can be generated efficiently using a very small amount of supervision. By using a small parallel dictionary with n = 5000 word pairs, it is possible to learn a line"
D18-1269,N15-1104,0,0.0429287,"ders. Cross-lingual embeddings can be generated efficiently using a very small amount of supervision. By using a small parallel dictionary with n = 5000 word pairs, it is possible to learn a linear mapping to minimize W ? = argmin kW X − Y kF = U V T , W ∈Od (R) where d is the dimension of the embeddings, and X and Y are two matrices of shape (d, n) that correspond to the aligned word embeddings that appear in the parallel dictionary, Od (R) is the group of orthogonal matrices of dimension d, and U and V are obtained from the singular value decomposition (SVD) of Y X T : U ΣV T = SVD(Y X T ). Xing et al. (2015) show that enforcing the orthogonality constraint on the linear mapping leads to better results on the word translation task. In this paper, we use common-crawl word embeddings (Grave et al., 2018) aligned with the MUSE library of Conneau et al. (2018). 4.2.2 Universal Multilingual Sentence Embeddings Most of the successful recent approaches for learning universal sentence representations have relied on English (Kiros et al., 2015; Arora et al., 2017; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018). While notable recent approaches have considered building a shared sentence en"
D18-1269,P16-1133,0,0.0359767,"multilingual sentence embeddings. For example, Chandar et al. (2013) train bilingual autoencoders with the objective of minimizing reconstruction error between two languages. Schwenk et al. (2017) and EspañaBonet et al. (2017) jointly train a sequence-tosequence MT system on multiple languages to learn a shared multilingual sentence embedding space. Hermann and Blunsom (2014) propose a compositional vector model involving unigrams and bigrams to learn document level representations. Pham et al. (2015) directly train embedding representations for sentences with no attempt at compositionality. Zhou et al. (2016) learn bilingual document representations by minimizing the Euclidean distance between document representations and their translations. Cross-lingual Evaluation Benchmarks The lack of evaluation benchmark has hindered the development of such multilingual representations. Most previous approaches use the Reuters crosslingual document classification corpus Klementiev et al. (2012) for evaluation. However, the classification in this corpus is done at document level, and, as there are many ways to aggregate sentence embeddings, the comparison between different sentence embeddings is difficult. Mor"
D18-1544,P06-1109,0,0.348752,"Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. Italics mark results that are worse than the random baseline. Underlining marks the best results from our runs. Results with RL-SPINN and ST-Gumbel are from Williams et al. (2018a), and are evaluated on the full WSJ. We run the model with 5 different random seeds to calculate the average F1. We use the model with the best F1 score to report ADJP, NP, PP, and INTJ. WSJ10 baselines are from Klein and Manning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). As the WSJ10 baselines are trained using additional information such as POS tags and dependency parser, they are not strictly comparable with the latent tree learning results. of WSJ) splits. To compare PRPN to the models studied in Williams et al. (2018a), we also retrain it on AllNLI. As the MultiNLI test set is not publicly available, we follow Williams et al. (2018a) and use the development set for testing. The parsing evaluation code in the original codebase does not support PRPN-LM, and we modify it in our experiments only to add this support. For early stopping, we remove 10"
D18-1544,D15-1075,1,0.844422,"ure and tend to identify noun phrases correctly. ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models on the WSJ test set, full WSJ10,1 and the MultiNLI development set. Our results indicate that PRPN-LM achieves better parsing performance than PRPN-UP on both WSJ and WSJ10 even though PRPN-UP was tuned—at least to some extent—for parsing. Surprisingly, a PRPN-LM model trained on the large out-of-dom"
D18-1544,P04-1015,0,0.214201,"rist offices . The entire Minoan civilization was destroyed by a volcanic eruption . There ’s nothing worth seeing in the tourist offices . The entire Minoan civilization was destroyed by a volcanic eruption . Figure 1: Left Parses from PRPN-LM trained on AllNLI. Right Parses from PRPN-UP trained on AllNLI (stopping criterion: parsing). We can observe that both sets of parses tend to have roughly reasonable high-level structure and tend to identify noun phrases correctly. ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of"
D18-1544,N18-1101,1,0.809114,"greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models on the WSJ test set, full WSJ10,1 and the MultiNLI development set. Our results indicate that PRPN-LM achieves better parsing performance than PRPN-UP on both WSJ and WSJ10 even though PRPN-UP was tuned—at least to some extent—for parsing. Surprisingly, a PRPN-LM model trained on the large out-of-domain AllNLI dataset achieves the best parsing performance on WSJ"
D18-1544,P02-1017,0,0.317857,"e results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction. 1 Introduction and Background Work on grammar induction attempts to find methods for syntactic parsing that do not require expensive and difficult-to-design expertlabeled treebanks for training (Charniak and Carroll, 1992; Klein and Manning, 2002; Smith and Eisner, 2005). Recent work on latent tree learning offers a new family of approaches to the problem (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). Latent tree learning models attempt to induce syntactic structure using the supervision from a downstream NLP task such as textual entailment. Though these models tend to show good task performance, they are often not evaluated using standard parsing metrics, and Williams et al. 3 (2018a) report that the parses they produce tend to be no better than random trees in a standard evaluation on the full Wall Street Journal"
D18-1544,Q16-1037,0,0.0789784,"st offices . The entire Minoan civilization was destroyed by a volcanic eruption . Figure 1: Left Parses from PRPN-LM trained on AllNLI. Right Parses from PRPN-UP trained on AllNLI (stopping criterion: parsing). We can observe that both sets of parses tend to have roughly reasonable high-level structure and tend to identify noun phrases correctly. ing decisions greedily and with no access to any words to the right of the point where each parsing decision must be made (Collins and Roark, 2004); (2) As RNN language models are known to be insufficient for capturing syntax-sensitive dependencies (Linzen et al., 2016), language modeling as the downstream task may not be well-suited to latent tree learning. In this replication we train PRPN on two corpora: The full WSJ, a staple in work on grammar induction, and AllNLI, the concatenation of the Stanford Natural Language Inference Corpus (SNLI; Bowman et al., 2015) and the Multi-Genre NLI Corpus (MultiNLI; Williams et al., 2018b), which is used in other latent tree learning work for its non-syntactic classification labels for the task of textual entailment, and which we include for comparison. We then evaluate the constituency trees produced by these models"
D18-1544,J93-2004,0,0.0624137,"rk on latent tree learning offers a new family of approaches to the problem (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018). Latent tree learning models attempt to induce syntactic structure using the supervision from a downstream NLP task such as textual entailment. Though these models tend to show good task performance, they are often not evaluated using standard parsing metrics, and Williams et al. 3 (2018a) report that the parses they produce tend to be no better than random trees in a standard evaluation on the full Wall Street Journal section of the Penn Treebank (WSJ; Marcus et al., 1993). This paper addresses the Parsing-ReadingPredict Network (PRPN; Shen et al., 2018), which was recently published at ICLR, and which reports near-state-of-the-art results on language modeling and strong results on grammar induction, a first for latent tree models (though they do not use that term). PRPN is built around a substantially novel architecture, and uses convolutional networks with a form of structured attention (Kim et al., 2017) rather than recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011) to evaluate and learn trees while performing straightforward backpropa"
D18-1544,Q18-1019,1,0.714537,"New York University 60 Fifth Avenue New York, NY 10011 Kyunghyun Cho1,2 CIFAR Global Scholar kyunghyun.cho@nyu.edu Samuel R. Bowman1,2,3 bowman@nyu.edu Dept. of Computer Science New York University 60 Fifth Avenue New York, NY 10011 Dept. of Linguistics New York University 10 Washington Place New York, NY 10003 2 Abstract A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report nearstate-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, an"
D19-1286,W18-5426,0,0.0943611,"Missing"
D19-1286,D17-1070,0,0.0302893,"ained-BERT nington et al., 2014) and (ii) BERT (Devlin et al., 2018): we use the cased version of BERT-large model, which works the best for our tasks in pilot experiments. In addition, since recent work (Liu et al., 2019; Stickland and Murray, 2019) has shown that intermediate training on related tasks can meaningfully impact BERT’s performance on downstream tasks, we also explore two additional BERT-based models—(iii) BERT!MNLI: BERT fine-tuned on the Multi-Genre Natural Language Inference corpus (Williams et al., 2018), motivated both by prior work on pretraining sentence encoders on MNLI (Conneau et al., 2017) as well as work showing significant improvements to BERT on downstream semantic tasks (Phang et al., 2018; Bowman et al., 2018) (iv) BERT!CCG: BERT fine-tuned on Combinatory Categorial Grammar Bank corpus (Hockenmaier and Steedman, 2007), motivated by Wilcox et al.’s (2019) finding that structural supervision may improve a LSTM-based sentence encoders knowledge on non-local syntactic dependencies. Training-Evaluation Configurations We are interested in whether sentence representation models learn NPI licensing as a unified property. Can the models generalize from trained environments to previ"
D19-1286,P18-1198,0,0.0467848,"Missing"
D19-1286,W16-2524,0,0.249452,"features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model’s grammatical knowledge in a given domain. 1 Introduction Recent sentence representation models have attained state-of-the-art results on language understanding tasks, but standard methodology for evaluating their knowledge of grammar has been slower to emerge. Recent work evaluating grammatical knowledge of sentence encoders like BERT (Devlin et al., 2018) has employed a variety of methods. For example, Shi et al. (2016), Ettinger et al. (2016), and Tenney et al. (2019) use probing tasks to target a model’s knowledge of particular grammatical features. Marvin and Linzen (2018) and Wilcox et al. (2019) compare language models’ probabilities for pairs of minimally different sentences differing in grammatical acceptability. Linzen et al. (2016), Warstadt et al. (2018), and Kann et al. (2019) use Boolean acceptability judgments inspired by methodologies in generative linguistics. However, we have not yet seen any substantial direct comparison between these methods, and it is not yet clear whether they tend to yield similar conclusions a"
D19-1286,C18-1152,0,0.0326973,"rb agreement, NPI licensing, and reflexive licensing. Another branch of work uses probing tasks in which the objective is to predict the value of a particular linguistic feature given an input sentence. Probing tasks have been used to investigate whether sentence embeddings encode syntactic and surface features such as tense and voice (Shi et al., 2016), sentence length and word content (Adi et al., 2016), or syntactic depth and morphological number (Conneau et al., 2018). Giulianelli et al. (2018) use diagnostic classifiers to track the propagation of information in RNNbased language models. Ettinger et al. (2018) and Dasgupta et al. (2018) use automatic data generation to evaluate compositional reasoning. Tenney et al. (2019) introduce sub-sentence level probing tasks derived from common NLP tasks. 2 Related Work Negative Polarity Items In the theoretical literature on NPIs, proposals have been made to unify the properties of the diverse NPI licensing environments. For example, a popular view states that NPIs are licensed if and only if they occur in downward entailing (DE) environments (Fauconnier, 1975; Ladusaw, 1979), i.e. an environment that licences inferences from sets to subsets.1 For instance,"
D19-1286,J07-3004,0,0.0109451,"land and Murray, 2019) has shown that intermediate training on related tasks can meaningfully impact BERT’s performance on downstream tasks, we also explore two additional BERT-based models—(iii) BERT!MNLI: BERT fine-tuned on the Multi-Genre Natural Language Inference corpus (Williams et al., 2018), motivated both by prior work on pretraining sentence encoders on MNLI (Conneau et al., 2017) as well as work showing significant improvements to BERT on downstream semantic tasks (Phang et al., 2018; Bowman et al., 2018) (iv) BERT!CCG: BERT fine-tuned on Combinatory Categorial Grammar Bank corpus (Hockenmaier and Steedman, 2007), motivated by Wilcox et al.’s (2019) finding that structural supervision may improve a LSTM-based sentence encoders knowledge on non-local syntactic dependencies. Training-Evaluation Configurations We are interested in whether sentence representation models learn NPI licensing as a unified property. Can the models generalize from trained environments to previously unseen environments? To answer these questions, for each NPI environment, we extensively test the performance of the models in the following configurations: (i) CoLA: training on CoLA, evaluating on the environment. (ii) 1 NPI: trai"
D19-1286,W18-5424,0,0.0776662,"ntrast between (1) and (2). (4) a. b. I haven’t been to France. ! I haven’t been to Paris. (DE) I have been to France. 6! I have been to Paris. (not DE) This view does not capture licensing in some environments, for example questions. No theory is yet accepted as identifying the unifying properties of all NPI licensing environments. Within computational linguistics, NPIs are used as a testing ground for neural network models’ grammatical knowledge. Marvin and Linzen (2018) find that LSTM LMs do not systematically prefer sentences with licensed NPIs (1) over sentencew with unlicensed NPIs (2). Jumelet and Hupkes (2018) shows LSTM LMs find a relation between the licensing context and the negative polarity item, and appears to be aware of the scope of this context. Wilcox et al. (2019) use NPIs and filler-gap dependencies, as instances of non-local grammatical dependencies, to probe the effect of supervision with hierarchical structure. They find that structurally-supervised models outperform state-of-the-art sequential LSTM models, showing the importance of structure in learning non-local dependencies like NPI licensing. CoLA We use the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) in our"
D19-1286,W19-0129,1,0.879581,"ethodology for evaluating their knowledge of grammar has been slower to emerge. Recent work evaluating grammatical knowledge of sentence encoders like BERT (Devlin et al., 2018) has employed a variety of methods. For example, Shi et al. (2016), Ettinger et al. (2016), and Tenney et al. (2019) use probing tasks to target a model’s knowledge of particular grammatical features. Marvin and Linzen (2018) and Wilcox et al. (2019) compare language models’ probabilities for pairs of minimally different sentences differing in grammatical acceptability. Linzen et al. (2016), Warstadt et al. (2018), and Kann et al. (2019) use Boolean acceptability judgments inspired by methodologies in generative linguistics. However, we have not yet seen any substantial direct comparison between these methods, and it is not yet clear whether they tend to yield similar conclusions about what a given model knows. We aim to better understand the trade-offs in task choice by comparing different methods inspired by previous work to evaluate sentence understanding models in a single empirical domain. We choose as our case study negative polarity item (NPI) licensing in English, an empirically rich phenomenon widely discussed in the"
D19-1286,Q16-1037,0,0.586775,"ts on language understanding tasks, but standard methodology for evaluating their knowledge of grammar has been slower to emerge. Recent work evaluating grammatical knowledge of sentence encoders like BERT (Devlin et al., 2018) has employed a variety of methods. For example, Shi et al. (2016), Ettinger et al. (2016), and Tenney et al. (2019) use probing tasks to target a model’s knowledge of particular grammatical features. Marvin and Linzen (2018) and Wilcox et al. (2019) compare language models’ probabilities for pairs of minimally different sentences differing in grammatical acceptability. Linzen et al. (2016), Warstadt et al. (2018), and Kann et al. (2019) use Boolean acceptability judgments inspired by methodologies in generative linguistics. However, we have not yet seen any substantial direct comparison between these methods, and it is not yet clear whether they tend to yield similar conclusions about what a given model knows. We aim to better understand the trade-offs in task choice by comparing different methods inspired by previous work to evaluate sentence understanding models in a single empirical domain. We choose as our case study negative polarity item (NPI) licensing in English, an emp"
D19-1286,P19-1441,0,0.0727389,"Missing"
D19-1286,J93-2004,0,0.0654056,"ining on CCG and MNLI does not improve performance, and even lowers performance in some cases. While results from Phang et al. (2018) lead us to hypothesize that intermediate pretraining might help, this is not what we observe on our data. This result is in direct contrast with the results from Wilcox et al. (2019), who find that syntactic pretraining does improve performance in the NPI domain. This difference in findings is likely due to differences in models and training procedure, as their model is an RNN jointly trained on language modeling and parsing over the much smaller Penn Treebank (Marcus et al., 1993). Future studies would benefit from employing a variety of different methodologies for assessing model performance withing a specified domain. In particular, a result showing generally good performance for a model should be regarded as possibly hiding actual differences in performance that a different task would reveal. Similarly, generally poor performance for a model does not necessarily mean that the model does not have systematic knowledge in a given domain; it may be that an easier task would reveal systematicity. 8 Conclusion We have shown that within a well-defined domain of English gra"
D19-1286,D18-1151,0,0.476697,"reveal all relevant aspects of a model’s grammatical knowledge in a given domain. 1 Introduction Recent sentence representation models have attained state-of-the-art results on language understanding tasks, but standard methodology for evaluating their knowledge of grammar has been slower to emerge. Recent work evaluating grammatical knowledge of sentence encoders like BERT (Devlin et al., 2018) has employed a variety of methods. For example, Shi et al. (2016), Ettinger et al. (2016), and Tenney et al. (2019) use probing tasks to target a model’s knowledge of particular grammatical features. Marvin and Linzen (2018) and Wilcox et al. (2019) compare language models’ probabilities for pairs of minimally different sentences differing in grammatical acceptability. Linzen et al. (2016), Warstadt et al. (2018), and Kann et al. (2019) use Boolean acceptability judgments inspired by methodologies in generative linguistics. However, we have not yet seen any substantial direct comparison between these methods, and it is not yet clear whether they tend to yield similar conclusions about what a given model knows. We aim to better understand the trade-offs in task choice by comparing different methods inspired by pre"
D19-1286,N19-1334,0,0.0875266,"of a model’s grammatical knowledge in a given domain. 1 Introduction Recent sentence representation models have attained state-of-the-art results on language understanding tasks, but standard methodology for evaluating their knowledge of grammar has been slower to emerge. Recent work evaluating grammatical knowledge of sentence encoders like BERT (Devlin et al., 2018) has employed a variety of methods. For example, Shi et al. (2016), Ettinger et al. (2016), and Tenney et al. (2019) use probing tasks to target a model’s knowledge of particular grammatical features. Marvin and Linzen (2018) and Wilcox et al. (2019) compare language models’ probabilities for pairs of minimally different sentences differing in grammatical acceptability. Linzen et al. (2016), Warstadt et al. (2018), and Kann et al. (2019) use Boolean acceptability judgments inspired by methodologies in generative linguistics. However, we have not yet seen any substantial direct comparison between these methods, and it is not yet clear whether they tend to yield similar conclusions about what a given model knows. We aim to better understand the trade-offs in task choice by comparing different methods inspired by previous work to evaluate se"
D19-1286,D14-1162,0,0.0928004,"ls reach near-human performance on CoLA. 3 Methods We experiment with five approaches to the evaluation of grammatical knowledge of sentence representation models like BERT using our generated NPI acceptability judgment dataset (§4). Each data sample in the dataset contains a sentence, a Boolean label which indicates whether the sentence is grammatically acceptable or not, and three Boolean meta-data variables (licensor, NPI, and scope; Table 2). We evaluate four model types: BERT-large, BERT with fine-tuning on one of two tasks, and a simple bag-of-words baseline using GloVe word embeddings (Pennington et al., 2014). Boolean Acceptability We test the model’s ability to judge the grammatical acceptability of the sentences in the NPI dataset. Following standards in linguistics, sentences for this task are assumed to be either totally acceptable or totally unacceptable. We fine-tune the sentence representation models to perform these Boolean judgments. For BERT-based sentence representation models, we add a classifier on top of the [CLS] embedding of the last layer. For BoW, we use a max pooling layer followed by an MLP classifier. The performance of the models is measured as Matthews Correlation Coefficien"
D19-1286,D16-1159,0,0.0753389,"knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model’s grammatical knowledge in a given domain. 1 Introduction Recent sentence representation models have attained state-of-the-art results on language understanding tasks, but standard methodology for evaluating their knowledge of grammar has been slower to emerge. Recent work evaluating grammatical knowledge of sentence encoders like BERT (Devlin et al., 2018) has employed a variety of methods. For example, Shi et al. (2016), Ettinger et al. (2016), and Tenney et al. (2019) use probing tasks to target a model’s knowledge of particular grammatical features. Marvin and Linzen (2018) and Wilcox et al. (2019) compare language models’ probabilities for pairs of minimally different sentences differing in grammatical acceptability. Linzen et al. (2016), Warstadt et al. (2018), and Kann et al. (2019) use Boolean acceptability judgments inspired by methodologies in generative linguistics. However, we have not yet seen any substantial direct comparison between these methods, and it is not yet clear whether they tend to yie"
D19-1286,N18-1101,1,0.845802,"-mll/jiant/ tree/blimp-and-npi/scripts/bert_npi 7 https://github.com/huggingface/ pytorch-pretrained-BERT nington et al., 2014) and (ii) BERT (Devlin et al., 2018): we use the cased version of BERT-large model, which works the best for our tasks in pilot experiments. In addition, since recent work (Liu et al., 2019; Stickland and Murray, 2019) has shown that intermediate training on related tasks can meaningfully impact BERT’s performance on downstream tasks, we also explore two additional BERT-based models—(iii) BERT!MNLI: BERT fine-tuned on the Multi-Genre Natural Language Inference corpus (Williams et al., 2018), motivated both by prior work on pretraining sentence encoders on MNLI (Conneau et al., 2017) as well as work showing significant improvements to BERT on downstream semantic tasks (Phang et al., 2018; Bowman et al., 2018) (iv) BERT!CCG: BERT fine-tuned on Combinatory Categorial Grammar Bank corpus (Hockenmaier and Steedman, 2007), motivated by Wilcox et al.’s (2019) finding that structural supervision may improve a LSTM-based sentence encoders knowledge on non-local syntactic dependencies. Training-Evaluation Configurations We are interested in whether sentence representation models learn NPI"
D19-1286,W18-5446,1,0.894889,"Missing"
D19-1329,P17-1183,0,0.0266486,", while staying as close to the original pronunciation as possible. Unlike for translation, focus lies on the sound; the target language meaning is usually ignored. Data. For our transliteration experiments, we follow Upadhyay et al. (2018). We experiment on datasets from the Named Entities Workshop 2015 (Duan et al., 2015) in Hindi, Kannada, Bengali, Tamil, and Hebrew. For this task, all languages are both development and target languages. Model. The last featured model is an LSTM sequence-to-sequence model similar to that by Bahdanau et al. (2015), except for using hard monotonic attention (Aharoni and Goldberg, 2017). It attends to a single character at a time, and attention moves monotonically over the input. We take hyperparameters and code from Upadhyay et al. (2018).5 Early stopping is done by training for 20 epochs and applying the best model regarding development accuracy to the test data. 4.4 Experimental Setup We run all experiments using the implementations from previous work or OpenNMT as described above. Existing code is only modified where necessary. Most importantly, we add storing of the DevLang model during the main training phase. 5 Results Development sets vs. development languages. We ar"
D19-1329,P17-1031,0,0.0565016,"to compare between tasks, we further limit this study to sequence-to-sequence tasks. 4.1 Historical Text Normalization (NORM) Task. The goal of historical text normalization is to convert old texts into a form that conforms with contemporary spelling conventions. Historical text normalization is a specific case of the general task of text normalization, which additionally encompasses, e.g., correction of spelling mistakes or normalization of social media text. Data. We experiment on the ten datasets from Bollmann et al. (2018), which represent eight different languages: German (two datasets; Bollmann et al., 2017; Odebrecht et al., 2017); English, Hungarian, Icelandic, and Swedish (Pettersson, 2016); Slovene (two datasets; Ljubeˇsic et al., 2016); and Spanish and Portuguese (Vaamonde, 2015). We treat the two datasets for German and Slovene as different languages. All languages serve both as development languages for all other languages and as target languages. Model. Our model for this task is an LSTM (Hochreiter and Schmidhuber, 1997) encoderdecoder model with attention (Bahdanau et al., 2015). Both encoder and decoder have a single hidden layer. We use the default model in OpenNMT (Klein et al., 201"
D19-1329,W18-3403,0,0.284664,"nd multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization (Bollmann et al., 2018), morphological segmentation (Kann et al., 2018), morphological inflection (Makarov and Clematide, 2018; Sharma et al., 2018), argument component identification (Schulz et al., 2018), and transliteration (Upadhyay et al., 2018). 3342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3342–3349, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics However, in a real-world setting with limited resources, it is unlikely that such a development set"
D19-1329,K18-3001,1,0.837587,"ding development accuracy is applied to the test set. 4.2 Morphological Inflection (MORPH) Task. Morphological inflection consists of mapping the canonical form of a word, the lemma, to an indicated inflected form. This task gets very complex for morphologically rich languages, where a single lemma can have hundreds or thousand of inflected forms. Recently, morphological inflection has frequently been cast as a sequenceto-sequence task, mapping the characters of the input word together with the morphological features specifying the target to the characters of the corresponding inflected form (Cotterell et al., 2018). Data. We experiment on the datasets released for a 2018 shared task (Cotterell et al., 2018), which cover 103 languages and feature an explicit low-resource setting. We randomly choose ten development languages: Armenian, Basque, Galician, Georgian, Greenlandic, Icelandic, Karbadian, Kannada, Latin, and Lithuanian. Model. For MORPH, we experiment with a pointer-generator network architecture (Gu et al., 2016; See et al., 2017). This is a sequence-tosequence model similar to that for NORM, but employs separate encoders for characters and features. It is further equipped with a copy mechanism:"
D19-1329,P07-1033,0,0.202739,"Missing"
D19-1329,N19-1423,0,0.0128989,"er, since T is finite, overfitting the training set might lead to poor generalization performance. One way to avoid fitting Equation 1 too closely is early stopping: a separate development or validation set is used to end training as soon as the loss on the development set LD (θ) starts increasing or model performance on the development set D starts decreasing. The best set of parameters θ is used in the final model. This works well when large amounts of data are available to create training, development and test splits. Recently, however, with the success of pretraining (Peters et al., 2018; Devlin et al., 2019) and multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization"
D19-1329,P13-1057,0,0.0892694,"Oliver et al. (2018) investigate how to evaluate semi-supervised training algorithms in a realistic way; they differ from us in that they focus exclusively on semi-supervised learning (SSL) algorithms, and do not consider NLP explicitly. However, in line with our conclusion, they report that recent practices for evaluating SSL techniques do not address the question of the algorithms’ real-word applicability in a satisfying way. In NLP, several earlier works have explicitly investigated real-world low-resource settings as opposed to artificial proxy settings, e.g., for part-of-speech tagging (Garrette et al., 2013) or machine translation (Irvine and Callison-Burch, 2013). While those mostly focus on real data-poor languages, we explicitly investigate the effect of the common practice to assume a relatively large development set for early stopping in the low-resource setting. Low-resource settings in NLP. Research in the area of neural methods for low-resource NLP has gained popularity in recent years, with a dedicated Experimental Design • Main training phase. We train models for all languages keeping both the model resulting from the original early stopping strategy (DevSet) and that from the epoch com"
D19-1329,P16-1154,0,0.02812,"ceto-sequence task, mapping the characters of the input word together with the morphological features specifying the target to the characters of the corresponding inflected form (Cotterell et al., 2018). Data. We experiment on the datasets released for a 2018 shared task (Cotterell et al., 2018), which cover 103 languages and feature an explicit low-resource setting. We randomly choose ten development languages: Armenian, Basque, Galician, Georgian, Greenlandic, Icelandic, Karbadian, Kannada, Latin, and Lithuanian. Model. For MORPH, we experiment with a pointer-generator network architecture (Gu et al., 2016; See et al., 2017). This is a sequence-tosequence model similar to that for NORM, but employs separate encoders for characters and features. It is further equipped with a copy mechanism: using attention to decide on what element from the input sequence to copy, the model computes a probability for either copying or generation while producing an output. The final probability distribution over the target vocabulary is a combination of both. Hyperparameters are taken from Sharma et al. (2018).4 For early stopping, we also follow Sharma et al. (2018): all models are trained 3 github.com/OpenNMT/O"
D19-1329,W18-3400,0,0.187709,"Missing"
D19-1329,W13-2233,0,0.0304037,"te semi-supervised training algorithms in a realistic way; they differ from us in that they focus exclusively on semi-supervised learning (SSL) algorithms, and do not consider NLP explicitly. However, in line with our conclusion, they report that recent practices for evaluating SSL techniques do not address the question of the algorithms’ real-word applicability in a satisfying way. In NLP, several earlier works have explicitly investigated real-world low-resource settings as opposed to artificial proxy settings, e.g., for part-of-speech tagging (Garrette et al., 2013) or machine translation (Irvine and Callison-Burch, 2013). While those mostly focus on real data-poor languages, we explicitly investigate the effect of the common practice to assume a relatively large development set for early stopping in the low-resource setting. Low-resource settings in NLP. Research in the area of neural methods for low-resource NLP has gained popularity in recent years, with a dedicated Experimental Design • Main training phase. We train models for all languages keeping both the model resulting from the original early stopping strategy (DevSet) and that from the epoch computed in the stopping point selection phase (DevLang).2 T"
D19-1329,P17-1182,1,0.882068,"Missing"
D19-1329,N18-1005,1,0.903263,"Missing"
D19-1329,P17-4012,0,0.0215587,"mann et al., 2017; Odebrecht et al., 2017); English, Hungarian, Icelandic, and Swedish (Pettersson, 2016); Slovene (two datasets; Ljubeˇsic et al., 2016); and Spanish and Portuguese (Vaamonde, 2015). We treat the two datasets for German and Slovene as different languages. All languages serve both as development languages for all other languages and as target languages. Model. Our model for this task is an LSTM (Hochreiter and Schmidhuber, 1997) encoderdecoder model with attention (Bahdanau et al., 2015). Both encoder and decoder have a single hidden layer. We use the default model in OpenNMT (Klein et al., 2017)3 as our implementation and employ the hyperparameters from Bollmann et al. (2018). In the original paper, early stopping is done by training for 50 epochs, and the best model regarding development accuracy is applied to the test set. 4.2 Morphological Inflection (MORPH) Task. Morphological inflection consists of mapping the canonical form of a word, the lemma, to an indicated inflected form. This task gets very complex for morphologically rich languages, where a single lemma can have hundreds or thousand of inflected forms. Recently, morphological inflection has frequently been cast as a sequ"
D19-1329,D18-1314,0,0.0237466,"re showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of historical text normalization (Bollmann et al., 2018), morphological segmentation (Kann et al., 2018), morphological inflection (Makarov and Clematide, 2018; Sharma et al., 2018), argument component identification (Schulz et al., 2018), and transliteration (Upadhyay et al., 2018). 3342 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3342–3349, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics However, in a real-world setting with limited resources, it is unlikely that such a development set would be available for early stopping, since it would be more effective to use at least part of it for"
D19-1329,N18-1202,0,0.0117913,"examples in T . However, since T is finite, overfitting the training set might lead to poor generalization performance. One way to avoid fitting Equation 1 too closely is early stopping: a separate development or validation set is used to end training as soon as the loss on the development set LD (θ) starts increasing or model performance on the development set D starts decreasing. The best set of parameters θ is used in the final model. This works well when large amounts of data are available to create training, development and test splits. Recently, however, with the success of pretraining (Peters et al., 2018; Devlin et al., 2019) and multi-task learning (Caruana, 1997; Ruder, 2017; Wang et al., 2019) approaches, neural models are showing promising results on various natural language processing (NLP) tasks also in lowresource or few-shot settings (Johnson et al., 2017; Kann et al., 2017; Yu et al., 2018). Often, the high-resource experimental setup and training procedure are kept unchanged, and the size of the original training set is reduced to simulate limited data. This leads to settings where validation examples may outnumber training examples. Table 1 shows such cases for the tasks of histori"
D19-1329,N18-2006,0,0.0573727,"n Cho and Samuel R. Bowman New York University, USA {kann, kyunghyun.cho, bowman}@nyu.edu Abstract # train Table 1: Number of examples used for training and development in recent low-resource NLP experiments; ES=early stopping on the development set. Experiments from papers in bold will be revisited here. Introduction Parametric machine learning models are frequently trained by minimizing the loss on the training set T , LT (θ) = X l (θ, x) , ES Bollmann et al. (2018) 5k 12k-46k Yes Kann et al. (2018) 400-700 100-200 Yes Makarov and Clematide (2018) 100 1k Yes Sharma et al. (2018) 100 100 Yes Schulz et al. (2018) 1k-21k 9k N/A Upadhyay et al. (2018) 500 1k Yes Development sets are impractical to obtain for real low-resource languages, since using all available data for training is often more effective. However, development sets are widely used in research papers that purport to deal with low-resource natural language processing (NLP). Here, we aim to answer the following questions: Does using a development set for early stopping in the low-resource setting influence results as compared to a more realistic alternative, where the number of training epochs is tuned on development languages? And does it l"
D19-6123,W18-3020,0,0.0143063,"be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are in"
D19-6123,D16-1073,0,0.0204782,"rmance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019)"
D19-6123,P17-2021,0,0.0709194,"Missing"
D19-6123,P19-1234,0,0.0156458,"ve been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cross-lingual transfer and difference in hyper parameters. In this work, we focus on adaptSetup Since transfer learning is mostly needed and also particularly effective in the low-data regime, we combine the training sets of 2,500 examples for English and German—the two of our languages which are related—to form a multilingual training set. We then train PRPN models on this combined training set, sharing all para"
D19-6123,N19-1388,0,0.0324253,"al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages"
D19-6123,W18-2704,0,0.0249191,"arsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words"
D19-6123,P06-1109,0,0.0575599,"ervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ"
D19-6123,W01-0713,0,0.232608,"mobile devices. Second, neural models which have been trained simultaneously on multiple languages have been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and s"
D19-6123,P17-1182,1,0.8623,"Missing"
D19-6123,P19-1228,0,0.221934,"Missing"
D19-6123,D18-1269,1,0.821128,"ask in another (usually low-resource) language, is very common when working on resource-poor languages in NLP. There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsi"
D19-6123,N19-1114,0,0.122249,"erarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 209–218 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 tion? From a practical NLP perspective, grammar induction enables us to obtain syntactic information without labeled data, i.e., even in lowresource settings and for resource-poor languages. This information can then be of help for downstream tasks like machine translation"
D19-6123,D16-1001,0,0.0169883,"lliams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investigated the behavior of th"
D19-6123,P02-1017,0,0.435434,"odels, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo),"
D19-6123,N19-1116,0,0.0197397,"nd Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cros"
D19-6123,P04-1061,0,0.16553,"ave been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates"
D19-6123,D18-1543,0,0.0392102,"Missing"
D19-6123,P03-1013,0,0.0582167,"splits: the development and test sets consist of 300 files each, while the training set consists of the remaining 2407. Balanced trees baseline (BTB). Finally, this baseline is similar to LBR/RBR, but considering balanced binary trees, which are created by recursively splitting each span into halves. For odd lengths, the middle word becomes a part of the right subtree. German. We use the NEGRA corpus (Skut et al., 1997), which consists of approximately 350, 000 words of German newspaper text (20,602 sentences). We divide the dataset into training, development, and test splits as suggested by Dubey and Keller (2003). 4 4.1 Monolingual Experiments Language-Dependence of Hyperparameters Best binary tree upper bound (BB). Since our datasets contain n-ary trees, but the PRPN only produces binary trees, obtaining a perfect F1 score is impossible. This upper bound represents the best score which can be obtained with binary trees. It is of practical importance to know whether a set of hyperparameters found for one language transfers to another one without any changes, especially for low-resource language without annotated (development) data. Therefore, we ask the following questions: (i) Do hyperparameters depe"
D19-6123,P19-1227,0,0.0247975,"ces for 12,500 examples and for the entire training sets for all languages. Thus, we conclude that disposing of more than 12,500 examples is generally beneficial. 5 0.186 (.08) 0.326 (.02) Results Results for single-language models as well as the multilingual versions are shown in Table 3. The parsing performance of the multilingual model is 215 ing knowledge gained from one (usually highresource) language for solving a task in another (usually low-resource) language, is very common when working on resource-poor languages in NLP. There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et"
D19-6123,D18-1162,0,0.0249036,"Missing"
D19-6123,D11-1118,0,0.0309377,"ted languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gatin"
D19-6123,P17-1076,0,0.0617362,"ist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investigated the behavior of the PRPN, a neural unsupervised constituency parsing model, for the languages Arabic, Chinese, English, and German. While, overall, our experiment"
D19-6123,Q18-1019,1,0.795528,"suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 201"
D19-6123,W19-4226,0,0.146783,"s needed for a set of languages, less memory is required to store all parameters. This facilitates, for instance, the application on mobile devices. Second, neural models which have been trained simultaneously on multiple languages have been shown to leverage knowledge from related languages to improve performance on other languages in the case of limited training data. Such cross-lingual transfer has been successfully employed for a variety of tasks, e.g., for supervised dependency parsing (de Lhoneux et al., 2018), for machine translation (Johnson et al., 2017a), or for paradigm completion (McCarthy et al., 2019). 5.2 6 Related Work Unsupervised parsing. Previous work on nonneural models for unsupervised parsing includes Clark (2001) and Klein and Manning (2002) for constituency parsing and Carroll and Charniak (1992); Klein and Manning (2004); Cohn et al. (2010); Spitkovsky et al. (2011); and Jiang et al. (2016) for dependency parsing. For Chinese, German, and English, previous work also observed differences in F1 scores; an overview can be found in Bod (2006). We have not included these nonneural baselines as part of our results due to lack of availability of trustworthy implementations. Following t"
D19-6123,J97-3002,0,0.261135,"Missing"
D19-6123,N18-4013,1,0.832279,"serve whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classification tasks like natural language inference (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018), machine translation (Bisk and Tran, 2018), or toy datasets where the correct parse can trivially be found by humans (Jacob et al., 2018; Nangia and Bowman, 2018). Latent tree learning models have been shown to outperform sequential models and TreeRNNs on multiple datasets (Maillard et al., 2017; Choi et al., 2018). However, the parses predicted by latent tree models have been shown to mostly be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figu"
D19-6123,P12-1066,0,0.0262921,"been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are sentence classificati"
D19-6123,H01-1035,0,0.216396,"Missing"
D19-6123,P92-1017,0,0.57111,"r grammar induction—i.e., models, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language learning and grammar design by demonstrating what can be learned without substantial prior knowledge. Further, it can also be practically relevant for low-resource languages or language styles. Recently, multiple types of neural network models have been added to the line of research on 1 Some work, e.g. Kim et al. (2019a), present PRPN results on Chinese, but without further analysis of languagedependent differences. 209 Proceedings of the 2nd Workshop on Deep Learning Approaches for L"
D19-6123,N19-1380,0,0.0180437,"There are two very intuitive ways of realizing such a transfer (Liu et al., 2019): One way is to translate the test data into a high-resource language and to solve the task using a system for that second language. Another way is to translate large amount of training data into a low-resource language and train a system in that language. Other methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to o"
D19-6123,P13-1043,0,0.02848,"be nonsensical (Williams et al., 2018). Supervised parsing. This research is further related to the line of work on supervised parsing. Two main parsing paradigms exist: dependency parsing, which is concerned with the relationships between words in a sentence, and constituency parsing (or phrase-structure parsing), which is what we are interested in here (cf. Figure 1). Neural network models have pushed the state of the art for supervised constituency parsing in the last years. Possible approaches include methods to either build parse trees sequentially by estimating transition probabilities (Zhu et al., 2013; Cross and Huang, 2016), employ a chart-based approach, which performs exact structured inference via dynamic programming (Durrett and Klein, 2015; Stern et al., 2017), or cast the problem as a sequence labeling task (G´omez-Rodr´ıguez and Vilares, 2018). Another, rather new option is to predict syntactic distances between words, which can then be converted into trees (Shen et al., 2018b). This is the same core concept that the PRPN is based on. Thus, we consider Shen et al. (2018b)’s approach one of our upper bounds on the unsupervised parsing performance of the PRPN. 7 Conclusion We investi"
D19-6123,P18-1108,0,0.207133,",416 18,598 1,000 1,000 see it as the supervised approach which is most comparable to the PRPN. Since our model does not predict labels which are used to recover nary trees in the original work, we compute the F1 score for this approach only with respect to binary gold trees. This is acceptable for our purposes, since we are interested in the supervised parser upper bound only to get an idea of the difficulty of the datasets in our different languages. For the supervised SUB baseline, we use hidden state and embedding dimensions of 100 and 300, respectively, and keep the default settings from Shen et al. (2018b) for all other hyperparameters. Table 1: Dataset statistics. English. We perform English constituency parsing experiments for comparison with the original work by Shen et al. (2018a). We use the Wall Street Journal Section of the Penn Treebank (Marcus et al., 1999). We use parts 00-21 for training, 22 for validation and 23 for testing. Left/right-branching trees baseline (LBR/RBR). Our next baseline consists of purely left- or right-branching trees. LBR refers to the F1 score strictly left-branching binary trees obtain compared to the gold annotations, and RBR denotes the score of strictly r"
D19-6123,P19-1180,0,0.0163381,"s as part of our results due to lack of availability of trustworthy implementations. Following the success of the neural PRPN model in 2018, various other neural unsupervised parsing approaches have been developed and shown promising results. Shen et al. (2018c) enhance the vanilla LSTM network with master forget and input gates to learn the tree structure through soft gating. Drozdov et al. (2019) use a recursive autoencoder-based architecture. Kim et al. (2019b) employ unsupervised recurrent neural network grammars, and Kim et al. (2019a) employ compound probabilistic context free grammars. Shi et al. (2019) show how image captions can be successfully leveraged to identify constituents in sentences. None of these papers performs an explicit analysis of differences between languages. Jin et al. (2019) extend the PCFG approach to show results on Chinese, English and German. There are certain question that remain unanswered about multilingual grammar induction, especially related to cross-lingual transfer and difference in hyper parameters. In this work, we focus on adaptSetup Since transfer learning is mostly needed and also particularly effective in the low-data regime, we combine the training set"
D19-6123,A97-1014,0,0.0531374,"ed for each of test and development, and the remaining 1434 constitute our training set. Chinese. We use the Chinese Penn Treebank v8.0 (Xue et al., 2005). Again, we randomly separate files into splits: the development and test sets consist of 300 files each, while the training set consists of the remaining 2407. Balanced trees baseline (BTB). Finally, this baseline is similar to LBR/RBR, but considering balanced binary trees, which are created by recursively splitting each span into halves. For odd lengths, the middle word becomes a part of the right subtree. German. We use the NEGRA corpus (Skut et al., 1997), which consists of approximately 350, 000 words of German newspaper text (20,602 sentences). We divide the dataset into training, development, and test splits as suggested by Dubey and Keller (2003). 4 4.1 Monolingual Experiments Language-Dependence of Hyperparameters Best binary tree upper bound (BB). Since our datasets contain n-ary trees, but the PRPN only produces binary trees, obtaining a perfect F1 score is impossible. This upper bound represents the best score which can be obtained with binary trees. It is of practical importance to know whether a set of hyperparameters found for one l"
D19-6123,P09-1009,0,0.028359,"l., 2018a,c). While the latter model family has been able to generate parse trees which show a high accordance with expert annotations, its members, with the prominent Parsing Reading Predict-Network (PRPN) being no exception, have mostly been evaluated on English.1 Thus, it is not obvious whether and when obtained results would hold true for other languages, especially if they are unrelated to English or dispose of significantly smaller training corpora. Some nonneural models for grammar induction—i.e., models, which perform unsupervised parsing—show language-dependent performance variation (Snyder et al., 2009), which motivates our investigation of recent neural models. In this work, we first aim to answer the following research questions, focusing on the parsingreading-predict network (PRPN; Shen et al., 2018a) and experimenting with Arabic, Chinese, Introduction Unsupervised parsing, the task of inducing hierarchical syntactic structure from a large amount of unlabeled text, has been widely studied in natural language processing (NLP) (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Klein and Manning, 2002, 2004). Work on this task bears on open research questions involving human language l"
D19-6123,P11-2120,0,0.0220928,"r methods have been developed as well, many based on parameter sharing—as we do in this work—, e.g., for cross-lingual natural language inference (Conneau et al., 2018), morphological generation (Kann et al., 2017; McCarthy et al., 2019), dialogue systems (Schuster et al., 2019), or machine translation (Johnson et al., 2017b; Aharoni et al., 2019). While we are not aware of any previous work exploring cross-lingual transfer for unsupervised parsing as done in this paper, approaches have been developed which leverage high-resource language data for supervised parsing in low-resource languages (Søgaard, 2011; Naseem et al., 2012). ing the PRPN to a multilingual setting, since it is the first neural model which has been shown to obtain robust unsupervised parsing results. Although we have primarily focused on PRPN due to its overall success, it would be interesting to observe whether similar trends in relative performance among languages hold for other models mentioned above. We leave this for future work. A closely related line of research, which is often referred to as latent tree learning, aims to create a parse structure which is well-suited for a particular NLP application. Common choices are"
K16-1002,P15-1162,0,0.426173,"Missing"
K16-1002,D14-1181,0,0.0191561,"keep rate for word dropout. et al. (2015), we train an `2 -regularized softmax classifier with 10-fold cross-validation to set the regularization. Note that using a linear classifier like this one may disadvantage our representations here, since the Gaussian distribution over hidden codes in a vae is likely to discourage linear separability. We present results in Table 10. Here, ae is a plain sequence autoencoder. We compare with results from a bag of word vectors (cbow, Zhao et al., 2015) and skip-thought (st). We also compare with an rnn classifier (Zhao et al., 2015) and a cnn classifier (Kim, 2014) both of which, unlike our model, are optimized end-to-end. We were not able to make the vae codes perform better than cbow in this case, but they did outperform features from the sequence autoencoder. Skipthought performed quite well, possibly because the skip-thought training objective of next sentence prediction is well aligned to this task: it essentially trains the model to generate sentences that address implicit open questions from the narrative of the book. Combining the two representations did not give any additional performance gain over the base skip-thought model. im fine . youre r"
K16-1002,C04-1051,0,0.0508109,"tence vector. We train classifiers on these means using the same experimental protocol as Kiros et al. (2015). no . he said . “ no , ” he said . “ no , ” i said . “ i know , ” she said . “ thank you , ” she said . “ come with me , ” she said . “ talk to me , ” she said . “ do n’t worry about it , ” she said . Table 8: Paths between pairs of random points in vae space: Note that intermediate sentences are grammatical, and that topic and syntactic structure are usually locally consistent. Paraphrase detection For the task of paraphrase detection, we use the Microsoft Research Paraphrase Corpus (Dolan et al., 2004). We compute features from the sentence vectors of sentence pairs in the same way as Kiros et al. (2015), concatenating the elementwise products and the absolute value of the elementwise differences of the two vectors. We train an `2 -regularized logistic regression classifier and tune the regularization strength using cross-validation. We present results in Table 9 and compare to several previous models for this task. Feats is the lexicalized baseline from Socher et al. (2011). rae uses the recursive autoencoder from that work, and dp adds their dynamic pooling step to calculate pairwise feat"
K16-1002,P15-1107,0,0.555961,"does not learn a vector representation of the full sentence. In order to incorporate a continuous latent sentence representation, we first need a method to map between sentences and distributed representations that can be trained in an unsupervised setting. While no strong generative model is available for this problem, three non-generative techniques have shown promise: sequence autoencoders, skip-thought, and paragraph vector. Sequence autoencoders have seen some success in pre-training sequence models for supervised downstream tasks (Dai and Le, 2015) and in generating complete documents (Li et al., 2015a). An autoencoder consists of an encoder function ϕenc and a probabilistic decoder model p(x|~z = ϕenc (x)), and maximizes the likelihood of an example x conditioned on ~z, the learned code for x. In the case of a sequence autoencoder, both encoder and decoder are rnns and examples are token sequences. Standard autoencoders are not effective at extracting for global semantic features. In Table 1, we present the results of computing a path or homotopy between the encodings for two sentences and decoding each intermediate code. The intermediate sentences are generally ungrammatical and do not t"
K16-1002,C02-1150,0,0.114437,"ntences through purely continuous sampling and provides interpretable homotopies that smoothly interpolate between sentences. We hope in future work to investigate factorization of the latent variable into separate style and content components, to generate sentences conditioned on extrinsic features, to learn sentence embeddings in a semi-supervised fashion for language understanding tasks like textual entailment, and to go beyond adversarial evaluation to a fully adversarial training objective. Question classification We also conduct experiments on the TREC Question Classification dataset of Li and Roth (2002). Following Kiros 18 Method Accuracy st Bi-st Combine-st AE vae cbow vae, combine-st 91.4 89.4 92.2 84.2 87.0 87.3 92.0 rnn cnn 90.2 93.6 amazing , is n’t it ? so , what is it ? it hurts , isnt it ? why would you do that ? “ you can do it . “ i can do it . i ca n’t do it . “ i can do it . “ do n’t do it . “ i can do it . i could n’t do it . i dont like it , he said . i waited for what had happened . it was almost thirty years ago . it was over thirty years ago . that was six years ago . he had died two years ago . ten , thirty years ago . “ it ’s all right here . “ everything is all right here"
K16-1002,J93-2004,0,0.0982698,"y the Gaussian prior. We increase this weight until it reaches 1, at which point the weighted cost function is equivalent to the true variational lower bound. In this setting, we do not optimize the proper lower bound on the training data likelihood during the early stages of training, but we nonetheless see improvements on the value of that bound at convergence. This can be thought of as annealing from a vanilla autoencoder to a vae. The rate of this increase is tuned as a hyperparameter. Figure 2 shows the behavior of the kl cost term during the first 50k steps of training on Penn Treebank (Marcus et al., 1993) language modeling with kl cost annealing in place. This example reflects a pattern that we observed often: kl spikes early in training while the model can encode information in ~z cheaply, then drops substantially once it begins paying the full kl divergence penalty, and finally slowly rises again before converging as the model learns to condense more information into ~z. 4 Results: Language modeling In this section, we report on language modeling experiments on the Penn Treebank in an effort to discover whether the inclusion of a global latent variable is helpful for this standard task. For"
K18-1049,J16-4005,0,0.0496416,"Missing"
K18-1049,D10-1115,0,0.119254,"unction before they participate in semantic composition. The theory bridges a gap between linguistic semantics and programming language theory, and reinterprets various linguistic phenomena from the view of computation. While we do not directly implement Continuation Semantics, we follow its rough contours: We convert low-level representations (vectors) to higher-order functions (matrices), and composition only takes place between the higher-order functions. 2 Related work Composition functions for tree-structured models have been thoroughly studied in recent years (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Socher et al., 2011). While this line of research has been successful, the majority of the existing models ultimately rely on the additive linear combination of vectors. The Tree-structured recursive neural networks (TreeRNN) of Socher et al. (2010) compose two child node vectors ~hl and ~hr using this method: &quot; # ~hl + ~b) (1) ~h = tanh(W ~ hr A number of models have been developed to capture the multiplicative interactions between distributed representations. While having a similar objective, the proposed model requires fewer model parameters than the predecessors be"
K18-1049,D15-1075,1,0.800079,"st set contains sentence pairs from only those five genres, and the mismatched test set contains sentence pairs from additional genres. We also experiment on the Stanford Sentiment Treebank (SST; Socher et al., 2013), which is constructed by extracting movie review excerpts written in English from rottentomatoes.com, and labeling them with Amazon Mechanical Turk. Each example in SST is paired with a parse tree, and each node of the tree is tagged with a finegrained sentiment label (5 classes). Datasets We first train and test our models on the Stanford Natural Language Inference corpus (SNLI; Bowman et al., 2015). The SNLI corpus contains 570,152 pairs of natural language sentences that are labeled for entailment, contradiction, and neutral. It consists of sentences that were written and validated by humans. Along with the MultiNLI corpus introduced below, it is two orders of magnitude larger than other human-authored resources for NLI tasks. The following example illustrates the general format of the corpus. 5 Results and Analysis Table 2 summarizes the results on SNLI and MultiNLI classification. We use the same preprocessing steps for all results we report, including loading the parse trees supplie"
K18-1049,P16-1139,1,0.933307,"model parameters. 3.2 ~i f~l f~r (23) Hcand = tanh(Hl Hr + BCOMB ) The second variant is more complex than the first, in a way that a weight matrix WCOMB ∈ √ √ R d× d is added to the equation (eq. 24). But unlike the full LMS-LSTM which has two tanh layers, it only utilizes one. (24) Hcand = tanh(WCOMB Hl Hr + BCOMB ) 4 4.1 Experiments Implementation details As our interest is in the performance of composition functions, we compare LMS-LSTM with TreeLSTM, the previous best known composition function for tree-structured models. To allow for efficient batching, we use the SPINN-PI-NT approach (Bowman et al., 2016), which implements standard TreeLSTM using stack and buffer data structures borrowed from parsing, rather than tree structures. We implement our model by replacing SPINN-PI-NT’s composition function with ours and adding the LIFT layer. We use the 300D reference GloVe vectors (840B token version; Pennington et al., 2014) for word representations. We fine-tune the word embeddings for improved results. We follow Bowman et al. (2016) and other prior work in our use of an MLP with product and difference features to classify pairs of sentences. LMS augmented with LSTM components We augment the base"
K18-1049,E17-2093,0,0.0315493,"Missing"
K18-1049,S15-1002,0,0.0193116,"s to the noun man, the second interpretation is given. However, if the structural information is lost, we would have no way to disambiguate the two readings. where ~hl , ~hr , ~b ∈ Rd × 1 , W ∈ Rd × 2d , and V ∈ Rd × d × d . RNTN improves on MV-RNN in that its parameter size is not proportional to the size of the corpus. However, the addition of the thirdorder tensor V of dimension d×d×d still requires proportionally more parameters. The last composition function relevant to this paper is the Tree-structured long short-term memory networks (TreeLSTM; Tai et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015), particularly the version over a constituency tree. It is an 510 Model Params. Associative Multiplicative Activation size w.r.t. TreeRNN TreeRNN/LSTM CMS MV-RNN RNTN O(d × d) O(V × d × d) O(V × d × d) O(d × d × d) No Yes No No No Yes Yes Yes 1 1/V 1/V 1/d LMS (this work) O(d × demb ) No Yes 1/demb Table 1: Summary of the models. Params. is the number of model parameters (not counting pretrained word vectors), d is the number of activations at each tree node, demb is the dimension of the word embeddings, and V is the size of the vocabulary. Associative and Multiplicative indicate whether compo"
K18-1049,W17-5308,0,0.0267536,"Missing"
K18-1049,D14-1162,0,0.0950114,"B ) 4 4.1 Experiments Implementation details As our interest is in the performance of composition functions, we compare LMS-LSTM with TreeLSTM, the previous best known composition function for tree-structured models. To allow for efficient batching, we use the SPINN-PI-NT approach (Bowman et al., 2016), which implements standard TreeLSTM using stack and buffer data structures borrowed from parsing, rather than tree structures. We implement our model by replacing SPINN-PI-NT’s composition function with ours and adding the LIFT layer. We use the 300D reference GloVe vectors (840B token version; Pennington et al., 2014) for word representations. We fine-tune the word embeddings for improved results. We follow Bowman et al. (2016) and other prior work in our use of an MLP with product and difference features to classify pairs of sentences. LMS augmented with LSTM components We augment the base model with LSTM components (LMS-LSTM) to circumvent the problem of long-term dependencies. As in the case of TreeLSTM, we additionally manage cell states (~cl , ~cr ). Since the LSTM components operate on vectors, we reshape Hcand , Hl , and Hr into d × 1 column vectors respectively, and produce ~g , ~hl , and ~hr . The"
K18-1049,D08-1094,0,0.123977,"Missing"
K18-1049,P10-1093,0,0.0594315,"Figure 1: The Lifted Matrix-Space model in schematic form. Words are stored as vectors and projected into matrix space by the LIFT layer. A parametric COMPO SITION function combines pairs of these matrices using multiplicative interactions. network composition function according to a binary tree structure supplied by a parser. The success of a tree-structured model largely depends on the design of its composition function. Introduction It has been repeatedly shown that a composition function that captures multiplicative interactions between the two items being composed yields better results (Rudolph and Giesbrecht, 2010; Socher et al., 2012, 2013) than do otherwise-equivalent functions based on simple linear interactions. This paper presents a novel model which advances this line of research, the Lifted Matrix-Space model. We utilize a tensor-parameterized LIFT layer that learns to produce matrix representations of words that are dependent on the content of pre-trained word embedding vectors. Composition of two matrix representations is carried out by a composition layer, into which the two matrices are sequentially Contemporary theoretical accounts of natural language syntax and semantics consistently hold"
K18-1049,D12-1110,0,0.404426,"pace model in schematic form. Words are stored as vectors and projected into matrix space by the LIFT layer. A parametric COMPO SITION function combines pairs of these matrices using multiplicative interactions. network composition function according to a binary tree structure supplied by a parser. The success of a tree-structured model largely depends on the design of its composition function. Introduction It has been repeatedly shown that a composition function that captures multiplicative interactions between the two items being composed yields better results (Rudolph and Giesbrecht, 2010; Socher et al., 2012, 2013) than do otherwise-equivalent functions based on simple linear interactions. This paper presents a novel model which advances this line of research, the Lifted Matrix-Space model. We utilize a tensor-parameterized LIFT layer that learns to produce matrix representations of words that are dependent on the content of pre-trained word embedding vectors. Composition of two matrix representations is carried out by a composition layer, into which the two matrices are sequentially Contemporary theoretical accounts of natural language syntax and semantics consistently hold that sentences are tr"
K18-1049,D13-1170,0,0.261247,"man with binoculars. (5) a. where ~a, ~b, ~h ∈ Rd × 1 , A, B, H ∈ Rd × d , and W , WM ∈ Rd×2d . MV-RNN is computationally costly as it needs to learn an additional d × d matrix for each lexical item. It is empirically known that the size of the vocabulary is roughly proportional to the size of the corpus (Heaps’ law; Herdan, 1960), therefore the number of model parameters increases as the corpus gets bigger. This makes the model less ideal for handling a large corpus: having a huge number of parameters causes a problem both for memory usage and for learning efficiency. Chen et al. (2013) and Socher et al. (2013) present the recursive neural tensor network (RNTN) which reduces the computational complexity of MV-RNN, while capturing the multiplicative interactions between child vectors. The model introduces a third-order tensor V which interacts with the child node vectors as follows: # &quot; ~hl + ~b + ~hTl V~hr ) (8) ~h = tanh(W ~ hr John saw a man with binoculars with binoculars b. John saw a man The sentence has two interpretations that can be disambiguated with the following paraphrases: (i) John saw a man via binoculars, and (ii) John saw a man who has binoculars. The common syntactic analysis of the"
K18-1049,P15-1150,0,0.193947,"Missing"
K18-1049,J16-4006,0,0.0125184,"-RNN is compositional and also captures the multiplicative interaction, but it requires a learned d × d matrix for each vocabulary item. RNTN is also compositional and incorporates multiplicative interaction, but it requires less parameters than MV-RNN. Nevertheless, it requires significantly more parameters than TreeRNN or TreeLSTM. Table 1 is an overview of the discussed models. Other interesting works enrich semantic composition with additional context such as grammatical roles or function/argumenthood (Clark et al., 2008; Erk and Pad´o, 2008; Grefenstette et al., 2014; Asher et al., 2016; Weir et al., 2016). extension of TreeRNN which adapts long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997) networks. It shares the advantage of LSTM networks in that it prevents the vanishing gradient problem (Hochreiter et al., 2001). Unlike TreeRNN, the output hidden state ~h of TreeLSTM is not directly calculated from the hidden states of its child nodes, ~hl and ~hr . Rather, each node in TreeLSTM maintains a cell state ~c that keeps track of important information of its child nodes. The output hidden state ~h is drawn from the cell state ~c by passing it through an output gate ~o. The cell state"
K18-1049,C10-1142,0,0.0835767,"Missing"
N12-2002,P98-1013,0,0.41653,"Missing"
N12-2002,J96-1002,0,0.0172087,"Missing"
N12-2002,Y09-1024,0,0.184215,"apable of humanlike volition: a key lexical semantic property which has been shown to trigger a number of morphological and syntactic phenomena across languages. Annotating a corpus with this information can facilitate statistical semantic work, as well as providing a potentially valuable feature—discussed in Zaenen et al.—for tasks like relation extraction, parsing1 , and 1 Using our model in parsing would require bootstrapping from c oarser parses, as our model makes use of some syntactic features. machine translation. The handful of papers that we have found on animacy annotation—centrally Ji and Lin (2009), Øvrelid (2005), and Orasan and Evans (2001)— classify only the basic ANIMATE / INANIMATE contrast, but show some promise in doing so. Their work shows success in automatically classifying individual words, and related work has shown that animacy can be used to improve parsing performance (Øvrelid and Nivre, 2007). We adopt the class set presented in Zaenen et al. (2004), and build our model around the annotated corpus presented in that work. Their hierarchy contains ten classes, meant to cover a range of categories known to influence animacy-related phenomena cross-linguistically. They are H"
N12-2002,W01-0716,0,0.616431,"ical semantic property which has been shown to trigger a number of morphological and syntactic phenomena across languages. Annotating a corpus with this information can facilitate statistical semantic work, as well as providing a potentially valuable feature—discussed in Zaenen et al.—for tasks like relation extraction, parsing1 , and 1 Using our model in parsing would require bootstrapping from c oarser parses, as our model makes use of some syntactic features. machine translation. The handful of papers that we have found on animacy annotation—centrally Ji and Lin (2009), Øvrelid (2005), and Orasan and Evans (2001)— classify only the basic ANIMATE / INANIMATE contrast, but show some promise in doing so. Their work shows success in automatically classifying individual words, and related work has shown that animacy can be used to improve parsing performance (Øvrelid and Nivre, 2007). We adopt the class set presented in Zaenen et al. (2004), and build our model around the annotated corpus presented in that work. Their hierarchy contains ten classes, meant to cover a range of categories known to influence animacy-related phenomena cross-linguistically. They are HUMAN, ORG (organizations), ANIMAL, MAC (autom"
N12-2002,W04-0216,0,0.561249,"Missing"
N12-2002,C98-1013,0,\N,Missing
N18-1101,W06-1615,0,0.0283989,". Our chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. These techniques—which use labeled training data for a source domain, and aim to train a model that performs well on test data from a target domain with a different distribution—have resulted in gains across many tasks (Daume III and Marcu, 2006; Ben-David et al., 2007), including sequence and part-of-speech tagging (Blitzer et al., 2006; Peng and Dredze, 2017). Moreover, in application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a). Nearly all successful applications of representation learning to NLU have involved models that"
N18-1101,H05-1079,0,0.0242457,"of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement. 1 The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU. In this task, also known as recognizing textual entailment (Cooper et al., 1996; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences—like one of those in Figure 1—and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, and CONTRADICTION. Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics). In particular, a model must handle phenomena like lexical entailment, quan"
N18-1101,D15-1075,1,0.8047,"art on natural language understanding (NLU) for success. While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Associ"
N18-1101,P16-1139,1,0.362207,"hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Met my first girlfriend that way. FACE - TO -FACE contradiction I didn’t meet my first girlfriend until later. CCNC 8 million in relief in the form of emergency housing. G OVERNMENT neutral NNNN The 8 million dollars for emergency housing was still not enough to solve the problem. Now, as children tend their gardens, they hav"
N18-1101,P17-1152,0,0.647153,"g problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Met my first girlfriend that way. FACE - TO -FACE contradiction I didn’t meet my first girlfriend until later. CCNC 8 million in relief in the form of emergency housing. G OVERNMENT neutral NNNN The 8 million dollars for emergency housing was still not enough to solve the problem. Now, as children tend their gardens, they have a new appreciation"
N18-1101,W03-0906,0,0.354266,"Missing"
N18-1101,D17-1070,0,0.304617,"erent genres, as opposed to just simple image captions—and its difficulty, containing a much higher percentage of sentences tagged with one or more elements from our tag set of thirteen difficult linguistic phenomena. This greater diversity is reflected in the dramatically lower baseline model performance on MultiNLI than on SNLI (see Table 5) and comparable interannotator agreement, suggesting that MultiNLI has a lot of headroom remaining for future work. The MultiNLI corpus was first released in draft form in the first half of 2017, and in the time since its initial release, work by others (Conneau et al., 2017) has shown that NLI can also be an effective source task for pre-training and transfer learning in the context of sentence-to-vector models, with models trained on SNLI and MultiNLI substantially outperforming all prior models on a suite of established transfer learning benchmarks. We hope that this corpus will continue to serve for many years as a resource for the development and evaluation of methods for sentence understanding. Acknowledgments This work was made possible by a Google Faculty Research Award. SB also gratefully acknowledges support from Tencent Holdings and Samsung Research. We"
N18-1101,P03-1054,0,0.0506173,"a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017). The corpus is available in two formats—tab separated text and JSON Lines (jsonl), following SNLI. For each example, premise and hypothesis strings, unique identifiers for the pair and prompt, and the following additional fields are specified: • gold label: label used for classification. In examples rejected during the validation process, the value of this field will be ‘-’. • sentence{1,2} parse: Each sentence as parsed by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003). • sentence{1,2} binary parse: parses in unlabeled binary-branching format. • label[1]: The label assigned during the creation of the sentence pair. In rare cases this may be different from gold label, if a consensus of annotators chose a different label during the validation phase. • label[2...5]: The four labels assigned during validation by individual annotators to each development and test example. These fields will be empty for training examples. The current version of the corpus is freely available at nyu.edu/projects/bowman/multinli/ for typical machine learning uses, and may be modifi"
N18-1101,P08-1118,0,0.0113135,"Missing"
N18-1101,W09-3714,0,0.0560612,"t setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement. 1 The task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU. In this task, also known as recognizing textual entailment (Cooper et al., 1996; Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences—like one of those in Figure 1—and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, and CONTRADICTION. Succeeding at NLI does not require a system to solve any difficult machine learning problems except, crucially, that of extracting effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics). In particular, a model must handle phenomena like lexical entailment, quantification, coreference, tense, belief, modality, a"
N18-1101,macleod-etal-2000-american,0,0.0973896,"premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis. This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy. Premise Text Sources The MultiNLI premise sentences are derived from ten sources of freely available text which are meant to be maximally diverse and roughly represent the full range of American English. We selected nine sources from the second release of the Open American National Corpus (OANC; Fillmore et al., 1998; Macleod et al., 2000; Ide and Macleod, 2001; Ide and Suderman, 2006, downloaded 12/20161 ), balancing the volume of source text roughly evenly across genres, and avoiding genres with content that would be too difficult for untrained annotators. OANC data constitutes the following nine genres: transcriptions from the Charlotte Narrative 1 http://www.anc.org/ and Conversation Collection of two-sided, inperson conversations that took place in the early 2000s (FACE - TO - FACE); reports, speeches, letters, and press releases from public domain government websites (G OVERNMENT); letters from the Indiana Center for Int"
N18-1101,S14-2001,0,0.128038,"2: Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI. (gethybrid.io), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI. We used this platform to hire an organized group of workers. 387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors. Validation We perform an additional round of annotation on test and development examples to ensure accurate labelling. The validation phase follows the same procedure used for SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label (ENTAILMENT, CONTRADICTION, NEUTRAL ) for the pair. Each pair is relabeled by four workers, yielding a total of five labels per example. Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours. For each"
N18-1101,marelli-etal-2014-sick,0,0.032827,"2: Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI. (gethybrid.io), a crowdsoucring platform similar to the Amazon Mechanical Turk platform used for SNLI. We used this platform to hire an organized group of workers. 387 annotators contributed through this group, and at no point was any identifying information about them, including demographic information, available to the authors. Validation We perform an additional round of annotation on test and development examples to ensure accurate labelling. The validation phase follows the same procedure used for SICK (Marelli et al., 2014b) and SNLI: Workers are presented with pairs of sentences and asked to supply a single label (ENTAILMENT, CONTRADICTION, NEUTRAL ) for the pair. Each pair is relabeled by four workers, yielding a total of five labels per example. Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours. For each"
N18-1101,D16-1046,0,0.00928244,"or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Met my first girlfriend that way. FACE - TO -FACE contradiction I didn’t meet my first girlfriend until later. CCNC 8 million in relief in the form of emergency housing. G OVERNMENT neutral NNNN The 8 million dollars for emergency housing was still not enough to solve the problem. Now, as children tend t"
N18-1101,P16-2022,0,0.456088,"or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Met my first girlfriend that way. FACE - TO -FACE contradiction I didn’t meet my first girlfriend until later. CCNC 8 million in relief in the form of emergency housing. G OVERNMENT neutral NNNN The 8 million dollars for emergency housing was still not enough to solve the problem. Now, as children tend t"
N18-1101,E17-1038,0,0.00636347,"ms, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Met my first girlfriend that way. FACE - TO -FACE contradiction I didn’t meet my first girlfriend until later. CCNC 8 million in relief in the form of emergency housing. G OVERNMENT neutral NNNN The 8 million dollars for emergency housing was still not en"
N18-1101,W17-5301,1,0.243687,"ble 1 shows randomly chosen development set examples from the collected corpus. Hypotheses tend to be fluent and correctly spelled, though not all are complete sentences. Punctuation is often omitted. Hypotheses can rely heavily on knowledge about the world, and often don’t correspond closely with their premises in syntactic structure. Unlabeled test data is available on Kaggle for both matched and mismatched sets as competitions that will be open indefinitely; Evaluations on a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017). The corpus is available in two formats—tab separated text and JSON Lines (jsonl), following SNLI. For each example, premise and hypothesis strings, unique identifiers for the pair and prompt, and the following additional fields are specified: • gold label: label used for classification. In examples rejected during the validation process, the value of this field will be ‘-’. • sentence{1,2} parse: Each sentence as parsed by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003). • sentence{1,2} binary parse: parses in unlabeled binary-branching format. • label[1]: The label assigned during"
N18-1101,D16-1244,0,0.523457,"Missing"
N18-1101,W17-2612,0,0.026,"in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. These techniques—which use labeled training data for a source domain, and aim to train a model that performs well on test data from a target domain with a different distribution—have resulted in gains across many tasks (Daume III and Marcu, 2006; Ben-David et al., 2007), including sequence and part-of-speech tagging (Blitzer et al., 2006; Peng and Dredze, 2017). Moreover, in application areas outside NLU, artificial neural network techniques have made it possible to train general-purpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a). Nearly all successful applications of representation learning to NLU have involved models that are trained on data clo"
N18-1101,D14-1162,0,0.104451,"Missing"
N18-1101,N16-1170,0,0.0134535,"arning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access. This makes it difficult to accurately judge the degree to As the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufficient testing ground for machine learning models in two ways. 1112 Proceedings of NAACL-HLT 2018, pages 1112–1122 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Met my first girlfriend that way. FACE - TO -FACE contradiction I didn’t meet my first girlfriend until later. CCNC 8 million in relief in the form of emergency housing. G OVERNMENT neutral NNNN The"
N18-1101,Q14-1006,0,0.0596607,"tml 10 gutenberg.org/cache/epub/18768/ pg18768.txt 11 gutenberg.org/cache/epub/31547/ pg31547.txt 12 gutenberg.org/files/1965/1965-0.txt 1114 3 sentences (under eight characters), and manually remove certain types of non-narrative writing, such as mathematical formulae, bibliographic references, and lists. Although SNLI is collected in largely the same way as MultiNLI, and is also permissively licensed, we do not include SNLI in the MultiNLI corpus distribution. SNLI can be appended and treated as an unusually large additional CAPTIONS genre, built on image captions from the Flickr30k corpus (Young et al., 2014). Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled ENTAILMENT), one which is necessarily false or inappropriate whenever the premise is true (CONTRADICTION), and one where neither condition applies (NEUTRAL). This method of data collection ensures that the three classes will be represented equally in the raw corpus. The prompts that surround each premise sentenc"
N18-2017,D15-1075,1,0.69331,". This suggests that, despite recently reported progress, natural language inference remains an open problem. Introduction Natural language inference (NLI; also known as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. T"
N18-2017,P17-2097,0,0.0400512,"ICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered wh"
N18-2017,P16-1223,0,0.0889145,"Missing"
N18-2017,N18-2123,0,0.0357975,"Missing"
N18-2017,D17-1070,0,0.0753495,"Missing"
N18-2017,N16-1098,0,0.0292386,"yMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered when annotating new datasets. to use them for evaluating NLI models (in addition to the original benc"
N18-2017,D16-1244,0,0.193867,"Missing"
N18-2017,S18-2023,0,0.153649,"Missing"
N18-2017,D16-1264,0,0.144407,"Missing"
N18-2017,W17-1609,0,0.0464368,"Missing"
N18-2017,D17-1215,0,0.24022,"Missing"
N18-2017,K17-1004,1,0.376046,"f discourse markers such as because. Once again, we observe that the example from the SNLI annotation guidelines does just that, by adding the purpose clause to catch a stick (Table 3). Contradiction. Negation words such as nobody, no, never and nothing are strong indicators of contradiction.3 Other (non-negative) words appear to be part of heuristics for contradicting whatever information is displayed in the premise; sleeping contradicts any activity, and naked (further down the list) contradicts any description of clothing. 3 Similar findings were observed in the ROC story cloze annotation (Schwartz et al., 2017). 109 Model DAM ESIM DIIN Full SNLI Hard Easy MultiNLI Matched Full Hard Easy MultiNLI Mismatched Full Hard Easy 84.7 85.8 86.5 69.4 71.3 72.7 72.0 74.1 77.0 72.1 73.1 76.5 92.4 92.6 93.4 55.8 59.3 64.1 85.3 86.2 87.6 56.2 58.9 64.4 85.7 85.2 86.8 Table 5: Performance of high-performing NLI models on the full, Hard, and Easy NLI test sets. 4 Re-evaluating NLI Models not be as straightforward to eliminate annotation artifacts once the dataset has been collected. First, after removing the Easy examples, Hard examples might not necessarily be artifact-free. For instance, removing all contradictin"
N18-2017,E17-2068,0,0.0173405,"9 35.2 52.3 Table 2: Performance of a premise-oblivious text classifier on NLI. The MultiNLI benchmark contains two test sets: matched (in-domain examples) and mismatched (out-of-domain examples). A majority baseline is presented for reference. 3.1 To see whether the use of certain words is indicative of the inference class, we compute the pointwise mutual information (PMI) between each word and class in the training set: To determine the degree to which such artifacts exist, we train a model to predict the label of a given hypothesis without seeing the premise. Specifically, we use fastText (Joulin et al., 2017), an off-the-shelf text classifier that models text as a bag of words and bigrams, to predict the entailment label of the hypothesis.1 This classifier is completely oblivious to the premise. Table 2 shows that a significant portion of each test set can be correctly classified without looking at the premise, well beyond the most-frequentclass baseline.2 Our finding demonstrates that it is possible to perform well on these datasets without modeling natural language inference. 3 Lexical Choice PMI(word, class) = log p(word, class) p(word, ·)p(·, class) We apply add-100 smoothing to the raw statis"
N18-2017,N18-1101,1,0.645347,"wn as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. These authors contributed equally to this work. 107 Proceedings of NAACL-HLT 2018, pages 107–112 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Compu"
N18-2017,S14-2055,0,0.0249089,"that such models rely heavily on annotation artifacts in the hypothesis to make their predictions. A natural question to ask is whether it is possible to select a set of NLI training and test samples which do not contain easy-to-exploit artifacts. One solution might be to filter Easy examples from the training set, retaining only Hard examples. However, initial experiments suggest that it might 5 Discussion We reflect on our results and relate them to other work that also analyzes annotation artifacts in NLP datasets, drawing three main conclusions. Many datasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark"
N18-2017,P12-2031,0,0.0251786,"emonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across"
N18-2017,P16-2041,1,0.826417,"atasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this wo"
N18-2017,N15-1098,1,0.787039,"lopment of additional challenging benchmarks that expose the true performance levels of state-of-the-art NLI models. Acknowledgments This research was supported in part by the DARPA CwC program through ARO (W911NF15-1-0543) and a hardware gift from NVIDIA Corporation. SB acknowledges gift support from Google and Tencent Holdings and support from Samsung Research. References Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the behavior of visual question answering models. In Proc. of EMNLP. https: //aclweb.org/anthology/D16-1203. Supervised models leverage annotation artifacts. Levy et al. (2015) demonstrated that supervised lexical inference models rely heavily on artifacts in the datasets, particularly the tendency of some words to serve as prototypical hypernyms. Agrawal et al. (2016); Jabri et al. (2016); Goyal et al. (2017) all showed that state-of-the-art visual question answering (Antol et al., 2015) systems leverage annotation biases in the dataset. Cirik et al. (2018) find that complex models for referring expression recognition achieve high performance without any text input. In parallel to this work, Dasgupta et al. (2018) found that the InferSent model (Conneau et al., 201"
N18-2017,D16-1203,0,\N,Missing
N18-2017,P18-1224,0,\N,Missing
N18-4013,D15-1075,1,0.812959,"ree learning models. ListOps is an environment where parsing is essential to success. So if a latent tree model is able to achieve high accuracy in this rigid environment, it indicates that the model is able to learn a sensible parsing strategy. Conversely, if it fails on ListOps, it may suggest that the model is simply incapable of learning to parse. Related Work To the best of our knowledge, all existing work on latent tree models studies them in a natural language setting. Williams et al. (2018a) experiment with two leading latent tree models on the textual entailment task, using the SNLI (Bowman et al., 2015) and MultiNLI corpora (Williams et al., 2018b). The Williams et al. (2018a) analysis studies the models proposed by Yogatama et al. (2017) (which they call RL-SPINN) and Choi et al. (2018) (which they call ST-Gumbel). A third latent tree learning model, which is closely related to ST-Gumbel, is presented by Maillard et al. (2017). All three models make use of TreeLSTMs (Tai et al., 2015) and learn to parse with distant supervision from a downstream semantic objective. The RL-SPINN model uses the REINFORCE algorithm (Williams, 1992) to train the model’s parser. 93 • MAX: the largest value of th"
N18-4013,N18-1101,1,0.861943,"yu.edu Center for Data Science New York University 60 Fifth Avenue New York, NY 10011 Dept. of Linguistics New York University 10 Washington Place New York, NY 10003 2 3 Dept. of Computer Science New York University 60 Fifth Avenue New York, NY 10011 Abstract Latent tree learning models learn to parse a sentence without syntactic supervision, and use that parse to build the sentence representation. Existing work on such models has shown that, while they perform well on tasks like sentence classification, they do not learn grammars that conform to any plausible semantic or syntactic formalism (Williams et al., 2018a). Studying the parsing ability of such models in natural language can be challenging due to the inherent complexities of natural language, like having several valid parses for a single sentence. In this paper we introduce ListOps, a toy dataset created to study the parsing ability of latent tree models. ListOps sequences are in the style of prefix arithmetic. The dataset is designed to have a single correct parsing strategy that a system needs to learn to succeed at the task. We show that the current leading latent tree models are unable to learn to parse and succeed at ListOps. These models"
N18-4013,D11-1014,0,0.17768,"Missing"
N18-4013,P15-1150,0,0.158452,"Missing"
N18-4017,P15-1162,0,0.0763168,"Missing"
N18-4017,D15-1075,1,0.692716,"a machine reader as shown in Figure 1. The focus of our work is to improve the ranker for QA performance. We use DrQA’s Document Reader as our reader. We train our ranker and reader models on QUASAR-T (Dhingra et al., 2017b) dataset. QUASAR-T provides a collection top 100 short paragraphs returned by search engine for each question in the dataset. Our goal is to find the correct answer span for a given question. 3 3.1 3.2.1 InferSent Ranker InferSent (Conneau et al., 2017) provides distributed representations for sentences.3 It is trained on Stanford Natural Language Inference Dataset (SNLI; Bowman et al., 2015) and MultiGenre NLI Corpus (MultiNLI; Williams et al., 2017) using supervised learning. It generalizes well and outperforms unsupervised sentence representations such as Skip-Thought Vectors (Kiros et al., 2015) in a variety of tasks. As InferSent representation captures the general semantics of a sentence, we use it to implement the ranker that ranks based on semantic similarity. To compose sentence representations into a paragraph representation, we simply sum the InferSent representations of all the sentences in the paragraph. This approach is inspired by the sum of word representations as"
N18-4017,P17-1171,0,0.461914,"e New York, NY 10003 2 Abstract 3 Dept. of Computer Science New York University 60 Fifth Avenue New York, NY 10011 looks for the answer in the unstructured text corpus (Brill et al., 2001). This approach eliminates the need to build and update knowledge bases by taking advantage of the large amount of text data available on the web. Complex parsing rules and information extraction methods are required to extract answers from unstructured text. As machine readers are excellent at this task, there have been attempts to combine search engines with machine reading for corpus-based open-domain QA (Chen et al., 2017; Wang et al., 2017). To achieve high accuracy in this setting, the top documents retrieved by the search engine must be relevant to the question. As the top ranked documents returned from search engine might not contain the answer that the machine reader is looking for, reranking the documents based on the likelihood of containing answer will improve the overall QA performance. Our focus is on building a neural network ranker to re-rank the documents retrieved by a search engine to improve overall QA performance. Semantic similarity is crucial in QA as the passage containing the answer may be"
N18-4017,P16-2022,0,0.0133531,"entence, we use it to implement the ranker that ranks based on semantic similarity. To compose sentence representations into a paragraph representation, we simply sum the InferSent representations of all the sentences in the paragraph. This approach is inspired by the sum of word representations as composition function for forming sentence representations (Iyyer et al., 2015). We implement a feed-forward neural network as our scoring function. The input feature vector is constructed by concatenating the question embedding, paragraph embedding, their difference, and their element-wise product (Mou et al., 2016): Model Architecture Overall Setup The overall pipeline consists of a search engine, ranker and reader. We do not build our own search engine as QUASAR-T provides 100 short passages already retrieved by the search engine for each question. We build two different rankers: InferSent ranker to evaluate the performance of semantic similarity in ranking for QA, and RelationNetworks ranker to evaluate the performance of relevance matching in ranking for QA. We use the Document Reader of DrQA (Chen et al., 2017) as our machine reader. 3.2 Ranker Given a question and a paragraph, the ranker model acts"
N18-4017,D14-1162,0,0.0809574,"ven a paragraph P (answerj |pij ), where answerj stands for the answer span of j th question in dataset and pij indicates the corresponding top 5 paragraphs. We can thus calculate the overall confidence of each answer span and corresponding paragraph P (pij , answerj ) by multiplying P (answerj |pij ) with P (pij ). We then choose the answer span with the highest P (answerj , pij ) as the output of our model. i,j where q = {q1 , q2 , ..., qn } is the question that contains n words and p = {p1 , p2 , ..., pm } is the paragraph that contains m words; E(qi ) is a 300 dimensional GloVe embedding (Pennington et al., 2014) of word qi , and [·; ·] is the concatenation operator. fφ and gθ are 3 layer feed-forward neural networks with ReLU activation function. The role of gθ is to infer the relation between two words while fφ serves as the scoring function. As we directly compare the word embeddings, this model will lose the contextual information and word order, which can provide us some semantic information. We do not fine-tune the word embeddings during training as we want to preserve the generalized meaning of GloVe embeddings. We hypothesize that this ranker will achieve a high retrieval recall as relevance m"
N18-4017,D17-1070,0,0.0663539,"acting the correct answer by the reader. We follow a similar pipeline as Wang et al. (2017). Our system consists of a neural network ranker and a machine reader as shown in Figure 1. The focus of our work is to improve the ranker for QA performance. We use DrQA’s Document Reader as our reader. We train our ranker and reader models on QUASAR-T (Dhingra et al., 2017b) dataset. QUASAR-T provides a collection top 100 short paragraphs returned by search engine for each question in the dataset. Our goal is to find the correct answer span for a given question. 3 3.1 3.2.1 InferSent Ranker InferSent (Conneau et al., 2017) provides distributed representations for sentences.3 It is trained on Stanford Natural Language Inference Dataset (SNLI; Bowman et al., 2015) and MultiGenre NLI Corpus (MultiNLI; Williams et al., 2017) using supervised learning. It generalizes well and outperforms unsupervised sentence representations such as Skip-Thought Vectors (Kiros et al., 2015) in a variety of tasks. As InferSent representation captures the general semantics of a sentence, we use it to implement the ranker that ranks based on semantic similarity. To compose sentence representations into a paragraph representation, we si"
N18-4017,P17-1168,0,0.271802,"estion and a document, relevance matching measures the word or phrase level local interactions between pieces of texts in a question and a document. As fixed size representations encode the general meaning of the whole sentence or document, they lose some distinctions about the keywords that are crucial for retrieval and question answering. To analyze the importance of relevance matching in QA, we build another ranker model that focuses on local interactions between words in the question and words in the document. We evaluate and analyze the performance of the two rankers on QUASAR-T dataset (Dhingra et al., 2017b). We observe that the ranker model that focuses on relevance matching (RelationNetworks ranker) achieves significantly higher retrieval recall but the ranker model that focuses on semantic similarity (InferSent ranker) has better overall QA performance. We achieve 11.6 percent improvement in overall QA performance by integrating InferSent ranker (6.4 percent improvement by Relation-Networks ranker). 2 Related Work With the introduction of large-scale datasets for machine reading such as CNN/DailyMail (Hermann et al., 2015) and The Stanford Question Answering Dataset (SQuAD; Rajpurkar et al.,"
N18-4017,D16-1264,0,\N,Missing
N18-4017,N18-1101,1,\N,Missing
N19-1063,D18-2029,0,0.0406542,"Missing"
N19-1063,W12-1008,0,0.537502,"Missing"
N19-1063,D17-1070,0,0.0653678,"Missing"
N19-1063,S18-2005,0,0.117511,"and “<word> are things.”. These templates make heavy use of deixis and are designed to convey little specific meaning beyond that of the terms inserted into them.2 For example, the word version of Caliskan Test 3 is illustrated in Table 1 and the sentence version is illustrated in Table 2. We choose this design to focus on the associations a sentence encoder makes with a given term rather than those it happens to make with the contexts of that term that are prevalent in the training data; a similar design was used in a recent sentiment analysis evaluation corpus stratified by race and gender (Kiritchenko and Mohammad, 2018). To facilitate future work, we publicly release code for SEAT and all of our experiments.3 Table 2: Subsets of target concepts and attributes from the bleached sentence version of Caliskan Test 3. 2 meanx∈X s(x, A, B) − meany∈Y s(y, A, B) . std devw∈X∪Y s(w, A, B) Methods The Word Embedding Association Test WEAT imitates the human implicit association test (Greenwald et al., 1998) for word embeddings, measuring the association between two sets of target concepts and two sets of attributes. Let X and Y be equal-size sets of target concept embeddings and let A and B be sets of attribute embeddi"
N19-1063,D14-1162,0,0.0923063,"ied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders. 1 Introduction Word embeddings quickly achieved wide adoption in natural language processing (NLP), precipitating the development of efficient, word-level neural models of human language. However, prominent word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) encode systematic biases against women and black people (Bolukbasi et al., 2016; Garg et al., 2018, i.a.), implicating many NLP systems in scaling up social injustice. We investigate whether sentence encoders, which extend the word embedding approach to sentences, are similarly biased.1 The previously developed Word Embedding Association Test (WEAT; Caliskan et al., 2017) measures bias in word embeddings by comparing two sets of target-concept words to two sets of attribute words. We propose a simple generaliza1 While encoder training data may contain perspectives from outside the U.S., we fo"
N19-1063,N18-1202,0,0.213263,"Missing"
N19-1063,W17-1609,1,0.884694,"Missing"
N19-1063,D18-1301,0,0.018042,"luding simple bag-of-words encoders, sentence-to-vector models, and state-ofthe-art sequence models.7 For all models, we use publicly available pretrained parameters. Table 4 shows effect size and significance at 0.01 before and after applying the HolmBonferroni multiple testing correction (Holm, 1979) for a subset of tests and models; complete results are provided in the supplement.8 6 We consider other formulations in the supplement. We provide further details and explore variations on these model configurations in the supplement. 8 We use the full set of tests and models when comput7 5 See Schluter (2018) for a recent exposition of the glass ceiling in the NLP research community. 624 Test C1: Flowers/Insects C1: Flowers/Insects C3: EA/AA Names C3: EA/AA Names C6: M/F Names, Career C6: M/F Names, Career ABW Stereotype ABW Stereotype Double Bind: Competent Double Bind: Competent Double Bind: Competent Double Bind: Likable Double Bind: Likable Double Bind: Likable Context word sent word sent word sent word sent word sent sent (u) word sent sent (u) CBoW InferSent GenSen ∗∗ ∗∗ ∗∗ 1.50 1.56∗∗ 1.41∗∗ 0.52∗∗ 1.81∗ 1.74∗∗ 1.10∗ 0.62∗∗ 1.62∗ 0.79∗∗ 0.84 1.29∗ 0.69∗ 0.51 1.56 1.65∗∗ 1.33∗∗ 1.07∗∗ 1.78∗"
N19-1063,W17-1606,0,0.0329342,"cluding those only presented in the supplement. 9 However, the double bind results differ across models; we show no significant associations for ELMo or GPT and only one each for USE and BERT. 10 Our SEAT implementation uses sampling with a precision of 10−5 , so 10−5 is the smallest p-value we can observe. 625 5 Conclusion distributional semantic word representations (Herbelot et al., 2012), natural language inference data (Rudinger et al., 2017), and facial recognition systems (Buolamwini and Gebru, 2018), as well as at the intersection of dialect and gender in automatic speech recognition (Tatman, 2017). We advocate for further consideration of intersectionality in future work in order to avoid reproducing the erasure of multiple minorities who are most vulnerable to bias. We have developed a simple sentence-level extension of an established word embedding bias instrument and used it to measure the degree to which pretrained sentence encoders capture a range of social biases, observing a large number of significant effects as well as idiosyncrasies suggesting limited external validity. This study is preliminary and leaves open to investigation several design choices that may impact the resul"
N19-1063,D15-1075,1,\N,Missing
N19-1063,P15-1162,0,\N,Missing
N19-1063,N18-1101,1,\N,Missing
N19-3002,P15-1073,0,0.0250391,"ndow. As shown in the results above, we see that the standard deviation (σ), absolute mean (µ) and slope of regression (β) reduce for smaller λ relative to those in training data and then increase with λ to match the variance in the original corpus. This holds for the experiments conducted with fixed context window as well as with exponential weightings. Analysis and Discussion We consider a text corpus to be biased when it has a skewed distribution of words cooccuring with one gender vs another. Any dataset that has such demographic bias can lead to (potentially unintended) social exclusion (Hovy, 2015). PTB and WikiText-2 consist of news articles related to business, science, politics, and sports. These are all male dominated fields. However, CNN/Daily Mail consists of articles across diverse set of categories like entertainment, health, travel etc. Among the three corpora, Penn Treebank has more frequent mentions of male words with respect to female words and CNN/Daily Mail has the least. As defined, bias score of zero implies perfectly neutral word, any value higher/lower implies female/male bias. Therefore, the absolute value of bias score signifies presence of bias. Overall bias in a da"
N19-3002,P14-2050,0,0.086125,"Missing"
N19-3002,W19-3821,0,0.0792925,"Missing"
N19-3002,P13-1162,0,0.0357594,"rage and reinforce harmful stereotypes, or distort the truth. Automated systems that depend on these models can take problematic actions based on biased profiling of individuals. The National Institute for Standards and Technology (NIST) evaluated several facial recognition algorithms and found that they are systematically biased based on gender (Ngan and Grother, 2015). Algorithms performed worse on faces labeled as female than those labeled as male. 2 Related Work A number of methods have been proposed for evaluating and addressing biases that exist in datasets and the models that use them. Recasens et al. (2013) studies the neutral point of view (NPOV) edit tags in the Wikipedia edit histories to understand linguistic realization of bias. According to their study, bias can be broadly categorized into two classes: framing and epistemological. While the framing bias is more explicit, the epistemological bias is implicit and subtle. Framing bias occurs when subjective or one-sided words are used. For example, in the 7 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 7–15 c Minneapolis, Minnesota, June 3 -"
N19-3002,N18-2002,0,0.197905,"Missing"
N19-3002,D17-1323,0,0.379039,"gender bias present in popular word embeddings, such as word2vec (Mikolov et al., 2013). They construct a gender subspace using a set of binary gender pairs. For words that are not explicitly gendered, the component of the word embeddings that project onto this subspace can be removed to debias the embeddings in the gender direction. They also propose a softer variation that balances reconstruction of the original embeddings while minimizing the part of the embeddings that project onto the gender subspace. We use the softer variation to debias the embeddings while training our language model. Zhao et al. (2017) look at gender bias in the context of using structured prediction for visual object classification and semantic role labeling. They ob1 It is Winograd Schema-style coreference dataset consisting of pair of sentences that differ only by a gender pronoun 8 However, the cosine similarity between sentences can be an inadequate measure of text similarity in sentences. In this paper, we attempt to minimize the cosine similarity between word embeddings and gender direction. Gonen and Goldberg (2019) conduct experiments using the debiasing techniques proposed by Bolukbasi et al. (2016) and Zhao et al"
N19-3002,D18-1521,0,0.293146,"17) conduct Word Embedding Association Test (WEAT). It is based on the hypothesis that word embeddings closer together in high dimensional space are semantically closer. They find strong evidence of social biases in pretrained word embeddings. Rudinger et al. (2018) introduce Winogender schemas1 and evaluate three coreference resolution systems—rule-based, statistical and neural systems. They find that these systems’ predictions strongly prefer one gender over the other for occupations. Font and Costa-Juss`a (2019) study the impact of gender debiasing techniques by Bolukbasi et al. (2016) and Zhao et al. (2018) in machine translation. They find these methods to be effective, and even a noted BLEU score improvement for the debiased model. Our work is closely related but while they use debiased pretrained embeddings, we train the word embeddings from scratch and debias them while the language model is trained. May et al. (2019) extend WEAT to state-ofthe-art sentence encoders: the Sentence Encoder Association Test (SEAT). They show that these tests can provide an evidence for presence of bias. sentence—“Usually, smaller cottage-style houses have been demolished to make way for these McMansions.”, the"
N19-3002,J93-2004,0,\N,Missing
N19-3002,W19-3621,0,\N,Missing
N19-3002,N19-1061,0,\N,Missing
N19-5002,D15-1075,1,0.734119,"et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the relationships between the truth-conditional meanings of two sentences or, in other words, decide whether one sentence follows from another. This task neatly isolates the core NLP problem of sentence understanding as a classification problem, and also offers promise as an intermediate step in the building of complex systems (Dagan et al., 2005; MacCartney, 2009; Bowman et al., 2015). The last few years have seen fast progress in NLI, with the introduction of a few large training datasets and many popular evaluation sets as well as an explosion of new model architectures and methods for using unlabeled data and outside knowledge. This tutorial will layout the motivations for work on NLI, survey the available resources for the task, and present highlights from recent research showing us what NLI can teach us about the capabilities and limits of deep learning models for language understanding and reasoning. The tutorial will start from a brief discussion on the motivations"
N19-5002,C18-1154,1,0.852207,"that utilize crosssentence statistics (Bowman et al., 2015; Chen et al., 2017a, 2018b; Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the"
N19-5002,P18-1224,1,0.831291,"that utilize crosssentence statistics (Bowman et al., 2015; Chen et al., 2017a, 2018b; Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the"
N19-5002,P17-1152,1,0.613604,"sentence-embeddingbased modeling (Bowman et al., 2015; Chen et al., 2017b, 2018a; Williams et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018) and deep-learning approaches that utilize crosssentence statistics (Bowman et al., 2015; Chen et al., 2017a, 2018b; Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize t"
N19-5002,D18-1176,0,0.0229797,"Missing"
N19-5002,C18-1198,0,0.0252854,"ion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the relationships between the truth-conditional meanings of two sentences or, in other words, decide whether one sentence follows from another. This task neatly isolates the core NLP problem of sentence understanding as a classification problem, and also offers prom"
N19-5002,N18-1202,0,0.0446999,"; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the relationships between the truth-conditional meanings of two sentences or, in other words, decide whether one se"
N19-5002,S18-2023,0,0.0586066,"Missing"
N19-5002,N18-1101,1,0.81029,"wledge. This tutorial will layout the motivations for work on NLI, survey the available resources for the task, and present highlights from recent research showing us what NLI can teach us about the capabilities and limits of deep learning models for language understanding and reasoning. The tutorial will start from a brief discussion on the motivations for NLI, problem definitions, and typical conventional approaches (Dagan et al., 2013; MacCartney, 2009; Iftene and Balahur-Dobrescu, 2007). Critical to the recent advance on NLI, the creation of larger annotated datasets (Bowman et al., 2015; Williams et al., 2018; Conneau et al., 2018) has made it feasible to train complex models that need to estimate a large number of parameters. The tutorial will present detailed discussion on the available datasets as well as the motivations for and insights from developing these datasets. Then based on more recent research on annotation artifacts, we will extend the discussion to what we should or shouldn’t take away from the current datasets. We will then focus on the cutting-edge deep learning models for NLI. We start from two basic 2 Tutorial Outline • Introduction • Background ◦ Problem definition ◦ Motivation"
P16-1139,D15-1075,1,0.173704,"volutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466–1477, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics stack the cat the cat composition composition reduce shift transition tracking transition tracking sat down buffer the cat sat tracking down sat down (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. ‘Tracking’, ‘transition’, and ‘composition’ are neural network layers. Gray arrows indicate connections which are blocked by a gating fu"
P16-1139,P15-2142,0,0.0138497,"tecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time."
P16-1139,D14-1082,1,0.425506,"s upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive searc"
P16-1139,D16-1053,0,0.0398047,"Missing"
P16-1139,P15-1033,0,0.0232099,"t test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents"
P16-1139,N16-1024,0,0.0202595,"and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural lan"
P16-1139,P04-1013,0,0.021347,"produces the required parse structure on the fly. This design improves upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of opera"
P16-1139,W03-3017,0,0.0499586,"lated work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003). A shift-reduce parser accepts a sequence of input tokens x = (x 0, . . . , x N −1 ) and consumes transitions a = (a0, . . . , aT −1 ), where each at ∈ {shift, reduce} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states. The parser uses two auxiliary data structures: a s"
P16-1139,P14-1062,0,0.0561508,". This component, the sentence encoder, is generally formulated as a learned parametric function from a sequence of word vectors to a sentence vector, and this function can take a range of different forms. Common sentence encoders include sequencebased recurrent neural network models (RNNs, see Figure 1a) with Long Short-Term Memory (LSTM, first two authors contributed equally. cat cat (a) A conventional sequence-based RNN for two sentences. Introduction ∗ The old the Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of t"
P16-1139,Q16-1032,0,0.0243001,"Missing"
P16-1139,D15-1278,0,0.0436106,"sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466–1477, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics stack the cat the cat composition composition reduce shift transition tracking transition tracking sat down buffer the cat sat tracking down sat down (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. ‘Tracking’, ‘transition’, and ‘composition’ are neural network layers. Gray arrows indicate connections which are b"
P16-1139,P16-2022,0,0.197438,"Missing"
P16-1139,P83-1017,0,0.426606,"rsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003). A shift-reduce parser accepts a sequence of input tokens x = (x 0, . . . , x N −1 ) and consumes transitions a = (a0, . . . , aT −1 ), where each at ∈ {shift, reduce} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states. The parser uses two auxiliary data s"
P16-1139,D11-1014,1,0.416432,"Missing"
P16-1139,P15-1150,1,0.752904,"Missing"
P16-1139,N16-1170,0,0.0699057,"Missing"
P16-1139,N16-1035,0,0.0142436,"ion for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence"
P16-1139,D14-1162,1,\N,Missing
P16-1139,W07-2218,0,\N,Missing
P19-1439,E17-2026,0,0.0800118,"ng and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as Collobert et al. (2011). Luong et al. (2016) show promising results combining translation and parsing; Subramanian et al. (2018) benefit from multitask learning in sentence-to-vector encoding; and Bingel and Søgaard (2017) and Changpinyo et al. (2018) offer studies of when multitask learning is helpful for lower-level NLP tasks. 3 Task |Train| Task Type GLUE Tasks CoLA SST MRPC QQP STS MNLI QNLI RTE WNLI 8.5K 67K 3.7K 364K 7K 393K 105K 2.5K 634 acceptability sentiment paraphrase detection paraphrase detection sentence similarity NLI QA (NLI) NLI coreference resolution (NLI) Outside Tasks Transfer Paradigms DisSent WT LM WT LM BWB MT En-De MT En-Ru Reddit SkipThought We consider two recent paradigms for transfer learning: pretraining and intermediate training. See Figure 1 for a graphical depiction. Pretraining"
P19-1439,S17-2001,0,0.0228104,"line is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train langua"
P19-1439,C18-1251,0,0.0459627,"Missing"
P19-1439,D14-1082,0,0.0210975,"Missing"
P19-1439,D17-1070,0,0.20309,"bservations suggest that while scaling up LM pretraining (as in Radford et al., 2019) is likely the most straightforward path to further gains, our current methods for multitask and transfer learning may be substantially limiting our results. 2 Related Work Work on reusable sentence encoders can be traced back at least as far as the multitask model of Collobert et al. (2011). Several works focused on learning reusable sentence-to-vector encodings, where the pretrained encoder produces a fixed-size representation for each input sentence (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017). More recent reusable sentence encoders such as CoVe (McCann et al., 2017) and GPT (Radford et al., 2018) instead represent sentences as sequences of vectors. These methods work well, but most use distinct pretraining objectives, and none offers a substantial investigation of the choice of objective like we conduct here. We build on two methods for pretraining sentence encoders on language modeling: ELMo and BERT. ELMo consists of a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), the hidden states of which are used to produce a contextual vector representation for each token in"
P19-1439,N19-1423,0,0.616052,"ond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning"
P19-1439,I05-5002,0,0.0294281,"a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample"
P19-1439,N16-1162,0,0.0469202,"Missing"
P19-1439,J07-3004,0,0.0375747,"Missing"
P19-1439,P18-1031,0,0.0247575,"(left), and learn a target task model on top of the representations it produces (right). Middle (intermediate ELMo training): We train a BiLSTM on top of ELMo for an intermediate task (left). We then train a target task model on top of the intermediate task BiLSTM and ELMo (right). Bottom (intermediate BERT training): We fine-tune BERT on an intermediate task (left), and then fine-tune the resulting model again on a target task (right). limitation has prompted interest in pretraining for these encoders: The encoders are first trained on outside data, and then plugged into a target task model. Howard and Ruder (2018), Peters et al. (2018a), Radford et al. (2018), and Devlin et al. (2019) establish that encoders pretrained on variants of the language modeling task can be reused to yield strong performance on downstream NLP tasks. Subsequent work has homed in on language modeling (LM) pretraining, finding that such mod4465 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4465–4476 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics els can be productively fine-tuned on intermediate tasks like natural language inference bef"
P19-1439,P15-1162,0,0.0816432,"Missing"
P19-1439,N18-1038,0,0.0431129,"Missing"
P19-1439,P19-1441,0,0.176928,"sed to produce a contextual vector representation for each token in the inputted sequence. ELMo is adapted to target tasks by freezing the model weights and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically inves"
P19-1439,J93-2004,0,0.0711626,"Missing"
P19-1439,P19-1442,0,0.0239473,"n WMT14 English-German (Bojar et al., 2014) and WMT17 English-Russian (Bojar et al., 2017). We train SkipThought-style sequence-to-sequence (seq2seq) models to read a 1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 2 QNLI has been re-released with updated splits since the original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP"
P19-1439,N18-1202,0,0.787227,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,W19-4302,0,0.0321696,"sed to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as"
P19-1439,D18-1179,0,0.247572,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,P18-2124,0,0.0384363,"Task Model BiLSTM Pretrained BiLSTM Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BiLSTM Intermediate Task-Trained BiLSTM ELMo Introduction State-of-the-art models in natural language processing (NLP) often incorporate encoder functions which generate a sequence of vectors intended to represent the in-context meaning of each word in an input text. These encoders have typically been trained directly on the target task at hand, which can be effective for data-rich tasks and yields human performance on some narrowlydefined benchmarks (Rajpurkar et al., 2018; Hassan et al., 2018), but is tenable only for the few tasks with millions of training data examples. This ∗ This paper supercedes “Looking for ELMo’s Friends: Sentence-Level Pretraining Beyond Language Modeling”, an earlier version of this work by the same authors. Correspondence to: alexwang@nyu.edu ❄ ❄ ❄ ELMo ❄ Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BERT Intermediate Task-Trained BERT Input Text Input Text Figure 1: Learning settings that we consider. Model components with frozen parameters are shown in gray and decorated"
P19-1439,P16-2022,0,0.0140103,"sion classifier. For seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset) we replace the classifier with a single-layer LSTM word-level decoder and initialize the hidden state with the [CLS] representation. For ELMo-style models, we use several model types: • Single-sentence classification tasks: We train a linear projection over the output states of the encoder, max-pool those projected states, and feed the result to an MLP. 4469 • Sentence-pair tasks: We perform the same steps on both sentences and use the heuristic feature vector [h1 ; h2 ; h1 · h2 ; h1 − h2 ] in the MLP, following Mou et al. (2016). When training target-task models on QQP, STS, MNLI, and QNLI, we use a cross-sentence attention mechanism similar to BiDAF (Seo et al., 2017). We do not use this mechanism in other cases as early results indicated it hurt transfer performance. • Seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset): We use a single-layer LSTM decoder where the hidden state is initialized with the pooled input representation. • Language modeling: We follow ELMo by concatenating forward and backward models and learning layer mixing weights. To use GLUE tasks for pretraining or intermediate training in a"
P19-1439,D13-1170,0,0.00874927,", 2019b). Accordingly, our pretraining and intermediate ELMo experiments include a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et"
P19-1439,W17-2625,0,0.0590097,"Missing"
P19-1439,P19-1452,1,0.863686,"lar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced b"
P19-1439,I17-1100,1,0.867491,"Missing"
P19-1439,N18-1101,1,0.826868,"om the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train language models on two datasets: WikiText-103 (WT; Merity et al., 2017) and Billion Word La"
P19-1439,1983.tc-1.13,0,0.144827,"Missing"
P19-1439,W18-3022,0,0.0192405,"he original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP (Gardner et al., 2017) and on a public PyTorch implementation of BERT.4 Appendix A presents additional details. Encoder Architecture For both the pretraining and intermediate ELMo experiments, we process words using a pretrained character-level convolutional neural network (CNN) f"
P19-1439,W18-5448,1,0.914222,"s and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP"
P19-1439,Q17-1027,1,0.847544,"Missing"
P19-1449,S17-2001,0,0.0390889,"Missing"
P19-1449,N18-1202,0,0.0253722,"s and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the datapoor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding. 1 Introduction This past year has seen tremendous progress in building general purpose models that can learn good language representations across a range of tasks and domains (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019; Howard and Ruder, 2018; Liu et al., 2019). Reusable models like these can be readily adapted to different language understanding tasks and genres. The General Language Understanding Evaluation (GLUE; Wang et al., 2019b) benchmark is designed to evaluate such models. GLUE is built around nine sentence-level natural language understanding (NLU) tasks and datasets, including instances 3 Dept. of Computer Science New York University of natural language inference, sentiment analysis, acceptability judgment, sentence similarity, and common sense reasoning. The recent BigBird m"
P19-1449,I05-5002,0,\N,Missing
P19-1449,W07-1401,0,\N,Missing
P19-1449,D13-1170,0,\N,Missing
P19-1449,P18-2124,0,\N,Missing
P19-1449,N19-1423,0,\N,Missing
P19-1449,N18-1101,1,\N,Missing
Q18-1019,P17-1080,0,0.0392529,"ata, 2018; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation; additionally, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering. These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion of syntactic constituency. stack the cat the cat composition REDUCE tracking buffer composition the cat sat SHIFT transition tracking sat down sat down"
Q18-1019,D15-1075,1,0.923009,"rds can be assigned two different, yet equally valid constituency structures reflecting the different interpretations for the string. Constituency can be straightforwardly expressed using unlabeled parse trees like the ones used in TreeRNNs, and expressing constituency information is generally thought to be the primary motivation for using trees in TreeRNNs. In this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al. (2018) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2018), and evaluate the results quantitatively and qualitatively with a focus on four 254 issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles. We confirm that both types of models succeed at producing usable sentence representations, but find that only the stronger of the two models—that of Choi et al. (2018)—o"
Q18-1019,P16-1139,1,0.936252,"etations for the sentence in example (1). He swung at the brute with his sword . He swung at the brute with his sword . (b) Parses generated by at ST-Gumbel model (left) and the Stanford Parser (right). Figure 1: Examples of unlabeled binary parse trees. Introduction Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016). Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce ∗ Now at eBay, Inc. parse trees that they then consume. Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification. These mode"
Q18-1019,P95-1031,0,0.565098,"e regularities, including a preference for shallow trees, a somewhat systematic treatment of negation, and a preference to treat pairs of adjacent words at the edges of a sentence as constituents. 2 Background The work discussed in this paper is closely related to work on grammar induction, in which statistical learners attempt to solve the difficult problem of reconstructing the grammar that generated a corpus of text using only that corpus and, optionally, some heuristics about the nature of the expected grammar. Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on an earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). Naseem and Barzilay (2011) additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here. In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model. They find that the resulting SRL model is more effective than one built on a purely unsupervised grammar induction"
Q18-1019,D11-1005,0,0.0203617,"es, including a preference for shallow trees, a somewhat systematic treatment of negation, and a preference to treat pairs of adjacent words at the edges of a sentence as constituents. 2 Background The work discussed in this paper is closely related to work on grammar induction, in which statistical learners attempt to solve the difficult problem of reconstructing the grammar that generated a corpus of text using only that corpus and, optionally, some heuristics about the nature of the expected grammar. Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on an earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977). Naseem and Barzilay (2011) additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here. In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model. They find that the resulting SRL model is more effective than one built on a purely unsupervised grammar induction system, but that us"
Q18-1019,D11-1018,0,0.0186952,"atent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation; additionally, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering. These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion of syntactic constituency. stack the cat the cat composition REDUCE tracking buffer composition the cat sat SHIFT transition tracking sat down sat down transition tracking down Figure 2: The SPINN model unrolled for two transitions dur"
Q18-1019,P16-1078,0,0.0266954,"). He swung at the brute with his sword . He swung at the brute with his sword . (b) Parses generated by at ST-Gumbel model (left) and the Stanford Parser (right). Figure 1: Examples of unlabeled binary parse trees. Introduction Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016). Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce ∗ Now at eBay, Inc. parse trees that they then consume. Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification. These models are designed to learn grammars—strateg"
Q18-1019,P14-1111,0,0.0596208,"Missing"
Q18-1019,P03-1054,0,0.0964096,"s This paper investigates the behavior of two models: RL-SPINN and ST-Gumbel. Both have been shown to outperform similar models based on supervised parsing, and the two represent substantially different approaches to latent tree learning. SPINN Variants Three of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016). Figure 2 shows and describes the architecture. In the base SPINN model, all model components are used, and the transition classifier is trained on binarized Penn Treebank-style parses from the Stanford PCFG Parser (Klein and Manning, 2003), which are included with SNLI and MultiNLI. These binary-branching parse trees are converted to SHIFT / REDUCE sequences for use in the model through a simple reversible transformation. RL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss. Because this component produces discrete decisions, the REINFORCE algorithm (with the standard exponential moving average baseline) is used to supply gradients for it. We explore"
Q18-1019,Q16-1037,0,0.083703,"ky et al., 2012; Kim et al., 2017; Liu and Lapata, 2018; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation; additionally, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering. These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion of syntactic constituency. stack the cat the cat composition REDUCE tracking buffer composition the cat"
Q18-1019,Q18-1005,0,0.0414999,"esent a model (which we call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al. (2017), but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016). This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al. (2017). They report substantial gains in both speed and accuracy over Maillard et al. (2017) and Yogatama et al. (2017) on SNLI. Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2018; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov"
Q18-1019,E17-1002,0,0.314034,"e call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al. (2017), but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016). This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al. (2017). They report substantial gains in both speed and accuracy over Maillard et al. (2017) and Yogatama et al. (2017) on SNLI. Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2018; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translat"
Q18-1019,D12-1074,0,0.0441555,"han RL-SPINN on SNLI. Choi et al. (2018) present a model (which we call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al. (2017), but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016). This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al. (2017). They report substantial gains in both speed and accuracy over Maillard et al. (2017) and Yogatama et al. (2017) on SNLI. Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2018; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. ("
Q18-1019,D12-1077,0,0.024009,"roposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation; additionally, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering. These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion of syntactic constituency. stack the cat the cat composition REDUCE tracking buffer composition the cat sat SHIFT transition tracking sat down sat down transition tracking down Figure 2: The SPINN model"
Q18-1019,D14-1162,0,0.0900215,"ftmax classifier that selects one of the labels entailment, neutral, and contradiction for the pair. Model 100D LSTM (Yogatama) 100D TreeLSTM (Yogatama) 300D SPINN (Bowman) 300D SPINN-PI-NT (Bowman) 300D BiLSTM (Williams) https://github.com/nyu-mll/spinn/tree/ is-it-syntax-release 258 80.2 78.5 83.2 80.9 81.5 – – – – 67.5 Prior Work: Latent Tree Learning 100D RL-SPINN (Yogatama) 100D Soft Gating (Maillard) 100D ST-Gumbel (Choi) w/o Leaf LSTM 300D ST-Gumbel (Choi) w/o Leaf LSTM 600D ST-Gumbel (Choi) We use GloVe vectors to represent words (standard 300D, 840B word package, without fine tuning; Pennington et al., 2014), and feed them into a singlelayer 2 × 300D bi-directional GRU RNN (based on the leaf LSTM of Choi et al. (2018)) to give the models access to local context information when making parsing decisions. To understand the impact of this component, we follow Choi et al. (2018) in also training each model with the leaf GRU replaced with a simpler context-insensitive input encoder that simply multiplies each GloVe vector by a matrix. We find that these models perform best when the temperature of the ST-Gumbel distribution is a trained parameter, rather than fixed at 1.0 as in Choi et al. (2018). 1 MN"
Q18-1019,P16-2022,0,0.0231742,"used in this work. Because the models under study produce and consume binary-branching constituency trees without labels (and because such trees are already included with SNLI and MultiNLI), we use the Stanford Parser’s CollapseUnaryTransformer and TreeBinarizer tools to convert these Penn Treebank Trees to this form. Sentence Pair Classification Because our textual entailment task requires a model to classify pairs of sentences, but the models under study produce vectors for single sentences, we concatenate the two sentence vectors, their difference, and their elementwise product (following Mou et al., 2016), and feed the result into a 1024D ReLU layer to produce a representation for the sentence pair. This representation is fed into a three-way softmax classifier that selects one of the labels entailment, neutral, and contradiction for the pair. Model 100D LSTM (Yogatama) 100D TreeLSTM (Yogatama) 300D SPINN (Bowman) 300D SPINN-PI-NT (Bowman) 300D BiLSTM (Williams) https://github.com/nyu-mll/spinn/tree/ is-it-syntax-release 258 80.2 78.5 83.2 80.9 81.5 – – – – 67.5 Prior Work: Latent Tree Learning 100D RL-SPINN (Yogatama) 100D Soft Gating (Maillard) 100D ST-Gumbel (Choi) w/o Leaf LSTM 300D ST-Gum"
Q18-1019,D11-1014,0,0.215104,"m restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of. 1 (a) Two parse trees correspond to two distinct interpretations for the sentence in example (1). He swung at the brute with his sword . He swung at the brute with his sword . (b) Parses generated by at ST-Gumbel model (left) and the Stanford Parser (right). Figure 1: Examples of unlabeled binary parse trees. Introduction Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016). Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce ∗ Now at eBay, Inc. parse trees that they then consume. Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the d"
Q18-1019,E17-1038,0,0.174167,"e call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al. (2017), but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016). This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al. (2017). They report substantial gains in both speed and accuracy over Maillard et al. (2017) and Yogatama et al. (2017) on SNLI. Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2018; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models. Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth. Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information. Recent highlights include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translat"
Q18-1019,D13-1170,0,0.0129921,"e trees correspond to two distinct interpretations for the sentence in example (1). He swung at the brute with his sword . He swung at the brute with his sword . (b) Parses generated by at ST-Gumbel model (left) and the Stanford Parser (right). Figure 1: Examples of unlabeled binary parse trees. Introduction Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016). Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce ∗ Now at eBay, Inc. parse trees that they then consume. Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream tas"
Q18-1019,P15-1150,0,0.358764,"Missing"
Q18-1019,N18-1101,1,0.85349,"Missing"
Q19-1040,C18-1152,0,0.134742,"ingh Samuel R. Bowman New York University New York University Facebook AI Research∗ New York University warstadt@nyu.edu bowman@nyu.edu amanpreet@nyu.edu Abstract We train neural networks to perform acceptability judgments—following work by Lawrence et al. (2000), Lau et al. (2016), and others—in order to evaluate their acquisition of the kinds of grammatical concepts linguists identify as central to human linguistic competence. This contributes to a growing effort to test ANNs’ ability to make fine-grained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018). This research program seeks to provide new informative ways to evaluate ANN models popular with engineers. Furthermore, it has the potential to address foundational questions in theoretical linguistics by investigating how well unbiased learners can acquire grammatical knowledge. This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical"
Q19-1040,P04-3031,0,0.229011,"iminaries Language model We use an LSTM language model (LSTM LM) at various stages in our experiments: (i) Several of our models use word embeddings or hidden states from the LM as input. (ii) The LM generates fake data for the real/fake task. (iii) The LM is an integral part of our implementation of the method proposed by Lau et al. (2016). We train the LM on the 100 million-token British National Corpus (BNC). It learns word embeddings from scratch for the 100k most frequent words in the BNC (with out of vocabulary words replaced by &lt;unk&gt;). We lowercase and tokenize the BNC data using NLTK (Bird and Loper, 2004). The LM achieves a word-level perplexity of 56.1 on the BNC. Word representations We experiment with three styles of word representations: (i) We train a set of conventional fixed word embeddings as part of the training of the LM described above, which we refer to as BNC embeddings. (ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent wi as a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM. (iii) We also use the pretrained 300-dimensional ("
Q19-1040,W16-2524,0,0.0292459,"ery high frequency. Sentences with binding and violations, including morphological violations, are among the hardest. We also find that our models perform poorly on sentences with question-like syntax. This difficulty is likely due to long-distance dependencies in these sentences. 635 Here, we run additional evaluations to probe whether our models can reliably classify sets of sentences that target a single grammatical contrast. This kind of evaluation can give insight into what kinds of grammatical features our models do and do not acquire easily. Using data generation techniques inspired by Ettinger et al. (2016), we build five auxiliary data sets (described below) using simple rewrite grammars which target specific grammatical contrasts. Unlike in CoLA, none of these judgments are meant to be difficult or subtle, and we expect that most humans could reach perfect accuracy. We also take care to make the test sentences as simple as possible to reduce classification errors unrelated to the target contrast. Specifically, we limit noun phrases to 1 or 2 words and use semantically related vocabulary items within examples. Subject-verb-object This test set consists of 100 triples of subject, verb, and objec"
Q19-1040,D17-1070,0,0.176622,"er points out, this may be due to the fact that Sprouse et al. measure agreement with minimal pairs of sentences using a forced choice task, which is more constrained and arguably easier than single sentence judgments. 4 Experiments We train several semi-supervised neural network models to do acceptability classification on CoLA. At 10k sentences, CoLA is likely too small to train a low-bias learner like a recurrent neural 630 network without additional prior knowledge. In similar low-resource settings, transfer learning with sentence embeddings has proven to be effective (Kiros et al., 2015; Conneau et al., 2017). Our best model uses a transfer learning approach in which a large sentence encoder is trained on an unsupervised real/fake discrimination objective, and a lightweight multilayer perceptron classifier is trained on top to do acceptability classification over CoLA. It also uses contexualized word embeddings inspired by ELMo (Peters et al., 2018). We compare our models to a continuous bag of words (CBOW) baseline, the unsupervised models proposed by Lau et al. (2016), and human performance. To make these comparisons more meaningful, we avoid giving our models distinct advantages over human lear"
Q19-1040,P18-1198,0,0.0535481,"Missing"
Q19-1040,P14-2029,0,0.0365007,"ith Boolean acceptability judgments from themselves or native speakers. 2.2 The Acceptability Classification Task Although acceptability classification has been explored previously in computational linguistics, there is no standard approach to this task. Following common practice in generative linguistics, our study focuses on the Boolean acceptability classification task. This approach is also taken in earlier computational work on this task (Lawrence et al., 2000; Wagner et al., 2009; Linzen et al., 2016). By contrast, other computational work aims to model gradient acceptability judgments (Heilman et al., 2014; Lau et al., 2016). Though Lau et al. argue that acceptability judgments are gradient in nature, we consider Boolean judgments in published examples sufficient for our purposes, since linguists generally design these examples to be unambiguously acceptable or unacceptable. Data sets for acceptability classification require a source of unacceptable sentences, which are not generally found in naturalistic speech or writing by native speakers. The sentences in CoLA consist entirely of examples from the linguistics literature. Lawrence et al. (2000) and Lau et al. (2016) build data sets similar i"
Q19-1040,D14-1162,0,0.0819912,"evel perplexity of 56.1 on the BNC. Word representations We experiment with three styles of word representations: (i) We train a set of conventional fixed word embeddings as part of the training of the LM described above, which we refer to as BNC embeddings. (ii) We train ELMo-style contextualized word embeddings, which, following ELMo (Peters et al., 2018), represent wi as a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from the original paper by using only a forward LM. (iii) We also use the pretrained 300-dimensional (6B) GloVe embeddings from Pennington et al. (2014).12 Real/fake auxiliary task We train sentence encoders on a real/fake task in which the objective is to distinguish real sentences from the BNC and ‘‘fake’’ English sentences automatically generated by two strategies: (i) We sample strings (2-a) from the LSTM LM. (ii) We manipulate sentences of the BNC (2-b) by randomly permuting a subset of the words, keeping the other words in situ. Training data includes the entire BNC and an equal amount of fake data. We lowercase and tokenize all real/fake data and replace out of vocabulary words as in LM training. (2) a. either excessive tenure does not"
Q19-1040,N18-1202,0,0.782599,", CoLA is likely too small to train a low-bias learner like a recurrent neural 630 network without additional prior knowledge. In similar low-resource settings, transfer learning with sentence embeddings has proven to be effective (Kiros et al., 2015; Conneau et al., 2017). Our best model uses a transfer learning approach in which a large sentence encoder is trained on an unsupervised real/fake discrimination objective, and a lightweight multilayer perceptron classifier is trained on top to do acceptability classification over CoLA. It also uses contexualized word embeddings inspired by ELMo (Peters et al., 2018). We compare our models to a continuous bag of words (CBOW) baseline, the unsupervised models proposed by Lau et al. (2016), and human performance. To make these comparisons more meaningful, we avoid giving our models distinct advantages over human learners by limiting the training data in two ways: (i) Aside from acceptability labels, our training has no grammatical annotation. (ii) Our large sentence encoders are limited to 100–200 million tokens of training data, which is within a factor of ten of the number of tokens human learners are exposed to during language acquisition (Hart and Risle"
Q19-1040,Q16-1037,0,0.439438,"ral Network Acceptability Judgments Alex Warstadt Amanpreet Singh Samuel R. Bowman New York University New York University Facebook AI Research∗ New York University warstadt@nyu.edu bowman@nyu.edu amanpreet@nyu.edu Abstract We train neural networks to perform acceptability judgments—following work by Lawrence et al. (2000), Lau et al. (2016), and others—in order to evaluate their acquisition of the kinds of grammatical concepts linguists identify as central to human linguistic competence. This contributes to a growing effort to test ANNs’ ability to make fine-grained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018). This research program seeks to provide new informative ways to evaluate ANN models popular with engineers. Furthermore, it has the potential to address foundational questions in theoretical linguistics by investigating how well unbiased learners can acquire grammatical knowledge. This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (C"
Q19-1040,D18-1151,0,0.465011,"ew York University New York University Facebook AI Research∗ New York University warstadt@nyu.edu bowman@nyu.edu amanpreet@nyu.edu Abstract We train neural networks to perform acceptability judgments—following work by Lawrence et al. (2000), Lau et al. (2016), and others—in order to evaluate their acquisition of the kinds of grammatical concepts linguists identify as central to human linguistic competence. This contributes to a growing effort to test ANNs’ ability to make fine-grained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettinger et al., 2018; Marvin and Linzen, 2018). This research program seeks to provide new informative ways to evaluate ANN models popular with engineers. Furthermore, it has the potential to address foundational questions in theoretical linguistics by investigating how well unbiased learners can acquire grammatical knowledge. This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from publ"
Q19-1040,D16-1159,0,0.080226,"Missing"
Q19-1040,W18-5423,0,0.0345826,"(intended: John loves himself.) Prepositions are good to end sentences with. *This train is arrivable. Table 1: Our informal classification of unnacceptable sentences, shown with their presence or absence in CoLA. from plausible scenarios only with access to realworld knowledge unrelated to grammar. judgment has been taken as a standard for replicability of reported judgments in syntax articles (Sprouse and Almeida, 2012; Sprouse et al., 2013; Linzen and Oseki, 2018). It is also increasingly used in computational linguistics (Linzen et al., 2016; Marvin and Linzen, 2018; Futrell et al., 2018; Wilcox et al., 2018, 2019). This task is often used to evaluate language models because the outputted probabilities for a pair of minimally different sentences are directly comparable, while the output for a single sentence cannot be taken as a measure of acceptability without some kind of normalization (Lau et al., 2016). We leave a comparison of this methodology with our own for future work. We settle on the single-sentence judgment task because it is directly comparable with methodology in generative linguistics. Although some work in theoretical linguists presents acceptability judgments as a ranking of two"
Q19-1040,N19-1334,0,0.0503022,"Missing"
Q19-1040,W18-5446,1,0.854099,"tion for Computational Linguistics, vol. 7, pp. 625–641, 2019. https://doi.org/10.1162/tacl a 00290 Action Editor: Alexander Clark. Submission batch: 2/2019; Revision batch: 6/2019; Published 9/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  Resources CoLA can be downloaded from the corpus Web site.2 The code for training our baselines is available as well.3 There are also two competition sites for evaluating acceptability classifiers on CoLA’s in-domain4 and out-ofdomain5 test sets (unlabeled). Finally, CoLA is included in the GLUE benchmark6 (Wang et al., 2018), which also hosts CoLA training data, unlabeled test data, and a leaderboard. 2 Acceptability Judgments 2.1 In Linguistics Our investigation of acceptability classification builds on decades of established scientific knowledge in generative linguistics, where acceptability judgments are studied extensively. In his foundational work on generative syntax, Chomsky (1957) defines an empirically adequate grammar of a language L as one that generates all and only those strings of L which native speakers of L judge to be acceptable. Evaluating grammatical theories against native speaker judgments ha"
S19-1026,J99-2004,0,0.0608644,"Missing"
S19-1026,W15-2712,0,0.0230209,"cture constant. We ask whether the linguistic properties implicitly captured by pretraining objectives measurably affect the types of linguistic information encoded in the learned representations. To this end, we explore whether qualitatively different objectives lead to demonstrably different sentence representations. We focus our analysis on function words because they play a key role in compositional meaning—e.g., introducing and identifying discourse referents or representing relationships between entities or ideas—and are not yet considered to be well-modeled by distributional semantics (Bernardi et al., 2015). Our results suggest that different pretraining objectives give rise to differences in function word comprehension; for instance, we see that natural language inference helps understanding negation, and CCG supertagging helps recognizing meaningful sentence boundaries. However, overall, we find that the observed differences are not always straightforwardly interpretable, and further investigation is needed to determine which specific aspects of pretraining tasks yield good representations of function words. The analyses we present contribute new results in an ongoing line of research aimed at"
S19-1026,N19-1423,0,0.201655,"n of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguistic properties implicitl"
S19-1026,W16-2524,0,0.0505528,"aining and probing datasets. A regression analysis shows that vocabulary overlap overall is not a significant predictor of performance on the probing set (p = 0.39). No single probing set performance was significantly affected by vocabulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use simil"
S19-1026,W17-5401,0,0.0333637,"Missing"
S19-1026,W18-5426,0,0.050884,"bulary overlap either (all p > .05 after Bonferroni correction for multiple comparisons). Figure 2: Prediction overlap on the probing tasks for models trained on different pretraining tasks (i.e., how often models make identical predictions on a particular probing set). 5 Related Work An active line of work focuses on “probing” neural representations of language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper compr"
S19-1026,N18-1038,0,0.0609676,"Missing"
S19-1026,W17-5405,0,0.026707,"Missing"
S19-1026,C18-1198,0,0.038925,"probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentencec"
S19-1026,N18-1202,0,0.406307,"elps the comprehension of negation. 1 Introduction Many recent advances in NLP have been driven by new approaches to representation learning— i.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications (Bowman et al., 2017). Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). This paper investigates the role of pretraining objectives of sentence encoders, with respect to ∗ Corresponding authors: Najoung Kim (n.kim@jhu.edu), Ellie Pavlick (ellie pavlick@brown.edu) their capacity to understand function words (e.g., prepositions, conjunctions). Although the importance of finding an effective pretraining objective for learning better (or more generalizable) representations is well acknowledged, relatively few studies offer a controlled comparison of diverse pretraining objectives, holding model architecture constant. We ask whether the linguisti"
S19-1026,N18-2082,1,0.879934,"Missing"
S19-1026,W18-5441,1,0.897134,"Missing"
S19-1026,P18-1079,0,0.0486921,"language. Ettinger et al. (2016, 2017); Zhu et al. (2018), i.a., use a task-based approach similar to ours, where tasks that require a specific subset of linguistic knowledge are used to perform qualitative evaluation. Gulordava et al. (2018), Giulianelli et al. (2018), Rønning et al. (2018), and Jumelet and Hupkes (2018) make a focused contribution towards a particular linguistic phenomenon (agreement, ellipsis, negative polarity). Using recast NLI, Poliak et al. (2018a) probe for semantic phenomena in neural machine translation encoders. Stali¯unaite and Bonfil (2017); Mahler et al. (2017); Ribeiro et al. (2018) use similar strategies to our structural mutation method, although their primary goal was to break existing systems by adversarial modifications rather than to compare different models. Ribeiro et al. (2018) and our work both test for proper comprehension of the modified expressions, but our modifications are designed to induce semantic changes whereas their modifications are intended to preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the rec"
S19-1026,W18-5409,0,0.0502653,"Missing"
S19-1026,P18-1018,0,0.0235763,"s none of them correctly (0/6). Here is an example of a numeric usage of below that the NLI model answered correctly but the image model answered incorrectly: P: Only those whose incomes do not exceed 125 percent of the federal poverty level qualify . . . H: Those whose incomes are below 125 percent qualify . . . (P→H) The image model’s bias towards the spatial usage is intuitive, since the numeric usage of below (i.e., as a counterpart to exceed) is difficult to learn from visual clues only. This concrete-abstract duality, which is not specific to below but common to most other prepositions (Schneider et al., 2018), may partially explain why the image-caption model behaves so differently from all other models, which are not trained on a multimodal objective. 4.3 Data Size and Genre Effects As can be seen from the varying sizes of the pretraining dataset reported in Figure 1, seeing more data during pretraining does not imply better performance on probing tasks. Also, as noted before, the fact that pretraining can hurt performance suggests that if the task is not the “right” task, adding more datapoints during pretraining can lead models to learn counterproductive representations. Another potential confo"
S19-1026,W17-5410,0,0.0296726,"Missing"
S19-1026,W17-2625,0,0.063243,"Missing"
S19-1026,W18-5446,1,0.80362,"o preserve the original meaning. Our strategy is close to that of Naik et al. (2018), but our modifications are more constrained and lexically targeted. The design of our NLI-style probing tasks follows the recent line of work which advocates for NLI as a general-purpose format for diagnostic tasks (White et al., 2017; Poliak et al., 2018b). This idea is similar in spirit to McCann et al. (2018), which advocates for question answering as a general-purpose format, to edge probing (Tenney et al., 2019) which probes for syntactic and semantic structures via a common labeling format, and to GLUE (Wang et al., 2018) which aggregates a variety of tasks that share a common sentenceclassification format. The primary difference in our work is that we focus specifically on the understanding of function words in context. We also present a suite of several tasks, but each one focuses on a particular structure, whereas tasks proposed in the works above generally aggregate multiple phenomena. Each of our tasks isolates each function word type and employ a targeted modification strategy that gives us a more narrowlyfocused, informative scope of analysis. 6 Conclusion We propose a new challenge set of nine tasks th"
S19-1026,I17-1100,1,0.878632,"Missing"
S19-1026,N18-1101,1,0.788304,"dex (σ = 2) and rounding to the nearest integer. 2.3 NLI-Based Tasks Our NLI-based probing tasks ask whether the choice of function word affects the inferences licensed by a sentence. These tasks consist of a pair of sentences—a premise p and a hypothesis h— and ask whether or not p entails h. We exploit the label changes induced by a targeted mutation of 3 We use WikiText instead of BWB because adjacent sentences in BWB are not logically contiguous and therefore may not be from the same discourse context. the sentence pairs taken from the Multi-genre Natural Language Inference dataset (MNLI, Williams et al., 2018). The rationale is that, if a change to a single function word in the premise changes the entailment label, that function word must play a significant role in the semantics of the sentence. Prepositions We manually curate a list of prepositions (see Appendix D) that are likely to be swapped with each other without affecting the grammaticality of the sentence. We generate mutated NLI pairs by finding occurrences of the prepositions in our list and randomly replacing them with other prepositions in the list. Our list consists of a set of locatives4 and several other manually-selected preposition"
S19-1026,Q17-1027,1,0.86489,"Missing"
S19-1026,P18-2100,0,0.0613273,"Missing"
silveira-etal-2014-gold,levy-andrew-2006-tregex,0,\N,Missing
silveira-etal-2014-gold,de-marneffe-etal-2006-generating,1,\N,Missing
silveira-etal-2014-gold,de-marneffe-etal-2014-universal,1,\N,Missing
silveira-etal-2014-gold,N06-2015,0,\N,Missing
silveira-etal-2014-gold,W07-1004,0,\N,Missing
silveira-etal-2014-gold,W13-3721,1,\N,Missing
silveira-etal-2014-gold,W03-3017,0,\N,Missing
W13-3721,D11-1037,0,0.0975334,"The SD scheme has been in use for seven years, but still lacks principled analyses of many of the difficult English constructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set o"
W13-3721,W07-2416,0,0.0184095,"staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotations. tactic formalism. For ex"
W13-3721,W06-2920,0,0.0547907,"ructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotatio"
W13-3721,de-marneffe-etal-2006-generating,1,0.178631,"Missing"
W13-3721,P80-1024,0,0.417167,"Missing"
W13-3721,W07-1004,0,0.0560843,"imell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotations. tactic formalism. For example, in (1a), the object of find can be “raised” to subject position in the main clause to form a tough adjective construction, as in (1b). One of the difficulties for generative grammar in modeling this construction is that the object being raised can be embedded arbitrarily deeply in the sentence, as in (1c). introduces a PP experiencer, the PP can “move” separately (4b), both supporting the hypothesis that the experience"
W13-3721,D09-1085,0,0.405499,"erent constructions The SD scheme has been in use for seven years, but still lacks principled analyses of many of the difficult English constructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 20"
W13-3721,W08-1301,1,\N,Missing
W15-4002,W11-0114,0,0.0253397,"ilment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN archite"
W15-4002,S13-1001,0,0.0608655,"Missing"
W15-4002,W13-3209,0,0.0242407,"x=y x ∩ y = ∅ ∧ x ∪ y 6= D x∩y =∅∧x∪y =D x ∩ y 6= ∅ ∧ x ∪ y = D (else) turtle, reptile reptile, turtle couch, sofa turtle, warthog able, unable animal, non-turtle turtle, pet Table 1: The seven relations of MacCartney and Manning (2009)’s logic are defined abstractly on pairs of sets drawing from the universe D, but can be straightforwardly applied to any pair of natural language words, phrases, or sentences. The relations are defined so as to be mutually exclusive. standard NN layer function (1) and those with the more powerful neural tensor network layer function (2) proposed in Chen et al. (2013). The nonlinearity f (x) = tanh(x) is applied elementwise to the output of either layer function.  (l)  ~x (1) ~yTreeRNN = f (M (r) + ~b ) ~x the tree and into the classifier. For an objective function, we use the negative log likelihood of the correct label with tuned L2 regularization. We initialize parameters uniformly, using the range (−0.05, 0.05) for layer parameters and (−0.01, 0.01) for embeddings, and train the model using stochastic gradient descent (SGD) with learning rates computed using AdaDelta (Zeiler, 2012). The classifier feature vector is fixed at 75 dimensions and the dime"
W15-4002,Q14-1006,0,0.029844,"Missing"
W15-4002,P03-1054,1,0.0236939,"Missing"
W15-4002,J13-2005,0,0.00819476,"edu {∗ Dept. of Linguistics, † NLP Group, ‡ Dept. of Computer Science} Stanford University Stanford, CA 94305, USA Abstract the kind of high-fidelity distributed representations proposed in recent algebraic work on vector space modeling (Coecke et al., 2011; Grefenstette, 2013; Hermann et al., 2013; Rockt¨aschel et al., 2014), and whether any such model can match the performance of grammars based in logical forms in their ability to model core semantic phenomena like quantification, entailment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model t"
W15-4002,E12-1004,0,0.0204642,"Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN architecture for natural lang"
W15-4002,W09-3714,1,0.792478,"2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference (and some successful implemented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. In our first three experiments, we test our models’ ability to learn the foundations of natural language inference by training them to reproduce the behavior of the natural logic of MacCartney and Manning (2009) on artificial data. This logic defines seven mutually-exclusive relations of synonymy, entailment, contradiction, and mutual consistency, as sum"
W15-4002,S14-2001,0,0.0219712,"el restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem. 6 The SICK textual entailment challenge The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real natural language data. To investigate our models’ ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus (Marelli et al., 2014b). The corpus consists of about 10k natural language sentence pairs, labeled with entailment, contradiction, or neutral. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to approximate natural logic-style inference in the real world. Adapting to this task requires us to make a few 4 We t"
W15-4002,marelli-etal-2014-sick,0,0.0210269,"el restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem. 6 The SICK textual entailment challenge The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real natural language data. To investigate our models’ ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus (Marelli et al., 2014b). The corpus consists of about 10k natural language sentence pairs, labeled with entailment, contradiction, or neutral. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to approximate natural logic-style inference in the real world. Adapting to this task requires us to make a few 4 We t"
W15-4002,D14-1162,1,0.108463,"Missing"
W15-4002,W14-2409,0,0.126108,"Missing"
W15-4002,D11-1014,1,0.0289584,". In our first set of experiments, we generate artificial data from a logical grammar and use it to evaluate the models’ ability to learn to handle basic relational reasoning, recursive structures, and quantification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 1 Introduction Tree-structured recursive neural network models (TreeRNNs; Goller and Kuchler 1996; Socher et al. 2011b) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis (Socher et al., 2011b; Irsoy and Cardie, 2014), image description (Socher et al., 2014), and paraphrase detection (Socher et al., 2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c B"
W15-4002,D12-1110,1,0.0397056,"scard training examples with more than 4 logical operators, yielding 60k short training examples, and 21k test examples across all 12 bins. In addition to the two tree models, we also train a summing NN baseline which is largely identical to the TreeRNN, except that instead of using a learned composition function, it simply sums the term vectors in each expression to compose them before passing them to the comparison layer. Unlike the two tree models, this baseline does not use word order, and is as such guaranteed to ignore some information that it would need in order to succeed perfectly. 3 Socher et al. (2012) show that a matrix-vector TreeRNN model somewhat similar to our TreeRNTN can learn boolean logic, a logic where the atomic symbols are simply the values T and F. While learning the operators of that logic is not trivial, the outputs of each operator can be represented accurately by a single bit. In the much more demanding task presented here, the atomic symbols are variables over these values, and 6 the sentence vectors must thus be able to distinguish up to 22 distinct conditions on valuations. Results Fig. 3 shows the relationship between test accuracy and statement size. While the summing"
W15-4002,D13-1170,1,0.00982266,"x linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN architecture for natural language inference which independently computes vector representations for each of two sentences using standard TreeRNN or TreeRNTN (Socher et al., 2013) models, and produces a judgment for the pair using only those representations. This allows us to gauge the abilities of these two models to represent all of the necessary semantic Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models— plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)—can correctly learn to ide"
W15-4002,Q14-1017,1,0.0553835,"antification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 1 Introduction Tree-structured recursive neural network models (TreeRNNs; Goller and Kuchler 1996; Socher et al. 2011b) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis (Socher et al., 2011b; Irsoy and Cardie, 2014), image description (Socher et al., 2014), and paraphrase detection (Socher et al., 2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference"
W15-4002,P15-1150,1,0.520584,"s a learned full rank third-order tensor T, of dimension N × N × N , modeling multiplicative interactions between the child vectors. The comparison layer uses the same layer function as the composition layers (either an NN layer or an NTN layer) with independently learned parameters and a separate nonlinearity function. Rather than use a tanh nonlinearity here, we found better results with the leaky rectified linear function (Maas et al., 2013): f (x) = max(x, 0) + 0.01 min(x, 0). Other strong tree-structured models have been proposed in past work (Socher et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015), but we believe that these two provide a valuable case study, and that positive results on here are likely to generalize well to stronger models. To run the model forward, we assemble the two tree-structured networks so as to match the structures provided for each phrase, which are either included in the source data or given by a parser. The word vectors are then looked up from the vocabulary embedding matrix V (one of the learned model parameters), and the composition and comparison functions are used to pass information up 3 Reasoning about semantic relations The simplest kinds of deduction"
W15-4002,J82-3002,0,0.724179,"stopher D. Manning∗†‡ sbowman@stanford.edu cgpotts@stanford.edu manning@stanford.edu {∗ Dept. of Linguistics, † NLP Group, ‡ Dept. of Computer Science} Stanford University Stanford, CA 94305, USA Abstract the kind of high-fidelity distributed representations proposed in recent algebraic work on vector space modeling (Coecke et al., 2011; Grefenstette, 2013; Hermann et al., 2013; Rockt¨aschel et al., 2014), and whether any such model can match the performance of grammars based in logical forms in their ability to model core semantic phenomena like quantification, entailment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). Ho"
W15-4002,C12-1171,0,0.0263234,"couraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference (and some successful implemented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. In our first three experiments, we test our models’ ability to learn the foundations of natural language inference by training them to reproduce the behavior of the natural logic of MacCartney and Manning (2009) on artificial data. This logic defines seven mutually-exclusive relations of synonymy, entailment, contradiction, and mutual consistency, as summarized in Table 1, and"
W15-4002,P11-1060,0,\N,Missing
W17-2610,D16-1241,0,0.0369901,"pread use in evaluating neural networks for language understanding, and the newer and more ∗ Figure 1: The Sequential Attention Model. RNNs first encode the question into a vector j and the document into a sequence of vectors H. For each word index i in the document, a scoring vector γi is then computed from j and hi using a function like the partial bilinear function shown here. These vectors are then used as inputs to another RNN layer, the outputs of which (ηi ) are summed elementwise and used as attention scores (αi ) in answer selection. carefully quality-controlled Who did What dataset (Onishi et al., 2016). In standard approaches to soft attention over passages, a scoring function is first applied to every word in the source text to evaluate how closely These authors contributed equally to this work. 75 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 75–80, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics there has been other work on soft attention over text, largely focusing on the problem of attending over single sentences. Luong et al. (2015) study several issues in the design of soft attention models in the context of translation,"
W17-2610,D14-1162,0,0.0884409,"th 6 billion tokens with an uncased vocab of 400K words, and were obtained from Wikipedia 2014 and Gigaword 5. We then feed the γi vectors into a new bidirectional GRU layer to get the hidden attention ηi vector representation. ← −−, γ ) η−i = GRU(← ηi+1 i (13) Training We implemented all our models in Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) and used the Stanford Reader (Chen et al., 2016) open source implementation as a reference. We largely used the same hyperparameters as Chen et al. (2016) in the Stanford Reader: |V |= 50K, embedding size d = 100, GloVe (Pennington et al., 2014) word embeddings3 for initialization, hidden size h = 128. The size of the hidden layer of the bidirectional RNN used to encode the attention vectors is double the size of the one that encodes the words, since it receives vectors that result from the concatenation of GRUs that go in both directions, η ∈ R256 . Attention and output parameters were In the Sequential Attention model instead of producing a single scalar value αi for each word in the passage by using a bilinear term, we define the vectors γi with a partial-bilinear term1 . Instead of doing the dot product as in the bilinear term, w"
W17-2610,P16-1223,0,0.0461023,"Missing"
W17-2610,D15-1166,0,0.0571852,"tion scores (αi ) in answer selection. carefully quality-controlled Who did What dataset (Onishi et al., 2016). In standard approaches to soft attention over passages, a scoring function is first applied to every word in the source text to evaluate how closely These authors contributed equally to this work. 75 Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 75–80, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics there has been other work on soft attention over text, largely focusing on the problem of attending over single sentences. Luong et al. (2015) study several issues in the design of soft attention models in the context of translation, and introduce the bilinear scoring function. They also propose the idea of attention input-feeding where the original attention vectors are concatenated with the hidden representations of the words and fed into the next RNN step. The goal is to make the model fully aware of the previous alignment choices. In work largely concurrent to our own, Kim et al. (2017) explore the use of conditional random fields (CRFs) to impose a variety of constraints on attention distributions achieving strong results on se"
W17-3108,D14-1162,0,0.0810925,"m serves two purposes. d acts as a contextual document representation which can be fed into a downstream model component for detection. In addition, the score vector u1:N , can be utilized to seed our explanation, which will be expanded on in a following section. Optionally for detection, we encode the document by using the last hidden state of a single forward GRU, without the reverse GRU and attention mechanism. Both encoding schemes are evaluated in our experiments. Word Embeddings Each token in the input is mapped to an embedding. We use reference GloVe embeddings trained on Twitter data (Pennington et al., 2014). We used the 200 dimensional embeddings for all our experiments, so each word wt is mapped to xt ∈ R200 . We denote the full embedded sequence as x1:T . 3.2 Attention over Words Recurrent Neural Networks A recurrent neural network (RNN) extends on a traditional neural network to recursively encode a sequence of vectors, x1:T , into a sequence of hidden states. The hidden state of the RNN at t − 1 is 3.4 Training Objective The final document encoding of each sample, d, is fed into a sigmoid layer with one node to detect 68 a configurable dial to control how attention was distributed over the i"
W17-3108,D15-1162,0,0.118446,"s to maintain similar output behavior for all three techniques; it also allows us to extend the seed functions to more complex models. We will now detail how seeding works for each of these mechanisms. 3.6 Explanation Generation Algorithm We use a novel algorithm for producing explanations that depends on seeds from a separatelydeveloped seeding module. The algorithm acts on the input text and the k explanation seeds. It works as follows. First, the sentence of importance is identified by taking the sentence with the most seeds. The identified sentence is then parsed with a dependency parser (Honnibal and Johnson, 2015), and traversed from the root to find the highest seed in the sentence. If the highest seed token is not a verb and not the head of the entire sentence, we then traverse to the seed’s head node. Subsequently, the subtree phrase of the highest seed is used for the explanation. Since the parse is projective, the subtree is necessarily a contiguous sequence of tokens. 4 4.1 Experiments Training Data Koko has an anonymous peer-to-peer therapy network based on an clinical trial at MIT (Morris et al., 2015), which is made available through chatbots on a variety of social media platforms including Fa"
W17-3108,N16-3020,0,0.0325739,"t¨aschel et al., 2015). These models induce a soft alignment between two sequences with the primary aim of using it to remove an information bottleneck, but this alignment can be also be used quite effectively to visualize which inputs drive model behavior. Lei et al. (2016) present a more direct method for training interpretable text classifiers. Their model is also trained end-to-end, but instead of inducing a soft weighting, it extracts a set of short subspans of each input text that are meant to serve as sufficient evidence for the final model decision. In another work with similar goals, Ribeiro et al. (2016) introduce a model agnostic framework for intepretability, LIME, that learns an interpretable model over a given sample input that is locally faithful to the original trained model. 3 ˜t ht = (1 − zt )ht−1 + zt h zt = σ(Wz xt + Uz ht−1 ) ˜ t = tanh(W xt + U (rt ht−1 )) h rt = σ(Wr xt + Ur ht−1 ) We use a bidirectional RNN (running one model in each direction) and concatenate the hidden states of each model for each word to obtain a contextual word representation hbi t . 3.3 With attention, a scoring function scores the relevance of each contextual word representation hbi t . We employ the unco"
W17-3108,W12-2102,0,0.528621,"rs who are assumed to be neurotypical, acknowledging that this data will be contaminated with users who also ideate and attempt suicide. They train simple, linear classifiers that show promise in detecting these suicidal users and discuss the difficulties of realizing this technology, highlighting privacy and intervention concerns. In our work, we attempt detection and explanation on phenomena that includes but is not limited to suicide on a dataset that is significantly larger and not artificially balanced. However, we do not incorporate the record of suicide attempt as signal when labeling. Lehrman et al. (2012) and O’Dea et al. (2015) also detect distress on small datasets using simple classifiers. Lehrman et al. (2012) annotate 200 samples for distress level and discretize counts related to bag of word, part of speech, sentence complexity and sentiment word features to train a variety of multiclass classifiers. O’Dea et al. (2015) annotated nearly 2000 tweets for different levels of suicidality and used word counts as features, filtered by document frequency. In our work, we compare neural techniques against linear models trained on word frequency counts both for detection and explanation as a base"
W17-3108,D16-1011,0,0.0608055,"Missing"
W17-3108,W04-1013,0,0.00908221,"Missing"
W17-3108,N16-1174,0,0.0206115,"an interpretable model over a given sample input that is locally faithful to the original trained model. 3 ˜t ht = (1 − zt )ht−1 + zt h zt = σ(Wz xt + Uz ht−1 ) ˜ t = tanh(W xt + U (rt ht−1 )) h rt = σ(Wr xt + Ur ht−1 ) We use a bidirectional RNN (running one model in each direction) and concatenate the hidden states of each model for each word to obtain a contextual word representation hbi t . 3.3 With attention, a scoring function scores the relevance of each contextual word representation hbi t . We employ the unconditional attention mechanism used to do document classification employed by Yang et al. (2016). ut = tanh(Ww hbi t + bw ) exp ut αt = P exp ut t X d= αt ht Methods Our training set consists of N examples i {X i , Y i }N i=1 where the input X is a sequence of tokens w1 , w2 , ..., wT , and the output Y i is a binary indicator of crisis. 3.1 t The attention mechanism serves two purposes. d acts as a contextual document representation which can be fed into a downstream model component for detection. In addition, the score vector u1:N , can be utilized to seed our explanation, which will be expanded on in a following section. Optionally for detection, we encode the document by using the la"
W17-3108,W16-4320,0,0.05573,"00 samples for distress level and discretize counts related to bag of word, part of speech, sentence complexity and sentiment word features to train a variety of multiclass classifiers. O’Dea et al. (2015) annotated nearly 2000 tweets for different levels of suicidality and used word counts as features, filtered by document frequency. In our work, we compare neural techniques against linear models trained on word frequency counts both for detection and explanation as a baseline. Due to the relatively large amount of data in our training set, we do not use any custom features for the baseline. Mowery et al. (2016) detect depression in Twitter data in two stages: 1) detecting evidence of de67 fed back into the RNN for the next time step. pression at all and 2) classify the specific depressive symptom if depression was detected. This is a kind of explanation in that it directly detects one of three symptoms of depression (fatigue, disturbed sleep, depressed mood). However, their data is explicitly annotated for these sub-factors, whereas our data is not. 1,656 tweets in their dataset were annotated with specific depressive symptoms. ht = f (xt , ht−1 ; Θ) This allows the network to construct a representa"
W17-3108,W16-0310,0,\N,Missing
W17-5301,D17-1070,0,0.0881476,"Missing"
W17-5301,W17-5308,0,0.128901,"Missing"
W17-5301,D14-1162,0,0.0854417,"Missing"
W17-5301,W17-5311,0,0.0144887,"y for negation, identity of the subject, and tense, though continuous tenses are not reliably differentiated from others. Probe Sentences During the competition, we additionally provided a set of automatically generated probe sentences meant to aid error analysis. These probe sentences are produced to vary along dimensions relevant to probing for semantic role and negation information. We asked submitting teams to supply vectors for these sentences in addition to those in the test set. Figure 1 shows the cosine similarity between a subset of these sentence vectors rendered by Nie and Bansal’s (2017) system. We find that all systems (except that of Balazs et al., who did not submit these vectors) show similar behavior on these sentences, and we do not observe a clear correlation between behavior here and model performance. Perhaps unsurprisingly, we observe that sentences tend to be more similar to one another the more structural features they have in common. We observe this 7 Conclusion We find that BiLSTM-based models with max pooling or intra-sentence attention represent a popular and effective strategy for sentence encoding, and that systems based on this technique perform very well a"
W17-5301,W17-5310,0,0.0755175,"Missing"
W17-5301,N16-1170,0,0.0126431,"purpose of the shard task is to evaluate techniques for training and using sentence encoders. To this end, we require that all models create fixed-length vectors for each sentence with no explicitly-imposed internal structure. Alignment strategies like attention that pass information between the two encoders handling the two input sentences in a pair are not allowed. Memory models that represent sentences as variable-length sets or sequences of vectors are also not permitted. While systems that use methods like attention and structured memory are effective for NLI (Rockt¨aschel et al., 2016; Wang and Jiang, 2016; Chen et al., 2017a; Williams et al., 2017, i.a.), much of the variation across models of this kind lies in the way that they explicitly or implicitly align related sentences, rather than the way that they extract representations for sentences. As a result, we expect that focusing our evaluation on a restricted subset of models will yield conclusions that are more generally applicable to work on natural language understanding than would have been the case otherwise. 4 Results and Leaderboard The competition results are shown in Table 3. All evaluated systems beat the BiLSTM baseline reported"
W17-5301,D15-1075,1,0.381679,", suggesting that all were able to produce systems for semantic representation which, while not perfect, were effective and not tightly adapted to any particular style of language or set of constructions. 2 written and spoken English, balanced across three labels. Each premise sentence (the first sentence in each pair) is derived from one of ten sources of text, which constitute the ten genre sections of the corpus. Each hypothesis sentence and pair label was composed by a crowd worker in response to a premise. MultiNLI was designed and collected in the style of the Stanford NLI Corpus (SNLI; Bowman et al. 2015), but covers a broader range of styles of text, rather than the relatively homogeneous captions used in SNLI. Testing and development sets are available for all genres, with 2000 examples per set per genre. Only five genres have accompanying training sets. So, for the matched development and test sets, models are tested on examples derived from the same sources as those in the training set, while for the mismatched sets, the text source is not repreDataset MultiNLI (Williams et al., 2017) consists of 393k pairs of sentences from a broad range of genres of 2 period—to run their systems on the u"
W17-5301,P17-1152,0,0.0331945,"task is to evaluate techniques for training and using sentence encoders. To this end, we require that all models create fixed-length vectors for each sentence with no explicitly-imposed internal structure. Alignment strategies like attention that pass information between the two encoders handling the two input sentences in a pair are not allowed. Memory models that represent sentences as variable-length sets or sequences of vectors are also not permitted. While systems that use methods like attention and structured memory are effective for NLI (Rockt¨aschel et al., 2016; Wang and Jiang, 2016; Chen et al., 2017a; Williams et al., 2017, i.a.), much of the variation across models of this kind lies in the way that they explicitly or implicitly align related sentences, rather than the way that they extract representations for sentences. As a result, we expect that focusing our evaluation on a restricted subset of models will yield conclusions that are more generally applicable to work on natural language understanding than would have been the case otherwise. 4 Results and Leaderboard The competition results are shown in Table 3. All evaluated systems beat the BiLSTM baseline reported in Williams et al.."
W17-5301,W17-5309,0,0.0273547,"Missing"
W17-5301,W17-5307,0,0.0793243,"task is to evaluate techniques for training and using sentence encoders. To this end, we require that all models create fixed-length vectors for each sentence with no explicitly-imposed internal structure. Alignment strategies like attention that pass information between the two encoders handling the two input sentences in a pair are not allowed. Memory models that represent sentences as variable-length sets or sequences of vectors are also not permitted. While systems that use methods like attention and structured memory are effective for NLI (Rockt¨aschel et al., 2016; Wang and Jiang, 2016; Chen et al., 2017a; Williams et al., 2017, i.a.), much of the variation across models of this kind lies in the way that they explicitly or implicitly align related sentences, rather than the way that they extract representations for sentences. As a result, we expect that focusing our evaluation on a restricted subset of models will yield conclusions that are more generally applicable to work on natural language understanding than would have been the case otherwise. 4 Results and Leaderboard The competition results are shown in Table 3. All evaluated systems beat the BiLSTM baseline reported in Williams et al.."
W18-2601,D14-1162,0,0.0962514,"acter embedding layer maps each word to a high dimensional vector using character features. It does so using a convolutional neural network with max pooling over learned character vectors (Lee et al., 2017; Kim et al., 2016). Thus we have a context character representation M ∈ Rf ×C and a query representation N ∈ Rf ×Q , where C is the sequence length of the context, Q is the sequence length of the query and f is the number of 1D convolutional neural network filters. Word Embedding Layer Again as in the base model, the word embedding layer uses pretrained word vectors (the 6B GloVe vectors of Pennington et al., 2014) to map the word into a high dimensional vector space. We do not update the word embeddings during training. The character embedding and the word embedding are concatenated and passed into a two-layer highway network (Sri3 vastava et al., 2015) to obtain a d dimensional vector representation of each single word. Hence, we have a context representation H ∈ Rd×C and a query representation U ∈ Rd×Q . Summarization Layer We propose summarization layer which produces a vector representation that summarizes the information in the queryaware context representation. The input to summarization layer is"
W18-2601,P16-1223,0,0.0185903,"cing the use of a pointer network (Vinyals et al., 2015) to extract the answer span from the context. Xiong et al. (2017b) propose the Dynamic Coattention Network, which uses co-dependent representations of the question and the context, and iteratively updates the start and end indices to recover from local maxima and to find the optimal answer span. Wang et al. (2016) propose the Multi-Perspective Context Matching model that matches the encoded context with query by combining various matching strategies, aggregates matching vector with bidirectional LSTM, and predict start and end positions. Chen et al. (2016) propose to use a bilinear term to calculate the attentional alignment between context and query. Among multi-hop reasoning systems: Hill et al. (2015) apply attention on window-based memory, by extending multi-hop end-to-end memory network (Sukhbaatar et al., 2015). Dhingra et al. (2016) extend attention-sum reader to multi-turn reasoning with an added gating mechanism. The Iterative Alternative (IA) reader (Sordoni et al., 2016) produces query glimpse and 4 4.1 Our Model Ruminating Reader In this section, we review the B I DAF model (Seo et al., 2017) and introduce our extension, the Ruminat"
W18-2601,P17-1171,0,0.0334553,"Missing"
W18-2601,P17-1018,0,0.0160763,"d. Unlike other datasets that such as CNN/Daily Mail whose questions are synthesized, Rajpurkar et al. (2016) uses a crowdsourcing platform to generate realistic question and answer pairs. SQuAD contains 107,785 question1 The latest results are listed at https://rajpurkar. github.io/SQuAD-explorer/ 2 document glimpse in each iteration and uses both glimpses to update recurrent state in each iteration. Shen et al. (2017) propose a multi-hop attention model that used reinforcement learning to dynamically determine when to stop digesting intermediate information and produce an answer. The r-net (Wang et al., 2017) adopts self-attention mechanism to allow the representation to select the important information in both context and query. Yu et al. (2018) design a new non-recurrent neural network topology, that achieves 13x faster in training and 4x to 9x faster in inference without losing accuracy with respect to its recurrent counterpart. answer pairs. The typical context length spans from 50 tokens to 250 tokens. The typical length of a question is around 10 tokens. The answer be any span of words from the context, resulting in O(|C|2 ) possible outputs. While model pefromance on SQuAD is nearing human"
W18-5446,N18-1202,0,0.363701,"fic linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions. Corpus |Train| Task Domain Single-Sentence Tasks CoLA SST-2 8.5k 67k acceptability sentiment misc. movie reviews Similarity and Paraphrase Tasks MRPC STS-B QQP 3.7k 7k 364k MNLI QNLI RTE WNLI 393k 108k 2.5k 634 paraphrase textual sim. paraphrase news misc. online QA Inference Tasks NLI QA/NLI NLI coref./NLI misc. Wikip"
W18-5446,S17-2001,0,0.0943845,"ough it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 353 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for"
W18-5446,D16-1264,0,0.443898,"65.1 62.3 65.1 65.1 65.1 65.1 Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation (Matthews, 1975). For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform. nizing Textual Entailment (RTE; aggregated from Dagan et al. 2006, Bar Haim et al. 2006, Giampiccolo et al. 2007, Bentivogli et al. 2009), as well as versions of SQuAD (Rajpurkar et al., 2016) and Winograd Schema Challenge (Levesque et al., 2011) recast as NLI (resp. QNLI, WNLI). Table 1 summarizes the tasks. Performance on the benchmark is measured per task as well as in aggregate, averaging performance across tasks. Tags Sentence Pair Quantifiers Double Negation I have never seen a hummingbird not flying. I have never seen a hummingbird. Active/Passive Named Entities World Knowledge Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also includes a dataset of hand-crafted examples for probing trained models. This dataset is designed to highlight commo"
W18-5446,D17-1070,0,0.111288,"Missing"
W18-5446,D13-1170,0,0.166913,"l tasks are binary classification, except STS-B (regression) and MNLI (three classes). quire models to share substantial knowledge (e.g., trained parameters) across tasks, while maintaining some task-specific components. Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we"
W18-5446,I05-5002,0,0.264722,"tial knowledge (e.g., trained parameters) across tasks, while maintaining some task-specific components. Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 353 P"
W18-5446,W07-1401,0,0.301573,"71.3 61.1 81.7 75.1 74.7 79.8 75.2 82.3 50.4 61.2 54.1 53.1 58.0 56.4 59.2 65.1 65.1 62.3 65.1 65.1 65.1 65.1 Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation (Matthews, 1975). For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform. nizing Textual Entailment (RTE; aggregated from Dagan et al. 2006, Bar Haim et al. 2006, Giampiccolo et al. 2007, Bentivogli et al. 2009), as well as versions of SQuAD (Rajpurkar et al., 2016) and Winograd Schema Challenge (Levesque et al., 2011) recast as NLI (resp. QNLI, WNLI). Table 1 summarizes the tasks. Performance on the benchmark is measured per task as well as in aggregate, averaging performance across tasks. Tags Sentence Pair Quantifiers Double Negation I have never seen a hummingbird not flying. I have never seen a hummingbird. Active/Passive Named Entities World Knowledge Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also includes a dataset of hand-crafted"
W18-5446,N18-1101,1,0.32004,"data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 353 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Model Avg Single-task Multi-task CBoW Skip"
W18-5446,N18-2017,1,0.716956,"et of hand-crafted examples for probing trained models. This dataset is designed to highlight common phenomena, such as the use of world knowledge, logical operators, and lexical entailments, that models must grasp if they are to robustly solve the tasks. Each of the 550 examples is an NLI sentence pair tagged with the phenomena demonstrated. We ensure that the data is reasonably diverse by producing examples for a wide variety of linguistic phenomena, and basing our examples on naturally-occurring sentences from several domains. We validate our data by using the hypothesis-only baseline from Gururangan et al. (2018) and having six NLP researchers manually validate a random sample of the data. Cape sparrows eat seeds, along with soft plant parts and insects. Cape sparrows are eaten. Musk decided to offer up his personal Tesla roadster. Musk decided to offer up his personal car. Table 3: Diagnostic set examples. Systems must predict the relationship between the sentences, either entailment, neutral, or contradiction when one sentence is the premise and the other is the hypothesis, and vice versa. Examples are tagged with the phenomena demonstrated. We group each phenomena into one of four broad categories."
W18-5446,D15-1075,1,\N,Missing
W18-5446,P04-1035,0,\N,Missing
W18-5446,P05-1015,0,\N,Missing
W18-5446,D14-1162,0,\N,Missing
W18-5446,P16-2038,0,\N,Missing
W18-5446,D17-1206,0,\N,Missing
W18-5446,W17-5307,0,\N,Missing
W18-5446,I17-1100,0,\N,Missing
W18-5446,W18-2501,0,\N,Missing
W18-5446,W17-1609,0,\N,Missing
W18-5448,J07-3004,0,0.0170486,"Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis 1 Kelly W. Zhang1 Samuel R. Bowman1,2,3 kz918@nyu.edu bowman@nyu.edu Dept. of Computer Science New York University 60 Fifth Avenue New York, NY 10011 1 2 Center for Data Science New York University 60 Fifth Avenue New York, NY 10011 Introduction Dept. of Linguistics New York University 10 Washington Place New York, NY 10003 use two syntactic evaluation tasks: part-of-speech (POS) tagging on Penn Treebank WSJ (Marcus et al., 1993) and Combinatorial Categorical Grammar (CCG) supertagging on CCG Bank (Hockenmaier and Steedman, 2007). CCG supertagging allows us to measure the degree to which models learn syntactic structure above the word. We also measure how much LSTMs simply memorize input sequences with a word identity prediction task. Recently, researchers have found that deep LSTMs (Hochreiter and Schmidhuber, 1997) trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech (Belinkov et al., 2017a,b; Blevins et al., 2018). These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial"
W18-5448,J93-2004,0,0.0613999,"Missing"
W18-5448,N18-1202,0,0.148758,"degree to which models learn syntactic structure above the word. We also measure how much LSTMs simply memorize input sequences with a word identity prediction task. Recently, researchers have found that deep LSTMs (Hochreiter and Schmidhuber, 1997) trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech (Belinkov et al., 2017a,b; Blevins et al., 2018). These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models (Peters et al., 2018; McCann et al., 2017). We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives—language modeling, translation, skip-thought, and autoencoding—on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture. 2 3 3 Results Comparing Pretraining Tasks For all pretraining dataset sizes, bidirectional language model (BiLM) and translation encoder repr"
W18-5452,N18-1101,1,0.850568,"g, and PRPN-UP for (unsupervised) parsing. PRPN-LM is much larger than PRPN-UP, with embedding layer that is 4 times larger and the number of units per layer that is 3 times larger. In the PRPN-UP experiments, we observe that the WSJ data is not split, such that the test data is used without parse information for training. This implies that the parsing results of PRPN-UP may not be generalizable in the way usually expected of machine learning evaluation results. We train PRPN on sentences from two datasets: The full WSJ and AllNLI, the concatenation of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b). We then evaluate the constituency trees produced by these models on the full WSJ, WSJ101 , and the MultiNLI development set. 3 Results Table 1 shows results with all the models under study, plus several baselines, on WSJ and WSJ10. Unexpectedly, the PRPN-LM models achieve higher parsing performance than PRPNUP. This shows that any tuning done to separate PRPN-UP from PRPN-LM was not necessary, and that the results described in the paper can be largely reproduced by a unified model in a fair setting. Moreover, the PRPN models trained on the larger, out-of-domain AllNLI perform better than t"
W18-5452,D15-1075,1,0.746123,"—PRPN-LM tuned for language modeling, and PRPN-UP for (unsupervised) parsing. PRPN-LM is much larger than PRPN-UP, with embedding layer that is 4 times larger and the number of units per layer that is 3 times larger. In the PRPN-UP experiments, we observe that the WSJ data is not split, such that the test data is used without parse information for training. This implies that the parsing results of PRPN-UP may not be generalizable in the way usually expected of machine learning evaluation results. We train PRPN on sentences from two datasets: The full WSJ and AllNLI, the concatenation of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018b). We then evaluate the constituency trees produced by these models on the full WSJ, WSJ101 , and the MultiNLI development set. 3 Results Table 1 shows results with all the models under study, plus several baselines, on WSJ and WSJ10. Unexpectedly, the PRPN-LM models achieve higher parsing performance than PRPNUP. This shows that any tuning done to separate PRPN-UP from PRPN-LM was not necessary, and that the results described in the paper can be largely reproduced by a unified model in a fair setting. Moreover, the PRPN models trained on the larger, out-of"
W18-5452,P02-1017,0,0.842674,"ees – – – – – – 34.7 21.3 (0.0) 21.4 – 21.3 (0.0) 21.3 5.3 4.6 17.4 22.1 – – Parsing F1 Depth Accuracy on WSJ by Tag WSJ10 WSJ WSJ ADJP NP PP INTJ µ (σ) max µ (σ) max 22.3 16.0 20.2 9.3 40.4 55.9 Table 1: Unlabeled parsing F1 test results broken down by training data and by early stopping criterion. The Accuracy columns represent the fraction of ground truth constituents of a given type that correspond to constituents in the model parses. Italics mark results that are worse than the random baseline. Results with RL-SPINN and ST-Gumbel are from Williams et al. (2018a). WSJ10 baselines are from Klein and Manning (2002, CCM), Klein and Manning (2005, DMV+CCM), and Bod (2006, UML-DOP). Model 300D SPINN w/o Leaf GRU 300D SPINN-NC w/o Leaf GRU NLI NLI NLI NLI 19.3 21.2 19.2 20.6 36.9 39.0 36.2 38.9 70.2 63.5 70.5 64.1 6.2 6.4 6.1 6.3 300D ST-Gumbel w/o Leaf GRU 300D RL-SPINN w/o Leaf GRU NLI NLI NLI NLI 32.6 30.8 95.0 99.1 37.5 35.6 13.5 10.7 23.7 27.5 18.8 18.1 4.1 4.6 8.6 8.6 PRPN-LM PRPN-UP PRPN-UP LM UP LM 25.6 26.9 45.7 19.4 41.0 46.3 19.9 37.4 48.6 4.9 4.9 4.9 – – 27.9 28.0 27.0 21.7 36.8 21.3 4.4 3.9 Random Trees Balanced Trees et al. (2018, called ST-Gumbel), despite the fact that the model was tuned e"
W19-0129,P98-1013,0,0.635257,". *Ada exercised herself. Ada exercised. Table 1: Examples from each verb frame in the dataset. Bolded verbs evoke both verb frames; other verbs evoke only one. Transitive verb frames include: Causative, S PRAY–L OAD with, S PRAY–L OAD locative, U NDERSTOOD O BJECT reflexive. Intransitive verb frames include: Inchoative, no-there (with locative adjunct), there (with locative adjunct), and U NDERSTOOD -O BJECT no-reflexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classified in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence, and as such, should potentially be learned by ANNs. We present two datasets and two experiments that compare ANNs’ knowledge of verb frame alternations at the word level and the sentence level, respectively. First, we ask if a verb’s word embedding can be used to predict which frames that verb can licitly appear in. We construct a dataset of verbs, the Lexical Verb–frame Alternations dataset (LaVA), based on Levin (1"
W19-0129,P04-3031,0,0.0663762,"rained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequently in natural language processing (NLP), so evaluating them for knowledge of verb frames will be relevant for their application to and future research on tasks requiring rich syntactic features. Second, we use embeddings trained on the smaller 100M token British National Corpus4 (BNC), optimizing a language modeling objective. The language model (LM) is a (single-directional) LSTM trained by Warstadt et al. (2018) using PyTorch and optimized using Adam (Kingma and Ba, 2015). The BNC data is tokenized using NLTK (Bird and Loper, 2004) and words outside the 100k most frequent words in the BNC are replaced with &lt;unk>. Our peripheral interest in how humans learn lexical frame-selectional properties motivates us to investigate these LM-trained word embeddings. We reduce the potential differences between human learners and our models by considering embeddings that are trained on an amount of data similar to what humans are exposed to during language acquisition. For this reason, most publicly available, pre-trained word vectors are a rather unnatural fit, since these embeddings are usually trained on several orders of magnitude"
W19-0129,N18-1083,0,0.0165497,"ndividual alternation for our models to learn. 7 Related Work This investigation is part of a growing body of work which seeks to investigate the linguistic competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine trans¨ lation. Finally, Ostling and Tiedemann (2017) learned language embeddings via multilingual language modeling and used them to reconstruct genealogical trees. However, we are interested in word or sentence embeddings. Extracting information from word embeddings is a common task in natural language processing. While most NLP research is application-o"
W19-0129,W11-0110,0,0.0339384,"about the task at hand as possible (e.g., by varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular embeddings. Also worth mentioning here is a lexical resource named VerbNet (Kipper-Schuler, 2005; KipperSchuler et al., 2006). This database contains verbs which were classified according to their seman295 tic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb. Since we want to investigate if argument structure information is present in sentence embeddings, we create a larger corpus. 8 Conclusions We present complementary word-level and sentence-level datasets, LaVA and FAVA, covering five verb-alternations. We train classifiers on verb embeddings to distinguish which syntactic frames a verb can evoke and whi"
W19-0129,W16-2524,0,0.245899,"example, the lexical set in (2) is used to generate 18 minimal pairs of sentences as in (3) (one pair for each combination of verb, patient, location, and preposition). (2) verbs = {hung, draped} patients = {the blanket, the towel, the cloth} locations = {the bed, the armchair, the couch} prepositions = {over} (3) a. Betty draped the blanket over the couch. b. *Betty draped the couch with the blanket. A similar, semi-automatic sentence creation method focusing only on the passive alternation (and non-argument structure syntactic reorderings using negation and relative clauses) was employed by Ettinger et al. (2016) and Warstadt et al. (2018). Using this method, we construct five sentence-level datasets highlighting different verb alternations (C AUSATIVE –I NCHOATIVE,2 DATIVE, S PRAY–L OAD, there-I NSERTION, U NDERSTOOD -O BJECT) that are chosen so that sentences could be generated with the maximum of variability in the choice of verbs. We split our data into training, development, and test sets by binning lexical sets into training and evaluation bins randomly, in equal proportions. The evaluation set is then split 80/20 into test and development set. Splitting by lexical bin rather than by sentence pr"
W19-0129,P06-1117,0,0.0601984,"s on obtaining embeddings which contain as much knowledge about the task at hand as possible (e.g., by varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular embeddings. Also worth mentioning here is a lexical resource named VerbNet (Kipper-Schuler, 2005; KipperSchuler et al., 2006). This database contains verbs which were classified according to their seman295 tic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb. Since we want to investigate if argument structure information is present in sentence embeddings, we create a larger corpus. 8 Conclusions We present complementary word-level and sentence-level datasets, LaVA and FAVA, covering five verb-alternations. We train classifiers on verb embeddings to distinguish"
W19-0129,C94-1042,0,0.0527846,".-Refl. U.-Obj.-No-Refl. *Ada exercised herself. Ada exercised. Table 1: Examples from each verb frame in the dataset. Bolded verbs evoke both verb frames; other verbs evoke only one. Transitive verb frames include: Causative, S PRAY–L OAD with, S PRAY–L OAD locative, U NDERSTOOD O BJECT reflexive. Intransitive verb frames include: Inchoative, no-there (with locative adjunct), there (with locative adjunct), and U NDERSTOOD -O BJECT no-reflexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classified in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence, and as such, should potentially be learned by ANNs. We present two datasets and two experiments that compare ANNs’ knowledge of verb frame alternations at the word level and the sentence level, respectively. First, we ask if a verb’s word embedding can be used to predict which frames that verb can licitly appear in. We construct a dataset of verbs, the Lexical Verb–frame Alternations dataset (LaVA"
W19-0129,kipper-etal-2006-extending,0,0.0433898,"ach verb frame in the dataset. Bolded verbs evoke both verb frames; other verbs evoke only one. Transitive verb frames include: Causative, S PRAY–L OAD with, S PRAY–L OAD locative, U NDERSTOOD O BJECT reflexive. Intransitive verb frames include: Inchoative, no-there (with locative adjunct), there (with locative adjunct), and U NDERSTOOD -O BJECT no-reflexive. 2-obj. class includes a ditransitive frame and a prepositional dative frame. can appear in) has been described and classified in several verb lexica (Grishman et al., 1994; Baker et al., 1998; Fillmore et al., 2003; Kipper-Schuler, 2005; Kipper-Schuler et al., 2006). Knowledge about verb frames and their alternations is part of a human speaker’s linguistic competence, and as such, should potentially be learned by ANNs. We present two datasets and two experiments that compare ANNs’ knowledge of verb frame alternations at the word level and the sentence level, respectively. First, we ask if a verb’s word embedding can be used to predict which frames that verb can licitly appear in. We construct a dataset of verbs, the Lexical Verb–frame Alternations dataset (LaVA), based on Levin (1993), and train a multi-class classifier to identify the licit syntactic fr"
W19-0129,Q16-1037,0,0.0438598,"81.8 Majority BL MCC 0.0 Acc. 66.6 0.0 77.6 0.0 82.1 0.0 60.3 0.0 77.5 0.0 53.7 Table 5: Results from Experiment 2. “w/o CoLA” are models trained on datasets not augmented with CoLA; “w/ CoLA” are models trained on augmented datasets; “Comb.” refers to an aggregate dataset. Bolded MCC values represent moderate correlations (above 0.45). of all the generated data, yet it was by far the hardest individual alternation for our models to learn. 7 Related Work This investigation is part of a growing body of work which seeks to investigate the linguistic competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings"
W19-0129,D17-1268,0,0.0147146,"c competence of ANNs. For instance, a study by Linzen et al. (2016) tested the ability of ANNs to identify mismatches in subject–verb agreement, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine trans¨ lation. Finally, Ostling and Tiedemann (2017) learned language embeddings via multilingual language modeling and used them to reconstruct genealogical trees. However, we are interested in word or sentence embeddings. Extracting information from word embeddings is a common task in natural language processing. While most NLP research is application-oriented and directly or indirectly focuses on obtaining embeddings which contain as much knowledge about the task at hand as possible (e.g., by varyi"
W19-0129,D12-1048,0,0.0147924,"varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular embeddings. Also worth mentioning here is a lexical resource named VerbNet (Kipper-Schuler, 2005; KipperSchuler et al., 2006). This database contains verbs which were classified according to their seman295 tic and syntactic properties, including their Levin classes.7 VerbNet has been used in various NLP applications, e.g., semantic role labeling (Giuglea and Moschitti, 2006), word sense disambiguation (Brown et al., 2011), information extraction (Mausam et al., 2012), or investigation of human language acquisition (Korhonen, 2010). While this resource is very extensive, it only provides a few example sentences (generally only one or two per frame) for each verb. Since we want to investigate if argument structure information is present in sentence embeddings, we create a larger corpus. 8 Conclusions We present complementary word-level and sentence-level datasets, LaVA and FAVA, covering five verb-alternations. We train classifiers on verb embeddings to distinguish which syntactic frames a verb can evoke and which it cannot. We further train acceptability c"
W19-0129,N13-1090,0,0.0752757,"lopment, and test sets by binning lexical sets into training and evaluation bins randomly, in equal proportions. The evaluation set is then split 80/20 into test and development set. Splitting by lexical bin rather than by sentence prevents models from finding a trivial solution to classification by learning to 2 The C AUSATIVE –I NCHOATIVE dataset presented here is an expanded version of an analysis dataset in Warstadt et al. (2018). 291 Embeddings, i.e., vector representations of linguistic objects like characters, words, or sentences, encode helpful information for downstream applications (Mikolov et al., 2013). In particular, they can be used to leverage knowledge from one task for another and have been shown to improve performance on a diverse set of tasks. Embeddings are usually low-dimensional; common sizes differ between 100 and 300. Our experiments make use of three types of word and sentence embeddings, which we will describe in the following. Word Embeddings For our word-level experiments, we use two different embeddings which differ in the way of their creation. First, we use 300-dimensional GloVe embeddings trained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequent"
W19-0129,E17-2102,0,0.0252246,"ent, even in the presence of intervening “distractor” nouns. Similarly, Ettinger et al. (2016) investigated whether sentence embeddings contain grammatical information, e.g., about the syntactic scope of negation. Further previous studies on which types of information are contained in embeddings include Bjerva and Augenstein (2018), which asked whether certain phonological, morphological and syntactic information can be extracted from language embeddings. Malaviya et al. (2017) predicted features from language embeddings which were trained as part of an ANN for machine trans¨ lation. Finally, Ostling and Tiedemann (2017) learned language embeddings via multilingual language modeling and used them to reconstruct genealogical trees. However, we are interested in word or sentence embeddings. Extracting information from word embeddings is a common task in natural language processing. While most NLP research is application-oriented and directly or indirectly focuses on obtaining embeddings which contain as much knowledge about the task at hand as possible (e.g., by varying the training corpus or embedding method), we are interested in the question how much information is trivially contained in selected popular emb"
W19-0129,D14-1162,0,0.103224,"nformation for downstream applications (Mikolov et al., 2013). In particular, they can be used to leverage knowledge from one task for another and have been shown to improve performance on a diverse set of tasks. Embeddings are usually low-dimensional; common sizes differ between 100 and 300. Our experiments make use of three types of word and sentence embeddings, which we will describe in the following. Word Embeddings For our word-level experiments, we use two different embeddings which differ in the way of their creation. First, we use 300-dimensional GloVe embeddings trained on 6B tokens (Pennington et al., 2014).3 GloVe embeddings are used frequently in natural language processing (NLP), so evaluating them for knowledge of verb frames will be relevant for their application to and future research on tasks requiring rich syntactic features. Second, we use embeddings trained on the smaller 100M token British National Corpus4 (BNC), optimizing a language modeling objective. The language model (LM) is a (single-directional) LSTM trained by Warstadt et al. (2018) using PyTorch and optimized using Adam (Kingma and Ba, 2015). The BNC data is tokenized using NLTK (Bird and Loper, 2004) and words outside the 1"
W19-0129,N18-1202,0,0.0621809,"ntil the completion of 4 training epochs without improvement in Matthews correlation coefficient on the development set. The architecture of the real/fake encoder is shown in Figure 1. A bidirectional long-short term memory network (LSTM, Hochreiter and Schmidhuber, 1997) reads the words of a sentence. A fixed-length sentence embedding is then produced by a max-pooling operation over the concatenations of the forward and backward hidden states at each time-step. This encoding serves as input to a sigmoid output layer, which outputs a binary prediction. The input to the encoder are ELMo-style (Peters et al., 2018) contextualized word embeddings from a trained LM. As in ELMo, the representation for a word wi is a linear combination of the hidden states hji for each layer j in an LSTM LM, though we depart from that paper by using only a forward LM. As argued in Warstadt et al. (2018), this sentence encoder is a reasonable model for a human learner because it is not exposed to any knowledge of language that could not plausibly be part of the input to a human learner. Its training data consists of the same 100 million tokens used to train the word embeddings, augmented with another 100 million generated to"
