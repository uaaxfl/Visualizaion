2003.mtsummit-systems.10,J95-4004,0,0.00485969,"descending order. We use the following document collection for the purpose of research and development of PRIME: • • • • • • • Japanese Patent Application (32,000), Japanese Patent Abstract (1,750,000), US Patent Application (32,000), Patent Abstract of Japan (1,750,000), Korean Patent Application (68,800), Korean Patent Abstract in Korean (68,800), Korean Patent Abstract in English (78,000). Here, Patent Abstract of Japan (PAJ) is an English translation of the Japanese Patent Abstract. We extract content words, such as nouns, and perform word-based indexing. We use ChaSen3 , the Brill Tagger [1], and the Cross Language’s morphological analyzer, to extract content words from Japanese, US, and Korean patent documents, respectively. 2.4 Document Clustering In the document clustering module, we use the Hierarchical Bayesian Clustering (HBC) method [5], which merges similar items (i.e., patents in our case) in a bottomup method, until all the items are merged into a single cluster. Thus, a specific number of clusters can be obtained by splitting the resultant hierarchy at a predetermined level. The HBC method also determines the most representative item (centroid) for each cluster. Thus,"
2003.mtsummit-systems.10,2001.mtsummit-papers.30,1,0.410607,"ation, trilingual (J/E/K) patent retrieval is available. We describe the system design and its evaluation. Keywords 1 multilingual, patent retrieval, machine translation, document clustering, translation extraction Introduction Given the growing number of patents filed in multiple countries, it is feasible that users are interested in retrieving patents across languages. However, many users have difficulty retrieving patents in foreign languages. To solve this problem, we developed a Japanese/English bilingual patent retrieval system, named PRIME (Patent Retrieval In Multilingual Environment) [4]. In brief, PRIME translates a query in a user language into a document language, retrieves the foreign patent documents relevant to the translated query, and translates retrieved documents into the user language. As a result, users can retrieve and browse foreign patent documents only by their native language. In this paper, we extend PRIME into a trilingual retrieval system, in which user can utilize Japanese, English, and Korean for both the query and document languages. We describe the architecture of PRIME (Section 2) and its evaluation (Section 3). 2 2.1 System Description Overview Figur"
2008.amta-papers.8,fujii-etal-2006-test,1,0.857007,"e machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval, which can be used to evaluate the contribution of machine translation to retrieving patent documents across languages. This paper describes our test collection, methods for evaluating machine translation, and preliminary experiments. 1 Introduction Since the Third NTCIR Workshop in 20011 , which was an evaluation forum for research and development in information retrieval and natural language processing, the Patent Retrieval Task has been performed repeatedly (Fujii et al., 2004; Fujii et al., 2006; Fujii et al., 2007b; Iwayama et al., 2006). In the Sixth NTCIR Workshop (Fujii et al., 2007b), patent documents published over a 10-year period by the Japanese Patent Office (JPO) and the US Patent & Trademark Office (USPTO) were independently used as target document collections. 1 Having explored patent retrieval issues for a long time, we decided to address another issue in patent processing. From among a number of research issues related to patent processing (Fujii et al., 2007a), we selected Machine Translation (MT) of patent documents, which is useful for a number of applications and se"
2008.amta-papers.8,2001.mtsummit-papers.30,1,0.541065,"cations in foreign countries. Reflecting the rapid growth in the use of multilingual corpora, a number of data-driven MT methods have recently been explored, most of which are termed “Statistical Machine Translation (SMT)”. While large bilingual corpora for European languages, Arabic, and Chinese are available for research and development purposes, these corpora are rarely associated with Japanese and therefore it is difficult for explore SMT with respect to Japanese. However, we found that the patent documents used for the NTCIR Workshops can potentially alleviate this data scarcity problem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting new translations. A patent family is a set of patent documents for the same or related inventions and these documents are usually filed in more than one country in various languages. Following Higuchi et al’s method, we can produce a bilingual corpus for Japanese and English. In addition, there are a number of SMT engines (decoders) available to the public, such as Pharaoh and Moses2 , which can be applied to bilingual corpora involving any pair of languages. Motivated by the above background, we de2 http://research.nii.ac.jp/ntcir/in"
2008.amta-papers.8,W04-3250,0,0.13242,"Missing"
2008.amta-papers.8,P02-1040,0,0.0861881,"train their MT system, whether it is a data-driven SMT or a conventional knowledge-intensive rule-based MT. Second, the organizers provide the groups with a test data set of sentences in either Japanese or English. Each group is requested to machine translate each sentence from its original language into the other language and submit their translation results to the organizers. Third, the organizers evaluate the submission from each group. We use both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we independently use both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which was proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we investigate the contribution of the MT to CLPR. In the Patent Retrieval Task at NTCIR5, aimed at CLPR, search topics in Japanese were translated into English by human experts. We reuse these search topics for the evaluation of the MT. We also analyze the relationship between different evaluation measures. The use of extrinsic evaluation, which is not performed in existing MT-related evaluation activities, such as the NIST MetricsMATR Challenge3 and the IWSLT Workshop4 , is a dist"
2008.amta-papers.8,2007.mtsummit-papers.63,1,0.907449,"Missing"
2009.mtsummit-wpt.1,fujii-etal-2006-test,1,0.835438,"blem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting translations. A patent family is a set of patent documents for the same or related inventions and -1- We used both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which had been proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we evaluated the contribution of the MT to Cross-Lingual Information Retrieval (CLIR). In the Patent Retrieval Task at NTCIR-5 (Fujii et al., 2006), aimed at CLIR, search topics in Japanese were translated into English by human experts. We reused these search topics for the evaluation of the MT. We analyzed the relationship between different evaluation measures. The use of extrinsic evaluation, which is not performed in existing MT-related evaluation activities, such as the NIST MetricsMATR Challenge1 and the IWSLT Workshop2 , is a distinctive feature of our research. We executed a preliminary trial and the final evaluation, using the terms “dry run” and “formal run”, respectively. This paper describes only the formal run. 1 2 http://www"
2009.mtsummit-wpt.1,2001.mtsummit-papers.30,1,0.650448,"ation, Patent information, Cross-lingual information retrieval 1 Introduction Reflecting the rapid growth in the use of multilingual corpora, a number of data-driven Machine Translation (MT) methods have recently been explored, most of which are termed “Statistical Machine Translation (SMT)”. While large bilingual corpora for European languages, Arabic, and Chinese are available for research and development purposes, these corpora are rarely associated with Japanese and it is difficult to explore SMT with respect to Japanese. However, patent documents can alleviate this data scarcity problem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting translations. A patent family is a set of patent documents for the same or related inventions and -1- We used both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which had been proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we evaluated the contribution of the MT to Cross-Lingual Information Retrieval (CLIR). In the Patent Retrieval Task at NTCIR-5 (Fujii et al., 2006), aimed"
2009.mtsummit-wpt.1,W06-3114,0,0.0263028,"rinsic evaluation. ever, because the reference translations for MRB300 are independent of the counterpart sentences in the training data set, unlike RBMT systems, these SMT systems did not perform effectively. Figure 4 graphs the value for “Human” in Table 2, in which the order of groups is the same as Figures 1–3. In Figure 4, tsbmt and JAPIO, which were not effective in SRB, outperformed the other groups with respect to human rating. BLEU is generally suitable for comparing the effectiveness of SMT methods, but not suitable for evaluating other types of methods (Callison-Burch et al., 2006; Koehn and Monz, 2006). Figures 5 and 6 graph the value for adequacy and fluency, respectively. Although the relative superiority of the groups was almost the same in Figures 5 and 6, differences of the groups are more noticeable in Figure 5. To further analyze this tendency, Figure 7 shows -6- Figure 5: Adequacy for J–E intrinsic evaluation. Figure 6: Fluency for J–E intrinsic evaluation. Table 3: Results of E–J int/ext evaluation. Figure 7: Relationship between BLEU and human rating for J–E intrinsic evaluation. the correlation coefficient (“R”) between human rating and each BLEU type. The value of R for SRB is 0"
2009.mtsummit-wpt.1,P07-2045,0,0.0055351,"Missing"
2009.mtsummit-wpt.1,W04-3250,0,0.139508,"Missing"
2009.mtsummit-wpt.1,N03-2021,0,0.0130034,"E–J int/ext evaluation. Figure 7: Relationship between BLEU and human rating for J–E intrinsic evaluation. the correlation coefficient (“R”) between human rating and each BLEU type. The value of R for SRB is 0.814, which is smaller than those for MRB300 and MRB600. This is mainly due to the two outliers on the right side that correspond to the results for tsbmt and JAPIO. However, the values of R for MRB300 and MRB600 are more than 0.9, showing a high correlation between human rating and BLEU. By using multiple references, the evaluation result by BLEU became similar to that by human rating (Melamed et al., 2003). In such a case, while human judgments are not reusable, we need only reference translations, which are reusable, for evaluating MT methods. We also calculated the values of R for each BLEU type in terms of adequacy and fluency although these values are not shown in Figure 7. For adequacy, the values of R for SRB, MRB300, and MRB600 were 0.733, 0.846, and 0.887, respectively. For fluency, the values of R for SRB, MRB300, and MRB600 were 0.864, 0.940, and 0.951, respectively. This implies that BLEU is highly correlated with fluency more than adequacy. 4.3 E–J Intrinsic Evaluation Table 3 shows"
2009.mtsummit-wpt.1,P02-1040,0,0.079534,"r European languages, Arabic, and Chinese are available for research and development purposes, these corpora are rarely associated with Japanese and it is difficult to explore SMT with respect to Japanese. However, patent documents can alleviate this data scarcity problem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting translations. A patent family is a set of patent documents for the same or related inventions and -1- We used both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which had been proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we evaluated the contribution of the MT to Cross-Lingual Information Retrieval (CLIR). In the Patent Retrieval Task at NTCIR-5 (Fujii et al., 2006), aimed at CLIR, search topics in Japanese were translated into English by human experts. We reused these search topics for the evaluation of the MT. We analyzed the relationship between different evaluation measures. The use of extrinsic evaluation, which is not performed in existing MT-related evaluation activities, such as the NIS"
2009.mtsummit-wpt.1,2007.mtsummit-papers.63,1,0.867471,"Missing"
C02-1078,P95-1017,0,0.597927,", 1995; Hobbs, 1978; Mitkov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998; Kim and Ehara, 1995; Soon et al., 2001) use statistical models produced based on corpora annotated with anaphoric relations. However, only a few attempts have been made in corpus-based anaphora resolution for Japanese zero pronouns. One of the reasons is that it is costly to produce a suﬃcient volume of training corpora annotated with anaphoric relations. In addition, those above methods focused mainly on identifying antecedents, and few attempts have been made to detect zero pronouns. Motivated by the above background, we propose a probabilistic model for analyzing Japanese"
C02-1078,W98-1119,0,0.059305,"kov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998; Kim and Ehara, 1995; Soon et al., 2001) use statistical models produced based on corpora annotated with anaphoric relations. However, only a few attempts have been made in corpus-based anaphora resolution for Japanese zero pronouns. One of the reasons is that it is costly to produce a suﬃcient volume of training corpora annotated with anaphoric relations. In addition, those above methods focused mainly on identifying antecedents, and few attempts have been made to detect zero pronouns. Motivated by the above background, we propose a probabilistic model for analyzing Japanese zero pronouns co"
C02-1078,C96-1079,0,0.00914834,"resolution in a single framework. The ﬁrst parameter quantiﬁes the degree to which a given case is a zero pronoun. The second parameter quantiﬁes the degree to which a given entity is the antecedent for a detected zero pronoun. To compute these parameters eﬃciently, we use corpora with/without annotations of anaphoric relations. We show the eﬀectiveness of our method by way of experiments. 1 Introduction Anaphora resolution is crucial in natural language processing (NLP), speciﬁcally, discourse analysis. In the case of English, partially motivated by Message Understanding Conferences (MUCs) (Grishman and Sundheim, 1996), a number of coreference resolution methods have been proposed. In other languages such as Japanese and Spanish, anaphoric expressions are often omitted. Ellipses related to obligatory cases are usually termed zero pronouns. Since zero pronouns are not expressed in discourse, they have to be detected prior to identifying their antecedents. Thus, although in English pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing Japanese zero pronouns is diﬀerent from general coreference resolution in English. ishikawa@ulis."
C02-1078,J95-2003,0,0.871527,"to obligatory cases are usually termed zero pronouns. Since zero pronouns are not expressed in discourse, they have to be detected prior to identifying their antecedents. Thus, although in English pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing Japanese zero pronouns is diﬀerent from general coreference resolution in English. ishikawa@ulis.ac.jp For identifying anaphoric relations, existing methods are classiﬁed into two fundamental approaches: rule-based and statistical approaches. In rule-based approaches (Grosz et al., 1995; Hobbs, 1978; Mitkov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone a"
C02-1078,P86-1031,0,0.605916,"ng to past literature associated with zero pronoun resolution and our preliminary study, we use the following six features to model zero pronouns and antecedents. • Features for zero pronouns – Verbs that govern zero pronouns (v), which denote verbs whose cases are omitted. – Surface cases related to zero pronouns (c), for which possible values are Japanese case marker suﬃxes, ga (nominative), wo (accusative), and ni (dative). Those values indicate which cases are omitted. • Features for antecedents – Post-positional particles (p), which play crucial roles in resolving Japanese zero pronouns (Kameyama, 1986; Walker et al., 1994). – Distance (d), which denotes the distance (proximity) between a zero pronoun and an antecedent candidate in an input text. In the case where they occur in the same sentence, its value takes 0. In the case where an antecedent occurs in n sentences previous to the sentence including a zero pronoun, its value takes n. – Constraint related to relative clauses (r), which denotes whether an antecedent is included in a relative clause or not. In the case where it is included, the value of r takes true, otherwise false. The rationale behind this feature is that Japanese zero p"
C02-1078,W98-1502,0,0.019479,"ermed zero pronouns. Since zero pronouns are not expressed in discourse, they have to be detected prior to identifying their antecedents. Thus, although in English pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing Japanese zero pronouns is diﬀerent from general coreference resolution in English. ishikawa@ulis.ac.jp For identifying anaphoric relations, existing methods are classiﬁed into two fundamental approaches: rule-based and statistical approaches. In rule-based approaches (Grosz et al., 1995; Hobbs, 1978; Mitkov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998;"
C02-1078,C96-2137,0,0.0664588,"Since zero pronouns are not expressed in discourse, they have to be detected prior to identifying their antecedents. Thus, although in English pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing Japanese zero pronouns is diﬀerent from general coreference resolution in English. ishikawa@ulis.ac.jp For identifying anaphoric relations, existing methods are classiﬁed into two fundamental approaches: rule-based and statistical approaches. In rule-based approaches (Grosz et al., 1995; Hobbs, 1978; Mitkov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998; Kim and Ehara, 1995; Soon"
C02-1078,W00-1706,0,0.0157396,"e and Bennett (1995) used a decision tree to determine appropriate antecedents for zero pronouns. They focused on proper and deﬁnite nouns used in anaphoric expressions as well as zero pronouns. However, their method resolves only anaphors that refer to organization names (e.g., private companies), which are generally easier to resolve than our case. Both above existing methods require annotated corpora for statistical modeling, while we used corpora with/without annotations related to anaphoric relations, and thus we can easily obtain large-scale corpora to avoid the data sparseness problem. Nakaiwa (2000) used Japanese/English bilingual corpora to identify anaphoric relations of Japanese zero pronouns by comparing J/E sentence pairs. The rationale behind this method is that obligatory cases zero-pronominalized in Japanese are usually expressed in English. However, in the case where corresponding English expressions are pronouns and anaphors, their method is not eﬀective. Additionally, bilingual corpora are more expensive to obtain than monolingual corpora used in our method. Finally, our method integrates a parameter for zero pronoun detection in computing the certainty score. Thus, we can imp"
C02-1078,C96-2147,0,0.720083,"t expressed in discourse, they have to be detected prior to identifying their antecedents. Thus, although in English pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing Japanese zero pronouns is diﬀerent from general coreference resolution in English. ishikawa@ulis.ac.jp For identifying anaphoric relations, existing methods are classiﬁed into two fundamental approaches: rule-based and statistical approaches. In rule-based approaches (Grosz et al., 1995; Hobbs, 1978; Mitkov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998; Kim and Ehara, 1995; Soon et al., 2001) use statist"
C02-1078,J01-4005,0,0.06668,"Missing"
C02-1078,J01-4004,0,0.0957743,"1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998; Kim and Ehara, 1995; Soon et al., 2001) use statistical models produced based on corpora annotated with anaphoric relations. However, only a few attempts have been made in corpus-based anaphora resolution for Japanese zero pronouns. One of the reasons is that it is costly to produce a suﬃcient volume of training corpora annotated with anaphoric relations. In addition, those above methods focused mainly on identifying antecedents, and few attempts have been made to detect zero pronouns. Motivated by the above background, we propose a probabilistic model for analyzing Japanese zero pronouns combined with a detection method. In brief,"
C02-1078,J94-2003,0,0.759662,"ed prior to identifying their antecedents. Thus, although in English pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing Japanese zero pronouns is diﬀerent from general coreference resolution in English. ishikawa@ulis.ac.jp For identifying anaphoric relations, existing methods are classiﬁed into two fundamental approaches: rule-based and statistical approaches. In rule-based approaches (Grosz et al., 1995; Hobbs, 1978; Mitkov et al., 1998; Nakaiwa and Shirai, 1996; Okumura and Tamura, 1996; Palomar et al., 2001; Walker et al., 1994), anaphoric relations between anaphors and their antecedents are identiﬁed by way of handcrafted rules, which typically rely on syntactic structures, gender/number agreement, and selectional restrictions. However, it is diﬃcult to produce rules exhaustively, and rules that are developed for a speciﬁc language are not necessarily eﬀective for other languages. For example, gender/number agreement in English cannot be applied to Japanese. Statistical approaches (Aone and Bennett, 1995; Ge et al., 1998; Kim and Ehara, 1995; Soon et al., 2001) use statistical models produced based on corpora annota"
C04-1093,P00-1062,1,0.676173,"However, the use of existing search engines is associated with the following problems: (a) search engines often retrieve extraneous pages not describing a submitted term, (b) even if desired pages are retrieved, a user has to identify page fragments describing the term, (c) word senses are not distinguished for polysemous terms, such as “hub (device and center)”, (d) descriptions in multiple pages are independent and do not comprise a condensed and coherent text as in existing encyclopedias. The authors of this paper have been resolving these problems progressively. For problems (a) and (b), Fujii and Ishikawa (2000) proposed an automatic method to extract term descriptions from the Web. For problem (c), Fujii and Ishikawa (2001) improved the previous method, so that the multiple descriptions extracted for a single term are categorized into domains and consequently word senses are distinguished. Using these methods, we have compiled an encyclopedic corpus for approximately 600,000 Japanese terms. We have also built a Web site called “Cyclone”1 to utilize this corpus, in which one or more paragraph-style descriptions extracted from diﬀerent pages can be retrieved in response to a user input. In Figure 1, t"
C04-1093,P01-1026,1,0.767561,"trieve extraneous pages not describing a submitted term, (b) even if desired pages are retrieved, a user has to identify page fragments describing the term, (c) word senses are not distinguished for polysemous terms, such as “hub (device and center)”, (d) descriptions in multiple pages are independent and do not comprise a condensed and coherent text as in existing encyclopedias. The authors of this paper have been resolving these problems progressively. For problems (a) and (b), Fujii and Ishikawa (2000) proposed an automatic method to extract term descriptions from the Web. For problem (c), Fujii and Ishikawa (2001) improved the previous method, so that the multiple descriptions extracted for a single term are categorized into domains and consequently word senses are distinguished. Using these methods, we have compiled an encyclopedic corpus for approximately 600,000 Japanese terms. We have also built a Web site called “Cyclone”1 to utilize this corpus, in which one or more paragraph-style descriptions extracted from diﬀerent pages can be retrieved in response to a user input. In Figure 1, three paragraphs describing “XML” are presented with the titles of their source pages. However, the above-mentioned"
C04-1093,J98-3005,0,0.0881099,"Missing"
C04-1093,P01-1059,0,0.0447374,"Missing"
C04-1093,N03-2037,0,0.108808,"utomatically compile a high-quality large encyclopedic corpus using the Web. Hand-crafted encyclopedias lack new terms and new deﬁnitions for existing terms, and thus the quantity problem is crucial. The Web contains unreliable and unorganized information and thus the quality problem is crucial. We intend to alleviate both problems. To the best of our knowledge, no attempt has been made to intend similar purposes. Our research is related to question answering (QA). For example, in TREC QA track, deﬁnition questions are intended to provide a user with the definition of a target item or person (Voorhees, 2003). However, while the expected answer for a TREC question is short deﬁnition sentences as in a dictionary, we intend to produce an encyclopedic text describing a target term from multiple viewpoints. The summarization method proposed in this paper is related to multi-document summarization (MDS) (Mani, 2001; Radev and McKeown, 1998; Schiﬀman et al., 2001). The novelty of our research is that we applied MDS to producing a condensed term description from unorganized Web pages, while existing MDS methods used newspaper articles to produce an outline of an event and a biography of a speciﬁc person."
C96-1012,P92-1032,0,0.411131,"Missing"
C96-1012,C94-1049,0,0.155908,"Missing"
C96-1012,P91-1034,0,0.395741,"Missing"
C96-1012,J94-4003,0,0.176682,"Missing"
C96-1012,P95-1026,0,0.135131,"Missing"
D15-1074,P12-1060,0,0.0230903,"get users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positiv"
D15-1074,C10-2036,0,0.0125367,", we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positive, negative, or neutral. Examples (7) and (8) are"
D15-1074,P11-1013,0,0.180189,"mall kids”. In this paper, we propose a method to extract condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 1 From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements"
D15-1074,D09-1019,0,0.035234,"identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positive, negative, or neutral. Examples (7) and (8) are such conditional sentences associated with neutral and positive categories, respectively. 3 Proposed method The task in this paper is to extract conditionopinion relations from r"
D15-1074,P06-2063,0,0.0320163,"price was not reasonable. (8) If you are looking for a hotel with a reasonable price, stay at hotel A. In example (7), although the subordinate clause includes the opinion word “reasonable”, none of the subordinate clause, main clause, or entire sentence is an opinion. In example (8), the entire sentence is an unconditional opinion about the price for hotel A, but the main and subordinate clauses are not opinions independently. In contrast, the purpose of our research is to identify conditional opinions, in which the main and subordinate clauses are an opinion and its condition, respectively. Kim and Hovy (2006) proposed a method to identify a reason for the evaluation in an opinion, 624 ing Figure 1, which depicts an example input sentence and information related to its constituent bunsetsu phrases. In the upper part of Figure 1, a rectangle and an arrow denote a bunsetstu phrase and a syntactic dependency between two phrases, respectively, and in each phrase we show Japanese words based on the Hepburn system and their English translations in parentheses. CFOs are associated with the following characteristics. CFOs and other CFOs, the above distinction only increases the number of categories to whic"
D15-1074,W02-2016,0,0.0291058,"F4: Phrase distance to aspect Similar to F1, F3 is not robust against errors of the dependency analysis. As in F2, we approximate the value of F4 by a phrase distance between a phrase including an aspect and a phrase in question. phrase that leads to the opinion word via a smaller number of dependency arrows is more likely to be a Cond-phrase. We use the dependency distance (i.e., the number of dependencies) between a phrase in question and the opinion word as the value for feature F1. The value for a phrase is −1 if there is no pass between that phrase and the opinion word. We use “CaboCha” (Kudo and Matsumoto, 2002) for dependency analysis purposes. F5: Difference between values for F2 and F1 A CFO usually consists of a sequence of Cond-phrases where each phrase modifies the next phrase, as in Figure 1. There is a tendency that as the difference of values of F1 and F2 for a phrase becomes smaller, that phrase is more likely to be a Cond-phrase. In Figure 1, the values for Condphrases #3–#6 are smaller than those for Otherphrases #0–#1. F2: Phrase distance to opinion word F1 is not robust against errors of the dependency analysis. To alleviate this problem, we approximate the dependency distance by a phra"
D15-1074,P13-1161,0,0.182481,"t condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 1 From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those who intend to improve the quality of hote"
D15-1074,D10-1006,0,0.219438,"or traveling with small kids”. In this paper, we propose a method to extract condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 1 From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combin"
D15-1074,P13-1172,0,0.156438,"a method to extract condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 1 From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those who intend to impr"
D15-1074,P14-1030,0,0.0115339,"ations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally. 1 From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those who intend to improve the quality of hotel A may investigate"
fujii-2008-producing,N03-2037,0,\N,Missing
fujii-2008-producing,P00-1062,1,\N,Missing
fujii-2008-producing,C04-1093,1,\N,Missing
fujii-2008-producing,P95-1026,0,\N,Missing
fujii-2008-producing,J98-1004,0,\N,Missing
fujii-2008-producing,P01-1026,1,\N,Missing
fujii-2008-producing,fujii-etal-2002-producing,1,\N,Missing
fujii-2008-producing,fujii-etal-2006-test,1,\N,Missing
fujii-2008-producing,A94-1027,0,\N,Missing
fujii-2010-modeling,P00-1062,1,\N,Missing
fujii-2010-modeling,C04-1093,1,\N,Missing
fujii-2010-modeling,P09-1024,0,\N,Missing
fujii-2010-modeling,P08-1092,0,\N,Missing
fujii-2010-modeling,P01-1026,1,\N,Missing
fujii-2010-modeling,fujii-etal-2002-producing,1,\N,Missing
fujii-2010-modeling,fujii-2008-producing,1,\N,Missing
fujii-etal-2002-producing,J93-2003,0,\N,Missing
fujii-etal-2002-producing,P00-1062,1,\N,Missing
fujii-etal-2002-producing,P95-1026,0,\N,Missing
fujii-etal-2002-producing,J98-1004,0,\N,Missing
fujii-etal-2002-producing,P01-1026,1,\N,Missing
fujii-etal-2002-producing,A94-1027,0,\N,Missing
fujii-etal-2004-test,W03-2003,1,\N,Missing
fujii-etal-2006-test,fujii-etal-2004-test,1,\N,Missing
fujii-etal-2008-producing,P02-1040,0,\N,Missing
fujii-etal-2008-producing,P03-1010,1,\N,Missing
fujii-etal-2008-producing,fujii-etal-2006-test,1,\N,Missing
fujii-etal-2012-effects,C04-1093,1,\N,Missing
fujii-etal-2012-effects,P09-1024,0,\N,Missing
fujii-etal-2012-effects,P08-1092,0,\N,Missing
fujii-etal-2012-effects,fujii-2008-producing,1,\N,Missing
fujii-etal-2012-effects,fujii-2010-modeling,1,\N,Missing
fujii-ishikawa-2000-applying,oard-1998-comparative,0,\N,Missing
fujii-ishikawa-2000-applying,mccarley-roukos-1998-fast,0,\N,Missing
fujii-ishikawa-2000-applying,W99-0605,1,\N,Missing
fujii-ishikawa-2000-applying,P99-1027,0,\N,Missing
I08-1001,W01-1703,0,0.0234718,"gy)”. This error occurs because the ending “-ологи (-ology)” does not appear in conventional Mongolian words. In addition, Khaltar et al. (2006)’s method applies (e) in Figure 1 to loanwords, whereas inflection (e) never occurs in noun and adjective loanwords. Lemmatization and stemming are arguably effective for indexing in information retrieval (Hull, 1996; Porter, 1980). Stemmers have been developed for a number of agglutinative languages, including Malay (Tai et al., 2000), Indonesian (Berlian Vega and Bressan, 2001), Finnish (Korenius et al., 2004), Arabic (Larkey et al., 2002), Swedish (Carlberger et al., 2001), Slovene (Popovič and Willett, 1992) and Turkish (Ekmekçioglu et al., 1996). Xu and Croft (1998) and Melucci and Orio (2003) independently proposed a languageindependent method for stemming, which analyzes a corpus in a target language and identifies an equivalent class consisting of an original form, inflected forms, and derivations. However, their method, which cannot identify the original form in each class, cannot be used for natural language applications where word occurrences must be standardized by their original forms. Finite State Transducers (FSTs) have been applied to lemmatization"
I08-1001,P06-1083,1,0.187013,"Type (a) No inflection (b) Vowel insertion (c) Consonant insertion (d) The letters “ь” or “и” are eliminated, and the vowel converts to “и” (e) Vowel elimination Example ном + ын → номын book + genitive case ах + д → ахад brother + dative case байшин + ийн→ байшингийн building + genitive case анги + аас → ангиас return + ablative case ажил + аас → ажлаас work + ablative case Figure 1: Inflection types of content words in Mongolian phrases. 1 correct word is “экологи (ecology)”. This error occurs because the ending “-ологи (-ology)” does not appear in conventional Mongolian words. In addition, Khaltar et al. (2006)’s method applies (e) in Figure 1 to loanwords, whereas inflection (e) never occurs in noun and adjective loanwords. Lemmatization and stemming are arguably effective for indexing in information retrieval (Hull, 1996; Porter, 1980). Stemmers have been developed for a number of agglutinative languages, including Malay (Tai et al., 2000), Indonesian (Berlian Vega and Bressan, 2001), Finnish (Korenius et al., 2004), Arabic (Larkey et al., 2002), Swedish (Carlberger et al., 2001), Slovene (Popovič and Willett, 1992) and Turkish (Ekmekçioglu et al., 1996). Xu and Croft (1998) and Melucci and Orio ("
I08-2086,P04-1021,0,0.0231238,"in Chinese is “肖邦”, where “肖” is commonly used for Chinese family names. Other Kanji characters with the same pronunciation as “肖” include “消”. However, “消”, which means “to disappear”, is not ideal for a person’s name. Thus, Kanji characters must be selected carefully during transliteration into Chinese. This is especially important when foreign companies intend to introduce their names and products into China. In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998; Xu et al., 2006). The second task is back-transliteration (Knight and Graehl, 1998), which identifies the source word corresponding to an existing transliterated word. Both tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered. Thus, backtransliteration is outside the scope of this paper. In the following, we use the term “transliteration” to refer to translite"
I08-2086,P98-2220,0,0.0377887,"here “肖” is commonly used for Chinese family names. Other Kanji characters with the same pronunciation as “肖” include “消”. However, “消”, which means “to disappear”, is not ideal for a person’s name. Thus, Kanji characters must be selected carefully during transliteration into Chinese. This is especially important when foreign companies intend to introduce their names and products into China. In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998; Xu et al., 2006). The second task is back-transliteration (Knight and Graehl, 1998), which identifies the source word corresponding to an existing transliterated word. Both tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be considered. Thus, backtransliteration is outside the scope of this paper. In the following, we use the term “transliteration” to refer to transliteration in the strict sen"
I08-2086,W06-1629,1,0.680495,"Missing"
I08-2086,W03-1508,0,\N,Missing
I08-2086,W03-0317,0,\N,Missing
I08-2086,P98-1036,0,\N,Missing
I08-2086,C98-1036,0,\N,Missing
I08-2086,C98-2215,0,\N,Missing
I08-2086,J98-4003,0,\N,Missing
I08-2104,J93-2003,0,0.0205719,"mono-lingual QA system. For example, after the English question sentence is translated into Japanese, a Japanese mono-lingual QA system can be applied to extract the answer from the Japanese target documents. Depending on the translation techniques used for the pre-processing, the previous CLQA approach can be classified into the machine translation based approach (Shimizu et al., 2005; Mori and Kawagishi, 2005) and the dictionary based approach (Isozaki et al., 2005). In this paper, we propose a novel approach for CLQA task. In the proposed method, the statistical machine translation (SMT) (Brown et al., 1993) is deeply incorporated into the question answering process, instead of using the SMT as the preprocessing before the mono-lingual QA process as in the previous work. Though the proposed method can be applied to any language pairs in principle, we focus on the English-to-Japanese (EJ) CLQA task, where a question sentence is given in English and its answer is extracted from a document collection in Japanese. Recently, language modeling approach for information retrieval has been widely studied (Croft and Lafferty, 2003). Among them, statistical translation model has been applied for mono-lingua"
I08-2104,N03-1010,0,0.0215912,"sed method was compared with the several reference methods. As the methods from previous works, three pre-translation methods were investigated. The first two methods translate the question by using machine translation. One of them used a commercial off-the-shell machine translation software 2 (referred to as RMT). The other used the statistical machine translation that had been created by using the IBM model 4 obtained from the same parallel corpus and tools described in Section 4.2, the tri-gram language model constructed by using the target documents of CLQA1, and the existing SMT decoder (Germann, 2003) (referred to as SMT). The two methods, RMT and SMT, differ only in the translation methods, while their backend monolingual QA systems are common. The third method translates the question by using translation dictionary (referred to as DICT). The cross-lingual IR system described in (Fujii and Ishikawa, 2001) was used for our “document retrieval” subsystem in Figure 2. The CLIR system enhances the basic translation dictionary, which has about 1,000,000 entries, with the compound words obtained by using the statistics of the target documents and with the borrowed words by using the translitera"
I08-2104,J03-1002,0,0.00288902,"scribed above, the corresponding sentence pairs, which are extracted from the articles from 2000 to 2001, were removed from the training corpus for CLQA1. Before training the translation model, both English and Japanese sides of the sentence pairs in parallel corpus were normalized. For the sentences of Japanese side, the inflectional words were normalized to their basic forms by using a Japanese morphological analyzer. For the sentences of English side, the inflectional words were also normalized to their basic forms by using a Part-of-Speech tagger and all the words were lowercased. GIZA++ (Och and Ney, 2003) was used for training the IBM model 4 from the normalized parallel corpus. The vocabulary sizes were about 58K words for Japanese side and 74K words for English side. The trained Japanese-to-English word translation model t(e|j) was used for our proposed document retrieval (Section 3.1) and passage similarity calculation (Section 3.2). 4.3 Compared methods The proposed method was compared with the several reference methods. As the methods from previous works, three pre-translation methods were investigated. The first two methods translate the question by using machine translation. One of them"
I08-2104,P03-1010,0,0.0805143,"Missing"
I13-1111,W03-1210,0,0.0127724,"volve modeling a unit of opinions and searching the corpus for those units, which typically comprise the evaluation by an author for a target object from an aspect. We take the following review sentence as an example opinionated description. “I think hotel A is in a good location for traveling with small kids”. 878 International Joint Conference on Natural Language Processing, pages 878–882, Nagoya, Japan, 14-18 October 2013. Although causal relations can be divided into inter-sentential and intra-sentential, our current interest is more related to the extraction of intrasentential relations (Girju, 2003; Chang and Choi, 2004; Inui et al., 2005). These methods generally identify two event-related components in a sentence and determine the type of the causal relationship between those components, if any, such as “precondition”, “cause-effect”, and “consequence”. An event-related component is usually a word, such as “cancer”, or a proposition, such as “he is a heavy smoker”. However, the above existing methods focused only on specific syntactic patterns, such as “&lt;Clause1, Marker (tame in Japanese), Clause2&gt;” (Inui et al., 2005) and “&lt;NP1, Verb, NP2&gt;” (Girju, 2003; Chang and Choi, 2004). In Sec"
I13-1111,P11-1013,0,0.0710376,"Missing"
I13-1111,P06-2063,0,0.0217622,"y on specific syntactic patterns, such as “&lt;Clause1, Marker (tame in Japanese), Clause2&gt;” (Inui et al., 2005) and “&lt;NP1, Verb, NP2&gt;” (Girju, 2003; Chang and Choi, 2004). In Section 1, none of the example sentences including evaluative conditions matches to those patterns, irrespective of whether in English or in Japanese. Additionally, looking at the examples for “counterfactual” and “concession”, the relation between the evaluation and evaluative condition is different from the causal relation. Besides this, our research is the first attempt to extract cause-like relations in opinion mining. Kim and Hovy (2006) proposed a method to identify a reason for the evaluation in an opinion, such as “the service was terrible because the staff was rude” and “in a good location close to the station”. However, their purpose is to identify grounds that justify the evaluation, which are different from evaluative conditions. and finer-grained analysis for opinion mining. To avoid any confusion, we consistently use the term “opinion unit” to refer to the traditional quintuplebased unit in which a few elements can be omitted. Motivated by the above background, in this paper we propose a method to extract evaluative"
I13-1111,W02-2016,0,0.0651382,"rase in question and a phrase whose value for feature D is 1. In Figure 1, values for phrases #3-5 are 1 and those for the remaining phrases are 0. tract a sequence of I-phrases as an evaluative condition. However, phrases in an opinion unit are always classified into O-phrases. We use Support Vector Machine (SVM) to train a binary classifier for bunsetsu phrases and propose several features associated with lexical and syntactic information. 3.2 Features for phrase classification Figure 1 depicts an example of syntactic dependency analysis for a review sentence in Japanese. We used “CaboCha” (Kudo and Matsumoto, 2002) for the dependency analysis. In Figure 1, a rectangle and an arrow denote a phrase and a dependency between two phrases, respectively, and in each phrase we show Romanized Japanese words and their English translations in parentheses. Looking at Figure 1, by definition the evaluative condition (phrases #3-6) modifies the evaluation expression (phrase #7), but does not modify other opinion elements including the aspect (phrase #2). Also, the evaluative condition ends with specific particles in phrase #6. These properties motivated us to propose the following five features for the binary phrase"
I13-1111,D10-1006,0,0.08459,"Missing"
J98-4002,P91-1034,0,0.0172838,"Missing"
J98-4002,P94-1020,0,0.00915099,"ough experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disa"
J98-4002,J94-4003,0,0.0096378,"d sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such a"
J98-4002,P96-1042,0,0.41794,"s. During this phase, a human expert supervises samples, that is, provides the correct interpretation for the verbs appearing in the samples. Thereafter, samples are simply incorporated into the database without any computational overhead (as would be associated with globally reestimating parameters in statistics-based systems), meaning that the system can be trained on the remaining examples (the ""residue"") for the next iteration. Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general, and have been identified by a number of researchers (Engelson and Dagan 1996; Lewis and Gale 1994; Uramoto 1994a; Yarowsky 1995). 574 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling sampling ~WSD~sD outu t ~ ~ : ( ~ ~ Figure 1 Flow of control of the example sampling system. phases, the system progressively enhances the database. Note that the selective sampiing procedure gives us an optimally informative database of a given size irrespective of the stage at which processing is terminated. Several researchers have proposed this type of approach for NLP applications. Engelson and Dagan (1996) proposed a committee-based sampling method, which is currently applied to"
J98-4002,C96-1012,1,0.808889,"to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is"
J98-4002,C94-2122,0,0.00946794,"tion because the verb in the input is disambiguated by superimposing the sense of the verb appearing in the example of highest similarity.3 The similarity between an input and an example is estimated based on the similarity between case •lers marked with the same case. Furthermore, since the restrictions imposed by the case fillers in choosing the verb sense are not equally selective, Fujii et al. (1996) proposed a weighted case contribution to disambiguation (CCD) of the verb senses. This CCD factor is taken into account 2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994; Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the best matched category with a given input, from candidates. 3 In this paper, we use ""example-based systems"" to refer to systems based on nearest neighbor resolution. predefined 576 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling © nominative accusative Figure 3 The semantic ranges of the nominative and accusative for the verb toru. nq-mc~nc Cs~,c~ nc3-rnllc3 v &,;3 -- v ~s3,c2 ~s3,C3- -- V (S3) J database -- (?) G,c2 (s~)l Figure 4 An input and the database. w h e n c o m p u t i n g the s"
J98-4002,P92-1032,0,0.0186836,"Missing"
J98-4002,P91-1019,0,0.0194231,"approaches to word sense disambiguation should be further investigated, this experimental result gives us good motivation to explore example-based verb sense disambiguation approaches, i.e., to introduce the notion of selective sampling into them. 2.4 Enhancement of Verb Sense Disambiguation Let us discuss how further enhancements to our example-based verb sense disambiguation system could be made. First, since inputs are simple sentences, information for word sense disambiguation is inadequate in some cases. External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyze"
J98-4002,P90-1034,0,0.0202315,"ion have been proposed, for example, by Yarowsky (1992). 578 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling IIII I I I kare kanojo (he) (she) otoko joshu hisho kane heya kippu uma (man) (assistant) (secretary)(money) (room) (ticket) (horse) Figure 5 A fragment of the Bunruigoihyo thesaurus. statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears. Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990). This can be expressed by Equation (3), where ff is the vector for the noun in question, and items ti represent the statistics for predicate argument structures including n. ff = (h, t2,..., ti . . . . ) (3) In regard to ti, we used the notion of TF. IDF (Salton and McGill 1983). TF (term frequency) gives each context (a case marker/verb pair) importance proportional to the number of times it occurs with a given noun. The rationale behind IDF (inverse document frequency) is that contexts that rarely occur over collections of nouns are valuable, and that therefore the IDF of a context is inver"
J98-4002,C92-2101,0,0.0102778,"Missing"
J98-4002,C96-2104,0,0.0251221,"sukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyzer) (Kameda 1996) could broaden the coverage of our system, as inputs are currently limited to simple, morphologically analyzed sentences. Finally, it should be noted that in Japanese, case markers can be omitted or topicalized (for example, marked with postposition wa), an issue which our framework does not currently consider. 3. Example Sampling Algorithm 3.1 Overview Let us look again at Figure 1 in Section 1. In this figure, ""WSD outputs"" refers to a corpus in which each sentence is assigned an expected verb interpretation during the WSD phase. In the training phase, the system stores supervised samples (w"
J98-4002,H93-1051,0,0.0535285,"Missing"
J98-4002,W96-0208,0,0.0139467,"ly one verb sense remains. W h e n more than one verb sense is selected for any given m e t h o d (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). 8 One may argue that this goes against the basis of the rule-based method, in that, given a proper threshold value for the association degree, the system could improve on accuracy (potentially sacrificing coverage), and that the trade-off between coverage and accuracy is therefore a more appropriate evaluation criterion. However, our trials on the rule-based method with different threshold values did not show significant correlation between the improvement of accuracy and the degeneration of coverage. 581 Computational Linguistics Volume 24, Number"
J98-4002,1993.tmi-1.15,0,0.0285586,"ense disambiguation should be further investigated, this experimental result gives us good motivation to explore example-based verb sense disambiguation approaches, i.e., to introduce the notion of selective sampling into them. 2.4 Enhancement of Verb Sense Disambiguation Let us discuss how further enhancements to our example-based verb sense disambiguation system could be made. First, since inputs are simple sentences, information for word sense disambiguation is inadequate in some cases. External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. Second, some idiomatic expressions represent highly restricted collocations, and overgeneralizing them semantically through the use of a thesaurus can cause further errors. Possible solutions would include one proposed by Uramoto, in which idiomatic expressions are described separately in the database so that the system can control their overgeneralization (Uramoto 1994b). Third, a number of existing NLP tools such as JUMAN (a morphological analyzer) (Matsumoto et al. 1993) and QJP (a morphological and syntactic analyzer) (Kameda 1996"
J98-4002,W97-0323,0,0.0563667,"ense remains. W h e n more than one verb sense is selected for any given m e t h o d (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). 8 One may argue that this goes against the basis of the rule-based method, in that, given a proper threshold value for the association degree, the system could improve on accuracy (potentially sacrificing coverage), and that the trade-off between coverage and accuracy is therefore a more appropriate evaluation criterion. However, our trials on the rule-based method with different threshold values did not show significant correlation between the improvement of accuracy and the degeneration of coverage. 581 Computational Linguistics Volume 24, Number 4 Table 2"
J98-4002,P96-1006,0,0.0285131,"verhead for search, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with sys"
J98-4002,C94-1049,0,0.00878414,"h, without the degeneration of the performance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other a"
J98-4002,A97-1056,0,0.0176876,"Missing"
J98-4002,E95-1016,0,0.0189486,"allow only those nouns dominated by the given class in the thesaurus structure as verb complements. In order to identify appropriate thesaurus classes, we used the association measure proposed by Resnik (1993), which computes the information-theoretic association degree between case fillers and thesaurus classes, for each verb sense (Equation (7)). 6 P(rls, c) A(s,c,r) = P(rls, c) • log p(rlc) (7) 6 Note that previous research has applied this technique to tasks other than verb sense disambiguation, such as syntactic disambiguation (Resnik 1993) and disambiguation of case filler noun senses (Ribas 1995). 580 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling Here, A(s, c, r) is the association degree between verb sense s and class r (selectional restriction candidate) with respect to case c. P(rls, c) is the conditional probability that a case filler example associated with case c of sense s is d o m i n a t e d b y class r in the thesaurus. P(rlc ) is the conditional probability that a case filler example for case c (disregarding verb sense) is d o m i n a t e d b y class r. Each probability is estimated based on training data. We used the semantic classes defined in the Bunruigoihyo thes"
J98-4002,C94-2114,0,0.257712,"erformance of the system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated * De"
J98-4002,C96-2163,0,0.0117674,"perimposing the sense of the verb appearing in the example of highest similarity.3 The similarity between an input and an example is estimated based on the similarity between case •lers marked with the same case. Furthermore, since the restrictions imposed by the case fillers in choosing the verb sense are not equally selective, Fujii et al. (1996) proposed a weighted case contribution to disambiguation (CCD) of the verb senses. This CCD factor is taken into account 2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994; Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the best matched category with a given input, from candidates. 3 In this paper, we use ""example-based systems"" to refer to systems based on nearest neighbor resolution. predefined 576 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling © nominative accusative Figure 3 The semantic ranges of the nominative and accusative for the verb toru. nq-mc~nc Cs~,c~ nc3-rnllc3 v &,;3 -- v ~s3,c2 ~s3,C3- -- V (S3) J database -- (?) G,c2 (s~)l Figure 4 An input and the database. w h e n c o m p u t i n g the score for each sense of the verb in question."
J98-4002,C94-2169,0,0.127069,"E TU(x = s) (13) sEK 3.4 Enhancement of Computation In this section, we discuss how to enhance the computation associated with our example sampling algorithm. First, we note that computation of TU(x = s) in Equation (11) above becomes time consuming because the system is required to search the whole set of unsupervised examples for examples whose interpretation certainty will increase after x is used for training. To avoid this problem, we could apply a method used in efficient database search techniques, by which the system can search for neighbor examples of x with optimal time complexity (Utsuro et al. 1994). However, in this section, we will explain another efficient algorithm to identify neighbors of x, in which neighbors of case fillers are considered to be given directly by the thesaurus structure. 12 The basic idea is the following: the system searches for neighbors of each case filler of x instead of x as a whole, and merges them as a set of neighbors of x. Note that by dividing examples along the lines of each case filler, we can retrieve neighbors based on the structure of the Bunruigoihyo thesaurus (instead of the conceptual semantic space as in Figure 7). Let Nx=s,c be a subset of unsup"
J98-4002,C92-2070,0,0.0363747,"pace model"" (VSM) (Frakes and Baeza-Yates 1992; Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which has a long history of application in information retrieval (IR) and text categorization (TC) tasks. In the case of IR/TC, VSM is used to compute the similarity between documents, which is represented by a vector comprising statistical factors of content words in a document. Similarly, in our case, each noun is represented by a vector comprising 4 Different types of application of hand-crafted thesauri to word sense disambiguation have been proposed, for example, by Yarowsky (1992). 578 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling IIII I I I kare kanojo (he) (she) otoko joshu hisho kane heya kippu uma (man) (assistant) (secretary)(money) (room) (ticket) (horse) Figure 5 A fragment of the Bunruigoihyo thesaurus. statistical factors, although statistical factors are calculated in terms of the predicate argument structure in which each noun appears. Predicate argument structures, which consist of complements (case filler nouns and case markers) and verbs, have also been used in the task of noun classification (Hindle 1990). This can be expressed by Equation (3), wh"
J98-4002,P95-1026,0,0.654354,"he system. 1. Introduction Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen 1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993). Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987]), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated * Department of Libra"
J98-4002,C92-2107,0,\N,Missing
P00-1062,J96-1001,0,\N,Missing
P00-1062,P99-1068,0,\N,Missing
P00-1062,P93-1023,0,\N,Missing
P01-1026,J93-2003,0,0.00294334,"is purpose, a number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine used in our system) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ (d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P (d) ≈ PL (d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia. We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and then used t"
P01-1026,P00-1062,1,0.717953,"nts describing the term. Finally, we organize extracted term descriptions based on word senses and domains. In addition, we apply an automatically generated encyclopedia to a question answering system targeting the Japanese InformationTechnology Engineers Examination. 1 Introduction Reflecting the growth in utilization of the World Wide Web, a number of Web-based language processing methods have been proposed within the natural language processing (NLP), information retrieval (IR) and artificial intelligence (AI) communities. A sample of these includes methods to extract linguistic resources (Fujii and Ishikawa, 2000; Resnik, 1999; Soderland, 1997), retrieve useful information in response to user queries (Etzioni, 1997; McCallum et al., 1999) and mine/discover knowledge latent in the Web (Inokuchi et al., 1999). In this paper, mainly from an NLP point of view, we explore a method to produce linguistic resources. Specifically, we enhance the method proposed by Fujii and Ishikawa (2000), which extracts encyclopedic knowledge (i.e., term descriptions) from the Web. In brief, their method searches the Web for pages containing a term in question, and uses linguistic expressions and HTML layouts to extract frag"
P01-1026,C00-1043,0,0.0553677,"Missing"
P01-1026,C92-2082,0,0.0157474,"Missing"
P01-1026,J98-1004,0,0.189489,"Missing"
P01-1026,A94-1027,0,0.422444,"model. Then we discard such c’s whose P (d|c) is below a specific threshold. As a result, for the input term, related domains and descriptions are simultaneously selected. Thus, we do not have to know a priori which domains are related to each term. In the following two sections, we explain methods to realize the domain and description models, respectively. Domain Model The domain model quantifies the extent to which description d is associated with domain c, which is fundamentally a categorization task. Among a number of existing categorization methods, we experimentally used one proposed by Iwayama and Tokunaga (1994), which formulates P (c|d) as in Equation (2). P (c|d) = P (c) ·  P (t|c) · P (t|d) t P (t) (2) Here, P (t|d), P (t|c) and P (t) denote probabilities that word t appears in d, c and all the domains, respectively. We regard P (c) as a constant. While P (t|d) is simply a relative frequency of t in d, we need predefined domains to compute P (t|c) and P (t). For this purpose, the use of large-scale corpora annotated with domains is desirable. However, since those resources are prohibitively expensive, we used the “Nova” dictionary for Japanese/English machine translation systems3 , which includes"
P01-1026,P95-1026,0,0.0424093,"the page, for which two rules are used. 1 http://www.google.com/ 2.4 Organization As discussed in Section 1, organizing information extracted from the Web is crucial in our framework. For this purpose, we classify extracted term descriptions based on word senses and domains. Although a number of methods have been proposed to generate word senses (for example, one based on the vector space model (Sch¨utze, 1998)), it is still difficult to accurately identify word senses without explicit dictionaries that define sense candidates. In addition, since word senses are often associated with domains (Yarowsky, 1995), word senses can be consequently distinguished by way of determining the domain of each description. For example, different senses for “pipeline (processing method/transportation pipe)” are associated with the computer and construction domains (fields), respectively. To sum up, the organization module classifies term descriptions based on domains, for which we use domain and description models. In Section 3, we elaborate on our organization model. 2 <DT> and <DD> are inherently provided to describe terms in HTML. 3 Statistical Organization Model 3.1 3.2 Overview Given one or more (in most cas"
P01-1026,P00-1071,0,0.0175469,"ctively. We used a probabilistic method (Robertson and Walker, 1994), which is one of the major IR methods. To sum up, given a question, its type and four choices, our QA system chooses one of four candidates as the answer, in which the resolution algorithm varies depending on the question type. 4.4 Related Work Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000), question answering has of late become one of the major topics within the NLP/IR communities. In fact, a number of QA systems targeting the TREC QA collection have recently been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000). Those systems are commonly termed “open-domain” systems, because questions expressed in natural language are not necessarily limited to explicit axes, including who, what, when, where, how and why. However, Moldovan and Harabagiu (2000) found that each of the TREC questions can be recast as either a single axis or a combination of axes. They also found that out of the 200 TREC questions, 64 questions (approximately one third) were associated with the what axis, for which the Web-based encyclopedia is expected to improve the quality of answers. Although Harabagiu et al."
P01-1026,C88-2098,0,0.0720208,"Missing"
P01-1026,P99-1068,0,\N,Missing
P06-1083,J96-1001,0,0.167848,"Missing"
P06-1083,W97-0119,0,\N,Missing
P06-1083,W04-1809,1,\N,Missing
W01-1208,J93-2003,0,0.00318391,"number of quality rating methods for Web pages (Amento et al., 2000; Zhu and Gauch, 2000) can be used. However, since Google (i.e., the search engine we used in the retrieval module) rates the quality of pages based on hyperlink information, and selectively retrieves those with higher quality (Brin and Page, 1998), we tentatively regarded PQ (d) as a constant. Thus, in practice the description model is approximated solely with the language model as in Equation (4). P (d) ≈ PL (d) (4) Statistical approaches to language modeling have been used in much NLP research, such as machine translation (Brown et al., 1993) and speech recognition (Bahl et al., 1983). Our language model is almost the same as existing models, but is different in two respects. First, while general language models quantify the extent to which a given word sequence is linguistically acceptable, our model also quantifies the extent to which the input is acceptable as a term description. Thus, we trained the model based on an existing machine readable encyclopedia. We used the ChaSen morphological analyzer to segment the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998) into words (we replaced headwords with a common symbol), and th"
W01-1208,P00-1062,1,0.887962,"sed QA system, most existing systems rely on conventional IR and shallow NLP methods. However, question answering is inherently a more complicated procedure that usually requires explicit knowledge bases. In this paper, we propose a question answering system which uses an encyclopedia as a knowledge base. However, since existing (published) encyclopedias usually lack technical/new terms, we generate one based on the World Wide Web, which includes a number of technical and recent information. For this purpose, we use a modified version of our method to extract term descriptions from Web pages (Fujii and Ishikawa, 2000). Intuitively, our system answers interrogative questions like “What is X?” in which a QA system searches an encyclopedia database for one or more descriptions related to term X. The performance of QA systems can be evaluated based on coverage and accuracy. Coverage is the ratio between the number of questions answered (disregarding their correctness) and the total number of questions. Accuracy is the ratio between the number of correct answers and the total number of answers made by the system. While coverage can be estimated objectively and systematically, estimating accuracy relies on human"
W01-1208,C00-1043,0,0.0629477,"Missing"
W01-1208,A94-1027,0,0.0165125,"del. Then we discard such c whose P (d|c) is below a specific threshold. As a result, for the input term, related domains and descriptions are simultaneously selected. Thus, we do not have to know a priori which domains are related to each term. In the following two sections, we explain methods to realize the domain and description models, respectively. 5.2 Domain Model The domain model quantifies the extent to which description d is associated with domain c, which is fundamentally a categorization task. Among a number of existing categorization methods, we experimentally used one proposed by Iwayama and Tokunaga (1994), which formulates P (c|d) as in Equation (2). P (c|d) = P (c) · (1) In practice, P (c) can be omitted because this factor is a constant, and thus does not affect the relative probability for different descriptions. In Equation (1), P (c|d) models a probability that d corresponds to domain c. P (d) models a probability that d can be a description for the term in question, disregarding the domain. We shall call them domain and description models, respectively. To sum up, in principle we select d’s that are strongly associated with a certain domain, and are likely to be descriptions themselves."
W01-1208,P00-1071,0,0.138722,"ct text fragments describing the term. Finally, extracted term descriptions are organized based on word senses and domains. We also evaluate our system by way of experiments, where the Japanese Information-Technology Engineers Examination is used as a test collection. 1 Introduction Motivated partially by the TREC-8 QA collection (Voorhees and Tice, 2000), question answering has of late become one of the major topics within the natural language processing and information retrieval communities, and a number of QA systems targeting the TREC collection have been proposed (Harabagiu et al., 2000; Moldovan and Harabagiu, 2000; Prager et al., 2000). Although Harabagiu et al. (2000) proposed a knowledge-based QA system, most existing systems rely on conventional IR and shallow NLP methods. However, question answering is inherently a more complicated procedure that usually requires explicit knowledge bases. In this paper, we propose a question answering system which uses an encyclopedia as a knowledge base. However, since existing (published) encyclopedias usually lack technical/new terms, we generate one based on the World Wide Web, which includes a number of technical and recent information. For this purpose, we us"
W01-1208,J98-1004,0,0.0505068,"Missing"
W01-1208,P95-1026,0,0.019108,"ged with <P>, 3. itemization tagged with <UL>, 4. N sentences, where we empirically set N = 3. 4.4 Organization For the purpose of organization, we classify extracted term descriptions based on word senses and domains. Although a number of methods have been proposed to generate word senses (for example, one based on the vector space model (Sch¨utze, 1998)), it is still difficult to accurately identify word senses without explicit dictionaries that predefine sense candidates. 3 <DT> and <DD> are inherently provided to describe terms in HTML. Since word senses are often associated with domains (Yarowsky, 1995), word senses can be consequently distinguished by way of determining the domain of each description. For example, different senses for “pipeline (processing method/transportation pipe)” are associated with computer and construction domains (fields), respectively. To sum up, the organization module classifies term descriptions based on domains, for which we use domain and description models. In Section 5, we elaborate on the organization model. 5 Statistical Organization Model 5.1 Overview Given one or more (in most cases more than one) descriptions for a single term, the organization module s"
W02-1025,H94-1074,0,0.0847069,"Missing"
W02-1025,H92-1073,0,0.157783,"Missing"
W04-1809,J96-1001,0,0.457745,"Missing"
W06-0303,C04-1200,0,0.0542724,"the methodologies of its components. Section 5 describes the experiments and discusses the results obtained. 2 Related Work For process (1) in Section 1, existing search engines can be used to search the Web for documents related to a specific topic. However, not all retrieved documents include subjective descriptions for the topic. A solution to this problem is to automatically identify diaries and blogs (Nanno et al., 2004), which usually include opinionated subjective descriptions. For process (2), existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy, 2004; Pang and Lee, 2004; Riloff and Wiebe, 2003). For process (3), machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al., 2003; Beineke et al., 2004; Hu and Liu, 2004; Pang and Lee, 2004) or multipoint scale categories (Kim and Hovy, 2004; Pang and Lee, 2005). For process (4), which is the subject of this paper, Ku et al. (2005) selected documents that include a large number of positive or negative sentences about a target topic, and used their headlines as a summary of the topic. This is the application of an existing extraction-based"
W06-0303,P04-1035,0,0.0197953,"f its components. Section 5 describes the experiments and discusses the results obtained. 2 Related Work For process (1) in Section 1, existing search engines can be used to search the Web for documents related to a specific topic. However, not all retrieved documents include subjective descriptions for the topic. A solution to this problem is to automatically identify diaries and blogs (Nanno et al., 2004), which usually include opinionated subjective descriptions. For process (2), existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy, 2004; Pang and Lee, 2004; Riloff and Wiebe, 2003). For process (3), machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al., 2003; Beineke et al., 2004; Hu and Liu, 2004; Pang and Lee, 2004) or multipoint scale categories (Kim and Hovy, 2004; Pang and Lee, 2005). For process (4), which is the subject of this paper, Ku et al. (2005) selected documents that include a large number of positive or negative sentences about a target topic, and used their headlines as a summary of the topic. This is the application of an existing extraction-based summarization method"
W06-0303,P05-1015,0,0.0238228,"ions for the topic. A solution to this problem is to automatically identify diaries and blogs (Nanno et al., 2004), which usually include opinionated subjective descriptions. For process (2), existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy, 2004; Pang and Lee, 2004; Riloff and Wiebe, 2003). For process (3), machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al., 2003; Beineke et al., 2004; Hu and Liu, 2004; Pang and Lee, 2004) or multipoint scale categories (Kim and Hovy, 2004; Pang and Lee, 2005). For process (4), which is the subject of this paper, Ku et al. (2005) selected documents that include a large number of positive or negative sentences about a target topic, and used their headlines as a summary of the topic. This is the application of an existing extraction-based summarization method to subjective descriptions. Hu and Liu (2004) summarized customer reviews of a product such as a digital camera. Their summarization method extracts nouns and noun phrases as features of the target product, (e.g., “picture” for a digital camera), and lists positive and negative reviews on a feat"
W06-0303,P04-1034,0,0.0304401,"Web for documents related to a specific topic. However, not all retrieved documents include subjective descriptions for the topic. A solution to this problem is to automatically identify diaries and blogs (Nanno et al., 2004), which usually include opinionated subjective descriptions. For process (2), existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy, 2004; Pang and Lee, 2004; Riloff and Wiebe, 2003). For process (3), machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al., 2003; Beineke et al., 2004; Hu and Liu, 2004; Pang and Lee, 2004) or multipoint scale categories (Kim and Hovy, 2004; Pang and Lee, 2005). For process (4), which is the subject of this paper, Ku et al. (2005) selected documents that include a large number of positive or negative sentences about a target topic, and used their headlines as a summary of the topic. This is the application of an existing extraction-based summarization method to subjective descriptions. Hu and Liu (2004) summarized customer reviews of a product such as a digital camera. Their summarization method extracts nouns and noun phrases as features o"
W06-0303,W03-1014,0,0.0524093,"ction 5 describes the experiments and discusses the results obtained. 2 Related Work For process (1) in Section 1, existing search engines can be used to search the Web for documents related to a specific topic. However, not all retrieved documents include subjective descriptions for the topic. A solution to this problem is to automatically identify diaries and blogs (Nanno et al., 2004), which usually include opinionated subjective descriptions. For process (2), existing methods aim to distinguish between subjective and objective descriptions in texts (Kim and Hovy, 2004; Pang and Lee, 2004; Riloff and Wiebe, 2003). For process (3), machine-learning methods are usually used to classify subjective descriptions into bipolar categories (Dave et al., 2003; Beineke et al., 2004; Hu and Liu, 2004; Pang and Lee, 2004) or multipoint scale categories (Kim and Hovy, 2004; Pang and Lee, 2005). For process (4), which is the subject of this paper, Ku et al. (2005) selected documents that include a large number of positive or negative sentences about a target topic, and used their headlines as a summary of the topic. This is the application of an existing extraction-based summarization method to subjective descriptio"
W06-1629,P04-1021,0,0.0472248,"–249, c Sydney, July 2006. 2006 Association for Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 source word 㓈ᡸ (safeguard) ҪҎ (another person) ⫳ᄬ (live) 㧹( ݏnutrition) 䊎䉺䊚䊮 (bitamin) Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word"
W06-1629,P98-2220,0,0.31143,"006. 2006 Association for Computational Linguistics Section 2 surveys previous research into automatic transliteration, in order to clarify the meaning and contribution of our research. Section 3 elaborates on our transliteration method. Section 4 evaluates the effectiveness of our method. 2 source word 㓈ᡸ (safeguard) ҪҎ (another person) ⫳ᄬ (live) 㧹( ݏnutrition) 䊎䉺䊚䊮 (bitamin) Related Work In a broad sense, the term “transliteration” has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word corresponding to an existing transliterated word. Back-transliteration is intended mainly for cross-lingual information retrieval and machine translation. Both transliteration tasks require methods that model pronunciation in the source and target languages. However, by definition, in back-transliteration, the word in question has already been transliterated and the meaning or impression of the source word does not have to be consi"
W06-1629,C98-2215,0,\N,Missing
W06-1629,J98-4003,0,\N,Missing
W12-5213,A00-1031,0,0.10963,"Missing"
W12-5213,2007.mtsummit-tutorials.1,0,0.0367619,"Missing"
W12-5213,J03-1002,0,0.00429709,"Missing"
W12-5213,P02-1040,0,0.0913632,"Missing"
W12-5213,W04-3250,0,\N,Missing
W96-0105,P91-1034,0,0.0225877,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,J94-4003,0,0.0277881,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,1993.mtsummit-1.10,0,0.0328193,"is small in both figure l l - a and ll-b. However, in the situation as in figure ll-b, since (a) the task of distinction between the verb senses 1 and 2 is easier, and (b) instances where the sense ambiguity of case fillers corresponds to distinct verb senses will be rare, training using either ""xl"" or ""x2"" will be less effective than as in figure ll-a. It should also be noted that since Bunruigoihyo is a relatively small-sized thesaurus and does not enumerate many word senses, this problem is not critical in our case. However, given other existing thesauri like the EDR electronic dictionary [4] or WordNet [15], these two situations should be strictly differentiated. 6 Conclusion In this paper we proposed an example sampling method for example-based verb sense disambiguation. We also reported on the system&apos;s performance by way of experiments. The experiments showed that our method, which is based on the notion of training utility, has reduced the overhead for the training of the system, as well as the size of the database. As pointed out in section 1, the generalization of examples [8, 19] is another method for reducing the size of the database. Whether coupling these two methods wou"
W96-0105,C96-1012,1,0.79323,"x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for each x, the system simply needs to compare it with the newly computed score between x and the newly stored"
W96-0105,P92-1032,0,0.0179473,"cles. Each of the sentences in the training/test data used 64 in our experiment contained one or several complement(s) followed by one of the ten verbs enumerated in table 2. In table 2, the column of ""English gloss"" describes typical English translations of the Japanese verbs. The column of "" # of sentences"" denotes the number of sentences in the corpus, "" # of senses"" denotes the number of verb senses based on IPAL, and ""lower bound"" denotes the precision gained by using a naive method, where the system systematically chooses the most frequently appearing interpretation in the training data [6]. Table 2: The corpus used for the experiments verb II English gloss ataeru kakeru kuwaeru noru osameru tsukuru torn umu wakaru yameru total [I # of sentences # of senses lower bound give hang add ride govern make take bear offspring understand stop 136 160 167 126 108 126 84 90 60 54 4 29 5 10 8 15 29 2 5 2 66.9 25.6 53.9 45.2 25.0 19.8 26.2 81.1 48.3 59.3 -- 1111 -- 43.7 We at first estimated the system&apos;s performance by its precision, that is the ratio of the number of correct outputs, compared to the number of inputs. In this experiment, we set = 0.5 in equation (7), and k = 1 in equation ("
W96-0105,C92-2101,0,0.216335,"with respect to the correctness of the answer. Dagan et al. proposed a committee-based sampling method, which is currently applied to HMM training for part-of-speech tagging [2]. This method selects samples based on the training utility factor of the examples, i.e. the informativity of the data with respect to future training. However, as all these methods are implemented for statistics-based models, there is a need to explore how to formalize and map these concepts into the examplebased approach. With respect to problem 3, a possible solution would be the generalization of redundant examples [8, 19]. However, such an approach implies a significant overhead for the manual training of each example prior to the generalization. This shortcoming is precisely what our approach allows to avoid: reducing both the overhead as well as the size of the database. Section 2 briefly describes our method for a verb sense disambiguation system. The next Section 3 elaborates on the example sampling method, while section 4 reports on the results of our experiment. Before concluding in section 6, discussion is added in section 5. 57 2 E x a m p l e - b a s e d verb sense disambiguation s y s t e m suri kano"
W96-0105,C94-1049,0,0.0253196,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,P95-1026,0,0.0470033,"). S =XUT (5) We introduce a utility function TUF(x), which computes the training utility figure for an example x. The sampling algorithm gives preference to examples of maximum utility, by way of equation (6). arg max TUF(x) (6) xEX We will explain in the following sections how one could estimate TUF, based on the estimation of the certainty figure of an interpretation. Ideally the sampling size, i.e. the number of samples selected at each iteration would be such as to avoid retraining of similar examples. It should be noted that this can be a critical problem for statistics-based approaches [1, 3, 18, 20, 24], as the reconstruction of statistic classifiers is expensive. However, example-based systems [5, 12, 21] do not require the reconstruction of the system, but examples have to be stored in the database. It also should be noted that in each iteration, the system needs only compute the similarity between each example x belonging to X and the newly stored example, instead of every example belonging to T, because of the following reasons: • storing an example of verb sense interpretation senses, 61 si, will not affect the score of other verb • if the system memorizes the current score of si for ea"
W96-0105,C92-2107,0,\N,Missing
W97-0803,A92-1013,0,0.0584445,"Missing"
W97-0803,P93-1024,0,0.251123,"Missing"
W97-0803,P93-1023,0,0.0593941,"Missing"
W97-0803,P90-1034,0,0.136806,"Missing"
W97-0803,A94-1027,1,0.887039,"Missing"
W97-0803,C96-2161,0,0.149207,"Missing"
W97-0803,C96-2212,0,0.0605532,"Missing"
W97-0803,C92-2070,0,0.10255,"Missing"
W97-0803,P92-1053,0,\N,Missing
W97-0807,P92-1032,0,0.0422726,"Missing"
W97-0807,C94-2119,0,0.0313115,"Missing"
W97-0807,P90-1034,0,0.0911779,"Missing"
W97-0807,C96-2104,0,0.0249835,"Missing"
W97-0807,P94-1038,0,0.0591022,"Missing"
W97-0807,P93-1024,0,0.124804,"Missing"
W97-0807,J98-4002,1,0.85571,"Missing"
W97-0807,C96-1012,1,\N,Missing
W99-0605,P98-1036,0,0.0309109,"y satisfactory to exhaustively enumerate newly emerging terms in dictionaries, (2) Asian languages often represent loanwords based on their special phonograms (primarily for technical terms and proper nouns), which creates new base words progressively (in the case of Japanese, the phonogram is called katakana). To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999), which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. For problem (2), we use &quot;transliteration&quot; (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998). Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters,"
W99-0605,J93-1001,0,0.0529882,"Missing"
W99-0605,P95-1032,0,0.0645406,"Missing"
W99-0605,C69-0401,0,0.709432,"w. rd. nacs is. ac. j p/-nt cadm/index-en, html terms of the implementation of the translation phase. The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998), while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997). The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996). We prefer the first approach, the &quot;query translation&quot;, to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. At the same time, we concede that other CLIR approaches"
W99-0605,J96-1001,0,0.0329114,"Missing"
W99-0605,P98-2220,0,0.0632683,"Missing"
W99-0605,C98-1036,0,\N,Missing
W99-0605,J98-4003,0,\N,Missing
