1993.iwpt-1.12,H92-1061,0,0.0332644,"Missing"
1993.iwpt-1.12,P91-1014,0,\N,Missing
1993.iwpt-1.12,P89-1018,0,\N,Missing
1995.tmi-1.13,P95-1016,0,0.062519,"rman, and English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing."
1995.tmi-1.13,E95-1026,0,0.0140163,"English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing. Thus, comp"
1995.tmi-1.13,1995.tmi-1.15,0,0.606961,"agram is shown in Figure 2.1 Processing starts with speech input in the source language. Recognition of the speech signal is done with acoustic modeling methods, constrained by a language model. The output of speech recognition is a word lattice. We prefer working with word lattices rather than the more common approach of processing N-best lists of hypotheses. An N-best list may be largely redundant and can be efficiently represented in the form of a lattice. Using a lattice parser can thus reduce time and space 1 Another approach being pursued in parallel in the Janus project is described in [10] 174 175 complexity relative to parsing a corresponding N-best list. Selection of the correct path through the lattice is accomplished during parsing when more information is available. Lattices, however, are potentially inefficient because of their size. We apply four steps to make them more tractable ([11]). The first step involves cleaning the lattice by mapping all non-human noises and pauses into a generic pause. Consecutive pauses are then adjoined to one long pause. The resulting lattice contains only linguistically meaningful information. The lattice is then broken at points where no h"
1995.tmi-1.13,1993.iwpt-1.12,1,0.826786,"et of sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice ar"
1995.tmi-1.13,P94-1045,1,0.890553,"sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are comb"
1995.tmi-1.13,P92-1025,0,0.0443447,"a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are combined, yielding a list of ILT sequences that represent the possible sentences in a full multi-sentence turn. An ILT n-gram is applied to each such list to determine the probability of each sequence of sentences. The discourse processor, based on Lambert’s work ([14, 15]), disambiguates the speech act of each sentence, normalizes temporal expressions, and incorporates the sentence into a discourse plan tree. The discourse processor's focusing heuristics and plan operators eliminate some ambiguity by filtering out hypotheses that do not fit into the current discourse context. The discourse component also updates a calendar in the dynamic discourse memory to keep track of what the speakers have said about their schedules. As processing continues, the N-best hypotheses for sequences of ILTs in a multisentence turn are sent to the generator. The generation output"
1996.amta-1.30,1993.iwpt-1.12,1,0.894049,"Missing"
1996.amta-1.30,P94-1045,1,0.845094,"Missing"
1996.amta-1.30,P81-1022,0,0.0225358,"Missing"
2000.iwpt-1.16,P89-1018,0,0.399379,"ses individually. As a result , an exponential number of parse trees can be succinctly represented , and parsing can still be performed in polynomial time. Obviously, in order to achieve optimal parsing efficiency, the parsing algorithm must identify all pos sible local ambiguities and pack them together. Certain context-free parsing algorithms are inherently better at this task than others. Tabular parsing algorithms such as CKY [21] by design synchronize processing in a way that supports easy identification of local ambiguities. On the other hand, as has been pointed out by Billot and Lang [1] , the Generalized LR Parsing algorithm (GLR) [18] is not capable of performing full ambiguity packing, due to the fact that stacks that end in different states must be kept distinct. There has been some debate in the parsing community regarding the relative efficiency of GLR and chart parsers, with conflicting evidence in both directions [16] , [20} , [2] , [14] . The relative effectiveness of performing ambiguity packing has not received full attention in this debate, and may in fact account for some of the conflicting evidence 1 . 2.2 The Problem: Ambiguity Packing in CFG Parsing with Inter"
2000.iwpt-1.16,J93-1002,0,0.0624449,"Missing"
2000.iwpt-1.16,P88-1035,0,0.0586349,"ith a significant effect on the performance of the parser. It should be noted that effective local ambiguity packing when parsing with unification-augmented I 1 CFGs requires not only a compact representation for the packed c-structure node, but also an ef fective representation of the associated f-structures. The issue of how to effectively pack ambiguous 1 For example, van Noord [20] compares left corner and chart parsers that do apply ambiguity packing with a GLR parser without ambiguity packing. 149 f-structures has received quite a bit of attention in the literature over the last decade [4] [11] [12] [13] [6] . While the two issues are related , we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. W hile there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-structure level is a necessary pre-condition for efficient parsing, regardless of how well f-structures are packed. 3 The Rule Priorit ization Heuristic In order to ensure effective ambiguity packing in unification-a"
2000.iwpt-1.16,P99-1061,0,0.163013,"Missing"
2000.iwpt-1.16,P94-1045,1,0.840585,"ture bundles that are associated with the non-terminals of the rules. Feature structure computation is, for the most part , specified and implemented via unification operations. This allows the grammar to constrain the applicability of context-free rules. A reduction by a context-free rule succeeds only if the associated feature structure unification is successful as well. The Generalized LR Parser/Compiler is implemented in Common Lisp, and has been used as the analysis component of several different projects at the Center for Machine Translation at CMU in the course of the last decade. GLR* [7] , [9] , [8] , the robust version of the parser, was constructed as an extended version of the unification-based Generalized LR Parser/Compiler. The parser skips parts of the utterance that it cannot incorporate into a well-formed sentence structure. Thus it is well-suited to domains in which non-grammaticality is common. The parser conducts a search for the maximal subset of the original input that is covered by the grammar. This is done using a beam search heuristic that limits the combinations of skipped words considered by the parser, and ensures that it operates within feasible time and s"
2000.iwpt-1.16,C96-1075,1,0.830539,"rtion of the input sentence may be reduced to a non-terminal symbol in many different ways, when considering different subsets of the input that may be skipped. Thus, efficient runtime performance of the robust versions of the parsers is even more dependent on effective local ambiguity packing. We therefore conducted evaluations with the two parsers in both robust and non-robust modes, in order to quantify the effect of the rule selection heuristic under both scenarios. All of the described experiments were conducted on a common test set of 520 sentences from the JANUS English Scheduling Task [10], using a general English syntactic grammar developed at Carnegie Mellon University. The grammar has 412 rules and 71 non-terminals, and produces a full predicate-argument structure analysis in the form of a feature structure. For the GLR parser, the 153 35 .-----.-----..------�-----. Sentenoo Length Dlatributlon -+30 25 20 15 10 o _____._____...______._______. 10 Sentence l.englh 15 20 Figure 2: Distribution of evaluation set sentence lengths grammar compiles into an SLR parsing table with 628 states and 8822 parsing actions. Figure 2 shows the distribution of sentence lengths in the evaluati"
2000.iwpt-1.16,J93-4001,0,0.0821809,"Missing"
2000.iwpt-1.16,P99-1075,0,0.0804833,"ant effect on the performance of the parser. It should be noted that effective local ambiguity packing when parsing with unification-augmented I 1 CFGs requires not only a compact representation for the packed c-structure node, but also an ef fective representation of the associated f-structures. The issue of how to effectively pack ambiguous 1 For example, van Noord [20] compares left corner and chart parsers that do apply ambiguity packing with a GLR parser without ambiguity packing. 149 f-structures has received quite a bit of attention in the literature over the last decade [4] [11] [12] [13] [6] . While the two issues are related , we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. W hile there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-structure level is a necessary pre-condition for efficient parsing, regardless of how well f-structures are packed. 3 The Rule Priorit ization Heuristic In order to ensure effective ambiguity packing in unification-augmented contex"
2000.iwpt-1.16,J87-1004,0,0.171613,"mber of parse trees can be succinctly represented , and parsing can still be performed in polynomial time. Obviously, in order to achieve optimal parsing efficiency, the parsing algorithm must identify all pos sible local ambiguities and pack them together. Certain context-free parsing algorithms are inherently better at this task than others. Tabular parsing algorithms such as CKY [21] by design synchronize processing in a way that supports easy identification of local ambiguities. On the other hand, as has been pointed out by Billot and Lang [1] , the Generalized LR Parsing algorithm (GLR) [18] is not capable of performing full ambiguity packing, due to the fact that stacks that end in different states must be kept distinct. There has been some debate in the parsing community regarding the relative efficiency of GLR and chart parsers, with conflicting evidence in both directions [16] , [20} , [2] , [14] . The relative effectiveness of performing ambiguity packing has not received full attention in this debate, and may in fact account for some of the conflicting evidence 1 . 2.2 The Problem: Ambiguity Packing in CFG Parsing with Interleaved Unification respect to the order in which v"
2000.iwpt-1.16,J97-3004,0,0.0375142,"cessed . Instead, the parser creates a new parse node for the newly discovered local ambiguity and processes the new node separately. As a result, the parser&apos;s overall local ambiguity packing is less than optimal, with a significant effect on the performance of the parser. It should be noted that effective local ambiguity packing when parsing with unification-augmented I 1 CFGs requires not only a compact representation for the packed c-structure node, but also an ef fective representation of the associated f-structures. The issue of how to effectively pack ambiguous 1 For example, van Noord [20] compares left corner and chart parsers that do apply ambiguity packing with a GLR parser without ambiguity packing. 149 f-structures has received quite a bit of attention in the literature over the last decade [4] [11] [12] [13] [6] . While the two issues are related , we focus here on how to achieve optimal packing of c-structures, so that unification operations do not have to be re-executed due to the later detection of an additional local ambiguity. W hile there is much to be gained from improved packing on the f-structure level, one should note that effective ambiguity packing on the c-st"
2001.mtsummit-papers.69,C96-1030,1,0.886417,"Missing"
2001.mtsummit-papers.69,hogan-frederking-1998-evaluation,1,\N,Missing
2001.mtsummit-papers.69,C00-1019,1,\N,Missing
2001.mtsummit-papers.69,H01-1002,1,\N,Missing
2001.mtsummit-road.7,J90-2002,0,0.143628,"Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are using the EBMT engine that was developed for the Diplomat and Tongues systems (Brown, 1996)1. In addition, we plan to develop statistical techniques for robust MT with sparse data using expone"
2001.mtsummit-road.7,C96-1030,1,0.787634,"rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are using the EBMT engine that was developed for the Diplomat and Tongues systems (Brown, 1996)1. In addition, we plan to develop statistical techniques for robust MT with sparse data using exponential models a"
2001.mtsummit-road.7,jones-havrilla-1998-twisted,0,0.0712645,"ng indigenous people to give up their languages. MT additionally facilitates the design of educational programs in endangered languages, which can be a tool for their documentation and preservation. Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are u"
2001.mtsummit-road.7,P98-2160,0,0.151102,"the internet without requiring indigenous people to give up their languages. MT additionally facilitates the design of educational programs in endangered languages, which can be a tool for their documentation and preservation. Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. Fo"
2001.mtsummit-road.7,sheremetyeva-nirenburg-2000-towards,0,0.0663845,"systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages. Keywords Low-density languages, minor languages, feature detection, version space learning 1. Introduction and Motivation Recently, efforts in machine translation have spread in two directions. Long-term, high-cost development cycles have given way to research on how to build MT systems for new languages quickly and cheaply (Somers, 1997; Nirenburg, 1998; Nirenburg & Raskin 1998; Sherematyeva & Nirenburg, 2000, Jones & Havrilla 1998). Rapid deployment of MT can be useful in situations such as crises in which time and money are short, but more importantly, lowering the cost of MT has, in turn, opened the option of building MT systems for languages that do not have enough speakers to financially support a costly development process. (See Frederking, to appear; Frederking, Rudnicky, Hogan 1997; and the SALTMIL discussion group, http://193.2.100.60/SALTMIL). There is now increasing awareness of the importance of MT for low-density languages as a way of providing access to government, education, healthc"
2001.mtsummit-road.7,1997.tc-1.13,0,0.046319,"gorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages. Keywords Low-density languages, minor languages, feature detection, version space learning 1. Introduction and Motivation Recently, efforts in machine translation have spread in two directions. Long-term, high-cost development cycles have given way to research on how to build MT systems for new languages quickly and cheaply (Somers, 1997; Nirenburg, 1998; Nirenburg & Raskin 1998; Sherematyeva & Nirenburg, 2000, Jones & Havrilla 1998). Rapid deployment of MT can be useful in situations such as crises in which time and money are short, but more importantly, lowering the cost of MT has, in turn, opened the option of building MT systems for languages that do not have enough speakers to financially support a costly development process. (See Frederking, to appear; Frederking, Rudnicky, Hogan 1997; and the SALTMIL discussion group, http://193.2.100.60/SALTMIL). There is now increasing awareness of the importance of MT for low-densit"
2001.mtsummit-road.7,C98-2155,0,\N,Missing
2002.tmi-papers.19,W00-0308,0,0.0224097,"rees to fit L2 sentences Insert missing lexical items and correct linear ordering of branches in the L2 trees Print grammar rules from L2 trees Tasks Done by Grammar Writer Tasks Done by Existing MT system Tasks Done by Native Speaker Tasks Done by Grammar Tools Proposed in this Paper Figure 1: Grammar Adaptation Process information. Finally, grammar induction tools link these two products together to form a forest of semantic parse trees and a grammar in the new language can be read from it. In the context of induction tools at large, our approach lies closer to the interactive techniques of Gavalda (2000) than to unsupervised approaches such as de Marcken’s (1995) and Lee’s (1996). In Section 2 we discuss the steps of this grammar development process in detail. Section 3 describes the tools that are used to induce grammar rules from translation examples and the Machine Translation system that we use in our experiments. Although our research into this topic is continuing, we have used these tools to develop a small grammar for Polish and we describe the experimental results of this feasibility study in Section 4. Section 5 concludes the paper. 2 Approach The grammar development process can be c"
2002.tmi-papers.19,H01-1018,1,0.838352,"Missing"
2002.tmi-papers.19,W95-0102,0,0.0602163,"Missing"
2004.eamt-1.14,J90-2002,0,0.23859,"ransfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpu"
2004.eamt-1.14,J93-2003,0,0.0115785,"encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods requi"
2004.eamt-1.14,1997.tmi-1.13,0,0.0783073,"stem to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel text for the two languages – on the order of magnitude of a million words or more. Such resources are curre"
2004.eamt-1.14,P02-1040,0,0.0790722,"Missing"
2004.eamt-1.14,2001.mtsummit-road.7,1,0.788397,"al of learning compositional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures"
2004.eamt-1.14,2002.tmi-papers.17,1,0.666028,"tional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures the compositional makeup o"
2004.eamt-1.14,2003.mtsummit-papers.53,1,0.816612,"ich four English reference translations are available. The following systems were evaluated in the experiment: 1. Three versions of the Hindi-to-English XFER system: 1a. XFER with No Grammar: the XFER system with no syntactic transfer rules (i.e. only lexical phrase-to-phrase matches and word-toword lexical transfer rules, with and without morphology). 1b. XFER with Learned Grammar: The XFER system with automatically learned syntactic transfer rules. 1c. XFER with Manual Grammar: The XFER system with the manually developed syntactic transfer rules. 2. SMT: The CMU Statistical MT (SMT) system (Vogel et al, 2003), trained on the limited-data parallel text resources. 3. EBMT: The CMU Example-based MT (EBMT) system (Brown, 1997), trained on the limiteddata parallel text resources. 4. MEMT: A “multi-engine” version that combines the lattices produced by the SMT system, and the XFER system with manual grammar. The decoder then selects an output from the joint lattice. Performance of the systems was measured using the NIST scoring metric (Doddington, 2002), as well as the BLEU score (Papineni et al, 2002). In order to validate the statistical significance of the differences in NIST and BLEU scores, we appl"
2004.eamt-1.14,P01-1067,0,0.0309867,"he source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel tex"
2004.eamt-1.14,C90-3044,0,\N,Missing
2004.eamt-1.14,P02-1038,0,\N,Missing
2004.tmi-1.1,carbonell-etal-2002-automatic,1,0.802015,"(Probst et al., 2002), and was previously used for rapid prototyping of an MT system for Hindi-to-English translation (Lavie et al., 2003). For the current Hebrew-to-English system, we manually developed a small set of transfer rules which reflect the most common local syntactic differences between Hebrew and English. This small set of rules turns out to be already sufficient for producing some legible translations of newspaper texts. Performance results are evaluated using state of the art measures and are shown to be encouraging. We also applied an automatic transfer-rule learning approach (Carbonell et al., 2002) to learning a Hebrew-to-English transfer grammar, and report performance results when using the acquired grammar. In the next section we provide some linguistic background about the Hebrew language, with an explicit focus on its challenging sources of ambiguity. Section 3 describes the structure of the MT system with an emphasis on the specific resources required for its application to the Hebrew-to-English language pair and how these resources were acquired and adapted. Section 4 provides some translation examples and describes an evaluation of the system. We conclude with directions for fut"
2004.tmi-1.1,P02-1040,0,0.0900666,", we tested the system on a set of 62 unseen sentences from HaAretz. Three versions of the system were tested on the same data set: a version using our manual transfer grammar; a version using our current automatically-learned grammar; and a version with no transfer grammar at all, which amounts to a word-to-word translation version of the system. Results were evaluated using several automatic metrics for MT evaluation, which compare the translations with human-produced reference translations for the test sentences. For this test set, two reference translations were obtained. We use the BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) automatic metrics for MT evaluation. We also include aggregate unigram-precision and unigram-recall as additional reported measures. The results can be seen in Table 1. To assess statistical significance of the differences in performance between the three versions of the system, we apply a commonly used bootstrapping technique (Efron and Tibshirani, 1986) to estimate the variability over the test set and establish confidence intervals for each reported performance score. As expected, the manual grammar system outperforms the no-grammar system according to all the m"
2004.tmi-1.1,2003.mtsummit-semit.12,1,0.754901,"” script. While a standard convention for this script officially exists, it is not strictly adhered to, even by the major newspapers and in government publications. Thus, the same word can be written in more than one way, sometimes even within the same document. This fact adds significantly to the degree of ambiguity, and requires creative solutions for practical Hebrew language processing applications. The challenge involved in constructing an MT system for Hebrew is amplified by the poverty of existing resources (Wintner, 2004). The collection of corpora for Hebrew is still in early stages (Wintner and Yona, 2003) and all existing significant corpora are monolingual. Hence the use of aligned bilingual corpora for MT purposes is currently not a viable option. There is no available large Hebrew language model which could help in disambiguation. Good morphological analyzers are proprietary and publicly available ones are limited (Wintner, 2004). No publicly available bilingual dictionaries currently exist, and no grammar is available from which transfer rules can be extracted. Still, we made full use of existing resources which we adapted and augmented to fit our needs, as we report in the next section. 1"
2005.eamt-1.13,P93-1035,0,0.119387,"Missing"
2005.eamt-1.13,2003.mtsummit-papers.8,0,0.0325102,"Missing"
2005.eamt-1.13,font-llitjos-carbonell-2004-translation,1,0.638046,"Missing"
2005.eamt-1.13,P03-1057,0,0.0726092,"Missing"
2005.eamt-1.13,C94-1023,0,0.0651462,"Missing"
2005.eamt-1.13,W01-1406,0,0.101271,"Missing"
2005.eamt-1.13,1995.tmi-1.27,0,0.155915,"Missing"
2005.eamt-1.13,1995.tmi-1.23,0,0.157955,"Missing"
2005.eamt-1.13,2001.mtsummit-ebmt.4,0,\N,Missing
2005.eamt-1.20,A94-1016,0,0.8807,"Missing"
2005.eamt-1.20,hogan-frederking-1998-evaluation,0,0.388331,"Missing"
2005.eamt-1.20,2004.eamt-1.14,1,0.872415,"Missing"
2005.eamt-1.20,lavie-etal-2004-significance,1,0.147669,"Missing"
2005.eamt-1.20,C00-2122,0,\N,Missing
2005.eamt-1.20,P02-1040,0,\N,Missing
2007.mtsummit-papers.25,2005.eamt-1.13,1,0.863818,"Missing"
2007.mtsummit-papers.25,font-llitjos-carbonell-2004-translation,1,0.890671,"Missing"
2007.mtsummit-papers.25,P03-1057,0,0.0184025,"repancies between intermediate representations of the source language and the target language side, namely an original MT system (Japanese to English) and a reverse MT system (English to Japanese) which was applied to the post-edited English translation. The grammar rules of our TBMT system integrate information from the three components of a typical transfer system: syntactic analysis (parsing), transfer and generation. Thus, in comparison with the PECOF system, blame assignment becomes highly simplified, since it is more directly inferable from corrections. Menezes and Richardson (2001) and Imamura et al. (2003) have proposed the use of reference translations to “clean” incorrect or redundant rules after automatic acquisition. The method of Imamura and colleagues consists of selecting or removing translation rules to increase the BLEU score of an evaluation corpus. In contrast to filtering out incorrect or redundant rules, we propose to actually refine the translation rules themselves, by editing valid but inaccurate rules that might be lacking a constraint, for example. 3 Error Correction Extraction The first step of the rule refinement process is the extraction of error correction information. Our"
2007.mtsummit-papers.25,2005.mtsummit-papers.11,0,0.00561019,"etween oracle scores, which means that the decoder can not fully leverage the improvements made in the grammar. This is also to be expected, since the decoder fails to select the best translation in most cases. 3 Translation rules in our MT system include parsing, transfer and generation information, which might otherwise be expressed with three different rules in other TBMT systems. 4 According to the standard paired two-tailed t-Test. Even though the language model (LM) for the BTEC data is rather small, using a larger LM with additional out-of-domain data from the Europarl training corpus (Koehn, 2005) did not improve these results. 5.5 Error Analysis After manual inspection, most of the differences between the Baseline and Refined systems were due to three of the 14 CIs processed by the Automatic Rule Refiner, namely 4, 5 and 7 in Figure 10, all of which yielded a BIFURCATE operation. In 56 cases, the additional generation capabilities of the refined system successfully produced a better translation than the baseline system; 37 of these improvements were ranked as 1-best by the decoder. Table 5 shows examples of the three most common types of fixes yielded by automatic refinements. 5.6 MER"
2007.mtsummit-papers.25,lavie-etal-2004-significance,1,0.818158,"Missing"
2007.mtsummit-papers.25,W01-1406,0,0.028629,"MT systems in order to detect discrepancies between intermediate representations of the source language and the target language side, namely an original MT system (Japanese to English) and a reverse MT system (English to Japanese) which was applied to the post-edited English translation. The grammar rules of our TBMT system integrate information from the three components of a typical transfer system: syntactic analysis (parsing), transfer and generation. Thus, in comparison with the PECOF system, blame assignment becomes highly simplified, since it is more directly inferable from corrections. Menezes and Richardson (2001) and Imamura et al. (2003) have proposed the use of reference translations to “clean” incorrect or redundant rules after automatic acquisition. The method of Imamura and colleagues consists of selecting or removing translation rules to increase the BLEU score of an evaluation corpus. In contrast to filtering out incorrect or redundant rules, we propose to actually refine the translation rules themselves, by editing valid but inaccurate rules that might be lacking a constraint, for example. 3 Error Correction Extraction The first step of the rule refinement process is the extraction of error co"
2007.mtsummit-papers.25,C88-2101,0,0.398343,"Missing"
2007.mtsummit-papers.25,P02-1040,0,0.0720235,"Missing"
2007.mtsummit-papers.25,takezawa-etal-2002-toward,0,0.0400664,"Missing"
2007.mtsummit-papers.25,2005.mtsummit-papers.33,0,0.0594552,"Missing"
2007.mtsummit-papers.33,J93-2003,0,0.0148959,"c; parsing Arabic side and extracting corresponding English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system. Arabic 135K 3.5M 145K Sentences Tokens Vocabulary English 135K 4.3M 63K Table 1: Training data statistics We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of the two. For each English NP, we search the aligned corpus for sentences that contain the NP and read off the alignment as its translation. To compensate for alignment errors we also include partial alignments as follows: We find maximum (max) and minimum (min) Arabic word indices that are aligned to the words in the English NP. All the Arabic words between min and max are considered to be the translation of the Engli"
2007.mtsummit-papers.33,A00-2018,0,0.144624,"وﺣﺪة وﻃﻨﻴﺔ ﺑﻌﺜﺔ ﺟﺪﻳﺪة ﻟﻼﻣﻢ اﻟﻤﺘﺤﺪة اﻓﺎد ﻣﺮاﺳﻞ وآﺎﻟﺔ ﻓﺮاﻧﺲ ﺑﺮس اﻟﻤﻨﺘﺠﺎت اﻟﺰراﻋﻴﺔ واﻟﻐﺬاﺋﻴﺔ # # # # # # a military campaign a military cooperation protocol a national unity government a new united nations mission agence france presse correspondent agricultural and food products Figure 1: Sample of NP translation table As our system translates Arabic text into English, it would be logical to start with Arabic; parsing Arabic side and extracting corresponding English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system. Arabic 135K 3.5M 145K Sentences Tokens Vocabulary English 135K 4.3M 63K Table 1: Training data statistics We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of th"
2007.mtsummit-papers.33,P05-1033,0,0.0437426,"ss; long exact matching phrases are relatively rare in the training data. In the decoder, these phrases have to compete with abundant shorter phrases. Due to this reason, Koehn et al. (2003) find that phrases longer than three words give little performance improvement. However, with limited reordering strategies used in most of the statistical machines translation systems, a combination of small short phrases does not always generate the desired translation. Zhang (2005) shows improved translation performance by using phrases of arbitrary length. Hierarchical models, such as the Hiero system (Chiang, 2005), that uses phrases with words as well as subphrases have shown better performance than standard phrase based systems. In this paper, we investigate a simplified two-level machine translations system that uses a linguistically motivated phrase decomposition. We think noun phrases (NPs) are good candidates for a hierarchical system. Semantically noun phrases describe objects and concepts using one or more nouns and adjectives. The vast majority of words in a language are nouns and hence NPs appear frequently in sentences. Noun phrases can often be translated independently into other languages i"
2007.mtsummit-papers.33,N03-1017,0,0.37612,"ization introduced by tagging NPs. 1. Introduction When using statistical machine translation (SMT) systems, we often notice that the phrases used to construct the translations are rather short. On average these phrases are less than two words long. This is in spite of that fact that some phrase extraction methods allow the extraction of arbitrarily long phrases. The main reason for this behavior is data sparseness; long exact matching phrases are relatively rare in the training data. In the decoder, these phrases have to compete with abundant shorter phrases. Due to this reason, Koehn et al. (2003) find that phrases longer than three words give little performance improvement. However, with limited reordering strategies used in most of the statistical machines translation systems, a combination of small short phrases does not always generate the desired translation. Zhang (2005) shows improved translation performance by using phrases of arbitrary length. Hierarchical models, such as the Hiero system (Chiang, 2005), that uses phrases with words as well as subphrases have shown better performance than standard phrase based systems. In this paper, we investigate a simplified two-level machi"
2007.mtsummit-papers.33,E03-1035,0,0.0121748,"nigram bigram trigrams 4-grams Original 630K 841K 2293K 3242K NP-tagged 628K 770K 2015K 2894K 1.3 words. For English sentences, the drop is about 2 words. Corpus Original NP-tagged We also compared the average length of the corpus before and after NP-tagging. These numbers are given in Table 6. Avg. length of an Arabic sentence has dropped by about NP-tagged 35.89 33.40 Table 6: Avg. length of training corpus before and after NP-tagging 2.3 Extract Phrases from NP-tagged Corpus We use the NP-tagged parallel corpus to extract phrase translation pairs. Our phrase extraction method is similar to Moore (2003) which is a variation of the IBM-1 word alignment model (Brown et al., 1993). Assuming a source sentence s1 = s1 K s I in the i bilingual corpus contains a phrase s i12 = s i K si2 we are j interested in the sequence of words t j12 =1 t j1 ...t j2 from J the respective target sentence t1 = t1 ...t J that is the optimal translation for this source phrase. We can estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source and target phrases. If the candidate is actually a good translation of the source phrase we expect higher IBM-1 probabili"
2007.mtsummit-papers.33,J03-1002,0,0.00323774,"g English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system. Arabic 135K 3.5M 145K Sentences Tokens Vocabulary English 135K 4.3M 63K Table 1: Training data statistics We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of the two. For each English NP, we search the aligned corpus for sentences that contain the NP and read off the alignment as its translation. To compensate for alignment errors we also include partial alignments as follows: We find maximum (max) and minimum (min) Arabic word indices that are aligned to the words in the English NP. All the Arabic words between min and max are considered to be the translation of the English NP. We filter out unbalance NP translation pai"
2007.mtsummit-papers.33,P03-1021,0,0.0302532,"er (Vogel et al., 2003). For our experiments, the decoder uses two translation resources in two levels to generate a hierarchy of phrases. NP translation table is used in the first level to identify possible NPs in the test sentence. NP-tagged phrase table is then used in the next level to build a translation lattice. The decoding process is organized into two steps: 1. 2. Build a translation lattice using all available word/phrase translation resources Find the best combination of partial translations by searching through the lattice In addition, it also performs minimum error-rate training (Och, 2003) to find the best scaling factors for each model used in the decoder. 2.4.1 Building the Translation Lattice The first step in decoding is building the translation lattice. We illustrate this process by using the following Arabic sentence. Note that the Arabic sentence is written from right-to-left. Arabic sentence: اﺑﺮاهﻴﻢ ﻳﺴﺘﻘﺒﻞ ﺿﺎﺑﻂ ﻓﻲ ﺑﻐﺪاد Reference translation: Ibrahim receives Baghdad officer in First the decoder converts the Arabic sentence into a lattice structure where words are attached to the edges (see figure 4a). Next, for each word sequence starting from the left-most node, it"
2007.mtsummit-papers.33,P02-1040,0,0.0861526,"Missing"
2007.mtsummit-papers.33,2005.eamt-1.39,1,0.884469,"Missing"
2008.amta-srw.1,W01-1819,0,0.0546157,"Missing"
2008.amta-srw.1,W05-0909,1,0.244127,"Missing"
2008.amta-srw.1,P05-1033,0,0.175217,"ies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA Abstract Phrase Based Statistical Machine Translation (PBSMT)(Koehn et al., 2003) being the most actively progressing area. While PB-SMT improves traditional word based machine translation approaches by incorporating more contextual information in the form of phrase pairs, it still has limitations in global block level reordering of phrasal units. Such reorderings can be captured by knowledge about the structure of the language. Recent research in syntax based machine translation (Yamada and Knight, 2001) (Marcu et al., 2006) (Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem of phrasal units. Some of the approaches operate within the resources of PB-SMT and induce hierarchical grammars from existing non-syntactic phrasal units, to provide better generality and structure for reordering (Chiang, 2005) (Wu, 1997). Other approaches use syntactic analysis of sentences on one side of the corpus to induce grammar rules (Galley et al., 2004) (Yamada and Knight, 2001) (Venugopal et al., 2007). Syntax-based approaches to statistical MT require syntax-aware methods for acquiring their underlying transla"
2008.amta-srw.1,W08-0307,0,0.0239233,"Missing"
2008.amta-srw.1,D07-1079,0,0.0518676,"on one side. 1 Most approaches that incorporate linguistic syntax start with word level alignments and a parse tree for one side of the language pair, and obtain phrase tables and hierarchical translation rules driven by the syntax. We call this the ‘TnS’ setting, where we have the tree on one side and only string on the other. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments which are usually extracted using syntactically uninformed generative models are not optimal for the syntactic phrase extraction problem (DeNeefe et al., 2007; DeNero and Klein, 2007). Some approaches (Crego and Habash, Introduction In recent years, corpus based approaches to machine translation have become predominant, with 235 [8th AMTA conference, Hawaii, 21-25 October 2008] 2008; Fossum et al., 2008) have been proposed to modify the word alignments in ways that make them more amenable to building syntactic models. Recently, other approaches have been proposed for using syntactic parse trees for both the languages, to extract highly precise and compositional phrase pairs and rules. We call this scenario the ‘TnT’ scenario. (Tinsley et al., 2007b"
2008.amta-srw.1,2003.mtsummit-papers.22,0,0.0603843,"be possible with limited high precision transfer grammars, which can be constructed manually or learnt from data. any modifications to the TnS approach that aims at increasing lexical coverage should carry over to improving coverage in our scenario as well. Approaches that incorporate trees on both sides have reported that the low recall of the translation models extracted is the primary reason for their inferior translation quality (Tinsley et al., 2007a). The extracted phrases are more precise as they are supported by not only the word alignments but also the parse tree on the target side. (Hearne and Way, 2003) describe an approach that uses syntactic information for both languages to derive reordering subtrees, which can then be used within a “dataoriented translation” (DOT) MT system, similar in framework to (Poutsma, 2000). (Lavie et al., 2008) also discuss a pipeline for extraction of such translation models in the form of phrases and grammar rules. The systems constructed using this pipeline were significantly weaker than current state-of-theart. Overall it can be observed from the results in the literature that approaches using syntax on both sides have not been able to surpass the approaches"
2008.amta-srw.1,N06-1031,0,0.321209,"hich can then be used for translation model extraction. We conclude with our experiments and future work. 2 Related Work Most of the previous approaches for acquiring syntactic translation models from parallel corpora use syntactic information from only one side of the parallel corpus, typically the source side. This already hurts the lexical coverage for translation (DeNeefe et al., 2007). PB-SMT techniques to extracting phrases although not syntactically motivated, enjoy very high coverage. In order to bridge the gap some successful approaches to syntax in MT resort to re-labeling of trees (Huang and Knight, 2006) and binarization techniques (Wang et al., 2007). Such techniques systematically alter the structure of the source side parse tree to increase the space of segmentation allowed by the tree. This improves the recall of the syntactic translation models in particular the flat rules corresponding to syntactic phrasal entries. In our work we do not modify the source tree at all, but we use the information from the target tree to improve the precision of the phrasal translations. Therefore our lexical coverage is exactly the same as that provided by any TnS approach. Additionally, 1 During translati"
2008.amta-srw.1,N03-1017,0,0.0293904,"igned with ”des principes” even though the word alignment does not provide any link between ‘with’ and ‘des’. In Figure 2 we show the TnS process of extraction and the phrases that are licensed by the word alignment and the source side syntax tree are shown in Table 2. We notice the problem with this approach, which does not take into consideration the target side syntactic boundaries of the phrases. The phrase ‘with the principles’ is only mapped to ‘principes’ which is clearly incorrect. One might argue that the heruristics applied for phrase extraction in standard phrase based SMT systems (Koehn et al., 2003), obtain all possible translation phrasal entries as allowed by the word alignment and that their maximum likelihood scores should reflect their quality. However the resulting translation models are often huge and introduce a great deal of ambiguity into the search process, leav6.1 Introduce Operation This operation is similar to the projection scenario as discussed in Section 4.1. We first traverse the source side parse tree S, starting from top to bottom. At each node we find a valid projection for the yield of the node in the target sentence as licensed by the word alignment. We use the lab"
2008.amta-srw.1,P07-2045,0,0.00986215,"2006) over 430 million words including the English side of the parallel corpus. Since we are interested in studying the affect of the lexical coverage licensed by these different extraction scenarios, we run our decoder in a monotonic mode without any hierarchical models. The weights on the features are tuned using standard MERT (Och, 2003) techniques over a 600-sentence dev set. The test set used was released by the WMT shared task 2007 and consists of 2000 sentences. When run without hierarchical syntax, our decoder is very similar to the decoder that is distributed with the Moses toolkit (Koehn et al., 2007). The results are shown in Table 5. The problem of low recall that the TnT extracted translation models have can be seen in the inferior translation scores. The TnS scenario has a benefit from its high recall and a huge jump is seen in the scores. Our non-isomorphic tree restructuring technique attempts to obtain the best • TnS setting produces much larger syntactic translation models when compared to the TnT setting. The source sides of the phrases that are extracted in the TnT approach are a complete subset of those that are projected in the TnS approach. • The target translations for all th"
2008.amta-srw.1,W08-0411,1,0.919585,"eNero and Klein, 2007). Some approaches (Crego and Habash, Introduction In recent years, corpus based approaches to machine translation have become predominant, with 235 [8th AMTA conference, Hawaii, 21-25 October 2008] 2008; Fossum et al., 2008) have been proposed to modify the word alignments in ways that make them more amenable to building syntactic models. Recently, other approaches have been proposed for using syntactic parse trees for both the languages, to extract highly precise and compositional phrase pairs and rules. We call this scenario the ‘TnT’ scenario. (Tinsley et al., 2007b),(Lavie et al., 2008) have used node alignment techniques to align trees on both sides and extract translation models, which can then be combined with hierarchical rules inside a syntactic machine translation system. In this paper, we study the issue of lexical coverage of both the TnS and the TnT scenarios. The TnS approach generally licenses more syntactic phrases when compared to TnT, as its space of segmentation over the parallel sentences is constrained by the word alignments and the source-side parse tree only. However, the phrases, although syntactic on the source-side, do not necessarily map to syntactic p"
2008.amta-srw.1,W06-1606,0,0.11327,"du Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA Abstract Phrase Based Statistical Machine Translation (PBSMT)(Koehn et al., 2003) being the most actively progressing area. While PB-SMT improves traditional word based machine translation approaches by incorporating more contextual information in the form of phrase pairs, it still has limitations in global block level reordering of phrasal units. Such reorderings can be captured by knowledge about the structure of the language. Recent research in syntax based machine translation (Yamada and Knight, 2001) (Marcu et al., 2006) (Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem of phrasal units. Some of the approaches operate within the resources of PB-SMT and induce hierarchical grammars from existing non-syntactic phrasal units, to provide better generality and structure for reordering (Chiang, 2005) (Wu, 1997). Other approaches use syntactic analysis of sentences on one side of the corpus to induce grammar rules (Galley et al., 2004) (Yamada and Knight, 2001) (Venugopal et al., 2007). Syntax-based approaches to statistical MT require syntax-aware methods for acquiring their und"
2008.amta-srw.1,J03-1002,0,0.0194553,"Missing"
2008.amta-srw.1,P03-1021,0,0.0236246,", only a very minor fraction of them were extracted when using trees on both sides . 7.2 Results We perform translation experiments using the experimental setup defined above and our Stat-XFER framework. We build a suffix array language model (SALM) (Zhang and Vogel, 2006) over 430 million words including the English side of the parallel corpus. Since we are interested in studying the affect of the lexical coverage licensed by these different extraction scenarios, we run our decoder in a monotonic mode without any hierarchical models. The weights on the features are tuned using standard MERT (Och, 2003) techniques over a 600-sentence dev set. The test set used was released by the WMT shared task 2007 and consists of 2000 sentences. When run without hierarchical syntax, our decoder is very similar to the decoder that is distributed with the Moses toolkit (Koehn et al., 2007). The results are shown in Table 5. The problem of low recall that the TnT extracted translation models have can be seen in the inferior translation scores. The TnS scenario has a benefit from its high recall and a huge jump is seen in the scores. Our non-isomorphic tree restructuring technique attempts to obtain the best"
2008.amta-srw.1,2005.mtsummit-papers.19,0,0.0475024,"Missing"
2008.amta-srw.1,C00-2092,0,0.0757386,"verage in our scenario as well. Approaches that incorporate trees on both sides have reported that the low recall of the translation models extracted is the primary reason for their inferior translation quality (Tinsley et al., 2007a). The extracted phrases are more precise as they are supported by not only the word alignments but also the parse tree on the target side. (Hearne and Way, 2003) describe an approach that uses syntactic information for both languages to derive reordering subtrees, which can then be used within a “dataoriented translation” (DOT) MT system, similar in framework to (Poutsma, 2000). (Lavie et al., 2008) also discuss a pipeline for extraction of such translation models in the form of phrases and grammar rules. The systems constructed using this pipeline were significantly weaker than current state-of-theart. Overall it can be observed from the results in the literature that approaches using syntax on both sides have not been able to surpass the approaches that use syntax on one side only. In our current work we do a careful study of the lexical coverage of the TnS and TnT scenarios. We then propose a novel technique to restructure the non-isomorphic target-side parse tre"
2008.amta-srw.1,2007.mtsummit-papers.62,0,0.115742,"(DeNeefe et al., 2007; DeNero and Klein, 2007). Some approaches (Crego and Habash, Introduction In recent years, corpus based approaches to machine translation have become predominant, with 235 [8th AMTA conference, Hawaii, 21-25 October 2008] 2008; Fossum et al., 2008) have been proposed to modify the word alignments in ways that make them more amenable to building syntactic models. Recently, other approaches have been proposed for using syntactic parse trees for both the languages, to extract highly precise and compositional phrase pairs and rules. We call this scenario the ‘TnT’ scenario. (Tinsley et al., 2007b),(Lavie et al., 2008) have used node alignment techniques to align trees on both sides and extract translation models, which can then be combined with hierarchical rules inside a syntactic machine translation system. In this paper, we study the issue of lexical coverage of both the TnS and the TnT scenarios. The TnS approach generally licenses more syntactic phrases when compared to TnT, as its space of segmentation over the parallel sentences is constrained by the word alignments and the source-side parse tree only. However, the phrases, although syntactic on the source-side, do not necessa"
2008.amta-srw.1,N07-1063,0,0.0146348,"ure of the language. Recent research in syntax based machine translation (Yamada and Knight, 2001) (Marcu et al., 2006) (Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem of phrasal units. Some of the approaches operate within the resources of PB-SMT and induce hierarchical grammars from existing non-syntactic phrasal units, to provide better generality and structure for reordering (Chiang, 2005) (Wu, 1997). Other approaches use syntactic analysis of sentences on one side of the corpus to induce grammar rules (Galley et al., 2004) (Yamada and Knight, 2001) (Venugopal et al., 2007). Syntax-based approaches to statistical MT require syntax-aware methods for acquiring their underlying translation models from parallel data. This acquisition process can be driven by syntactic trees for either the source or target language, or by trees on both sides. Work to date has demonstrated that using trees for both sides suffers from severe coverage problems. This is primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce, which adversely affects the recall of the resulting translation models. Approaches that project from trees"
2008.amta-srw.1,D07-1078,0,0.0328714,"n. We conclude with our experiments and future work. 2 Related Work Most of the previous approaches for acquiring syntactic translation models from parallel corpora use syntactic information from only one side of the parallel corpus, typically the source side. This already hurts the lexical coverage for translation (DeNeefe et al., 2007). PB-SMT techniques to extracting phrases although not syntactically motivated, enjoy very high coverage. In order to bridge the gap some successful approaches to syntax in MT resort to re-labeling of trees (Huang and Knight, 2006) and binarization techniques (Wang et al., 2007). Such techniques systematically alter the structure of the source side parse tree to increase the space of segmentation allowed by the tree. This improves the recall of the syntactic translation models in particular the flat rules corresponding to syntactic phrasal entries. In our work we do not modify the source tree at all, but we use the information from the target tree to improve the precision of the phrasal translations. Therefore our lexical coverage is exactly the same as that provided by any TnS approach. Additionally, 1 During translation source is French and target is English, but w"
2008.amta-srw.1,J97-3002,0,0.679678,"he form of phrase pairs, it still has limitations in global block level reordering of phrasal units. Such reorderings can be captured by knowledge about the structure of the language. Recent research in syntax based machine translation (Yamada and Knight, 2001) (Marcu et al., 2006) (Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem of phrasal units. Some of the approaches operate within the resources of PB-SMT and induce hierarchical grammars from existing non-syntactic phrasal units, to provide better generality and structure for reordering (Chiang, 2005) (Wu, 1997). Other approaches use syntactic analysis of sentences on one side of the corpus to induce grammar rules (Galley et al., 2004) (Yamada and Knight, 2001) (Venugopal et al., 2007). Syntax-based approaches to statistical MT require syntax-aware methods for acquiring their underlying translation models from parallel data. This acquisition process can be driven by syntactic trees for either the source or target language, or by trees on both sides. Work to date has demonstrated that using trees for both sides suffers from severe coverage problems. This is primarily due to the highly restrictive spac"
2008.amta-srw.1,P01-1067,0,0.520264,"shi Ambati vamshi@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA Abstract Phrase Based Statistical Machine Translation (PBSMT)(Koehn et al., 2003) being the most actively progressing area. While PB-SMT improves traditional word based machine translation approaches by incorporating more contextual information in the form of phrase pairs, it still has limitations in global block level reordering of phrasal units. Such reorderings can be captured by knowledge about the structure of the language. Recent research in syntax based machine translation (Yamada and Knight, 2001) (Marcu et al., 2006) (Chiang, 2005) incorporates syntactic information to ameliorate the reordering problem of phrasal units. Some of the approaches operate within the resources of PB-SMT and induce hierarchical grammars from existing non-syntactic phrasal units, to provide better generality and structure for reordering (Chiang, 2005) (Wu, 1997). Other approaches use syntactic analysis of sentences on one side of the corpus to induce grammar rules (Galley et al., 2004) (Yamada and Knight, 2001) (Venugopal et al., 2007). Syntax-based approaches to statistical MT require syntax-aware methods fo"
2009.mtsummit-posters.2,W05-0909,1,0.175736,"nce we are interested in studying the affect of the lexical coverage licensed by these different extraction scenarios, in all our experiments we run our decoder in a monotonic mode without any hierarchical models. We performed translation experiments using the experimental setup defined above and use StatResults The results are shown in Table 3. The overall problem of low coverage from using trees on both sides can be seen in the final translation quality too. Using syntax on one side produces syntax tables of larger coverage which also reflects in MT quality as judged both by BLEU and METEOR(Banerjee and Lavie, 2005) metrics. Our non-isomorphic tree restructuring technique that benefits from syntactic boundaries from both trees shows significant 2 improvement over the two other approaches. Symmetric rule induction that is a union of rules extracted by restructuring both trees has similar benefits from restructuring and also produces the largest possible syntactic phrase table. Although these results are slightly worse when compared to standard PB-SMT baseline (30.18 BLEU), it is to be noted that we are working with only syntactic phrase tables which are clean resources and are still relatively smaller in"
2009.mtsummit-posters.2,P05-1033,0,0.117643,"in (Lavie et al., 2008). In this paper, we are interested in the constraints introduced by the parse trees and study them in detail. Syntax trees from both sides introduce constraints on the possible segmentations. One variation to the above tree-tree model is to extract rules using target tree only. This can be seen as an instance of the framework, where CS , LS , PS = N U LL . A dependency tree can be seen as constituting a hidden unlabeled constituent tree. In such a case, the rule extraction can be used in a configuration where LT , LS = ‘X 0 , simulating a ’hiero’ style rule extraction (Chiang, 2005). As seen previously, variations with the choice of label sets LS , LT are also possible. For a language where only base-np chunkers and part-of-speech taggers are available, the framework can be instantiated as CT = {(i, j)} as the set of span boundaries from the chunker and LT = Lpos ∪ {0 X 0 } where Lpos come from the part-of-speech tagger and can be assigned to the preterminals, and ’X’ label to the non-terminals. We experiment with two configurations, using tree on one side vs. both sides. 3 Restructuring Parse Trees The parsers that generate syntactic analysis are built under varying ass"
2009.mtsummit-posters.2,P07-2045,0,0.0107491,"Missing"
2009.mtsummit-posters.2,N04-1035,0,0.128285,"ctic information that may come from either sides of the language pair. Our rule learning framework is general and works with syntax on both sides or any other annotation that is available, such as dependency structures, part-ofspeech tags etc. We achieve this by factorizing syntax into three essential components - structure, constituency and labeling. 2.1 Problem and Inputs A parallel corpus D is defined as a set of sentence pairs (FI1 , EJ1 , A), where A ⊆ {(i, j) : i = 1..I 0 , j = 1..J 0 } is the word alignment relation over each pair. We subscribe to the theory of alignments discussed in (Galley et al., 2004), that explains the concept of ’consistency of word-alignment’ for minimal rule extraction from a tree. Consistent alignment requires all the words in a particular segment of the source side to align with a particular contiguous segment of the target sentence, as decided by the word-level alignment. Formally, a span (is , js ) projects to a target span(it , jt ) if and only if ∀ks ∈ (is , js ), A(ks ) ∈ (it , jt ). We introduce syntax into this induction process by defining every node in the source tree as a tuple of three entities T ree(FI1 ) = {nsi : hcsi , lsi , psi i}, where csi ∈ CS is a"
2009.mtsummit-posters.2,W09-2301,1,0.907789,"B-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approaches to syntax in MT resort to binarization of trees(Wang et al., 2007) that systematically alter the structure of the source side parse tree to increase the space of segmentation allowed. This improves the recall of the syntactic translation models in particular the flat rules corresponding to syntactic phrasal entries. Another promising approach for bridging the coverage gap is combination of non-syntactic phrases with syntactic phrases (Hanneman and Lavie, 2009). Such techniques have shown that starting with large syntactic phrase tables and preferring syntactic phrases when overlapping with non-syntactic ones is beneficial for a syntactic MT system. They show improvements in decoding speeds and also improvement in translation quality that results from the precision of these syntax motivated phrases. The syntactic tables we produce in our work are precise and much high in coverage and can directly support these approaches. (Hanneman and Lavie, 2009) also show that a small set of manual synchronous grammar rules already benefit from clean syntactic ta"
2009.mtsummit-posters.2,N06-1031,0,0.0679123,"rmat below, where cs ∈ LS and ct ∈ LT represent syntactic categories and ws and wt are the word or phrase strings for the source- and targetsides correspondingly. The phrasal entries are collected from the entire corpus and scored by conditioning on the source side together with the label for assigning probabilities and form a syntactic phrase table. cs :: ct → [ws ] :: [wt ] Our alignment algorithm does not depend on the label sets LS or LT . Given that the original syntactic labels associated with the two parse trees are designed independent from each other, they may be sub-optimal for MT. (Huang and Knight, 2006) achieved improved translation quality by relabeling trees from which translation models were extracted. Decoupling the choice of labels from the alignment algorithm and delaying the assignment of labels until the output phase, enables our framework to experiment with various labeling strategies. In all our experiments, we retain the labels from both the sets. Exploring other possibilities for labeling, although interesting, is beyond the scope of this paper. 2.2.2 Hierarchical Rule Extraction: Synchronous Grammar Given two synchronous trees and their node alignment AN , we developed a tree tr"
2009.mtsummit-posters.2,W08-0411,1,0.946656,"ill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce. Phrase based statistical MT (PB-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approaches to syntax in MT"
2009.mtsummit-posters.2,W06-1606,0,0.015637,"ned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces significantly better translation models. The improved precision and coverage of our syntax tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations t"
2009.mtsummit-posters.2,J03-1002,0,0.00719302,"Experimental Setup We build a French to English translation system using our decoding framework. We do not exploit the hierarchical nature of the decoder, as the translation models with which we would like to experiment are flat syntactic phrases. The parallel data we used to build our translation models is the Europarl data consisting of 1.3M translation sentence pairs. The English side of the corpus is parsed using the Stanford parser(Klein and Manning, 2002). The French side of the corpus was parsed by the Berkeley Parser. Word alignments for the parallel corpus were obtained using GIZA++ (Och and Ney, 2003) followed by a symmertrization technique called ‘sym2’ from the Thot toolkit (Ortiz-Mart´ınez et al., 2005). We used this technique as it was shown to provide good node alignment results across trees in both (Lavie et al., 2008) and (Tinsley et al., 2007). We then perform the extraction of the phrase pairs under all modes of rule learning - tree on one side, trees on both sides, restructuring tree and symmetric rule induction. Since we are interested in studying the affect of the lexical coverage licensed by these different extraction scenarios, in all our experiments we run our decoder in a m"
2009.mtsummit-posters.2,P03-1021,0,0.00656821,"ne side we lose grammar rules related to composing the verb ’am’ with the pronoun ’I’, but symmetrization fills the gap. The new generalized rule V N :: V N [P RP 1 AU X 2 ] → [CL1 V2 ] can now combine other verbs like ‘see’,‘think’ to form constituents ‘I see’, ‘I think’ etc, which were earlier not possible. 1 http://code.google.com/berkeleyparser XFER (Lavie, 2008) as the decoding framework. We built a suffix array language model (SALM) (Zhang and Vogel, 2006) over 430 million words including the English side of the parallel corpus. The weights on the features are tuned using standard MERT (Och, 2003) techniques over a 600-sentence dev set. The test set used was released by the WMT shared task 2007 and consists of 2000 sentences. When run without hierarchical syntax, this decoder is very similar to Moses decoder (et al, 2007). 5.2 Figure 3: Symmetric Rule Induction: Joint restructuring 5 5.1 Evaluation Experimental Setup We build a French to English translation system using our decoding framework. We do not exploit the hierarchical nature of the decoder, as the translation models with which we would like to experiment are flat syntactic phrases. The parallel data we used to build our trans"
2009.mtsummit-posters.2,2005.mtsummit-papers.19,0,0.0941244,"Missing"
2009.mtsummit-posters.2,2007.mtsummit-papers.62,0,0.0346035,"x tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce. Phrase based statistical MT (PB-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approa"
2009.mtsummit-posters.2,D07-1078,0,0.0183577,"ntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce. Phrase based statistical MT (PB-SMT) techniques for extracting phrases although not syntactically motivated, enjoy a very high coverage. In order to bridge the gap some successful approaches to syntax in MT resort to binarization of trees(Wang et al., 2007) that systematically alter the structure of the source side parse tree to increase the space of segmentation allowed. This improves the recall of the syntactic translation models in particular the flat rules corresponding to syntactic phrasal entries. Another promising approach for bridging the coverage gap is combination of non-syntactic phrases with syntactic phrases (Hanneman and Lavie, 2009). Such techniques have shown that starting with large syntactic phrase tables and preferring syntactic phrases when overlapping with non-syntactic ones is beneficial for a syntactic MT system. They show"
2009.mtsummit-posters.2,P01-1067,0,0.0602187,"onstituents that were aligned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces significantly better translation models. The improved precision and coverage of our syntax tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches. 1 Introduction Recent approaches to Syntax based Machine Translation (MT) incorporate linguistic syntax for one side of the language pair, and obtain phrase tables and hierarchical translation rules. While this has indeed proven successful (Yamada and Knight, 2001) (Marcu et al., 2006), it has been shown that the word alignments, which are usually extracted using syntactically uninformed generative models, are not optimal for the syntactic phrase extraction problem. Other approaches (Tinsley et al., 2007),(Lavie et al., 2008) have been proposed for using syntactic parse trees for both the languages, employing node alignment techniques to align them and extract hierarchical translation models for syntactic machine translation systems. Using trees for both sides suffers from severe coverage problems, primarily due to the highly restrictive space of consti"
2010.amta-papers.17,P07-1038,1,0.878948,"and their relative rankings. The ranking is determined according to their actual translation quality (measured by BLEU-3 against a human reference). Duplicate translations are removed so that each ranked list specifies a total order. The learning task is to predict the gold standard ranking based on the features that can be extracted from the translation candidates. In other words, the ranker’s objective can be seen as performing a kind of automatic MT evaluation metric that does not require references on the translation candidates. Based on related work in MT evaluation (Specia et al., 2009; Albrecht and Hwa, 2007), we use a similar set of features such as: • N-gram matching with the underlying LM corpus • N-gram probabilities from the LM • Target to source-language lexical ambiguity • Average word movement from source to targetlanguage • The ratio of punctuation and digit in the source and target phrases • The average BLEU score: A BLEU evaluation of the hypothesis, using the other competing hypothesis as the pseudo references model’s top choice resulted in translation improvements over the baseline. LM weight used Baseline adapt. by group adapt. of indiv. wt. • Bigram and trigram POS tags: In order to"
2010.amta-papers.17,W08-0331,0,0.0129242,"expected performance. To perform this kind of judgment, we make use of techniques similar to confidence estimation and automatic MT evaluation. Previous work on MT evaluation without references focuses on sentential or corpus level evaluation and uses regression to predict a translation quality score (Albrecht and Hwa, 2007; Specia et al., 2009). In contrast, we apply the evaluation metric to segments at a sub-sentential level. We are less interested in the absolute scores than in the quality of the candidates relative to each other. In this way, our approach is more similar to the metric of (Duh, 2008), in which the evaluation is conducted by ranking. Also relevant is the series of work on system modification, such as post-decoding discriminative re-ranking. The discriminative re-ranking benefits from a set of richer but more computationally expensive features. These features range from deeper linguistic knowledge (Och et al., 2004) to system related knowledge like word and phrase level confidence score (Zens and Ney, 2006). Similarly we benefit from additional features in our weight ranking; however, the set of candidate hypotheses generated for weight ranking is different from a decoder’s"
2010.amta-papers.17,koen-2004-pharaoh,0,0.0473095,"eling errors, but the LM features are influential enough for finding segments with LM-related problems. In most experiments in this work (except section 6), the most difficult segment is gold-standard. That means that for each sentence, the reference translations are used to find the segment with the lowest translation quality. 2.2 Implementation We modify the standard PB-SMT pipeline to facilitate LM weight adaptation. Our basic PB-SMT system uses the SRI package (Stolcke, 2002) as the target language model and Phramer (Olteanu et al., 2006), an open source implementation similar to Pharaoh (Koehn, 2004), as the decoder. We modify the decoder to allow the usage of different LM weights for different parts of a sentence; it accepts two decoding weights: One is used for the difficult segment, and the other one is used for the translation of the rest of the sentence. In order to apply this separation of the decoding weights, we constrain the choice of phrases in hypothesis expansion at the boundaries of the difficult segment. We allow the decoder to shift the difficult segment’s boundaries with one word to use more of the phrase table entries. We assume that every sentence holds at least one segm"
2010.amta-papers.17,W07-0737,1,0.912592,"We argue that the right scope for a language model weight to excise its influence is at the subsentential level. Even a long and complex sentence may contain parts that are relatively straightforward. Our approach is to first identify the portion of a source sentence whose translation may be problematic for the target language model (we refer to this special portion as a segment); then use an adapted LM weight for the translation of the special segment but use the default LM weight for the rest of the sentence. We previously defined such special segment as Difficult to Translate Phrase (DTP) (Mohit and Hwa, 2007). We also described an SVM classifier that can predict whether a segment of source text will be problematic for the MT system with good accuracy. In an extended work, we adapted the language model of a SMT system for the translation of the DTPs (Mohit et al., 2009). For each DTP we automatically selected the relevant subset of the training data and constructed an adapted language model. Moreover, we translated the difficult phrase with the adapted model. In this paper, we continue our segment-specific system customization framework for testing our idea of modifying the language model weights."
2010.amta-papers.17,2009.eamt-1.22,1,0.786776,"e whose translation may be problematic for the target language model (we refer to this special portion as a segment); then use an adapted LM weight for the translation of the special segment but use the default LM weight for the rest of the sentence. We previously defined such special segment as Difficult to Translate Phrase (DTP) (Mohit and Hwa, 2007). We also described an SVM classifier that can predict whether a segment of source text will be problematic for the MT system with good accuracy. In an extended work, we adapted the language model of a SMT system for the translation of the DTPs (Mohit et al., 2009). For each DTP we automatically selected the relevant subset of the training data and constructed an adapted language model. Moreover, we translated the difficult phrase with the adapted model. In this paper, we continue our segment-specific system customization framework for testing our idea of modifying the language model weights. To find a better LM weight for the special segments, we consider two options. One is to treat all the special segments as a group and learn an appropriate weight for the group; another is to predict a weight value for each segment. The first option can be performed"
2010.amta-papers.17,P03-1021,0,0.00966444,"ic properties of the source-language text, etc. The influence of each feature is decided by its associated weight value; they are combined to determine the decoding score. For example, in the Table 1 which presents a standard Phrase-Based SMT (PB-SMT) formulation, the (λ)s are the decoding weights for the Language Model (LM), Translation Model (TM), Word Penalty (WP) and Distortion (d) features. The translation model feature function(φ) holds four model parameters, and each parameter gets an entry in the λφ weight vector. The weights are estimated using the Minimum Error Rate Training (MERT) (Och, 2003). MERT tunes the SMT system based on an iterative translation task performed on a development set. In each iteration, the MERT estimates a new set of decoding weights. It then check the effects of the new weights on the translation quality of the development data, using automatic evaluation metrics such as BLEU (Papineni et al., 2002). This evaluation is usually performed at the corpus level. After the training converges, the decoder computes scores for all translation hypotheses using the tuned weights, regardless of their characteristics. The goal of our work is to improve translation by usi"
2010.amta-papers.17,W06-3121,0,0.0139166,"uses of translation difficulties includes factors other than language modeling errors, but the LM features are influential enough for finding segments with LM-related problems. In most experiments in this work (except section 6), the most difficult segment is gold-standard. That means that for each sentence, the reference translations are used to find the segment with the lowest translation quality. 2.2 Implementation We modify the standard PB-SMT pipeline to facilitate LM weight adaptation. Our basic PB-SMT system uses the SRI package (Stolcke, 2002) as the target language model and Phramer (Olteanu et al., 2006), an open source implementation similar to Pharaoh (Koehn, 2004), as the decoder. We modify the decoder to allow the usage of different LM weights for different parts of a sentence; it accepts two decoding weights: One is used for the difficult segment, and the other one is used for the translation of the rest of the sentence. In order to apply this separation of the decoding weights, we constrain the choice of phrases in hypothesis expansion at the boundaries of the difficult segment. We allow the decoder to shift the difficult segment’s boundaries with one word to use more of the phrase tabl"
2010.amta-papers.17,P02-1040,0,0.0780057,"M), Translation Model (TM), Word Penalty (WP) and Distortion (d) features. The translation model feature function(φ) holds four model parameters, and each parameter gets an entry in the λφ weight vector. The weights are estimated using the Minimum Error Rate Training (MERT) (Och, 2003). MERT tunes the SMT system based on an iterative translation task performed on a development set. In each iteration, the MERT estimates a new set of decoding weights. It then check the effects of the new weights on the translation quality of the development data, using automatic evaluation metrics such as BLEU (Papineni et al., 2002). This evaluation is usually performed at the corpus level. After the training converges, the decoder computes scores for all translation hypotheses using the tuned weights, regardless of their characteristics. The goal of our work is to improve translation by using different decoder weights for different parts of a source sentence. Specifically, we propose to adapt the language model weight for decoding the problematic segment of a source sentence. This is a sequence of five to fifteen words whose characteristics are significantly different from the average case so as to cause problems for th"
2010.amta-papers.17,D08-1090,0,0.025947,"ned on a larger parallel corpus, there is still a 0.85 BLEU score improvement. This suggests that individual weight adaptation may still be helpful for larger MT systems. 7 Related Work The major concepts used in our work are adaptation, re-scoring, automatic MT evaluation, and ranking; they have been widely studied in the MT literature. In this section, we highlight some of the most relevant previous work. In previous work on MT adaptation, many proposed to modify the baseline system to incorporate features from the source language, lexical translations and from the underlying system itself (Snover et al., 2008; Tam et al., 2007; Kim, 2004). In these studies, the components of the baseline SMT system are modified. Furthermore, new models are constructed for the translation of special test sets or individual phrases and sentences. In contrast, our approach does not directly change the baseline components. Instead, we allow the decoder to adjust the influence from a component according to its expected performance. To perform this kind of judgment, we make use of techniques similar to confidence estimation and automatic MT evaluation. Previous work on MT evaluation without references focuses on sentent"
2010.amta-papers.17,2009.eamt-1.5,0,0.101502,"anslation candidates and their relative rankings. The ranking is determined according to their actual translation quality (measured by BLEU-3 against a human reference). Duplicate translations are removed so that each ranked list specifies a total order. The learning task is to predict the gold standard ranking based on the features that can be extracted from the translation candidates. In other words, the ranker’s objective can be seen as performing a kind of automatic MT evaluation metric that does not require references on the translation candidates. Based on related work in MT evaluation (Specia et al., 2009; Albrecht and Hwa, 2007), we use a similar set of features such as: • N-gram matching with the underlying LM corpus • N-gram probabilities from the LM • Target to source-language lexical ambiguity • Average word movement from source to targetlanguage • The ratio of punctuation and digit in the source and target phrases • The average BLEU score: A BLEU evaluation of the hypothesis, using the other competing hypothesis as the pseudo references model’s top choice resulted in translation improvements over the baseline. LM weight used Baseline adapt. by group adapt. of indiv. wt. • Bigram and trig"
2010.amta-papers.17,P07-1066,0,0.0196056,"lel corpus, there is still a 0.85 BLEU score improvement. This suggests that individual weight adaptation may still be helpful for larger MT systems. 7 Related Work The major concepts used in our work are adaptation, re-scoring, automatic MT evaluation, and ranking; they have been widely studied in the MT literature. In this section, we highlight some of the most relevant previous work. In previous work on MT adaptation, many proposed to modify the baseline system to incorporate features from the source language, lexical translations and from the underlying system itself (Snover et al., 2008; Tam et al., 2007; Kim, 2004). In these studies, the components of the baseline SMT system are modified. Furthermore, new models are constructed for the translation of special test sets or individual phrases and sentences. In contrast, our approach does not directly change the baseline components. Instead, we allow the decoder to adjust the influence from a component according to its expected performance. To perform this kind of judgment, we make use of techniques similar to confidence estimation and automatic MT evaluation. Previous work on MT evaluation without references focuses on sentential or corpus leve"
2010.amta-papers.17,W06-3110,0,0.0141766,"ial level. We are less interested in the absolute scores than in the quality of the candidates relative to each other. In this way, our approach is more similar to the metric of (Duh, 2008), in which the evaluation is conducted by ranking. Also relevant is the series of work on system modification, such as post-decoding discriminative re-ranking. The discriminative re-ranking benefits from a set of richer but more computationally expensive features. These features range from deeper linguistic knowledge (Och et al., 2004) to system related knowledge like word and phrase level confidence score (Zens and Ney, 2006). Similarly we benefit from additional features in our weight ranking; however, the set of candidate hypotheses generated for weight ranking is different from a decoder’s n-best list. 8 Conclusion In contrast with the traditional method of using static decoding weights, here we introduced a framework of using dynamic decoding weights. We explored varying the language model’s decoder weights based on the characteristics of the source text. Following the insight that the weight adaptation should be performed on the part of a sentence with which the baseline MT system is having problems, we turn"
2010.amta-papers.17,N04-1021,0,\N,Missing
2010.amta-papers.20,P05-1074,0,0.0895465,"Missing"
2010.amta-papers.20,W07-0718,0,0.775602,"tion metrics to stand in for human judgments, we conduct a comprehensive experiment tuning versions of the M ETEOR - NEXT metric (Denkowski and Lavie, 2010) on multiple types of human judgments from multiple evaluations to determine which types of judgments are best suited for metric development. 2 Related Work The Association for Computational Linguistics (ACL) Workshop on Statistical Machine Translation (WMT) has conducted yearly evaluations of machine translation quality as well as meta-evaluation of human judgments of translation quality and automatic evaluation metric performance. WMT07 (Callison-Burch et al., 2007) compares multiple types of human MT evaluation tasks, including adequacy-fluency scale judgments and ranking judgments, across various criteria. Ranking judgments, in which annotators rank translation hypotheses of the same source sentence from different MT systems, are shown to have higher inter-annotator and intra-annotator agreement than relative and absolute adequacy-fluency judgments. The workshop also evaluates the correlation of several automatic evaluation metrics with both types of human judgments, showing that different metrics perform best on different tasks. While our work also di"
2010.amta-papers.20,W08-0309,0,0.229134,"by single words or phrases that would be forced into the same adequacy category can be easily ranked. This is especially important in evaluations where many similar MT systems compete, producing output that is nearly identical for many source sentences. As with adequacy and fluency, annotator agreement in the ranking task can be evaluated with the kappa coefficient. Shown in Table 1, both interannotator and intra-annotator agreement are higher in the ranking task than for adequacy or fluency. Based on the results of WMT07, the ranking task is made the default form of human judgment in WMT08 (Callison-Burch et al., 2008) and WMT09 (Callison-Burch et al., 2009). Although Table 2 shows a general slight decline in annotator agreement over these evaluations, attributable to the increasing number of similar MT systems providing translation hypotheses, the kappa values remain relatively high. Despite reported advantages, annotators still disagree on rankings in many cases and participants in WMT evaluations report several instances where ranking translations presents particular difficulty. Notably, the problem of longer sentences is even greater when annotators must keep multiple sentences in mind, leading to annot"
2010.amta-papers.20,W10-1751,1,0.871963,"t can be gleaned from collected judgments and how reliable this information will be when selecting an evaluation task. This work examines several types of human judgment tasks across multiple evaluations. We discuss the motivation, design, and results of these tasks in both theory and practice, focusing on sources of difficulty for annotators, informativeness of results, and consistency of evaluation conditions. As it is also advantageous to develop automatic evaluation metrics to stand in for human judgments, we conduct a comprehensive experiment tuning versions of the M ETEOR - NEXT metric (Denkowski and Lavie, 2010) on multiple types of human judgments from multiple evaluations to determine which types of judgments are best suited for metric development. 2 Related Work The Association for Computational Linguistics (ACL) Workshop on Statistical Machine Translation (WMT) has conducted yearly evaluations of machine translation quality as well as meta-evaluation of human judgments of translation quality and automatic evaluation metric performance. WMT07 (Callison-Burch et al., 2007) compares multiple types of human MT evaluation tasks, including adequacy-fluency scale judgments and ranking judgments, across"
2010.amta-papers.20,W07-0734,1,0.87519,"th human evaluation tasks and the task of tuning automatic evaluation metrics, we tune versions of M ETEOR - NEXT on various human judgment data sets. Following Snover et al. (2009), we examine the optimal parameter values for each type of human judgment. We also examine the correlation of each M ETEOR - NEXT version with human judgments from all other sets to determine the relative benefit of tuning to various types of human judgments. Correlation results for M ETEOR - NEXT are compared to those for three baseline metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and M ETEOR (Lavie and Agarwal, 2007). 5.1 Data We conduct tuning and evaluation experiments on six data sets spanning three human judgment tasks over two consecutive years. Adequacy data includes the full NIST Open MT human evaluation sets for (1) 2008 (Przybocki, 2008) and (2) 2009 (Przybocki, 2009). Ranking data includes all WMT ranking judgments for translations into English for (1) 2008 (Callison-Burch et al., 2008) and (2) 2009 (Callison-Burch et al., 2009). HTER data includes the GALE (Olive, 2005) unsequestered human evaluation data for (1) Phase 2 and (2) Phase 3. Where possible, we use the same data and evaluation crite"
2010.amta-papers.20,P03-1021,0,0.00857831,"f using monolingual post-editors to correct MT output. To our knowledge, no work has yet utilized data from this task to develop automatic metrics. 4 Automatic Evaluation Metrics Originally developed to stand in for human judgments in cases where collecting such judgments would be prohibitively time-consuming or expensive, automatic metrics of translation quality have many attractive properties. Not only do metrics score data sets quickly, but the problems of annotator agreement are not encountered as most metric scoring algorithms are deterministic. During minimum error rate training (MERT) (Och, 2003), many nearly-identical hypotheses must be reliably scored in a short amount of time. During error analysis, a single feature might be added or subtracted from a translation system, resulting in changes barely detectable by humans. In such cases, any annotator disagreement can undermine the informativeness of judgment data. To be effective, metrics must also have high correlation with the human judgments they are standing in for. To accomplish this, many recent metrics include several parameters that can be tuned to maximize correlation with various types of judgments. This leads to the questi"
2010.amta-papers.20,P02-1040,0,0.0962837,"to more balance parameters for HTER. Also notable is the HTER task’s low weight for stem matches, caused by the TER metric’s lack of such matches. Finally, the ranking task has the slightest fragmentation penalty, reflecting the highly similar word order of ranked hypotheses, followed by the adequacy task, while the HTER task has the harshest penalty, reflecting the strict requirement that each reordering requires an edit to correct. Table 4 shows the correlation and rank consistency results for M ETEOR - NEXT versions tuned on each type of data, as well as results for baseline metrics BLEU (Papineni et al., 2002), TER (Snover et Experiments To explore both human evaluation tasks and the task of tuning automatic evaluation metrics, we tune versions of M ETEOR - NEXT on various human judgment data sets. Following Snover et al. (2009), we examine the optimal parameter values for each type of human judgment. We also examine the correlation of each M ETEOR - NEXT version with human judgments from all other sets to determine the relative benefit of tuning to various types of human judgments. Correlation results for M ETEOR - NEXT are compared to those for three baseline metrics: BLEU (Papineni et al., 2002)"
2010.amta-papers.20,2006.amta-papers.25,0,0.713718,"ently on different judgment tasks, supporting the notion that metrics can be designed or tuned to have improved correlation with various types of human judgments. While many further analyses can be conducted on the resulting data, the evaluation results do not directly discuss the relative merits of the included human judgment scenarios or the task of metric tuning. Snover et al. (2009) explore several types of human judgments using TER-Plus (TERp), a highly configurable automatic MT evaluation metric. The authors tune versions of TERp to maximize correlation with adequacy, fluency, and HTER (Snover et al., 2006) scores and present an analysis of the resulting parameter values for each task. Adequacy and Fluency parameters favor recall, having low edit costs for inserting additional words in translation hypotheses and high edit costs for removing words in hypotheses, while HTER parameters are more balanced between precision and recall. In all cases, correlation with human judgments is significantly improved by tuning TERp on similar data. Our work includes a similar metric tuning experiment using the M ETEOR - NEXT (Denkowski and Lavie, 2010) metric on similar adequacy and HTER judgments as well as ra"
2010.amta-papers.20,W09-0441,0,0.0824451,"ne Translation Challenge (MetricsMATR) (Przybocki et al., 2008) comprehensively evaluates the correlation of 39 automatic MT evaluation metrics with several types of human judgments. The results indicate that metrics perform differently on different judgment tasks, supporting the notion that metrics can be designed or tuned to have improved correlation with various types of human judgments. While many further analyses can be conducted on the resulting data, the evaluation results do not directly discuss the relative merits of the included human judgment scenarios or the task of metric tuning. Snover et al. (2009) explore several types of human judgments using TER-Plus (TERp), a highly configurable automatic MT evaluation metric. The authors tune versions of TERp to maximize correlation with adequacy, fluency, and HTER (Snover et al., 2006) scores and present an analysis of the resulting parameter values for each task. Adequacy and Fluency parameters favor recall, having low edit costs for inserting additional words in translation hypotheses and high edit costs for removing words in hypotheses, while HTER parameters are more balanced between precision and recall. In all cases, correlation with human ju"
2010.amta-papers.20,D08-1076,0,\N,Missing
2010.amta-papers.20,W09-0401,0,\N,Missing
2010.amta-papers.20,C04-1046,0,\N,Missing
2010.amta-papers.34,C08-1005,0,0.0760964,"e have not the interest of control on the Palestinians life,” and “We do not have a desire to control the lives of the Palestinians.” Flexible ordering schemes consider “have not” versus “do not have” separately from “Palestinians life” versus “lives of the Palestinians.” Our match features have the most impact here because word order is less constrained. This is the type of search space we use in our experiments. 3.2 N -gram Match Features Agreement is central to system combination and most schemes have some form of n-gram match features. Of these, the simplest consider only unigram matches (Ayan et al., 2008; Heafield et al., 2009; Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 20"
2010.amta-papers.34,D08-1024,0,0.0428803,"lem here is that the decoder sees mostly the same λ each time and the optimizer sees mostly the same output each time, missing potentially better but different weights. Random restarts inside the optimizer do not solve this problem because this technique only finds better weights subject to decoded hypotheses. As the number of features increases (in some experiments to 39), the problem becomes more severe because the space of feature weights is much larger than the explored space. We propose a simulated annealing method to address problems with MERT, leaving other tuning methods such as MIRA (Chiang et al., 2008) and lattice MERT to future work. Specifically, when the decoder is given weights λ to use for decoding in iteration 0 ≤ j &lt; 10, it instead uses weights µ sampled according to j j µi ∼ U λi , 2 − λi 10 10     where U is the uniform distribution and subscript i denotes the ith feature. This sampling is done on a per-sentence basis, so the first sentence is decoded with different weights than the second sentence. The amount of random perturbation decreases linearly each iteration until the 10th and subsequent iterations where weights are used in the normal, unperturbed, fashion. The process"
2010.amta-papers.34,N10-1141,0,0.0367115,"r than unigrams, and system weights differ by task. 3 Related Work System combination takes a variety of forms that pair a space of hypothesis combinations with features to score these hypotheses. Here, we are primarily interested in three aspects of each combination scheme: the space of hypotheses, features that reward n-gram matches with system outputs, and the system weights used for those features. 3.1 Search Spaces Hypothesis selection (Hildebrand and Vogel, 2009) and minimum Bayes risk (Kumar and Byrne, 2004) select from k-best lists output by each system. In the limit case for large k, DeNero et al. (2010) adopt the search spaces of the translation systems being combined. Confusion networks preserve the word order of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this search space, the impact of our match features is limited to selection at the word level, multiword"
2010.amta-papers.34,D09-1125,0,0.0222909,"rder of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this search space, the impact of our match features is limited to selection at the word level, multiword lexical choice, and possibly selection of the backbone. Flexible ordering schemes use a reordering model (He and Toutanova, 2009) or dynamically switch backbones (Heafield et al., 2009) to create word orders not seen in any single translation. For example, these translations appear in NIST MT09 (Peterson et al., 2009a): “We have not the interest of control on the Palestinians life,” and “We do not have a desire to control the lives of the Palestinians.” Flexible ordering schemes consider “have not” versus “do not have” separately from “Palestinians life” versus “lives of the Palestinians.” Our match features have the most impact here because word order is less constrained. This is the type of search space we use in our"
2010.amta-papers.34,W09-0408,1,0.875916,"ckbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this search space, the impact of our match features is limited to selection at the word level, multiword lexical choice, and possibly selection of the backbone. Flexible ordering schemes use a reordering model (He and Toutanova, 2009) or dynamically switch backbones (Heafield et al., 2009) to create word orders not seen in any single translation. For example, these translations appear in NIST MT09 (Peterson et al., 2009a): “We have not the interest of control on the Palestinians life,” and “We do not have a desire to control the lives of the Palestinians.” Flexible ordering schemes consider “have not” versus “do not have” separately from “Palestinians life” versus “lives of the Palestinians.” Our match features have the most impact here because word order is less constrained. This is the type of search space we use in our experiments. 3.2 N -gram Match Features Agreement is cen"
2010.amta-papers.34,W09-0406,0,0.391648,"ed, these comprise a combination scheme that differs from others in three key ways: the search space is more flexible, the features consider matches longer than unigrams, and system weights differ by task. 3 Related Work System combination takes a variety of forms that pair a space of hypothesis combinations with features to score these hypotheses. Here, we are primarily interested in three aspects of each combination scheme: the space of hypotheses, features that reward n-gram matches with system outputs, and the system weights used for those features. 3.1 Search Spaces Hypothesis selection (Hildebrand and Vogel, 2009) and minimum Bayes risk (Kumar and Byrne, 2004) select from k-best lists output by each system. In the limit case for large k, DeNero et al. (2010) adopt the search spaces of the translation systems being combined. Confusion networks preserve the word order of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with"
2010.amta-papers.34,P08-2021,0,0.565426,"best system on NIST MT09 Arabic-English test data. Compared to a baseline system combination scheme from WMT 2009, we show improvement in the range of 1 BLEU point. 1 Introduction System combination merges the output of several machine translation systems to form an improved translation. While individual systems perform similarly overall, human evaluators report different error distributions for each system (Peterson et al., 2009b). For example, some systems are weak at word order while others have more trouble with nouns and verbs. Existing system combination techniques (Rosti et al., 2008; Karakos et al., 2008; Leusch et al., 2009a) ignore these distinctions by learning a single weight for each system. This weight is used in word-level decisions and therefore captures only lexical choice. We see two problems: system behavior differs in more ways than captured by a single weight and further current features only guide decisions at the word level. To remedy this situation, we propose new features that account for multiword behavior, each with a separate set of system weights. 2 Features Most combination schemes generate many hypothesis combinations, score them using a battery of features, and search"
2010.amta-papers.34,P07-2045,0,0.00669912,"esis with highest score to output. Formally, the system generates hypothesis h, evaluates feature function f , and multiplies linear weight vector λ by feature vector f (h) to obtain score λT f (h). The score is used to rank final hypotheses and to prune partial hypotheses during beam search. The beams contain hypotheses of equal length. With the aim of improving score and therefore translation quality, this paper focuses on the structure of features f and their corresponding weights λ. The feature function f consists of the following feature categories: Length Length of the hypothesis, as in Koehn et al. (2007). This compensates, to first order, for the impact of length on other features. LM Log probability from an SRI (Stolcke, 2002) language model. When the language model scores a word, it finds the longest n-gram in the model with the same word and context. We use the length n as a second feature. The purpose of this second feature is to provide the scoring model with limited control over language model backoff penalties. System 1: Supported Proposal of France System 2: Support for the Proposal of France Candidate: Support for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Trigram 1"
2010.amta-papers.34,N04-1022,0,0.608903,"from others in three key ways: the search space is more flexible, the features consider matches longer than unigrams, and system weights differ by task. 3 Related Work System combination takes a variety of forms that pair a space of hypothesis combinations with features to score these hypotheses. Here, we are primarily interested in three aspects of each combination scheme: the space of hypotheses, features that reward n-gram matches with system outputs, and the system weights used for those features. 3.1 Search Spaces Hypothesis selection (Hildebrand and Vogel, 2009) and minimum Bayes risk (Kumar and Byrne, 2004) select from k-best lists output by each system. In the limit case for large k, DeNero et al. (2010) adopt the search spaces of the translation systems being combined. Confusion networks preserve the word order of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this"
2010.amta-papers.34,1983.tc-1.13,0,0.684335,"Missing"
2010.amta-papers.34,P03-1021,0,0.264981,"that capture lexical and multiword agreement with each system. The weight on match count cs,n corresponds to confidence in n-grams from system s. However, this weight also accounts for correlation between features, which is quite high within the same system and across related systems. Viewed as language modeling, each cs,n is a miniature language model trained on the translated sentence output by system s and jointly interpolated with a traditional language model and with peer models. As described further in Section 4, we jointly tune the weights λ using modified minimum error rate training (Och, 2003). In doing so, we simultaneously learn several weights for each system, one for each length n-gram output by that system. The unigram weight captures confidence in lexical choice while weights on longer n-gram features capture confidence in word order and phrasal choices. These features are most effective with a variety of hypotheses from which to choose, so in Section 5 we describe a search space with more flexible word order. Combined, these comprise a combination scheme that differs from others in three key ways: the search space is more flexible, the features consider matches longer than u"
2010.amta-papers.34,P02-1040,0,0.0903278,"ere because word order is less constrained. This is the type of search space we use in our experiments. 3.2 N -gram Match Features Agreement is central to system combination and most schemes have some form of n-gram match features. Of these, the simplest consider only unigram matches (Ayan et al., 2008; Heafield et al., 2009; Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 2009a; Zens and Ney, 2006; Zhao and He, 2009). Some of these are conceptualized as a language model that, up to edge effects, exposes the log ratio of (n + 1)-gram matches to n-gram matches. An equivalent linear combination of these features exposes the log n-gram match counts directly. These separate features enable tu"
2010.amta-papers.34,P07-1040,0,0.0716064,"so system-level weights suffice here. For methods that use k-best lists, system weight may be moderated by some decreasing function of rank in the k-best list (Ayan et al., 2008; Zhao and He, 2009). Minimum Bayes risk (Kumar and Byrne, 2004) takes the technique a step further by using the overall system scores that determined the ranking. 4 Parameter Tuning Key to our model is jointly tuning the feature weights λ. In our experiments, weight vector λ is tuned using minimum error rate training (MERT) (Och, 2003) towards BLEU (Papineni et al., 2002). We also tried tuning towards TER minus BLEU (Rosti et al., 2007) and METEOR (Lavie and Denkowski, 2010), finding at best minor improvement in the targeted metric with longer tuning time. This may be due to underlying systems tuning primarily towards BLEU. In ordinary MERT, the decoder produces hypotheses given weights λ and the optimizer selects λ to rank the best hypotheses at the top. These steps alternate until λ converges or enough iterations happen. As the feature weights converge, the k-best lists output also converge. In our experiments, we use k = 300. For long sentences, this is a small fraction of the hypotheses that our flexible ordering scheme"
2010.amta-papers.34,W08-0329,0,0.34164,"f 6.67 BLEU over the best system on NIST MT09 Arabic-English test data. Compared to a baseline system combination scheme from WMT 2009, we show improvement in the range of 1 BLEU point. 1 Introduction System combination merges the output of several machine translation systems to form an improved translation. While individual systems perform similarly overall, human evaluators report different error distributions for each system (Peterson et al., 2009b). For example, some systems are weak at word order while others have more trouble with nouns and verbs. Existing system combination techniques (Rosti et al., 2008; Karakos et al., 2008; Leusch et al., 2009a) ignore these distinctions by learning a single weight for each system. This weight is used in word-level decisions and therefore captures only lexical choice. We see two problems: system behavior differs in more ways than captured by a single weight and further current features only guide decisions at the word level. To remedy this situation, we propose new features that account for multiword behavior, each with a separate set of system weights. 2 Features Most combination schemes generate many hypothesis combinations, score them using a battery of"
2010.amta-papers.34,2006.amta-papers.25,0,0.0492678,"ccording to human judges (CallisonBurch et al., 2009). The organizers of the following WMT also dropped Hungarian. Official tuning and evaluation sets are used, except for MT09 Arabic-English where only unsequestered portions are used for evaluation. Language model training data for WMT is constrained to the provided English from monolingual and French-English corpora. There was no constrained informal system combination track for MT09 so we use a model trained on the Gigaword (Graff, 2003) corpus. Scores are reported using uncased BLEU (Papineni et al., 2002) from mteval-13a.pl, uncased TER (Snover et al., 2006) 0.7.25, and METEOR (Lavie and Denkowski, 2010) 1.0 with Adequacy-Fluency parameters. For each source language, we selected a few subsets of systems to combine and picked the set that combined best on tuning data. Performance is surprisingly good on Arabic and competitive with top MT09 combinations. On French and Spanish, Google scored much higher than did other systems. Like Leusch et al. (2009b), we show no gain over Google on these source languages. Most system combination schemes showed larger gains in MT09 than in WMT. In addition to different language pairs, one possible explanation is t"
2010.amta-papers.34,W06-3110,0,0.0516877,"field et al., 2009; Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 2009a; Zens and Ney, 2006; Zhao and He, 2009). Some of these are conceptualized as a language model that, up to edge effects, exposes the log ratio of (n + 1)-gram matches to n-gram matches. An equivalent linear combination of these features exposes the log n-gram match counts directly. These separate features enable tuning n-gram weights. 3.3 System Weighting Different and correlated system strengths make it important to weight systems when combining their votes on n-grams. The simplest method treats these system weights as a hyper parameter (Heafield et al., 2009; Hildebrand and Vogel, 2009). The hyper parameter mig"
2010.amta-papers.34,N09-2052,0,0.270683,"Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 2009a; Zens and Ney, 2006; Zhao and He, 2009). Some of these are conceptualized as a language model that, up to edge effects, exposes the log ratio of (n + 1)-gram matches to n-gram matches. An equivalent linear combination of these features exposes the log n-gram match counts directly. These separate features enable tuning n-gram weights. 3.3 System Weighting Different and correlated system strengths make it important to weight systems when combining their votes on n-grams. The simplest method treats these system weights as a hyper parameter (Heafield et al., 2009; Hildebrand and Vogel, 2009). The hyper parameter might be set to an incr"
2010.amta-papers.34,W09-0407,0,\N,Missing
2010.amta-papers.34,W09-0401,0,\N,Missing
2010.amta-papers.4,P05-1071,0,0.705304,"ble when Arabic is the source language, but is clearly problematic when translating into Arabic. Therefore, we use the “enriched” form of the Arabic raw text throughout this work. The enriched form of text uses the correct form of Alif  اand the right form of Ya  يand Alif Maqsura  ىin word final position. 3 Arabic Preprocessing Schemes We experiment with various Arabic preprocessing schemes by splitting off different subsets of the clitics mentioned in Section 2. The raw Arabic text is enriched and tokenized using the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow, 2005). The various Arabic tokenization schemes that we experiment with, span a segmentation spectrum ranging from coarse segmentation, which uses unsegmented text, to fine segmentation which splits off all possible clitics. All the different tokenization schemes are described in detail below from coarse to fine : • • • • • UT: This scheme uses the full (untokenized) enriched form of the word. This scheme is used as input to produce the other schemes. S0: This scheme splits off the conjunction proclitic w+. S1: This scheme splits off +f in addition to the w+ split by S0. S2: This scheme splits off a"
2010.amta-papers.4,P08-2039,0,0.206301,"Missing"
2010.amta-papers.4,W08-0509,0,0.0123851,"of 4,867,675 sentence pairs with 152 million words on the English side. The Arabic side of the training corpora is used to train nine 5gram language models for the different tokenization schemes using the SRILM toolkit (Stolcke, 2002). An additional two 7-gram language models were trained for the S3-S5 tokenization schemes in order to account for the increase in length of the segmented Arabic. Tokens and type counts of the processed Arabic training corpora, for the different tokenization schemes, is given in Table 3. The processed and filtered parallel corpora was then aligned using MGIZA++ (Gao and Vogel, 2008); an extended and optimized multi-threaded version of GIZA++ (Och and Ney, 2003). The Moses toolkit (Koehn et. al, 2007) is then used to symmetrize the alignement using the grow-diagfinal-and heuristic and to extract phrases with maximum length of 7. A distortion model lexically conditioned on both the Arabic phrase and English phrase is then trained. 5.2 Tuning and testing sets We use existing Arabic-to-English test sets available within the NIST09 resources to construct our English-to-Arabic tuning and test sets. As all NIST09 test sets were intended for use in Arabicto-English machine trans"
2010.amta-papers.4,J03-1002,0,0.00311687,"c side of the training corpora is used to train nine 5gram language models for the different tokenization schemes using the SRILM toolkit (Stolcke, 2002). An additional two 7-gram language models were trained for the S3-S5 tokenization schemes in order to account for the increase in length of the segmented Arabic. Tokens and type counts of the processed Arabic training corpora, for the different tokenization schemes, is given in Table 3. The processed and filtered parallel corpora was then aligned using MGIZA++ (Gao and Vogel, 2008); an extended and optimized multi-threaded version of GIZA++ (Och and Ney, 2003). The Moses toolkit (Koehn et. al, 2007) is then used to symmetrize the alignement using the grow-diagfinal-and heuristic and to extract phrases with maximum length of 7. A distortion model lexically conditioned on both the Arabic phrase and English phrase is then trained. 5.2 Tuning and testing sets We use existing Arabic-to-English test sets available within the NIST09 resources to construct our English-to-Arabic tuning and test sets. As all NIST09 test sets were intended for use in Arabicto-English machine translation, each Arabic source sentences is associated with four English references."
2010.amta-papers.4,P02-1040,0,0.0779283,"Missing"
2010.amta-papers.4,2006.amta-papers.25,0,0.0199256,"Missing"
2010.amta-papers.4,N06-2013,0,0.100317,"ion schemes and lay out a set of general observations on the effect of splitting off different sets of clitics (affixes) on the performance of a broad coverage PBSMT system. As the Arabic output of systems is segmented it needs to be recombined (detokenized). We experiment with six different detokenization techniques increasing in the level of complexity. The best re tokeninzation technique is used in recombining the output of the different systems. Previous works that addressed the effect of Arabic rich morphology and tokenization on SMT concentrated on Arabic-to-English machine translation (Habash and Sadat, 2006; Zollmann, 2006; Lee, 2004). However, few works focused on SMT into Arabic. Sarikaya and Deng (2007) use joint morphological-lexical language models to rerank the output English-dialectal Arabic MT. A research more relevant to our work was done by Badr et. al (2008) . In their work they compare a segmented Englsih-to-Arabic system with an unsegmented system. They also experiment with a number of detokenization techniques. However, in their work they just compare a single segmentation scheme (and one variation of it) to the unsegmented baseline without mentioning what motivated the choice of t"
2010.amta-papers.4,N04-4015,0,0.0205826,"observations on the effect of splitting off different sets of clitics (affixes) on the performance of a broad coverage PBSMT system. As the Arabic output of systems is segmented it needs to be recombined (detokenized). We experiment with six different detokenization techniques increasing in the level of complexity. The best re tokeninzation technique is used in recombining the output of the different systems. Previous works that addressed the effect of Arabic rich morphology and tokenization on SMT concentrated on Arabic-to-English machine translation (Habash and Sadat, 2006; Zollmann, 2006; Lee, 2004). However, few works focused on SMT into Arabic. Sarikaya and Deng (2007) use joint morphological-lexical language models to rerank the output English-dialectal Arabic MT. A research more relevant to our work was done by Badr et. al (2008) . In their work they compare a segmented Englsih-to-Arabic system with an unsegmented system. They also experiment with a number of detokenization techniques. However, in their work they just compare a single segmentation scheme (and one variation of it) to the unsegmented baseline without mentioning what motivated the choice of this specific segmentation. T"
2010.amta-papers.4,N06-2051,0,0.0458021,"Missing"
2010.amta-papers.4,W04-3250,0,0.0337378,"Missing"
2010.amta-papers.4,N07-2037,0,0.148629,"of clitics (affixes) on the performance of a broad coverage PBSMT system. As the Arabic output of systems is segmented it needs to be recombined (detokenized). We experiment with six different detokenization techniques increasing in the level of complexity. The best re tokeninzation technique is used in recombining the output of the different systems. Previous works that addressed the effect of Arabic rich morphology and tokenization on SMT concentrated on Arabic-to-English machine translation (Habash and Sadat, 2006; Zollmann, 2006; Lee, 2004). However, few works focused on SMT into Arabic. Sarikaya and Deng (2007) use joint morphological-lexical language models to rerank the output English-dialectal Arabic MT. A research more relevant to our work was done by Badr et. al (2008) . In their work they compare a segmented Englsih-to-Arabic system with an unsegmented system. They also experiment with a number of detokenization techniques. However, in their work they just compare a single segmentation scheme (and one variation of it) to the unsegmented baseline without mentioning what motivated the choice of this specific segmentation. They also use a training corpora of 3M words and conclude that the effect"
2010.amta-srw.4,J90-2002,0,0.48683,"East somewhat closer together by better understanding each other’s societies. 1 In certain respects, Arabic Dialects have morpho-syntactic features closer to Hebrew than Modern Standard Arabic, e.g., the absence of nominal case and verbal mood, the behavior of the feminine ending in genitive constructions, the gendernumber invariance of the relativizer, and the dominance of SVO order over VSO order. We do not discuss Arabic dialects here. Alon Lavie Shuly Wintner LTI Dept. of Computer Science Carnegie Mellon U. U. of Haifa Pittsburgh, PA Haifa, Israel The dominant paradigm in contemporary MT (Brown et al., 1990) relies on large-scale parallel corpora from which correspondences between the two languages can be extracted. However, such abundant parallel corpora currently exist only for few language pairs; and low- and medium-density languages (Varga et al., 2005) require alternative approaches. Specifically, no parallel corpora exist for Hebrew–Arabic.2 As an alternative to the pure statistical approach, we are currently developing a Hebrew-to-Arabic MT system, using the Stat-XFER framework (Lavie, 2008), which is particularly suited for low-resource language pairs. We discuss in Section 2 some linguis"
2010.amta-srw.4,N06-2013,1,0.867525,"encode information on gender and rationality of nouns, which is crucial for enforcing N-Adj agreement. The implication is that in order to generate Arabic, one must overgenerate both masculine and feminine forms, delegating the choice to the language model, which chooses poorly in long-distance dependencies. 3.2 Morphological challenges Translating between two morphologically rich languages poses challenges in analysis, transfer and generation. The complex morphology induces an inherent data sparsity problem, and the limitation imposed by the dearth of available parallel corpora is magnified (Habash and Sadat, 2006). We use a morphological analyzer (Itai and Wintner, 2008) for the Hebrew source, with no morphological disambiguation module.4 This causes many wrong analyses to be processed and dramatically increases the size of the hypothesis lattice. For generation, we use an Arabic morphological generator (Habash, 2004) which requires proper specification of the morpho-syntactic features in order to generate the correct inflected form. Clitics are generated separately and then attached as a postprocess (El Kholy and Habash, 2010). 3.3 Syntactic challenges Arabic word order is relatively free, as in Hebre"
2010.amta-srw.4,W09-0425,1,0.763174,"ntrols the underlying parsing and transfer process. Crucially, Stat-XFER is a statistical MT framework, which uses statistical information to weigh word translations, phrase correspondences and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manuallycrafted language resources, including dictionaries, morphological processors and transfer rules. Stat-XFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English or Mapudungunto-Spanish. Specifically, we use a Hebrew morphological analyzer (Itai and Wintner, 2008), a mediumsized dictionary, an Arabic morphological generator (Habash, 2004), and a tokenized version of the Arabic Gigaword (Graff et al., 2006) corpus as a language model. We manually constructed a grammar, currently consisting of 42 rules. Some rules manipulate morphemes. After decoding (which uses the language model) we detokenize the output sentence in its morpheme representation (El Kholy and Habash, 2010) to produce"
2010.amta-srw.4,D07-1005,0,0.0596921,"Missing"
2010.amta-srw.4,2004.tmi-1.1,1,0.873797,"linear combination of several features, and a beam-search controls the underlying parsing and transfer process. Crucially, Stat-XFER is a statistical MT framework, which uses statistical information to weigh word translations, phrase correspondences and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manuallycrafted language resources, including dictionaries, morphological processors and transfer rules. Stat-XFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English or Mapudungunto-Spanish. Specifically, we use a Hebrew morphological analyzer (Itai and Wintner, 2008), a mediumsized dictionary, an Arabic morphological generator (Habash, 2004), and a tokenized version of the Arabic Gigaword (Graff et al., 2006) corpus as a language model. We manually constructed a grammar, currently consisting of 42 rules. Some rules manipulate morphemes. After decoding (which uses the language model) we detokenize the output sentence in its"
2010.amta-srw.4,J05-4003,0,0.0364466,"pected to be ambiguous; however, Google produces the following wrong translations in such cases: (15) atm / atn =⇒ Ant you.pl.m / you.pl.f =⇒ you.sg.m/f amrti say.1sg.past qlt say.1sg.past lkm =⇒ to+you.2.pl.m-dat. =⇒ lk to+you.2.sg.m/f-gen. The second test uses the fact that plural nouns in English are unspecified for gender, whereas in Hebrew and Arabic they are. Here, gender is lost in translation of plurality, and the decoder chose the most common option according to the LM. (16) mwrim / mwrwt =⇒ mςlmyn teachers.m / teachers.f =⇒ teachers.m 5 A third approach is to use comparable corpora (Munteanu and Marcu, 2005); but with no parallel data whatsoever, this is unlikely to succeed. 6 http://www.google.com/language_tools, accessed May 5th, 2010. 7 Another Hebrew-to-Arabic MT system, http://www. microsofttranslator.com/, also uses English as a pivot language, and shows similar characteristics. In the third test, we use words which are lexically ambiguous in English but not in Hebrew or Arabic. (17)(a) Tblh =⇒ TAwl¯ h table (data) =⇒ table (furniture) (b) bnq =⇒ sAHl bank (financial) =⇒ bank (shore) (c) idni =⇒ ktyb manual (by-hand) =⇒ manual (booklet) Finally, we used proper names and morphologically comp"
2010.amta-srw.4,1987.mtsummit-1.16,0,0.657819,"ures: person, gender, number, aspect (perfective, imperfective and imperative), voice (passive or active), and mood (indicative, subjunctive or jussive). For every noun, 72 forms are returned (excluding possible clitics), as a result of the various values of the features gender, number, case, possessiveness and definiteness. 4 Possible approaches As the standard paradigm of statistical MT is not applicable to Hebrew-to-Arabic MT, due to the dearth of available parallel corpora, two alternatives present themselves. One is translating using a third language (most naturally, English) as a pivot (Muraki, 1987; Wu and Wang, 2007); the other is relying on linguistically-motivated transfer rules, augmented by deep linguistic processing of both the source and the target languages.5 We consider both approaches below. 4.1 Using English as pivot The dominant Hebrew-to-Arabic MT system is Google’s.6 Google has been known to use ‘bridge’ languages in translation (Kumar et al., 2007). We provide evidence that Google’s Hebrew-to-Arabic MT uses English as a pivot, and demonstrate the shortcomings of this approach.7 As a first test, we use the number- and genderambiguity of second-person pronouns in English (y"
2010.amta-srw.4,P07-1108,0,0.0214551,"gender, number, aspect (perfective, imperfective and imperative), voice (passive or active), and mood (indicative, subjunctive or jussive). For every noun, 72 forms are returned (excluding possible clitics), as a result of the various values of the features gender, number, case, possessiveness and definiteness. 4 Possible approaches As the standard paradigm of statistical MT is not applicable to Hebrew-to-Arabic MT, due to the dearth of available parallel corpora, two alternatives present themselves. One is translating using a third language (most naturally, English) as a pivot (Muraki, 1987; Wu and Wang, 2007); the other is relying on linguistically-motivated transfer rules, augmented by deep linguistic processing of both the source and the target languages.5 We consider both approaches below. 4.1 Using English as pivot The dominant Hebrew-to-Arabic MT system is Google’s.6 Google has been known to use ‘bridge’ languages in translation (Kumar et al., 2007). We provide evidence that Google’s Hebrew-to-Arabic MT uses English as a pivot, and demonstrate the shortcomings of this approach.7 As a first test, we use the number- and genderambiguity of second-person pronouns in English (you). Since Hebrew an"
2010.amta-tutorials.4,lavie-etal-2004-significance,1,\N,Missing
2010.amta-tutorials.4,W09-0441,0,\N,Missing
2010.amta-tutorials.4,P02-1040,0,\N,Missing
2010.amta-tutorials.4,W08-0312,1,\N,Missing
2010.amta-tutorials.4,W09-0401,0,\N,Missing
2010.amta-tutorials.4,W05-0909,1,\N,Missing
2010.amta-tutorials.4,W07-0734,1,\N,Missing
2010.amta-tutorials.4,H05-1093,1,\N,Missing
2010.amta-tutorials.4,D08-1076,0,\N,Missing
2012.amta-papers.4,2010.amta-papers.16,0,0.0900591,"Missing"
2012.amta-papers.4,W09-0432,0,0.169229,"tems are often used for information assimilation, which allows users to make sense of information written in various languages they do not speak. This use case is particularly important for translation web services, such as Google Translate and Microsoft Bing Translator, which seek to make more of the web accessible to more users. A crucial challenge facing such systems is that they must translate documents from a variety of different domains, but it has been observed that the performance of statistical systems can suffer substantially when testing conditions deviate from training conditions (Bertoldi and Federico, 2009). However, it is not always possible or costeffective to collect training data for all of the desired application domains. In fact, training data tends to be collected opportunistically from any available sources rather than from curated sources that match the distribution of test data (Koehn and Schroeder, 2007). As a result, inputs from each application domain may frequently be very different from the training data. With such domain mismatches being commonplace, this paper looks at a way of adapting the behavior of a translation system based on the domain of the input documents, which can be"
2012.amta-papers.4,W06-1615,0,0.056453,"edze et al. (2007) report their experiments in a shared task for domain adaptation of dependency parsing in which they explored adding features more likely to transfer across domains and removing features less likely to transfer. Machine learning: Domain adaptation has also been well-studied from a more general machine learning perspective. Daum´e (2006) points out that “the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution” and goes on to formuvvlate the M EGA (Maximum Entropy Genre Adaptation) model. Blitzer et al. (2006; Blitzer (2007) describe structural correspondence learning, which automatically discovers features that behave similarly in both a source and target domain. Ben-David et al. (2010) provide formal bounds on source and target error for various discriminative learning criteria. Huang and Yates (2012) propose a domain adaptation method of representation learning, which automatically discovers feature more likely to generalize across domains. By using posterior regularization to bias the process of representation learning, they observe improvements on a part of speech tagging task and a named ent"
2012.amta-papers.4,D08-1024,0,0.0610337,"due to sentence-level LMs being impractical due to their large size, which results from the large space of target translations. 3 guage model probability of each partial hypothesis. In the case of the cdec decoder, which was used to implement this work, the modification involved only a few lines of code. 2.2 Pairwise Ranking Optimization As the number of domains increases, feature augmentation can result in much larger feature sets to be optimized. While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008) has been shown to be effective over larger feature sets, as an on-line margin learning algorithm, it is more difficult to regularize – this will become important in Section 2.4. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner. PRO works by sampling pairs of hypotheses from the decoder’s k-best list and then providing these pairs as a binary training examples to a standard binary classifier to obtain a new set of weights for the linear model. In our case, the binary classifier is trained using L-BFGS. This procedure of sampling training pairs and then optimi"
2012.amta-papers.4,J07-2003,0,0.0779102,"regularization as partitioning the feature set into 2 groups (domain-agnostic and domain-specific) and then applying a `2 regularizer with weight γ to the domainspecific group. This preserves the convexity of the objective function and makes it easy to incorporate into the gradient-based updates of PRO. With this in mind, the gradient is: ∇w Rcomplexity = γ |h| X 2wi (2) i=0 iff SPECIFIC(hi ) We apply this penalty to the domain-specific features in addition to the default `2 regularizer. 3 Experimental Setup Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first wordaligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. Such aligned subphrases are used to generalize their parent phrases by being substituted as a single nonterminal symbol [X]. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrase-based and chart-based paradigms including Hiero and syntactic systems. Our decision to use Hiero was primarily motivated by the cdec decoder’s API being most amenable to im"
2012.amta-papers.4,W07-0722,0,0.0306277,"ature weights to aggregate the component models. Unlike the work in this paper, the optimization procedure for training these final feature weights cannot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et al. (2007) use source monolingual data in various ways to improve target domain performance, focusing on corpus filtering techniques such that more releva"
2012.amta-papers.4,P07-1033,0,0.338681,"Missing"
2012.amta-papers.4,P10-4002,1,0.822415,"se translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. Such aligned subphrases are used to generalize their parent phrases by being substituted as a single nonterminal symbol [X]. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrase-based and chart-based paradigms including Hiero and syntactic systems. Our decision to use Hiero was primarily motivated by the cdec decoder’s API being most amenable to implenting these techniques. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011), though analysis indicates that the parameters converged much earlier. The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization (as described in Section 2.4) is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix arr"
2012.amta-papers.4,W07-0717,0,0.0722863,"on adapting the translation model and language model. Koehn and Schroeder (2007) explore several techniques for domain adaptation in SMT including multiple translation models (via multiple factored decoding paths), interpolated langauge models, and multiple language models. Xu et al. (2007) build a general domain translation system and then construct domain-specific language models and tune domain-specific feature weights to aggregate the component models. Unlike the work in this paper, the optimization procedure for training these final feature weights cannot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to ac"
2012.amta-papers.4,D10-1044,0,0.0681968,"nguage models. Xu et al. (2007) build a general domain translation system and then construct domain-specific language models and tune domain-specific feature weights to aggregate the component models. Unlike the work in this paper, the optimization procedure for training these final feature weights cannot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et"
2012.amta-papers.4,W12-3134,0,0.0128464,"eline learner. PRO works by sampling pairs of hypotheses from the decoder’s k-best list and then providing these pairs as a binary training examples to a standard binary classifier to obtain a new set of weights for the linear model. In our case, the binary classifier is trained using L-BFGS. This procedure of sampling training pairs and then optimizing the pairwise rankings is repeated for a specified number of iterations. It has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). 2.3 Impact on Time and Space Requirements In this section, we describe the minimal impact that this technique has on development and runtime time and space requirements. During system development, no additional time nor space is required for building additional translation models or language models since we construct only one per language pair. This also holds at runtime, which can be important if multiple deployed translation systems are competing for CPU and RAM resources on a shared server. The main burden introduced by feature augmentation is the larger number of features itself. As disc"
2012.amta-papers.4,D12-1120,0,0.020982,"a more general machine learning perspective. Daum´e (2006) points out that “the most basic assumption used in statistical learning theory is that training data and test data are drawn from the same underlying distribution” and goes on to formuvvlate the M EGA (Maximum Entropy Genre Adaptation) model. Blitzer et al. (2006; Blitzer (2007) describe structural correspondence learning, which automatically discovers features that behave similarly in both a source and target domain. Ben-David et al. (2010) provide formal bounds on source and target error for various discriminative learning criteria. Huang and Yates (2012) propose a domain adaptation method of representation learning, which automatically discovers feature more likely to generalize across domains. By using posterior regularization to bias the process of representation learning, they observe improvements on a part of speech tagging task and a named entity recognition task. Domain adaptation has also been framed as a semi-supervised learning problem in which unlabelled in-domain data is used to augment out-of-domain labelled data (Daum´e III et al., 2010). Perhaps most similar to this work is Daum´e (2007), which proposes a method for feature augm"
2012.amta-papers.4,2007.mtsummit-papers.34,0,0.0262638,"ot share statistics among domains. Foster and Kuhn (2007) train independent models on each domain and use a mixture model (both linear and log-linear) to weight the component models (both the translation model and language model) appropriately for the current context. This was later extended by Foster et al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et al. (2007) use source monolingual data in various ways to improve target domain performance, focusing on corpus filtering techniques such that more relevant data is included in the models. Bertoldi and Federico (2009) saw large improvements by using automatic translations of monolingual in-domain data to"
2012.amta-papers.4,W07-0733,0,0.493994,"e to more users. A crucial challenge facing such systems is that they must translate documents from a variety of different domains, but it has been observed that the performance of statistical systems can suffer substantially when testing conditions deviate from training conditions (Bertoldi and Federico, 2009). However, it is not always possible or costeffective to collect training data for all of the desired application domains. In fact, training data tends to be collected opportunistically from any available sources rather than from curated sources that match the distribution of test data (Koehn and Schroeder, 2007). As a result, inputs from each application domain may frequently be very different from the training data. With such domain mismatches being commonplace, this paper looks at a way of adapting the behavior of a translation system based on the domain of the input documents, which can be matched during both tuning and test time. One of the key trade offs in designing a statistical model is the balance between bias and variance. In domain adaptation, we would like to bias each domain’s model toward the distribution of that specific domain, yet we also desire models with low variance. Since each d"
2012.amta-papers.4,D07-1104,0,0.0202934,"syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011), though analysis indicates that the parameters converged much earlier. The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization (as described in Section 2.4) is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008a; Lopez, 2007; Lopez, 2008b), which is distributed with cdec: • log Pcoherent (e|f ): The coherent phrase-tophrase translation probability (Lopez, 2008b, p. 103). The phrasal probability of each English SCFG antecedent (e.g. “el [X] gato”) given a particular foreign SCFG antecedent “the [X] cat” combined with coherence, the ratio of successful source extractions to the number of attempted extractions • log Plex (e|f ), log Plex (f |e): The lexical alignment probabilities within each translation rule, as computed by a maximum likelihood estimation over the Viterbi alignments Sentences Translations NIST Trai"
2012.amta-papers.4,C08-1064,0,0.0186954,"that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011), though analysis indicates that the parameters converged much earlier. The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization (as described in Section 2.4) is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008a; Lopez, 2007; Lopez, 2008b), which is distributed with cdec: • log Pcoherent (e|f ): The coherent phrase-tophrase translation probability (Lopez, 2008b, p. 103). The phrasal probability of each English SCFG antecedent (e.g. “el [X] gato”) given a particular foreign SCFG antecedent “the [X] cat” combined with coherence, the ratio of successful source extractions to the number of attempted extractions • log Plex (e|f ), log Plex (f |e): The lexical alignment probabilities within each translation rule, as computed by a maximum likelihood estimation over the Viterbi alignments Sentences Translat"
2012.amta-papers.4,P02-1040,0,0.0867868,"t the 0.01 level according to approximate randomization over 5 optimizer replications. Test (All Domains) Baseline Domain Augmented 46.5 47.5 (+1.0) Figure 6: Results of feature augmentation experiments on the 1M sentence Czech→English data set as measured by the BLEU metric. The domain augmented system has 49 features. Both the Meteor and TER evaluation metrics also showed improvements. Results are significant at the 0.01 level according to approximate randomization over 3 optimizer replications. Evaluation: We quantify increases in translation quality using automatic metrics including BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by measuring statistical significance according to approximate randomization (Clark et al., 2010).7 Evaluation is performed on tokenized lowercased references. 4 Results and Analysis We show the results of using feature augmentation for domain adaptation to an Arabic→English system in Figure 5. There, we see an overall improvement of only 0.2 - 0.3 BLEU. Interestingly, we see an improvement of up to 0.7 BLEU in the weblog domain while we see no improvement in the newswire do7 MultEval 0.4.2 is available at github.com/jhclark/ multev"
2012.amta-papers.4,P07-1004,0,0.0504022,"al. (2010) to examine fine-grained instance-level characteristics rather than requiring each domain to have a distinct model. Word alignment: Civera and Juan (2007) explore an extension of the HMM alignment model that performs domain adaptation using mixture modelling. Automatic Post-Editing: Isabelle et al. (2007) use an automatic post-editor to accomplish domain adaptation, effectively “translating” from a domain-agnostic version of the target language into a domain-adapted version of the target language. Monolingual data: Others have used monolingual data to improve in-domain performance. Ueffing et al. (2007) use source monolingual data in various ways to improve target domain performance, focusing on corpus filtering techniques such that more relevant data is included in the models. Bertoldi and Federico (2009) saw large improvements by using automatic translations of monolingual in-domain data to augment the training data of their original system. Domain identification: Closely related to domain adaptation is domain identification. Banerjee et al. (2010) focus on the problem of determining the domain of the input data so that the appropriate domain-specific translation system can be used. EBMT:"
2012.amta-papers.4,P11-2031,1,\N,Missing
2012.amta-papers.4,N10-1031,1,\N,Missing
2012.amta-papers.4,D07-1112,0,\N,Missing
2012.amta-papers.4,bojar-etal-2012-joy,0,\N,Missing
2012.amta-papers.4,D11-1125,0,\N,Missing
2012.amta-papers.6,E06-1032,0,0.0855659,"rsome to correct. 3.3 Automatic Metrics While shared evaluation tasks often include human evaluation, the majority of research in machine translation relies on automatic metric scores to measure improvement, and even in shared evaluations, most translation systems are optimized toward automatic metrics. Generally, improvement in BLEU score, which measures simple surface word n-gram precision balanced with a brevity penalty (Papineni et al., 2002), is presented as evidence of improvement in translation quality. However, BLEU has been shown to be an insufficient stand-in for adequacy judgments (Callison-Burch et al., 2006). We similarly evaluate its ability to predict post-editing effort. Table 2 shows BLEU-scored system outputs from the WMT11 Czech-to-English translation track (Callison-Burch et al., 2011), along with reference translations. We additionally provide minimally post-edited translations and HTER scores. In each example, the translation with the higher BLEU score actually requires more effort to post-edit. In the first case, sentence 2 is penalized for using a different Reference System 1 Post-edit 1 System 2 Post-edit 2 Reference System 1 Post-edit 1 System 2 Post-edit 2 Reference System 1 Post-ed"
2012.amta-papers.6,W07-0718,0,0.0995054,"Missing"
2012.amta-papers.6,W11-2105,0,0.0131881,"trics that are widely used as objective functions in MT system optimization and as criteria for selecting the best system configurations to use in production environments, including post-editing workflows. The low correlation coefficients for BLEU, in addition to the examples in Section 3.3, provide a counterexample to the common notion that improvements in BLEU translate to improvements in quality since BLEU is most often unduly harsh. In reality, increases and decreases in BLEU score are only weakly correlated with translation utility as predicted by experts and scored by HTER. 2 The AMBER (Chen and Kuhn, 2011) and MPF (Popovi´c, 2011) families of metrics correlate better with human judgments of translations into Spanish but have no publicly available scoring tools. 4.2 Predicting Translation Usability Reliable prediction of translation usability is one of the most important aspects of incorporating MT into translation workflows. To avoid wasting translators’ time, systems should be able to predict when MT output is sufficiently good to serve as a starting point for post-editing or sufficiently bad to require total re-translation and recommend accordingly. In an additional experiment, we simplify th"
2012.amta-papers.6,W11-2107,1,0.926103,"Versus Post-editing Utility Large machine translation evaluation campaigns such as the ACL Workshops on Statistical Machine Translation (Callison-Burch et al., 2011) and NIST Open Machine Translation Evaluations (Przybocki, 2009) focus on improving translation adequacy, the perceived quality of fully automatic translations compared to reference translations. As such, current techniques for MT system building, optimization, and evaluation are largely geared toward improving performance on this task. Automatic metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and Meteor (Denkowski and Lavie, 2011), designed to correlate well with adequacy judgments, are often used as stand-ins for actual judgments during optimization and evaluation. 3.1 Adequacy and Ranking Originally introduced by the Linguistics Data Consortium, adequacy ratings elicit straightforward quality judgments of machine translation output according to numeric scales (LDC, 2005). Traditionally, these judgments were split between adequacy, the degree to which MT output captures the meaning of a reference translation, and f luency, the degree to which MT output is grammatically correct in the target language. Due to observed h"
2012.amta-papers.6,2010.amta-papers.21,0,0.0113055,"redict post-editor effort as measured by HTER, editing time, and editor post-assessments. Post-assessments (editors’ selfratings of how much effort was required to correct MT output) appear to be most predictable across language pairs. Particularly useful confidence estimation features include source and target language model scores for both surface word forms and partof-speech tags. This work bears some similarities to our experiments, though we focus on the challenges of predicting overall MT system utility for future inputs based on automatically evaluated performance on a development set. Hardt and Elming (2010) show the potential benefit of tighter integration of MT into translation workflows by conducting an experiment that simulates incrementally re-training a MT system as translators provide post-edited translations. Retraining is shown to greatly improve performance when input sentences are taken from the same domain. This type of work, focusing heavily on MT system training, especially requires reliable automatic metrics for predicting post-editing effort. In addition to measuring quantitative performance of MT systems for post-editing, work such as that by Blain et al. (2011) focuses on qualit"
2012.amta-papers.6,2010.amta-papers.27,0,0.306926,"Missing"
2012.amta-papers.6,P07-2045,0,0.00805075,"necessarily yield the most efficient translation workflow. 4 Experiments To empirically evaluate the behavior of human posteditors and the effectiveness of current MT techniques for predicting translation utility, we conduct a series of experiments to simulate a real-world localization scenario. We selected 5 English–Spanish parallel documents in the software documentation domain, totaling 90 sentences. Each English sentence was translated into Spanish using two translation engines: Microsoft Translator’s online service1 and a phrase-based Moses system representative of the 2011 WMT baseline (Hoang et al., 2007; Callison-Burch et al., 2011). The outputs of the two systems, which have significant lexical differences but are statistically indistinguishable by automatic metrics, were combined into a single data set of 180 translations. The Spanish side of the parallel data provides reference translations. We employed the assistance of an expert translator and several students of translation studies to obtain two types of annotation for each automatic translation. First, the expert translator assigned a rating from 1 to 4 predicting the degree of post editing that should be required to correct the sente"
2012.amta-papers.6,P03-1021,0,0.0321314,"hine translation evaluation campaign to use human-targeted translation edit rate (HTER) (Snover et al., 2006) as the primary evaluation metric. This was motivated by the interpretation of HTER as the distance between MT output and the closest conceivable correct translation. Participants in the program noted that techniques developed for adequacy-evaluated tasks do not necessarily carry over to post-editing tasks. This led to the adaptation of traditional MT pipelines to better suit the evaluation objective. For instance, the Z-MERT implementation of minimum error rate training (Zaidan, 2009; Och, 2003) added support for optimizing systems toward TER-BLEU, which is observed to correlate with HTER better than standard BLEU. Versions of the TER-plus and Meteor automatic metrics were also tuned to maximize correlation with HTER (Snover et al., 2009; Denkowski and Lavie, 2010). In the following sections, we discuss the major differences between adequacy and post-editing tasks and illustrate the need for such adaptations. The 2010 ACL Workshop on Statistical Machine Translation (Callison-Burch et al., 2010) featured a post-editing task where human judges were shown MT outputs without source sente"
2012.amta-papers.6,2011.mtsummit-papers.17,0,0.113079,"stediting. The authors find that both TM and MT outputs were selected regularly and in some cases, translators were unable to tell the difference. An automatic classifier that uses features from the MT and TM systems in addition to widely used confidence estimation features shows good performance predicting which output will be preferred. While this work focuses on the case were both human translations (aproximately matched) and automatic translations are available, we focus on the case where only automatic translations are available and directly measure utility rather than preference. Specia (2011) evaluates the effectiveness of using MT confidence estimation methods to predict post-editor effort as measured by HTER, editing time, and editor post-assessments. Post-assessments (editors’ selfratings of how much effort was required to correct MT output) appear to be most predictable across language pairs. Particularly useful confidence estimation features include source and target language model scores for both surface word forms and partof-speech tags. This work bears some similarities to our experiments, though we focus on the challenges of predicting overall MT system utility for future"
2012.amta-papers.6,P02-1040,0,0.0918811,", for which we use HTER as described in Section 3.2. 3 Adequacy Versus Post-editing Utility Large machine translation evaluation campaigns such as the ACL Workshops on Statistical Machine Translation (Callison-Burch et al., 2011) and NIST Open Machine Translation Evaluations (Przybocki, 2009) focus on improving translation adequacy, the perceived quality of fully automatic translations compared to reference translations. As such, current techniques for MT system building, optimization, and evaluation are largely geared toward improving performance on this task. Automatic metrics such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and Meteor (Denkowski and Lavie, 2011), designed to correlate well with adequacy judgments, are often used as stand-ins for actual judgments during optimization and evaluation. 3.1 Adequacy and Ranking Originally introduced by the Linguistics Data Consortium, adequacy ratings elicit straightforward quality judgments of machine translation output according to numeric scales (LDC, 2005). Traditionally, these judgments were split between adequacy, the degree to which MT output captures the meaning of a reference translation, and f luency, the degree to which MT output"
2012.amta-papers.6,W11-2110,0,0.0372634,"Missing"
2012.amta-papers.6,2006.amta-papers.25,0,0.683019,"We find that current automatic metrics under-perform on this task, illustrating the need for further work to develop utility prediction methods better suited for post-editing applications. We encourage the machine translation community to consider these challenges when developing new techniques targeted at improving performance on this increasingly popular task. 2 Related Work While not explicitly focused on post-editing applications, the DARPA GALE program (Olive et al., 2011) included the first major machine translation evaluation campaign to use human-targeted translation edit rate (HTER) (Snover et al., 2006) as the primary evaluation metric. This was motivated by the interpretation of HTER as the distance between MT output and the closest conceivable correct translation. Participants in the program noted that techniques developed for adequacy-evaluated tasks do not necessarily carry over to post-editing tasks. This led to the adaptation of traditional MT pipelines to better suit the evaluation objective. For instance, the Z-MERT implementation of minimum error rate training (Zaidan, 2009; Och, 2003) added support for optimizing systems toward TER-BLEU, which is observed to correlate with HTER bet"
2012.amta-papers.6,W09-0441,0,0.0317293,"sest conceivable correct translation. Participants in the program noted that techniques developed for adequacy-evaluated tasks do not necessarily carry over to post-editing tasks. This led to the adaptation of traditional MT pipelines to better suit the evaluation objective. For instance, the Z-MERT implementation of minimum error rate training (Zaidan, 2009; Och, 2003) added support for optimizing systems toward TER-BLEU, which is observed to correlate with HTER better than standard BLEU. Versions of the TER-plus and Meteor automatic metrics were also tuned to maximize correlation with HTER (Snover et al., 2009; Denkowski and Lavie, 2010). In the following sections, we discuss the major differences between adequacy and post-editing tasks and illustrate the need for such adaptations. The 2010 ACL Workshop on Statistical Machine Translation (Callison-Burch et al., 2010) featured a post-editing task where human judges were shown MT outputs without source sentences or reference translations and asked to edit them based on perceived meaning, if possible. This is different from post-editing tasks in translation workflows that feature bilingual editors working with source sentences and to our knowledge the"
2012.amta-papers.6,2011.eamt-1.12,0,0.022796,"for postediting. The authors find that both TM and MT outputs were selected regularly and in some cases, translators were unable to tell the difference. An automatic classifier that uses features from the MT and TM systems in addition to widely used confidence estimation features shows good performance predicting which output will be preferred. While this work focuses on the case were both human translations (aproximately matched) and automatic translations are available, we focus on the case where only automatic translations are available and directly measure utility rather than preference. Specia (2011) evaluates the effectiveness of using MT confidence estimation methods to predict post-editor effort as measured by HTER, editing time, and editor post-assessments. Post-assessments (editors’ selfratings of how much effort was required to correct MT output) appear to be most predictable across language pairs. Particularly useful confidence estimation features include source and target language model scores for both surface word forms and partof-speech tags. This work bears some similarities to our experiments, though we focus on the challenges of predicting overall MT system utility for future"
2012.amta-papers.6,D08-1076,0,\N,Missing
2012.amta-papers.6,N10-1031,1,\N,Missing
2012.amta-papers.6,W10-1703,0,\N,Missing
2014.amta-wptp.6,2012.amta-papers.6,1,0.946748,"the theoretical and applied perspectives of post-editing process research, it is also relevant to the MT community. The amount of effort post-editors need to exert affects their productivity levels. Accordingly, a good understanding of what features of a machine translation result in higher post-editing effort levels will provide a valuable resource for machine translation researchers as they work to increase the utility of their systems. This is a different, but perhaps more effective focus than the traditional emphasis on improving adequacy compared to gold-standard reference translations (Denkowski and Lavie, 2012a). Types of effort: Krings (2001) made significant early contributions to the study of effort in post-editing. He created a three-way categorization of different types of effort (temporal: time spent; cognitive: mental processin; and technical: physical action) and proposed that the combination of cognitive and technical effort gives rise to temporal effort. However, it is too simplistic to think that the time spent thinking without obvious action plus the time spent on keyboarding and mouse actions is the total time spent on the post-editing task. In particular, post-editors will be thinking"
2014.amta-wptp.6,2012.amta-wptp.2,0,0.478652,"oduct. HTER can be viewed as a measure of required technical effort, rather than a measure of actual technical effort. It is computed as the ratio  ൌ   ǡ   where the number of edits refers to the least number of insertions, deletions, substitutions, and shifts required to convert the MT output to the final post-edited version, and the number of reference words is the number of words in the MT output. When the required technical effort for post-editing is low, HTER is also low, and MT quality is inferred to be high. However, as observed for example by Koponen et al. (2012), HTER is not a perfect measure of actual technical effort exerted by the post editor. HTER measures the shortest route to the final product, but post-editors will often take a route that is not optimal. A simple example is where the post-editor begins to make a change in the MT output, but then reverses course and accepts the MT output without modification. The corresponding HTER will be zero. Nevertheless, the changes begun but then undone by the post-editor certainly constitute non-zero technical effort. Along with this technical effort, the post-editor will also have made cognitive effort"
2014.amta-wptp.6,2012.amta-wptp.3,1,0.903693,"ll also have made cognitive effort through evaluating how to change the MT output and then deciding to abandon the change mid-stream. HTER also fails to fully capture cognitive effort (Koponen, 2012). Pauses and cognitive effort: Overall processing rate is of great concern to businesses and translation professionals, and there are several promising studies that relate this to cognitive effort. See, for example, O’Brien (2011) and Koponen et. al. (2012). However, there are other parameters that also appear to give good insight into levels of cognitive effort during post-editing. Previous work (Lacruz et al., 2012; Lacruz and Shreve, 2014; Lacruz and Muñoz, 2014; Green et al., 2013) has provided evidence that pauses in post-editing are indicators of cognitive effort, just as they are in other types of language production. Indeed, triangulation between keystroke logs and eye tracking data on fixations and gaze 74 duration demonstrate that pauses are associated with cognitive effort in monolingual language production (e.g., Schilperoord, 1996) and in translation and interpreting (e.g. Krings, 2001, Dragsted and Hansen, 2008, Shreve et al., 2011; Timarová et al., 2011). In post-editing, there is evidence"
2014.amta-wptp.6,2014.amta-wptp.6,1,0.0617925,"Post-Editing Isabel Lacruz Institute for Applied Linguistics, Kent State University, Kent OH 44240, U.S.A. ilacruz@kent.edu Michael Denkowski mdenkows@cs.cmu.edu Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, U.S.A. Alon Lavie alavie@cs.cmu.edu Language Technologies Institute, Carnegie Mellon University, Pittsburgh PA 15213, U.S.A. ___________________________________________________________________________________ Abstract The pause to word ratio, the number of pauses per word in a post-edited MT segment, is an indicator of cognitive effort in post-editing (Lacruz and Shreve, 2014). We investigate how low the pause threshold can reasonably be taken, and we propose that 300 ms is a good choice, as pioneered by Schilperoord (1996). We then seek to identify a good measure of the cognitive demand imposed by MT output on the post-editor, as opposed to the cognitive effort actually exerted by the post-editor during post-editing. Measuring cognitive demand is closely related to measuring MT utility, the MT quality as perceived by the post-editor. HTER, an extrinsic edit to word ratio that does not necessarily correspond to actual edits per word performed by the post-editor, is"
2014.amta-wptp.6,2002.eamt-1.11,0,0.929773,"Missing"
2014.amta-wptp.6,P02-1040,0,0.0949064,"quality is highest when the effort required for post-editing carried out by that post-editor is least. Initially, MT quality was measured through subjective human judgments (King, 1996). It is important to note that human judgments are a measure of MT quality that is extrinsic to the post-editing process, since they are not made during the course of the post-editing process. They are the product of reflection and do not necessarily capture the complexities of subconscious processing during post-editing. Subsequently, a variety of automatic metrics - including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and METEOR (Lavie and Denkowski, 2009) – were developed to assess MT quality by measuring how well MT output matches one of a set of reference translations. Versions of these metrics (HTER, HBLEU, and HMETEOR) measure how well MT output matches the single post-edited version produced by an individual post-editor. Snover et al. (2006) report good correlations of the order of .6 between human judgments and each of HTER, HBLEU, and HMETEOR. These metrics are also extrinsic to the post-editing process. They do not measure the steps that were actually carried out by the post-editor. Instead, they"
2014.amta-wptp.6,2006.amta-papers.25,0,0.54541,"y individual post-editor, MT quality is highest when the effort required for post-editing carried out by that post-editor is least. Initially, MT quality was measured through subjective human judgments (King, 1996). It is important to note that human judgments are a measure of MT quality that is extrinsic to the post-editing process, since they are not made during the course of the post-editing process. They are the product of reflection and do not necessarily capture the complexities of subconscious processing during post-editing. Subsequently, a variety of automatic metrics - including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), and METEOR (Lavie and Denkowski, 2009) – were developed to assess MT quality by measuring how well MT output matches one of a set of reference translations. Versions of these metrics (HTER, HBLEU, and HMETEOR) measure how well MT output matches the single post-edited version produced by an individual post-editor. Snover et al. (2006) report good correlations of the order of .6 between human judgments and each of HTER, HBLEU, and HMETEOR. These metrics are also extrinsic to the post-editing process. They do not measure the steps that were actually carried out by"
2014.amta-wptp.6,2009.eamt-1.5,0,0.17244,"Missing"
2014.amta-wptp.6,temnikova-2010-cognitive,0,0.0726829,"t in the middle, place high levels of cognitive demand on the post-editor and so are associated with elevated cognitive effort on the part of the post-editor. Thus, we seek to determine intrinsic measures of MT quality such that increases in MT quality are associated with reductions in cognitive effort in post-editing, as measured by PWR. This agenda was advocated by Lacruz and Muñoz (2014). Drawing on the work of Koponen (2012) and Koponen et al. (2012), they grounded their approach in the analysis of MT errors, categorized according to the linguistically based difficulty ranking proposed by Temnikova (2010) and later modified by Koponen et al. (2012). Temnikova classified MT errors into nine categories assumed to pose increasing cognitive difficulty for the posteditor. These categories are specified in Table 1. Error ranking 1 2 Error Type Correct word, incorrect form Incorrect style synonym 3 4 5 6 7 8 Incorrect word Extra word Missing word Idiomatic expression Wrong punctuation Missing punctuation 9 10 Word order at word level Word order at phrase level Table 1. Temnikova’s MT error classification Lacruz and Muñoz defined a cognitive demand metric for MT segments that they called Mental Load ("
2020.emnlp-main.213,W17-4755,0,0.0468348,"er, we rely on XLM-RoBERTa (base) as our encoder model. Given an input sequence x = [x0 , x1 , ..., xn ], (`) the encoder produces an embedding ej for each token xj and each layer ` ∈ {0, 1, ..., k}. In our framework, we apply this process to the source, MT hypothesis, and reference in order to map them into a shared feature space. 2 2.2 Model Architectures Human judgements of MT quality usually come in the form of segment-level scores, such as DA, MQM and HTER. For DA, it is common practice to convert scores into relative rankings (DARR) when the number of annotations per segment is limited (Bojar et al., 2017b; Ma et al., 2018, 2019). This means that, for two MT hypotheses hi and hj of the same source s, if the DA score assigned to hi is higher than the score assigned to hj , hi is regarded as a “better” hypothesis.2 To encompass these differences, our framework supports two distinct architectures: The Estimator model and the Translation Ranking model. The fundamental difference between them is the training objective. While the Estimator is trained to regress directly on a quality score, the Translation Ranking model is trained to minimize the distance between a “better” hypothesis and both its co"
2020.emnlp-main.213,P18-1031,0,0.0131731,"(details are included in the Appendices). For training, we load the pretrained encoder and initialize both the pooling layer and the feed-forward regressor. Whereas the layer-wise scalars α from the pooling layer are initially set to zero, the weights from the feed-forward are initialized randomly. During training, we divide the model parameters into two groups: the encoder parameters, that include the encoder model and the scalars from α; and the regressor parameters, that include the parameters from the top feed-forward network. We apply gradual unfreezing and discriminative learning rates (Howard and Ruder, 2018), meaning that the encoder model is frozen for one epoch while the feed-forward is optimized with a learning rate of 3e−5. After the first epoch, the entire model is fine-tuned but the learning rate for the encoder parameters is set to 1e−5 in order to avoid catastrophic forgetting. In contrast with the two Estimators, for the C OMET- RANK model we fine-tune from the outset. Furthermore, since this model does not add any new parameters on top of XLM-RoBERTa (base) other than the layer scalars α, we use one single learning rate of 1e−5 for the entire model. 4.2 Evaluation Setup We use the test"
2020.emnlp-main.213,W11-2107,1,0.747216,"(Falcon, 2019), a lightweight PyTorch wrapper, that was created for maximal flexibility and reproducibility. 7 Related Work Classic MT evaluation metrics are commonly characterized as n-gram matching metrics because, using hand-crafted features, they estimate MT quality by counting the number and fraction of ngrams that appear simultaneous in a candidate translation hypothesis and one or more humanreferences. Metrics such as B LEU (Papineni et al., 2002), M ETEOR (Lavie and Denkowski, 2009), and CHR F (Popovi´c, 2015) have been widely studied and improved (Koehn et al., 2007; Popovi´c, 2017; Denkowski and Lavie, 2011; Guo and Hu, 2019), but, by design, they usually fail to recognize and capture semantic similarity beyond the lexical level. In recent years, word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019) have emerged as a commonly used alternative to n-gram matching for capturing word semantics similarity. Embeddingbased metrics like M ETEOR -V ECTOR (Servan et al., 2016), B LEU 2 VEC (T¨attar and Fishel, 2017), Y I S I -1 (Lo, 2019), M OVER S CORE (Zhao et al., 2019), and B ERTSCORE (Zhang et al., 2020) create soft-alignments between reference and"
2020.emnlp-main.213,W19-5406,0,0.0816244,"Missing"
2020.emnlp-main.213,W19-5302,0,0.233484,"rrault et al., 2019). While an increased research interest in neural methods for training MT models and systems has resulted in a recent, dramatic improvement in MT quality, MT evaluation has fallen behind. The MT research community still relies largely on outdated metrics and no new, widely-adopted standard has emerged. In 2019, the WMT News Translation Shared Task received a total of 153 MT system submissions (Barrault et al., 2019). The Metrics Shared Task of the same year saw only 24 submissions, almost half of which were entrants to the Quality Estimation Shared Task, adapted as metrics (Ma et al., 2019). The findings of the above-mentioned task highlight two major challenges to MT evaluation which we seek to address herein (Ma et al., 2019). Namely, that current metrics struggle to accurately correlate with human judgement at segment level and fail to adequately differentiate the highest performing MT systems. In this paper, we present C OMET1 , a PyTorchbased framework for training highly multilingual and adaptable MT evaluation models that can function as metrics. Our framework takes advantage of recent breakthroughs in cross-lingual language modeling (Artetxe and Schwenk, 2019; Devlin et"
2020.emnlp-main.213,P19-1269,0,0.111839,"LEU 2 VEC (T¨attar and Fishel, 2017), Y I S I -1 (Lo, 2019), M OVER S CORE (Zhao et al., 2019), and B ERTSCORE (Zhang et al., 2020) create soft-alignments between reference and hypothesis 6 These will be hosted at: https://github.com/ Unbabel/COMET in an embedding space and then compute a score that reflects the semantic similarity between those segments. However, human judgements such as DA and MQM, capture much more than just semantic similarity, resulting in a correlation upperbound between human judgements and the scores produced by such metrics. Learnable metrics (Shimanaka et al., 2018; Mathur et al., 2019; Shimanaka et al., 2019) attempt to directly optimize the correlation with human judgments, and have recently shown promising results. B LEURT (Sellam et al., 2020), a learnable metric based on BERT (Devlin et al., 2019), claims state-of-the-art performance for the last 3 years of the WMT Metrics Shared task. Because B LEURT builds on top of English-BERT (Devlin et al., 2019), it can only be used when English is the target language which limits its applicability. Also, to the best of our knowledge, all the previously proposed learnable metrics have focused on optimizing DA which, due to a sca"
2020.emnlp-main.213,P02-1040,0,0.110237,"dimensional Quality Metrics. Our models achieve new state-ofthe-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems. 1 Introduction Historically, metrics for evaluating the quality of machine translation (MT) have relied on assessing the similarity between an MT-generated hypothesis and a human-generated reference translation in the target language. Traditional metrics have focused on basic, lexical-level features such as counting the number of matching n-grams between the MT hypothesis and the reference translation. Metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Lavie and Denkowski, 2009) remain popular as a means of evaluating MT systems due to their light-weight and fast computation. Modern neural approaches to MT result in much higher quality of translation that often deviates from monotonic lexical transfer between languages. For this reason, it has become increasingly evident that we can no longer rely on metrics such as B LEU to provide an accurate estimate of the quality of MT (Barrault et al., 2019). While an increased research interest in neural methods for training MT models and systems has resulted in a recent, dramatic improv"
2020.emnlp-main.213,D14-1162,0,0.0863254,"s because, using hand-crafted features, they estimate MT quality by counting the number and fraction of ngrams that appear simultaneous in a candidate translation hypothesis and one or more humanreferences. Metrics such as B LEU (Papineni et al., 2002), M ETEOR (Lavie and Denkowski, 2009), and CHR F (Popovi´c, 2015) have been widely studied and improved (Koehn et al., 2007; Popovi´c, 2017; Denkowski and Lavie, 2011; Guo and Hu, 2019), but, by design, they usually fail to recognize and capture semantic similarity beyond the lexical level. In recent years, word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019) have emerged as a commonly used alternative to n-gram matching for capturing word semantics similarity. Embeddingbased metrics like M ETEOR -V ECTOR (Servan et al., 2016), B LEU 2 VEC (T¨attar and Fishel, 2017), Y I S I -1 (Lo, 2019), M OVER S CORE (Zhao et al., 2019), and B ERTSCORE (Zhang et al., 2020) create soft-alignments between reference and hypothesis 6 These will be hosted at: https://github.com/ Unbabel/COMET in an embedding space and then compute a score that reflects the semantic similarity between those segments. However, human judgement"
2020.emnlp-main.213,N18-1202,0,0.350802,"those segments are excluded from the DARR data. Pooling Layer The embeddings generated by the last layer of the pretrained encoders are usually used for fine-tuning models to new tasks. However, (Tenney et al., 2019) showed that different layers within the network can capture linguistic information that is relevant for different downstream tasks. In the case of MT evaluation, (Zhang et al., 2020) showed that different layers can achieve different levels of correlation and that utilizing only the last layer often results in inferior performance. In this work, we used the approach described in Peters et al. (2018) and pool information from the most important encoder layers into a single embedding for each token, ej , by using a layer-wise attention mechanism. This embedding is then computed as: exj = µEx>j α (1) where µ is a trainable weight coefficient, Ej = (0) (1) (k) [ej , ej , . . . ej ] corresponds to the vector of layer embeddings for token xj , and α = softmax([α(1) , α(2) , . . . , α(k) ]) is a vector corresponding to the layer-wise trainable weights. In order to avoid overfitting to the information contained in any single layer, we used layer dropout (Kondratyuk and Straka, 2019), in which wi"
2020.emnlp-main.213,2020.acl-main.704,0,0.0478671,"eference and hypothesis 6 These will be hosted at: https://github.com/ Unbabel/COMET in an embedding space and then compute a score that reflects the semantic similarity between those segments. However, human judgements such as DA and MQM, capture much more than just semantic similarity, resulting in a correlation upperbound between human judgements and the scores produced by such metrics. Learnable metrics (Shimanaka et al., 2018; Mathur et al., 2019; Shimanaka et al., 2019) attempt to directly optimize the correlation with human judgments, and have recently shown promising results. B LEURT (Sellam et al., 2020), a learnable metric based on BERT (Devlin et al., 2019), claims state-of-the-art performance for the last 3 years of the WMT Metrics Shared task. Because B LEURT builds on top of English-BERT (Devlin et al., 2019), it can only be used when English is the target language which limits its applicability. Also, to the best of our knowledge, all the previously proposed learnable metrics have focused on optimizing DA which, due to a scarcity of annotators, can prove inherently noisy (Ma et al., 2019). Reference-less MT evaluation, also known as Quality Estimation (QE), has historically often regres"
2020.wmt-1.101,2020.lrec-1.455,0,0.0493861,"Missing"
2020.wmt-1.101,W19-5406,0,0.0350777,"Missing"
2020.wmt-1.101,W19-5302,0,0.134698,"verage of the corresponding scores predicted by a given metric. Because the goal of some metrics is to maximize the correlation with human judgements (i.e. B LEU), while for others is to minimize that correlation (i.e. HTER), the value reported is its absolute value. Segment-Level Task At segment-level, we take each of our Estimator models trained to predict MQM, HTER and DA and predict segment-level scores on the DARR data from WMT19. We then generate pairwise rankings based on these predicted scores. For each language pair we apply the formulation of Kendall’s Tau (τ ) from the shared task (Ma et al., 2019) as follows: 6 called micro-average score), where the same is weighted by segment length. To calculate this score at inference time we pass the entire document (divided into segments) through the model as a single batch. This has the added effect of reducing inference time. Robustness to high-permorming systems One important finding from WMT19 is the general deterioration of metrics’ performance when considering only the top n MT systems (Ma et al., 2019). Previously, we showed robustness of our metrics in this scenario in terms of Kendall’s Tau at segmentlevel (Rei et al., 2020). Mathur et al"
2020.wmt-1.101,2020.acl-main.448,0,0.176354,"t al., 2019) as follows: 6 called micro-average score), where the same is weighted by segment length. To calculate this score at inference time we pass the entire document (divided into segments) through the model as a single batch. This has the added effect of reducing inference time. Robustness to high-permorming systems One important finding from WMT19 is the general deterioration of metrics’ performance when considering only the top n MT systems (Ma et al., 2019). Previously, we showed robustness of our metrics in this scenario in terms of Kendall’s Tau at segmentlevel (Rei et al., 2020). Mathur et al. (2020) show that at system-level, Pearson correlation is highly influenced by outliers and that performances for metrics such as B LEU drop significantly when considering only the top systems. To address this, we propose a system-level pairwise comparison measured with the same Kendall’s Tau formulation used for segment-level analysis outlined in section 5 above. By doing this, we are not only better handling possible outliers, but emulating a real world application of these metrics: In most cases (both in academia and industry), we want a metric that can successfully differentiate between two syste"
2020.wmt-1.101,P02-1040,0,0.107326,"than help improve the correlations obtained using only one single high-quality reference. With regard to the Kendall Tau measured at segment-level, by looking at Figure 1 (en-de), we see no significant differences in using the multireference technique. This suggests that having a higher Pearson’s r score does not necessarily guarantee a better Kendall’s Tau. We note that by design, with an approach such as C OMET that is based on a meaning-representation of references, extra references are expected to provide only minor additional value, especially versus lexical-based metrics such as B LEU (Papineni et al., 2002). Whereas the adequacy of the reference(s) 916 is (again by design) expected to have a more significant impact on the performance of the model. Our initial results seem to strongly support this hypothesis. 11 Conclusions In this paper we present C OMET, Unbabel’s contribution to the WMT 2020 Metrics shared task. We leverage the framework outlined in Rei et al. (2020) to demonstrate state-of-the-art or otherwise competitive levels of correlation with human judgements in all tasks and introduce a novel method of making optimal use of alternative references and demonstrate that the quality of the"
2020.wmt-1.101,2020.emnlp-main.213,1,0.478453,"recently proposed C OMET framework: we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art. 1 Introduction In this paper we describe our submission to the WMT20 Metrics shared task. Our work is based on the C OMET1 framework, as presented in Rei et al. (2020), and extended here to evaluation of MT output at segment, document and system-level, forming the basis of our submissions to the corresponding task tracks. Recently, automatic evaluation of MT has followed most other sub-fields in NLP with a notable interest in leveraging the power of large, pre-trained language models. Metrics such as B ERT R EGRESSOR (Shimanaka et al., 2019), B ERTSCORE (Zhang et al., 2020), B LEURT (Sellam et al., 2020) and our more recent C OMET (Rei et al., 2020), all build upon developments in language modelling to generate automatic metrics with high correlation with h"
2020.wmt-1.101,2020.acl-main.704,0,0.169908,"the-art. 1 Introduction In this paper we describe our submission to the WMT20 Metrics shared task. Our work is based on the C OMET1 framework, as presented in Rei et al. (2020), and extended here to evaluation of MT output at segment, document and system-level, forming the basis of our submissions to the corresponding task tracks. Recently, automatic evaluation of MT has followed most other sub-fields in NLP with a notable interest in leveraging the power of large, pre-trained language models. Metrics such as B ERT R EGRESSOR (Shimanaka et al., 2019), B ERTSCORE (Zhang et al., 2020), B LEURT (Sellam et al., 2020) and our more recent C OMET (Rei et al., 2020), all build upon developments in language modelling to generate automatic metrics with high correlation with human judgement. Our 1 Crosslingual Optimized Metric for Evaluation of Translation hosted at: https://github.com/ Unbabel/COMET MT evaluation models follow a similar strategy, specifically utilizing the most recent iterations of the XLM-RoBERTa model presented in Conneau et al. (2020). The uniqueness of our approach comes from our inclusion of the source text as input which was demonstrated in Takahashi et al. (2020) and Rei et al. (2020) to"
2020.wmt-1.101,2006.amta-papers.25,0,0.637362,"te MT systems. In our contribution to the shared task, we introduce two varieties of models built on the C OMET framework that are extensions of the models evaluated in Rei et al. (2020). 3 3.1 C OMET Models Estimator Models Our Estimators generally follow the architecture proposed in Rei et al. (2020), that is to say we encode segment-level representations using XLMRoBERTa and pass these outputs through a feedforward regressor. As in Rei et al. (2020), we train three versions of this basic estimator model against different types of human judgement; Humanmediated Translation Edit Rate (HTER) (Snover et al., 2006), a proprietary implementation of Multidimensional Quality Metric (MQM) (Lommel et al., 2014) and (in-line with the present task) Direct Assessments (DA) (Graham et al., 2013). The hyper-parameters used for these models are exactly as described in Rei et al. (2020), excluding the following alterations: we use XLM-RoBERTa large instead of base and we increase the feed-forward hidden sizes (from 2304 in the first layer and 1152 in the second to 3072 and 1536 hidden units, respectively). We also keep the embedding layer frozen and apply a layer-wise learning rate decay (as proposed in Howard and"
2020.wmt-1.101,2020.acl-main.327,0,0.0211955,"ang et al., 2020), B LEURT (Sellam et al., 2020) and our more recent C OMET (Rei et al., 2020), all build upon developments in language modelling to generate automatic metrics with high correlation with human judgement. Our 1 Crosslingual Optimized Metric for Evaluation of Translation hosted at: https://github.com/ Unbabel/COMET MT evaluation models follow a similar strategy, specifically utilizing the most recent iterations of the XLM-RoBERTa model presented in Conneau et al. (2020). The uniqueness of our approach comes from our inclusion of the source text as input which was demonstrated in Takahashi et al. (2020) and Rei et al. (2020) to be beneficial to the model. In our contribution to the shared task, we demonstrate methods of further exploiting information in the source text as well as a technique to fully harness the power of pre-trained language models to further improve the prediction accuracy of our evaluation framework when more than one reference translation is available. For the shared task, we utilize two primary types of models built using the C OMET framework, namely; the Estimator models, which regress directly on human scores of MT quality such as Direct Assessment; and the C OMET- RAN"
2020.wmt-1.101,2020.emnlp-main.8,0,0.0858278,"ontributions: 1. We introduce a method for handling multiple references at inference time and for optimizing the utility of information from all available text inputs 2. We propose a simple technique for calculating a document-level score from a weighted average of segment-level scores We demonstrate that our C OMET framework trained models achieve state-of-the-art results or are competitive on all settings introduced in the WMT19 Metrics shared task, outperforming, in some cases, more recently proposed metrics such as B ERTSCORE (Zhang et al., 2020), B LEURT (Sellam et al., 2020) and P RISM (Thompson and Post, 2020). 911 Proceedings of the 5th Conference on Machine Translation (WMT), pages 911–920 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2 The C OMET Framework As outlined in Rei et al. (2020), the C OMET framework allows for training of specialized evaluation metrics that correlate well with different types of human-generated quality scores. The general structure of the framework consists of a cross-lingual encoder that produces a series of token-level vector embeddings for source, hypothesis and reference inputs, a pooling layer which converts the various token-leve"
2021.acl-demo.9,2020.eamt-1.29,0,0.0159885,"0.5154 on the isolated sub-sample. In Table 1 we illustrate an example of a translation in which the Online-G system produces critical errors as a consequence of translating named entities incorrectly, specifically isolated by the DCF feature. 3.2 DCF: Terminology Similarly to named entities, enforcing that MT systems use specific terminology during translation is a challenging task with particular relevance in commercial use cases. Measuring terminology adherence typically involves relying on automated metrics for MT as well as measuring the accuracy of terminology output (Dinu et al., 2019; Exel et al., 2020). This approach presents two concrete problems: a) applying terminology constraints typically results in only minimal variance between translations, which limits the utility of using automated metrics at the corpus level; and b) measuring accuracy in terminology usage typically relies on exact string matching between a translation hypothesis and its respective reference, which implies that properly inflected translated terms often do not receive proper credit. MT-T ELESCOPE offers a DCF Terminology feature which allows a user to optionally upload a glossary by which to isolate a corresponding"
2021.acl-demo.9,aziz-etal-2012-pet,0,0.0337118,"mparative analysis of segment-level errors with highlighting of variant n-grams. The tool also provides some limited aggregate analysis. Both of the above tools also offer statistical significance testing in the form of a bootstrapped t-test. V IZ S EQ (Wang et al., 2019), whilst only tangentially related, is one of the only comparative tools that offers a web-based interface. Moreover, V IZ S EQ has impressive coverage in terms of Natural Language Generation metrics. However, V IZ S EQ was developed for multi-model comparison and is primarily focused at corpus-level. Other tools such as PET (Aziz et al., 2012) and A P PRAISE (Federmann, 2012) are complementary to MT-T ELESCOPE in that they offer features which leverage annotation and post-edition. DCF: Duplication The removal of duplicates can be particularly important in situations where the test corpus sample contains repetition. Repeated segments in a test sample can artificially inflate the corpus-level score, particularly where that score results from an average of segment-level scores. Whilst we acknowledge that removal of duplicate segments is fairly common in public data sets such as that used in the WMT Shared Tasks and consequently our ex"
2021.acl-demo.9,W17-3204,0,0.0148807,"ynamic Corpus Filtering feature (DCF) updates the output evaluation in real-time to allow the user to ‘zoom in’ on relevant data points. Currently, MT-T ELESCOPE supports filtering by named entity, glossary and source segment length, as well as an option to remove duplicates. Whenever any of these options is selected, the interface will output the size of the sub-sample as a percentage of the original test corpus. 3.1 DCF: Named Entities Successful rendering of named entities is a known challenge for even modern MT systems and can lead to distortion of locations, organization and other names (Koehn and Knowles, 2017; Modrzejewski et al., 2020). Recently, several methods have been proposed to improve the translation 76 Table 1: Example of named entity errors produced Online-G system in comparison to the PROMT system from the WMT20 shared task. C OMET Source Online-G PROMT Reference Маругов врезался на мотоцикле в такси, которым управлял Акбаров. Murugov crashed into a motorcycle taxi, which was ruled by Akbar. Marugov crashed into a taxi driven by Akbarov on a motorcycle. Marugov crashed on a motorcyle into the taxi Akbarov was driving. of named entities in Neural Machine Translation (NMT) (Sennrich and H"
2021.acl-demo.9,2020.eamt-1.24,0,0.0270101,"visualizations are dynamic, interactive and highly customizable. The tools have been built specifically with ease of use in mind, in the hope of expanding access to high quality MT evaluation. There is tremendous scope in the adaptation of the DCF framework to target many other phenomena and future work will be focused primarily in this area. We envisage for example adding filters for specific discourse phenomenon such as pronoun translation. Ideally such filter would allow researchers to measure context usage in NMT without having to rely only on contrastive evaluation (M¨uller et al., 2018; Lopes et al., 2020) and/or human evaluation. 78 We also plan to extend MT-T ELESCOPE to handle a (possibly empty) set of references. This will bring more flexibility to the tool allowing more informed decision when multiple references are available while also supporting Quality Estimation (Specia et al., 2018) when references are not available. Finally we hope to implement exporting functionality to allow saving of analysis output in commonly used formats (e.g. json and PDF). Given that MTT ELESCOPE is an open source platform, we are excited to encourage other users to contribute to its growth with suggestions a"
2021.acl-demo.9,W05-0909,1,0.364415,"stical significance as a means of evaluating the rigor of the resulting system ranking. MT-T ELESCOPE is open source1 , written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality. 1 Introduction When developing MT systems or comparing experiments across papers, it has been common practice for researchers and developers to rely on automated metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) as a means of quantifying the relative performance difference between two models. Commercial deployment of systems and the establishment of state-of-theart in academia is often driven by these metrics alone. Automated metrics have long been an essential means for assessing quality improvements 1 Code available at: https://github.com/ Unbabel/MT-Telescope and Demo video at: https://youtu.be/MZOe1yX8mII 73 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Sys"
2021.acl-demo.9,2020.wmt-1.77,0,0.0173901,"ely accessible framework that requires little technical skill to operate and exposes information about the critical differences between MT outputs that is interactive, informative and highly customizable. 2 MT-T ELESCOPE is opened in a web browser and takes four text (.txt) files as input; source and reference segments and one set of MT outputs for each of the compared systems. Users drag and drop these files directly onto the interface to begin evaluation. C OMET (Rei et al., 2020a) is provided as a default metric given its proven value in the WMT Metrics Shared Task 2020 (Rei et al., 2020b; Mathur et al., 2020). Optionally the user can choose an alternate metric using a selection box. Currently available metrics include B LEU, M ETEOR and CHR F, and a selection of more recently proposed metrics such as P RISM, B LEURT, and B ERT S CORE. 2.2 Visualizations High-level results of the analysis are output in table format with the corresponding system scores. MT-T ELESCOPE then exposes segment-level comparison in three primary visualizations: First, a bubble plot (Figure 1) where the position of bubbles show how scores between the two systems differ for each segment, notable differences being highlighted"
2021.acl-demo.9,2020.eamt-1.6,0,0.0327428,"Missing"
2021.acl-demo.9,P19-1294,0,0.0157087,"erformance -0.1799 0.5154 on the isolated sub-sample. In Table 1 we illustrate an example of a translation in which the Online-G system produces critical errors as a consequence of translating named entities incorrectly, specifically isolated by the DCF feature. 3.2 DCF: Terminology Similarly to named entities, enforcing that MT systems use specific terminology during translation is a challenging task with particular relevance in commercial use cases. Measuring terminology adherence typically involves relying on automated metrics for MT as well as measuring the accuracy of terminology output (Dinu et al., 2019; Exel et al., 2020). This approach presents two concrete problems: a) applying terminology constraints typically results in only minimal variance between translations, which limits the utility of using automated metrics at the corpus level; and b) measuring accuracy in terminology usage typically relies on exact string matching between a translation hypothesis and its respective reference, which implies that properly inflected translated terms often do not receive proper credit. MT-T ELESCOPE offers a DCF Terminology feature which allows a user to optionally upload a glossary by which to isol"
2021.acl-demo.9,2020.emnlp-main.8,0,0.0328337,"Missing"
2021.acl-demo.9,W18-6307,0,0.0607191,"Missing"
2021.acl-demo.9,C18-1274,0,0.0342668,"Missing"
2021.acl-demo.9,N19-4007,0,0.0921556,"inition of ‘improvement’ as an increase in a relevant corpus-level score is insufficient, especially when the relative difference between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of MT systems, such as C OMPARE -MT (Neubig et al., 2019) and MTC OMPAR E VAL (Klejch et al., 2015) and other more general comparative tools such as V IZ S EQ (Wang et al., 2019). Despite the intention of such tools in addressing the above problem, none have been widely adopted as a standard method of evaluating MT. MT-T ELESCOPE was specifically developed to leverage the best of existing approaches in a manner that is as user friendly as possible, with features specifically tailored to the MT use case. The platform supports fine-grained segment-level analysis and interactive visualisations that provide relevant and informative quality intelligence."
2021.acl-demo.9,D19-3043,0,0.0823159,"erence between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of MT systems, such as C OMPARE -MT (Neubig et al., 2019) and MTC OMPAR E VAL (Klejch et al., 2015) and other more general comparative tools such as V IZ S EQ (Wang et al., 2019). Despite the intention of such tools in addressing the above problem, none have been widely adopted as a standard method of evaluating MT. MT-T ELESCOPE was specifically developed to leverage the best of existing approaches in a manner that is as user friendly as possible, with features specifically tailored to the MT use case. The platform supports fine-grained segment-level analysis and interactive visualisations that provide relevant and informative quality intelligence. In particular, the platform also supports focused analWe present MT-T ELESCOPE, a visualization platform designed to fac"
2021.acl-demo.9,P02-1040,0,0.110593,"ides a bootstrapped t-test for statistical significance as a means of evaluating the rigor of the resulting system ranking. MT-T ELESCOPE is open source1 , written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality. 1 Introduction When developing MT systems or comparing experiments across papers, it has been common practice for researchers and developers to rely on automated metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) as a means of quantifying the relative performance difference between two models. Commercial deployment of systems and the establishment of state-of-theart in academia is often driven by these metrics alone. Automated metrics have long been an essential means for assessing quality improvements 1 Code available at: https://github.com/ Unbabel/MT-Telescope and Demo video at: https://youtu.be/MZOe1yX8mII 73 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confere"
2021.acl-demo.9,2020.acl-demos.14,0,0.0211026,"Missing"
2021.acl-demo.9,2020.emnlp-main.213,1,0.903593,"babel.com luisa.coheur@inesc-id.pt Abstract and driving progress in the field of MT. Recent state-of-the-art metrics such as C OMET (Rei et al., 2020a), P RISM (Thompson and Post, 2020), and B LEURT (Sellam et al., 2020), show much higher levels of correlation with human judgement than their predecessors. Notwithstanding the strength of available metrics, when applied and reported at corpus-level, they are only able to provide a general indication of whether one system is superior, based on a single score which in some cases is limited to an arithmetic mean of segment-level score predictions (Rei et al., 2020a). We contend that the broad definition of ‘improvement’ as an increase in a relevant corpus-level score is insufficient, especially when the relative difference between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of M"
2021.acl-demo.9,2020.wmt-1.101,1,0.867216,"babel.com luisa.coheur@inesc-id.pt Abstract and driving progress in the field of MT. Recent state-of-the-art metrics such as C OMET (Rei et al., 2020a), P RISM (Thompson and Post, 2020), and B LEURT (Sellam et al., 2020), show much higher levels of correlation with human judgement than their predecessors. Notwithstanding the strength of available metrics, when applied and reported at corpus-level, they are only able to provide a general indication of whether one system is superior, based on a single score which in some cases is limited to an arithmetic mean of segment-level score predictions (Rei et al., 2020a). We contend that the broad definition of ‘improvement’ as an increase in a relevant corpus-level score is insufficient, especially when the relative difference between high-performing MT systems is negligible. Exposure of the changing distribution of performance at segment-level on targeted phenomena is fundamental to our understanding of translation quality. Manual inspection at this level is often too time-consuming and inefficient to be done rigorously and on a regular basis. MT-T ELESCOPE was inspired by other recent work on developing holistic approaches for finegrained comparison of M"
2021.acl-demo.9,2020.acl-main.704,0,0.0231012,"Missing"
2021.acl-demo.9,W16-2209,0,0.026699,"Missing"
C96-1075,P94-1045,1,0.888768,"Missing"
C96-1075,P95-1005,1,0.822394,"Missing"
C96-1075,J87-1004,0,0.258485,"signed to be language-independent in the sense that they 442 S • . • iSource n Language &gt; J I OU ~ rI "" FGenKit enerator [ . . I s °oo. Figure 1: T h e J A N U S expressions, and incorporates the sentence into a discourse plan tree. &apos;the discourse processor also updates a calendar which keeps track of what the speakers haw&apos;~ said about their schedules. The discourse processor is described in greater detail else.where (R,osd et 31. 1995). 3 The QLR Translation Module The (]LR.* parser (Lavie and Tomita 11993; I,avie 1994) is a parsing system based on Tomita&apos;s Generalized LI~ parsing algorithm (Tomita 1987). The parser skips parts of the utterance that it cannot incorporate into a well-formed sentence structure. Thus it is well-suited to doinains ill which nongrammaticality is c o a l i t i o n . T h e parser conducts a search for the maximal subset of the original input that is covered by the grammar. This is done using a beam search heuristic that limits tile combinations of skipped words considered by the parser, and ensures that it operates within feasible time and space bonnds. The GI,R* parser was implemented as an extension to the G LR parsing system, a unificationI&gt;ased practical natural"
C96-1075,C90-1012,0,\N,Missing
carbonell-etal-2002-automatic,2001.mtsummit-road.7,1,\N,Missing
clark-lavie-2010-loonybin,C08-5001,0,\N,Missing
clark-lavie-2010-loonybin,W06-3119,0,\N,Missing
clark-lavie-2010-loonybin,W01-1812,0,\N,Missing
D12-1107,D07-1090,0,0.0471064,"o and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007). Whether to use one smoothing technique or the other then becomes largely an issue of training costs and quality after quantization. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N w"
D12-1107,W11-2103,1,0.847704,"k Y k−1 k−1 k rules) and the order in which rules are tried during p(w1 )p(wk |w1 ) b(wj ) cube pruning. j=f 1173 3.3 Combined Scheme Our two language model modifications can be trivially combined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998)"
D12-1107,J07-2003,0,0.528898,"ability and backoff. We will show that the probability and backoff values in a language model can be collapsed into a single value for each n-gram without changing sentence probability. This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse. Specifically, the rest cost pessimistically assumes that the model will back off to unigrams immediately following the sentence fragment. The two modifications can be used independently or simultaneously. To measure the impact of their different rest costs, we experiment with cube pruning (Chiang, 2007) in syntactic machine transla2 Other smoothing techniques, including Witten-Bell (Witten and Bell, 1991), do not make this assumption. 1170 tion. Cube pruning’s goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop l"
D12-1107,P10-4002,0,0.0701019,"Missing"
D12-1107,W06-3113,0,0.0321054,"rnal to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007)."
D12-1107,D10-1026,0,0.0733215,"h passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams whic"
D12-1107,2011.iwslt-evaluation.24,1,0.682728,"are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N ways back off to w1n−1 or fewer words4 . This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N . The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N -gram language model 3. Backoff b from the N -gram language model where c(w1n ) is the number of times w1n appears in the training dat"
D12-1107,W11-2123,1,0.86198,"options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and b"
D12-1107,P07-1019,0,0.0445105,"oring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa. 2 Related Work Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and"
D12-1107,P07-2045,1,0.0206737,"ete sentence is held constant. We first show how to improve rest cost quality over standard practice by using additional space. Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1 . Language models are designed to assign probability to sentences. However, approximate search algorithms use estimates for sentence fragments. If the language model has order N (an N -gram model), then the first N − 1 words of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment w1k is Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lowerorder entries in an N -gram model to score the"
D12-1107,2005.mtsummit-papers.11,1,0.0635177,"fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Fea"
D12-1107,W08-0402,0,0.841283,"tion. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N ways back off to w1n−1 or fewer words4 . This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N . The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N -gram language"
D12-1107,W09-0424,0,0.0571311,"Missing"
D12-1107,P03-1021,0,0.00735297,"999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Rest Costs as Prediction Scoring the first few words of a sentence fragment is a prediction task. The goal is to predict what the probability will be when more context becomes known. In order to measure performance on this task, we ran the decoder on the hierarchical system with a pop limit of 1000. Every time more context became known, we logged5 the prediction e"
D12-1107,P02-1040,0,0.0836066,"cause lower variance means cube pruning’s relative rankings are more accurate. Our lower-order rest costs are better across the board in terms of absolute bias, mean squared error, and variance. 4.2 Pop Limit Trade-Offs The cube pruning pop limit is a trade-off between search accuracy and CPU time. Here, we measure how our rest costs improve (or degrade) that trade-off. Search accuracy is measured by the average model score of single-best translations. Model scores are scale-invariant and include a large constant factor; higher is better. We also measure overall performance with uncased BLEU (Papineni et al., 2002). CPU time is the sum of user and system time used by Moses divided by the number of sentences (3003). Timing includes time to load, though files were forced into the disk cache in advance. Our test machine has 64 GB of RAM and 32 cores. Results are shown in Figures 3 and 4. Lower-order rest costs perform better in both systems, reaching plateau model scores and BLEU with less CPU time. The gain is much larger for tarPop 2 10 50 500 700 Baseline Model BLEU -105.56 20.45 -104.74 21.13 -104.31 21.36 -104.25 21.33 -104.25 21.34 CPU 3.29 5.21 23.30 54.61 64.08 Lower Order CPU Model BLEU 3.68 -105."
D12-1107,P07-1065,0,0.0615394,"re could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per"
D12-1107,2008.iwslt-papers.8,0,0.0205576,"ng, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned sole"
D12-1107,J03-4003,0,\N,Missing
D15-1284,W09-2004,0,0.0176664,"cation performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorou"
D15-1284,P11-2016,0,0.0936674,"ics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they"
D15-1284,N15-1070,1,0.765615,"ing distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5 https://code.google.com/p/word2vec/ We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. 2369 6 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense"
D15-1284,P12-2030,0,0.0360941,"stances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies"
D15-1284,H05-1067,0,0.36465,"d phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-gra"
D15-1284,P15-1070,0,0.0619138,"ncongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5 , we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5 https://code.google.com/p/word2vec/ We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. 2369 6 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whol"
D15-1284,W06-1625,0,0.213259,"n for Computational Linguistics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognitio"
D15-1284,N12-2012,0,0.0706261,"First, a universal definition of humor is hard to achieve, because different people hold different understandings of even the same sentence. Second, humor is always situated in a broader context that sometimes requires a lot of external knowledge to fully understand it. For example, consider the sentence, “The one who invented the door knocker got a No Bell prize” and “Veni, Vidi, Visa: I came, I saw, I did a little shopping”. One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor (Raz, 2012), such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them. Although it is impossible to understand universal humor characteristics, one can still capture the possible latent structures behind humor (Bucaria, 2004; Binsted and Ritchie, 1997). In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambiguity, phonetic style and personal a"
D15-1284,W05-1614,0,0.109392,"e high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devot"
D15-1284,P05-3029,0,0.0413941,"r positive instances will have high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and g"
D15-1284,N03-1033,0,0.0277565,"occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {w1 , w2 · · · wk } via WordNet and Qkcalculate the sense combinations as log( i=1 nwi ). nwi is the total number of senses of word wi . • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, hu"
D15-1284,P06-1134,0,0.0105404,"ds {w1 , w2 · · · wk } via WordNet and Qkcalculate the sense combinations as log( i=1 nwi ). nwi is the total number of senses of word wi . • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive)"
D15-1284,H05-1044,0,0.031023,"tility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words. 7 Path Similarity: http://www.nltk.org/howto/ wordnet.html • Weak (Strong) Subjectivity: the number of occurrences of all Weak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people’s opinions, evaluations, beliefs or speculations. 4.4 Phonetic Style Many humorous texts play with sounds, creating incongruous sound"
D15-1284,P12-1074,0,\N,Missing
E14-1042,W12-3102,0,0.0382064,"Missing"
E14-1042,2012.eamt-1.60,0,0.015108,"Missing"
E14-1042,J07-2003,0,0.0847015,"Missing"
E14-1042,P11-2031,1,0.0803537,"Missing"
E14-1042,P10-4002,1,0.708845,"Missing"
E14-1042,N13-1073,1,0.315558,"Missing"
E14-1042,W12-3160,0,0.196093,"Missing"
E14-1042,2009.mtsummit-btm.7,0,0.0406991,"Missing"
E14-1042,2010.amta-papers.21,0,0.748015,"Missing"
E14-1042,P13-2121,0,0.016193,"Missing"
E14-1042,D11-1125,0,0.0158403,"Missing"
E14-1042,N10-1062,0,0.501773,"Missing"
E14-1042,C08-1064,0,0.0808059,"Missing"
E14-1042,D08-1076,0,0.0144994,"Missing"
E14-1042,W04-3225,0,0.568137,"Missing"
E14-1042,C88-2101,0,0.104491,"Missing"
E14-1042,P03-1021,0,0.0557394,"Missing"
E14-1042,2012.amta-papers.14,0,0.558928,"Missing"
E14-1042,2013.mtsummit-papers.24,0,0.521722,"Missing"
E14-1042,2009.mtsummit-posters.20,0,0.0815556,"Missing"
E14-1042,P06-1124,0,0.0644723,"Missing"
E14-1042,C04-1059,0,0.022975,"Missing"
E14-1042,2012.amta-wptp.10,0,0.107696,"Missing"
E14-1042,N10-1079,0,0.418105,"Missing"
E14-1042,P02-1040,0,0.0936261,"Missing"
H01-1007,W00-0203,1,0.70336,"reside locally on the PC of the customer. This implies a server-type architecture in which speech recognition and translation are accomplished via interaction with a dedicated server. The extent to which this server is centralized or distributed is one of the major design considerations taken into account in our system. 2. NESPOLE! INTERLINGUA-BASED TRANSLATION APPROACH Our translation approach builds upon previous work that we have conducted within the context of the C-STAR consortium. We use an interlingua-based approach with a relatively shallow task-oriented interlingua representation [2] [1], that was initially designed for the C-STAR consortium and has been significantly extended for the NESPOLE! project. Interlingual machine translation is convenient when more than two languages are involved because it does not require each language to be connected by a set of transfer rules to each other language in each direction [3]. Adding a new language that has all-ways translation with existing languages requires only writing one analyzer that maps utterances into the interlingua and one generator that maps interlingua representations into sentences. The interlingua approach also allows"
H01-1018,W00-0308,0,0.139736,"Missing"
H01-1018,W00-0203,1,0.842507,"Missing"
H01-1018,woszczcyna-etal-1998-modular,1,0.742909,"Missing"
H01-1018,H01-1003,1,\N,Missing
H05-1093,2001.mtsummit-papers.3,0,0.174157,"on subsequences (also known as skip-ngrams). Common skip-ngrams are sequences of words in their sentence order that are found both in the reference and candidate translations. By generalizing and separating the overlap statistics from the function used to combine them, and by identifying the latter as a learnable component, B LANC subsumes the ngram based evaluation metrics as special cases and can better reflect the need of end applications for adequacy/fluency tradeoffs . 1.1 Related Work Initial work in evaluating translation quality focused on edit distance-based metrics (Su et al., 1992; Akiba et al., 2001). In the MT context, edit distance (Levenshtein, 1965) represents the amount of word insertions, deletions and substitutions necessary to transform a candidate translation into a reference translation. Another evaluation metric based on edit distance is the Word Error Rate (Niessen et al., 2000) which computes the normalized edit distance. B LEU is a weighted precision evaluation metric introduced by IBM (Papineni et al., 2001). B LEU and its extensions/variants (e.g. N IST (Doddington, 2002)) have become de-facto standards in the MT community and are consistently being used for system optimiz"
H05-1093,2003.mtsummit-papers.10,0,0.0928898,"ipngrams in polynomial time. We show that the B LEU and ROUGE metric families are special cases of B LANC, and we compare correlations with human judgments across these three metric families. We analyze the algorithmic complexity of ACS and argue that it is more powerful in modeling both local meaning and sentence-level structure, while offering the same practicality as the established algorithms it generalizes. 1 Introduction Although recent MT evaluation methods show promising correlations to human judgments in terms of adequacy and fluency, there is still considerable room for improvement (Culy and Riehemann, 2003). Most of these studies have been performed at a system level and have not investigated metric robustness at a lower granularity. Moreover, even though the emphasis on adequacy vs. fluency is applicationdependent, automatic evaluation metrics do not distinguish between the need to optimize correlation with regard to one or the other. Machine translation automatic evaluation metrics face two important challenges: the lack of powerful features to capture both sentence level structure and local meaning, and the difficulty of designing good functions for combining these features into meaningful qu"
H05-1093,P04-1077,0,0.249844,"it distance. B LEU is a weighted precision evaluation metric introduced by IBM (Papineni et al., 2001). B LEU and its extensions/variants (e.g. N IST (Doddington, 2002)) have become de-facto standards in the MT community and are consistently being used for system optimization and tuning. These methods rely on local features and do not explicitly capture sentence-level features, although implicitly longer n-gram matches are rewarded in B LEU. The General Text Matcher (G TM) (Turian et al., 2003) is another MT evaluation method that rewards longer ngrams instead of assigning them equal weight. (Lin and Och, 2004) recently proposed a set of metrics (ROUGE) for MT evaluation. ROUGE -L is a longest common subsequence (LCS) based automatic evaluation metric for MT. The intuition behind it is that long common subsequences reflect a large overlap between a candidate translation and a reference translation. ROUGE -W is also based on LCS, but assigns higher weights to sequences that have fewer gaps. However, these metrics still do not distinguish 741 among translations with the same LCS but different number of shorter sized subsequences, also indicative of overlap. ROUGE -S attempts to correct this problem by"
H05-1093,niessen-etal-2000-evaluation,0,0.0752544,"atter as a learnable component, B LANC subsumes the ngram based evaluation metrics as special cases and can better reflect the need of end applications for adequacy/fluency tradeoffs . 1.1 Related Work Initial work in evaluating translation quality focused on edit distance-based metrics (Su et al., 1992; Akiba et al., 2001). In the MT context, edit distance (Levenshtein, 1965) represents the amount of word insertions, deletions and substitutions necessary to transform a candidate translation into a reference translation. Another evaluation metric based on edit distance is the Word Error Rate (Niessen et al., 2000) which computes the normalized edit distance. B LEU is a weighted precision evaluation metric introduced by IBM (Papineni et al., 2001). B LEU and its extensions/variants (e.g. N IST (Doddington, 2002)) have become de-facto standards in the MT community and are consistently being used for system optimization and tuning. These methods rely on local features and do not explicitly capture sentence-level features, although implicitly longer n-gram matches are rewarded in B LEU. The General Text Matcher (G TM) (Turian et al., 2003) is another MT evaluation method that rewards longer ngrams instead"
H05-1093,2001.mtsummit-papers.68,0,0.0150489,"end applications for adequacy/fluency tradeoffs . 1.1 Related Work Initial work in evaluating translation quality focused on edit distance-based metrics (Su et al., 1992; Akiba et al., 2001). In the MT context, edit distance (Levenshtein, 1965) represents the amount of word insertions, deletions and substitutions necessary to transform a candidate translation into a reference translation. Another evaluation metric based on edit distance is the Word Error Rate (Niessen et al., 2000) which computes the normalized edit distance. B LEU is a weighted precision evaluation metric introduced by IBM (Papineni et al., 2001). B LEU and its extensions/variants (e.g. N IST (Doddington, 2002)) have become de-facto standards in the MT community and are consistently being used for system optimization and tuning. These methods rely on local features and do not explicitly capture sentence-level features, although implicitly longer n-gram matches are rewarded in B LEU. The General Text Matcher (G TM) (Turian et al., 2003) is another MT evaluation method that rewards longer ngrams instead of assigning them equal weight. (Lin and Och, 2004) recently proposed a set of metrics (ROUGE) for MT evaluation. ROUGE -L is a longest"
H05-1093,P04-1078,0,0.0177819,"th traditional and recent automatic evaluation metrics. We also describe the parameter conditions under which B LANC can emulate them. Throughout the remainder of this paper, we distinguish between two components of automatic MT evaluation: the statistics computed on candidate and reference translations and the function used in defining evaluation metrics and generating translation scores. Commonly used statistics include bagof-words overlap, edit distance, longest common subsequence, ngram overlap, and skip-bigram overlap. Preferred functions are various combinations of precision and recall (Soricut and Brill, 2004), including 1 Since existing evaluation metrics (e.g. B LEU, ROUGE ) are special cases of our metric family, it is only natural to name it Broad Learning and Adaptation for Numeric Criteria (B LANC) – white light contains light of all frequencies 740 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 740–747, Vancouver, October 2005. 2005 Association for Computational Linguistics weighted precision and F-measures (Van-Rijsbergen, 1979). B LANC implements a practical algorithm with learnable parameters for"
H05-1093,C92-2067,0,0.347296,"ghted sum of common subsequences (also known as skip-ngrams). Common skip-ngrams are sequences of words in their sentence order that are found both in the reference and candidate translations. By generalizing and separating the overlap statistics from the function used to combine them, and by identifying the latter as a learnable component, B LANC subsumes the ngram based evaluation metrics as special cases and can better reflect the need of end applications for adequacy/fluency tradeoffs . 1.1 Related Work Initial work in evaluating translation quality focused on edit distance-based metrics (Su et al., 1992; Akiba et al., 2001). In the MT context, edit distance (Levenshtein, 1965) represents the amount of word insertions, deletions and substitutions necessary to transform a candidate translation into a reference translation. Another evaluation metric based on edit distance is the Word Error Rate (Niessen et al., 2000) which computes the normalized edit distance. B LEU is a weighted precision evaluation metric introduced by IBM (Papineni et al., 2001). B LEU and its extensions/variants (e.g. N IST (Doddington, 2002)) have become de-facto standards in the MT community and are consistently being us"
H05-1093,2003.mtsummit-papers.51,0,0.420135,"r evaluation metric based on edit distance is the Word Error Rate (Niessen et al., 2000) which computes the normalized edit distance. B LEU is a weighted precision evaluation metric introduced by IBM (Papineni et al., 2001). B LEU and its extensions/variants (e.g. N IST (Doddington, 2002)) have become de-facto standards in the MT community and are consistently being used for system optimization and tuning. These methods rely on local features and do not explicitly capture sentence-level features, although implicitly longer n-gram matches are rewarded in B LEU. The General Text Matcher (G TM) (Turian et al., 2003) is another MT evaluation method that rewards longer ngrams instead of assigning them equal weight. (Lin and Och, 2004) recently proposed a set of metrics (ROUGE) for MT evaluation. ROUGE -L is a longest common subsequence (LCS) based automatic evaluation metric for MT. The intuition behind it is that long common subsequences reflect a large overlap between a candidate translation and a reference translation. ROUGE -W is also based on LCS, but assigns higher weights to sequences that have fewer gaps. However, these metrics still do not distinguish 741 among translations with the same LCS but d"
H05-1093,P02-1040,0,\N,Missing
lavie-etal-2002-nespole,W00-0203,1,\N,Missing
lavie-etal-2002-nespole,costantini-etal-2002-nespole,1,\N,Missing
lavie-etal-2002-nespole,H01-1007,1,\N,Missing
lavie-etal-2004-significance,niessen-etal-2000-evaluation,0,\N,Missing
lavie-etal-2004-significance,N03-2021,0,\N,Missing
lavie-etal-2004-significance,P02-1040,0,\N,Missing
lavie-etal-2004-significance,C92-2067,0,\N,Missing
lavie-etal-2004-significance,N03-1024,0,\N,Missing
lavie-etal-2004-significance,N03-1020,0,\N,Missing
lavie-etal-2004-significance,2003.mtsummit-papers.9,0,\N,Missing
lavie-etal-2004-significance,2001.mtsummit-papers.3,0,\N,Missing
levin-etal-2000-lessons,P99-1073,0,\N,Missing
levin-etal-2000-lessons,W00-0203,1,\N,Missing
levin-etal-2000-lessons,P97-1035,0,\N,Missing
monson-etal-2004-data,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2004-data,carbonell-etal-2002-automatic,1,\N,Missing
monson-etal-2008-linguistic,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2008-linguistic,2004.tmi-1.1,1,\N,Missing
monson-etal-2008-linguistic,N06-2002,1,\N,Missing
monson-etal-2008-linguistic,J01-2001,0,\N,Missing
monson-etal-2008-linguistic,W07-1315,1,\N,Missing
N03-4015,W02-0717,1,\N,Missing
N03-4015,lavie-etal-2002-nespole,1,\N,Missing
N06-2033,P05-1022,0,0.227474,"Missing"
N06-2033,C96-1058,0,0.551723,"ach of the individual parsers. This is done in a two stage process of reparsing. In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure. In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage. Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set. 2 Dependency Reparsing In dependency reparsing we focus on unlabeled dependencies, as described by Eisner (1996). In this scheme, the syntactic structure for a sentence with n words is a dependency tree representing head-dependent relations between pairs of words. When m parsers each output a set of dependencies (forming m dependency structures) for a given sentence containing n words, the dependencies can be combined in a simple wordby-word voting scheme, where each parser votes for the head of each of the n words in the sentence, and the head with most votes is assigned to each word. This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does"
N06-2033,P04-1013,0,0.0284939,"Missing"
N06-2033,W99-0623,0,0.801181,"ll is accomplished by discarding every constituent with weight below a threshold t before the search for the final parse tree starts. In the simple case where each constituent starts out with weight 1.0 (before any merging), this means that a constituent is only considered for inclusion in the final parse tree if it appears in at least t of the m initial parse trees. Intuitively, this should increase precision, since we expect that a constituent that appears in the output of more parsers to be more likely to be correct. By changing the threshold t we can control the precision/recall tradeoff. Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees. The latter scheme performed better, producing remarkable results despite its simplicity. The combination is done with a simple majority vote of whether or not constituents should appear in the combined tree. In other words, if a constituent appears at least (m + 1)/2 times in the output of the m parsers, the constituent is added to the final tree. This simple vote resulted in trees with f-score significantly higher"
N06-2033,J93-2004,0,0.0308004,"lon University Pittsburgh, PA 15213 {sagae,alavie@cs.cmu.edu} Abstract We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers. 1 Introduction Over the past decade, remarkable progress has been made in data-driven parsing. Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993). In fact, years of extensive research on training and testing parsers on the Wall Street Journal (WSJ) corpus of the Penn Treebank have resulted in the availability of several high-accuracy parsers. We present a framework for combining the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers. This is done in a two stage process of reparsing. In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure. In the second stage, a parsing algorithm is applied to the original sentence,"
N06-2033,H05-1066,0,0.578386,"ge should be created, the corresponding weights are simply added. As long as at least one of the m initial structures is a well-formed dependency structure, the directed graph created this way will be connected. 1 Determining the weights is discussed in section 4.1. 129 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 129–132, c New York, June 2006. 2006 Association for Computational Linguistics Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005). Finding the optimal dependency structure given the set of weighted dependencies is simply a matter of finding the maximum spanning tree (MST) for the directed weighted graph, which can be done using the Chu-Liu/Edmonds directed MST algorithm (Chu & Liu, 1965; Edmonds, 1967). The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree. If projectivity (no crossing branches) is desired, Eisner’s (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead. 3 Constituent Reparsing In constitu"
N06-2033,C04-1010,0,0.0313552,"vote resulted in trees with f-score significantly higher than the one of the best parser in the combination. However, the scheme heavily favors precision over recall. Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score). 4 Experiments In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn Treebank using the same head-table as Yamada and Matsumoto (2003), using sections 02-21 as training data and section 23 as test data, following (McDonald et al., 2005; Nivre & Scholz, 2004; Yamada & Matsumoto, 2003). Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data. For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research. 4.1 Dependency Reparsing Experiments Six dependency parsers were used in our combination experiments, as described below. The deterministic shift-reduce parsing algorithm of (Nivre & Scholz, 2004) was used to create two parsers2, one that processes the input sentence from left-to-right (LR), and on"
N06-2033,W05-1513,1,0.871599,"Missing"
N06-2033,W03-3023,0,0.427944,"stituent appears at least (m + 1)/2 times in the output of the m parsers, the constituent is added to the final tree. This simple vote resulted in trees with f-score significantly higher than the one of the best parser in the combination. However, the scheme heavily favors precision over recall. Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score). 4 Experiments In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn Treebank using the same head-table as Yamada and Matsumoto (2003), using sections 02-21 as training data and section 23 as test data, following (McDonald et al., 2005; Nivre & Scholz, 2004; Yamada & Matsumoto, 2003). Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data. For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research. 4.1 Dependency Reparsing Experiments Six dependency parsers were used in our combination experiments, as described below. The deterministic shift-reduce parsing algorithm of (N"
N06-2033,W05-1518,0,0.435934,"s. When m parsers each output a set of dependencies (forming m dependency structures) for a given sentence containing n words, the dependencies can be combined in a simple wordby-word voting scheme, where each parser votes for the head of each of the n words in the sentence, and the head with most votes is assigned to each word. This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does not guarantee that the final voted set of dependencies will be a well-formed dependency tree. In fact, the resulting graph may not even be connected. Zeman & Žabokrtský (2005) apply this dependency voting scheme to Czech with very strong results. However, when the constraint that structures must be well-formed is enforced, the accuracy of their results drops sharply. Instead, if we reparse the sentence based on the output of the m parsers, we can maximize the number of votes for a well-formed dependency structure. Once we have obtained the m initial dependency structures to be combined, the first step is to build a graph where each word in the sentence is a node. We then create weighted directed edges between the nodes corresponding to words for which dependencies"
N10-1031,W08-0312,1,0.83021,"on with the HTER scores. This produced globally optimal α, β, and γ values for M ETEOR and optimal α, β, γ values plus stem, synonym, and paraphrase match weights for 252 Task Adequacy & Fluency Ranking HTER HTER (extended) α 0.81 0.95 0.70 0.65 Stem 0 β 0.83 0.50 1.95 1.95 Syn 0.4 γ 0.28 0.50 0.50 0.45 Par 0.9 Table 1: Parameter values for various M ETEOR tasks for translations into English. M ETEOR - NEXT (with the weight of exact matches fixed at 1). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). As observed by Snover et al. (2009), HTER prefers metrics which are more balanced between precision and recall: this results in the lowest values of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segm"
N10-1031,W05-0909,1,0.218261,"umantargeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic post-editing based metric which measures the distance between MT output and a targeted reference. We identify several limitations of the original M ETEOR metric and describe our modifications to improve performance on this task. Our extended metric, M ETEOR - NEXT, is The M ETEOR - NEXT Metric Traditional M ETEOR Scoring Given a machine translation hypothesis and a reference translation, the traditional M ETEOR metric calculates a lexical similarity score based on a wordto-word alignment between the two strings (Banerjee and Lavie, 2005). When multiple references are available, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are built incrementally in a series of stages using the following M ETEOR matchers: Exact: Words are matched if and only if their surface forms are identical. Stem: Words are stemmed using a languageappropriate Snowball Stemmer (Porter, 2001) and matched if the stems are identical. Synonym: Words are matched if they are both members of a synonym set according to the WordNet (Miller and Fellbaum, 2007) database. This matcher is limited to translations"
N10-1031,P05-1074,0,0.106573,"calculated: Fmean = P ·R α · P + (1 − α) · R To account for differences in word order, the minimum number of “chunks” (ch) is calculated where a chunk is defined as a series of matched unigrams that is contiguous and identically ordered in both sentences. The fragmentation (f rag = ch/m) is then used to calculate a fragmentation penalty: P en = γ · f rag β The final M ETEOR score is then calculated: words) if one phrase is considered a paraphrase of the other by a paraphrase database. For English, we use the paraphrase database developed by Snover et al. (2009), using techniques presented by Bannard and Callison-Burch (2005). The extended aligner first constructs a search space by applying all matchers in sequence to identify all possible matches between the hypothesis and reference. To reduce redundant matches, stem and synonym matches between pairs of words which have already been identified as exact matches are not considered. Matches have start positions and lengths in both sentences; a word occurring less than length positions after a match start is said to be covered by the match. As exact, stem, and synonym matches will always have length one in both sentences, they can be considered phrase matches of leng"
N10-1031,W07-0734,1,0.850822,"can be considered phrase matches of length one. Since other matches can cover phrases of different lengths in the two sentences, matches are now said to be one-to-one at the phrase level rather than the word level. Once all possible matches have been identified, the aligner identifies the final alignment as the largest subset of these matches meeting the following criteria in order of importance: 1. Each word in each sentence is covered by zero or one matches Score = (1 − P en) · Fmean The free parameters α, β, and γ can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). 2.2 Extending the M ETEOR Aligner Traditional M ETEOR is limited to unigram matches, making it strictly a word-level metric. By focusing on only one match type per stage, the aligner misses a significant part of the possible alignment space. Further, selecting partial alignments based only on the fewest number of per-stage crossing alignment links can in practice lead to missing full alignments with the same number of matches in fewer chunks. Our extended aligner addresses these limitations by introducing support for multiple-word phrase matches and considering all possible matches in a sing"
N10-1031,P02-1040,0,0.112724,"s of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segments from 195 documents in this data set. Our evaluation metric (M ETEOR - NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al., 2002) with a maximum N gram length of 4, TER (Snover et al., 2006), versions of M ETEOR based on release 0.7 tuned for adequacy and fluency (M ETEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (M ETEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (M ETEOR-0.7-hter). Also included is the HTER-tuned version of TER-plus (TERp-hter), a metric with state-of-the-art performance in recent evaluations (Snover et al., 2009). Length-weighted Pearson’s and Spearman’s correlation are shown for all metrics at both the segment (Table 2) and document level (Table 3). System level correlations are not shown as the"
N10-1031,2006.amta-papers.25,0,0.826113,"of M ETEOR, and approaches the correlation level of a state-of-theart metric, TER-plus (TERp). 1 2 2.1 Introduction Recent focus on the need for accurate automatic metrics for evaluating the quality of machine translation output has spurred much development in the field of MT. Workshops such as WMT09 (CallisonBurch et al., 2009) and the MetricsMATR08 challenge (Przybocki et al., 2008) encourage the development of new MT metrics and reliable human judgment tasks. This paper describes our work extending the M E TEOR metric to improve correlation with humantargeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic post-editing based metric which measures the distance between MT output and a targeted reference. We identify several limitations of the original M ETEOR metric and describe our modifications to improve performance on this task. Our extended metric, M ETEOR - NEXT, is The M ETEOR - NEXT Metric Traditional M ETEOR Scoring Given a machine translation hypothesis and a reference translation, the traditional M ETEOR metric calculates a lexical similarity score based on a wordto-word alignment between the two strings (Banerjee and Lavie, 2005). When multiple references are availab"
N10-1031,W09-0441,0,0.386669,"monic mean of P and R (van Rijsbergen, 1979) is then calculated: Fmean = P ·R α · P + (1 − α) · R To account for differences in word order, the minimum number of “chunks” (ch) is calculated where a chunk is defined as a series of matched unigrams that is contiguous and identically ordered in both sentences. The fragmentation (f rag = ch/m) is then used to calculate a fragmentation penalty: P en = γ · f rag β The final M ETEOR score is then calculated: words) if one phrase is considered a paraphrase of the other by a paraphrase database. For English, we use the paraphrase database developed by Snover et al. (2009), using techniques presented by Bannard and Callison-Burch (2005). The extended aligner first constructs a search space by applying all matchers in sequence to identify all possible matches between the hypothesis and reference. To reduce redundant matches, stem and synonym matches between pairs of words which have already been identified as exact matches are not considered. Matches have start positions and lengths in both sentences; a word occurring less than length positions after a match start is said to be covered by the match. As exact, stem, and synonym matches will always have length one"
N10-1031,W09-0401,0,\N,Missing
N13-1029,2009.mtsummit-posters.2,1,0.847152,"which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to exact constituent matches only. However, the gains in coverage come with a corresponding increase in computational and modeling complexity due to the larger label set involved. Derivational ambiguity — the condition of having multiple derivations for the same output string — is a particular problem for parsing-based MT system"
N13-1029,J12-2006,0,0.0214992,"future work. 2 2.1 Background Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to exact constituent matches only. However, the gains in coverage come with a corresponding increase in computational and modeling complexity due to the larger label set involved. Derivational ambiguity — the condition"
N13-1029,J07-2003,0,0.327541,"while reducing the occurrence of sparsity and ambiguity problems common to large label sets. 1 Introduction The formulation of statistical machine translation in terms of synchronous parsing has become both theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-g"
N13-1029,P10-1146,0,0.798335,"en describe our method of label collapsing in SAMT grammars in Section 3. Experimental results are presented in Section 4 and analyzed in Section 5. Finally, Section 6 offers some conclusions and avenues for future work. 2 2.1 Background Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories allow for the extraction of a larger number of rules, increasing coverage and translation performance over systems that are limited to ex"
N13-1029,P11-2031,1,0.831635,"a set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We report these results in the tables as well for three MERT runs and a p-value of 0.05. Systems that were judged statistically different from the SAMT baseline have triangles in the appropriate “Sig. SAMT?” columns; systems judged different from the Hiero baseline have triangles under the “Sig. Hiero?” columns. An up-triangle (N) indicates that the system was better, while a down-triangle (O) means that the baseline was better. 1 https://github.com/jhclark/multeval System"
N13-1029,W11-2107,1,0.866519,"grammar with a single nonterminal. Each of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We report these results in the tables a"
N13-1029,P10-4002,0,0.0381544,"ithm, while we strip out the source-side labels to create the baseline SAMT grammar with 4181 unique target-side labels. Table 1 summarizes how the number of target labels, unique extracted rules, and the average number of pruned rules available per sentence change as the initial grammar is label-collapsed to three progressively coarser degrees. Once the collapsing process has occurred exhaustively, the original SAMT grammar becomes a Hiero-format grammar with a single nonterminal. Each of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown"
N13-1029,N04-1035,0,0.125294,"s of synchronous parsing has become both theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data spars"
N13-1029,W11-1011,1,0.912606,"rained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Syntax-Augmented MT formalism (Zollmann and Venugopal, 2006), a monolingually labeled version of Hiero that can create up to 4000 “extended” category labels based on pairs of parse nodes. We take a standard SAMT grammar with target-side labels and extend its labeling to a bilingual format (Zollmann, 2011). We then coarsen the bilingual labels following the “label collapsing” algorithm of Hanneman and Lavie (2011). This represents a novel extension of the tree-to-tree collapsing algorithm to the SAMT formalism. After removing the source-side labels, we obtain a new SAMT grammar with coarser target-side labels than the original. Coarsened grammars provide improvement of up to 1.14 BLEU points over the baseline SAMT results on two Chinese–English test sets; they also outperform a Hiero baseline by up to 0.60 BLEU on one of the sets. Aside from improved translation quality, in analysis we find significant reductions in derivational ambiguity and rule sparsity, two problems that make large nonterminal sets"
N13-1029,N06-1031,0,0.0365252,"addressing those problems. The section also summarizes the tree-to-tree label collapsing algorithm and the process of SAMT rule extraction. We then describe our method of label collapsing in SAMT grammars in Section 3. Experimental results are presented in Section 4 and analyzed in Section 5. Finally, Section 6 offers some conclusions and avenues for future work. 2 2.1 Background Working with Large Label Sets Aside from the SAMT method of grammar extraction, which we treat more fully in Section 2.3, several other lines of work have explored increasing the nonterminal set for syntax-based MT. Huang and Knight (2006), for example, augmented the standard Penn Treebank labels for English by adding lexicalization to certain types of nodes. Chiang (2010) and Zollmann (2011) worked with a bilingual extension of SAMT that used its notion of “extended categories” on both the source and target sides. Taking standard monolingual SAMT as a baseline, Baker et al. (2012) developed a tagger to augment syntactic labels with some semantically derived information. Ambati et al. (2009) extracted tree-to-tree rules with similar extensions for sibling nodes, resulting again in a large number of labels. Extended categories a"
N13-1029,D10-1014,0,0.347468,"Missing"
N13-1029,W08-0411,1,0.831881,"h theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Sy"
N13-1029,P06-1077,0,0.0869141,"ing has become both theoretically and practically successful. In a parsingbased MT formalism, synchronous context-free grammar rules that match a source-language input can be hierarchically composed to produce a corresponding target-language output. SCFG translation grammars can be extracted automatically from data. While formally syntactic approaches with a single grammar nonterminal have often worked well (Chiang, 2007), the desire to exploit linguistic knowledge has motivated the use of translation grammars with richer, linguistically syntactic nonterminal inventories (Galley et al., 2004; Liu et al., 2006; Lavie et al., 2008; Liu et al., 2009). Linguistically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper"
N13-1029,P09-1063,0,0.0487157,"Missing"
N13-1029,J03-1002,0,0.00475553,"ly observed target right-hand sides for each source right-hand side. A second pass globally removes hierarchical rules that were extracted fewer than six times in the training data. 4 Experiments We conduct experiments on Chinese-to-English MT, using systems trained from the FBIS corpus of approximately 302,000 parallel sentence pairs. We parse both sides of the training data with the Berkeley parsers (Petrov and Klein, 2007) for Chinese and English. The English side is lowercased after parsing; the Chinese side is segmented beforehand. Unidirectional word alignments are obtained with GIZA++ (Och and Ney, 2003) and symmetrized, resulting in a parallel parsed corpus with Viterbi word alignments for each sentence pair. Our modified version of Thrax takes the parsed and aligned corpus as input and returns a list of rules, which can then be label-collapsed and scored as previously described. In Thrax, we retain most of the default settings for Hiero- and SAMT-style grammars as specified in the extractor’s configuration file. Inheriting from Hiero, we require the right-hand side of all rules to contain at least one pair of aligned terminals, no more than two nonterminals, and no more than five terminals"
N13-1029,P02-1040,0,0.0868735,"T grammar becomes a Hiero-format grammar with a single nonterminal. Each of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We"
N13-1029,N07-1051,0,0.0585181,"umber of target-side labels, unique rules in the whole grammar, and average number of pruned rules after filtering to individual sentences. size. One pruning pass keeps only the 80 most frequently observed target right-hand sides for each source right-hand side. A second pass globally removes hierarchical rules that were extracted fewer than six times in the training data. 4 Experiments We conduct experiments on Chinese-to-English MT, using systems trained from the FBIS corpus of approximately 302,000 parallel sentence pairs. We parse both sides of the training data with the Berkeley parsers (Petrov and Klein, 2007) for Chinese and English. The English side is lowercased after parsing; the Chinese side is segmented beforehand. Unidirectional word alignments are obtained with GIZA++ (Och and Ney, 2003) and symmetrized, resulting in a parallel parsed corpus with Viterbi word alignments for each sentence pair. Our modified version of Thrax takes the parsed and aligned corpus as input and returns a list of rules, which can then be label-collapsed and scored as previously described. In Thrax, we retain most of the default settings for Hiero- and SAMT-style grammars as specified in the extractor’s configuratio"
N13-1029,2006.amta-papers.25,0,0.0676805,"ach of the five grammars in Table 1 is used to build an MT system. All systems are tuned and decoded with cdec (Dyer et al., 2010), an open-source decoder for SCFG-based MT with arbitrary rule formats and nonterminal labels. We tune the systems on the 1664-sentence NIST Open MT 2006 data set, optimizing towards the BLEU metric. Our test sets are the NIST 2003 data set of 919 sentences and the NIST 2008 data set of 1357 sentences. The tuning set and both test sets all have four English references. We evaluate systems on BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), as calculated in all three cases by MultEval version 0.5.0.1 These scores for the MT ’03 test set are shown in Table 2, and those for the MT ’08 test set in Table 3, combined by MultEval over three optimization runs on the tuning set. MultEval also implements statistical significance testing between systems based on multiple optimizer runs and approximate randomization. This process (Clark et al., 2011) randomly swaps outputs between systems and estimates the probability that the observed score difference arose by chance. We report these results in the tables as well for three MERT runs and"
N13-1029,N09-1027,0,0.197442,"t match — has been variously identified as “data sparsity,” the “matching constraint,” and “rule sparsity” in the grammar. It arises from the definition of SCFG rule application: in order to compose two rules, the left-hand-side label of the smaller rule must match a right-hand-side label in the larger rule it is being plugged in to. With large label sets, it becomes less likely that two arbitrarily chosen rules can compose, making the grammar less flexible for representing new sentences. Previous research has attempted to address both of these problems in different ways. Preference grammars (Venugopal et al., 2009) are a technique for reducing derivational ambiguity by summing scores over labeled variants of the same derivation during decoding. Chiang (2010) addressed rule sparsity by introducing a soft matching constraint: the decoder may pay a learned label-pair-specific penalty for substituting a rule headed by one label into a substitution slot marked for another. Combining properties of both of the above methods, Huang et al. (2010) modeled monolingual labels as distributions over latent syntactic categories and calculated similarity scores between them for rule composition. 2.2 Label Collapsing in"
N13-1029,W11-2160,0,0.0123171,"lingual labels, together with the frequency count for each rule, using the modified version of Thrax described in Section 2.3. The rules can be grouped according to the target-side label of their left-hand sides (Figure 3(a)). The rule counts are then used to compute labeling probabilities P (s |t) and P (t |s) over left-handside usages of each source label s and each target label t. These are simple maximum-likelihood estimates: if #(si , tj ) represents the combined frequency counts of all rules with si ::tj on the left-hand We implement bilingual SAMT grammar extraction by modifying Thrax (Weese et al., 2011), an open-source, Hadoop-based framework for extracting standard SAMT grammars. By default, Thrax can produce grammars labeled either on the source or target side, but not both. It also outputs rules that are already scored according to a user-specified 291 3 Label Collapsing in SAMT Rules (a) (b) (c) (d) Figure 3: Stages of preparing label-collapsed rules for SAMT grammars. (a) SAMT rules with bilingual nonterminals are extracted and collected based on their target left-hand sides. (b) Probabiliites P (t |s) and P (s |s) are computed. (c) Nonterminals are clustered according to the label coll"
N13-1029,W06-3119,0,0.556988,"ically syntactic MT systems can derive their label sets, either monolingually or bilingually, from parallel corpora that have been annotated with source- and/or target-side parse trees provided by a statistical parser. The MT system may exactly adopt the parser’s label set or modify it in some way. Larger label sets are able to represent more precise, fine-grained categories. On the other hand, they also exacerbate a number of computational and modeling problems by increasing grammar size, derivational ambiguity, and data sparsity. In this paper, we focus on the Syntax-Augmented MT formalism (Zollmann and Venugopal, 2006), a monolingually labeled version of Hiero that can create up to 4000 “extended” category labels based on pairs of parse nodes. We take a standard SAMT grammar with target-side labels and extend its labeling to a bilingual format (Zollmann, 2011). We then coarsen the bilingual labels following the “label collapsing” algorithm of Hanneman and Lavie (2011). This represents a novel extension of the tree-to-tree collapsing algorithm to the SAMT formalism. After removing the source-side labels, we obtain a new SAMT grammar with coarser target-side labels than the original. Coarsened grammars provid"
N13-1116,D07-1090,0,0.0402154,"by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces"
N13-1116,W11-2103,1,0.819339,"or the beam is full. After the loop terminates, the beam is given to the root node of the state tree; other nodes will be built lazily as described in §3.2. Overall, the algorithm visits hypergraph vertices in bottom-up order. Our beam filling algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (O"
N13-1116,W12-3102,1,0.129996,"Missing"
N13-1116,D12-1103,0,0.0589301,"e same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language model uses this pointer to immediately retrieve denominator r(wn |win−1 ) and as a starting point to retrieve numerator r(wn |w1n−1 ). It can therefore avoid looking 3 We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. 963 n−1 up r(wn ), r(wn |wn−1 ), . . . , r(wn |wi+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes are leaves, then the p"
N13-1116,P05-1033,0,0.0725644,"vity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model"
N13-1116,J07-2003,0,0.343299,"this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previous paragraph are special cases of a directed acyclic hypergraph. As used here, the difference from a normal graph is that an edge can go from one vertex to any number of vertices; this number is the arity of th"
N13-1116,P10-4002,0,0.169629,"unt for new language model context. Each edge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its descendants no longer c"
N13-1116,2010.iwslt-papers.8,0,0.14625,"entry then pushing multiple entries. However, our queue entries are a group of hypotheses while cube pruning’s entries are a single hypothesis. Hypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation system"
N13-1116,2011.iwslt-evaluation.24,1,0.848843,"’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the few a  ` Korea) and therefore includes estimate log r(the)r(few |the). Estimates are updated as words are revealed. Continuing the example, “is (  )[0+ ] .” has best child “is (the  Korea)[0+ ] .” In this best child, the estimate r(.) is updated to r(. |Korea). Similarly, r(the) is replaced with r(the |is). Updates examine only words that have been revealed: r(few |the) remains unrevised. Updates are computed efficiently by using pointers (Heafield et al., 2011) with KenLM. To summarize, the language model computes r(wn |w1n−1 ) r(wn |win−1 ) in a single call. In the popular reverse trie data structure, the language model visits win while retrieving w1n , so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language mo"
N13-1116,D12-1107,1,0.862517,"score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its descendants no longer contribute to the maximum. The score of partial edge “is (  )[0+ ] .” is the sum of scores from its two parts: edge “is v .” and bread crumb (  )[0+ ]. The edge’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the"
N13-1116,2009.iwslt-papers.4,1,0.374035,"e is updated to account for new language model context. Each edge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its des"
N13-1116,D09-1007,0,0.0138539,"ypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then"
N13-1116,P07-1019,0,0.154769,"algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just"
N13-1116,D10-1027,0,0.0793645,"racy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous verti"
N13-1116,P12-1064,0,0.0293356,", so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language model uses this pointer to immediately retrieve denominator r(wn |win−1 ) and as a starting point to retrieve numerator r(wn |w1n−1 ). It can therefore avoid looking 3 We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. 963 n−1 up r(wn ), r(wn |wn−1 ), . . . , r(wn |wi+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes"
N13-1116,D11-1127,0,0.00569847,"Missing"
N13-1116,W01-1812,0,0.0533662,"nes groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and"
N13-1116,W12-3139,1,0.1598,"d suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previou"
N13-1116,2005.mtsummit-papers.11,1,0.0198184,"ing algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all -1"
N13-1116,W08-0402,0,0.739541,"can be expressed as weights on edges that sum to form hypothesis features. However, log probability from an N –gram language model is non958 Proceedings of NAACL-HLT 2013, pages 958–968, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics additive because it examines surface strings across edge and vertex boundaries. Non-additivity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a"
N13-1116,P08-1023,0,0.0325037,"iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypothes"
N13-1116,D09-1078,0,0.0191413,"al and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears some similarity to our algorithm in that partially overlapping state will be collapsed and efficiently handled together. However, the key advatage to our approach is that groups have a score that can be used for pruning before the group is expanded, enabling pruning without first constructing the intersected automaton. 2.5 Coarse-to-Fine Coarse-to-fine (Petrov et al., 2008) performs multiple pruning passes, each time with more detail. Search is a subroutine of coarse-to-fine and our work is inside search, so the two are compatible. There are several for"
N13-1116,P03-1021,0,0.0146071,"). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all -101.4 Average model score Average model score -101.4 -101.5 -101.6 This work Additive cube pruning Cube pruning 0 -101.5 -101.6 1 2 CPU seconds/sentence This work Gesmundo 1 Gesmundo 2 Cube pruning 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap."
N13-1116,P02-1040,0,0.114857,"dec 1.56 to 2.24 times as fast as the best baseline. At first, this seems to suggest that cdec is faster. In fact, the opposite is true: comparing Figures 5 and 6 reveals that cdec has a higher parsing cost than Moses5 , thereby biasing the speed ratio towards 1. In subsequent experiments, we use Moses because it more accurately reflects search costs. 4.3 Average-Case Rest Costs Previous experiments used the common-practice probability estimate described in §3.5. Figure 7 shows the impact of average-case rest costs on our algorithm and on cube pruning in Moses. We also looked at uncased BLEU (Papineni et al., 2002) scores, finding that our algorithm attains near-peak BLEU in less time. The relationship between model score and BLEU is noisy due to model errors. 4 The glue rule builds hypotheses left-to-right. In Moses, glued hypotheses start with <s> and thus have empty left state. In cdec, sentence boundary tokens are normally added last, so intermediate hypotheses have spurious left state. Running cdec with the Moses glue rule led to improved time-accuracy performance. The improved version is used in all results reported. We accounted for constant-factor differences in feature definition i.e. whether <"
N13-1116,D08-1012,0,0.480051,"Missing"
N13-1116,P11-1008,0,0.00997303,"admissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then be recovered by taking the dual and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears so"
N13-1116,W96-0108,0,0.0624739,"ast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The"
N13-1116,P06-1098,0,0.0721756,"improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the be"
N13-1116,J03-4003,0,\N,Missing
P05-1025,briscoe-carroll-2002-robust,0,0.0433703,"Missing"
P05-1025,C02-1013,0,0.025729,"Missing"
P05-1025,A00-2018,0,0.0971946,"ian MacWhinney Department of Psychology Carnegie Mellon University Pittsburgh, PA 15232 macw@cmu.edu Kenji Sagae and Alon Lavie Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15232 {sagae,alavie}@cs.cmu.edu Abstract To facilitate the use of syntactic information in the study of child language acquisition, a coding scheme for Grammatical Relations (GRs) in transcripts of parent-child dialogs has been proposed by Sagae, MacWhinney and Lavie (2004). We discuss the use of current NLP techniques to produce the GRs in this annotation scheme. By using a statistical parser (Charniak, 2000) and memorybased learning tools for classification (Daelemans et al., 2004), we obtain high precision and recall of several GRs. We demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the Index of Productive Syntax (Scarborough, 1990) at similar levels to what child language researchers compute manually. 1 Introduction Automatic syntactic analysis of natural language has benefited greatly from statistical and corpus-based approaches in the past decade. The availability of syntactically annotated data has fueled the development of high"
P05-1025,P96-1025,0,0.0209002,"nlabeled Dependency Identification Once we have isolated the text that should be analyzed in each sentence, we parse it to obtain unlabeled dependencies. Although we ultimately need labeled dependencies, our choice to produce unlabeled structures first (and label them in a later step) is motivated by available resources. Unlabeled dependencies can be readily obtained by processing constituent trees, such as those in the Penn Treebank (Marcus et al., 1993), with a set of rules to determine the lexical heads of constituents. This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree. This dependency extraction procedure from constituent trees gives us a straightforward way to obtain unlabeled dependencies: use an existing statistical parser (Charniak, 2000) trained on the Penn Treebank to produce constituent trees, and extract unlabeled dependencies using the aforementioned head-finding rules. Our target data (transcribed child language) is 199 from a very different domain than the one of the data used to train the statistical parser (the Wall Street Journal section of the Penn Treebank), but the degradation in the parser’s accuracy is acce"
P05-1025,P02-1017,0,0.0116555,"ter obtaining unlabeled dependencies as described above, we proceed to label those dependencies with the GR labels listed in Figure 2. Determining the labels of dependencies is in general an easier task than finding unlabeled dependencies in text.3 Using a classifier, we can choose one of the 30 possible GR labels for each dependency, given a set of features derived from the dependencies. Although we need manually labeled data to train the classifier for labeling dependencies, the size of this training set is far smaller than what would be necessary to train a parser to find labeled dependen3 Klein and Manning (2002) offer an informal argument that constituent labels are much more easily separable in multidimensional space than constituents/distituents. The same argument applies to dependencies and their labels. cies in one pass. We use a corpus of about 5,000 words with manually labeled dependencies to train TiMBL (Daelemans et al., 2003), a memory-based learner (set to use the k-nn algorithm with k=1, and gain ratio weighing), to classify each dependency with a GR label. We extract the following features for each dependency: • The head and dependent words; • The head and dependent parts-of-speech; • Whe"
P05-1025,J93-2004,0,0.0240471,"art-of-speech tagger (Parisse and Le Normand, 2000). This results in fairly clean sentences, accompanied by full morphological and part-of-speech analyses. 3.2 Unlabeled Dependency Identification Once we have isolated the text that should be analyzed in each sentence, we parse it to obtain unlabeled dependencies. Although we ultimately need labeled dependencies, our choice to produce unlabeled structures first (and label them in a later step) is motivated by available resources. Unlabeled dependencies can be readily obtained by processing constituent trees, such as those in the Penn Treebank (Marcus et al., 1993), with a set of rules to determine the lexical heads of constituents. This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree. This dependency extraction procedure from constituent trees gives us a straightforward way to obtain unlabeled dependencies: use an existing statistical parser (Charniak, 2000) trained on the Penn Treebank to produce constituent trees, and extract unlabeled dependencies using the aforementioned head-finding rules. Our target data (transcribed child language) is 199 from a very different domain than the one of"
P05-1025,C04-1010,0,0.0232112,"Missing"
P05-1025,sagae-etal-2004-adding,1,0.937275,"technologies. Similarly, in the study of child language, the availability of large amounts of electronically accessible empirical data in the form of child language transcripts has been shifting much of the research effort towards a corpus-based mentality. However, child language researchers have only recently begun to utilize modern NLP techniques for syntactic analysis. Although it is now common for researchers to rely on automatic morphosyntactic analyses of transcripts to obtain part-of-speech and morphological analyses, their use of syntactic parsing is rare. Sagae, MacWhinney and Lavie (2004) have proposed a syntactic annotation scheme for the CHILDES database (MacWhinney, 2000), which contains hundreds of megabytes of transcript data and has been used in over 1,500 studies in child language acquisition and developmental language disorders. This annotation scheme focuses on syntactic structures of particular importance in the study of child language. In this paper, we describe the use of existing NLP tools to parse child language transcripts and produce automatically annotated data in the format of the scheme of Sagae et al. We also validate the usefulness of the annotation scheme"
P05-1025,P95-1037,0,\N,Missing
P05-3026,A94-1016,0,0.903254,"Missing"
P05-3026,2004.eamt-1.14,1,0.864016,"Missing"
P05-3026,lavie-etal-2004-significance,1,0.138924,"Missing"
P05-3026,P02-1040,0,0.113475,"Missing"
P05-3026,C00-2122,0,0.473653,"ning these systems into an MT system that carries many of the advantages of the individual systems and suffers from few of their disadvantages. Attempts at combining the output of different systems have proved useful in other areas of language technologies, such as the ROVER approach for speech recognition (Fiscus 1997). Several approaches to multi-engine machine translation systems have been proposed over the past decade. The Pangloss system and work by several other researchers attempted to combine lattices from many different MT systems (Frederking et Nirenburg 1994, Frederking et al 1997; Tidhar & Küssner 2000; Lavie, Probst et al. 2004). These systems suffer from requiring cooperation from all the systems to produce compatible lattices as well as the hard research problem of standardizing confidence scores that come from the individual engines. In 2001, Bangalore et al used string alignments between the different translations to train a finite state machine to produce a consensus translation. The alignment algorithm described in that work, which only allows insertions, deletions and substitutions, does not accurately capture long range phrase movement. In this paper, we propose a new way of combin"
P05-3026,hogan-frederking-1998-evaluation,0,\N,Missing
P06-2089,gimenez-marquez-2004-svmtool,0,0.0547486,"Missing"
P06-2089,C04-1010,0,0.173875,"asonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still well below that of state-of-the-art statistical parsers. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. The parser retains much of the simplicity of deterministic classifier-based parsers, but achieves results that are closer in accuracy to state-of-the-art statistical parsers. Furthermore, a simple combination of the shift-reduce parsing model with an existing generative parsing model produces results with accuracy that surpasses any that of any single (nonreranked) parser tested on the WSJ Penn Treebank, and comes close to the best results obtained with discriminative reranking (Charniak and"
P06-2089,J96-1002,0,0.0103098,"ary and binary productions. REDUCE-RIGHT-XX : represents a binary reduce action, where the root of the new subtree pushed onto S is of non-terminal type XX. Additionally, the head of the new subtree is the same as the head of the right child of the root node. To implement a parser based on the best-first algorithm, instead of just using a classifier to determine one parser action given a stack and a queue, we need a classification approach that provides us with probabilities for different parser actions associated with a given parser state. One such approach is maximum entropy classification (Berger et al., 1996), which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser (Tsuruoka and Tsujii, 2005). We used the same classes and the same features as Sagae and Lavie, and an additional feature that represents the previous parser action applied the current parser state (figure 1). 3 Related Work 4 As mentioned in section 2, our parsing approach can be seen as an extension of the approach of Sagae and Lavie (2005). Sagae and Lavie evaluated their deterministic classifier-based parsing framework using two classifiers: support vector machines (SVM) and k-nearest n"
P06-2089,W97-0301,0,0.76175,"SHIFT : represents a shift action; REDUCE-UNARY-XX : represents a unary reduce action, where the root of the new subtree pushed onto S is of type XX (where XX is a non-terminal symbol, typically N P , V P , P P , for example); REDUCE-LEFT-XX : represents a binary reduce action, where the root of the new sub693 tree pushed onto S is of non-terminal type XX. Additionally, the head of the new subtree is the same as the head of the left child of the root node; is based on reframing the parsing task as several sequential chunking tasks. Finally, our parser is in many ways similar to the parser of Ratnaparkhi (1997). Ratnaparkhi’s parser uses maximum-entropy models to determine the actions of a parser based to some extent on the shift-reduce framework, and it is also capable of pursuing several paths and returning the topn highest scoring parses for a sentence. However, in addition to using different features for parsing, Ratnaparkhi’s parser uses a different, more complex algorithm. The use of a more involved algorithm allows Ratnaparkhi’s parser to work with arbitrary branching trees without the need of the binarization transform employed here. It breaks the usual reduce actions into smaller pieces (CH"
P06-2089,E03-1005,0,0.0143229,"Missing"
P06-2089,J93-1002,0,0.0215226,"the top of the heap, resulting in the desired search behavior. The accuracy of deterministic parsers suggest that this may in fact be the types of probabilities a classifier would produce given features that describe the parser state, and thus the context of the parser action, specifically enough. The experiments described in section 4 support this assumption. simply as the product of the probabilities of each action in the path that resulted in that parse tree (the derivation of the tree). This produces a probabilistic shift-reduce parser that resembles a generalized probabilistic LR parser (Briscoe and Carroll, 1993), where probabilities are associated with an LR parsing table. In our case, although there is no LR table, the action probabilities are associated with several aspects of the current state of the parser, which to some extent parallel the information contained in an LR table. Instead of having an explicit LR table and pushing LR states onto the stack, the state of the parser is implicitly defined by the configurations of the stack and queue. In a way, there is a parallel between how modern PCFG-like parsers use markov grammars as a distribution that is used to determine the probability of any p"
P06-2089,W05-1513,1,0.939065,"d the state-of-theart. A reasonable and commonly held assumption is that the accuracy of deterministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still well below that of state-of-the-art statistical parsers. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. The parser retains much of the simplicity of deterministic classifier-based parsers, but achieves results that are closer in accuracy to state-of-the-art statistical parsers. Furthermore, a simple combination of the shift-reduce parsing model with an existing generative parsing model produces results with accuracy that surpasses any that of any single (nonreranked) parser tested on the WSJ Penn Treebank, and comes close to the best results obtained with discrimina"
P06-2089,P05-1022,0,0.127155,"Missing"
P06-2089,J87-1004,0,0.0773574,"lgorithm described in section 2.1 does not handle ambiguity. By choosing a single parser action at each opportunity, the input string is parsed deterministically, and a single constituent structure is built during the parsing process from beginning to end (no other structures are even considered). A simple extension to this idea is to eliminate determinism by allowing the parser to choose several actions at each opportunity, creating different paths that lead to different parse trees. This is essentially the difference between deterministic LR parsing (Knuth, 1965) and Generalized-LR parsing (Tomita, 1987; Tomita, 1990). Furthermore, if a probability is assigned to every parser action, the probability of a parse tree can be computed • Shift: A shift action consists only of removing (shifting) the first item (part-of-speechtagged word) from W (at which point the next word becomes the new first item), and placing it on top of S. 692 the most probable parse. To obtain a list of n-best parses, we simply continue parsing once the first parse tree is found, until either n trees are found, or H is empty. We note that this approach does not use dynamic programming, and relies only on the bestfirst sea"
P06-2089,W98-1115,0,0.0212347,"parser of Sagae and Lavie (2005). That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The algorithm considers only trees with unary and binary productions. Training the parser with arbitrary branching trees is accomplished by a simple procedure to transform those trees into trees with at most binary productions. This is done by converting each production with n children, where n > 2, into n − 1 binary productions. This binarization process is similar to the one described in (Charniak et al., 1998). Additional nonterminal nodes introduced in this conversion must be clearly marked. Transforming the parser’s output into arbitrary branching trees is accomplished using the reverse process. The deterministic parsing algorithm involves two main data structures: a stack S, and a queue W . Items in S may be terminal nodes (part-ofspeech-tagged words), or (lexicalized) subtrees of the final parse tree for the input string. Items in W are terminals (words tagged with parts-of-speech) corresponding to the input string. When parsing begins, S is empty and W is initialized by inserting every word fr"
P06-2089,A00-2018,0,0.706361,"Missing"
P06-2089,W05-1514,0,0.0911745,"Missing"
P06-2089,W03-3023,0,0.0794049,"Missing"
P06-2089,P97-1003,0,0.172477,"Missing"
P06-2089,J93-2004,0,\N,Missing
P06-2089,W00-1604,0,\N,Missing
P06-2089,J03-4003,0,\N,Missing
P06-2089,C90-1012,0,\N,Missing
P11-1042,H05-1009,0,0.0582652,"si like like one of ”. that that is but since since hsi when , how , not 6 θk 3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems,"
P11-1042,N10-1083,0,0.158266,"Missing"
P11-1042,P06-1009,0,0.395676,"tion scores that are precomputed by looking at the full training data: Dice’s coefficient (discretized), which we use to measure association strength between pairs of source and target word types across sentence pairs (Dice, 1945), IBM Model 1 forward and reverse probabilities, and the geometric mean of the Model 1 forward and reverse probabilities. Finally, we also cluster the source and target vocabularies (Och, 1999) and include class pair indicator features, which can learn generalizations that, e.g., “nouns tend to translate into nouns but not modal verbs.” Positional features. Following Blunsom and Cohn (2006), we include features indicating closeness to the alignment matrix diagonal, aj j h(aj , j, m, n) = m − n . We also conjoin this feature with the source word class type indicator to enable the model to learn that certain word types are more or less likely to favor a location on the diagonal (e.g. Urdu’s sentence-final verbs). Source features. Some words are functional elements that fulfill purely grammatical roles and should not be the “source” of a translation. For example, Romance languages require a preposition in the formation of what could be a noun-noun compound in English, thus, it may"
P11-1042,P08-1024,0,0.0215045,"t, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM M"
P11-1042,D08-1023,0,0.0170163,"t, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM M"
P11-1042,bojar-prokopova-2006-czech,0,0.300645,"Missing"
P11-1042,J93-2003,0,0.180904,"r model yields better alignments than generative baselines in a number of language pairs. 1 Introduction Word alignment is an important subtask in statistical machine translation which is typically solved in one of two ways. The more common approach uses a generative translation model that relates bilingual string pairs using a latent alignment variable to designate which source words (or phrases) generate which target words. The parameters in these models can be learned straightforwardly from parallel sentences using EM, and standard inference techniques can recover most probable alignments (Brown et al., 1993). This approach is attractive because it only requires parallel training data. An alternative to the generative approach uses a discriminatively trained 409 alignment model to predict word alignments in the parallel corpus. Discriminative models are attractive because they can incorporate arbitrary, overlapping features, meaning that errors observed in the predictions made by the model can be addressed by engineering new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required f"
P11-1042,J07-2003,0,0.0848504,"n test sets.10 While neither a decrease in the average singleton fertility nor an increase in the number of rules induced guarantees better alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experim"
P11-1042,P11-2031,1,0.446686,"Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order. For this language pair, we evaluate alignment error rate using the manual alignment corpus described by Bojar and Prokopov´a (2006). Table 2 summarizes the results. Chinese-English. Chinese-English poses a different set of problems"
P11-1042,E09-1020,0,0.0365102,"Missing"
P11-1042,P10-1147,0,0.0270736,"0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith a"
P11-1042,N10-1128,1,0.745719,"se problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fe"
P11-1042,P10-4002,1,0.451451,"Missing"
P11-1042,E09-1037,1,0.831167,"as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM Model 3) useful for translation and alignment modeling. To be truly general, it must be possible to utilize such features. Unfortunately, features like this that depend on global properties of the alignment vector, a, make 417 the inference problem NP-hard, and approximations are necessary. Fortunately, there is much recent work on approximate inference techniques for incorporating nonlocal features (Blunsom et al., 2008b; Gimpel and Smith, 2009; Cromi`eres and Kurohashi, 2009; Weiss and Taskar, 2010), suggesting that this problem too can be solved using established techniques. 8 Conclusion We have introduced a globally normalized, loglinear lexical translation model that can be trained discriminatively using only parallel sentences, which we apply to the problem of word alignment. Our approach addresses two important shortcomings of previous work: (1) that local normalization of generative models constrains the features that can be used, and (2) that previous discriminatively trained word alignment models required supervised alignme"
P11-1042,N06-2013,0,0.0188684,"ring new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required for training, which is problematic for at least three reasons. Manual alignments are notoriously difficult to create and are available only for a handful of language pairs. Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers. Third, the “correct” alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments may be optimal for different approaches to (downstream) translation model induction (Lopez, 2008; Fraser, 2007). Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively “generates” the values of the variables. At each step, the probability of some value being gene"
P11-1042,P09-1104,0,0.020558,"3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastiv"
P11-1042,N03-1017,0,0.0190572,"d confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running. Future work will explore the scalability characteristics and limits of the model. 5.1 Methodology For each language pair, we train two log-linear translation models as described above (§3), once with English as the source and once with English as the target language. For a baseline, we use the Giza++ toolkit (Och and Ney, 2003) to learn Model 4, again in both directions. We symmetrize the alignments from both model types using the grow-diag-final-and heuristic (Koehn et al., 2003) producing, in total, six alignment sets. We evaluate them both intrinsically and in terms of their performance in a translation system. Since we only have gold alignments for CzechEnglish (Bojar and Prokopov´a, 2006), we can report alignment error rate (AER; Och and Ney, 2003) only for this pair. However, we offer two further measures that we believe are suggestive and that do not require gold alignments. One is the average alignment “fertility” of source words that occur only a single time in the training data (so-called hapax legomena). This assesses the impact of a typical alignment proble"
P11-1042,P09-1019,1,0.827278,"xtracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech word"
P11-1042,N09-1069,0,0.0123084,"depicted graph is determined by the features that we use (§4). the gradient of the unregularized objective (Tsuruoka et al., 2009). This method is quite attractive since it is only necessary to represent the active features, meaning impractically large feature spaces can be searched provided the regularization strength is sufficiently high. Additionally, not only has this technique been shown to be very effective for optimizing convex objectives, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective of the generative variant). To choose the regularization strength β and the initial learning rate η0 ,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). For the remainder of the experiments, we use the values we obtained, β = 0.4 and η0 = 0.3. 3.2 Inference with WFSAs We now describe how to use"
P11-1042,D09-1106,0,0.0217753,"ed guarantees better alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” m"
P11-1042,J10-3002,0,0.0438443,"92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation techn"
P11-1042,C08-1064,0,0.0291766,"only for a handful of language pairs. Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers. Third, the “correct” alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments may be optimal for different approaches to (downstream) translation model induction (Lopez, 2008; Fraser, 2007). Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively “generates” the values of the variables. At each step, the probability of some value being generated may depend only on the generation history (or a subset thereof), and the possible values a variable will take must form a locally normalized conditional probability distribution (CPD). While these locally normalized CPDs may be paProceedings of the 49th Annual Meeting of the Association for Computat"
P11-1042,W03-0301,0,0.0383228,"s, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective of the generative variant). To choose the regularization strength β and the initial learning rate η0 ,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). For the remainder of the experiments, we use the values we obtained, β = 0.4 and η0 = 0.3. 3.2 Inference with WFSAs We now describe how to use weighted finite-state automata (WFSAs) to compute the quantities neces3.1 Parameter Learning sary for training. We begin by describing the ideal To learn the parameters of our model, we select the WFSA representing the full translation search space, θ ∗ that minimizes the `1 regularized conditional log- which we call the discriminative neighborhood, and likelihood of a set of training data T : then discuss strategies for reducing its size in the X X X"
P11-1042,H05-1011,0,0.332958,"alignment variable a = ha1 , a2 , . . . , an i ∈ [0, m]n , where aj = 0 represents a special null token. X p(t |s, n) = p(t, a |s, n) 2 3 Model In this section, we develop a conditional model p(t |s) that, given a source language sentence s with length m = |s|, assigns probabilities to a target sentence t with length n, where each word tj is an element in the finite target vocabulary Ω. We begin by using the chain rule to factor this probability into two components, a translation model and a length model. p(t |s) = p(t, n |s) = p(t |s, n) × p(n |s) |{z } |{z } translation model length model 1 Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features. 410 a So far, our model is identical to that of (Brown et al., 1993); however, we part ways here. Rather than using the chain rule to further decompose this probability and motivate opportunities to make independence assumptions, we use a log-linear model with parameters θ ∈ Rk and feature vector function H that maps each tuple ha, s, t, ni into Rk to model p(t, a |s, n) directly: pθ (t, a |s, n) = Zθ (s, n) = exp θ &gt; H(t, a, s, n) , where Zθ (s, n) X X exp θ &gt; H(t0 , a0 , s, n) t0 ∈Ωn a0"
P11-1042,J03-1002,0,0.00681142,"translation. 9 http://statmt.org/wmt10 414 der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running. Future work will explore the scalability characteristics and limits of the model. 5.1 Methodology For each language pair, we train two log-linear translation models as described above (§3), once with English as the source and once with English as the target language. For a baseline, we use the Giza++ toolkit (Och and Ney, 2003) to learn Model 4, again in both directions. We symmetrize the alignments from both model types using the grow-diag-final-and heuristic (Koehn et al., 2003) producing, in total, six alignment sets. We evaluate them both intrinsically and in terms of their performance in a translation system. Since we only have gold alignments for CzechEnglish (Bojar and Prokopov´a, 2006), we can report alignment error rate (AER; Och and Ney, 2003) only for this pair. However, we offer two further measures that we believe are suggestive and that do not require gold alignments. One is the average alignment “fert"
P11-1042,E99-1010,0,0.034305,"phic feature was computed after first applying a heuristic Romanization, which made the orthographic forms somewhat comparable. 413 regardless of length). We also include “global” association scores that are precomputed by looking at the full training data: Dice’s coefficient (discretized), which we use to measure association strength between pairs of source and target word types across sentence pairs (Dice, 1945), IBM Model 1 forward and reverse probabilities, and the geometric mean of the Model 1 forward and reverse probabilities. Finally, we also cluster the source and target vocabularies (Och, 1999) and include class pair indicator features, which can learn generalizations that, e.g., “nouns tend to translate into nouns but not modal verbs.” Positional features. Following Blunsom and Cohn (2006), we include features indicating closeness to the alignment matrix diagonal, aj j h(aj , j, m, n) = m − n . We also conjoin this feature with the source word class type indicator to enable the model to learn that certain word types are more or less likely to favor a location on the diagonal (e.g. Urdu’s sentence-final verbs). Source features. Some words are functional elements that fulfill purely"
P11-1042,P02-1040,0,0.108705,"nments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relativel"
P11-1042,D10-1052,1,0.841806,"?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is glob"
P11-1042,P05-1044,1,0.911055,"= 0.01) on the translation distributions and use the empirical Bayes (EB) method to infer a point estimate, using variational inference. Table 1: Comparison of alternative definitions Ωs (arrows indicate whether higher or lower is better). P Ωs time (s) ↓ AER ↓ s |Ωs |↓ =Ω 22.4 86.0M 0.0 co-occ. 8.9 0.68M 0.0 Model 1 0.2 0.38M 6.2 EB-Model 1 1.0 0.15M 2.9 Table 1 compares the average per-sentence time required to run the inference algorithm described 5 Future work will explore alternative formulations of the discriminative neighborhood with the goal of further improving inference efficiency. Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. above under these four different definitions of Ωs on a 10,000 sentence subset of the Hansards FrenchEnglish corpus that includes manual word alignments. While our constructions guarantee that all references are reachable even in the reduced neighborhoods, not all alignments between source and target are possible. The last column is the oracle AER. Although EB variant of Model 1 neighborhood is slightly more expensive to"
P11-1042,2006.amta-papers.25,0,0.0389463,"guage model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order. For this language pair, we evaluate alignment erro"
P11-1042,takezawa-etal-2002-toward,0,0.0213892,"word translates as itself (for example, a name or a date, which occurs in languages that share the same alphabet) in position j, but then is translated again (as something else) in position j − 1 or j + 1. 5 Experiments We now turn to an empirical assessment of our model. Using various datasets, we evaluate the performance of the models’ intrinsic quality and theirtheir alignments’ contribution to a standard machine translation system. We make use of parallel corpora from languages with very different typologies: a small (0.8M words) Chinese-English corpus from the tourism and travel domain (Takezawa et al., 2002), a corpus of Czech-English news commentary (3.1M words),9 and an Urdu-English corpus (2M words) provided by NIST for the 2009 Open MT Evaluation. These pairs were selected since each poses different alignment challenges (word or8 This is of course what makes history-based language model integration an inference challenge in translation. 9 http://statmt.org/wmt10 414 der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation u"
P11-1042,H05-1010,0,0.0995513,"is but since since hsi when , how , not 6 θk 3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in"
P11-1042,P09-1054,0,0.228503,"me using dynamic programming. For example, when the graph has a sequential structure, exact inference can be carried out using the familiar forwardbackward algorithm (Lafferty et al., 2001). Although our features look at more structure than this, they are designed to keep treewidth low, meaning exact inference is still possible with dynamic programming. Figure 1 gives a graphical representation of our model as well as the more familiar generative (directed) variants. The edge set in the depicted graph is determined by the features that we use (§4). the gradient of the unregularized objective (Tsuruoka et al., 2009). This method is quite attractive since it is only necessary to represent the active features, meaning impractically large feature spaces can be searched provided the regularization strength is sufficiently high. Additionally, not only has this technique been shown to be very effective for optimizing convex objectives, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective o"
P11-1042,2008.amta-papers.18,1,0.874421,"er alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate trainin"
P11-1042,C96-2141,0,0.939204,"rization and only depends on hood, the set Ωn ×[0, m]n , such that every path from 2 One way to understand expressiveness is in terms of indethe start state to goal yields a pair ht0 , ai with weight pendence assumptions, of course. Research in graphical models has done much to relate independence assumptions to the complexity of inference algorithms (Koller and Friedman, 2009). 411 3 For the other free parameters of the algorithm, we use the default values recommended by Tsuruoka et al. (2009). s s n n s a1 a2 a3 ... an t1 t2 t3 ... tn a1 s s a2 s t1 Fully directed model (Brown et al., 1993; Vogel et al., 1996; Berg-Kirkpatrick et al., 2010) s a3 ... t3 ... s t2 s an s tn Our model Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value). H(t0 , a, s, n). With our feature set (§4), number of states in this WFSA is O(m × n) since at each target index j, there is a different state for each possible index of th"
P11-2031,W05-0909,1,0.0850933,"m is the average of all mi s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find differ"
P11-2031,J08-1003,0,0.0192515,"Missing"
P11-2031,W08-0304,0,0.0315931,"ther uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental"
P11-2031,N10-1080,0,0.00491755,"eous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system"
P11-2031,D08-1024,0,0.049401,"initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2 This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3 Online subgradient techniques such as MIRA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics data and held-out test data, independently of any experimental manipulation. Thus, when trying to determine whether the difference between two measurements is significant, it is necessary to control for variance due to noisy parameter estimates. This can be done by replication of the optimization proce"
P11-2031,J07-2003,0,0.277048,"e MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT imple"
P11-2031,M93-1008,0,0.427905,"rocessing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to impro"
P11-2031,N10-1031,1,0.375888,"i s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise"
P11-2031,P10-4002,1,0.432465,"er to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 ra"
P11-2031,N09-1046,1,0.489862,"ebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al., 2009). The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al., 2009). Rather than"
P11-2031,W09-0439,0,0.0241928,"been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize B"
P11-2031,P07-2045,1,0.0508851,"periments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Mo"
P11-2031,W04-3250,0,0.348961,"ine result paired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. We now turn to the question of how to reduce the probability falling into this trap. 3 Related Work The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instab"
P11-2031,P09-1019,1,0.208024,"egmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al., 2009). The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al., 2009). Rather than using restart points, in addition to optimizing each feature independently, it optimizes in 5 random directions per iteration by constructing a search vector by uniformly sampling each element of the vector from (−1, 1) and then renormalizing so it has length 1. For all systems, the initial weight vector was manually initialized so as to yield reasonable translations. 4 http://statmt.org/wmt11/ System Avg ssel sdev BTEC Chinese-English (n = 300) System A 48.4 1.6 0.2 BLEU ↑ System B 49.9 1.5 0.1 System A 63.3 0.9 MET ↑ System B 63.8 0.9 System A 30.2 1.1 TER ↓ System B 28.7 1.0 W"
P11-2031,C08-1074,0,0.00521439,"8). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pa"
P11-2031,P03-1021,0,0.949219,"proves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. 1 2 Introduction The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003). In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets. However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on 1 We hypothesize that the convention of “trusting”"
P11-2031,P02-1040,0,0.112437,"for the ith optimization run, and m is the average of all mi s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, dif"
P11-2031,W05-0908,0,0.060395,"has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significanc"
P11-2031,2006.amta-papers.25,0,0.190763,"over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise due to this variable can depend"
P11-2031,zhang-etal-2004-interpreting,0,0.00388873,"ired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. We now turn to the question of how to reduce the probability falling into this trap. 3 Related Work The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our c"
P16-1103,J93-2003,0,0.0531595,"φfwd = −0.495208 φrev = −0.455368 φfwd = −0.499118 φrev = −0.269879 φfwd = −3.718241 φuses_suf_n = 1.0 φfwd = −2.840721 φuses_end_s = 1.0 Table 1: A fragment of the word-to-character rules used in the compound generation system. beddings work well. The recurrent part of the neural network uses two-layer LSTM (Hochreiter and Schmidhuber, 1997) cells with the hidden layer size set to 10. The final MLP’s hidden layer size is also set to 10. The training data is processed such that each span of length two to four is considered one training example, and is labeled as positive if it is wellaligned (Brown et al., 1993) to a single German compound word. Since most spans do not translate as compounds, we are faced with an extreme class imbalance problem (a ratio of about 300:1). We therefore experiment with down sampling the negative training examples to have an equal number of positive and negative examples. 3.3 Training the Compound Generation Model As a translation model, there are two components to learning the translation system: learning the rule inventory and their features (§3.3.1) and learning the parameters of the generation model (§3.3.2). 3.3.1 Learning Word to Character Sequence Translation Rules"
P16-1103,E14-1061,0,0.0333313,"U ↑ METR ↑ TER ↓ Len Baseline 12.3 29.0 72.7 96.5 29.1 72.8 96.8 +Our Compounds 12.3 Baseline 11.4 29.9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having"
P16-1103,D13-1174,1,0.807908,"only does generating them make the output seem more natural, but they are contentrich. Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we introduce a two-stage method (§2) to dynamically generate novel compound word forms given a source language input text and incorporate these as “synthetic rules” in a standard phrase-based translation system (Bhatia et al., 2014; Chahuneau et al., 2013; Tsvetkov et al., 2013). First, a binary classifier examines each source-language sentence and labels each span therein with whether that span could become a compound word when translated into the target language. Second, we transduce the identified phrase into the target language using a word-to-character translation model. This system makes a closed vocabulary assumption, albeit at the character (rather than word) level—thereby enabling new word forms to be generated. Training data for these models is extracted from automatically aligned and compound split parallel corpora (§3). We evaluate"
P16-1103,H05-1098,0,0.0129145,", and extract our generator’s top ten hypotheses for each of the postively identified spans. These English phrases are then added to a synthetic phrase table, along with their German compound translations, and two features: the compound generator’s score, and an indicator feature simply showing that the rule represents a synthetic compound. Table 5 shows some example rules of this form. The weights of these features are learned, along with the standard translation system weights, by the MIRA algorithm as part of the MT training procedure. The underlying translation system is a standard Hiero (Chiang et al., 2005) system using the cdec (Dyer et al., 2010) decoder, trained on all constrained-track WMT English–German data as of the 2014 translation task. Tokenization was done with cdec’s tokenize-anything script. The first character of each sentence was down cased if the unigram probability of the downcased version of the first word was higher than that of the original casing. Word alignment was performed using cdec’s fast_align tool, WMT2012 WMT2013* Table 3: Mean reciprocal rank, character error rate, and precision at K statistics of our baseline MT system and our compound generator. BLEU ↑ METR ↑ TER"
P16-1103,P11-2031,1,0.867193,"Missing"
P16-1103,P11-1004,0,0.0265227,"unds 12.3 Baseline 11.4 29.9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their"
P16-1103,N12-1017,0,0.0305634,"anslation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and translating German into English at the morpheme level. Their approach works excellently when translating out of a compounding language, but is unable to g"
P16-1103,P08-1115,0,0.0410213,"d Dropping We observe that in order to generate many compounds, including bananenmarkts from “market for bananas”, a system must be able to both reorder and drop source words at will. Implemented naïvely, however, these allowances may produce invalid interleavings of source words and SUF/END tokens. For example, if we (correctly) drop the word “for” from our example, we might feed the decoder the sequence “market SUF SUF bananas END. To disallow such bogus input sequences we disable all reordering inside the decoder, and instead encode all possible reorderings in the form of an input lattice (Dyer et al., 2008). Moreover, we allow the decoder to drop non-content words by skipping over them in the lattice. Each edge in our lattices contains a list of features, including the indices, lexical forms, and parts of speech of each word kept or dropped. Each possible sequence in the lattice also encodes features of the full path of source words kept, the full list of source words dropped, the parts of speech of the path and all dropped words, and the order of indices traversed. With these constraints in place we can train the compound generator as though it were a normal MT system with no decode-time reorde"
P16-1103,P10-4002,1,0.825787,"eses for each of the postively identified spans. These English phrases are then added to a synthetic phrase table, along with their German compound translations, and two features: the compound generator’s score, and an indicator feature simply showing that the rule represents a synthetic compound. Table 5 shows some example rules of this form. The weights of these features are learned, along with the standard translation system weights, by the MIRA algorithm as part of the MT training procedure. The underlying translation system is a standard Hiero (Chiang et al., 2005) system using the cdec (Dyer et al., 2010) decoder, trained on all constrained-track WMT English–German data as of the 2014 translation task. Tokenization was done with cdec’s tokenize-anything script. The first character of each sentence was down cased if the unigram probability of the downcased version of the first word was higher than that of the original casing. Word alignment was performed using cdec’s fast_align tool, WMT2012 WMT2013* Table 3: Mean reciprocal rank, character error rate, and precision at K statistics of our baseline MT system and our compound generator. BLEU ↑ METR ↑ TER ↓ Len 16.2 34.5 64.8 94.1 +Our Compounds 1"
P16-1103,N09-1046,1,0.873726,"ond, we build a compound generator that outputs hypothesis word forms, given a source phrase. We will detail each of these steps in turn. 3.1 Extracting Compounds from Bitext In order to learn to generate compound words we naturally require training data. Ideally we would like a large list of English phrases with their natural contexts and translations as German compounds. Of course virtually no such data exists, but it is possible to extract from parallel data, using a technique similar to that used by Tsvetkov and Wintner (2012). To this end, we take our tokenized bitext and pass it through Dyer (2009)’s German compound splitter. We then align the segmented variant using the fast_align tool in both the forward and reverse directions, which produces both word alignments and lexical translation tables, which give the probability of a compound part given an English phrase. We then symmetrize the produced pair of alignments with the intersection heuristic. This results in a sparse alignment in which each target word is aligned to either 0 or 1 source words. We then undo any splits performed by the compound splitter, resulting in a corpus where the only words aligned many-to-one are precisely we"
P16-1103,W10-1734,0,0.0219955,"ons that characterize natural language, it is particularly challenging in languages such as German and Finnish that have productive compounding processes. In such languages, expressing compositions of basic concepts can require an unbounded number of words. For example, English multiword phrases like market for bananas, market for pears, and market for plums are expressed in German with single compound words (respectively, as Bananenmarkt, Birnenmarkt, and Pflaumenmarkt). Second, while they are individually rare, compound words are, on the whole, frequent in native texts (Baroni et al., 2002; Fritzinger and Fraser, 2010). Third, compounds are crucial for translation quality. Not only does generating them make the output seem more natural, but they are contentrich. Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we introduce a two-stage method (§2) to dynamically generate novel compound word forms given a source language input text and incorporate these as “synthetic rules” in a stan"
P16-1103,P13-2121,0,0.080794,"Missing"
P16-1103,N13-1056,0,0.0318055,"en Baseline 12.3 29.0 72.7 96.5 29.1 72.8 96.8 +Our Compounds 12.3 Baseline 11.4 29.9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the s"
P16-1103,W01-0504,0,0.0666443,"age, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and translating German into English at the morpheme level. Their approach works excellently when translating out of a compounding language, but is unable to generate novel compound words in the target language without some sort of post processing. Dynamic generation of compounds in a target language using such post processing has been examined in the past by Cap et al. (2014) and Clifton and Sarkar (2011). Both perform compound sp"
P16-1103,W11-2129,0,0.0234795,".9 71.6 96.2 +Our Compounds 11.6 30.1 71.5 96.4 Baseline 10.8 28.4 73.4 96.7 +Our Compounds 10.9 28.5 73.3 96.9 Table 7: Improvements in English–Finnish translation quality using our method of compound generation on WMT 2014 tuning, devtest, and test sets. * indicates the set used for tuning the MT system. cated here, first translating the source language into a morphologically analyzed and segmented variant of the target language, and then performing morphological generation on this sequence (Cap et al., 2014; Irvine and Callison-Burch, 2013; Denkowski et al., 2013; Clifton and Sarkar, 2011; Stymne and Cancedda, 2011). Requesting multiple translations from a translator has been used in the past, most notably to create HyTER reference lattices (Dreyer and Marcu, 2012). However, in contrast to full-sentence translations the space of possible grammatical compounds is far smaller, substantially simplifying our task. The splitting of German compound phrases for translation from German into English has been addressed by Koehn and Knight (2001) and Dyer (2009). They elegantly solve the problem of having a large, open vocabulary on the source side by splitting compound words into their constituent morphemes and tr"
P16-1103,N03-1033,0,0.0156958,"non-compoundable using independent binary predictions. Rather than attempting to hand-engineer features to represent phrases, we use a bidirectional LSTM to learn a fixed-length vector representation hi,j that is computed by composing representations of the tokens (fi , fi+1 , . . . , fj ) in the input sentence. The probability that a span is compoundable is then modeled as: p(compoundable? |fi , fi+1 , . . . , fj ) =   σ w> tanh(Vhi,j + b) + a , where σ is the logistic sigmoid function, and w, V, b, and a are parameters. To represent tokens that are inputs to the LSTM, we run a POS tagger (Toutanova et al., 2003), and for each token concatenate a learned embedding of the tag and word. Figure 1 shows the architecture. p(is a compound) p(not a compound) Suppose we want to translate the sentence the market for bananas has collapsed . MLP hidden layer from English into German. In order to produce the following (good) translation, Backward LSTM der bananenmarkt ist abgestürzt . Forward LSTM a phrase-based translation system would need to contain a rule similar to market for bananas → bananenmarkt. While it is possible that such a rule would be learned from parallel corpora using standard rule extraction te"
P16-1103,W13-2234,1,0.709531,"em make the output seem more natural, but they are contentrich. Since each compound has, by definition, at least two stems, they are intuitively (at least) doubly important for translation adequacy. Fortunately, compounding is a relatively regular process (as the above examples also illustrate), and it is amenable to modeling. In this paper we introduce a two-stage method (§2) to dynamically generate novel compound word forms given a source language input text and incorporate these as “synthetic rules” in a standard phrase-based translation system (Bhatia et al., 2014; Chahuneau et al., 2013; Tsvetkov et al., 2013). First, a binary classifier examines each source-language sentence and labels each span therein with whether that span could become a compound word when translated into the target language. Second, we transduce the identified phrase into the target language using a word-to-character translation model. This system makes a closed vocabulary assumption, albeit at the character (rather than word) level—thereby enabling new word forms to be generated. Training data for these models is extracted from automatically aligned and compound split parallel corpora (§3). We evaluate our approach on both in"
P16-1103,W07-0705,0,0.0322208,"h handles this problem seamlessly. Stymne (2012) gives an excellent taxonomy of compound types in Germanic languages, and discusses many different strategies that have been used to split and merge them for the purposes of machine translation. She identifies several difficulties with the split-translate-merge approach and points out some key subtleties, such as handling of bound morphemes that never occur outside of 1091 compounds, that one must bear in mind when doing translation to or from compounding languages. The idea of using entirely character-based translation systems was introduced by Vilar et al. (2007). While their letter-level translation system alone did not outperform standard phrase-based MT on a Spanish–Catalan task, they demonstrated substantial BLEU gains when combining phraseand character-based translation models, particularly in low resource scenarios. 7 Conclusion In this paper we have presented a technique for generating compound words for target languages with open vocabularies by dynamically introducing synthetic translation options that allow spans of source text to translate as a single compound word. Our method for generating such synthetic rules decomposes into two steps. F"
P16-1103,W14-3315,1,\N,Missing
P16-1103,C14-1100,1,\N,Missing
P94-1045,J93-1002,0,0.0805752,"Missing"
P94-1045,1993.iwpt-1.12,1,0.755445,"Missing"
probst-lavie-2004-structurally,jones-havrilla-1998-twisted,0,\N,Missing
probst-lavie-2004-structurally,sheremetyeva-nirenburg-2000-towards,0,\N,Missing
Q14-1031,D13-1106,0,0.0640405,"ational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and demonstrate that disc"
Q14-1031,D10-1066,0,0.0129665,"Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with in"
Q14-1031,N12-1047,0,0.0373472,"MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3 Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4 Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the `1 -renormalized variant of regularized MERT. 395 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produc"
Q14-1031,D08-1024,0,0.0852249,"arability remains an informative tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N ). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), A"
Q14-1031,J07-2003,0,0.150009,"ment of weights. We augment RLNR with a smooth damping term D(w, j), which has the shape of a bathtub curve with steepness γ: 1 2γ 2 (wj−1 + wj+1 ) − wj D(w, j) = tanh (12) 1 2 (wj−1 − wj+1 ) RMNR (w) = β |h|−1 X j=2 D(w, j)RLNR (w, j) (13) D is nearly zero while wj ∈ [wj−1 , wj+1 ] and nearly one otherwise. Briefly, the numerator measures how far wj is from the midpoint of wj−1 and wj+1 while the denominator scales that distance by the radius from the midpoint to the neighboring weight. Experimental Setup6 5 Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implem"
Q14-1031,P11-2031,1,0.952373,"Bojar et al., 2012). First, we lowercased and performed sentence-level deduplication of the data.7 Then, we uniformly sampled a training set of 1M sentences (sections 1 – 97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.8 Sentences less than 5 words were discarded due to noise. Evaluation: We quantify increases in translation quality using case-insensitive BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by averaging over multiple optimizer replicas (Clark et al., 2011).9 7 CzEng is distributed deduplicated at the document level, leading to very high sentence-level overlap. 8 The section splits recommended by Bojar et al. (2012). 9 MultEval 0.5.1: github.com/jhclark/multeval Bits Features Test BLEU 4 101 36.4 8 1302 36.6 12 12,910 36.8 Table 2: Translation quality for Cz→En system with varying bits for discretization. For all other experiments, we tune the number of bits on held-out data. Condition P log P Disc P Over. P LNR P MNR P MNR C Zh→En 20.8? (-2.7) 23.5† 23.4† (-0.1) 20.7? (-2.8) 23.1?† (-0.4) 23.8† (+0.3) 23.6† (±) Ar→En 44.3? (-3.6) 47.9† 47.2† (-"
Q14-1031,P10-4002,1,0.798396,"tance by the radius from the midpoint to the neighboring weight. Experimental Setup6 5 Formalism: In our experiments, we use a hierarchical phrase-based translation model (Chiang, 2007). A corpus of parallel sentences is first word-aligned, and then phrase translations are extracted heuristically. In addition, hierarchical grammar rules are extracted where phrases are nested. In general, our choice of formalism is rather unimportant – our techniques should apply to most common phrasebased and chart-based paradigms including Hiero and syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6 All code at http://github.com/"
Q14-1031,N13-1025,1,0.860909,"in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N ). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are al"
Q14-1031,D13-1201,0,0.243368,"OLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3 Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4 Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the `1 -renormalized variant of regularized MERT. 395 3 Discretization and Feature Induction In this section, we propose a feature induction technique based on discretization that produces a feature set that is less prone to non-linearities (see §2.2"
Q14-1031,W12-3134,0,0.0145241,", it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we use the PRO optimizer (Hopkins and May, 2011) as our baseline learner since it has been shown to perform comparably to MERT for a small number of features, and to significantly outperform MERT for a large number of features (Hopkins and May, 2011; Ganitkevitch et al., 2012). Other very recent MT optimizers such as the linear structured SVM (Cherry and Foster, 2012), AROW (Chiang, 2012) and regularized MERT (Galley et al., 2013) are also compatible with the discretization and structured regularization techniques described in this article.4 3 Gimpel et al. eventually used raw probabilities in their model rather than log-probabilities. 4 Since we dispense with nearly all of the original dense features and our structured regularizer is scale sensitive, one would need to use the `1 -renormalized variant of regularized MERT. 395 3 Discretization and Feature Induction"
Q14-1031,W07-0719,0,0.0503606,"Missing"
Q14-1031,D09-1023,0,0.255479,"ence translations, we can always recover the reference. In practice, both of these conditions are typically violated to a certain degree. However, if we modify our feature set such that some lower-ranked higher-quality hypothesis can be separated from all higher-ranked lower-quality hypotheses, then we can improve translation quality. For this reason, we believe that separability remains an informative tool for thinking about modeling in MT. Currently, non-linearities in novel real-valued features are typically addressed via manual feature engineering involving a good deal of trial and error (Gimpel and Smith, 2009)3 or by manually discretizing features (e.g. indicator features for count=N ). We will explore one technique for automatically avoiding non-linearities in Section 3. 2.3 Learning with Large Feature Sets While MERT has proven to be a strong baseline, it does not scale to larger feature sets in terms of both inefficiency and overfitting. While MIRA (Chiang et al., 2008), Rampion (Gimpel and Smith, 2012), and HOLS (Flanigan et al., 2013) have been shown to be effective over larger feature sets, they are difficult to explicitly regularize – this will become important in Section 4.2. Therefore, we"
Q14-1031,P12-1031,0,0.0761962,"for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured `p regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 20"
Q14-1031,H94-1052,0,0.0537078,"ns a shape that deviates from the log in several regions. Each non-monotonic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence align"
Q14-1031,D13-1176,0,0.0251975,", 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and demonstrate that discretization can be used to recover"
Q14-1031,P13-1078,0,0.363753,"Missing"
Q14-1031,I13-1032,0,0.229139,"Missing"
Q14-1031,C08-1064,0,0.0230693,"syntactic systems. Decoder: For decoding, we will use cdec (Dyer et al., 2010), a multi-pass decoder that supports syntactic translation models and sparse features. Optimizer: Optimization is performed using PRO (Hopkins and May, 2011) as implemented by the cdec decoder. We run PRO for 30 iterations as suggested by Hopkins and May (2011). The PRO optimizer internally uses a L-BFGS optimizer with the default `2 regularization implemented in cdec. Any additional regularization is explicitly noted. Baseline Features: We use the baseline features produced by Lopez’ suffix array grammar extractor (Lopez, 2008), which is distributed with cdec. 6 All code at http://github.com/jhclark/cdec 400 Bidirectional lexical log-probabilities, the coherent phrasal translation log-probability, target word count, glue rule count, source OOV count, target OOV count, and target language model logprobability. Note that these features may be simplified or removed as specified in each experimental condition. Zh→En Ar→En Cz→En Train 303K 5.4M 1M WeightTune 1664 1797 3000 HyperTune 1085 1056 2000 Test 1357 1313 2000 Table 1: Corpus statistics: number of parallel sentences. Chinese Resources: For the Chinese→English expe"
Q14-1031,P95-1037,0,0.0751733,"nic segment represents the learner choosing to better fit the data while paying a strong regularization penalty. 7 Related Work Previous work on feature discretization in machine learning has focused on the conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference ("
Q14-1031,D13-1024,0,0.0303365,"n proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured `p regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater than or equal to the previous point in the curve. Nearly isotonic regression allows violations in monotonicity (Tibshirani et al., 2011). 8 Conclusion In the absence of highly refined knowledge about a feature, discretization wit"
Q14-1031,W07-0710,0,0.65197,"log curve, especially for low probabilities, that lead to 10 We also keep a real-valued copy of the word penalty to help normalize the language model. 11 These features can single-out rules with c(s) = 1, c(s, t) = 1, subsuming separate low-count features Weight improvements 0.07 1.0in translation quality. 0.8 0.02 0.6 0.4 0.07 Weight For example, Cortes et al. (2009) investigated learning non-linear combinations of kernels. In MT, Gim´enez and M`arquez (2007) used a SVM to annotate a phrase table with binary features indicating whether or not a phrase translation was appropriate in context. Nguyen et al. (2007) also applied nonlinear features for SMT n-best reranking. 0.02 0.04 0.06 0.08 0.10 Original probability feature value 0.2 0.08 0.00.0 0 50 0.2 1000.4 150 0.6200 0.8 250 300 1.0 Original raw count feature value Figure 7: Plots of weights learned for the discretized pcoherent (e|f ) (top) and c(f ) (bottom) for the Ar→En system with 4 bits and monotone neighbor regularization. p(e|f ) > 0.11 is omitted for exposition as values were constant after this point. The gray line fits a log curve to the weights. The system learns a shape that deviates from the log in several regions. Each non-monotonic"
Q14-1031,P02-1038,0,0.432968,"on of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers. 1 Alon Lavie† Introduction Linear models using log-transformed probabilities as features have emerged as the dominant model in MT systems. This practice can be traced back to the IBM noisy channel models (Brown et al., 1993), which decompose decoding into the product of a translation model (TM) and a language model (LM), motivated by Bayes’ Rule. When Och and Ney (2002) introduced a log-linear model for translation (a linear sum of log-space features), they noted that the noisy channel model was a special case of their model using log probabilities. This ∗ This work was conducted as part of the first author’s Ph.D. work at Carnegie Mellon University. The community has abandoned the original motivations for a linear interpolation of two logtransformed features. Is there empirical evidence that we should continue using this particular transformation? Do we have any reason to believe it is better than other non-linear transformations? To answer these, we explor"
Q14-1031,P03-1021,0,0.157504,"Missing"
Q14-1031,P02-1040,0,0.0899745,"on NIST MT 2005, and test on NIST MT 2008. Czech resources: We also construct a Czech→English system based on the CzEng 1.0 data (Bojar et al., 2012). First, we lowercased and performed sentence-level deduplication of the data.7 Then, we uniformly sampled a training set of 1M sentences (sections 1 – 97) along with a weighttuning set (section 98), hyperparameter-tuning (section 99), and test set (section 99) from the paraweb domain contained of CzEng.8 Sentences less than 5 words were discarded due to noise. Evaluation: We quantify increases in translation quality using case-insensitive BLEU (Papineni et al., 2002). We control for test set variation and optimizer instability by averaging over multiple optimizer replicas (Clark et al., 2011).9 7 CzEng is distributed deduplicated at the document level, leading to very high sentence-level overlap. 8 The section splits recommended by Bojar et al. (2012). 9 MultEval 0.5.1: github.com/jhclark/multeval Bits Features Test BLEU 4 101 36.4 8 1302 36.6 12 12,910 36.8 Table 2: Translation quality for Cz→En system with varying bits for discretization. For all other experiments, we tune the number of bits on held-out data. Condition P log P Disc P Over. P LNR P MNR P"
Q14-1031,C12-2104,0,0.0192198,"tion for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalable solutions to integrating the hypothesis-level non-linear feature transforms typically associated with kernel methods while still maintaining polynomial time search. Another alternative is incorporating a recurrent neural network (Schwenk, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) or an additive neural network (Liu et al., 2013a). While these models have shown promise as methods of augmenting existing models, they have not yet offered a path for replacing or transforming existing real-valued features. In this article, we discuss background (§2), describe local discretization, our approach to learning non-linear transformations of individual features, compare it with globally non-linear models (§3), present our experimental setup (§5), empirically verify the importance of non-linear feature transformations in MT and de"
Q14-1031,P13-2004,0,0.0638803,"Missing"
Q14-1031,P13-2072,0,0.304047,"Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ahn (2013) use a form of regression decision trees to induce locally non-linear features in a n-best reranking framework. He and Deng (2012) directly optimize the lexical and phrasal features using expected BLEU. Nelakanti et al. (2013) use tree-structured `p regularizers to train language models and improve perplexity over Kneser-Ney. Learning parameters under weak order restrictions has also been studied for regression. Isotonic regression (Barlow et al., 1972; Robertson et al., 1988; Silvapulle and Sen, 2005) fits a curve to a set of data points such that each point in the fitted curve is greater tha"
Q14-1031,N07-2047,0,0.433472,"to believe it is better than other non-linear transformations? To answer these, we explore the issue of non-linearity in models for MT. In the process, we will discuss the impact of linearity on feature engineering and develop a general mechanism for learning a class of non-linear transformations of real-valued features. Applying a non-linear transformation such as log to features is one way of achieving a non-linear response function, even if those features are aggregated in a linear model. Alternatively, we could achieve a non-linear response using a natively nonlinear model such as a SVM (Wang et al., 2007) or RankBoost (Sokolov et al., 2012). However, MT is a structured prediction problem, in which a full hypothesis is composed of partial hypotheses. MT decoders take advantage of the fact that the model 393 Transactions of the Association for Computational Linguistics, 2 (2014) 393–404. Action Editor: Robert C. Moore. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. score decomposes as a linear sum over both local features and partial hypotheses to efficiently perform inference in these structured spaces (§2) – currently, there are no scalab"
Q14-1031,P04-1081,0,0.0362688,"e conversion of realvalued features into discrete values for learners that are either incapable of handling real-valued inputs or perform suboptimally given real-valued inputs (Dougherty et al., 1995; Kotsiantis and Kanellopoulos, 2006). Decision trees and random forests have been successfully used in language modeling (Jelinek et al., 1994; Xu and Jelinek, 2004) and parsing (Charniak, 2010; Magerman, 1995). Kernel methods such as support vector machines (SVMs) are often considered when non-linear interactions between features are desired since they allow for easy usage of non-linear kernels. Wu et al. (2004) showed improvements using non-linear kernel PCA for word sense disambiguation. Taskar et al. (2003) describes a method for incorporating kernels into structured Markov networks. Tsochantaridis et al. (2004) then proposed a structured SVM for grammar learning, named-entity recognition, text classification, and sequence alignment. This was followed by a structured SVM with inexact inference (Finley and Joachims, 2008) and the latent structured SVM (Yu and Joachims, 2009). Even within kernel methods, learning non-linear mappings with kernels remains an open area of research; 402 Toutanova and Ah"
Q14-1031,W04-3242,0,\N,Missing
Q14-1031,J93-2003,0,\N,Missing
Q14-1031,bojar-etal-2012-joy,0,\N,Missing
Q14-1031,D11-1125,0,\N,Missing
Q14-1031,N12-1023,0,\N,Missing
ries-etal-2000-shallow,J97-1003,0,\N,Missing
sagae-etal-2004-adding,W01-1816,1,\N,Missing
sagae-etal-2004-adding,W03-3019,1,\N,Missing
sagae-etal-2004-adding,A00-2018,0,\N,Missing
sagae-etal-2004-adding,N01-1006,0,\N,Missing
sagae-etal-2004-adding,P96-1025,0,\N,Missing
sagae-etal-2004-adding,P95-1037,0,\N,Missing
sagae-etal-2004-adding,rambow-etal-2002-dependency,0,\N,Missing
W00-0203,levin-etal-2000-lessons,1,0.873434,"Missing"
W01-1816,A97-1051,0,0.0695592,"Missing"
W01-1816,W97-1310,0,0.030633,"n process. The NEGRA Corpus (Skut, 1997) is another example of human-machine collaboration in annotating a corpus with syntactic and grammatical function information. In fact, a number of syntactically annotated corpora (or treebanks) have been produced in recent years, with varying amounts of automation, but typically with human effort playing a major role in the annotation process. Although the study of syntax and grammar can be viewed as the major global concern in all treebanks, some collections address specific concerns in natural language research, from the study of anaphora resolution (McEnery et al., 1997), to parser evaluation (Carrol et al., 1999). For a collection of papers on different treebanks, annotation techniques, methodologies and foci, please see (Garside et al., 1997). Our work differs in focus from these other projects in that our primary goal is to provide information specific to the process of human language acquisition. Although this information could be used in the training of automatic learning systems, we also aim to make it suitable for researchers in various aspects of linguistics and child development to draw reliable conclusions on the human language learning process. Bec"
W01-1816,J93-2004,0,0.0261902,"Missing"
W01-1816,A97-1014,0,0.0872551,"Missing"
W01-1816,W00-0743,0,0.0542053,"Missing"
W01-1816,J95-4004,0,\N,Missing
W02-0703,2000.iwpt-1.12,0,0.193328,"Missing"
W02-0703,H01-1007,1,0.857394,"Missing"
W02-0703,A00-1012,0,0.117687,"Missing"
W02-0703,woszczcyna-etal-1998-modular,1,0.876903,"Missing"
W02-0703,W00-0203,1,\N,Missing
W02-0708,J94-4004,0,0.0926924,"Missing"
W02-0708,W02-0717,1,0.890666,"Missing"
W02-0708,H01-1041,0,0.0396561,"Missing"
W02-0708,W00-0203,1,0.849921,"Missing"
W02-0708,lavie-etal-2002-nespole,1,\N,Missing
W02-0717,W02-0700,0,0.554922,"Missing"
W02-0717,costantini-etal-2002-nespole,1,0.876553,"Missing"
W02-0717,H01-1007,1,0.884986,"Missing"
W02-0717,W00-0203,1,0.867339,"Missing"
W02-0717,lavie-etal-2002-nespole,1,0.570647,"Missing"
W03-2118,2000.iwpt-1.12,0,0.0208218,"avel, and medical in Figure 3) show only a small increase in the number of domain actions Langley et al. (2002; Langley and Lavie, 2003) describe the hybrid analysis approach that is used in the NESPOLE! system (Lavie et al., 2002). The hybrid analysis approach combines grammar-based phrasal parsing and machine learning techniques to transform utterances into our interlingua representation. Our analyzer operates in three stages to identify the domain action and arguments. First, an input utterance is parsed into a sequence of arguments using phrase-level semantic grammars and the SOUP parser (Gavaldà, 2000). Four grammars are defined for argument parsing: an argument grammar, a pseudo-argument grammar, a cross-domain grammar, and a shared grammar. The argument grammar contains phrase-level rules for parsing arguments defined in the interlingua. The pseudo-argument grammar contains rules for parsing common phrases that are not covered by interlingua arguments. For example, all booked up, full, and sold out might be grouped into a class of phrases that indicate unavailability. The cross-domain grammar contains rules for parsing complete DAs that are domain independent. For example, this grammar co"
W03-2118,W03-3014,1,0.890479,"is partially captured with a direct or indirect speech act. However, whereas speech acts are generally domain independent, task-oriented language abounds with fixed expressions that have domain specific functions. For example, the phrases We have… or There are… in the hotel reservation domain express availability of rooms in addition to their more literal meanings of possession and existence. In the past six years, we have been successful in using domain specific domain actions as the basis for translation of limited-domain taskoriented spoken language (Levin et al., 1998, Levin et al. 2002; Langley and Lavie, 2003) 4 Scalability and Portability of Domain Actions Domain actions, like speech acts, convey speaker intention. However, domain actions also represent components of meaning and are therefore more numerous than domain independent speech acts. 1168 unique domain actions are used in our NESPOLE database, in contrast to only 72 speech acts. We show in this section that domain actions yield good coverage of task-oriented domains, that domain actions can be coded effectively by humans, and that scaling up to larger domains or porting to new domains is feasible without an explosion of domain actions. Co"
W03-2118,W02-0708,1,0.814441,". Speaker intention is partially captured with a direct or indirect speech act. However, whereas speech acts are generally domain independent, task-oriented language abounds with fixed expressions that have domain specific functions. For example, the phrases We have… or There are… in the hotel reservation domain express availability of rooms in addition to their more literal meanings of possession and existence. In the past six years, we have been successful in using domain specific domain actions as the basis for translation of limited-domain taskoriented spoken language (Levin et al., 1998, Levin et al. 2002; Langley and Lavie, 2003) 4 Scalability and Portability of Domain Actions Domain actions, like speech acts, convey speaker intention. However, domain actions also represent components of meaning and are therefore more numerous than domain independent speech acts. 1168 unique domain actions are used in our NESPOLE database, in contrast to only 72 speech acts. We show in this section that domain actions yield good coverage of task-oriented domains, that domain actions can be coded effectively by humans, and that scaling up to larger domains or porting to new domains is feasible without an explo"
W03-2118,C96-1061,0,0.0232167,"eech acts and an inventory of concepts. The allowable combinations of speech acts and concepts are formalized in a human- and machine-readable specification document. The specification document is supported by a database of over 14,000 tagged sentences in English, German, and Italian. The discourse community has long recognized the potential for improving NLP systems by identifying speaker intention. It has been hypothesized that predicting speaker intention of the next utterance would improve speech recognition (Reithinger et al., Stolcke et al.), or reduce ambiguity for machine translation (Qu et al., 1996, Qu et al., 1997). Identifying speaker intention is also critical for sentence generation. We argue in this paper that the explicit representation of speaker intention using domain actions can serve as the basis for an effective language-independent representation of meaning for speech-to-speech translation and that the relevant units of speaker intention are the domain specific domain action as well as the domain independent speech act. After a brief description of our database, we present linguistic motivation for domain actions. We go on to show that although domain actions are domain spec"
W03-2118,J00-3003,0,0.203575,"Missing"
W03-2118,W02-0703,1,\N,Missing
W03-3014,2000.iwpt-1.12,0,0.327111,"he analyzer must identify the domain action and arguments. The hybrid analyzer operates in three stages. First, semantic grammars are used to parse an utterance into a sequence of arguments. Next, the utterance is segmented into SDUs using memory-based learning (k-nearest neighbor) techniques. Finally, additional memory-based classifiers are used to identify the domain action (speech-act and sequence of concepts). 4.1 Argument Parsing The first step in our analysis approach is to parse an utterance for arguments. Utterances are parsed with phrase-level semantic grammars using the SOUP parser (Gavaldà, 2000). 4.1.1 The Parser SOUP is a stochastic, chart-based, top-down parser designed to provide real-time analysis of spoken language using context-free semantic grammars. SOUP provides several features that are useful for phrase-level argument parsing. One important feature provided by SOUP is word skipping. The amount of skipping allowed is configurable, and a list of words which cannot be skipped may be defined. Another critical feature for phrase-level parsing is the ability to produce analyses consisting of multiple parse trees. SOUP also supports modular grammar development (Woszczyna et al.,"
W03-3014,W00-0203,1,0.808066,"POLE! is to provide speech-translation for common users engaged in real-world e-commerce applications such as travel and tourism. NESPOLE! translates via an interlingua-based approach in four basic steps: (1) an automatic speech recognizer processes the spoken input; (2) the best-ranked text hypothesis from speech recognition is processed by the analyzer, producing an interlingua representation; (3) target language text is generated from the interlingua; and (4) the text is synthesized into speech. 3 The Interlingua The interlingua we use is called Interchange Format (IF) (Levin et al., 1998; Levin et al., 2000). The IF defines a shallow semantic representation for task-oriented utterances that abstracts away from language-specific syntax and idiosyncrasies while capturing the meaning of the input. Each utterance is divided into semantic segments called semantic dialog units (SDUs), and an IF is assigned to each SDU. An IF representation consists of four parts: a speaker tag, a speech act, an optional sequence of concepts, and an optional set of arguments. The representation takes the following form: &lt;speaker tag&gt;:&lt;speech act&gt; +&lt;concept&gt;* (&lt;argument&gt;*) The speaker tag indicates the role of the speake"
W03-3014,A00-1012,0,0.0175496,"e of the speech recognizer, all of the analyzer components, and the generator. 6 Related Work Lavie et al. (1997) developed a method for identifying SDU boundaries in a speech-to-speech translation system. The method combines acoustic information about silences and noises with a statistical model that uses three word-based bigram frequencies computed from a four-word window, in order to estimate the likelihood of an SDU boundary between each pair of words. Lexical cue phrases were then used to boost the likelihood estimate. Identifying SDU boundaries is similar to sentence boundary detection. Stevenson and Gaizauskas (2000) point out that text produced by a speech recognizer differs in important ways from standard text composed by humans. Unlike standard text, speech recognizer output typically contains no punctuation or case information. Furthermore, spoken language often contains phrases and sentence fragments. Finally, speech recognizer output may contain errors. Stevenson and Gaizauskas (2000) use TiMBL (Daelemans et al., 2000) to identify sentence boundaries in automatic speech recognizer output, and Gotoh and Renals (2000) use a statistical approach to identify sentence boundaries in automatic speech recog"
W03-3014,woszczcyna-etal-1998-modular,1,0.837639,"er (Gavaldà, 2000). 4.1.1 The Parser SOUP is a stochastic, chart-based, top-down parser designed to provide real-time analysis of spoken language using context-free semantic grammars. SOUP provides several features that are useful for phrase-level argument parsing. One important feature provided by SOUP is word skipping. The amount of skipping allowed is configurable, and a list of words which cannot be skipped may be defined. Another critical feature for phrase-level parsing is the ability to produce analyses consisting of multiple parse trees. SOUP also supports modular grammar development (Woszczyna et al., 1998). Subgrammars designed for different domains or purposes can be developed separately and applied in parallel during parsing. Parse tree nodes are then marked with a subgrammar label. When an input can be parsed in multiple ways, SOUP can provide a ranked list of interpretations. In the version of the analyzer described here, word skipping is only allowed between parse trees, and only the best-ranked argument parse is used. 4.1.2 The Grammars Four grammars are defined for argument parsing: an argument grammar, a pseudo-argument grammar, a cross-domain grammar, and a shared grammar. The argument"
W03-3014,W02-0703,1,\N,Missing
W03-3019,P92-1024,0,0.0833088,"Missing"
W03-3019,A00-2031,0,0.0126436,"that this combination of the rulebased and data-driven systems outperforms either system in isolation. 4 Related Work Carroll and Briscoe (2002) present a wide-coverage parser that outputs grammatical relations, and discuss the trade-off between precision and recall of grammatical relations, as well as useful ways to manipulate such trade-off to achieve high precision at the expense of recall. This trade-off is also observed in our experiments. However, our angle on this issue focuses on the combination of systems with different precision/recall behavior, to achieve a higher combined F-score. Blaheta and Charniak (2000) discuss the assignment of Penn Treebank (Marcus et al., 1993) function tags to constituent structure trees. They use a statistical approach to assign tags (similar in many ways to grammatical relations) to parse tree nodes. Our corpus-based model also uses parse trees, but only to determine that a GR exists between two words. The work of Gildea and Palmer (2002) has shown that the use of constituent structure information is useful in determining predicateargument structure. While their work involved propositions of a more semantic nature, we believe their results to be applicable to the ident"
W03-3019,C02-1013,0,0.0247492,"d affect the overall performance of the Grammatical Relation Number of instances in test set Subject Object Adjunct Predicate nominal 20 14 8 3 Table 5: Number of instances of each GR in failed sentences Grammatical Relation Precision Recall F-score Subject Object Adjunct Predicate nominal 0.60 0.80 0.75 0.00 0.60 0.57 0.37 0.00 0.60 0.67 0.50 0.00 Table 6: Results using the data-driven system on failed sentences combined system. However, the results as they stand already show that this combination of the rulebased and data-driven systems outperforms either system in isolation. 4 Related Work Carroll and Briscoe (2002) present a wide-coverage parser that outputs grammatical relations, and discuss the trade-off between precision and recall of grammatical relations, as well as useful ways to manipulate such trade-off to achieve high precision at the expense of recall. This trade-off is also observed in our experiments. However, our angle on this issue focuses on the combination of systems with different precision/recall behavior, to achieve a higher combined F-score. Blaheta and Charniak (2000) discuss the assignment of Penn Treebank (Marcus et al., 1993) function tags to constituent structure trees. They use"
W03-3019,A00-2018,0,0.524935,"ontained in the words themselves is lost in the assignment of GR tags to part-of-speech tags alone, the output of this tagger is further refined by an error-driven transformation-based learning strategy that takes the actual words into account, implemented using the fnTBL toolkit (Ngai & Florian, 2001). While the above GR “tagger” can assign grammatical relation labels to words, we still have to determine the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank constituent head table” originally designed by Magerman (1995) is then used to determine the heads of constituents in the parse tree. By stipulating that there is a directional link from every word in a constituent (except for the head) to the head of the constituent, and applying that notion to the entire parse-tree, we determine a set of unlabeled dependency links for the sentence. The combination of th"
W03-3019,P02-1031,0,0.0308609,"on at the expense of recall. This trade-off is also observed in our experiments. However, our angle on this issue focuses on the combination of systems with different precision/recall behavior, to achieve a higher combined F-score. Blaheta and Charniak (2000) discuss the assignment of Penn Treebank (Marcus et al., 1993) function tags to constituent structure trees. They use a statistical approach to assign tags (similar in many ways to grammatical relations) to parse tree nodes. Our corpus-based model also uses parse trees, but only to determine that a GR exists between two words. The work of Gildea and Palmer (2002) has shown that the use of constituent structure information is useful in determining predicateargument structure. While their work involved propositions of a more semantic nature, we believe their results to be applicable to the identification of grammatical relations. 5 Conclusions and Future Work We have presented a way to combine rule-based and data-driven NLP techniques in the extraction of grammatical relations. We have shown that starting with a rule-based system, we can use unlabeled data and a corpus-based system to improve recall (and F-score) of grammatical relations. While the expe"
W03-3019,P95-1037,0,0.106403,"ed using the fnTBL toolkit (Ngai & Florian, 2001). While the above GR “tagger” can assign grammatical relation labels to words, we still have to determine the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank constituent head table” originally designed by Magerman (1995) is then used to determine the heads of constituents in the parse tree. By stipulating that there is a directional link from every word in a constituent (except for the head) to the head of the constituent, and applying that notion to the entire parse-tree, we determine a set of unlabeled dependency links for the sentence. The combination of the unlabeled links and the GR labels results in our target output of grammatical relations. 2.3 Identifying Grammatical Relations with Rule-Based and Data-Driven Methods Once we have established a corpus-based procedure that makes GR assignments to every"
W03-3019,J93-2004,0,0.0449404,"e assignment of GR tags to part-of-speech tags alone, the output of this tagger is further refined by an error-driven transformation-based learning strategy that takes the actual words into account, implemented using the fnTBL toolkit (Ngai & Florian, 2001). While the above GR “tagger” can assign grammatical relation labels to words, we still have to determine the target of the directional link established by the grammatical relation indicated by the label. To accomplish this, the raw text input sentence was also parsed using a statistical parser (Charniak, 2000) trained on the Penn Treebank (Marcus et al., 1993), yielding an approximation of the skeletal constituent structure (parse tree) of the sentence. A slightly modified version of the “treebank constituent head table” originally designed by Magerman (1995) is then used to determine the heads of constituents in the parse tree. By stipulating that there is a directional link from every word in a constituent (except for the head) to the head of the constituent, and applying that notion to the entire parse-tree, we determine a set of unlabeled dependency links for the sentence. The combination of the unlabeled links and the GR labels results in our"
W03-3019,W01-1816,1,0.761875,"Missing"
W04-0107,carbonell-etal-2002-automatic,1,0.892932,"Missing"
W04-0107,W99-0904,0,0.186112,"ame roam solve show sow saw sing ring -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings →sang/eI/ V -/d/ -/d/ blamed roamed solved showed sowed sawed Perfective or Passive -/d/ -/n/ blamed roamed solved shown sown sawn -/i / -/i / -/i / Progressive blaming roaming solving showing sowing sawing singing ringing Past ŋ ŋ rang →sung/Λ/ V rung ŋ Table 1: A few inflection classes of the English verb paradigm ity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and b"
W04-0107,J01-2001,0,0.314787,"m a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus. Goldsmith (2001), by searching over a space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography. Segmenting word forms in a corpus, Goldsmith creates an inventory of stems and suffixes. Suffixes which can interchangeably concatenate onto a set of stems form a signature. After defining the space of signatures, Goldsmith searches for that choice of word segmentations resulting in a minimum description length local optimum. Finally, the work of Harris (1955; 1967), and later Hafer and Weiss (1974), has direct bearing on the approach taken in this paper. Couched in"
W04-0107,P04-2012,1,0.332363,"rently pursuing MT systems with Mapudungun, an indigenous language spoken by 900,000 people in southern Chile and Argentina, and Aymara, spoken by 3 million people in Bolivia, Peru, and northern Chile, as lowdensity languages and Spanish the resource rich language. A vital first step in a rule-based machine translation system is morphological analysis. This paper outlines a framework for automatic natural language morphology induction inspired by the traditional and linguistic concept of inflection classes. Additional details concerning the candidate inflection class framework can be found in Monson (2004). This paper then goes on to describe one implemented search strategy within this framework, presenting both a simple summary of results and an in depth error analysis. While the intent of this research direction is to define techniques applicable to low-density languages, this paper employs English to illustrate the main conjectures and Spanish, a language with a reasonably complex morphological system, for quantitative analysis. All experiments detailed in this paper are over a Spanish newswire corpus of 40,011 tokens and 6,975 types. 2 Previous Work It is possible to organize much of the re"
W04-0107,W00-0712,0,0.261239,"phic shape of related word forms. Next along the spectrum of orthographic similarVerb Paradigm Basic 3rd Person Singular Non-past Inflection Classes A B C blame roam solve show sow saw sing ring -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings →sang/eI/ V -/d/ -/d/ blamed roamed solved showed sowed sawed Perfective or Passive -/d/ -/n/ blamed roamed solved shown sown sawn -/i / -/i / -/i / Progressive blaming roaming solving showing sowing sawing singing ringing Past ŋ ŋ rang →sung/Λ/ V rung ŋ Table 1: A few inflection classes of the English verb paradigm ity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work"
W04-0107,N01-1024,0,0.634042,"who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus. Goldsmith (2001), by searching over a space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography. Segmenting word forms in a corpus, Goldsmith creates an inventory of stems and suffixes. Suffixes which can interchangeably concatenate onto a set of stems form a signature. After defining the space of signatures, Goldsmith searches for that choice of word segmentations resulting in a minimum description le"
W04-0107,H01-1035,0,0.0603411,"efine techniques applicable to low-density languages, this paper employs English to illustrate the main conjectures and Spanish, a language with a reasonably complex morphological system, for quantitative analysis. All experiments detailed in this paper are over a Spanish newswire corpus of 40,011 tokens and 6,975 types. 2 Previous Work It is possible to organize much of the recent work on unsupervised morphology induction by considering the bias each approach has toward discovering morphologically related words that are also orthographically similar. At one end of the spectrum is the work of Yarowsky et al. (2001), who derive a morphological analyzer for a language, L, by projecting the morphological analysis of a resource-rich language onto L through a clever application of statistical machine translation style word alignment probabilities. The word alignments are trained over a sentence aligned parallel bilingual text for the language pair. While the probabilistic model they use to generalize their initial system contains a bias toward orthographic similarity, the unembellished algorithm contains no assumptions on the orthographic shape of related word forms. Next along the spectrum of orthographic s"
W05-0909,2003.mtsummit-papers.30,0,0.0779016,"the development of the system based on concrete performance improvements. Evaluation of Machine Translation has traditionally been performed by humans. While the main criteria that should be taken into account in assessing the quality of MT output are fairly intuitive and well established, the overall task of MT evaluation is both complex and task dependent. MT evaluation has consequently been an area of significant research in itself over the years. A wide range of assessment measures have been proposed, not all of which are easily quantifiable. Recently developed frameworks, such as FEMTI (King et al, 2003), are attempting to devise effective platforms for combining multi-faceted measures for MT evaluation in effective and user-adjustable ways. While a single one-dimensional numeric metric cannot hope to fully capture all aspects of MT We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advance"
W05-0909,lavie-etal-2004-significance,1,0.102171,"on a combination of several features. These currently include unigram-precision, unigram-recall, and a direct measure of how out-oforder the words of the MT output are with respect to the reference. The score assigned to each individual sentence of MT output is derived from the best scoring match among all matches over all reference translations. The maximal-scoring match1 ing is then also used in order to calculate an aggregate score for the MT system over the entire test set. Section 2 describes the metric in detail, and provides a full example of the matching and scoring. In previous work (Lavie et al., 2004), we compared METEOR with IBM's BLEU metric and it’s derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments. We demonstrated that METEOR has significantly improved correlation with human judgments. Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments. The previous analysis focused on correlation with human judgments at the system level. In this"
W05-0909,N03-1024,0,0.0745392,"are not quite synonyms of each other. More Effective Use of Multiple Reference Translations: Our current metric uses multiple reference translations in a weak way: we compare the translation with each reference separately and select the reference with the best match. This was necessary in order to incorporate recall in our metric, which we have shown to be highly advantageous. As our matching approach improves, the need for multiple references for the metric may in fact diminish. Nevertheless, we are exploring ways in which to improve our matching against multiple references. Recent work by (Pang et al, 2003) provides the mechanism for producing semantically meaningful additional “synthetic” references from a small set of real references. We plan to explore whether using such synthetic references can improve the performance of our metric. Weigh Matches Produced by Different Modules Differently: Our current multi-stage approach prefers metric imposes a priority on the different matching modules. However, once all the stages have been run, unigrams mapped through different mapping modules are treated the same. Another 72 approach to treating different mappings differently is to apply different weigh"
W05-0909,P02-1040,0,0.14152,"Missing"
W05-0909,2003.mtsummit-papers.51,0,0.100987,"l in the reference translation respectively) are said to cross if and only if the following formula evaluates to a negative number: (pos(ti) – pos(tk)) * (pos(rj) – pos(rl)) where pos(tx) is the numeric position of the unigram tx in the system translation string, and pos(ry) is the numeric position of the unigram ry in the reference string. For a given alignment, every pair of unigram mappings is evaluated as a cross or not, and the alignment with the least total crosses is selected in this second phase. Note that these two phases together constitute a variation of the algorithm presented in (Turian et al, 2003). Each stage only maps unigrams that have not been mapped to any unigram in any of the preceding stages. Thus the order in which the stages are run imposes different priorities on the mapping modules employed by the different stages. That is, if the first stage employs the “exact” mapping module and the second stage employs the “porter stem” module, METEOR is effectively preferring to first map two unigrams based on their surface forms, and performing the stemming only if the surface forms do not match (or if the mapping based on surface forms was too “costly” in terms of the total number of c"
W05-1513,W00-0735,0,0.044751,"Missing"
W05-1513,A00-2018,0,0.869294,"Missing"
W05-1513,P97-1003,0,0.337194,"Missing"
W05-1513,P02-1031,0,0.0122831,"lowing parsing decisions made by a classifier. Despite their greedy nature, these parsers achieve high accuracy in determining dependencies. Although state-of-the-art statistical parsers (Collins, 1997; Charniak, 2000) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data. However, dependency analyses lack important information contained in constituent structures. For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). We present a parser that shares much of the simplicity and efficiency of the deterministic dependency parsers, but produces both dependency and constituent structures simultaneously. Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time. While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to"
W05-1513,W04-3203,0,0.165259,"Missing"
W05-1513,W04-3239,0,0.0472103,"Missing"
W05-1513,N01-1025,0,0.0966019,"Missing"
W05-1513,J98-4004,0,0.1162,"work we focus only on the processing that occurs once POS tagging is completed. In the sections that follow, we assume that the input to the parser is a sentence with corresponding POS tags for each word. 2 Parser Description Our parser employs a basic bottom-up shift-reduce parsing algorithm, requiring only a single pass over the input string. The algorithm considers only 126 trees with unary and binary branching. In order to use trees with arbitrary branching for training, or generating them with the parser, we employ an instance of the transformation/detransformation process described in (Johnson, 1998). In our case, the transformation step involves simply converting each production with n children (where n &gt; 2) into n – 1 binary productions. Trees must be lexicalized1, so that the newly created internal structure of constituents with previous branching of more than two contains only subtrees with the same lexical head as the original constituent. Additional nonterminal symbols introduced in this process are clearly marked. The transformed (or “binarized”) trees may then be used for training. Detransformation is applied to trees produced by the parser. This involves the removal of non-termin"
W05-1513,J93-2004,0,0.0355478,"Missing"
W05-1513,C04-1010,0,0.424712,"d of the new item is either the lexical head of its left child, or the lexical head of its right child. If S is empty, only a shift action is allowed. If W is empty, only a reduce action is allowed. If both S and W are non-empty, either shift or reduce actions are possible. Parsing terminates when W is empty and S contains only one item, and the single item in S is the parse tree for the input string. Because the parse tree is lexicalized, we also have a dependency structure for the sentence. In fact, the binary reduce actions are very similar to the reduce actions in the dependency parser of Nivre and Scholz (2004), but they are executed in a different order, so constituents can be built. If W is empty, and more than one item remain in S, and no further reduce actions take place, the input string is rejected. 127 2.2 Determining Actions with a Classifier A parser based on the algorithm described in the previous section faces two types of decisions to be made throughout the parsing process. The first type concerns whether to shift or reduce when both actions are possible, or whether to reduce or reject the input when only reduce actions are possible. The second type concerns what syntactic structures are"
W05-1513,W03-3023,0,0.143752,"Missing"
W05-1513,P01-1069,0,\N,Missing
W07-0604,J96-1002,0,0.00259386,"the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing approach is very similar to the one used successfully by Nivre et al. (2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast. In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.’s approach is deterministic. See Sagae and Tsujii (2007) for more information on the parser. Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser’s current configuration (contents of the stack and queue) at that point. The specific features used are:4 • Word and its POS tag: s(1), q(2), and q(1). • POS: s(3) and q(2)."
W07-0604,W06-2920,0,0.0359076,"arsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combin"
W07-0604,A00-2018,0,0.284917,"ng punctuation) and 65,363 words. The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall. The annotated Eve corpus is available at http://childes.psy.cmu. edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007). 3 Parsing Although the CHILDES annotation scheme proposed by Sagae et al. (2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing a"
W07-0604,W06-2933,0,0.0264284,"used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing approach is very similar to the one used successfully by Nivre et al. (2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast. In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.’s approach is deterministic. See Sagae and Tsujii (2007) for more information on the parser. Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser’s current configuration (contents of the stack and queue) at that point. The specific features used are:4 • Wor"
W07-0604,P06-2089,1,0.365037,"reebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic"
W07-0604,D07-1111,1,0.215691,"bilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing a"
W07-0604,sagae-etal-2004-adding,1,0.867299,"Missing"
W07-0604,P05-1025,1,0.927916,"0 adult and 8,563 child. The utterances consist of 84,226 GRs (including punctuation) and 65,363 words. The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall. The annotated Eve corpus is available at http://childes.psy.cmu. edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007). 3 Parsing Although the CHILDES annotation scheme proposed by Sagae et al. (2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essenti"
W07-0734,lavie-etal-2004-significance,1,0.215524,"Missing"
W07-0734,E06-1031,0,0.0108199,"T system development. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. 2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation. If more than one reference translation is available, the translation is scored against each reference independently, and the best scoring pair is used. Given a pair of strings to be compared, Meteor creates a word alignment between the two strings. An alignment is mapping between words, such that every word in each string maps to at most one word in the other string. This alignment is"
W07-0734,N03-2021,0,0.0121068,"and can be applied on a frequent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. 2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation. If more than one reference translation is available, the translation is scored against each reference independently, and the best scoring pair is used. Given a pair of strings to be compared, Meteor creates a word alignment between the two strings. An alignment is mapping between words, such that every word in each string maps t"
W07-0734,P03-1021,0,0.126206,"ics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. 2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation. If more than one reference translation is available, the translation is scored against each ref"
W07-0734,P02-1040,0,0.113715,"ish, French and German, in addition to English. 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years. Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. 2 The Meteor Metric Meteor evaluates a translation by computing a scor"
W07-0734,2006.amta-papers.25,0,0.174165,"uent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. 2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation. If more than one reference translation is available, the translation is scored against each reference independently, and the best scoring pair is used. Given a pair of strings to be compared, Meteor creates a word alignment between the two strings. An alignment is mapping between words, such that every word in each string maps to at most one word in the o"
W07-0734,W05-0909,1,\N,Missing
W07-0734,C04-1046,0,\N,Missing
W07-0734,D08-1076,0,\N,Missing
W07-0909,N03-1003,0,0.0109416,"wo entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the ex"
W07-0909,itai-etal-2006-computational,1,0.829879,"l the partial templates of a learned template. These are templates that contain just one of variables in the original template. We then generate rules between these partial templates that correspond to the original rules. With partial templates/rules, expansion for the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Yona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different compone"
W07-0909,2004.tmi-1.1,1,0.912872,"r the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Yona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different components of the system: Hebrew documents are morphologically processed to provide better indexing; query terms in English are translated to Hebrew and vice versa; and Hebrew snippets are translated to English. We discuss each of these components in this section. Linguis"
W07-0909,P02-1006,0,0.208246,"the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may b"
W07-0909,E06-1052,1,0.803107,"ed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew texts, but still benefit"
W07-0909,I05-5011,0,0.0136238,"trieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentences are parsed an"
W07-0909,P03-1029,0,0.0952938,"phrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew te"
W07-0909,W04-3206,1,0.810829,"IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentenc"
W07-0909,C02-1161,0,0.0245238,"n Retrieval (IR), it is crucial to recognize that a specific target meaning can be inferred from different text variants. For example, a QA system needs to induce that “Mendelssohn wrote incidental music” can be inferred from “Mendelssohn composed incidental music” in order to answer the question “Who wrote incidental music?”. This type of reasoning has been identified as a core semantic in66 ference task by the generic textual entailment framework (Dagan et al., 2006; Bar-Haim et al., 2006). The typical way to address variability in IR is to use lexical query expansion (Lytinen et al., 2000; Zukerman and Raskutti, 2002). However, there are variability patterns that cannot be described using just constant phrase to phrase entailment. Another important type of knowledge representation is entailment rules and paraphrases. An entailment rule is a directional relation between two templates, text patterns with variables, e.g., ‘X compose Y → X write Y ’. The left hand side is assumed to entail the right hand side in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building block"
W07-1315,J01-2001,0,0.538661,"scarding the large number of erroneous initially selected candidate inflection classes. Finally, with a strong grasp on the paradigm structure, ParaMor straightforwardly segments the words of a corpus into morphemes. 1.3 Related Work In this section we highlight previously proposed minimally supervised approaches to the induction of morphology that, like ParaMor, draw on the unique structure of natural language morphology. One facet of NL morphological structure commonly leveraged by morphology induction algorithms is that morphemes are recurrent building blocks of words. Brent et al. (1995), Goldsmith (2001), and Creutz (2006) emphasize the building block nature of morphemes when they each use recurring word segments to efficiently encode a corpus. These approaches then hypothesize that those recurring segments which most efficiently encode a corpus are likely morphemes. Another technique that exploits morphemes as repeating sub-word segments encodes the lexemes of a corpus as a character tree, i.e. trie, (Harris, 1955; Hafer and Weis, 1974), or as a finite state automaton (FSA) over characters (Johnson, H. and Martin, 119 2003; Altun and M. Johnson, 2001). A trie or FSA conflates multiple instan"
W07-1315,N01-1024,0,\N,Missing
W07-1315,N03-2015,0,\N,Missing
W08-0312,W05-0909,1,0.539132,"art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English. This paper provides a brief technical description of Meteor and describes our experiments in re-tuning the metric for improving correlation with the human rankings of translation hypothe"
W08-0312,W07-0718,0,0.0295304,"we extend the Bleu and Ter metrics to use the stemming and Wordnet based word mapping modules from Meteor . Given a translation hypothesis and reference pair, we first align them using the word mapping modules from Meteor . We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute Bleu and Ter with these new references without changing anything inside the metrics. To get meaningful Bleu scores at segment level, we compute smoothed Bleu as described in (Lin and Och, 2004). 4 Re-tuning Meteor for Rankings (Callison-Burch et al., 2007) reported that the intercoder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation. Based on that finding, in WMT-08, only ranking judgments are being collected from the human judges. The current version of Meteor uses parameters optimized towards maximizing the Pearson’s correlation with human judgments of adequacy scores. It is not clear that the same parameters would be optimal for correlation with human rankings. So we would like to re-tune the parameters in"
W08-0312,W07-0734,1,0.635581,"(Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English. This paper provides a brief technical description of Meteor and describes our experiments in re-tuning the metric for improving correlation with the human rankings of translation hypotheses corresponding to a sin"
W08-0312,lavie-etal-2004-significance,1,0.833006,"pineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English. This"
W08-0312,E06-1031,0,0.0135543,"bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with hu"
W08-0312,C04-1072,0,0.0581001,"ble matching criteria. In the following experiments, we extend the Bleu and Ter metrics to use the stemming and Wordnet based word mapping modules from Meteor . Given a translation hypothesis and reference pair, we first align them using the word mapping modules from Meteor . We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute Bleu and Ter with these new references without changing anything inside the metrics. To get meaningful Bleu scores at segment level, we compute smoothed Bleu as described in (Lin and Och, 2004). 4 Re-tuning Meteor for Rankings (Callison-Burch et al., 2007) reported that the intercoder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation. Based on that finding, in WMT-08, only ranking judgments are being collected from the human judges. The current version of Meteor uses parameters optimized towards maximizing the Pearson’s correlation with human judgments of adequacy scores. It is not clear that the same parameters would be optimal for correlation wit"
W08-0312,P06-2070,0,0.0168491,"ment between the two strings is calculated as: The free parameters in the metric, α, β and γ are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007). 3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu (Papineni et al., 2002) and Ter (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor . Most of them, however, depend on finding exact matches between the words in two strings. Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria. In the following experiments, we extend the Bleu and Ter metrics to use the stemming and Wordnet based word mapping modules from Meteor . Given a translation hypothesis and reference pair, we first align them using the word mapping modules from Meteor . We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute Bleu and Ter with these new references without changing anything inside the metrics. To get meaningful Bleu scores at segme"
W08-0312,J82-2005,0,0.763994,"Missing"
W08-0312,P03-1021,0,0.00873278,"temming and Wordnet in Meteor . 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years. Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Ag"
W08-0312,P02-1040,0,0.109798,"also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor . 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years. Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004)"
W08-0312,2006.amta-papers.25,0,0.252801,"uations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s Bleu metric (Papineni et al., 2002). Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, Bleu does not produce very reliable sentence-level scores. Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to o"
W08-0312,W07-0736,0,0.0182484,"ing the preferred hypothesis between a given pair. There are also cases where both the hypotheses in the pair are judged to be equal. In order to convert these binary judgments into full rankings, we do the following: 2. Construct a directed graph where nodes correspond to the translation hypotheses and every binary judgment is represented by a directed edge between the corresponding nodes. 3. Do a topological sort on the resulting graph and assign ranks in the sort order. The cycles in the graph are broken by assigning same rank to all the nodes in the cycle. Measuring Correlation Following (Ye et al., 2007), we first compute the Spearman correlation between the human rankings and Meteor rankings of the translation hypotheses corresponding to a single source sentence. Let N be the number of translation hypotheses and D be the difference in ranks assigned to a hypothesis by two rankings, then Spearman correlation is given by: P 6 D2 r =1− N (N 2 − 1) The final score for the metric is the average of the Spearman correlations for individual sentences. 5 5.1 Experiments Data We use the human judgment data from WMT-07 which was released as development data for the evaluation shared task. Amount of dat"
W08-0312,N03-2021,0,\N,Missing
W08-0312,D08-1076,0,\N,Missing
W08-0324,W01-1819,0,0.0494464,"Missing"
W08-0324,W07-0718,0,0.102022,"Missing"
W08-0324,W07-0734,1,0.802564,"i et al., 2002) with several iterations of minimum error rate training on n-best lists. In each iteration’s list, we also included the lists from previous iterations in order to maintain a diversity of hypothesis types and scores. The provided “test2007” and “nc-test2007” data sets, identical with the test data from the 2007 Workshop on Statistical Machine Translation shared task, were used as internal development tests. Tables 2, 3, and 4 report scores on these data sets for our primary French, secondary French, and German systems. We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al., 2002), and version 5 of TER (Snover et al., 2006). 165 METEOR 0.5332 0.5358 0.5369 BLEU 0.2063 0.2078 0.1719 TER 64.81 64.75 69.83 Table 2: Results for the primary French–English system on provided development and development test sets. Data Set dev2006 test2007 nc-test2007 METEOR 0.5330 0.5386 0.5311 BLEU 0.2086 0.2129 0.1680 TER 65.02 64.29 70.90 Table 3: Results for the secondary French–English system on provided development and development test sets. 4 Analysis and Conclusions From the development test results in S"
W08-0324,W08-0411,1,0.879519,"ation task. We first describe the acquisition and processing of resources for each language pair and the roles of those resources within the Stat-XFER system (Section 2); we then report results on common test sets Building a new machine translation system under the Stat-XFER framework involves constructing a bilingual translation lexicon and a transfer grammar. Over the past six months, we have developed new methods for extracting syntax-based translation lexicons and transfer rules fully automatically from parsed and word-aligned parallel corpora. These new methods are described in detail by Lavie et al. (2008). Below, we detail the statistical methods by which these resources were extracted for our French–English and German–English systems. 2.1 Lexicon The bilingual lexicon is automatically extracted from automatically parsed and word-aligned parallel corpora. To obtain high-quality statistical word alignments, we run GIZA++ (Och and Ney, 2003) in both the source-to-target and target-to-source directions, then combine the resulting alignments with the Sym2 symmetric alignment heuristic of OrtizMart´ınez et al. (2005)1 . From this data, we extract a lexicon of both word-to-word and syntactic phraset"
W08-0324,J03-1002,0,0.00425181,"d a transfer grammar. Over the past six months, we have developed new methods for extracting syntax-based translation lexicons and transfer rules fully automatically from parsed and word-aligned parallel corpora. These new methods are described in detail by Lavie et al. (2008). Below, we detail the statistical methods by which these resources were extracted for our French–English and German–English systems. 2.1 Lexicon The bilingual lexicon is automatically extracted from automatically parsed and word-aligned parallel corpora. To obtain high-quality statistical word alignments, we run GIZA++ (Och and Ney, 2003) in both the source-to-target and target-to-source directions, then combine the resulting alignments with the Sym2 symmetric alignment heuristic of OrtizMart´ınez et al. (2005)1 . From this data, we extract a lexicon of both word-to-word and syntactic phraseto-phrase translation equivalents. The word-level correspondences are extracted directly from the word alignments: parts of speech for these lexical entries are obtained from the preter1 We use Sym2 over more well-known heuristics such as “grow-diag-final” because Sym2 has been shown to give the best results for the node-alignment subtask t"
W08-0324,2005.mtsummit-papers.19,0,0.0502124,"Missing"
W08-0324,P02-1040,0,0.0758417,"d target-given-source lexical probabilities, parse fragmentation, and length. For more details, see Lavie (2008). The use of a German transfer grammar an order of magnitude larger than the corresponding French grammar was made possible due to a recent optimization made in the engine. When enabled, it constrains the search of translation hypotheses to the space of hypotheses whose structure satisfies the consituent structure of a source-side parse. 3 Evaluation We trained our model parameters on a subset of the provided “dev2006” development set, optimizing for case-insensitive IBM-style BLEU (Papineni et al., 2002) with several iterations of minimum error rate training on n-best lists. In each iteration’s list, we also included the lists from previous iterations in order to maintain a diversity of hypothesis types and scores. The provided “test2007” and “nc-test2007” data sets, identical with the test data from the 2007 Workshop on Statistical Machine Translation shared task, were used as internal development tests. Tables 2, 3, and 4 report scores on these data sets for our primary French, secondary French, and German systems. We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarw"
W08-0324,2006.amta-papers.25,0,0.0168124,"d the lists from previous iterations in order to maintain a diversity of hypothesis types and scores. The provided “test2007” and “nc-test2007” data sets, identical with the test data from the 2007 Workshop on Statistical Machine Translation shared task, were used as internal development tests. Tables 2, 3, and 4 report scores on these data sets for our primary French, secondary French, and German systems. We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al., 2002), and version 5 of TER (Snover et al., 2006). 165 METEOR 0.5332 0.5358 0.5369 BLEU 0.2063 0.2078 0.1719 TER 64.81 64.75 69.83 Table 2: Results for the primary French–English system on provided development and development test sets. Data Set dev2006 test2007 nc-test2007 METEOR 0.5330 0.5386 0.5311 BLEU 0.2086 0.2129 0.1680 TER 65.02 64.29 70.90 Table 3: Results for the secondary French–English system on provided development and development test sets. 4 Analysis and Conclusions From the development test results in Section 3, we note that the Stat-XFER systems’ performance currently lags behind the state-of-the-art scores on the 2007 test"
W08-0411,P05-1033,0,0.148987,"mbati vambati@cs.cmu.edu Introduction Phrase-based Statistical MT (PB-SMT) (Koehn et al., 2003) has become the predominant approach to Machine Translation in recent years. PB-SMT requires broad-coverage databases of phrase-to-phrase translation equivalents. These are commonly acquired from large volumes of automatically wordaligned sentence-parallel text corpora. Accurate identification of sub-sentential translation equivalents, however, is a critical process in all data-driven MT approaches, including a variety of data-driven syntax-based approaches that have been developed in recent years. (Chiang, 2005) (Imamura et al., 2004) (Galley et al., 2004). In this paper, we describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees for both languages. Our method consists of three steps. In the first step, we apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees. In the second step, we extract all alig"
W08-0411,N04-1035,0,0.242488,"Phrase-based Statistical MT (PB-SMT) (Koehn et al., 2003) has become the predominant approach to Machine Translation in recent years. PB-SMT requires broad-coverage databases of phrase-to-phrase translation equivalents. These are commonly acquired from large volumes of automatically wordaligned sentence-parallel text corpora. Accurate identification of sub-sentential translation equivalents, however, is a critical process in all data-driven MT approaches, including a variety of data-driven syntax-based approaches that have been developed in recent years. (Chiang, 2005) (Imamura et al., 2004) (Galley et al., 2004). In this paper, we describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees for both languages. Our method consists of three steps. In the first step, we apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees. In the second step, we extract all aligned sub-sentential syntactic constituents fro"
W08-0411,P06-1121,0,0.0621441,"cessing step for PB-SMT (and other approaches), to decrease the word-order and syntactic distortion between the source and target languages (Xia and McCord, 2004). A variety of hierarchical and syntax-based models, which are applied during decoding, have also been developed. Many of these approaches involve automatic learning and extraction of the underlying syntax-based rules from data. The underlying formalisms used has been quite 92 broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al., 2004) and (Galley et al., 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al., 2002) and others. Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus, typically the target side. (Hearne and Way, 2003) describes an approach that uses syntactic information from the source side to derive reordering subtrees, which can then be used within a “data-oriented translation” (DOT) MT system, similar in framewo"
W08-0411,C04-1154,0,0.218771,"gning nodes in parallel trees has been investigated by a number of previous researchers. (Samuelsson and Volk, 2007) describe a process for manual alignment of nodes in parallel trees. This approach is well suited for generating reliable parallel treebanks, but is impractical for accumulating resources from large parallel data. (Tinsley et al., 2007) use statistical lexicons derived from automatic statistical word alignment for aligning nodes in parallel trees. In our approach, we use the word alignment information directly, which we believe may be more reliable than the statistical lexicon. (Groves et al., 2004) propose a method of aligning nodes between parallel trees automatically, based on word alignments. In addition to the word alignment information, their approach uses the constituent labels of nodes in the trees, and the general structure of the tree. Our approach is more general in the sense that we only consider the word alignments, thereby making the approach applicable to any parser or phrasestructure representation, even ones that are quite different for the two languages involved. 2.3 Word-level alignment of phrase-level translation equivalents often leaves some words unaligned. For exam"
W08-0411,2003.mtsummit-papers.22,0,0.0331211,"derlying formalisms used has been quite 92 broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al., 2004) and (Galley et al., 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al., 2002) and others. Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus, typically the target side. (Hearne and Way, 2003) describes an approach that uses syntactic information from the source side to derive reordering subtrees, which can then be used within a “data-oriented translation” (DOT) MT system, similar in framework to (Poutsma, 2000). Our work is different from the above in that we use syntactic trees for both source and target sides to infer constituent node alignments, from which we then learn synchronous trees and rules. Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of (Galley et al., 2004). 5.2 Synchronous Tree Fragm"
W08-0411,C04-1015,0,0.0137181,"s.cmu.edu Introduction Phrase-based Statistical MT (PB-SMT) (Koehn et al., 2003) has become the predominant approach to Machine Translation in recent years. PB-SMT requires broad-coverage databases of phrase-to-phrase translation equivalents. These are commonly acquired from large volumes of automatically wordaligned sentence-parallel text corpora. Accurate identification of sub-sentential translation equivalents, however, is a critical process in all data-driven MT approaches, including a variety of data-driven syntax-based approaches that have been developed in recent years. (Chiang, 2005) (Imamura et al., 2004) (Galley et al., 2004). In this paper, we describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees for both languages. Our method consists of three steps. In the first step, we apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees. In the second step, we extract all aligned sub-sentential synt"
W08-0411,N03-1017,0,0.0560991,"Missing"
W08-0411,2005.mtsummit-papers.19,0,0.361655,"Missing"
W08-0411,C00-2092,0,0.22303,"ronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al., 2002) and others. Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus, typically the target side. (Hearne and Way, 2003) describes an approach that uses syntactic information from the source side to derive reordering subtrees, which can then be used within a “data-oriented translation” (DOT) MT system, similar in framework to (Poutsma, 2000). Our work is different from the above in that we use syntactic trees for both source and target sides to infer constituent node alignments, from which we then learn synchronous trees and rules. Our process of extraction of rules as synchronous trees and then converting them to synchronous CFG rules is most similar to that of (Galley et al., 2004). 5.2 Synchronous Tree Fragment Pair Extraction The main concept underlying our syntactic rule extraction process is that we treat the node alignments discovered by the PFA algorithm (described in previous sections) as synchronous tree decomposition p"
W08-0411,C90-3045,0,0.445101,"by the aligned nodes where the decomposition took place. Since the subtrees are rooted at aligned nodes, their yields are translation equivalents of each other. The other synchronous tree fragment pair consists of the remaining portions of the trees. The translation equivalence of the complete tree (or subtree) prior to decomposition implies that these tree fragments (which exclude the detached subtrees) also correspond to translation equivalents. The tree fragments that are obtained by decomposing the synchronous trees in this fashion are similar to the Synchronous Tree Insertion Grammar of (Shieber and Schabes, 1990). We developed a tree traversal algorithm that decomposes parallel trees into all minimal tree fragments. Given two synchronous trees and their node alignment decomposition information, our tree fragment extraction algorithm operates by an “in-order” traversal of the trees top down, starting from the root nodes. The traversal can be guided by either the source or target parse tree. Each node in the tree that is marked as an aligned node triggers a decomposition. The subtree that is rooted at this node is removed from the currently traversed tree. A copy of the removed subtree is then recursive"
W08-0411,J97-3002,0,0.162725,"onous Tree Fragment and CFG Rule Extraction Related Work Syntax-based reordering rules can be used as a preprocessing step for PB-SMT (and other approaches), to decrease the word-order and syntactic distortion between the source and target languages (Xia and McCord, 2004). A variety of hierarchical and syntax-based models, which are applied during decoding, have also been developed. Many of these approaches involve automatic learning and extraction of the underlying syntax-based rules from data. The underlying formalisms used has been quite 92 broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al., 2004) and (Galley et al., 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al., 2002) and others. Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus, typically the target side. (Hearne and Way, 2003) describes an approach that uses syntactic information from the source side to de"
W08-0411,C04-1073,0,0.0241084,"e of 0.8174. We then evaluated the most ‘noisy’ condition that involves both automatic word alignments and automatic parse trees. We evaluated the phrase extraction with different Viterbi combination strategies. The ‘sym2’ combination gave the best results, with a precision of 0.6251, recall of 0.3566, thus an F-0.5 measure of 0.4996. 5 5.1 Synchronous Tree Fragment and CFG Rule Extraction Related Work Syntax-based reordering rules can be used as a preprocessing step for PB-SMT (and other approaches), to decrease the word-order and syntactic distortion between the source and target languages (Xia and McCord, 2004). A variety of hierarchical and syntax-based models, which are applied during decoding, have also been developed. Many of these approaches involve automatic learning and extraction of the underlying syntax-based rules from data. The underlying formalisms used has been quite 92 broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al., 2004) and (Galley et al., 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Pr"
W08-0411,P01-1067,0,0.062457,"and syntactic distortion between the source and target languages (Xia and McCord, 2004). A variety of hierarchical and syntax-based models, which are applied during decoding, have also been developed. Many of these approaches involve automatic learning and extraction of the underlying syntax-based rules from data. The underlying formalisms used has been quite 92 broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al., 2004) and (Galley et al., 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al., 2002) and others. Most of the previous approaches for acquiring syntactic transfer or reordering rules from parallel corpora use syntactic information from only one side of the parallel corpus, typically the target side. (Hearne and Way, 2003) describes an approach that uses syntactic information from the source side to derive reordering subtrees, which can then be used within a “data-oriented translation” (DOT) MT system, similar in framework to (Poutsma, 2000). Our work is different from the above in that we use syn"
W08-0411,W90-0102,0,\N,Missing
W08-0708,J01-2001,0,0.0655504,"al. (1995) take this approach. A third technique leverages inflectional paradigms as the organizational structure of morphology. The ParaMor algorithm, which this paper extends, joins Snover (2002), Zeman (2007), and Goldsmith’s Linguistica in building morphology models around the paradigm. ParaMor tackles three challenges that face morphology induction systems which Goldsmith&apos;s Linguistica algorithm does not yet address. First, section 2.2 of this paper introduces an agglutinative segmentation model. This agglutinative model segments words into as many morphemes as the data justify. Although Goldsmith (2001) and Goldsmith and Hu (2004) discuss ideas for segmenting individual words into more than two morphemes, the implemented Linguistica algorithm, as presented in Goldsmith (2006), permits at most a single morpheme boundary in each word. Second, ParaMor decouples the task of paradigm identification from that of word segmentation (Monson et al., 2007b). In contrast, morphology models in Linguistica inherently encode both a belief about paradigm structure on individual words as well as a segmentation of those words. Without ParaMor’s decoupling of paradigm structure from specific segmentation model"
W08-0708,W07-1315,1,0.826288,"ently lack morphological analysis systems. Unsupervised induction could facilitate, for these lesser-resourced languages, the quick development of morphological systems from raw text corpora. Unsupervised morphology induction has been shown to help NLP tasks including speech recognition (Creutz, 2006) and information retrieval (Kurimo et al., 2007b). In this paper we work with languages like Spanish, German, and Turkish for which morphological analysis systems already exist. The baseline ParaMor algorithm which we extend here competed in the English and German tracks of Morpho Challenge 2007 (Monson et al., 2007b). The peer operated competitions of the Morpho Challenge series standardize the evaluation of unsupervised morphology induction algorithms (Kurimo et al., 2007a; 2007b). The ParaMor algorithm showed promise in the 2007 Challenge, placing first in the linguistic evaluation of German. Developed after the close of Morpho Challenge 2007, our improvements to the ParaMor algorithm could not officially compete in this Challenge. However, the Morpho Challenge 2007 Organizing Committee (Kurimo et al., 2008) graciously oversaw the quantitative evaluation of our agglutinative version of ParaMor. Abstra"
W08-0708,N01-1024,0,\N,Missing
W08-0708,P07-1013,0,\N,Missing
W08-0708,N03-2015,0,\N,Missing
W08-0708,H05-1085,0,\N,Missing
W09-0408,W05-0909,1,0.278739,"h this constraint disallows. Alignments indicate where words are synchronous. Words near an alignment are also likely to be synchronous even without an explicit alignment. For example, in the fragments “even more serious, you” and “even worse, you” from WMT 2008, “serious” and “worse” do not align but do share relative position from other alignments, suggesting these are synchronous. We formalize this by measuring the relative position of frontiers from alignments on each side. For example, Alignment Sentences from different systems are aligned in pairs using a modified version of the METEOR (Banerjee and Lavie, 2005) matcher. This identifies alignments in three phases: exact matches up to case, WordNet (Fellbaum, 1998) morphology matches, and shared WordNet synsets. These sources of alignments are quite precise and unable to pick up on looser matches such as “mentioned” and “said” that legitimately appear in output from different systems. Artificial alignments are intended to fill gaps by using surrounding alignments as clues. If a word is not aligned to any word in some other sentence, we search left and right for words that are aligned into that sentence. If these alignments are sufficiently close to ea"
W09-0408,P05-3026,1,0.872605,"nd unable to pick up on looser matches such as “mentioned” and “said” that legitimately appear in output from different systems. Artificial alignments are intended to fill gaps by using surrounding alignments as clues. If a word is not aligned to any word in some other sentence, we search left and right for words that are aligned into that sentence. If these alignments are sufficiently close to each other in the other sentence, words between them are considered for artificial alignment. An artificial alignment is added if a matching part of speech is found. The algorithm is described fully by Jayaraman and Lavie (2005). 2.2 Synchronization Phrases Switching between systems is permitted outside phrases or at phrase boundaries. We find phrases in two ways. Alignment phrases are maximally 57 hypotheses are detected on insertion and packed, with the combined hypothesis given the highest score of those packed. Once a beam contains the top scoring partial hypotheses of length l, these hypotheses are extended to length l + 1 and placed in another beam. Those hypotheses reaching end of sentence are placed in a separate beam, which is equivalent to packing them into one final hypothesis. Once we remove partial hypot"
W09-0408,P08-2021,0,0.12206,"ent of 2 BLEU and 1 METEOR point over the best HungarianEnglish system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task. 1 Introduction Many systems for machine translation, with different underlying approaches, are of competitive quality. Nonetheless these approaches and systems have different strengths and weaknesses. By offsetting weaknesses with strengths of other systems, combination can produce higher quality than does any component system. One approach to system combination uses confusion networks (Rosti et al., 2008; Karakos et al., 2008). In the most common form, a skeleton sentence is chosen from among the one-best system outputs. This skeleton determines the ordering of the final combined sentence. The remaining outputs are aligned with the skeleton, producing a list of alternatives for each word in the skeleton, which comprises a confusion network. A decoder chooses from the original skeleton word and its alternatives to produce a final output sentence. While there are a number of variations on this theme, our approach differs fundamentally in that the effective skeleton changes on a per-phrase basis. 2 System The system c"
W09-0408,W07-0734,1,0.765805,"version 1.04 of IBM-style BLEU (Papineni et al., 2002) in case-insensitive mode. We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation. This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences. We chose to do only two folds due to limited computational time and a desire to test many models. We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (Snover et al., 2006). For each source language, we exThe N -Gram and Overlap features are intended to improve fluency across phrase boundaries. Features are combined using a log-linear model trained as discussed in Section 3. Hypotheses are scored using the geometric average score of each word in the hypothesis. 2.5 Tuning Search Of note is that a word’s score is impacted only by its alignments and the n-gram found by the language model. Therefore two partial hypotheses that differ only in words preceding the n-gram and in their average score are in some sense dup"
W09-0408,P02-1040,0,0.0827738,"monolingual and French-English data provided by the contest. order−ngram N -Gram 31 using language model order and length of ngram found. overlap Overlap order−1 where overlap is the length of intersection between the preceding and current n-grams. 3 Given the 502 sentences made available for tuning by WMT 2009, we selected feature weights for scoring, a set of systems to combine, confidence in each selected system, and the type and distance s of synchronization. Of these, only feature weights can be trained, for which we used minimum error rate training with version 1.04 of IBM-style BLEU (Papineni et al., 2002) in case-insensitive mode. We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation. This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences. We chose to do only two folds due to limited computational time and a desire to test many models. We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (S"
W09-0408,W08-0329,0,0.159798,"data showed improvement of 2 BLEU and 1 METEOR point over the best HungarianEnglish system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task. 1 Introduction Many systems for machine translation, with different underlying approaches, are of competitive quality. Nonetheless these approaches and systems have different strengths and weaknesses. By offsetting weaknesses with strengths of other systems, combination can produce higher quality than does any component system. One approach to system combination uses confusion networks (Rosti et al., 2008; Karakos et al., 2008). In the most common form, a skeleton sentence is chosen from among the one-best system outputs. This skeleton determines the ordering of the final combined sentence. The remaining outputs are aligned with the skeleton, producing a list of alternatives for each word in the skeleton, which comprises a confusion network. A decoder chooses from the original skeleton word and its alternatives to produce a final output sentence. While there are a number of variations on this theme, our approach differs fundamentally in that the effective skeleton changes on a per-phrase basis"
W09-0408,2006.amta-papers.25,0,0.0373435,") in case-insensitive mode. We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation. This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences. We chose to do only two folds due to limited computational time and a desire to test many models. We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (Snover et al., 2006). For each source language, we exThe N -Gram and Overlap features are intended to improve fluency across phrase boundaries. Features are combined using a log-linear model trained as discussed in Section 3. Hypotheses are scored using the geometric average score of each word in the hypothesis. 2.5 Tuning Search Of note is that a word’s score is impacted only by its alignments and the n-gram found by the language model. Therefore two partial hypotheses that differ only in words preceding the n-gram and in their average score are in some sense duplicates. With the same set of used words and same"
W09-0408,2005.eamt-1.20,1,\N,Missing
W09-0425,2008.amta-srw.1,1,0.818132,"Missing"
W09-0425,N07-1051,0,0.0758505,"Missing"
W09-0425,W08-0509,0,0.12583,"Missing"
W09-0425,2006.amta-papers.25,0,0.0170085,"mary and contrastive systems on four data sets. First, we report final (tuned) performance on our two tuning sets — the last 425 sentences of news-dev2009a for the primary system, and the first 600 sentences of the same set for the contrastive. We also include our development test (news-dev2009b) and, for additional comparison, the “nc-test2007” news commentary test set from the 2007 WMT shared task. For each, we give case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBMstyle BLEU (Papineni et al., 2002), and version 5 of TER (Snover et al., 2006). From these results, we highlight two interesting areas of analysis. First, the low tuning and development test set scores bring up questions about system coverage, given that the news domain was not strongly represented in our system’s Acknowledgments This research was supported in part by NSF grants IIS-0121631 (AVENUE) and IIS-0534217 (LETRAS), and by the DARPA GALE program. We thank Yahoo! for the use of the M45 research computing cluster, where we ran the parsing stage of our data processing. 2 Due to a data processing error, the choice of the primary submission was based on incorrectly"
W09-0425,2005.iwslt-1.8,0,0.0320543,"Missing"
W09-0425,W07-0734,1,0.830778,"sentence, depending on the size of the final bilingual lexicon. 4 Evaluation and Analysis Figure 2 shows the results of our primary and contrastive systems on four data sets. First, we report final (tuned) performance on our two tuning sets — the last 425 sentences of news-dev2009a for the primary system, and the first 600 sentences of the same set for the contrastive. We also include our development test (news-dev2009b) and, for additional comparison, the “nc-test2007” news commentary test set from the 2007 WMT shared task. For each, we give case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBMstyle BLEU (Papineni et al., 2002), and version 5 of TER (Snover et al., 2006). From these results, we highlight two interesting areas of analysis. First, the low tuning and development test set scores bring up questions about system coverage, given that the news domain was not strongly represented in our system’s Acknowledgments This research was supported in part by NSF grants IIS-0121631 (AVENUE) and IIS-0534217 (LETRAS), and by the DARPA GALE program. We thank Yahoo! for the use of the M45 research computing cluster, where we ran the parsing st"
W09-0425,W08-0411,1,0.799957,"un a recaser as a postprocessing step on our output. Our mixed-case decision may also be validated by preliminary test set results, which show that our submission has the smallest drop in BLEU score (0.0074) between uncased and cased evaluation of any system in the French–English translation task. Syntactic Grammar Syntactic phrase extraction specifies a node-tonode alignment across parallel parse trees. If these aligned nodes are used as decomposition points, a set of synchronous context-free rules that produced the trees can be collected. This is our process of syntactic grammar extraction (Lavie et al., 2008). For our 2009 WMT submission, we extracted 11.0 million unique grammar rules, 9.1 million of which were singletons, from our parallel parsed corpus. These rules operate on our syntactically extracted phrase pairs, which have category labels, but they may also be partially lexicalized with explicit source or target word strings. Each extracted grammar rule is scored according to Equations 1 and 2, where now the right-hand sides of the rule are used as ws and wt . As yet, we have made only minimal use of the Stat-XFER framework’s grammar capabilities, especially for large-scale MT systems. For"
W09-0425,J03-1002,0,0.00593324,"Missing"
W09-0425,P02-1040,0,\N,Missing
W09-0425,P07-2045,0,\N,Missing
W09-0425,W08-0324,1,\N,Missing
W09-2301,W01-1819,0,0.0350064,"Missing"
W09-2301,2008.amta-srw.1,1,0.734603,"above, as well as non-terminals and pre-terminals from the grammar. Constituent alignment information, shown here as co-indexes on the nonterminals, specifies one-to-one correspondences between source-language and target-language constituents on the right-hand side of the SCFG rule. Rule scores rt|s and rs|t for grammar rules, if they are learned from data, are calculated in the same way as the scores for lexical entries. 2.2 Syntax-Based Phrase Extraction In this section, we briefly summarize the automatic resource extraction approach described by Lavie et al. (2008) and recently extended by Ambati and Lavie (2008), which we use here, specifically as applied to the extraction of syntax-based phrase pairs for the bilingual lexicon. The grammar and lexicon are extracted from a large parallel corpus that has been statistically wordaligned and independently parsed on both sides with 1 If no syntactic category information is available, cs and ct can be set to dummy values, but the rule score equations remain unchanged. 3 automatic parsers. Word-level entries for the bilingual lexicon are directly taken from the word alignments; corresponding syntactic categories for the left-hand side of the SCFG rules are o"
W09-2301,W07-0718,0,0.0248413,"Missing"
W09-2301,N03-1017,0,0.0417339,"Missing"
W09-2301,W04-3250,0,0.0189906,"Missing"
W09-2301,2005.mtsummit-papers.11,0,0.00972188,"nally inserted into the system. The Stat-XFER system behaves the same way in each variant. All phrase pairs are applied jointly to the input sentence during the parsing stage, getting added to the translation according to their syntactic category and scores, although phrases tagged as PHR cannot participate in any grammar rules. The second-stage decoder then receives the joint lattice and assembles complete output hypotheses regardless of syntactic category labels. 4 Experiments We extracted the lexical resources for our MT system from version 3 of the French–English Europarl parallel corpus (Koehn, 2005), using the officially released training set from the 2008 Workshop in Statistical Machine Translation (WMT)3 . This gives us a corpus of approximately 1.2 million sentence 3 www.statmt.org/wmt08/shared-task.html Phrase Table Total syntax-prioritized table Syntactic component PBSMT component Total baseline PBSMT table Overlap with syntax-prioritized # Entries 3,052,121 1,081,233 1,970,888 8,069,480 6,098,592 # Source Sides 113,988 39,105 74,883 113,972 39,089 Amb. Factor 26.8 27.7 26.3 70.8 156.0 Figure 2: Statistical characteristics of the syntax-prioritized phrase table (top) compared with t"
W09-2301,W07-0734,1,0.835617,"Missing"
W09-2301,W08-0411,1,0.846392,"G backbone may include lexicalized items, as above, as well as non-terminals and pre-terminals from the grammar. Constituent alignment information, shown here as co-indexes on the nonterminals, specifies one-to-one correspondences between source-language and target-language constituents on the right-hand side of the SCFG rule. Rule scores rt|s and rs|t for grammar rules, if they are learned from data, are calculated in the same way as the scores for lexical entries. 2.2 Syntax-Based Phrase Extraction In this section, we briefly summarize the automatic resource extraction approach described by Lavie et al. (2008) and recently extended by Ambati and Lavie (2008), which we use here, specifically as applied to the extraction of syntax-based phrase pairs for the bilingual lexicon. The grammar and lexicon are extracted from a large parallel corpus that has been statistically wordaligned and independently parsed on both sides with 1 If no syntactic category information is available, cs and ct can be set to dummy values, but the rule score equations remain unchanged. 3 automatic parsers. Word-level entries for the bilingual lexicon are directly taken from the word alignments; corresponding syntactic categori"
W09-2301,P06-1077,0,0.0635907,"city take into account the syntax of the sentences being translated. One simple approach is to limit the phrases learned by a standard Zollmann and Venugopal (2006) overcome the restrictiveness of the syntax-only model by starting with a complete set of phrases as produced by traditional PBSMT heuristics, then annotating the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span. They then introduce new constituent labels to handle the cases where the phrasal entries do not exactly correspond to the syntactic constituents. Liu et al. (2006) also add non-syntactic PBSMT phrases into their tree-to-string translation system. Working from the other direction, Marton and Resnik (2008) extend a hierarchical PBSMT 1 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 1–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics system with a number of features to prefer or disprefer certain types of syntactic phrases in different contexts. Restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, Wang et al. (2007) binarize so"
W09-2301,P08-1114,0,0.0624192,"Zollmann and Venugopal (2006) overcome the restrictiveness of the syntax-only model by starting with a complete set of phrases as produced by traditional PBSMT heuristics, then annotating the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span. They then introduce new constituent labels to handle the cases where the phrasal entries do not exactly correspond to the syntactic constituents. Liu et al. (2006) also add non-syntactic PBSMT phrases into their tree-to-string translation system. Working from the other direction, Marton and Resnik (2008) extend a hierarchical PBSMT 1 Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 1–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics system with a number of features to prefer or disprefer certain types of syntactic phrases in different contexts. Restructuring the parse trees to ease their restrictiveness is another recent approach: in particular, Wang et al. (2007) binarize source-side parse trees in order to provide phrase pair coverage for phrases that are partially syntactic. Tinsley et al. (2007) showed an impro"
W09-2301,J03-1002,0,0.00766328,"Missing"
W09-2301,P02-1040,0,0.0799033,"Missing"
W09-2301,N07-1051,0,0.0453913,"Missing"
W09-2301,2006.amta-papers.25,0,0.035238,"Missing"
W09-2301,D07-1078,0,0.185888,"Missing"
W09-2301,W06-3119,0,0.0800418,"Missing"
W09-2301,P07-2045,0,\N,Missing
W10-0709,W05-0909,1,0.54914,"nt task for untrained Arabicspeaking annotators and discuss several techniques for normalizing the resulting data. We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores. 1 2 Introduction Human judgments of translation quality play a vital role in the development of effective machine translation (MT) systems. Such judgments can be used to measure system quality in evaluations (CallisonBurch et al., 2009) and to tune automatic metrics such as M ETEOR (Banerjee and Lavie, 2005) which act as stand-ins for human evaluators. However, collecting reliable human judgments often requires significant time commitments from expert annotators, leading to a general scarcity of judgments and a significant time lag when seeking judgments for new tasks or languages. Amazon’s Mechanical Turk (MTurk) service facilitates inexpensive collection of large amounts of data from users around the world. However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers attempt to game the system by submitting random answers. For th"
W10-0709,D09-1030,0,0.109156,"judgments from annotators with P (A) below some threshold. We set this threshold such that the highest overall agreement can be achieved while retaining at least one judgment for each translation. 3.3 Removing Outlying Judgments For a given translation and human judgments (j1 ...jn ), we calculate the distance (δ) of each judgment from the mean (¯j): δ(ji ) = |ji − ¯j| We then remove outlying judgments with δ(ji ) exceeding some threshold. This threshold is also set such that the highest agreement is achieved while retaining at least one judgment per translation. 3.4 Weighted Voting Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator’s contribution is weighted by agreement with either a gold standard or with other annotators. For this evaluation, we weigh contribution by P (A) with the 101 gold standard judgments. Type Uniform-a Uniform-b Gaussian-2 Gaussian-2.5 Gaussian-3 3.5 Scaling Judgments To account for the notion that some annotators judge translations more harshly than others, we apply perannotator scaling to the adequacy judgments based on annotators’ signed distance from gold standard judgments. For judgments (J = j1 ...jn ) and gold standard"
W10-0711,P05-1074,0,0.0575231,"9) constrained training data. The target corpus to be paraphrased consists of the 728 Arabic sentences from the OpenMT 2002 (Garofolo, 2002) development data. 2.1 Paraphrase Extraction We conduct word alignment and phrase extraction on the parallel data to produce a phrase table containing Arabic-English phrase pairs (a, e) with translation probabilities P (a|e) and P (e|a). Follow66 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 66–70, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ing Bannard and Callison-Burch (2005), we identify Arabic phrases (a1 ) in the target corpus that are translated by at least one English phrase (e). We identify paraphrase candidates as alternate Arabic phrases (a2 ) that translate e. The probability of a2 being a paraphrase of a1 given foreign phrases e is defined: P (a2 |a1 ) = X P (e|a1 )P (a2 |e) e A language model trained on the Arabic side of the parallel corpus is used to further score the possible paraphrases. As each original phrase (a1 ) occurs in some sentence (s1 ) in the target corpus, a paraphrased sentence (s2 ) can be created by replacing a1 with one of its paraph"
W10-0711,P03-1021,0,0.00427263,"Missing"
W10-0711,P02-1040,0,0.12402,"Missing"
W10-0711,D08-1076,0,\N,Missing
W10-1709,N03-1017,0,0.0146579,"ment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word alignments in the yield of s all land within the yield of t, and vice versa. This node alignment is similar in spirit to the subtree alignment method of Zhechev and Way (2008), except our method is based on the specific Viterbi word alignment links found for each 1 Introduction From its earlier focus on linguist"
W10-1709,W07-0734,1,0.868809,"Missing"
W10-1709,W08-0411,1,0.747796,". We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word alignments in the yield of s all land within the yield of t, and vice versa. This node alignment is similar in spirit to the subtree alignment method of Zhechev and Way (2008), except our method is based on the specific Viterbi word alignment links found for each 1 Introduction From its earlier focus on linguistically rich machine translation for resource-poor languages, the statistical transfer MT group at Carnegie Mell"
W10-1709,clark-lavie-2010-loonybin,1,0.489456,"pairs and grammar rules are collected together and scored according to a variety of features (Section 3). Instead of decoding with the very large complete set of extracted grammar rules, we select only a small number of rules meeting certain criteria (Section 4). In contrast to previous years, when we used the Stat-XFER decoder, this year we switched to the the Joshua decoder (Li et al., 2009) to take advantage of its more efficient architecture and implementation of modern decoding techniques, such as cube pruning and multi-threading. We also managed system-building workflows with LoonyBin (Clark and Lavie, 2010), a toolkit for managing multi-step experiments across different servers or computing clusters. Section 5 details our experimental results. no feature is estimated from counts in both spaces. We define an aggregate rule instance as a 5tuple r = (L, S, T, Cphr , Csyn ) that contains a left-hand-side label L, a sequence of terminals and non-terminals for the source (S) and target (T ) right-hand sides, and aggregated counts from phrase-based SMT extraction heuristics Cphr and the syntactic rule extractor Csyn . In preparation for feature scoring, we: 1. Run phrase instance extraction using stand"
W10-1709,W09-0424,0,0.30975,"Missing"
W10-1709,P06-1121,0,0.0603999,"ppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics sentence rather than on the general word translation probabilities computed for the corpus as a whole. This enables us to use efficient dynamic programming to infer node alignments, rather than resorting to a greedy search or the enumeration of all possible alignments. Finally, in the third step, we use the node alignments from syntactic phrase pair extraction to extract grammar rules. Each aligned node in a tree pair specifies a decomposition point for breaking the parallel trees into a series of SCFG rules. Like Galley et al. (2006), we allow “composed” (non-minimal) rules when they build entirely on lexical items. However, to control the size of the grammar, we do not produce composed rules that build on other non-terminals, nor do we produce multiple possible rules when we encounter unaligned words. Another difference is that we discard internal structure of composed lexical rules so that we produce SCFG rules rather than synchronous tree substitution grammar rules. The extracted phrase pairs and grammar rules are collected together and scored according to a variety of features (Section 3). Instead of decoding with the"
W10-1709,J03-1002,0,0.00526721,"eport a gain of 1.73 BLEU by using the new features and decoding environment, and a gain of up to 0.52 BLEU from improved grammar selection. 2 System Overview We built our system on a partial selection of the provided French–English training data, using the Europarl, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second,"
W10-1709,W08-0509,0,0.0276813,"translation model and improved filtering of SCFG rules. Compared to our WMT 2009 submission, we report a gain of 1.73 BLEU by using the new features and decoding environment, and a gain of up to 0.52 BLEU from improved grammar selection. 2 System Overview We built our system on a partial selection of the provided French–English training data, using the Europarl, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase"
W10-1709,P02-1040,0,0.0844358,"Missing"
W10-1709,N07-1051,0,0.110961,"tem on a partial selection of the provided French–English training data, using the Europarl, News Commentary, and UN sets, but ignoring the Giga-FrEn data. After tokenization and some pruning of our training data, this left us with a corpus of approximately 8.6 million sentence pairs. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). Word alignments were symmetrized with the “grow-diag-final-and” heuristic. We automatically parsed the French side of the corpus with the Berkeley parser (Petrov and Klein, 2007), while we used the fast vanilla PCFG model of the Stanford parser (Klein and Manning, 2003) for the English side. These steps resulted in a parallel parsed corpus from which to extract phrase pairs and grammar rules. Phrase extraction involves three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the"
W10-1709,C08-1139,0,0.273104,"s three distinct steps. In the first, we perform standard (non-syntactic) phrase extraction according to the heuristics of phrase-based SMT (Koehn et al., 2003). In the second, we obtain syntactic phrase pairs using the tree-to-tree matching method of Lavie et al. (2008). Briefly, this method aligns nodes in parallel parse trees by projecting up from the word alignments. A source-tree node s will be aligned to a target-tree node t if the word alignments in the yield of s all land within the yield of t, and vice versa. This node alignment is similar in spirit to the subtree alignment method of Zhechev and Way (2008), except our method is based on the specific Viterbi word alignment links found for each 1 Introduction From its earlier focus on linguistically rich machine translation for resource-poor languages, the statistical transfer MT group at Carnegie Mellon University has expanded in recent years to the increasingly successful domain of syntax-based statistical MT in large-data scenarios. Our submission to the 2010 Workshop on Machine Translation is a syntax-based SMT system with a synchonous context-free grammar (SCFG), where the SCFG rules are derived from full constituency parse trees on both the"
W10-1709,W06-3119,0,0.0847663,"ave grammar rule probabilities Psyn (T |S), Psyn (S |T ), Psyn (L |S), Psyn (L |T ), and Psyn (L |S, T ) estimated using Csyn ; these apply only to rules where S and T contain non-terminals. By no longer including counts from phrase-based SMT extraction heuristics in these features, we encourage rules where L 6= PHR since the smaller counts from the rule learner would have otherwise been overshadowed 3 Translation Model Construction One major improvement in our system this year is the feature scores we applied to our grammar and phrase pairs. Inspired largely by the SyntaxAugmented MT system (Zollmann and Venugopal, 2006), our translation model contains 22 features in addition to the language model. In contrast to earlier formulations of our features (Hanneman and Lavie, 2009), our maximum-likelihood features are now based on a strict separation between counts drawn from non-syntactic phrase extraction heuristics and our syntactic rule extractor; 83 by the much larger counts from the phrase-based SMT heuristics. Finally, we estimate “not labelable” (NL) features Psyn (NL |S) and Psyn (NL |T ). With R denoting the set of all extracted rules, Psyn (NL |S) = P r ′ ∈R Psyn (NL |T ) = P Csyn s.t. S ′ =S ′ Csyn Csyn"
W10-1709,E09-1044,0,\N,Missing
W10-1709,W09-0425,1,\N,Missing
W10-1744,P05-1074,0,0.379455,"which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candidate combinations by forming left-to-right paths through the aligned system outputs, and scoring these candidates using"
W10-1744,W08-0309,0,0.0907409,"Missing"
W10-1744,P02-1040,0,0.085232,"ecoded with different weights than the second sentence. The amount of random perturbation decreases linearly each iteration until the 10th and subsequent iterations whose learned weights are not perturbed. We emphasize that the point is to introduce randomness in sentences decoded during MERT, and therefore considered during parameter tuning, and not on the specific formula presented in this system description. In practice, this technique increases the number of iterations and decreases the difference in tuning scores following MERT. In our experiments, weights are tuned towards uncased BLEU (Papineni et al., 2002) or the combined metric TERBLEU (Snover et al., 2006). 6.2 Hyperparameters In total, we tried 1167 hyperparameter configurations, limited by CPU time during the evaluation period. For each of these configurations, the feature weights were fully trained with MERT and scored on the same tuning set, which we used to select the submitted combinations. Because these configurations represent a small fraction of the hyperparameter space, we focused on values that work well based on prior experience and tuning scores as they became available: Parameter Optimization Feature Weights Feature weights are"
W10-1744,N10-1031,1,0.818569,"use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candidate combinations by forming left-to-right paths through the aligned system outputs, and scoring these candidates using a battery of features. Improvements this year include unigram paraphrase alignment, support for all target languages, new features, language modeling without pruning, and more parameter optimization. This paper describes our scheme with emphasis on improved areas. 3 Alignment System outputs are aligned at the token level using a variant of the METEOR (Denkowski and Lavie, 2010) aligner. This identifies, in decreasing order of priority: exact, stem,"
W10-1744,W08-0329,0,0.0387352,"ction 5.2 details our training data and backoff features. less systems are able to vote on a word order decision mediated by the bigram and trigram features. We find that both versions have their advantages, and therefore include two sets of match features: one that counts only exact alignments and another that counts all alignments. We also tried copies of the match features at the stem and synonym level but found these impose additional tuning cost with no measurable improvement in quality. Since systems have different strengths and weaknesses, we avoid assigning a single system confidence (Rosti et al., 2008) or counting n-gram matches with uniform system confidence (Hildebrand and Vogel, 2009). The weight on match feature ms,n corresponds to our confidence in ngrams from system s. These weights are fully tunable. However, there is another hyperparameter: the maximum length of n-gram considered; we typically use 2 or 3 with little gain seen above this. Features are combined into a score using a linear model. Equivalently, the score is the dot product of a weight vector with the vector of our feature values. The weight vector is a parameter optimized in Section 6. 5.1 Match Features The n-gram matc"
W10-1744,W09-0408,1,0.899763,"ely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candidate combinations by forming left-to-right paths through the aligned system outputs, and scoring these candidates using a battery of features. Improvements this year include unigram paraphrase alignment, support for all target languages, new features, language modeling without pruning, and more parameter optimization. This paper describes our scheme with emphasis on improved areas. 3 Alignment System outputs are aligned at the to"
W10-1744,2006.amta-papers.25,0,0.15525,"Work Confusion networks (Rosti et al., 2008) are the most popular form of system combination. In this approach, a single system output acts as a backbone to which the other outputs are aligned. This backbone determines word order while other outputs vote for substitution, deletion, and insertion operations. Essentially, the backbone is edited to produce a combined output which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translatio"
W10-1744,W09-0406,0,0.188752,"to vote on a word order decision mediated by the bigram and trigram features. We find that both versions have their advantages, and therefore include two sets of match features: one that counts only exact alignments and another that counts all alignments. We also tried copies of the match features at the stem and synonym level but found these impose additional tuning cost with no measurable improvement in quality. Since systems have different strengths and weaknesses, we avoid assigning a single system confidence (Rosti et al., 2008) or counting n-gram matches with uniform system confidence (Hildebrand and Vogel, 2009). The weight on match feature ms,n corresponds to our confidence in ngrams from system s. These weights are fully tunable. However, there is another hyperparameter: the maximum length of n-gram considered; we typically use 2 or 3 with little gain seen above this. Features are combined into a score using a linear model. Equivalently, the score is the dot product of a weight vector with the vector of our feature values. The weight vector is a parameter optimized in Section 6. 5.1 Match Features The n-gram match features reward agreement between the candidate combination and underlying system out"
W10-1744,P08-2021,0,0.0456918,"i et al., 2008) are the most popular form of system combination. In this approach, a single system output acts as a backbone to which the other outputs are aligned. This backbone determines word order while other outputs vote for substitution, deletion, and insertion operations. Essentially, the backbone is edited to produce a combined output which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-co"
W10-1744,2005.mtsummit-papers.11,0,0.02645,"6. 5.1 Match Features The n-gram match features reward agreement between the candidate combination and underlying system outputs. For example, feature m1,1 counts tokens in the candidate that also appear in system 1’s output for the sentence being combined. Feature m1,2 counts bigrams appearing in both the candidate and the translation suggested by system 1. Figure 2 shows example feature values. 5.2 System 1: Supported Proposal of France We built language models for each of the five target languages with the aim of using all constrained data. For each language, we used the provided Europarl (Koehn, 2005) except for Czech, News Commentary, and News monolingual corpora. In addition, we used: System 2: Support for the Proposal of France Candidate: Support for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Language Model Trigram 1 1 ˇ Czech CzEng (Bojar and Zabokrtsk´ y, 2009) sections 0–7 English Gigaword Fourth Edition (Parker et al., 2009), Giga-FrEn, and CzEng (Bojar and ˇ Zabokrtsk´ y, 2009) sections 0–7 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. French Gigaword Second Edition"
W10-1744,W09-0407,0,0.020231,"ion, deletion, and insertion operations. Essentially, the backbone is edited to produce a combined output which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candi"
W10-1744,P03-1021,0,\N,Missing
W10-1751,P05-1074,0,0.141003,"accurate automatic metrics for evaluating the quality of machine translation (MT) output. While these workshops evaluate metric performance on many target languages, most metrics are limited to English due to the relative lack of lexical resources for other languages. This paper describes a language-independent method for adding paraphrase support to the M ETEOR - NEXT metric for all WMT10 target languages. Taking advantage of the large parallel corpora released for the translation tasks often accompanying evaluation tasks, we automatically construct paraphrase tables using the pivot method (Bannard and Callison-Burch, 2005). We use the WMT09 human evaluation data to tune versions of M ETEOR - NEXT with and without paraphrases and report significantly better performance for versions with paraphrase support. 2 The M ETEOR - NEXT Metric The M ETEOR - NEXT metric (Denkowski and Lavie, 2010) evaluates a machine translation hypothesis against a reference translation by calculating a similarity score based on an alignment be1. Each word in each sentence is covered by zero or one matches 2. Largest number of covered words across both 339 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and Metric"
W10-1751,W08-0309,0,0.0668349,"Missing"
W10-1751,W07-0734,1,0.50823,"he translation hypothesis (t) and reference (r) are counted. For each of the matchers (mi ), count the number of words covered by matches of this type in the hypothesis (mi (t)) and reference (mi (r)) and apply matcher weight (wi ). The weighted Precision and Recall are then calculated: P P wi · mi (r) i wi · mi (t) P = R= i |t| |r| NEXT P (n2 |n1 ) = To improve paraphrase accuracy, we apply multiple filtering techniques during paraphrase extraction. The following are applied to each paraphrase instance (n1 , f, n2 ): To account for gaps and differences in word order, a fragmentation penalty (Lavie and Agarwal, 2007) is calculated using the total number of matched words (m) and number of chunks (ch): P en = γ · ch m 1. Discard paraphrases with very low probability (P (f |n1 ) · P (n2 |f ) < 0.001) β 2. Discard paraphrases for which n1 , f , or n2 contain any punctuation characters. The final M ETEOR - NEXT score is then calculated: Score = (1 − P en) · Fmean 3. Discard paraphrases for which n1 , f , or n2 contain only common words. Common words are defined as having relative frequency of 0.001 or greater in the parallel corpus. The parameters α, β, γ, and wi ...wn can be tuned to maximize correlation wit"
W10-1751,W09-0441,0,0.083071,"duce the final paraphrase tables described in Table 2. To keep table size reasonable, we only extract paraphrases for phrases occurring in target corpora consisting of the pooled development data from the WMT08, WMT09, and WMT10 translation tasks (10,158 sentences for Czech, 20,258 sentences for all other languages). 5 Experiments To evaluate the impact of our paraphrase tables on metric performance, we tune versions of M ETEOR - NEXT with and without the paraphrase matchers for each language. For further comparison, we tune a version of M ETEOR - NEXT using the TERp English paraphrase table (Snover et al., 2009) used by previous versions of the metric. As shown in Table 4, the addition of paraphrases leads to a better tuning point for every target language. The best scoring subset of paraphrase ta341 Language English Czech German Spanish French Paraphrases none TERp de+es+fr none en none en+es none en+de none en Rank Consistency 0.619 0.625 0.629 0.564 0.574 0.550 0.576 0.586 0.608 0.696 0.707 α 0.85 0.70 0.75 0.95 0.95 0.20 0.75 0.95 0.15 0.95 0.90 β 2.35 1.40 0.60 0.20 2.15 0.75 0.80 0.55 0.25 0.80 0.85 γ 0.45 0.25 0.35 0.70 0.35 0.25 0.90 0.90 0.75 0.35 0.45 wexact 1.00 1.00 1.00 1.00 1.00 1.00 1."
W10-1751,N10-1031,1,\N,Missing
W11-1011,P10-1146,0,0.0588054,"structure up to labeling can be combined with the use of a preference grammar (Venugopal et al., 2009), which replaces the variant labelings with a single SCFG rule using generic “X” labels. The generic rule’s “preference” over possible labelings is stored as a probability distribution inside the rule for use at decoding time. Preference grammars thus reduce the label set size to one for the purposes of some feature calculations — which avoids the fragmentation of rule scores due to labeling ambiguity — but the original labels persist for specifying which rules may combine with which others. Chiang (2010) extended SAMT-style labels to both source- and target-side parses, also introducing a mechanism by which SCFG rules may apply at run time even if their labels do not match. Under Chiang’s soft matching constraint, a rule headed by a label A::Z may still plug into a substitution site labeled B::Y by paying additional model costs substB→A and substY →Z . This is an on-the-fly method of coarsening the effective label set on a case-by-case basis. Unfortunately, it also requires tuning a separate decoder feature for each pair of source-side and each pair of target-side labels. This tuning can beco"
W11-1011,N04-1035,0,0.0780198,"rammar and the decoding process is also given. 1 Introduction A common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (SCFG), where a source-language string and a target-language string are produced simultaneously by applying a series of re-write rules. Given a parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, SCFG models for MT can be learned via a variety of methods. Parsing may be applied on the source side (Liu et al., 2006), on the target side (Galley et al., 2004), or on both sides of the parallel corpus (Lavie et al., 2008; Zhechev and Way, 2008). In any of these cases, using the raw label set from source- and/or target-side parsers can be undesirable. Label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists. These labels sets are not necessarily ideal for statistical parsing, let alone for bilingual syntax-based translation models. Further, the side(s) on which syntax is represented defines the nonterminal label"
W11-1011,W04-3250,0,0.0329608,"R version 0.7 (Snover et al., 2006). No matter the degree of label collapsing, we find significant improvements in BLEU and TER scores on both test sets. On the MT 2003 set, labelcollapsed systems score 1.77 to 2.84 BLEU points and 3.09 to 5.25 TER points better than the baseline. On MT 2008, improvements range from 1.25 to 2.21 points on BLEU and from 2.43 to 4.30 points on TER. Improvements on both sets according to METEOR, though smaller, are still noticable (up to 0.89 points). In the case of BLEU, we verified the significance of the improvements by conducting paired bootstrap resampling (Koehn, 2004) on the MT 2003 102 output. With n = 1000 and p &lt; 0.05, all five labelcollapsed systems were statistically significant improvements over the baseline, and all other collapsed systems were significant improvements over the 99iteration system. Thus, though the system that provides the highest score changes across metrics and test sets, the overall pattern of scores suggests that over-collapsing labels may start to weaken results. A more moderate stopping point is thus preferable, but beyond that we suspect the best result is determined more by the test set, automatic metric choice, and MERT inst"
W11-1011,W08-0411,1,0.867788,"common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (SCFG), where a source-language string and a target-language string are produced simultaneously by applying a series of re-write rules. Given a parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, SCFG models for MT can be learned via a variety of methods. Parsing may be applied on the source side (Liu et al., 2006), on the target side (Galley et al., 2004), or on both sides of the parallel corpus (Lavie et al., 2008; Zhechev and Way, 2008). In any of these cases, using the raw label set from source- and/or target-side parsers can be undesirable. Label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists. These labels sets are not necessarily ideal for statistical parsing, let alone for bilingual syntax-based translation models. Further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting SCFG. A pair of aligned adjectiv"
W11-1011,W09-0424,0,0.0550244,"Missing"
W11-1011,P06-1077,0,0.0640685,"s of label collapsing’s effect on the grammar and the decoding process is also given. 1 Introduction A common modeling choice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (SCFG), where a source-language string and a target-language string are produced simultaneously by applying a series of re-write rules. Given a parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, SCFG models for MT can be learned via a variety of methods. Parsing may be applied on the source side (Liu et al., 2006), on the target side (Galley et al., 2004), or on both sides of the parallel corpus (Lavie et al., 2008; Zhechev and Way, 2008). In any of these cases, using the raw label set from source- and/or target-side parsers can be undesirable. Label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists. These labels sets are not necessarily ideal for statistical parsing, let alone for bilingual syntax-based translation models. Further, the side(s) on which syntax is"
W11-1011,P02-1040,0,0.0819376,"es. In addition to producing a minimal rule, each decomposition point also produces a phrase pair rule with the node pair’s yields as the right-hand side, as long as the length of the yield is less than a specified threshold. Following grammar extraction, labels are optionally clustered and collapsed according to the algorithm in Section 3. The grammar is re-written with the modified nonterminals, then scored as usual according to our translation model features. Feature weights themselves are learned via minimum error rate training as implemented in Z-MERT (Zaidan, 2009) with the BLEU metric (Papineni et al., 2002). Decoding is carried out with Joshua (Li et al., 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional errorcorrection and filtering steps to every system. First, we observed that the syntactic parsers were most likely to make labeling errors for cardinal numbers in English and punctuation marks in all languages. We thus post-process the parses of our training data to tag all English cardinal numbers as CD and to overwrite the labels of various punctuation marks with the correct labels as defined by each la"
W11-1011,N07-1051,0,0.15449,"Missing"
W11-1011,2006.amta-papers.25,0,0.0537893,"27.03 63.77 54.65 26.69 62.76 55.11 27.23 63.06 54.87 26.87 64.92 54.86 26.16 64.17 MT 2008 Test Set METEOR BLEU TER 45.68 18.27 69.18 46.25 19.78 65.88 46.02 19.60 64.88 46.30 20.19 65.18 45.70 20.48 66.75 45.87 19.52 65.61 Table 1: Results of applying increasing degrees of label collapsing on our Chinese–English baseline system. Bold figures indicate the best score in each column. references.) Table 1 reports automatic metric results for version 1.0 of METEOR (Lavie and Denkowski, 2009) using the default settings, uncased IBM-style BLEU (Papineni et al., 2002), and uncased TER version 0.7 (Snover et al., 2006). No matter the degree of label collapsing, we find significant improvements in BLEU and TER scores on both test sets. On the MT 2003 set, labelcollapsed systems score 1.77 to 2.84 BLEU points and 3.09 to 5.25 TER points better than the baseline. On MT 2008, improvements range from 1.25 to 2.21 points on BLEU and from 2.43 to 4.30 points on TER. Improvements on both sets according to METEOR, though smaller, are still noticable (up to 0.89 points). In the case of BLEU, we verified the significance of the improvements by conducting paired bootstrap resampling (Koehn, 2004) on the MT 2003 102 out"
W11-1011,N09-1027,0,0.0350449,"6). In SAMT rule extraction, rules whose left-hand sides correspond exactly to a target-side parse node t retain that label in the grammar. Additional nonterminal labels of the form t1 + t2 are created for rules spanning two adjacent parse nodes, while categorial grammar–style nonterminals t1 /t2 and t1 	2 are used for rules spanning a partial t1 node that is missing a t2 node to its right or left. These compound nonterminals in practice lead to a very large label set. Probability estimates for rules with the same structure up to labeling can be combined with the use of a preference grammar (Venugopal et al., 2009), which replaces the variant labelings with a single SCFG rule using generic “X” labels. The generic rule’s “preference” over possible labelings is stored as a probability distribution inside the rule for use at decoding time. Preference grammars thus reduce the label set size to one for the purposes of some feature calculations — which avoids the fragmentation of rule scores due to labeling ambiguity — but the original labels persist for specifying which rules may combine with which others. Chiang (2010) extended SAMT-style labels to both source- and target-side parses, also introducing a mec"
W11-1011,C08-1139,0,0.0201321,"ice among syntax-based statistical machine translation systems is the use of synchronous context-free grammar (SCFG), where a source-language string and a target-language string are produced simultaneously by applying a series of re-write rules. Given a parallel corpus that has been statistically word-aligned and annotated with constituency structure on one or both sides, SCFG models for MT can be learned via a variety of methods. Parsing may be applied on the source side (Liu et al., 2006), on the target side (Galley et al., 2004), or on both sides of the parallel corpus (Lavie et al., 2008; Zhechev and Way, 2008). In any of these cases, using the raw label set from source- and/or target-side parsers can be undesirable. Label sets used in statistical parsers are usually inherited directly from monolingual treebank projects, where the inventory of category labels was designed by independent teams of human linguists. These labels sets are not necessarily ideal for statistical parsing, let alone for bilingual syntax-based translation models. Further, the side(s) on which syntax is represented defines the nonterminal label space used by the resulting SCFG. A pair of aligned adjectives, for example, may be"
W11-1011,W06-3119,0,0.160323,"o a baseline syntax-based MT system (Section 5). In our analysis of the results (Section 6), we find that the largest immediate effect of coarsening the label set is to reduce the number of fully abstract hierarchical SCFG rules present in the grammar. These rules’ increased permissiveness, in 99 turn, directs the decoder’s search into a largely disjoint realm from the search space explored by the baseline system. A full summary and ideas for future work are given in Section 7. 2 Related Work One example of modifying the SCFG nonterminal set is seen in the Syntax-Augmented MT (SAMT) system of Zollmann and Venugopal (2006). In SAMT rule extraction, rules whose left-hand sides correspond exactly to a target-side parse node t retain that label in the grammar. Additional nonterminal labels of the form t1 + t2 are created for rules spanning two adjacent parse nodes, while categorial grammar–style nonterminals t1 /t2 and t1 	2 are used for rules spanning a partial t1 node that is missing a t2 node to its right or left. These compound nonterminals in practice lead to a very large label set. Probability estimates for rules with the same structure up to labeling can be combined with the use of a preference grammar (Ve"
W11-1015,2009.mtsummit-posters.2,1,0.92046,", most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat “dummy” parse for the missing tree. Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010). However, we significantly broaden the scope of allowable rules compared to the Stat-XFER heuristics, and our approach differs from Chiang’s system in its respect of the linguistic constituency constraints expressed in the input tree structure. In summary, we attempt to extract the greatest possible number of syntactically motivated rules while not allowing them to violate explicit constituent boundaries on either the source or target side. This is achieved by allowing creation of virtual nodes, by allowing multiple decompositions of"
W11-1015,P05-1033,0,0.538753,"ltering schemes will also provide a key to future gains. 1 Introduction Syntax-based machine translation systems, regardless of the underlying formalism they use, depend on a method for acquiring bilingual rules in that formalism to build the system’s translation model. In modern syntax-based MT, this formalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods. Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed. Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplyin"
W11-1015,P10-1146,0,0.092087,"ormalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods. Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed. Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat “dummy” parse for the missing tree. Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010). However, we significantly broaden the scope of allowable rules compared to the"
W11-1015,N04-1035,0,0.138243,"build the system’s translation model. In modern syntax-based MT, this formalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods. Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed. Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat “dummy” parse for the missing tree. Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010). However, we s"
W11-1015,P06-1121,0,0.488834,"Yes No No Yes Multiple Alignments — No No No No Yes Virtual Nodes — Some No Yes Yes Yes Multiple Derivations Yes No Yes Yes Yes Yes Table 1: Comparisons between the rule extractor described in this paper and other SCFG rule extraction methods. 3 Comparison to Other Methods Table 1 compares the rule extractor described in Section 2 to other SCFG extraction methods described in the literature. We include comparisons of our work against the Hiero system (Chiang, 2005), the Stat-XFER system rule learner most recently described by Ambati et al. (2009), the composed version of GHKM rule extraction (Galley et al., 2006), the so-called Syntax-Augmented MT (SAMT) system (Zollmann and Venugopal, 2006), and a Hiero– SAMT extension with source- and target-side syntax described by Chiang (2010). Note that some of these methods make use of only target-side parse trees — or no parse trees at all, in the case of Hiero — but our primary interest in comparison is the constraints placed on the rule extraction process rather than the final output form of the rules themselves. We highlight four specific dimensions along these lines. Tree Constraints. As we mentioned in this paper’s introduction, we do not allow any part o"
W11-1015,W10-1709,1,0.736799,"defined tags. Therefore, before rule extraction, we globally correct the nodel labels of allnumeral terminals in English and certain punctuation marks in both English and Chinese. Second, we attempt to reduce derivational ambiguity in cases where the same SCFG right-hand side appears in the grammar after extraction with a large number of possible left-hand-side labels. To this end, we sort the possible left-hand sides by frequency for each unique right-hand side, and we remove the least frequent 10 percent of the label distribution. Our translation model scoring is based on the feature set of Hanneman et al. (2010). This includes the standard bidirectional conditional maximumlikelihood scores at both the word and phrase level on the right-hand side of rules. We also include maximum-likelihood scores for the left-hand-side label given all or part of the right-hand side. Using statistics local to each rule, we set binary indicator features for rules whose frequencies are ≤ 3, plus five additional indicator features according to the format of the rule’s right-hand side, such as whether it is fully abstract. Since the system in this paper is not constructed using any non-syntactic rules, we do not include t"
W11-1015,N03-1017,0,0.0284245,"act a number of SCFG rules that are licensed by this input. 2.1 Node Alignment Our algorithm first computes a node alignment between the parallel trees. A node s in tree S is aligned to a node t in tree T if the following constraints are 136 met. First, all words in the yield of s must either be aligned to words within the yield of t, or they must be unaligned. Second, the reverse must also hold: all words in the yield of t must be aligned to words within the yield of s or again be unaligned. This is analogous to the word-alignment consistency constraint of phrase-based SMT phrase extraction (Koehn et al., 2003). In Figure 1, for example, the NP dominating the French words les voitures bleues is aligned to the equivalent English NP node dominating blue cars. As in phrase-based SMT, where a phrase in one language may be consistent with multiple possible phrases in the other language, we allow parse nodes in both trees to have multiple node alignments. This is in contrast to one-derivation rule extractors such as that of Lavie et al. (2008), in which each node in S may only be aligned to a single node in T and vice versa. The French NP node Ma m`ere, for example, aligns to both the NNP and NP nodes in"
W11-1015,W08-0411,1,0.953875,"tax-based MT, this formalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods. Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed. Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat “dummy” parse for the missing tree. Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010). However, we significantly broaden the scope of allowable rules"
W11-1015,W09-0424,0,0.0174942,"set. Any improvement in phrase pair coverage during the extraction stage is thus directly passed along to decoding. For hierarchical rules, we experiment with retaining the 10,000 or 100,000 most frequently extracted unique rules. We also separate fully abstract hierarchical rules from partially lexicalized hierarchical rules, and in a further selection technique we retain the 5,000 most frequent abstract and 100,000 most frequent partially lexicalized rules. Given these final rule sets, we tune our MT systems on the NIST MT 2006 data set using the minimum error-rate training package Z-MERT (Zaidan, 2009), and we test on NIST MT 2003. Both sets have four reference translations. Table 4 presents case-insensitive evaluation results on the test set according to the automatic metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009).4 The trend in the results is that including a larger grammar is generally better for performance, but filtering techniques also play a substantial role in determining how well a given grammar will perform at run time. We first compare the results in Table 4 for different rule sets all filtered the same way at decoding time"
W11-1015,J03-1002,0,0.00508811,"y within some limit, although in our case we use a rank limit on a rule’s right-hand side rather than a limit on the depth of the subnode subtractions. Our constraint achieves the goal of controlling the size of the rule set while remaining flexibile in terms of depth depending on the shape of the parse trees. 4 Experiments We conducted experiments with our rule extractor on the FBIS corpus, made up of approximately 302,000 Chinese–English sentence pairs. We parsed the corpus with the Chinese and English grammars of the Berkeley parser (Petrov and Klein, 2007) and word-aligned it with GIZA++ (Och and Ney, 2003). The parsed and word-aligned FBIS corpus served as the input to our rule extractor, which we ran with a number of different settings. First, we acquired a baseline rule extraction (“xfer-orig”) from our corpus using an implementation of the basic Stat-XFER rule learner (Lavie et al., 2008), which decomposes each input tree pair into a single set of minimal SCFG rules2 using only original nodes in the parse trees. Next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also only make use of original nodes (“compatible”). Fina"
W11-1015,P02-1040,0,0.102082,"requently extracted unique rules. We also separate fully abstract hierarchical rules from partially lexicalized hierarchical rules, and in a further selection technique we retain the 5,000 most frequent abstract and 100,000 most frequent partially lexicalized rules. Given these final rule sets, we tune our MT systems on the NIST MT 2006 data set using the minimum error-rate training package Z-MERT (Zaidan, 2009), and we test on NIST MT 2003. Both sets have four reference translations. Table 4 presents case-insensitive evaluation results on the test set according to the automatic metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009).4 The trend in the results is that including a larger grammar is generally better for performance, but filtering techniques also play a substantial role in determining how well a given grammar will perform at run time. We first compare the results in Table 4 for different rule sets all filtered the same way at decoding time. With only 10,000 hierarchical rules in use (“10k”), the improvements in scores indicate that an important contribution is being made by the additional phrase pair coverage provided by each suc4 For METEOR"
W11-1015,N07-1051,0,0.0584882,"ned to the French D+N and N nodes. we do this exhaustively within some limit, although in our case we use a rank limit on a rule’s right-hand side rather than a limit on the depth of the subnode subtractions. Our constraint achieves the goal of controlling the size of the rule set while remaining flexibile in terms of depth depending on the shape of the parse trees. 4 Experiments We conducted experiments with our rule extractor on the FBIS corpus, made up of approximately 302,000 Chinese–English sentence pairs. We parsed the corpus with the Chinese and English grammars of the Berkeley parser (Petrov and Klein, 2007) and word-aligned it with GIZA++ (Och and Ney, 2003). The parsed and word-aligned FBIS corpus served as the input to our rule extractor, which we ran with a number of different settings. First, we acquired a baseline rule extraction (“xfer-orig”) from our corpus using an implementation of the basic Stat-XFER rule learner (Lavie et al., 2008), which decomposes each input tree pair into a single set of minimal SCFG rules2 using only original nodes in the parse trees. Next, we tested the effect of allowing multiple decompositions by running our own rule learner, but restricting its rules to also"
W11-1015,2006.amta-papers.25,0,0.0191272,"les. We also separate fully abstract hierarchical rules from partially lexicalized hierarchical rules, and in a further selection technique we retain the 5,000 most frequent abstract and 100,000 most frequent partially lexicalized rules. Given these final rule sets, we tune our MT systems on the NIST MT 2006 data set using the minimum error-rate training package Z-MERT (Zaidan, 2009), and we test on NIST MT 2003. Both sets have four reference translations. Table 4 presents case-insensitive evaluation results on the test set according to the automatic metrics BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009).4 The trend in the results is that including a larger grammar is generally better for performance, but filtering techniques also play a substantial role in determining how well a given grammar will perform at run time. We first compare the results in Table 4 for different rule sets all filtered the same way at decoding time. With only 10,000 hierarchical rules in use (“10k”), the improvements in scores indicate that an important contribution is being made by the additional phrase pair coverage provided by each suc4 For METEOR scoring we use version 1.0"
W11-1015,C08-1139,0,0.0394353,"Missing"
W11-1015,W06-3119,0,0.672591,"anslation model. In modern syntax-based MT, this formalism is often synchronous context-free grammar (SCFG), and the SCFG rules are obtained automatically from parallel data through a large variety of methods. Some SCFG rule extraction techniques require only Viterbi word alignment links between the source and target sides of the input corpus (Chiang, 2005), while methods based on linguistic constituency structure require the source and/or target side of the input to be parsed. Among such techniques, most retain the dependency on Viterbi word alignments for each sentence (Galley et al., 2004; Zollmann and Venugopal, 2006; Lavie et al., 2008; Chiang, 2010) while others make use of a general, This paper describes a new, general-purpose rule extractor intended for cases in which two parse trees and Viterbi word alignment links are provided for each sentence, although compatibility with singleparse-tree extraction methods can be achieved by supplying a flat “dummy” parse for the missing tree. Our framework for rule extraction is thus most similar to the Stat-XFER system (Lavie et al., 2008; Ambati et al., 2009) and the tree-to-tree situation considered by Chiang (2010). However, we significantly broaden the scope"
W11-2107,W05-0909,1,0.108938,"t This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. 1 Introduction The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010). However, previous versions of the metric are still limited by lack of punctuation handling, noise in paraphrase matching, and lack of discrimination between word types. We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists. We show that the addition of these resources to Meteor allows tuning versions"
W11-2107,P05-1074,0,0.0527874,"based organization Language English Czech French German Spanish Corpus Size (sents) 836M 230M 374M 309M 168M FW Learned 93 68 85 92 66 Table 2: Monolingual corpus size (words) and number of function words learned for each language While intended for Meteor evaluation, use of this normalizer is a suitable preprocessing step for other metrics to improve accuracy when reference sentences are stylistically different from hypotheses. Filtered Paraphrase Tables The original Meteor paraphrase tables (Denkowski and Lavie, 2010b) are constructed using the phrase table “pivoting” technique described by Bannard and Callison-Burch (2005). Many paraphrases suffer from word accumulation, the appending of unaligned words to one or both sides of a phrase rather than finding a true rewording from elsewhere in parallel data. To improve the precision of the paraphrase tables, we filter out all cases of word accumulation by removing paraphrases where one phrase is a substring of the other. Table 1 lists the number of phrase pairs found in each paraphrase table before and after filtering. In addition to improving accuracy, the reduction of phrase table sizes also reduces the load time and memory usage of the Meteor paraphrase matcher."
W11-2107,2010.amta-papers.20,1,0.886854,"eor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. 1 Introduction The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010). However, previous versions of the metric are still limited by lack of punctuation handling, noise in paraphrase matching, and lack of discrimination between word types. We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists. We show that the addition of these resources to Meteor allows tuning versions of the metric that show hi"
W11-2107,W10-1751,1,0.876193,"eor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system. 1 Introduction The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010). However, previous versions of the metric are still limited by lack of punctuation handling, noise in paraphrase matching, and lack of discrimination between word types. We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists. We show that the addition of these resources to Meteor allows tuning versions of the metric that show hi"
W11-2107,W08-0509,0,0.0248444,"le 4: Optimal Meteor parameters for WMT target languages on 2009 and 2010 data (Meteor 1.3 Ranking) devtest set, we select a version of Meteor by exploring the effectiveness of using multiple versions of the metric to tune phrase-based translation systems for the same language pair. We use the 2009 NIST Open Machine Translation Evaluation Urdu-English parallel data (Przybocki, 2009) plus 900M words of monolingual data from the English Gigaword corpus (Parker et al., 2009) to build a standard Moses system (Hoang et al., 2007) as follows. Parallel data is word aligned using the MGIZA++ toolkit (Gao and Vogel, 2008) and alignments are symmetrized using the “growdiag-final-and” heuristic. Phrases are extracted using standard phrase-based heuristics (Koehn et al., 2003) and used to build a translation table and lexicalized reordering model. A standard SRI 5-gram language model (Stolke, 2002) is estimated from monolingual data. Using Z-MERT, we tune this system to baseline metrics as well as the versions of Meteor discussed in previous sections. We also tune to a balanced Tuning version of Meteor designed to minimize bias. This data set provides a single set of reference translations for MERT. To account fo"
W11-2107,P07-2045,0,0.0330849,"to-end tune-test runs on this data set. The versions of Meteor corresponding to the translation evaluation task submissions, (Ranking and Adequacy), are described in Sections 3 through 5 while the submission to the tunable metrics task, (Tuning), is described in Section 6. 2 New Metric Resources 2.1 Meteor Normalizer Whereas previous versions of Meteor simply strip punctuation characters prior to scoring, version 1.3 includes a new text normalizer intended specifically for translation evaluation. The normalizer first replicates the behavior of the tokenizer distributed with the Moses toolkit (Hoang et al., 2007), including handling of non-breaking prefixes. After tokenization, we add several rules for normalization, intended to reduce meaning-equivalent punctuation styles to common forms. The following two rules are particularly helpful: • Remove dashes between hyphenated words. (Example: far-off → far off) 85 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 85–91, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Language English Czech German Spanish French • Remove full stops in acronyms/initials. (Example: U.N. → UN) Consider the b"
W11-2107,N03-1017,0,0.0393879,"the effectiveness of using multiple versions of the metric to tune phrase-based translation systems for the same language pair. We use the 2009 NIST Open Machine Translation Evaluation Urdu-English parallel data (Przybocki, 2009) plus 900M words of monolingual data from the English Gigaword corpus (Parker et al., 2009) to build a standard Moses system (Hoang et al., 2007) as follows. Parallel data is word aligned using the MGIZA++ toolkit (Gao and Vogel, 2008) and alignments are symmetrized using the “growdiag-final-and” heuristic. Phrases are extracted using standard phrase-based heuristics (Koehn et al., 2003) and used to build a translation table and lexicalized reordering model. A standard SRI 5-gram language model (Stolke, 2002) is estimated from monolingual data. Using Z-MERT, we tune this system to baseline metrics as well as the versions of Meteor discussed in previous sections. We also tune to a balanced Tuning version of Meteor designed to minimize bias. This data set provides a single set of reference translations for MERT. To account for the variance of MERT, we run end-to-end tuning 3 times for each metric and report the average results on two unseen test sets: newswire and weblog. Test"
W11-2107,2005.mtsummit-papers.11,0,0.0705488,"his can be problematic for ranking-based evaluations where two system 86 outputs can differ by a single word, such as mistranslating either a main verb or a determiner. To improve Meteor’s discriminative power in such cases, we introduce a function word list for each WMT language and a new δ parameter to adjust the relative weight given to content words (any word not on the list) versus function words (see Section 3). Function word lists are estimated according to relative frequency in large monolingual corpora. For each language, we pool freely available WMT 2011 data consisting of Europarl (Koehn, 2005), news (sentence-uniqued), and news commentary data. Any word with relative frequency of 10−3 or greater is added to the function word list. Table 2 lists corpus size and number of function words learned for each language. In addition to common words, punctuation symbols consistently rise to the tops of function word lists. 3 Meteor Scoring Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores. This section describes our extended version of the metric. For a hypothesis-reference pair, the search space of possible ali"
W11-2107,W09-0424,0,0.0144027,"Missing"
W11-2107,P02-1040,0,0.115951,"higher correlation with human translation rankings and adequacy scores on unseen 1 The metric name has previously been stylized as “METEOR” or “M ETEOR”. As of version 1.3, the official stylization is simply “Meteor”. test data. The evaluation resources are modular, usable with any other evaluation metric or MT software. We also conduct a MT system tuning experiment on Urdu-English data to compare the effectiveness of using multiple versions of Meteor in minimum error rate training. While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set. The versions of Meteor corresponding to the translation evaluation task submissions, (Ranking and Adequacy), are described in Sections 3 through 5 while the submission to the tunable metrics task, (Tuning), is described in Section 6. 2 New Metric Resources 2.1 Meteor Normalizer Whereas previous versions of Meteor simply strip punctuation characters prior to scoring, version 1.3 includes a new text normalizer intended specifically for translation evaluation. The normaliz"
W11-2107,2006.amta-papers.25,0,0.498488,"or the official Ranking version. 88 Tune / Test MT08 MT09 Tune / Test P2 P3 Meteor-1.2 r MT08 MT09 0.620 0.625 0.612 0.630 P2 P3 -0.640 -0.596 -0.638 -0.600 Meteor-1.3 r MT08 MT09 0.650 0.636 0.642 0.648 P2 P3 -0.642 -0.594 -0.625 -0.612 Table 6: Meteor 1.2 and 1.3 correlation with adequacy and H-TER scores on tune and test data 5.1 Generalization to Other Tasks To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations. Data sets include 2008 and 2009 NIST Open Machine Translation Evaluation adequacy data (Przybocki, 2009) and GALE P2 and P3 H-TER data (Olive, 2005). For each type of judgment, metric versions are tuned and tested on each year and scores are compared. We compare Meteor 1.3 results with those from version 1.2 with results shown in Table 6. For both adequacy data sets, Meteor 1.3 significantly outperforms version 1.2 on both tune and test data. The version tuned on MT09 data is selected as the official Adequacy version of Meteor 1.3. H-TER versions either show no improve"
W11-2107,W10-1703,0,\N,Missing
W11-2117,P05-1074,0,0.0452899,"is year added a novel-bigram penalty that penalizes bigrams in the output if they do not appear in one of the system outputs. This is the complement of our bigram match count features (and, since, we have a length feature, the same up to rearranging weights). However, they threshold it to indicate whether the bigram appears at all instead of how many systems support the bigram. 4 Resources The resources we use are constrained to those provided for the shared task. For the paraphrase matches described in Section 2.1, METEOR (Denkowski and Lavie, 2010) trains its paraphrase tables via pivoting (Bannard and Callison-Burch, 2005). The phrase tables are trained using parallel data from Europarl v6 (Koehn, 2005) (fr-en, es-en, de-en, and es-de), news commentary (fr-en, es-en, de-en, and cz-en), United Nations (fr-en and es-en), and CzEng (cz-en) (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–8. 4.1 Language Modeling As with previous versions of the system, we use language model log probability as a feature to bias translations towards fluency. We add a second feature per language model that counts OOVs, allowing MERT to independently tune the OOV penalty. Language models often have poor OOV estimates for translation becaus"
W11-2117,W10-1751,1,0.928007,"to switch after each word. Interestingly, BBN (Rosti et al., 2010) this year added a novel-bigram penalty that penalizes bigrams in the output if they do not appear in one of the system outputs. This is the complement of our bigram match count features (and, since, we have a length feature, the same up to rearranging weights). However, they threshold it to indicate whether the bigram appears at all instead of how many systems support the bigram. 4 Resources The resources we use are constrained to those provided for the shared task. For the paraphrase matches described in Section 2.1, METEOR (Denkowski and Lavie, 2010) trains its paraphrase tables via pivoting (Bannard and Callison-Burch, 2005). The phrase tables are trained using parallel data from Europarl v6 (Koehn, 2005) (fr-en, es-en, de-en, and es-de), news commentary (fr-en, es-en, de-en, and cz-en), United Nations (fr-en and es-en), and CzEng (cz-en) (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–8. 4.1 Language Modeling As with previous versions of the system, we use language model log probability as a feature to bias translations towards fluency. We add a second feature per language model that counts OOVs, allowing MERT to independently tune the OOV"
W11-2117,P10-4002,0,0.0325963,"e concerned with combining translations into German that may be segmented differently. These can be due to stylistic choices; for example both “jahrzehnte lang” and “jahrzehntelang” appear with approximately equal frequency as shown in Table 1. Translation systems add additional biases due to the various preprocessing approaches taken by individual sites and inherent biases in models such as word alignment. In order to properly align differently segmented words, we normalize by segmenting all system outputs and our language model training data using 148 the single-best segmentation from cdec (Dyer et al., 2010). Running our system therefore produces segmented German output. Internally, we tuned towards segmented references but for final output it is desirable to rejoin compound words. Since the cdec segmentation was designed for GermanEnglish translation, no corresponding desegmenter was provided. We created a German desegmenter in the natural way: segment German words then invert the mapping to identify words that should be rejoined. To do so, we ran every word from the German monolingual data and system outputs through the cdec segmenter, counted both the compounded and segmented versions in the m"
W11-2117,W10-1744,1,0.851764,"otone. This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Translation’s system combination task. We show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments. Humans judged our combination the best on eight of ten tracks. 1 Twice that produced by nuclear plants Introduction We participated in all ten tracks of the 2011 Workshop on Machine Translation system combination task as cmu-heafield-combo. This uses a system combination scheme that builds on our prior work (Heafield and Lavie, 2010), especially with respect to language modeling and handling nonEnglish languages. We present a summary of the system, describe improvements, list the data used (all of the constrained monolingual data), and present automatic results in anticipation of human evaluation by the workshop. 2 Alignment Our Combination Scheme Given single-best outputs from each system, the scheme aligns system outputs then searches a space based on these alignments. The scheme is a continuation of our previous system (Heafield and Lavie, 2010) so we describe unchanged parts of the system in less detail, preferring in"
W11-2117,W11-2123,1,0.694484,"an important role here because frequent anonymization markers such as “[firstname]” do not appear in the large language model. To scale to larger language models, we use 147 BigFatLM1 , an open-source builder of large unpruned models with modified Kneser-Ney smoothing. Then, we filter the models to the system outputs. In order for an n-gram to be queried, all of the words must appear in system outputs for the same sentence. This enables a filtering constraint stronger than normal vocabulary filtering, which permits ngrams supported only by words in different sentences. Finally, we use KenLM (Heafield, 2011) for inference at runtime. Our primary use of data is for language modeling. We used essientially every constrained resource available and appended them together to build one large model. For every language, we used the provided Europarl v6 (Koehn, 2005), News Crawl, and News Commentary corpora. In addition, we used: English Gigaword Fourth Edition (Parker et al., 2009) and the English parts of United Nations documents, Giga-FrEn, and CzEng (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–7. For the Haitian Creole-English tasks, we built a separate language model on the SMS messages and used it alo"
W11-2117,W09-0406,0,0.0266442,"inexact matches collect more votes that better handle word order, we use both sets of features. However, 146 the limit N may be different i.e. Ne = 2 counts exact matches up to length 2 and Na = 3 counts inexact matches up to length 3. System 1: Supported Proposal of France System 2: Support for the Proposal of France Candidate: Support for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Trigram 1 1 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. 3 Related Work Hypothesis selection (Hildebrand and Vogel, 2009) selects an entire sentence at a time instead of picking and merging words. This makes the approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we"
W11-2117,P07-2045,0,0.00915827,"Missing"
W11-2117,2005.mtsummit-papers.11,0,0.299678,"of the system outputs. This is the complement of our bigram match count features (and, since, we have a length feature, the same up to rearranging weights). However, they threshold it to indicate whether the bigram appears at all instead of how many systems support the bigram. 4 Resources The resources we use are constrained to those provided for the shared task. For the paraphrase matches described in Section 2.1, METEOR (Denkowski and Lavie, 2010) trains its paraphrase tables via pivoting (Bannard and Callison-Burch, 2005). The phrase tables are trained using parallel data from Europarl v6 (Koehn, 2005) (fr-en, es-en, de-en, and es-de), news commentary (fr-en, es-en, de-en, and cz-en), United Nations (fr-en and es-en), and CzEng (cz-en) (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–8. 4.1 Language Modeling As with previous versions of the system, we use language model log probability as a feature to bias translations towards fluency. We add a second feature per language model that counts OOVs, allowing MERT to independently tune the OOV penalty. Language models often have poor OOV estimates for translation because they come not from new text in the same language but from new text in a differen"
W11-2117,W10-1746,0,0.0200391,"4 5 Bigram 2 3 Trigram 1 1 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. 3 Related Work Hypothesis selection (Hildebrand and Vogel, 2009) selects an entire sentence at a time instead of picking and merging words. This makes the approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we never do. In practice, using both signals would likely work better. Confusion networks (Rosti et al., 2010; Narsale, 2010) are the dominant method for system combination. These base their word order on one system, dubbed the backbone, and have all systems vote on editing the backbone. Word order is largely fixed to that of one system; by contrast, ours can piece together"
W11-2117,P02-1040,0,0.0824806,"ave that a score of n2 . The total score is a sum of these squares, favoring compounds that cover more words. Maximizing the score is a fast and exact dynamic programming algorithm. Casing of unchanged words comes from equally-weighted system votes at the character level while casing of rejoined words is based on the majority appearance in the corpus; this is almost always initial capital. We ran our desegmenter followed by the workshop’s provided detokenizer to produce the submitted output. 5 Results We tried many variations on the scheme, such as selecting different systems, tuning to BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2010), and changing the structure of the match count features from Section 2.3. To try these, we ran MERT 242 times, or about 24 times for each of the ten tasks in which we participated. Then we selected the best performing systems on the tuning set and submitted them, with the secondary system chosen to meaningfully differ from the primary while still scoring well. Once the evaluation released references, we scored against them to generate Table 2. On the featured Haitian Creole task, we show no and sometimes even negative improvement. This we attribute to the"
W11-2117,W10-1748,0,0.0204424,"e approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we never do. In practice, using both signals would likely work better. Confusion networks (Rosti et al., 2010; Narsale, 2010) are the dominant method for system combination. These base their word order on one system, dubbed the backbone, and have all systems vote on editing the backbone. Word order is largely fixed to that of one system; by contrast, ours can piece together word orders taken from multiple systems. In a loose sense, our approach is a confusion network where the backbone is permitted to switch after each word. Interestingly, BBN (Rosti et al., 2010) this year added a novel-bigram penalty that penalizes bigrams in the output if they do not appear in one of the system outputs. This is th"
W11-2117,2006.amta-papers.25,0,0.0543806,"upport for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Trigram 1 1 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. 3 Related Work Hypothesis selection (Hildebrand and Vogel, 2009) selects an entire sentence at a time instead of picking and merging words. This makes the approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we never do. In practice, using both signals would likely work better. Confusion networks (Rosti et al., 2010; Narsale, 2010) are the dominant method for system combination. These base their word order on one system, dubbed the backbone, and have all systems vote on editing the backbone. Word order is largely fixed to that of"
W11-2143,P05-1033,0,0.127846,"Missing"
W11-2143,P10-1146,0,0.0560178,"Missing"
W11-2143,clark-lavie-2010-loonybin,1,0.717422,"Missing"
W11-2143,W08-0509,0,0.0153329,"compared with a coverage of 545,000 words without using Giga-FrEn. We made the decision to leave the training data in mixed case for our entire system-building process. At the cost of slightly sparser estimates for word alignments and translation probabilities, a mixedcase system avoids the extra step of building a statistical recaser to treat our system’s output. 2.2 Grammar Extraction and Scoring Once we had assembled the final training corpus, we annotated it with statistical word alignments and constituent parse trees on both sides. Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the 366 grow-diag-final-and heuristic (Koehn et al., 2005). For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007). Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission. We extracted both syntactic and non-syntactic portions of the translation grammar. The non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based SMT (Koehn et al., 2003). The syn"
W11-2143,W10-1709,1,0.798072,"past year, the statistical transfer machine translation group at Carnegie Mellon University has continued its work on large-scale syntactic MT systems based on automatically acquired synchronous context-free grammars (SCFGs). For the 2011 Workshop on Machine Translation, we built a hybrid MT system, including both syntactic and non-syntactic rules, and submitted it as a constrained entry to the French–English translation task. This is our fourth yearly submission to the WMT shared translation task. In design and construction, the system is similar to our submission from last year’s workshop (Hanneman et al., 2010), with changes in the methods we employed for training data selection and SCFG filtering. Continuing WMT’s general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French–English training data and an English language model of 1.8 billion words. Decod2.1 Training Data Selection WMT 2011’s provided French–English training data consisted of 36.8 million sentence pairs from the Europarl, news commentary, UN documents, and GigaFrEn corpora (Table 1). The first three of these are, for the most part, clean data resources that have be"
W11-2143,N03-1017,0,0.0122273,"GIZA++ (Gao and Vogel, 2008), then symmetrized with the 366 grow-diag-final-and heuristic (Koehn et al., 2005). For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007). Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission. We extracted both syntactic and non-syntactic portions of the translation grammar. The non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based SMT (Koehn et al., 2003). The syntactic grammar was produced using the method of Lavie et al. (2008), which decomposes each pair of word-aligned parse trees into a series of minimal SCFG rules. The word alignments are first generalized to node alignments, where nodes s and t are aligned between the source and target parse trees if all word alignments in the yield of s land within the yield of t and vice versa. Minimal SCFG rules are derived from adjacent levels of node alignments: the labels from each pair of aligned nodes forms a rule’s left-hand side, and the right-hand side is made up of the labels from the fronti"
W11-2143,2005.iwslt-1.8,0,0.0317919,"sion to leave the training data in mixed case for our entire system-building process. At the cost of slightly sparser estimates for word alignments and translation probabilities, a mixedcase system avoids the extra step of building a statistical recaser to treat our system’s output. 2.2 Grammar Extraction and Scoring Once we had assembled the final training corpus, we annotated it with statistical word alignments and constituent parse trees on both sides. Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the 366 grow-diag-final-and heuristic (Koehn et al., 2005). For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007). Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission. We extracted both syntactic and non-syntactic portions of the translation grammar. The non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based SMT (Koehn et al., 2003). The syntactic grammar was produced using the method of Lavie et al. (2008), which decompo"
W11-2143,W04-3250,0,0.0480565,"Missing"
W11-2143,W08-0411,1,0.8547,"and heuristic (Koehn et al., 2005). For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007). Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission. We extracted both syntactic and non-syntactic portions of the translation grammar. The non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based SMT (Koehn et al., 2003). The syntactic grammar was produced using the method of Lavie et al. (2008), which decomposes each pair of word-aligned parse trees into a series of minimal SCFG rules. The word alignments are first generalized to node alignments, where nodes s and t are aligned between the source and target parse trees if all word alignments in the yield of s land within the yield of t and vice versa. Minimal SCFG rules are derived from adjacent levels of node alignments: the labels from each pair of aligned nodes forms a rule’s left-hand side, and the right-hand side is made up of the labels from the frontier of aligned nodes encountered when walking the left-hand side’s subtrees."
W11-2143,W09-0424,0,0.0227547,"d, it applied a lexical-match filter such that a partially lexicalized rule was retained only if all its lexicalized source phrases up to bigrams matched the intended tuning or testing set. The final translation grammar in this case was made up of three parts: all phrase pair rules matching the test set (as before), the 100,000 most frequently extracted partially lexicalized rules whose bigrams match the test set, and the 2000 most frequently extracted fully abstract rules. 3 Experimental Results and Analysis We tuned each system variant on the newstest2008 data set, using the Z-MERT package (Zaidan, 2009) for minimum error-rate training to the BLEU metric. We ran development tests on the newstest2009 and newstest2010 data sets; Table 2 reports the results obtained according to various automatic metrics. The evaluation consists of case-insensitive scoring according to METEOR 1.0 (Lavie and Denkowski, 2009) tuned to HTER with the exact, stemming, and synonymy modules enabled, case-insensitive BLEU (Papineni et al., 2002) as implemented by the NIST mteval-v13 script, and case-insensitive TER 0.7.25 (Snover et al., 2006). Table 2 gives comparative results for two major systems: one based on our WM"
W11-2143,P02-1040,0,0.0809728,"set, and the 2000 most frequently extracted fully abstract rules. 3 Experimental Results and Analysis We tuned each system variant on the newstest2008 data set, using the Z-MERT package (Zaidan, 2009) for minimum error-rate training to the BLEU metric. We ran development tests on the newstest2009 and newstest2010 data sets; Table 2 reports the results obtained according to various automatic metrics. The evaluation consists of case-insensitive scoring according to METEOR 1.0 (Lavie and Denkowski, 2009) tuned to HTER with the exact, stemming, and synonymy modules enabled, case-insensitive BLEU (Papineni et al., 2002) as implemented by the NIST mteval-v13 script, and case-insensitive TER 0.7.25 (Snover et al., 2006). Table 2 gives comparative results for two major systems: one based on our WMT 2011 data selection as outlined in Section 2.1, and one based on the smaller WMT 2010 training data that we used last year (8.6 million sentence pairs). Each system was run with the two grammar filtering variants described in Section 2.4: the 10,000 most frequently extracted hierarchical rules of any type (“10k”), and a combination of the 2000 most frequently extracted abstract rules and the 100,000 most frequently e"
W11-2143,N07-1051,0,0.0277738,"es for word alignments and translation probabilities, a mixedcase system avoids the extra step of building a statistical recaser to treat our system’s output. 2.2 Grammar Extraction and Scoring Once we had assembled the final training corpus, we annotated it with statistical word alignments and constituent parse trees on both sides. Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the 366 grow-diag-final-and heuristic (Koehn et al., 2005). For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007). Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission. We extracted both syntactic and non-syntactic portions of the translation grammar. The non-syntactic grammar was extracted from the parallel corpus and word alignments following the standard heuristics of phrase-based SMT (Koehn et al., 2003). The syntactic grammar was produced using the method of Lavie et al. (2008), which decomposes each pair of word-aligned parse trees into a series of minimal SCFG rules. The word alignments are first generalized to node"
W11-2143,W10-1722,0,0.0551019,"Missing"
W11-2143,2006.amta-papers.25,0,0.0283426,"We tuned each system variant on the newstest2008 data set, using the Z-MERT package (Zaidan, 2009) for minimum error-rate training to the BLEU metric. We ran development tests on the newstest2009 and newstest2010 data sets; Table 2 reports the results obtained according to various automatic metrics. The evaluation consists of case-insensitive scoring according to METEOR 1.0 (Lavie and Denkowski, 2009) tuned to HTER with the exact, stemming, and synonymy modules enabled, case-insensitive BLEU (Papineni et al., 2002) as implemented by the NIST mteval-v13 script, and case-insensitive TER 0.7.25 (Snover et al., 2006). Table 2 gives comparative results for two major systems: one based on our WMT 2011 data selection as outlined in Section 2.1, and one based on the smaller WMT 2010 training data that we used last year (8.6 million sentence pairs). Each system was run with the two grammar filtering variants described in Section 2.4: the 10,000 most frequently extracted hierarchical rules of any type (“10k”), and a combination of the 2000 most frequently extracted abstract rules and the 100,000 most frequently extracted partially lexicalized rules that matched the test set (“2k+100k”). Our primary submission t"
W11-2143,P07-2045,0,\N,Missing
W12-3131,P96-1041,0,0.153725,"e model inference. The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). The aligned corpus is then encoded as a suffix array to facilitate sentence-level grammar extraction and scoring (Lopez, 2008). Grammars are extracted using the heuristics described by Chiang (Chiang, 2007) and feature scores are calculated according to Lopez (2008). Modified Knesser-Ney smoothed (Chen and Goodman, 1996) n-gram language models are built from the monolingual English data using the SRI language modeling toolkit (Stolke, 2002). We experiment with both 4-gram and 5-gram models. System parameters are optimized using minimum error rate training (Och, 2003) to maximize the corpus-level cased BLEU score (Papineni et al., Base: Custom: Base: Custom: Base: Custom: Base: Custom: Y a-t-il un coll` egue pour prendre la parole Y a -t-il un coll` egue pour prendre la parole Peut-ˆ etre , ` a ce sujet , puis-je dire ` a M. Ribeiro i Castro Peut-ˆ etre , ` a ce sujet , puis -je dire ` a M. Ribeiro i Castro le"
W12-3131,W11-2107,1,0.862117,"ferable to untranslated foreign words. Examples of cognate translations for OOV words in newstest 2011 are shown in Figure 2.1 1 Some OOVs are caused by misspellings in the dev-test source sentences. In these cases we can salvage misspelled English words in place of misspelled French words 264 Table 3: Newstest 2011 (dev-test) translation results 4 Experiments Beginning with a baseline translation system, we incrementally evaluate the contribution of additional data and components. System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al., 2006). For full consistency with WMT11, we use the NIST scoring script, TER-0.7.25, and Meteor-1.3 to evaluate cased, detokenized translations. Results are shown in Table 3, where each evaluation point is the result of a full tune/test run that includes MERT for parameter optimization. The baseline translation system is built from 14 million parallel sentences (Europarl, news commentary, and UN doc) and all monolingual data. Grammars are extracted using the “tight” heuristic that requires phrase pairs to be bounded by word alignments. Both 4-gram and 5-gram language m"
W12-3131,P10-4002,0,0.157879,"Missing"
W12-3131,W08-0509,0,0.0162471,"ra. This model assigns high scores to grammatical source sentences and lower scores to ungrammatical sentences and non-sentences such as site maps, large lists of names, and blog comments. Scores are normalized by number of n-grams scored per sentence (length + 1). The model is built using the SRILM toolkit (Stolke, 2002). Target language model: a 4-gram modified Kneser-Ney smoothed language model trained on English Europarl, news commentary, UN doc, and news crawl corpora. This model scores grammaticality on the target side. 262 Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). Model parameters are estimated on 2 million words of French-English Europarl and news commentary text. Scores are normalized by the number of alignment links. These features measure the extent to which translations are parallel with their source sentences. Fraction of aligned words: source-target and target-source ratios of aligned words to total words. These features balance the link-normalized alignment scores. To determine selection criteria, we use this feature set to score the news test sets from 2008 through 2011 (10K paralle"
W12-3131,W11-2123,0,0.0964037,"Missing"
W12-3131,P03-1054,0,0.00635906,"Missing"
W12-3131,2005.mtsummit-papers.11,0,0.0481993,"ors and make sentences more acceptable to human judges. The post-processing step alone yields an improvement of 0.3 BLEU to the final system. We conclude with a discussion of the impact of data size on important decisions for system building. Experimental results show that “best practice” decisions for smaller data sizes do not necessarily carry over to systems built with “WMT-scale” data, and provide some explanation for why this is the case. 2 Training Data Training data provided for the French-English translation task includes parallel corpora taken from European Parliamentary proceedings (Koehn, 2005), news commentary, and United Nations documents. Together, these sets total approximately 13 million sentences. In addition, a large, web-crawled parallel corpus termed the “Giga-FrEn” (Callison-Burch et al., 2009) is made available. While this corpus contains over 22 million parallel sentences, it is inherently noisy. Many parallel sentences crawled from the web are neither parallel nor sentences. To make use of this large data source, we employ data selection techniques discussed in the next subsection. 261 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 261–266, c"
W12-3131,N04-1022,0,0.270728,"y word boundaries. pr´e-´e´lectoral mosaˆıque d´eragulation → → → BLEU (cased) Meteor TER base 5-gram 28.4 27.4 33.7 53.2 base 4-gram 29.1 28.1 34.0 52.5 +1stdev GFE 29.3 28.3 34.2 52.1 +2stdev GFE 29.8 28.9 34.5 51.7 +5g/1K/MBR 29.9 29.0 34.5 51.5 +post-process 30.2 29.2 34.7 51.3 pre-electoral mosaique deragulation Figure 2: Examples of cognate translation 2002) on news-test 2008 (2051 sentences). This development set is chosen for its known stability and reliability. Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 - BLEU as the loss function. 3.1 Post-Processing Our final system includes a monolingual rule-based post-processing step that corrects obvious translation errors. Examples of correctable errors include capitalization, mismatched punctuation, malformed numbers, and incorrectly split compound words. We finally employ a coarse cognate translation system to handle out-of-vocabulary words. We assume that uncapitalized French source words passed through to the English output are cognates of English words and translate them by removing accents. This frequently leads to (i"
W12-3131,C08-1064,0,0.374234,"present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building. 1 Introduction We describe the French-English translation system constructed by the Avenue research group at Carnegie Mellon University for the shared translation task in the Seventh Workshop on Statistical Machine Translation. The core translation system uses the hierarchical phrase-based model described by Chiang (2007) with sentence-level grammars extracted and scored using the methods described by Lopez (2008). Improved techniques for data selection and monolingual text processing significantly improve the performance of the baseline system. Over half of all parallel data for the FrenchEnglish track is provided by the Giga-FrEn corpus (Callison-Burch et al., 2009). Assembled from crawls of bilingual websites, this corpus is known to be noisy, containing sentences that are either not parallel or not natural language. Rather than simply including or excluding the resource in its entirety, we use a relatively simple technique inspired by work in machine translation quality estimation to select the bes"
W12-3131,J03-1002,0,0.00331898,"ces and lower scores to ungrammatical sentences and non-sentences such as site maps, large lists of names, and blog comments. Scores are normalized by number of n-grams scored per sentence (length + 1). The model is built using the SRILM toolkit (Stolke, 2002). Target language model: a 4-gram modified Kneser-Ney smoothed language model trained on English Europarl, news commentary, UN doc, and news crawl corpora. This model scores grammaticality on the target side. 262 Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). Model parameters are estimated on 2 million words of French-English Europarl and news commentary text. Scores are normalized by the number of alignment links. These features measure the extent to which translations are parallel with their source sentences. Fraction of aligned words: source-target and target-source ratios of aligned words to total words. These features balance the link-normalized alignment scores. To determine selection criteria, we use this feature set to score the news test sets from 2008 through 2011 (10K parallel sentences) and calculate the mean and standard deviation of"
W12-3131,P03-1021,0,0.0384829,"Missing"
W12-3131,P02-1040,0,0.0850555,"l of the above are generally preferable to untranslated foreign words. Examples of cognate translations for OOV words in newstest 2011 are shown in Figure 2.1 1 Some OOVs are caused by misspellings in the dev-test source sentences. In these cases we can salvage misspelled English words in place of misspelled French words 264 Table 3: Newstest 2011 (dev-test) translation results 4 Experiments Beginning with a baseline translation system, we incrementally evaluate the contribution of additional data and components. System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al., 2006). For full consistency with WMT11, we use the NIST scoring script, TER-0.7.25, and Meteor-1.3 to evaluate cased, detokenized translations. Results are shown in Table 3, where each evaluation point is the result of a full tune/test run that includes MERT for parameter optimization. The baseline translation system is built from 14 million parallel sentences (Europarl, news commentary, and UN doc) and all monolingual data. Grammars are extracted using the “tight” heuristic that requires phrase pairs to be bounded by word alignment"
W12-3131,2006.amta-papers.25,0,0.0150611,". Examples of cognate translations for OOV words in newstest 2011 are shown in Figure 2.1 1 Some OOVs are caused by misspellings in the dev-test source sentences. In these cases we can salvage misspelled English words in place of misspelled French words 264 Table 3: Newstest 2011 (dev-test) translation results 4 Experiments Beginning with a baseline translation system, we incrementally evaluate the contribution of additional data and components. System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al., 2006). For full consistency with WMT11, we use the NIST scoring script, TER-0.7.25, and Meteor-1.3 to evaluate cased, detokenized translations. Results are shown in Table 3, where each evaluation point is the result of a full tune/test run that includes MERT for parameter optimization. The baseline translation system is built from 14 million parallel sentences (Europarl, news commentary, and UN doc) and all monolingual data. Grammars are extracted using the “tight” heuristic that requires phrase pairs to be bounded by word alignments. Both 4-gram and 5-gram language models are evaluated. Viterbi de"
W12-3131,2009.mtsummit-papers.16,0,0.0280406,"Missing"
W12-3131,W09-0401,0,\N,Missing
W12-3131,J07-2003,0,\N,Missing
W12-3131,D08-1076,0,\N,Missing
W13-2205,P06-1121,0,0.0416165,"sh and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules were extracted from the corpus following the method of Hanneman et al. (2011). This decomposes each tree pair into a collection of SCFG rules by exhaustively identifying aligned subtrees to serve as rule left-hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of new nodes in the parse tree to subdivide regions of flat structure. Virtual nodes are similar to the A+B extended categories of SAMT (Zollmann and Venugopal, 2006), but with the added constraint that they may not conflict with the surrounding tree structure. Because the SCFG rules are labeled with nont"
W13-2205,W12-4410,1,0.923402,"for the 2013 Workshop on Machine Translation shared translation task: French–English, Russian–English, and English–Russian. Our French–English system (§3) showcased our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011). Our Russian–English and English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation"
W13-2205,J92-4003,0,0.07177,"e 500-best outputs of a syntactic system constructed similarly to the French–English system. 6 30.8 30.9 31.1 Russian side of the training corpus. An unpruned, modified Kneser-Ney smoothed 4-gram language model (Chen and Goodman, 1996) was estimated from all available Russian text (410 million words) using the KenLM toolkit (Heafield et al., 2013). A standard hierarchical phrase-based system was trained with rule shape indicator features, obtained by replacing terminals in translation rules by a generic symbol. MIRA training was performed to learn feature weights. Additionally, word clusters (Brown et al., 1992) were obtained for the complete monolingual Russian data. Then, an unsmoothed 7-gram language model was trained on these clusters and added as a feature to the translation system. Indicator features were also added for each cluster and bigram cluster occurence. These changes resulted in an improvement of more than a BLEU point on our held-out development set. Slavic languages like Russian have a large number of different inflected forms for each lemma, representing different cases, tenses, and aspects. Since our training data is rather limited relative to the number of inflected forms that are"
W13-2205,W11-1011,1,0.933305,"alized modules to create “synthetic translation options” that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. 1 Introduction The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in three language pairs for the 2013 Workshop on Machine Translation shared translation task: French–English, Russian–English, and English–Russian. Our French–English system (§3) showcased our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011). Our Russian–English and English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastru"
W13-2205,P96-1041,0,0.103448,"Missing"
W13-2205,N13-1029,1,0.816688,"kens, contained a token of more than 30 characters, or had particularly unbalanced length ratios were also removed. After filtering, 30.9 million sentence pairs remained for rule extraction: 14.4 million from the clean data, and 16.5 million from the web data. 3.2 1 Selecting the stopping point still requires a measure of intuition. The label set size of 1814 chosen here roughly corresponds to the number of joint labels that would exist in the grammar if virtual nodes were not included. This equivalence has worked well in practice in both internal and published experiments on other data sets (Hanneman and Lavie, 2013). Preprocessing and Grammar Extraction Our French–English system uses parse trees in both the source and target languages, so tokeniza71 most common type. We conducted experiments with three different frequency cutoffs: 100, 200, and 500, with each increase decreasing the grammar size by 70–80 percent. Extracted rules each have 10 features associated with them. For an SCFG rule with source lefthand side `s , target left-hand side `t , source righthand side rs , and target right-hand side rt , they are: 3.3 • phrasal translation log relative frequencies log f (rs |rt ) and log f (rt |rs ); • la"
W13-2205,J07-2003,0,0.201588,"Missing"
W13-2205,W11-1015,1,0.856156,"hop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules were extracted from the corpus following the method of Hanneman et al. (2011). This decomposes each tree pair into a collection of SCFG rules by exhaustively identifying aligned subtrees to serve as rule left-hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of ne"
W13-2205,P11-2031,1,0.827764,"n relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomization algorithm. The closest is the variant with cutoff 200, which is generally judged to be sl"
W13-2205,P13-2121,0,0.0613657,"Missing"
W13-2205,W11-2107,1,0.844656,"f (rs |rt ) and log f (rt |rs ); • labeling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomiz"
W13-2205,W11-2123,0,0.0303697,"iterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by"
W13-2205,W12-3131,1,0.818679,"s in the various system pairs. 3 French-English Syntax System Our submission for French–English is a tree-totree translation system that demonstrates several innovations from group’s research on SCFG-based translation. 3.1 Data Selection We divided the French–English training data into two categories: clean data (Europarl, News Commentary, UN Documents) totaling 14.8 million sentence pairs, and web data (Common Crawl, Giga-FrEn) totaling 25.2 million sentence pairs. To reduce the volume of data used, we filtered non-parallel and other unhelpful segments according to the technique described by Denkowski et al. (2012). This procedure uses a lexical translation model learned from just the clean data, as well as source and target n-gram language models to compute the following feature scores: • French and English 4-gram log likelihood (normalized by length); • French–English and English–French lexical translation log likelihood (normalized by length); and, • Fractions of aligned words under the French– English and English–French models. We pooled previous years’ WMT news test sets to form a reference data set. We computed the same features. To filter the web data, we retained only sentence for which each fea"
W13-2205,P09-1019,1,0.851065,"based on a histogrambased similarity function that looks at what target labels correspond to a particular source label and vice versa. The number of clusters used is determined based on spikes in the distance between successive clustering iterations, or by the number of source, target, or joint labels remaining. Starting from a default grammar of 877 French, 2580 English, and 131,331 joint labels, we collapsed the label space for our WMT system down to 50 French, 54 English, and 1814 joint categories.1 the French–English system, using the hypergraph MERT algorithm and optimizing towards BLEU (Kumar et al., 2009). The remainder of the paper will focus on our primary innovations in the various system pairs. 3 French-English Syntax System Our submission for French–English is a tree-totree translation system that demonstrates several innovations from group’s research on SCFG-based translation. 3.1 Data Selection We divided the French–English training data into two categories: clean data (Europarl, News Commentary, UN Documents) totaling 14.8 million sentence pairs, and web data (Common Crawl, Giga-FrEn) totaling 25.2 million sentence pairs. To reduce the volume of data used, we filtered non-parallel and"
W13-2205,P10-4002,1,0.867841,"d English–Russian system demonstrate a new multiphase approach to translation that our group is using, in which synthetic translation options (§4) to supplement the default translation rule inventory that is extracted from word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria,"
W13-2205,C08-1064,0,0.0127826,"odels, we trained 4-gram Markov models using the target side of the bitext and any available monolingual data (including Gigaword for English). Additionally, we trained 7-gram language models using 600-class Brown clusters with Witten-Bell smoothing.2 5.2 Baseline System Our baseline Russian–English system is a hierarchical phrase-based translation model as implemented in cdec (Chiang, 2007; Dyer et al., 2010). SCFG translation rules that plausibly match each sentence in the development and deftest sets were extracted from the aligned parallel data using the suffix array indexing technique of Lopez (2008). A Russian morphological analyzer was used to lemmatize the training, development, and test data, and the “noisier channel” translation approach of Dyer (2007) was used in the Russian– English system to let unusually inflected surface forms back off to per-lemma translations. • transliterations of OOV Russian words (§5.3); • English target sides with varied function words (for example, given a phrase that translates into cat we procedure variants like the cat, a cat and of the cat); and, 2 73 http://www.ark.cs.cmu.edu/cdyer/ru-600/. 5.3 Synthetic Translations: Transliteration Table 2: Russian"
W13-2205,W11-2139,1,0.884244,"en incorporated in current statistical MT systems. For our Russian–English system, we additionally used a secondary “pseudo-reference” translation when tuning the parameters of our Russian– English system. This was created by automatically translating the Spanish translation of the provided development data into English. While the output of an MT system is not always perfectly grammatical, previous work has shown that secondary machine-generated references improve translation quality when only a single human reference is available when BLEU is used as an optimization criterion (Madnani, 2010; Dyer et al., 2011). We describe the CMU systems submitted to the 2013 WMT shared task in machine translation. We participated in three language pairs, French–English, Russian– English, and English–Russian. Our particular innovations include: a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create “synthetic translation options” that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context. 1 Introduction The MT research group at Carnegie Mellon Uni"
W13-2205,D10-1004,0,0.0328156,"Missing"
W13-2205,N13-1073,1,0.782282,"om word-aligned training data. In the Russian-English system (§5), we used a CRF-based transliterator (Ammar et al., 2012) to propose transliteration candidates for out-ofvocabulary words, and used a language model to insert or remove common function words in phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokeniz"
W13-2205,W07-0729,0,0.0220307,"trained 7-gram language models using 600-class Brown clusters with Witten-Bell smoothing.2 5.2 Baseline System Our baseline Russian–English system is a hierarchical phrase-based translation model as implemented in cdec (Chiang, 2007; Dyer et al., 2010). SCFG translation rules that plausibly match each sentence in the development and deftest sets were extracted from the aligned parallel data using the suffix array indexing technique of Lopez (2008). A Russian morphological analyzer was used to lemmatize the training, development, and test data, and the “noisier channel” translation approach of Dyer (2007) was used in the Russian– English system to let unusually inflected surface forms back off to per-lemma translations. • transliterations of OOV Russian words (§5.3); • English target sides with varied function words (for example, given a phrase that translates into cat we procedure variants like the cat, a cat and of the cat); and, 2 73 http://www.ark.cs.cmu.edu/cdyer/ru-600/. 5.3 Synthetic Translations: Transliteration Table 2: Russian-English summary. Analysis revealed that about one third of the unseen Russian tokens in the development set consisted of named entities which should be transli"
W13-2205,P02-1040,0,0.087928,"on log relative frequencies log f (rs |rt ) and log f (rt |rs ); • labeling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others accordin"
W13-2205,W12-3160,0,0.0121088,"n phrases according to an n-gram English language 2 Common System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using the Model 2 variant described in Dyer et al. (2013). Language models used modified KneserNey smoothing estimated using KenLM (Heafield, 2011). Translation model parameters were discriminatively set to optimize BLEU on a held-out development set using an online passive aggressive algorithm (Eidelman, 2012) or, in the case of 70 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70–77, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics tion in this language pair was carried out to match the tokenizations expected by the parsers we used (English data was tokenized with the Stanford tokenizer for English and an in-house tokenizer for French that targets the tokenization used by the Berkeley French parser). Both sides of the parallel training data were parsed using the Berkeley latent variable parser. Synchronous context-free grammar rules w"
W13-2205,2006.amta-papers.25,0,0.0492601,"beling relative frequency log f (`s , `t |rs , rt ) and generation relative frequency log f (rs , rt |`s , `t ); • lexical translation log probabilities log plex (rs | rt ) and log plex (rt |rs ), defined similarly to Moses’s definition; French–English Experiments We tuned our system to the newstest2008 set of 2051 segments. Aside from the official newstest2013 test set (3000 segments), we also collected test-set scores from last year’s newstest2012 set (3003 segments). Automatic metric scores are computed according to BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006), all computed according to MultEval v. 0.5 (Clark et al., 2011). Each system variant is run with two independent MERT steps in order to control for optimizer instability. Table 1 presents the results, with the metric scores averaged over both MERT runs. Quite interestingly, we find only minor differences in both tune and test scores despite the large differences in filtered/pruned grammar size as the cutoff for partially abstract rules increases. No system is fully statistically separable (at p &lt; 0.05) from the others according to MultEval’s approximate randomization algorithm. The closest is"
W13-2205,W13-2234,1,0.806649,"se pairs whose source sides match the test sentence are retained. • Abstract rules (whose RHS are all nonterminals) are globally pruned. Only the 4000 most frequently observed rules are retained. • Mixed rules (whose RHS are a mix of terminals and nonterminals) must match the test sentence, and there is an additional frequency cutoff. 4 Synthetic Translation Options Before discussing our Russian–English and English–Russian systems, we introduce the concept of synthetic translation options, which we use in these systems. We provide a brief overview here; for more detail, we refer the reader to Tsvetkov et al. (2013). In language pairs that are typologically similar, words and phrases map relatively directly from source to target languages, and the standard approach to learning phrase pairs by extraction from parallel data can be very effective. However, in language pairs in which individual source language words have many different possible translations (e.g., when the target language word could have many different inflections or could be surrounded by different function words that have no After this filtering, the number of completely lexical rules that match a given sentence is typically low, up to a f"
W13-2205,W06-3119,0,0.0265547,"hand sides and smaller aligned subtrees to be abstracted as right-hand-side nonterminals. Basic subtree alignment heuristics are similar to those by Galley et al. (2006), and composed rules are allowed. The computational complexity is held in check by a limit on the number of RHS elements (nodes and terminals), rather than a GHKM-style maximum composition depth or Hiero-style maximum rule span. Our rule extractor also allows “virtual nodes,” or the insertion of new nodes in the parse tree to subdivide regions of flat structure. Virtual nodes are similar to the A+B extended categories of SAMT (Zollmann and Venugopal, 2006), but with the added constraint that they may not conflict with the surrounding tree structure. Because the SCFG rules are labeled with nonterminals composed from both the source and target trees, the nonterminal inventory is quite large, leading to estimation difficulties. To deal with this, we automatically coarsening the nonterminal labels (Hanneman and Lavie, 2011). Labels are agglomeratively clustered based on a histogrambased similarity function that looks at what target labels correspond to a particular source label and vice versa. The number of clusters used is determined based on spik"
W13-2205,sharoff-etal-2008-designing,0,\N,Missing
W14-0311,W12-3160,0,0.0178252,"licable to the wealth of scenarios that still require precise human-quality translation that MT is currently unable to deliver, including an everincreasing number of government, commercial, and community-driven projects. 72 Workshop on Humans and Computer-assisted Translation, pages 72–77, c Gothenburg, Sweden, 26 April 2014. 2014 Association for Computational Linguistics and all post-editing data, letting the system upweight translations with newly learned vocabulary and phrasing absent in the large monolingual text. Finally, the margin-infused relaxed algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012) is used to make an online parameter update after each sentence is post-edited, minimizing model error. This allows the system to continuously rescale weights for translation and language model features that adapt over time. Since true post-editing data is infeasible to collect during system development and internal testing, as standard MT pipelines require tens of thousands of sentences to be translated with low latency, a simulated post-editing paradigm (Hardt and Elming, 2010) can be used, wherein pregenerated reference translations act as a stand-in for actual post-editing. This approximat"
W14-0311,2010.amta-papers.21,0,0.0400339,"phrasing absent in the large monolingual text. Finally, the margin-infused relaxed algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012) is used to make an online parameter update after each sentence is post-edited, minimizing model error. This allows the system to continuously rescale weights for translation and language model features that adapt over time. Since true post-editing data is infeasible to collect during system development and internal testing, as standard MT pipelines require tens of thousands of sentences to be translated with low latency, a simulated post-editing paradigm (Hardt and Elming, 2010) can be used, wherein pregenerated reference translations act as a stand-in for actual post-editing. This approximation is effective for tuning and internal evaluation when real post-editing data is unavailable. In simulated post-editing tasks, decoding (for both the test corpus and each pass over the development corpus during optimization) begins with baseline models trained on standard bilingual and monolingual text. After each sentence is translated, the following take place in order: First, MIRA uses the new source–reference pair to update weights for the current models. Second, the source"
W14-0311,2013.mtsummit-papers.5,0,0.0165575,"new computeraided translation (CAT) tools that leverage adaptive machine translation. The CASMACAT7 project (Alabau et al., 2013) focuses on building state-of-the-art tools for computer-aided translation. This includes translation predictions backed by machine translation systems that incrementally update model parameters as users edit translations (Mart´ınez-G´omez et al., 2012; L´opez-Salcedo et al., 2012). The MateCat8 project (Cattelan, 2013) specifically aims to integrate machine translation (including online model adaptation and translation quality estimation) into a web-based CAT tool. Bertoldi et al. (2013) show improvements in translator productivity when using the MateCat tool with an adaptive MT system that uses cache-based translation and language models. ios, we compare a static Spanish–English MT system to a comparable adaptive system on a blind out-of-domain test. Competitive with the current state-of-the-art, both systems are trained on the 2012 NAACL WMT (Callison-Burch et al., 2012) constrained resources (2 million bilingual sentences) using the cdec toolkit (Dyer et al., 2010). Blind post-editing evaluation sets are drawn from the Web Inventory of Transcribed and Translated Talks (WIT"
W14-0311,W11-2123,0,0.0131309,"Single instances of the directional word alignment models are loaded into memory for force-aligning post-edited data. When a new user requests a translation, a new context is started. The following are loaded into memory: a table of all postedited data from the user, a user-specific dynamic language model, and a user-specific decoder (in this case an instance of MIRA that has a userspecific decoder and set of weights). Each user also requires an instance of the large static language model, though all users effectively share a single instance through the memory mapped implementation of KenLM (Heafield, 2011). When a new sentence is to be translated, the grammar extractor samples from the shared background data plus the user-specific post-editing data to generate a sentence-specific grammar incorporating data from all prior sentences translated by the same user. The sentence is then decoded using the user and time-specific grammar, current weights, and current dynamic language model. When a postedited sentence is available as feedback, the following happen in order: (1) the source-reference pair is used to update feature weights with MIRA, (2) the source-reference pair is force-aligned and added t"
W14-0311,W12-3102,0,0.0547918,"Missing"
W14-0311,2012.amta-wptp.3,1,0.736462,"focused, allowing for exact timing measurements. A pause button is available if the translator needs to take breaks. TransCenter can generate reports 5 Experiments In a preliminary experiment to evaluate the impact of adaptive MT in real-world post-editing scenar4 https://github.com/mjdenkowski/ transcenter-live 75 Baseline Adaptive HTER 19.26 17.01 made freely available for further analysis.6 TransCenter records all data necessary for more sophisticated editing time analysis (Koehn, 2012) as well as analysis of translator behavior, including pauses (used as an indicator of cognitive effort) (Lacruz et al., 2012). Rating 4.19 4.31 Table 1: Aggregate HTER scores and average translator self-ratings (5 point scale) of postediting effort for translations of TED talks from Spanish into English. 6 There has been a recent push for new computeraided translation (CAT) tools that leverage adaptive machine translation. The CASMACAT7 project (Alabau et al., 2013) focuses on building state-of-the-art tools for computer-aided translation. This includes translation predictions backed by machine translation systems that incrementally update model parameters as users edit translations (Mart´ınez-G´omez et al., 2012; L"
W14-0311,J07-2003,0,0.0464049,"ost-editing can then be deployed to serve real human translators without further modification. These extensions allow the MT system to generate improved translations that require significantly less effort to correct for later sentences in the document. This paradigm is now implemented in the freely available cdec (Dyer et al., 2010) machine translation toolkit as Realtime, part of the pycdec (Chahuneau et al., 2012) Python API. Standard MT systems use aggregate statistics from all training text to learn a single large translation grammar (in the case of cdec’s hierarchical phrase-based model (Chiang, 2007), a synchronous context-free grammar) consisting of rules annotated with feature scores. As an alternative, the bitext can be indexed using a suffix array (Lopez, 2008), a data structure allowing fast source-side lookups. When a new sentence is to be translated, training sentences that share spans of text with the input sentence are sampled from the suffix array. Statistics from the sample are used to learn a small, sentence-specific grammar on-thefly. The adaptive paradigm extends this approach to support online updates by also indexing the new bilingual sentences generated as a post-editor w"
W14-0311,2006.amta-papers.25,0,0.0683311,"nto English. Five students training to be professional translators post-edit machine translations of these excerpts using TransCenter. Translations are provided by either the static or fully adaptive system. Tasks are divided such that each user translates 2 excerpts with the static system and 2 with the adaptive system and each excerpt is post-edited either 2 or 3 times with each system. Users do not know which system is providing the translations. Using the data collected by TransCenter, we evaluate post-editing effort with the established human-targeted translation edit rate (HTER) metric (Snover et al., 2006). HTER computes an edit distance score between initial MT outputs and the “targeted” references created by human postediting, with lower scores being better. Results for the two systems are aggregated over all users and documents. Shown in Table 1, introducing an adaptive MT system results in a significant reduction in editing effort. We additionally average the user post-ratings for each translation by system to evaluate user perception of the adaptive system compared to the static baseline. Also shown in Table 1, we see a slight preference for the adaptive system. This data, as well as preci"
W14-0311,E14-1042,1,0.819691,"are freely available under an open source license. 1 2 Adaptive Machine Translation Traditional machine translation systems operate in batch mode: statistical translation models are estimated from large volumes of sentence-parallel bilingual text and then used to translate new text. Incorporating new data requires a full system rebuild, an expensive operation taking up to days of time. As such, MT systems in production scenarios typically remain static for large periods of time (months or even indefinitely). Recently, an adaptive MT paradigm has been introduced specifically for post-editing (Denkowski et al., 2014). Three major MT system components are extended to support online updates, allowing human posteditor feedback to be immediately incorporated: • An online translation model is updated to include new translations extracted from postediting data. • A dynamic language model is updated to include post-edited target language text. • An online update is made to the system’s feature weights after each sentence is postedited. Introduction This paper describes the end-to-end machine translation post-editing setup provided by cdec Realtime and TransCenter. As the quality of MT systems continues to improv"
W14-0311,P06-1124,0,0.011483,"ne translation toolkit (Dyer et al., 2010), Realtime1 provides an efficient implementation of the adaptive MT paradigm that can serve an arbitrary number of unique post-editors concurrently. A full Realtime tutorial, including stepby-step instructions for installing required software and building full adaptive systems, is availThe adaptive paradigm uses two language models. A standard (static) n-gram language model estimated on large monolingual text allows the system to prefer translations more similar to humangenerated text in the target language. A (dynamic) Bayesian n-gram language model (Teh, 2006) can be updated with observations of the post-edited output in a straightforward way. This smaller model exactly covers the training bitext 1 https://github.com/redpony/cdec/tree/ master/realtime 73 import rt # Start new Realtime translator using a Spanish--English # system and automatic, language-independent text normalization # (pre-tokenization and post-detokenization) translator = rt.RealtimeTranslator(’es-en.d’, tmpdir=’/tmp’, cache_size=5, norm=True) # Translate a sentence for user1 translation = translator.translate(’Muchas gracias Chris.’, ctx_name=’user1’) # Learn from user1’s post-ed"
W14-0311,P10-4002,1,0.574109,"added to the Bayesian language model. As sentences are translated, the models gain valuable context information, allowing them to adapt to the specific target document and translator. Context is reset at the start of each development or test corpus. Systems optimized with simulated post-editing can then be deployed to serve real human translators without further modification. These extensions allow the MT system to generate improved translations that require significantly less effort to correct for later sentences in the document. This paradigm is now implemented in the freely available cdec (Dyer et al., 2010) machine translation toolkit as Realtime, part of the pycdec (Chahuneau et al., 2012) Python API. Standard MT systems use aggregate statistics from all training text to learn a single large translation grammar (in the case of cdec’s hierarchical phrase-based model (Chiang, 2007), a synchronous context-free grammar) consisting of rules annotated with feature scores. As an alternative, the bitext can be indexed using a suffix array (Lopez, 2008), a data structure allowing fast source-side lookups. When a new sentence is to be translated, training sentences that share spans of text with the input"
W14-0311,2012.eamt-1.60,0,\N,Missing
W14-3315,W12-4410,1,0.859453,"Hindi–English system includes improved data cleaning of development data, a sophisticated linguistically-informed tokenization scheme, a transliteration module, a synthetic phrase generator that improves handling of function words, and a synthetic phrase generator that leverages source-side paraphrases. We will discuss each of these five in turn. 4.1 4.3 We used the 12,000 Hindi–English transliteration pairs from the ACL 2012 NEWS workshop on transliteration to train a linear-chained CRF tagger1 that labels each character in the Hindi token with a sequence of zero or more English characters (Ammar et al., 2012). At decoding, unseen Hindi tokens are fed to the transliterator, which produces the 100 most probable transliterations. We add a synthetic translation option for each candidate transliteration. In addition to this sophisticated transliteration scheme, we also employ a rule-based transliterator that specifically targets acronyms. In Hindi, many acronyms are spelled out phonetically, such as NSA being rendered as enese (en.es.e). We detected such words in the input segments and generated synthetic translation options both with and without periods (e.g. N.S.A. and NSA). Development Data Cleaning"
W14-3315,W11-1011,1,0.850261,"guage pairs for the 2014 Workshop on Machine Translation shared translation task: German–English and Hindi–English. Our systems showcase our multi-phase approach to translation, in which synthetic translation options supplement the default translation rule inventory that is extracted from word-aligned training data. In the German–English system, we used our compound splitter (Dyer, 2009) to reduce data sparsity, and we allowed the translator to back off to translating lemmas when it detected caseinflected OOVs. We also demonstrate our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011) as a contrastive German–English submission. In both the German–English and Hindi–English systems, we used an array of supplemental ideas to enhance translation quality, ranging from lemmatization and synthesis of inflected phrase pairs to novel reordering and rule preference features. Our primary German–English and Hindi– English systems were Hiero-based (Chiang, 2007), while our contrastive German–English system used cdec’s tree-to-tree SCFG formalism. Before submitting, we ran cdec’s implementation of MBR on 500-best lists from each of our systems. For both language pairs, we used the Nelde"
W14-3315,J92-4003,0,0.264159,"ead method to optimize the MBR parameters. In the German–English system, we ran MBR on 500 hypotheses, combining the output of the Hiero and tree-to-tree systems. The remainder of the paper will focus on our primary innovations in the two language pairs. 142 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142–149, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 3 Common System Improvements Both source and target surface-form LM used modified Kneser-Ney smoothing (Kneser and Ney, 1995), while the model over Brown clusters (Brown et al., 1992) used subtract-0.5 smoothing. A number of our techniques were used for both our German–English and Hindi–English primary submissions. These techniques each fall into one of three categories: those that create translation rules, those involving language models, or those that add translation features. A comparison of these techniques and their performance across the two language pairs can be found in Section 6. 3.1 3.3 In addition to the standard array of features, we added four new indicator feature templates, leading to a total of nearly 150,000 total features. The first set consists of target"
W14-3315,N13-1029,1,0.879212,"Missing"
W14-3315,D13-1174,1,0.829223,"ible translations, e.g., when the target language word has many different morphological inflections or is surrounded by different function words that have no direct counterpart in the source language. Therefore, when very large quantities of parallel data are not available, we can expect our phrasal inventory to be incomplete. Synthetic translation option generation seeks to fill these gaps using secondary generation processes that exploit existing phrase pairs to produce plausible phrase translation alternatives that are not directly extractable from the training data (Tsvetkov et al., 2013; Chahuneau et al., 2013). To generate synthetic phrases, we first remove function words from the source and target sides of existing non-gappy phrase pairs. We manually constructed English and Hindi lists of common function words, including articles, auxiliaries, pronouns, and adpositions. We then employ the SRILM hidden-ngram utility (Stolcke, 2002) to restore missing function words according to an ngram language model probability, and add the resulting synthetic phrases to our phrase table. Nominal Normalization Another facet of our system was normalization of Hindi nominals. The Hindi nominal system shows much mor"
W14-3315,W11-1015,1,0.855376,"ore suitable for input into constituency parsing. Importantly, we left We filtered tokenized training sentences by sen146 tence length, token length, and sentence length ratio. The final corpus for parsing and word alignment contained 3,897,805 lines, or approximately 86 percent of the total training resources released under the WMT constrained track. Word alignment was carried out using FastAlign (Dyer et al., 2013), while for parsing we used the Berkeley parser (Petrov et al., 2006). Given the parsed and aligned corpus, we extracted synchronous context-free grammar rules using the method of Hanneman et al. (2011). In addition to aligning subtrees that natively exist in the input trees, our grammar extractor also introduces “virtual nodes.” These are new and possibly overlapping constituents that subdivide regions of flat structure by combining two adjacent sibling nodes into a single nonterminal for the purposes of rule extraction. Virtual nodes are similar in spirit to the “A+B” extended categories of SAMT (Zollmann and Venugopal, 2006), and their nonterminal labels are constructed in the same way, but with the added restriction that they do not violate any existing syntactic structure in the parse t"
W14-3315,J07-2003,0,0.0746686,"2009) to reduce data sparsity, and we allowed the translator to back off to translating lemmas when it detected caseinflected OOVs. We also demonstrate our group’s syntactic system with coarsened nonterminal types (Hanneman and Lavie, 2011) as a contrastive German–English submission. In both the German–English and Hindi–English systems, we used an array of supplemental ideas to enhance translation quality, ranging from lemmatization and synthesis of inflected phrase pairs to novel reordering and rule preference features. Our primary German–English and Hindi– English systems were Hiero-based (Chiang, 2007), while our contrastive German–English system used cdec’s tree-to-tree SCFG formalism. Before submitting, we ran cdec’s implementation of MBR on 500-best lists from each of our systems. For both language pairs, we used the Nelder–Mead method to optimize the MBR parameters. In the German–English system, we ran MBR on 500 hypotheses, combining the output of the Hiero and tree-to-tree systems. The remainder of the paper will focus on our primary innovations in the two language pairs. 142 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142–149, c Baltimore, Maryland USA"
W14-3315,P11-2031,1,0.887607,"Missing"
W14-3315,W11-2123,0,0.0122222,"observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon"
W14-3315,W11-2107,1,0.826032,"inal test set. Another interesting result is that only one feature set, namely our rule shape features based on Brown clusters, helped on the test set in both language pairs. No feature hurt the BLEU score on the test set in both language pairs, meaning the majority of features helped in one language and hurt in the other. If we compare results on the tuning sets, however, some clearer patterns arise. Brown cluster language models, n-gram features, and our new rule shape features all helped. Furthermore, there were a few features, such as the Brown cluster language model and tuning to Meteor (Denkowski and Lavie, 2011), that helped substantially in one language pair while just barely hurting the other. In particular, the fact that tuning to Meteor instead of BLEU can actually help both BLEU and Meteor scores was rather unexpected. German–English Specific Improvements Our German–English system also had its own suite of tricks, including the use of “pseudoreferences” and special handling of morphologically inflected OOVs. 5.1 Pseudo-References The development sets provided have only a single reference, which is known to be sub-optimal for tuning of discriminative models. As such, we use the output of one or m"
W14-3315,J03-1002,0,0.00414058,"nthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like"
W14-3315,W12-3131,1,0.878781,"dec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in two language pairs for the 2014 Workshop on Machine Translation shared translation task: German–English and Hindi–English. Our systems showcase our multi-p"
W14-3315,P02-1040,0,0.0893725,"r extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginning of segments, while words like Obama remained capitalized. The MT research group at Carnegie Mellon University’s Language Technologies Institute participated in two language pairs for the 2014 Workshop on Machi"
W14-3315,W13-2212,0,0.0231279,"Meteor scores was rather unexpected. German–English Specific Improvements Our German–English system also had its own suite of tricks, including the use of “pseudoreferences” and special handling of morphologically inflected OOVs. 5.1 Pseudo-References The development sets provided have only a single reference, which is known to be sub-optimal for tuning of discriminative models. As such, we use the output of one or more of last year’s top performing systems as pseudo-references during tuning. We experimented with using just one pseudo-reference, taken from last year’s Spanish– English winner (Durrani et al., 2013), and with using four pseudo-references, including the output of last year’s winning Czech–English, French– English, and Russian–English systems (Pino et al., 2013). 5.2 Results 7 German–English Syntax System In addition to our primary German–English system, we also submitted a contrastive German– English system showcasing our group’s tree-totree syntax-based translation formalism. Morphological OOVs Examination of the output of our baseline systems lead us to conclude that the majority of our 145 System Baseline *Meteor Tuning Sentence Boundaries Double Aligners Manual Number Rules Brown Clus"
W14-3315,P06-1055,0,0.0580255,"must be parsed in addition to being word-aligned, we prepared separate copies of the training, tuning, and testing data that are more suitable for input into constituency parsing. Importantly, we left We filtered tokenized training sentences by sen146 tence length, token length, and sentence length ratio. The final corpus for parsing and word alignment contained 3,897,805 lines, or approximately 86 percent of the total training resources released under the WMT constrained track. Word alignment was carried out using FastAlign (Dyer et al., 2013), while for parsing we used the Berkeley parser (Petrov et al., 2006). Given the parsed and aligned corpus, we extracted synchronous context-free grammar rules using the method of Hanneman et al. (2011). In addition to aligning subtrees that natively exist in the input trees, our grammar extractor also introduces “virtual nodes.” These are new and possibly overlapping constituents that subdivide regions of flat structure by combining two adjacent sibling nodes into a single nonterminal for the purposes of rule extraction. Virtual nodes are similar in spirit to the “A+B” extended categories of SAMT (Zollmann and Venugopal, 2006), and their nonterminal labels are"
W14-3315,P10-4002,1,0.866575,"2 We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, German–English and Hindi–English. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012)"
W14-3315,W13-2225,0,0.0352624,"Missing"
W14-3315,N13-1073,1,0.91499,", several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules. 1 Core System Components The decoder infrastructure we used was cdec (Dyer et al., 2010). For our primary systems, all data was tokenized using cdec’s tokenization tool. Only the constrained data resources provided for the shared task were used for training both the translation and language models. Word alignments were generated using both FastAlign (Dyer et al., 2013) and GIZA++ (Och and Ney, 2003). All our language models were estimated using KenLM (Heafield, 2011). Translation model parameters were chosen using MIRA (Eidelman, 2012) to optimize BLEU (Papineni et al., 2002) on a held-out development set. Introduction Our data was filtered using qe-clean (Denkowski et al., 2012), with a cutoff of two standard deviations from the mean. All data was left in fully cased form, save the first letter of each segment, which was changed to whichever form the first token more commonly used throughout the data. As such, words like The were lowercased at the beginnin"
W14-3315,P14-1064,0,0.0199677,"e phrases, namely the target phrasal inventory, can also be represented in a graph form, where the distributional features can also be computed from the target monolingual data. Translation information is then propagated from the labeled phrases to the unlabeled phrases in the source graph, proportional to how similar the phrases are to each other on the source side, as well as how similar the translation candidates are to each other on the target side. The newly acquired translation distributions for the unlabeled phrases are written out to a secondary phrase table. For more information, see Saluja et al. (2014). 5 6 As we added each feature to our systems, we first ran a one-off experiment comparing our baseline system with and without each individual feature. The results of that set of experiments are shown in Table 1 for Hindi–English and Table 2 for German–English. Features marked with a * were not included in our final system submission. The most surprising result is the strength of our Hindi–English baseline system. With no extra bells or whistles, it is already half a BLEU point ahead of the second best system submitted to this shared task. We believe this is due to our filtering of the tuning"
W14-3315,W13-2234,1,0.830476,"ave many different possible translations, e.g., when the target language word has many different morphological inflections or is surrounded by different function words that have no direct counterpart in the source language. Therefore, when very large quantities of parallel data are not available, we can expect our phrasal inventory to be incomplete. Synthetic translation option generation seeks to fill these gaps using secondary generation processes that exploit existing phrase pairs to produce plausible phrase translation alternatives that are not directly extractable from the training data (Tsvetkov et al., 2013; Chahuneau et al., 2013). To generate synthetic phrases, we first remove function words from the source and target sides of existing non-gappy phrase pairs. We manually constructed English and Hindi lists of common function words, including articles, auxiliaries, pronouns, and adpositions. We then employ the SRILM hidden-ngram utility (Stolcke, 2002) to restore missing function words according to an ngram language model probability, and add the resulting synthetic phrases to our phrase table. Nominal Normalization Another facet of our system was normalization of Hindi nominals. The Hindi nomi"
W14-3315,W06-3119,0,0.0377712,"Missing"
W14-3315,N09-1046,1,\N,Missing
W14-3315,W12-3160,0,\N,Missing
W14-3348,P02-1040,0,0.130922,"uage specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes across languages. 2 Meteor Scoring Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores. For a hypothesisreference pair, the space of possible alignments is constructed by exhaustively identifying all possible matches between the sentences according to the following m"
W14-3348,W11-2103,0,0.0193608,"pported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target"
W14-3348,W09-0441,0,0.0223169,"se tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT syste"
W14-3348,W12-3102,0,0.0638326,"Missing"
W14-3348,W12-3107,0,0.0250211,"universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes across languages. 2 Met"
W14-3348,W12-3104,0,0.0157683,"ems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter set that generalizes"
W14-3348,W11-2106,0,0.0291611,"xt used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a universal metric parameter s"
W14-3348,W11-2107,1,0.150795,"n word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically learned from MT system training data with a univ"
W14-3348,N03-1017,0,0.0669985,"eater importance for content words. 3.2 Paraphrase Tables Paraphrase tables allow many-to-many matches that can encapsulate any local language phenomena, including morphology, synonymy, and true paraphrasing. Identifying these matches allows far more sophisticated evaluation than is possible with simple surface form matches. In Meteor Universal, paraphrases act as the catch-all for nonexact matches. Paraphrases are automatically extracted from the training bitext using the translation pivot approach (Bannard and Callison-Burch, 2005). First, a standard phrase table is learned from the bitext (Koehn et al., 2003). Paraphrase extraction then proceeds as follows. For each target language phrase (e1 ) in the table, find each The parameterized harmonic mean of P and R (van Rijsbergen, 1979) is then calculated: Fmean = Language Specific Resources P ·R α · P + (1 − α) · R To account for gaps and differences in word order, a fragmentation penalty is calculated using the total number of matched words (m, averaged over 377 Direction cs-en de-en es-en fr-en en-cs en-de en-es en-fr Total source phrase f that e1 translates. Each alternate phrase (e2 6= e1 ) that translates f is considered a paraphrase with probab"
W14-3348,P07-2045,0,0.0988306,"60 Table 2: Comparison of parameters for language specific and universal versions of Meteor. WMT13 τ English Czech German Spanish French Russian M-Full 0.214 0.092 0.163 0.106 0.150 – M-Universal 0.206 0.085 0.157 0.101 0.137 0.128 BLEU 0.124 0.044 0.097 0.068 0.099 0.068 WMT14 τ Hindi M-Full – M-Universal 0.264 BLEU 0.227 Meteor 1.5 can be downloaded from the official webpage1 and a full tutorial for Meteor Universal is available online.2 Building a version of Meteor for a new language requires a training bitext (corpus.f, corpus.e) and a standard Moses format phrase table (phrase-table.gz) (Koehn et al., 2007). To extract linguistic resources for Meteor, run the new language script: $ python scripts/new_language.py out  corpus.f corpus.e phrase-table.gz Table 3: Sentence-level correlation with human rankings (Kendall’s τ ) for Meteor (language specific versions), Meteor Universal, and BLEU To use the resulting files to score translations with Meteor, use the new language option: in Table 3, Meteor Universal significantly outperforms baseline BLEU in all cases while suffering only slight degradation compared to versions of Meteor tuned for individual languages. For Russian, correlation is nearly do"
W14-3348,W13-2202,0,0.0276878,"resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14). 1 Introduction Recent WMT evaluations have seen a variety of metrics employ language specific resources to replicate human translation rankings far better than simple baselines (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013; Snover et al., 2009; Denkowski and Lavie, 2011; Dahlmeier et al., 2011; Chen et al., 2012; Wang and Manning, 2012, inter alia). While the wealth of linguistic resources for the WMT languages allows the development of sophisticated metrics, most of the world’s 7,000+ languages lack the prerequisites for building advanced metrics. Researchers working on low resource languages are usually limited to baseline BLEU (Papineni et al., 2002) for evaluating translation quality. Meteor Universal brings language specific evaluation to any target language by combining linguistic resources automatically"
W14-3348,P05-1074,0,\N,Missing
W14-3627,P06-1086,1,0.865853,"ia industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect. DA is now emerging as the language of informal communication online. DA differs phonologically, lexically, morphologically, and syntactically from MSA. And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script. Thus, MSA tools cannot effectively model DA; for instance, over one-third of Levantine verbs cannot be analyzed using an MSA morphological analyzer (Habash and Rambow, 2006). These differences make the direct use of MSA NLP tools and applications for handling dialects impractical. In this paper, we present a statistical machine translation system for English to Dialectal Arabic (DA), using Modern Standard Arabic (MSA) as a pivot. We create a core system to translate from English to MSA using a large bilingual parallel corpus. Then, we design two separate pathways for translation from MSA into DA: a two-step domain and dialect adaptation system and a one-step simultaneous domain and dialect adaptation system. Both variants of the adaptation systems are trained on"
W14-3627,W12-2301,1,0.940968,"for Machine Translation into Egyptian Arabic Serena Jeblee1 , Weston Feely1 , Houda Bouamor2 Alon Lavie1 , Nizar Habash3 and Kemal Oflazer2 1 Carnegie Mellon University {sjeblee, wfeely, alavie}@cs.cmu.edu 2 Carnegie Mellon University in Qatar hbouamor@qatar.cmu.edu, ko@cs.cmu.edu 3 New York University Abu Dhabi nizar.habash@nyu.edu Abstract chine translation (Zbib et al., 2012; Salloum and Habash, 2013; Salloum et al., 2014; Al-Mannai et al., 2014) and in terms of data collection (Cotterell and Callison-Burch, 2014; Bouamor et al., 2014; Salama et al., 2014) and basic enabling technologies (Habash et al., 2012; Pasha et al., 2014). However, the focus is on a small number of iconic dialects, (e.g., Egyptian). The Egyptian media industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect. DA is now emerging as the language of informal communication online. DA differs phonologically, lexically, morphologically, and syntactically from MSA. And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script. Thus, MSA tools cannot effective"
W14-3627,N13-1044,1,0.929805,"ey may not always be able to pinpoint exact linguistic differences. In the context of natural language processing (NLP), some Arabic dialects have started receiving increasing attention, particularly in the context of maEgyptian Arabic is much closer to MSA than it is to English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to tran"
W14-3627,W14-3628,0,0.190206,"Missing"
W14-3627,bouamor-etal-2014-multidialectal,1,0.847169,"r experimental setup and the results obtained. Then, we give an analysis of our system output in Section 7. Finally, we conclude and describe our future work in Section 8. 2 Related work Machine translation (MT) for dialectal Arabic (DA) is quite challenging given the limited resources to build rule-based models or train statistical models for MT. While there has been a considerable amount of work in the context of standard Arabic NLP (Habash, 2010), DA is impoverished in terms of available tools and resources compared to MSA, e.g., there are few parallel DAEnglish corpora (Zbib et al., 2012; Bouamor et al., 2014). The majority of DA resources are for speech recognition, although more and more resources for machine translation and enabling tech3 Using Phrase-Based MT as an Adaptation System For commercial use, MT output is usually postedited by a human translator in order to fix the errors generated by the MT system. This is often faster and cheaper than having a human translate 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and mak"
W14-3627,W14-5311,0,0.0124183,"y translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that differences in genre betwe"
W14-3627,I13-1048,0,0.0168271,"ent research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that di"
W14-3627,P13-2121,0,0.0484788,"Missing"
W14-3627,P11-2031,1,0.921571,"m, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from n"
W14-3627,W11-2123,0,0.0346794,"e one-step adaptation system, where no reordering produced the best result. We also tested two different heuristics for symmetrizing the word alignments: grow-diag and grow-diag-final-and (Och and Ney, 2003). We found that using grow-diag as our symmetrization heuristic produced slightly better scores on the 100k datasets. For the baseline and adaptation systems we built 5-gram language models with KenLM (Heafield et al., 2013) using the target side of the training set, and for the core system we used the large MSA language model described in section 4. We use KenLM because it has been shown (Heafield, 2011) to be faster and use less memory than SRILM (Stolcke, 2002) and IRSTLM (Federico et al., 2008). below each table. The difference in scores between the different reordering window sizes (7, 4, and 0) we tried for the adaptation systems was not large (between 0 and 0.7 BLEU). In the following tables we present the best results for each adaptation system, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4),"
W14-3627,cotterell-callison-burch-2014-multi,0,0.147926,"Missing"
W14-3627,2007.mtsummit-papers.34,0,0.0414651,"te 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and make it more closely resemble the references. Simard et al. (2007) used a phrase-based MT system as an automatic posteditor for the output of a commercial rule-based MT system, showing that it produced better results than both the rule-based system alone and a single pass phrase-based MT system. This technique is also useful for adapting to a specific domain or dataset. Isabelle et al. (2007) used a statistical MT system to automatically post-edit the output of a generic rule-based MT system, to avoid manually customizing a system dictionary and to reduce the amount of manual post-editing required. For our adaptation systems, we build a core phrase-based MT system with a large amount of out-of-domain data, which allows us to have better coverage of the target language. For an adaptation system, we then build a second phrase-based MT system by translating the in-domain train, tune, and test sets through the core translation system, then using that data to build the second system. T"
W14-3627,P07-2045,0,0.00633831,"stem below. 1 198 http://arz.wikipedia.org/ System Design Baseline MT System 100K sent. English Translation Egyptian Arabic One-Step Adaptation MT System 5M sent. English Translation 100K sent. Domain & MSA Dialect Adaptation Egyptian Arabic Two-Step Adaptation MT System English 5M sent. 100K sent. Translation Domain Adaptation MSA 100K sent. In-domain MSA Dialect Adaptation Egyptian Arabic Figure 1: An overview of the different system architectures. Baseline System Two-Step Adaptation System Our baseline system is a single phrase-based English to Egyptian Arabic MT system, built using Moses (Koehn et al., 2007) on the 100k corpus described in Section 4. This system does not include any MSA data, nor does it have an adaptation system; it is a typical, one-pass MT system that translates English directly into Egyptian Arabic. We will show that using adaptation systems improves the results significantly. We also build a two-step adaptation system that consists of two adaptation steps: one to adapt the MSA output of the core system to the domain of the MSA in the 100k corpus, and a second system to translate the MSA output of the domain adaptation system into Egyptian Arabic. We use the first adaptation"
W14-3627,W11-2107,1,0.838244,"he phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.2 B"
W14-3627,P12-2035,1,0.927262,"ools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel"
W14-3627,P05-1071,1,0.686452,"and also to adapt to the Egyptian dialect. What we refer to as the “one-step” system is a core system plus one adaptation system, whereas the “two-step” system consists of the core plus two subsequent adaptation systems. We describe the systems in more detail in Section 5. sentences from NIST MT09 (NIST Multimodal Information Group, 2010b). We use a 5-gram MSA language model built using the SRILM toolkit (Stolcke, 2002) on 260 million words of MSA from the Arabic Gigaword (Parker et al., 2011). All our MSA parallel data and monolingual MSA language modeling data were tokenized with MADA v3.1 (Habash and Rambow, 2005) using the ATB (Arabic Treebank) tokenization scheme. For the adaptation systems, we build a 100k tri-parallel corpus Egyptian-MSA-English corpus. The MSA and English parts are extracted from the NIST corpus distributed by the Linguistic Data Consortium. The Egyptian sentences are obtained automatically by extending Mohamed et al. (2012) method for generating Egyptian Arabic from morphologically disambiguated MSA sentences. This rule-based method relies on 103 transformation rules covering essentially nouns, verbs and pronouns as well as certain lexical items. For each MSA sentence, this metho"
W14-3627,N07-1064,0,0.0331669,"ch recognition, although more and more resources for machine translation and enabling tech3 Using Phrase-Based MT as an Adaptation System For commercial use, MT output is usually postedited by a human translator in order to fix the errors generated by the MT system. This is often faster and cheaper than having a human translate 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and make it more closely resemble the references. Simard et al. (2007) used a phrase-based MT system as an automatic posteditor for the output of a commercial rule-based MT system, showing that it produced better results than both the rule-based system alone and a single pass phrase-based MT system. This technique is also useful for adapting to a specific domain or dataset. Isabelle et al. (2007) used a statistical MT system to automatically post-edit the output of a generic rule-based MT system, to avoid manually customizing a system dictionary and to reduce the amount of manual post-editing required. For our adaptation systems, we build a core phrase-based MT"
W14-3627,J03-1002,0,0.00421305,"nts Since MSA and Egyptian are more similar to each other than they are to English, we tried several different reordering window sizes to find the optimal reordering distance for adapting MSA to Egyptian Arabic, including the typical reordering window of length 7, a smaller window of length 4, and no reordering at all. We found a reordering window 199 size of 7 to work best for all our systems, except for the one-step adaptation system, where no reordering produced the best result. We also tested two different heuristics for symmetrizing the word alignments: grow-diag and grow-diag-final-and (Och and Ney, 2003). We found that using grow-diag as our symmetrization heuristic produced slightly better scores on the 100k datasets. For the baseline and adaptation systems we built 5-gram language models with KenLM (Heafield et al., 2013) using the target side of the training set, and for the core system we used the large MSA language model described in section 4. We use KenLM because it has been shown (Heafield, 2011) to be faster and use less memory than SRILM (Stolcke, 2002) and IRSTLM (Federico et al., 2008). below each table. The difference in scores between the different reordering window sizes (7, 4,"
W14-3627,P02-1040,0,0.10335,"7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is gener"
W14-3627,2006.amta-papers.25,0,0.0165285,"d dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.2 Below we report BLEU scores"
W14-3627,pasha-etal-2014-madamira,1,0.871023,"Missing"
W14-3627,P11-2007,0,0.118831,"thod relies on 103 transformation rules covering essentially nouns, verbs and pronouns as well as certain lexical items. For each MSA sentence, this method provides more than one possible candidate, in its original version, the Egyptian sentence kept was chosen randomly. We extend the selection algorithm by scoring the different sentences using a language model. For this, we use SRILM with modified Kneser-Ney smoothing to build a 5-gram language model. The model is trained on a corpus including articles extracted from the Egyptian version of Wikipedia1 and the Egyptian side of the AOC corpus (Zaidan and Callison-Burch, 2011). We chose to include Egyptian Wikipedia for the formal level of sentences in it different from the regular DA written in blogs or microblogging websites (e.g., Twitter) and closer to the ones generated by our system. We split this data into train, tune, and test sets of 98,027, 960, and 961 sentences respectively, after removing duplicates across sets. The MSA corpus was tokenized using MADA and the Egyptian Arabic data was tokenized with MADA-ARZ v0.4 (Habash et al., 2013), both using the ATB tokenization scheme, with alif/ya normalization. 4 5 Data For the core English to MSA system, we use"
W14-3627,P13-2001,0,0.101077,"ble for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sen"
W14-3627,N12-1006,0,0.393994,"ts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that differences in genre between MSA and DA make bridging through MSA of limited value. For this reason, while pivoting through MSA, it is important to consider the domain and add an additional step: domain adaptation. The majority of previous e"
W14-3627,salama-etal-2014-youdacc,1,0.710505,"Missing"
W14-3627,N13-1036,1,0.844451,"English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation betwe"
W14-3627,P14-2125,1,0.860146,"Missing"
W14-3627,2010.amta-papers.5,0,0.287107,"than it is to English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowled"
W97-0303,C96-1075,1,0.623663,"ng representation hypothesis. In this paper, only the Hypothesis Formation phase is described and evaluated. Since repairs beyond those made possible by the partial parser are performed during the Combination stage, we refer to the implementation of the Combination stage as the repair module. Though a set of hypotheses are produced by during the Combination stage, in the evaluation presented in this paper, only the repair hypothesis scored by the repair module as best is returned. The ROSE approach was developed in the context of the JANUS large-scale multi-lingual machine translation system (Lavie et al., 1996; Woszcyna et al., 1993; Woszcyna et al., 1994). Currently, the JANUS system deals with the scheduling domain where two speakers a t t e m p t to schedule a meeting together over the phone. The system is composed of four language independent and domain independent modules including speech-recognition, parsing, discourse processing, and generation. The repair module described in this paper is similarly language Although Minimum Distance Parsing (MDP) offers a theoretically attractive solution to the problem of extragrammaticality, it is often computationally infeasible in large scale practical"
W97-0303,1993.iwpt-1.12,1,0.903264,"llon University Baker Hall 135F Pittsburgh, PA 15213 cprose@cs.cmu.edu Carnegie Mellon University Center for Machine Translation Pittsburgh, PA 15213 alavie~cs.cmu.edu Abstract pletely automatic portion of the ROSE 1 approach. ROSE, RObustness with Structural Evolution, repairs extragrammatical input in two phases. The first phase, Repair Hypothesis Formation, is responsible for assembling a set of hypotheses about the meaning of the ungrammatical utterance. This phase is itself divided into two stages, Partial Parsing and Combination. A restricted version of Lavie&apos;s GLR* parser (Lavie, 1995; Lavie and Tomita, 1993) is used to obtain an analysis of islands of the speaker&apos;s sentence in cases where it is not possible to obtain an analysis for the entire sentence. In the Combination stage, the fragments from the partial parse are assembled into a set of alternative meaning representation hypotheses. A genetic programming approach is used to search for different ways to combine the fragments in order to avoid requiring any hand-crafted repair rules. In ROSE&apos;s second phase, Interaction with the User, the system generates a set of queries, negotiating with the speaker in order to narrow down to a single best m"
W97-0303,H93-1041,0,0.0278991,"Missing"
W97-0303,1993.tmi-1.16,0,\N,Missing
W97-0410,C96-1075,1,0.852035,"ented. Introduction Spoken language understanding systems have been reasonably successful in limited semantic domains I. The limited domains naturally constrain vocabulary and perplexity, making speech recognition tractable. In addition, the relatively small range of meanings that could be conveyed make parsing and understanding tractable. Now, with the increasing success of large vocabulary continuous speech recognition (LVCSR), the challenge is to similarly scale up spoken language understanding. In this paper we describe our plans for extending the JANUS speech-to-speech translation system [1] [2] from the Appointment Scheduling domain to a broader domain, Travel Planning, which has a rich sub-domain structure, covering many topics. In the last three years, the JANUS project has been developing a speech-to-speech translation system for the Appointment Scheduling domain (two people setting up a time to meet with each other). Although the data we have been working with is spontaneous speech, the scheduling scenario naturally limits the vocabulary to about 3000 words in English and about 4000 words in Spanish and German, which have more inflection. Similarly, the types of dialogues ar"
W97-0410,C96-1061,0,0.0193217,"guage input string is first analyzed by a parser, which produces a languageindependent interlingua content representation. The interlingua is then passed to a generation component, which produces an output string in the target language. In an attempt to achieve both robustness and translation accuracy when faced with speech disfluencies and recognition errors, we use two different parsing strategies: a GLFt parser designed to be more accurate, and a Phoenix parser designed to be more robust. Detailed descriptions of the system components appear in our previous publications [1] [2] [3] [4] [5] [6]. 68 s c h e d u l e - m e e t i n g in addition to syntactic categories such as NP and VP. There were several reasons for chosing semantic grammars. First, the domain lends itself well to semantic g r a m m a r s because there are many fixed expressions and c o m m o n expressions that are almost formulaic. Breaking these down syntactically would be an unnecessary complication. Additionally, spontaneous spoken language is often syntactically ill formed, yet semantically coherent. Semantic g r a m m a r s allow our robust parsers to extract the key concepts being conveyed, even when the input"
W99-0306,J97-1002,0,0.203131,"Missing"
W99-0306,woszczcyna-etal-1998-modular,1,0.730827,"Missing"
woszczcyna-etal-1998-modular,H90-1027,0,\N,Missing
woszczcyna-etal-1998-modular,H94-1093,0,\N,Missing
woszczcyna-etal-1998-modular,P98-2229,0,\N,Missing
woszczcyna-etal-1998-modular,C98-2224,0,\N,Missing
woszczcyna-etal-1998-modular,W97-0410,1,\N,Missing
woszczcyna-etal-1998-modular,P80-1024,0,\N,Missing
