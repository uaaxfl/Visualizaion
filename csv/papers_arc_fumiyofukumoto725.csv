2021.emnlp-main.290,"Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification",2021,-1,-1,3,0,9284,zhiwei zhang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Scientific claim verification can help the researchers to easily find the target scientific papers with the sentence evidence from a large corpus for the given claim. Some existing works propose pipeline models on the three tasks of abstract retrieval, rationale selection and stance prediction. Such works have the problems of error propagation among the modules in the pipeline and lack of sharing valuable information among modules. We thus propose an approach, named as ARSJoint, that jointly learns the modules for the three tasks with a machine reading comprehension framework by including claim information. In addition, we enhance the information exchanges and constraints among tasks by proposing a regularization term between the sentence attention scores of abstract retrieval and the estimated outputs of rational selection. The experimental results on the benchmark dataset SciFact show that our approach outperforms the existing works."
2020.sdp-1.14,Multi-task Peer-Review Score Prediction,2020,-1,-1,4,1,9285,jiyi li,Proceedings of the First Workshop on Scholarly Document Processing,0,"Automatic prediction on the peer-review aspect scores of academic papers can be a useful assistant tool for both reviewers and authors. To handle the small size of published datasets on the target aspect of scores, we propose a multi-task approach to leverage additional information from other aspects of scores for improving the performance of the target. Because one of the problems of building multi-task models is how to select the proper resources of auxiliary tasks and how to select the proper shared structures. We propose a multi-task shared structure encoding approach which automatically selects good shared network structures as well as good auxiliary resources. The experiments based on peer-review datasets show that our approach is effective and has better performance on the target scores than the single-task method and naive multi-task methods."
2020.lrec-1.200,Semi-Automatic Construction and Refinement of an Annotated Corpus for a Deep Learning Framework for Emotion Classification,2020,-1,-1,4,0,17023,jiajun xu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In the case of using a deep learning (machine learning) framework for emotion classification, one significant difficulty faced is the requirement of building a large, emotion corpus in which each sentence is assigned emotion labels. As a result, there is a high cost in terms of time and money associated with the construction of such a corpus. Therefore, this paper proposes a method of creating a semi-automatically constructed emotion corpus. For the purpose of this study sentences were mined from Twitter using some emotional seed words that were selected from a dictionary in which the emotion words were well-defined. Tweets were retrieved by one emotional seed word, and the retrieved sentences were assigned emotion labels based on the emotion category of the seed word. It was evident from the findings that the deep learning-based emotion classification model could not achieve high levels of accuracy in emotion classification because the semi-automatically constructed corpus had many errors when assigning emotion labels. In this paper, therefore, an approach for improving the quality of the emotion labels by automatically correcting the errors of emotion labels is proposed and tested. The experimental results showed that the proposed method worked well, and the classification accuracy rate was improved to 55.1{\%} from 44.9{\%} on the Twitter emotion classification task."
2020.figlang-1.4,{D}eep{M}et: A Reading Comprehension Paradigm for Token-level Metaphor Detection,2020,-1,-1,2,0,19997,chuandong su,Proceedings of the Second Workshop on Figurative Language Processing,0,"Machine metaphor understanding is one of the major topics in NLP. Most of the recent attempts consider it as classification or sequence tagging task. However, few types of research introduce the rich linguistic information into the field of computational metaphor by leveraging powerful pre-training language models. We focus a novel reading comprehension paradigm for solving the token-level metaphor detection task which provides an innovative type of solution for this task. We propose an end-to-end deep metaphor detection model named DeepMet based on this paradigm. The proposed approach encodes the global text context (whole sentence), local text context (sentence fragments), and question (query word) information as well as incorporating two types of part-of-speech (POS) features by making use of the advanced pre-training language model. The experimental results by using several metaphor datasets show that our model achieves competitive results in the second shared task on metaphor detection."
2020.emnlp-main.545,{HSCNN}: A Hybrid-{S}iamese Convolutional Neural Network for Extremely Imbalanced Multi-label Text Classification,2020,-1,-1,3,0,20545,wenshuo yang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"The data imbalance problem is a crucial issue for the multi-label text classification. Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data. We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories. We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN. The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories."
2020.coling-main.194,A Neural Local Coherence Analysis Model for Clarity Text Scoring,2020,-1,-1,3,0,21287,panitan muangkammuen,Proceedings of the 28th International Conference on Computational Linguistics,0,"Local coherence relation between two phrases/sentences such as cause-effect and contrast gives a strong influence of whether a text is well-structured or not. This paper follows the assumption and presents a method for scoring text clarity by utilizing local coherence between adjacent sentences. We hypothesize that the contextual features of coherence relations learned by utilizing different data from the target training data are also possible to discriminate well-structured of the target text and thus help to score the text clarity. We propose a text clarity scoring method that utilizes local coherence analysis with an out-domain setting, i.e. the training data for the source and target tasks are different from each other. The method with language model pre-training BERT firstly trains the local coherence model as an auxiliary manner and then re-trains it together with clarity text scoring model. The experimental results by using the PeerRead benchmark dataset show the improvement compared with a single model, scoring text clarity model. Our source codes are available online."
2020.aacl-srw.17,Multi-task Learning for Automated Essay Scoring with Sentiment Analysis,2020,-1,-1,2,0,21287,panitan muangkammuen,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Automated Essay Scoring (AES) is a process that aims to alleviate the workload of graders and improve the feedback cycle in educational systems. Multi-task learning models, one of the deep learning techniques that have recently been applied to many NLP tasks, demonstrate the vast potential for AES. In this work, we present an approach for combining two tasks, sentiment analysis, and AES by utilizing multi-task learning. The model is based on a hierarchical neural network that learns to predict a holistic score at the document-level along with sentiment classes at the word-level and sentence-level. The sentiment features extracted from opinion expressions can enhance a vanilla holistic essay scoring, which mainly focuses on lexicon and text semantics. Our approach demonstrates that sentiment features are beneficial for some essay prompts, and the performance is competitive to other deep learning models on the Automated StudentAssessment Prize (ASAP) benchmark. TheQuadratic Weighted Kappa (QWK) is used to measure the agreement between the human grader{'}s score and the model{'}s prediction. Ourmodel produces a QWK of 0.763."
P19-1105,Text Categorization by Learning Predominant Sense of Words as Auxiliary Task,2019,0,0,3,1,15428,kazuya shimura,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Distributions of the senses of words are often highly skewed and give a strong influence of the domain of a document. This paper follows the assumption and presents a method for text categorization by leveraging the predominant sense of words depending on the domain, i.e., domain-specific senses. The key idea is that the features learned from predominant senses are possible to discriminate the domain of the document and thus improve the overall performance of text categorization. We propose multi-task learning framework based on the neural network model, transformer, which trains a model to simultaneously categorize documents and predicts a predominant sense for each word. The experimental results using four benchmark datasets show that our method is comparable to the state-of-the-art categorization approach, especially our model works well for categorization of multi-label documents."
D19-5904,A Dataset of Crowdsourced Word Sequences: Collections and Answer Aggregation for Ground Truth Creation,2019,0,0,2,1,9285,jiyi li,Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP,0,"The target outputs of many NLP tasks are word sequences. To collect the data for training and evaluating models, the crowd is a cheaper and easier to access than the oracle. To ensure the quality of the crowdsourced data, people can assign multiple workers to one question and then aggregate the multiple answers with diverse quality into a golden one. How to aggregate multiple crowdsourced word sequences with diverse quality is a curious and challenging problem. People need a dataset for addressing this problem. We thus create a dataset (CrowdWSA2019) which contains the translated sentences generated from multiple workers. We provide three approaches as the baselines on the task of extractive word sequence aggregation. Specially, one of them is an original one we propose which models the reliability of workers. We also discuss some issues on ground truth creation of word sequences which can be addressed based on this dataset."
D18-1093,{HFT}-{CNN}: Learning Hierarchical Category Structure for Multi-label Short Text Categorization,2018,0,4,3,1,15428,kazuya shimura,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We focus on the multi-label categorization task for short texts and explore the use of a hierarchical structure (HS) of categories. In contrast to the existing work using non-hierarchical flat model, the method leverages the hierarchical relations between the pre-defined categories to tackle the data sparsity problem. The lower the HS level, the less the categorization performance. Because the number of training data per category in a lower level is much smaller than that in an upper level. We propose an approach which can effectively utilize the data in the upper levels to contribute the categorization in the lower levels by applying the Convolutional Neural Network (CNN) with a fine-tuning technique. The results using two benchmark datasets show that proposed method, Hierarchical Fine-Tuning based CNN (HFT-CNN) is competitive with the state-of-the-art CNN based methods."
D15-1093,Learning Timeline Difference for Text Categorization,2015,24,0,1,1,9286,fumiyo fukumoto,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"This paper addresses text categorization problem that training data may derive from a different time period from the test data. We present a learning framework which extends a boosting technique to learn accurate model for timeline adaptation. The results showed that the method was comparable to the current state-of-theart biased-SVM method, especially the method is effective when the creation time period of the test data differs greatly from the training data."
U14-1011,The Effect of Temporal-based Term Selection for Text Classification,2014,23,0,1,1,9286,fumiyo fukumoto,Proceedings of the Australasian Language Technology Association Workshop 2014,0,"This paper addresses the text classification problem that training data may derive from a different time period from the test data. We present a method of temporal-based term selection for timeline adaptation. We selected two types of informative terms according to corpus statistics. One is temporal independent terms that are salient regardless of the timeline. Another is temporal dependent terms which are important for a specific period of time. For temporal dependent terms extracted from the training documents, we applied weighting function that weights terms according to the temporal distance between training and test data in the process of training classifiers. The results using Mainichi Japanese newspaper documents showed improvement over the three baselines."
P14-2040,Detection of Topic and its Extrinsic Evaluation Through Multi-Document Summarization,2014,23,5,2,0,17026,yoshimi suzuki,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents a method for detecting words related to a topic (we call them topic words) over time in the stream of documents. Topic words are widely distributed in the stream of documents, and sometimes they frequently appear in the documents, and sometimes not. We propose a method to reinforce topic words with low frequencies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (Blei et al., 2003) to these documents. For the results of LDA, we identified topic words by using Moving Average Convergence Divergence. In order to evaluate the method, we applied the results of topic detection to extractive multi-document summarization. The results showed that the method was effective for sentence selection in summarization."
matsuyoshi-etal-2014-annotating,Annotating the Focus of Negation in {J}apanese Text,2014,12,1,3,0,29950,suguru matsuyoshi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper proposes an annotation scheme for the focus of negation in Japanese text. Negation has its scope and the focus within the scope. The scope of negation is the part of the sentence that is negated; the focus is the part of the scope that is most prominently or explicitly negated. In natural language processing, correct interpretation of negated statements requires precise detection of the focus of negation in the statements. As a foundation for developing a negation focus detector for Japanese, we have annotated textdata of {``}Rakuten Travel: User review data{''} and the newspaper subcorpus of the {``}Balanced Corpus of Contemporary Written Japanese{''} with labels proposed in our annotation scheme. We report 1,327 negation cues and the foci in the corpora, and present classification of these foci based on syntactic types and semantic types. We also propose a system for detecting the focus of negation in Japanese using 16 heuristic rules and report the performance of the system."
P13-2084,Text Classification from Positive and Unlabeled Data using Misclassified Data Correction,2013,12,5,1,1,9286,fumiyo fukumoto,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614."
W12-4702,Exploiting Discourse Relations between Sentences for Text Clustering,2012,17,5,2,0,42140,nik zahri,Proceedings of the Workshop on Advances in Discourse Analysis and its Computational Aspects,0,"Over the years, the usage of discourse relations has been proven to enhance many applications such as text summarization, question answering and natural language generation. This paper proposes an approach that expands the benefit of discourse relations for natural language processing from a different aspect. We exploit the discourse relations existing between sentences to generate clusters of similar sentences from document sets. We first examined and defined the type of discourse relations that useful to retrieve sentences with identical content. We then assigned these relations to each sentence pair using a machine learning method. Finally we performed discourse relation-based clustering algorithm to generate clusters of similar sentences. We evaluated our method by measuring the cohesion and separation of the clusters and compared to a well recognized clustering method. The experimental result shows that our method performed significantly well, which demonstrated that discourse relation between sentences can be exploited for text clustering."
P11-2097,Identification of Domain-Specific Senses in a Machine-Readable Dictionary,2011,13,1,1,1,9286,fumiyo fukumoto,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method first identifies each sense of a word in the dictionary to its corresponding category. We used a text classification technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the first sense heuristics in the WSD task. These results suggest that identification of domain-specific senses (IDSS) may actually be of benefit."
I11-1156,Cluster Labelling based on Concepts in a Machine-Readable Dictionary,2011,11,5,1,1,9286,fumiyo fukumoto,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper addresses the issue of cluster labeling and presents a method for assigning labels by using concepts in a machinereadable dictionary. We assume that salient terms in the cluster content have the same hypernym because hypernymic semantic relation represents a generalization that goes from specific to generic. Our experimental results reveal that hypernymic semantic relations can be exploited to increase labeling accuracy, as the results of 0.441 F-score improves over the two baselines."
W10-2316,Eliminating Redundancy by Spectral Relaxation for Multi-Document Summarization,2010,12,5,1,1,9286,fumiyo fukumoto,Proceedings of {T}ext{G}raphs-5 - 2010 Workshop on Graph-based Methods for Natural Language Processing,0,"This paper focuses on redundancy, overlapping information in multi-documents, and presents a method for detecting salient, key sentences from documents that discuss the same event. To eliminate redundancy, we used spectral clustering and classified each sentence into groups, each of which consists of semantically related sentences. Then, we applied link analysis, the Markov Random Walk (MRW) Model to deciding the importance of a sentence within documents. The method was tested on the NTCIR evaluation data, and the result shows the effectiveness of the method."
W09-3205,Classifying {J}apanese Polysemous Verbs based on Fuzzy {C}-means Clustering,2009,-1,-1,2,0.666049,17026,yoshimi suzuki,Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing ({T}ext{G}raphs-4),0,None
W08-2005,Graph-Based Clustering for Semantic Classification of Onomatopoetic Words,2008,21,4,2,0,47709,kenichi ichioka,Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graph-based Algorithms for Natural Language Processing,0,"This paper presents a method for semantic classification of onomatopoetic words like [Abstract contained text which could not be displayed.] (hum) and [Abstract contained text which could not be displayed.] (clip clop) which exist in every language, especially Japanese being rich in onomatopoetic words. We used a graph-based clustering algorithm called Newman clustering. The algorithm calculates a simple quality function to test whether a particular division is meaningful. The quality function is calculated based on the weights of edges between nodes. We combined two different similarity measures, distributional similarity, and orthographic similarity to calculate weights. The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure."
C08-1030,Retrieving Bilingual Verb-Noun Collocations by Integrating Cross-Language Category Hierarchies,2008,10,1,1,1,9286,fumiyo fukumoto,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a method of retrieving bilingual collocations of a verb and its objective noun from cross-lingual documents with similar contents. Relevant documents are obtained by integrating cross-language hierarchies. The results showed a 15.1% improvement over the baseline non-hierarchy model, and a 6.0% improvement over use of relevant documents retrieved from a single hierarchy. Moreover, we found that some of the retrieved collocations were domain-specific."
P06-2030,Using Bilingual Comparable Corpora and Semi-supervised Clustering for Topic Tracking,2006,13,1,1,1,9286,fumiyo fukumoto,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We address the problem dealing with skewed data, and propose a method for estimating effective training stories for the topic tracking task. For a small number of labelled positive stories, we extract story pairs which consist of positive and its associated stories from bilingual comparable corpora. To overcome the problem of a large number of labelled negative stories, we classify them into some clusters. This is done by using k-means with EM. The results on the TDT corpora show the effectiveness of the method."
I05-1002,Topic Tracking Based on Linguistic Features,2005,16,1,1,1,9286,fumiyo fukumoto,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper explores two linguistically motivated restrictions on the set of words used for topic tracking on newspaper articles: named entities and headline words. We assume that named entities is one of the linguistic features for topic tracking, since both topic and event are related to a specific place and time in a story. The basic idea to use headline words for the tracking task is that headline is a compact representation of the original story, which helps people to quickly understand the most important information contained in a story. Headline words are automatically generated using headline generation technique. The method was tested on the Mainichi Shimbun Newspaper in Japanese, and the results of topic tracking show that the system works well even for a small number of positive training data."
W04-2409,A Comparison of Manual and Automatic Constructions of Category Hierarchy for Classifying Large Corpora,2004,19,0,1,1,9286,fumiyo fukumoto,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,"We address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora. We use two wellknown techniques, partitioning clustering, means and a to create category hierarchy. -means is to cluster the given categories in a hierarchy. To select the proper number of , we use a which measures the degree of our disappointment in any differences between the true distribution over inputs and the learnerxe2x80x99s prediction. Once the optimal number of is selected, for each cluster, the procedure is repeated. Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy."
C04-1125,Correcting Category Errors in Text Classification,2004,11,9,1,1,9286,fumiyo fukumoto,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We address the problem dealing with category annotation errors which deteriorate the overall performance of text classification. We use two techniques. The first is support vectors which are extracted from the training samples by a machine learning technique, Support Vector Machines (SVM). The second is a loss function which measures the degree of our disappointment in any differences between the true distribution over inputs and the learner's prediction. We apply it to the extracted support vectors, and correct annotation errors. Experimental results with the RWCP and the Reuters 1996 corpora show that our method achieves high precision in detecting and correcting annotation errors. Further, results on text classification improves accuracy."
W02-1026,Manipulating Large Corpora for Text Classification,2002,17,4,1,1,9286,fumiyo fukumoto,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs). NB is based on the assumption of word independence in a text, which makes the computation of it far more efficient. SVMs, on the other hand, have the potential to handle large feature spaces, which makes it possible to produce better performance. The training data for SVMs are extracted using NB classifiers according to the category hierarchies, which makes it possible to reduce the amount of computation necessary for classification without sacrificing accuracy."
C02-2012,Topic Tracking using Subject Templates and Clustering Positive Training Instances,2002,8,2,2,0.909091,17026,yoshimi suzuki,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"Topic tracking, which starts from a few sample stories and finds all subsequent stories that discuss the same topic, is a new challenge for the text categorization task and is useful for timeline-based IR systems. Much previous research on topic tracking use machine learning techniques. However, the small size of the training data, especially positive training stories, presents difficulties in training the parameters of the topic tracking system to produce optimal results. In this paper, we present a method for topic tracking using subject templates and k -means clustering algorithm to select a suitable training set. The method was tested on the TDT1 corpus, and the result shows the effectiveness of the method."
C02-1085,Detecting Shifts in News Stories for Paragraph Extraction,2002,13,10,1,1,9286,fumiyo fukumoto,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"For multi-document summarization where documents are collected over an extended period of time, the subject in a document changes over time. This paper focuses on subject shift and presents a method for extracting key paragraphs from documents that discuss the same event. Our extraction method uses the results of event tracking which starts from a few sample documents and finds all subsequent documents that discuss the same event. The method was tested on the TDT1 corpus, and the result shows the effectiveness of the method."
W00-0404,Extracting Key Paragraph based on Topic and Event Detection Towards Multi-Document Summarization,2000,17,7,1,1,9286,fumiyo fukumoto,NAACL-ANLP 2000 Workshop: Automatic Summarization,0,This paper proposes a method for extracting key paragraph for multi-document summarization based on distinction between a topic and an event. A topic and an event are identified using a simple criterion called domain dependency of words. The method was tested on the TDT1 corpus which has been developed by the TDT Pilot Study and the result can be regarded as promising the idea of domain dependency of words effectively employed.
E99-1028,Word Sense Disambiguation in Untagged Text based on Term Weight Learning,1999,15,6,1,1,9286,fumiyo fukumoto,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning. In our method, collocations which characterise every sense are extracted using similarity-based estimation. For the results, term weight learning is performed. Parameters of term weighting are then estimated so as to maximise the collocations which characterise every sense and minimise the other collocations. The results of experiment demonstrate the effectiveness of the method."
W98-1509,An Empirical Approach to Text Categorization Based on Term Weight Learning,1998,3,0,1,1,9286,fumiyo fukumoto,Proceedings of the Third Conference on Empirical Methods for Natural Language Processing,0,None
P98-2207,Keyword Extraction using Term-Domain Interdependence for Dictation of Radio News,1998,4,19,2,0.909091,17026,yoshimi suzuki,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors shows term-domain interdependence and are used for selecting a suitable domain of each part of radio news. Keywords are extracted by using the selected domain. The results of keyword extraction experiments showed that our methods are robust and effective for dictation of radio news."
C98-2202,Keyword Extraction using Term-Domain Interdependence for Dictation of Radio News,1998,4,19,2,0.909091,17026,yoshimi suzuki,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains. In our method, newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors. The feature vectors shows term-domain interdependence and are used for selecting a suitable domain of each part of radio news. Keywords are extracted by using the selected domain. The results of keyword extraction experiments showed that our methods are robust and effective for dictation of radio news."
A97-1043,An Automatic Extraction of Key Paragraphs Based on Context Dependency,1997,23,19,1,1,9286,fumiyo fukumoto,Fifth Conference on Applied Natural Language Processing,0,"In this paper, we propose a method for extracting key paragraphs in articles based on the degree of context dependency. Like Luhn's technique, our method assumes that the words related to theme in an article appear throughout paragraphs. Our extraction technique of keywords is based on the degree of context dependency that how strongly a word is related to a given context. The results of experiments demonstrate the applicability of our proposed method."
C96-1069,An Automatic Clustering of Articles Using Dictionary Definitions,1996,7,23,1,1,9286,fumiyo fukumoto,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"In this paper, we propose a statistical approach for clustering of articles using on-line dictionary definitions. One of the characteristics of our approach is that every sense of word in articles is automatically disambiguated using dictionary definitions. The other is that in order to cope with the problem of a phrasal lexicon, linking which links words with their semantically similar words in articles is introduced in our method. The results of experiments demonstrate the effectiveness of the proposed method."
C94-2122,Automatic Recognition of Verbal Polysemy,1994,5,11,1,1,9286,fumiyo fukumoto,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Polysemy is one of the major causes of difficulties in semantic clustering of words in a corpus. In this paper, we first give a definition of polysemy from the viewpoint of clustering and then, based on this definition, we propose a clustering method which recognises verbal polysemies from a textual corpus. The results of experiments demonstrate the effectiveness of the proposed method."
