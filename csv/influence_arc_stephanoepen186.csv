2000.iwpt-1.19,E93-1010,0,0.0281879,"Missing"
2000.iwpt-1.19,W89-0206,0,0.0364308,"parsing algorithm that is now used standardly in both the LKB a nd PAGE; for the Ger man and Japanese VerbMobil grammars in PAGE, the observed benefits of hyper-active parsing were broadly confirmed . 2.2 Rule Instantiation Strategies Head-driven approaches to parsing have been explored successfully with lexicalized grammars like HPSG (see van Noord , 1997, for a recent overview) because , basically, they can avoid proliferatio n of partial rule instantiations (i.e . active edges in a chart parser) with rules that contain very u nspecific argument positions . Many authors either implicitly (Kay, 1989) or explicitly (Bouma & van Noord , 1993) assume the linguistic head to be the argument position that the parser should instantiate first . However, the right choice of argument position in each rule , such that it best constrains rule applicability ( with respect to all categories derived by the grammar) cannot be determined a nalytically. Though the selection is likely to be related to the amount and specificity of information e ncoded for each argument , for some rules a single feature value (e .g. the [WH +] constraint on the non-head daughter in one of the instantiations of the filler - h"
2000.iwpt-1.19,P99-1061,0,0.64736,"practical behaviour of such (Carroll, 1994) parsers. 1 Early 1994, research groups at Saarbriicken (Uszkoreit et al. , 1994) and CSLI Stanford2 (Copestake, 1992; Flickinger & Sag, 1998) started to collaborate on the development of large-scale HPSG grammars, suitable grammar engineering platforms, and efficient processors. While both sites had worked on HPSG implementation before, the joint effort has greatly increased productivity, resulted in a mutual exchange of knowledge and technology, and helped building a collection of grammar development environments, several highly engineered parsers (Kiefer, Krieger, Carroll, & Malouf, 1999) and an efficient generator (Carroll, Copestake, Flickinger, & Poznanski, 1999) . Recently, the HPSG group at Tokyo University3 (Torisawa & Tsujii, 1996) has entered the stage and now supplies additional expertise on (abstract-machine-based) compilation of typed feature structures, Japanese HPSG, and grammar approximation techniques. *This play builds heavily on results obtained through collaboration between a largish group of people at several sites; Act 5 presents a list of individuals who have contributed to the development providing the background for our current discussion. Technical det"
2000.iwpt-1.19,J97-3004,0,0.033728,"Missing"
2000.iwpt-1.19,P94-1040,0,0.107014,"ach, and apply the empirical methodology in a fairly detailed discussion of what was achieved during a development period of three years. Given the collaborative nature in setup, the empirical results we present involve research and achievements of a large group of people. Dramatis Personre [ ... ] we view the discovery of parsing strategies as a largely experimental process of incremental optimization. (Erbach, 1991a) [ ... ] the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such (Carroll, 1994) parsers. 1 Early 1994, research groups at Saarbriicken (Uszkoreit et al. , 1994) and CSLI Stanford2 (Copestake, 1992; Flickinger & Sag, 1998) started to collaborate on the development of large-scale HPSG grammars, suitable grammar engineering platforms, and efficient processors. While both sites had worked on HPSG implementation before, the joint effort has greatly increased productivity, resulted in a mutual exchange of knowledge and technology, and helped building a collection of grammar development environments, several highly engineered parsers (Kiefer, Krieger, Carroll, & Malouf, 1999) a"
2000.iwpt-1.19,W97-1513,0,0.0450608,"Missing"
2000.iwpt-1.19,C94-1072,1,0.910442,"Missing"
2000.iwpt-1.19,C96-2160,0,\N,Missing
2000.iwpt-1.19,A92-1012,0,\N,Missing
2000.iwpt-1.19,C96-2120,1,\N,Missing
2000.iwpt-1.19,P91-1041,0,\N,Missing
2000.iwpt-1.19,1991.iwpt-1.19,0,\N,Missing
2000.iwpt-1.19,C94-2144,0,\N,Missing
2004.tmi-1.2,P92-1005,0,0.0634629,"nent communication is in terms of sets of MRSs and, thus, can easily be managed in a distributed and (potentially) parallel client – server set-up. Both the analysis and generation grammars ‘publish’ their interface to transfer—i.e. the inventory and synopsis of semantic predicates— in the form of a Semantic Interface specification (‘SEM-I’), such that transfer can operate without knowledge about grammar internals. In practical terms SEM-Is are an important 1 In this respect, MRS is closely related to a tradition of underspecified semantics reflected in, among others, Quasi-Logical Form (QLF; Alshawi & Crouch, 1992), Underspecified Discourse Representation Theory (UDRT; Reyle, 1993), Hole Semantics (Bos, 1995), and the Constraint Language for Lambda Structures (CLLS; Egg, Koller, & Niehren, 2001). development tool (facilitating wellformedness testing of interface representations at all levels), but they also have interesting theoretical status with regard to transfer. The SEM-Is for the Norwegian analysis and English generation grammars, respectively, provide an exhaustive enumeration of legitimate semantic predicates (i.e. the transfer vocabulary) and ‘terms of use’, i.e. for each predicate its set of a"
2004.tmi-1.2,C96-1023,0,0.0187876,"o multiple phases and optionally apply output filters upon the completion of each phase. The LOGON transfer grammar, for example, includes two sets of language-specific MTRs to accommodate grammar-specific idiosyncrasies before and after the core transfer phase, in some cases simply suppressing superfluous information (e.g. predicates introduced by selected-for prepositions and some aspectual markers), in others re-arranging or augmenting semantics to facilitate English generation. Transfer outputs incorporating plural mass nouns, for example, require the insertion of a suitable ‘classifier’ (Bond, Ogura, & Ikehara, 1996), in order to generate, say, two pieces of information instead of the ungrammatical ∗ two informations. temp loc at p temp in p temp temp abstr on p temp afternoon n day n ··· year n Figure 4: Excerpt from predicate hierarchies provided by English SEM-I. Temporal, directional, and other usages of prepositions give rise to distinct, but potentially related, semantic predicates. Likewise, the SEM-I incorporates some ontological information, e.g. a classification of temporal entities, though crucially only to the extent that is actually grammaticized in the language proper. create a non-determin"
2004.tmi-1.2,1995.tmi-1.2,1,0.818746,"Missing"
2004.tmi-1.2,P00-1061,0,0.0112111,"Missing"
2005.eamt-1.27,2004.tmi-1.2,1,0.743699,"Missing"
2005.eamt-1.27,C02-2025,1,0.798577,"y size remains limited to a minimum breadth (Flickinger, Nerbonne, Sag, & Wasow, 1987). Test suites typically include some degree of linguistic and extra-linguistic annotation. In the case of LOGON, these consist primarily of an internal break-down by linguistic phenomena and the determination of the intended or preferred reading(s) per test item, as with grammars of relatively broad coverage some ambiguities are inevitable. The latter annotation, in turn, is part of our regular treebanking (or MRS banking) of LOGON test suites and domain corpora (in the spirit of the LinGO Redwoods approach; Oepen et al., 2002). It is not only needed to maintain training material for stochastic processes (like parse and realization ranking), but at the same time makes regression testing a lot more concise. A general ability to track all changes in system outputs or intermediate analyses is very useful already, but the added information about which hypotheses are actually in-focus clearly facilitates more targeted diagnostics. The two main test suites used so far exemplify aspects of semantic composition (e.g. variation in complementation and linking, modiﬁcation, quantiﬁcation, scopal relations, et al.) and various"
2005.eamt-1.27,2003.mtsummit-systems.14,0,0.0263188,"Missing"
2005.mtsummit-papers.15,I05-1015,1,0.850946,"phrases (e.g. by using bleu or other string-similarity measures typically used for evaluation) prior to training might be a well-suited approach also for building models for realization ranking. In initial experiments, however, we were unable to improve ranker performance over the results reported here when training our MaxEnt model against a graded distribution, although we have not yet obtained conclusive results for this set115 up. More practically, the way our realization rankers actually get deployed in the LOGON system is by means of selective unpacking from the packed generator forest: Carroll & Oepen (2005) present the unpacking procedure in full detail, but quite obviously there is a tradeoﬀ between the ability to prune competing but dis-preferred realizations early, on the one hand, and improved realization ranking accuracy obtained from feature templates that take into account structural properties of larger constituents, on the other hand. Acknowledgements This paper has beneﬁted in many ways from feedback given by members of the LOGON research team at Oslo University, speciﬁcally Francis Bond and Dan Flickinger. Comments given to a preliminary report on the underlying research by the audien"
2005.mtsummit-papers.15,N04-1021,0,0.0769829,"Missing"
2005.mtsummit-papers.15,2004.tmi-1.2,1,0.884139,"Missing"
2005.mtsummit-papers.15,C02-2025,1,0.802474,"t we use for evaluating the diﬀerent statistical rankers and also for training the MaxEnt models. In order to select a preferred surface realization we want a conditional model that gives us the probability of a string given its semantics. It is worth noting that the problem of realization ranking in many ways can be seen as ‘inversely similar’ to the problem of parse selection, i.e. choosing the best analysis for a given string. Our work on constructing models for realization draws heavily on the previous work on parse disambiguation in relation to the HPSG Redwoods2 treebank, as reported by Oepen et al. (2002). 2 See ‘http://www.delph-in.net/redwoods/’ more information about the Redwoods project. for 110 Stochastic models for parse selection are typically trained on a treebank consisting of strings paired with their optimal analyses. When training the discriminative models (described in Section 3.2) for realization selection we use a treebank where this optimality relation is taken to be bidirectional in the sense that the original string is also treated as an optimal realization of the corresponding semantic analysis (i.e. ‘meaning’). For each input, the Redwoods treebank provides a full HPSG anal"
2005.mtsummit-papers.15,C00-1085,0,0.0507954,"ypes (‘Combined’). The data items are binned with respect to number of distinct realizations. others as plain wrong. In realization ranking, on the other hand, it is perhaps more meaningful to think of a graded continuum of more or less natural verbalizations (given an input semantics). All outputs of the lkb realizer are semantically equivalent and guaranteed to be well-formed with respect to the underlying grammar. This means that the kind of properties we aim at capturing with the discriminative model are soft constraints that govern the degree of ‘correctness’ among competing paraphrases. Osborne (2000) and Malouf & Noord (2004) describe an approach to parse disambiguation using maximum entropy models where the empirical distribution that deﬁnes the constraints for the model are not based on frequency counts from a corpus but rather some measure of similarity towards the reference. Deﬁning such a preference weighting of the candidate paraphrases (e.g. by using bleu or other string-similarity measures typically used for evaluation) prior to training might be a well-suited approach also for building models for realization ranking. In initial experiments, however, we were unable to improve rank"
2005.mtsummit-papers.15,P99-1069,0,0.357477,"rkers). We then rank the realizations by computing their negative log-probabilities with respect to the model. In other words, the score of a string with k tokens, score(w1k ), is computed as − ln pn (w1k ) =  − ki=1 ln pn (wi |wi−n , . . . , wi−1 ). After training and testing several language models for varying values of n, we ended up using an 4-gram model (backing-oﬀ for unobserved n-grams) for the results reported here. 3.2 A Maximum Entropy Ranker Log-linear models provide a very ﬂexible framework that has been widely used for a range of tasks in NLP, including parse selection (see e.g. Johnson, Geman, Canon, Chi, & Riezler, 1999; Malouf & Noord, 2004) and reranking for machine translation (see e.g. Och et al., 2004). A model is speciﬁed by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Given a set of d such features, each realization r is represented as a feature vector f (r) ∈ d , and a vector of weights λ ∈ d is then ﬁtted to optimize the likelihood of the training data. A conditional log-linear model for the probability of a realization r given the semantics s, has the general parametric form"
2005.mtsummit-papers.15,P02-1040,0,0.0725411,"trained on tiny amounts of data can compete favorably on the realization ranking task when compared to a n-gram language model trained on a large text corpus. In the current paper we train and evaluate rankers on an expanded treebank and using a richer inventory of feature 109 types. Three models are described: a traditional surface-oriented n-gram model, a maximum entropy model using structural features, and a combination of these two. We evaluate the diﬀerent models, as well as the utility of individual feature types, by comparing exact match accuracy and averaged per-sentence bleu scores (Papineni, Roukos, Ward, & Zhu, 2002). The paper is organized as follows. In Section 2 we brieﬂy review our notion of symmetric treebanks and the properties of the data set used for our experiments. Section 3 describes the three models that we train, including the feature types of the maximum entropy (MaxEnt) models. An evaluation of the performance of the diﬀerent models is presented in Section 4, before we go on to discuss the results and sketch directions for ongoing work in Section 5. 2 Background and Data The LOGON system has an architecture based on semantic transfer that uses meaning representations based on Minimal Recur"
2005.mtsummit-papers.15,A00-2021,0,0.0590806,"ined best overall performance. When instantiating all feature templates as described above our models contain close to 65000 features. 3.4 A Combined Model The second MaxEnt model is a combination of the two models described in Section 3.1 and 3.2 above; in addition to the set of structural feature types it includes as a separate feature the sentence scores computed by the n-gram language model. In other words, the value of the d + 1’th feature is the log-probability of the string as given by the n-gram model pn , i.e. fd+1 (r) = ln pn (y(r)), where y(r) is the yield of r and n = 4 as before. Johnson & Riezler (2000) show an interesting equivalence between using log-probabilities as features and using a geometric mixture of the same probabilities for the default distribution q of Equation 1 (where the λ-parameters of the features would correspond to their weights in the mixture). This means that a special case of the simple combined model we present here would be a MEMD model where the uniform distribution q is replaced by the language model pn . If λd+1 = 1 then exp(fd+1 λd+1 ) = pn and we would eﬀectively have a MEMD model as described above with q = pn . Both of the log-linear models described in this"
2005.mtsummit-papers.15,W02-2030,0,0.0415427,"KLdiveregence) between the model and the reference distribution D(pλ ||q) on the one hand, and between the empirical distribution and the model D(˜ p ||pλ ) on the other. Given a MaxEnt model pλ , the scores used for ranking the candidate realizations can be com puted simply as score(r) = i λi fi (r) since we are only interested in the rank order. 3.3 Maximum Entropy Features The ﬁrst MaxEnt model that we trained uses structural features deﬁned over HPSG derivation trees as summarized in Table 3. For the purpose of parse selection, Toutanova, Manning, Shieber, Flickinger, & Oepen (2002) and Toutanova & Manning (2002) train a discriminative log-linear model on the Redwoods parse treebank, using features deﬁned over derivation trees with non-terminals representing the construction types and lexical types of the HPSG 112 # 1 1 1 1 1 2 2 3 3 3 4 4 4 0 1 0 1 2 0 0 1 2 3 1 2 3 sample features subjh hspec third sg fin verb  subjh hspec third sg fin verb hspec det the le sing noun subjh hspec det the le sing noun  subjh hspec det the le sing noun subjh third sg fin verb hspec sing noun n intr le dog det the le n intr le dog  det the le n intr le dog n intr le det the le n intr le  det"
2005.mtsummit-papers.15,W98-1426,0,0.0543788,"(excluding items with only one realization), average string length, and average structural ambiguity. The rightmost column shows a random choice baseline, i.e. the probability of selecting the preferred realization by chance. go on to look at the two maximum entropy or log-linear models that we train using structural features from the symmetric treebank described in the previous section. 3.1 A Language Model Ranker The ﬁrst statistical model that we apply for ranking the generator outputs is an n-gram language model.3 This approach is in many ways similar to those presented by, among others, Langkilde & Knight (1998) and White (2004) and quite generally still appears predominant in the realization ranking literature. The model is trained on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words. As the realizations in our symmetric treebank also include punctuations, these are also treated as separate tokens by the language model (in addition to sentence boundary markers). We then rank the realizations by computing their negative log-probabilities with respect to the model. In other words, the score of a string with k tokens, score(w1k ), is computed as − ln pn ("
2005.mtsummit-papers.15,W02-2018,0,0.0213258,"ations of s. The so-called reference or default distribution q is often only implicit since in maximum entropy estimation this is just the constant function |Y 1(s) |(for a given s). One can, however, also replace this uniform distribution by some other reference distribution to incorporate prior knowledge in the model. This approach is also known as maximum entropy / minimum divergence (MEMD) modeling, and we will return to this more general framework below. The estimation4 of the λ-parameters seek to maximize the (log of) a penalized likelihood 4 3  We use the estimate open-source package (Malouf, 2002) for training, using its limited-memory variable metric as the optimization method. subjh hspec third sg fin verb det the le sing noun v unerg le the n intr le barks dog Figure 1: Sample HPSG derivation tree for the input the dog barks. Phrasal nodes are labeled with identiﬁers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexical entries. function as in (3) ˆ = arg max log L(λ) − λ λ d 2 i=1 λi 2σ 2 where L(λ) is the ‘conditionalized’ likelihood of the training data (as described by Johnson et al., 1999), computed as L(λ) = N i=1 pλ (ri |si ). The second t"
2005.mtsummit-papers.22,W02-1503,1,0.809591,"ationship between the two formalisms in general, while our task goes beyond that. We need to interrelate speciﬁc f-structures and MRS representations which are not only well-formed, but which also satisfy further, mutually independent constraints. In the ﬁrst place, already the fact that f-structures are syntactic representations and MRSs semantic representations designed to capture translational relations frequently motivates diﬀerent packagings of information on the two levels. Furthermore, the NorGram f-structures meet the requirements for f-structures developed within the ParGram project (Butt, Dyvik, King, Masuichi, & Rohrer, 2002; Dyvik, 2003), while the NorGram MRS representations are constructed according to the same general principles as the MRS representations of the target ERG grammar. As a result the f-structure and MRS analyses of the same sentence are not always in a simple structural correspondence with each other. One example is nominal phrases with several speciﬁers in the f-structure, and phrases with no speciﬁers (Norwegian bare singulars), contrasted with the MRS requirement that a variable must always be bound by one single quantiﬁer; other examples involve diﬀerent dominance relations among predicates"
2005.mtsummit-papers.22,1995.tmi-1.2,1,0.840886,"ally grounded component of each grammar, capturing several classes of lexical regularities while also serving the crucial engineering function of supplying a reliable and complete speciﬁcation of the elementary predications the grammar can realize. We make extensive use of underspeciﬁcation and type hierarchies to maximize generality and precision. 1 Introduction In this paper we introduce two interesting features of the Norwegian-to-English machine translation system LOGON. (1) It is the ﬁrst system to use the full power of Minimal Recursion Semantics in translation (originally introduced by Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995). (2) The transfer modules use the SEM-I, an interface speciﬁcation designed to allow the use of deep grammars in various applications without knowledge of the grammar internals (Copestake & Flickinger, 2003). The motivation for the SEM-I is threefold. First, it allows the semantic representation to be underspeciﬁed. In the LOGON system, if the analysis system does not have enough information to commit to an interpretation then the ambiguity is retained. Doing this by naively expanding all interpretations is so ineﬃcient as to be unworkable. Second, it exposes only the information that is rel"
2005.mtsummit-papers.22,P97-1052,0,0.0715544,"Missing"
2005.mtsummit-papers.22,1997.tmi-1.18,0,0.00828586,"ular count {IND +} nouns, we will generate for example She wrote a good paper but not *She wrote good paper. The underspeciﬁed ﬁrst entry above will be useful in translating between English and, say, German, where in both languages one word can be used for both senses, and where for some sentences no disambiguation is required, as in That paper is good. This approach diﬀers from many semantic-transfer based systems, particularly knowledge-based MT. In these systems lexical semantic information (including word senses, semantic classes and selectional preferences) is used to disambiguate input (Mahesh et al., 1997; Ikehara et al., 1996). The interface between transfer and generation is then a single fully speciﬁed semantic representation (although often with scope issues ignored), rather than an underspeciﬁed one. The SEM-I is compatible with such an approach — it would then link the grammars to the ontology of lexical semantic information and provide the input to the sense disambiguation module. 5.2 Productive Derivation Languages typically include productive derivational processes which result in multiple words whose syntactic and semantic properties are related but distinct. One such common process"
2005.mtsummit-papers.22,2004.tmi-1.2,1,0.832796,"Missing"
2006.eamt-1.29,2005.mtsummit-papers.22,1,0.63267,"f semantic transfer rules becomes a more interesting task. In a nutshell, the transfer-level mapping that needs to be established is three-fold: (a) relating a source language MRS predicate to surface forms and thus dictionary entries; (b) looking up candidate translations from the dictionary and analyzing their internal structure (if any); and (c) finding target language predicates that correspond to each dictionary translation(s) and its components. Besides the dictionary (accessed as a relational database), the source and target language Semantic Interfaces (called SEM-Is; see Figure 2 and Flickinger, Lønning, Dyvik, Oepen, & Bond, 2005 for background) are central knowledge sources in this process. For both the analysis and generation grammars, their SEM-Is spell out the inventory of valid semantic predicates, jointly with information about the ‘terms of use’ for each predicate, e.g. its range of semantic roles, value constraints, indication of optionality, et al. Based on certain naming conventions in semantic predicates and the typing of MRS variables, we can further read out a limited number of syntactic properties for the class of words introducing each predicate from its SEM-I entry. To faciliate the mapping from semant"
2006.eamt-1.29,2004.tmi-1.2,1,0.794944,"Missing"
2007.tmi-papers.18,I05-1015,1,0.851038,"as a feature in the model. TRANSFER METRICS Two additional features capture information about the transfer step: the total number of transfer rules that were invoked (as a measure of transfer granularity, e.g. where idiomatic transfer of a larger cluster of EPs contrasts with stepwise transfer of component EPs), as well as the ratio of EP counts, |E|/|F |. SEMANTIC DISTANCE Generation proceeds in two phases: a chart-based bottom-up search enumerates candidate realizations, of which a final semantic compatiblity test selects the one(s) whose MRS is subsumed by the original generator input MRS (Carroll & Oepen, 2005). Given an imperfect input (or error in the generation grammar), it is possible for none of the candidate outputs to fulfill the semantic compatiblity test. In this case, the generator will gradually relax MRS com150 parison, going through seven pre-defined levels of semantic mismatch, which we encode as one integer-valued feature in the re-ranking model. Training the Model While batch translating, the LOGON controller records all candidate translations, intermediate semantic representations, and a large number of processing and resource consumption properties in a database, which we call a pr"
2007.tmi-papers.18,W97-1502,0,0.0125593,"on to the 109 items that translate in both configurations, our BLEU score over the first translation drops from 37.41 to 30.29.1 MRS { prpstn m[MARG recommend v] recommend v[ARG1 pron, ARG2 hike n] a q[ARG0 hike n] around p[ARG1 hike n, ARG2 source n] implicit q[ARG0 source n] poss[ARG1 waterway n, ARG2 source n] def q[ARG0 waterway n] } Figure 4: Variable-free reduction of the MRS for the utterance ‘We recommend a hike around the waterway’s sources’. ing set; this could be done automatically. The second approach is motivated by the hypothesis that discriminants, as used in manual annotation (Carter, 1997), represent promising alternative feature functions to the predefined templates. Initial tests (see table 2) show that the discriminant approach (which is not yet used in the LOGON system) scores better than the templatebased approach. 5 Ranking Transfer Outputs While MRS formulae are highly structured graphs, Oepen & Lønning (2006) suggest a reduction into a variable-free form that resembles elementary dependency structures. For the ranking of transfer outputs, MRSs are broken down into basic dependency triples, whose probabilities are estimated by adaptation of standard n-gram sequence model"
2007.tmi-papers.18,P99-1069,0,0.0438269,"nd a given feature can even itself be a separate statistical model. In the following we first give a brief high-level presentation of conditional log-linear modeling, and then we go on to present the actual feature functions in our setup. Given a set of m real-valued features, each pair of source sentence f and target sentence e are represented as a feature vector Φ(f, e) ∈ ℜm . A vector of weights λ ∈ ℜm is then fitted to optimize some objective function of the training data. For the experiments reported in this paper the weights are fitted to maximize the conditional (or pseudo) likelihood (Johnson, Geman, Canon, Chi, & Riezler, 1999).3 In other words, for each input source sentence in the training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU)"
2007.tmi-papers.18,koen-2004-pharaoh,0,0.0252225,"ise word-to-word probabilities. STRING PROBABILITY Although a part of the (conditional) realization ranker already, we include the string probability (according to the tri4 Of these, 9,410 sentences are taken from the LOGON development data, while an additional 12,946 sentences are from the English-Norwegian Parallel Corpus (Oksefjell, 1999). 5 The ML estimation of the lexical probabilities, as well as the final word alignments produced from the output of GIZA++ , are carried out using the training scripts provided by Phillip Koehn, and as distributed with the phrase-based SMT module Pharaoh (Koehn, 2004). gram language model trained on the BNC) of candidate translations ek as an independent indicator of output fluency. DISTORTION Elementary predications (EPs) in our MRS are linked to corresponding surface elements, i.e. sub-string pointers. Surface links are preserved in transfer, such that post-generation, for each EP—or group of EPs, as transfer need not be a one-to-one mapping—there is information about its original vs. its output sub-string span. To gauge reordering among constituents, for both the generator input and output, each EP is compared pairwise to other EPs in the same MRS, and"
2007.tmi-papers.18,W98-1426,0,0.119221,"Missing"
2007.tmi-papers.18,W02-2018,0,0.0150677,"resent the actual feature functions in our setup. Given a set of m real-valued features, each pair of source sentence f and target sentence e are represented as a feature vector Φ(f, e) ∈ ℜm . A vector of weights λ ∈ ℜm is then fitted to optimize some objective function of the training data. For the experiments reported in this paper the weights are fitted to maximize the conditional (or pseudo) likelihood (Johnson, Geman, Canon, Chi, & Riezler, 1999).3 In other words, for each input source sentence in the training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU) directly, following the Minimum Error Rate approach of Och (2003). The three most fundamental features that are supplied in our log-linear re-ranker correspon"
2007.tmi-papers.18,P03-1021,0,0.0280051,"e training data we seek to maximize 3 For estimation we use the TADM open-source toolkit (Malouf, 2002), using its limited-memory variable metric as the optimization method. As is standard practice, the model is regularized by including a zero-mean Gaussian prior on the feature weights to reduce the risk of overfitting. 149 the probability of its annotated reference translation relative to the other competing candidates. However, for future work we plan to also experiment with optimizing the scores of a given evaluation metric (e.g. BLEU) directly, following the Minimum Error Rate approach of Och (2003). The three most fundamental features that are supplied in our log-linear re-ranker correspond to the three ranking modules of the baseline system, as described in Sections § 4, § 5, and § 6 above. In other words, these features record the scores of the parse ranker, the MRS ranker, and the realization ranker, respectively. But our re-ranker also includes several other features that are not part of the baseline model. Other Features Our experiments so far have taken into account another eight properties of the translation process, in some cases observing internal features of individual compone"
2007.tmi-papers.18,P02-1038,0,0.0471966,"mony between the string lengths of the source and target. Log-linear models provide a very flexible framework for discriminative modeling that allows us to combine disparate and overlapping sources of information in a single model without running the risk of making unwarranted independence assumptions. In this section we describe a model that directly estimates the posterior translation probability Pλ (e|f ), for a given source sentence f and translation e. Although the re-ranker we describe here is built on top of a hybrid baseline system, the overall approach is similar to that described by Och & Ney (2002) in the context of SMT. Log-Linear Models A log-linear model is given in terms of (a) a set of specified features that describe properties of the data, and (b) an associated set of learned weights that determine the contribution of each feature. One advantage of working with a discriminative re-ranking setup is that the model can use global features that the baseline system would not be able to incorporate. The information that the feature functions record can be arbitrarily complex, and a given feature can even itself be a separate statistical model. In the following we first give a brief hig"
2007.tmi-papers.18,2005.eamt-1.27,1,0.855742,"Missing"
2007.tmi-papers.18,2004.tmi-1.2,1,0.736592,"Missing"
2007.tmi-papers.18,oepen-lonning-2006-discriminant,1,0.843509,"waterway n] } Figure 4: Variable-free reduction of the MRS for the utterance ‘We recommend a hike around the waterway’s sources’. ing set; this could be done automatically. The second approach is motivated by the hypothesis that discriminants, as used in manual annotation (Carter, 1997), represent promising alternative feature functions to the predefined templates. Initial tests (see table 2) show that the discriminant approach (which is not yet used in the LOGON system) scores better than the templatebased approach. 5 Ranking Transfer Outputs While MRS formulae are highly structured graphs, Oepen & Lønning (2006) suggest a reduction into a variable-free form that resembles elementary dependency structures. For the ranking of transfer outputs, MRSs are broken down into basic dependency triples, whose probabilities are estimated by adaptation of standard n-gram sequence modeling techniques. The actual training is done using the freely available CMU SLM toolkit (Clarkson & Rosenfeld, 1997). Based on a training set of some 8,500 indomain MRSs, viz. the treebanked version of the English translations of the (full) LOGON development corpus, our target language ‘semantic model’ is defined as a smoothed tri-gr"
2007.tmi-papers.18,C00-1085,0,0.0240505,"xample by assessing the relative contribution of individual features, fine-tuning parameter estimation, and including additional properties. Our current maximum likelihood training of the log-linear model is based on a binarized empirical distribution, where for each input we consider the candidate translation(s) with maximum NEVA score(s) as preferred, and all others as dis-preferred. Obviously, however, the degradation in quality among alternate candidates is continuous (rather than absolute), and we have started experimentation with a graded empirical distribution, adapting the approach of Osborne (2000) to the re-ranking task. Finally, in a parallel refinement cycle, we aim to contrast our current ( LL) re-ranking model with Minimum Error Rate (MER) training, a method that aims to estimate model parameters to directly optimize BLEU scores (or another quality metric) as its objective function. Trading coverage for increased output quality may be economic for a range of tasks—say as a complement to other tools in the workbench of a professional translator. Our re-ranking approach, with access to rich intermediate representations, probabilities, and confidence measures, provides a fertile envir"
2007.tmi-papers.18,P02-1040,0,0.0737594,"the speaker had intended. Our argument for the first translation can be illustrated within our earlier example of a wordlevel noun vs. verb ambiguity in analysis. The many different realizations of the noun in the target language may fall into classes of near synonyms, in which case it does not matter for the quality of the result which synonym is chosen. Even though each of the individual realizations has a low probability, it may be a good translation. Observe here also that an automatic evaluation measure—measuring the similarities to a set of reference translations, like the BLEU metric (Papineni, Roukos, Ward, & Zhu, 2002)—will favor the view of most likely translation. We conjecture, however, that a human evaluation will correspond better to the first translation. From a theoretical point of view, it seems most correct to go for the first translation. But it presupposes that we choose the correct interpretation of the source sentence, which we cannot expect to always do. In cases where we have chosen an incorrect analysis, this might be revealed by trying to translate it into the target language and consider the result. If all the candidate translations sound bad—or have a very low probability—in the target l"
2007.tmi-papers.18,N06-1032,0,0.0685787,"Missing"
2007.tmi-papers.18,W04-3223,0,0.0275221,"t matches and matches among the five top-ranked analyses. Figures in parentheses show a random choice baseline. Both models were trained on seven of nine treebanked texts and evaluated on the two remaining texts. was used to build a treebank for the LOGON development corpus. Parse selection in LOGON uses training data from this treebank; all sentences with full parses with low ambiguity (fewer than 100 readings) were at least partially disambiguated. The parse selection method employed in the LOGON demonstrator uses the stochastic disambiguation scheme and training software developed at PARC (Riezler & Vasserman, 2004). The XLE system provides a set of parameterized feature function templates that must be expanded in accordance with the grammar or the training set at hand. Application of these feature functions to the training data yields feature forests for both the labeled data (the partially disambiguated parse forests) and the unlabeled data (the full parse forests). These feature forests are the input to the statistical estimation algorithm, which generates a property weights file that is used to rank solutions. One of the challenges in applying the probability model to a given grammar and training set"
2009.eamt-1.19,W04-0404,0,0.0767184,"selection of experimental data, available resources, and specifics of our approach (Section 3), lay out the design of our experiments (Section 4), present a wealth of empirical results (Section 5), and finally conclude with a critical discussion of our findings (Section 6). 2 Earlier Work In investigating the automatic translation of Norwegian nominal compounds, our starting point is the influential approach of Tanaka and Baldwin— henceforth T&B—who explore various ways of translating Japanese nominal compounds into English and vice versa (Tanaka and Baldwin, 2003a; Tanaka and Baldwin, 2003b; Baldwin and Tanaka, 2004). Abstractly, our steps (a) to (d) as sketched above are all taken from T&B, but there are important differences in the specifics of our approach, as well as extensions beyond the results of T&B. Besides, our focus on another language pair (with severely more limited resources available on the Norwegian source language side), most of the relevant differences pertain to the ranking step, arguably the key component in obtaining highquality translations. Tanaka and Baldwin (2003b) suggest to rank candidate translations based on target language (TL) distributional properties, essentially corpus fr"
2009.eamt-1.19,P06-4020,0,0.0273739,"Missing"
2009.eamt-1.19,P05-1022,0,0.0209959,"nsed by template t and the probability of w2 being the second element, respectively. An example would be the count of machine translation occurring as two nouns in a sequence (the template) divided by the total count of all template instances, added to how often machine is the first word of such couples, and translation is the second, to capture what words more often let themselves be combined in such compounds. 4.5 MaxEnt Basics: Mono-Lingual Features The Maximum Entropy (MaxEnt) framework has been applied successfully to NLP tasks before (Ratnaparkhi, 1996; Ratnaparkhi, 1998; Mikheev, 2000; Charniak and Johnson, 2005; Velldal, 2008) in areas like parsing, sentence boundary detection, and PoS tagging, but notably (re-)ranking, for which it is also used in this paper. The various statistics for each translation candidate (which will be discussed in further detail below), can be used as features in a conditional MaxEnt model (the family of MaxEnt models is also commonly referred to as log-linear or exponential models).8 mated by maximum likehood over the training corpus, should be conditioned on t or not: Tanaka and Baldwin (2003b) discuss the terms as ‘conditional’ probabilities, but equation 1 suggests a n"
2009.eamt-1.19,1999.tc-1.8,0,0.0115233,"of mono-lingual training data— of our various methods. While T&B have been the foremost source of inspiration for our work, earlier approaches to the compound analysis and translation problem include Rackow et al. (1992), who explore the translation of German compounds into English. While their task is quite similar, this work has its emphasis on the segmentation and analysis of SL compounds, although it proposes using corpus data (counts) to distinguish between the various candidate translations. From the available information, the approach was not fully implemented or evaluated empirically. Grefenstette (1999), translating German and Spanish compounds, shows how WWW counts can be used to rank candidates, although his experiments are confined only to compounds for which a translation exists in a bi-lingual dictionary. 4 Baldwin and Tanaka (2004) report that, in their SVM experiments, most of their training runs failed to converge, i.e. did not result in a functional classifier. This observation may well be owed to their creative use of the SVM framework. 137 3 Methodology and Preparational Steps We pursued a data-driven approach both in the selection of training and test compounds and in the discove"
2009.eamt-1.19,W02-2018,0,0.0194689,"8 mated by maximum likehood over the training corpus, should be conditioned on t or not: Tanaka and Baldwin (2003b) discuss the terms as ‘conditional’ probabilities, but equation 1 suggests a non-conditional formalization (in contrast to, for example, p(w1E , w2E |t)). We implemented both variants and found the non-conditional CTQ to perform substantially better, hence restrict ourselves to this variant in the following. Just like T&B, we use α = 0.9 and β = 0.1. 8 Like Velldal (2008) and much other current work, we make use of the open-source TADM framework, see http:// tadm.sourceforge.net (Malouf, 2002). Table 2: Bi-lingual features, extracted from the dictionary. N1 and N2 denote the first and second element of the Norwegian compound and E1 and E2 designate the English translations of these components in the current translation template. Given a source language compound n, our model estimates the probability of a candidate translation ei as the normalized dot product of a vector f~ of so-called features—arbitrary properties determined by so-called feature functions—and a vector ~λ of corresponding weights: P exp j λj fj (ei , n) P p(ei |n) = Pn j λj fj (ek , n) k=1 exp (2) The search for th"
2009.eamt-1.19,A00-2035,0,0.0237898,"nstruction licensed by template t and the probability of w2 being the second element, respectively. An example would be the count of machine translation occurring as two nouns in a sequence (the template) divided by the total count of all template instances, added to how often machine is the first word of such couples, and translation is the second, to capture what words more often let themselves be combined in such compounds. 4.5 MaxEnt Basics: Mono-Lingual Features The Maximum Entropy (MaxEnt) framework has been applied successfully to NLP tasks before (Ratnaparkhi, 1996; Ratnaparkhi, 1998; Mikheev, 2000; Charniak and Johnson, 2005; Velldal, 2008) in areas like parsing, sentence boundary detection, and PoS tagging, but notably (re-)ranking, for which it is also used in this paper. The various statistics for each translation candidate (which will be discussed in further detail below), can be used as features in a conditional MaxEnt model (the family of MaxEnt models is also commonly referred to as log-linear or exponential models).8 mated by maximum likehood over the training corpus, should be conditioned on t or not: Tanaka and Baldwin (2003b) discuss the terms as ‘conditional’ probabilities,"
2009.eamt-1.19,2004.tmi-1.2,1,0.891085,"Missing"
2009.eamt-1.19,C92-4201,0,0.050447,"Missing"
2009.eamt-1.19,W96-0213,0,0.00994226,"ity of w1 as the first element in a construction licensed by template t and the probability of w2 being the second element, respectively. An example would be the count of machine translation occurring as two nouns in a sequence (the template) divided by the total count of all template instances, added to how often machine is the first word of such couples, and translation is the second, to capture what words more often let themselves be combined in such compounds. 4.5 MaxEnt Basics: Mono-Lingual Features The Maximum Entropy (MaxEnt) framework has been applied successfully to NLP tasks before (Ratnaparkhi, 1996; Ratnaparkhi, 1998; Mikheev, 2000; Charniak and Johnson, 2005; Velldal, 2008) in areas like parsing, sentence boundary detection, and PoS tagging, but notably (re-)ranking, for which it is also used in this paper. The various statistics for each translation candidate (which will be discussed in further detail below), can be used as features in a conditional MaxEnt model (the family of MaxEnt models is also commonly referred to as log-linear or exponential models).8 mated by maximum likehood over the training corpus, should be conditioned on t or not: Tanaka and Baldwin (2003b) discuss the ter"
2009.eamt-1.19,W03-1803,0,0.387353,"closely related earlier work (Section 2), sketch the selection of experimental data, available resources, and specifics of our approach (Section 3), lay out the design of our experiments (Section 4), present a wealth of empirical results (Section 5), and finally conclude with a critical discussion of our findings (Section 6). 2 Earlier Work In investigating the automatic translation of Norwegian nominal compounds, our starting point is the influential approach of Tanaka and Baldwin— henceforth T&B—who explore various ways of translating Japanese nominal compounds into English and vice versa (Tanaka and Baldwin, 2003a; Tanaka and Baldwin, 2003b; Baldwin and Tanaka, 2004). Abstractly, our steps (a) to (d) as sketched above are all taken from T&B, but there are important differences in the specifics of our approach, as well as extensions beyond the results of T&B. Besides, our focus on another language pair (with severely more limited resources available on the Norwegian source language side), most of the relevant differences pertain to the ranking step, arguably the key component in obtaining highquality translations. Tanaka and Baldwin (2003b) suggest to rank candidate translations based on target languag"
2009.eamt-1.19,2003.mtsummit-papers.50,0,0.278266,"closely related earlier work (Section 2), sketch the selection of experimental data, available resources, and specifics of our approach (Section 3), lay out the design of our experiments (Section 4), present a wealth of empirical results (Section 5), and finally conclude with a critical discussion of our findings (Section 6). 2 Earlier Work In investigating the automatic translation of Norwegian nominal compounds, our starting point is the influential approach of Tanaka and Baldwin— henceforth T&B—who explore various ways of translating Japanese nominal compounds into English and vice versa (Tanaka and Baldwin, 2003a; Tanaka and Baldwin, 2003b; Baldwin and Tanaka, 2004). Abstractly, our steps (a) to (d) as sketched above are all taken from T&B, but there are important differences in the specifics of our approach, as well as extensions beyond the results of T&B. Besides, our focus on another language pair (with severely more limited resources available on the Norwegian source language side), most of the relevant differences pertain to the ranking step, arguably the key component in obtaining highquality translations. Tanaka and Baldwin (2003b) suggest to rank candidate translations based on target languag"
2020.conll-shared.1,W13-2322,0,0.190653,"owards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 4: Abstract Meaning Representation (AMR) for the running example A simila"
2020.conll-shared.1,W15-0128,1,0.796384,"des). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). Elementary Dependency Structures The EDS graphs (Oepen and Lønning, 2006) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015).3 Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) encode English Resource Semantics in a variablefree semantic dependency graph—not limited to bi-lexical dependencies—where graph nodes correspond to logical predications and edges to labeled argument positions. The EDS conversion from underspecified logical forms to directed graphs discards partial information on semantic scope from the full ERS, which makes these graphs abstractly— if not linguistically—similar to Abstract Meaning Representation (see below). Nodes in EDS are in principle independent of surface lexical units, b"
2020.conll-shared.1,D16-1134,1,0.827452,"(UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency fr"
2020.conll-shared.1,W13-0101,1,0.816811,"imilar 〈2:9〉 sempos adj.denot ACT #Benef sempos x EXT almost 〈23:29〉 sempos adv.denot.grad.neg coref.gram #Gen sempos x RSTR other 〈53:58〉 sempos adj.denot Figure 2: Semantic dependency graphs for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Prague Tectogrammatical Graphs (PTG). In addition to node properties, visualized similarly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al.,"
2020.conll-shared.1,E17-2039,1,0.896998,"Missing"
2020.conll-shared.1,2020.emnlp-main.195,0,0.0866269,"with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite"
2020.conll-shared.1,2020.conll-shared.2,1,0.940067,"Structure (DRS), the meaning representations at the core of Discourse Representation Theory (DRT; Kamp and Reyle, 1993; Van der Sandt, 1992; Asher, 1993). DRSs can model many challenging semantic phenomena including quantifiers, negation, scope, pronoun resolution, presupposition accommodation, and discourse structure. Moreover, they are directly translatable into first-order logic formulas to account for logical inference. DRG used in the shared task represents a type of graph encoding of DRS that makes the graphs structurally as close as possible to the structures found in other frameworks; Abzianidze et al. (2020) provide more details on the design choices in the DRG encoding. The source DRS annotations are taken from data release 3.0.0 of the Parallel Meaning Bank (PMB; Abzianidze et al., 2017; Bos et al., 2017).6 Although the annotations in the PMB are compositionally derived from lexical semantics, anchoring information is not explicit in its DRSs; thus, (like AMR) the DRG framework formally instantiates Flavor (2) of meaning representations. The DRG of the running example is given in Figure 5. The concepts (vissualized as oval shapes) are represented by WordNet 3.0 senses and semantic roles (in dia"
2020.conll-shared.1,W19-1201,1,0.906261,"Missing"
2020.conll-shared.1,2020.lrec-1.234,1,0.763502,"s (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP task series, to go well beyond the impressionistic observations from §3 and ideally lead to contrastive refinement across linguistic sc"
2020.conll-shared.1,2020.conll-shared.7,1,0.75652,"Missing"
2020.conll-shared.1,D19-1393,0,0.0309981,"nt alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR"
2020.conll-shared.1,2020.findings-emnlp.89,0,0.016475,"ebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (20"
2020.conll-shared.1,2020.acl-main.119,0,0.179058,"s similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that do not enforce strict correspondences between tokens in the input sentence and the concepts in meaning representation graphs. The iterative inference framework is also based on an encoder–decoder architecture. The encoder takes the sentence as input and computes contextualized token embeddings that are used as text memory by a decoder that iteratively predicts the next node given the text memory and a predicted parent node in the partially constructed graph memory at the previous time step, and then identifies the parent node for the newly pre"
2020.conll-shared.1,2020.conll-shared.6,0,0.0921787,"guage per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu 1 Background and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the correspon"
2020.conll-shared.1,P13-2131,0,0.187673,"Missing"
2020.conll-shared.1,W11-2927,1,0.757531,"was not formally enforced. 5 Evaluation Following the previous edition of the shared task, the official MRP metric for the task is the microaverage F1 score across frameworks over all tuple types that encode ‘atoms’ of information in MRP graphs. The cross-framework metric uniformly evaluates graphs of different flavors, regardless of a specific framework exhibiting (a) labeled or unlabeled nodes or edges, (b) nodes with or without anchors, and (c) nodes and edges with optional properties and attributes, respectively (see Table 4). The MRP metric generalizes earlier frameworkspecific metrics (Dridan and Oepen, 2011; Cai and Knight, 2013; Hershcovich et al., 2019a) in terms of decomposing each graph into sets of typed tuples, as indicated in Figure 6. To quantify graph similarity in terms of tuple overlap, a correspondence relation between the nodes of the goldstandard and system graphs must be determined. Adapting a search procedure for the NP-hard maximum common edge subgraph (MCES) isomorphism problem, the MRP scorer will search for the node-to-node correspondence that maximizes the intersection of tuples between two graphs, where node identifiers (m and n in Figure 6) act like variables that can be e"
2020.conll-shared.1,K19-2007,0,0.0722366,"ground and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the corresponding (but richer) frameworks in 2020, EDS and PTG, respectively, and the original bi-lexical semantic dependency graphs remain independently available (Oepen et al., 2015). 1 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 1–22 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics (a) a unifying formal model over different semantic graph banks (§2)"
2020.conll-shared.1,1997.iwpt-1.10,0,0.772333,"Missing"
2020.conll-shared.1,W19-1202,0,0.0200921,"ntation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP"
2020.conll-shared.1,K19-2016,0,0.0872997,"UCCA, and AMR. This allows a comparison on nearly equal grounds: as Table 9 shows, in terms of LPPS F1 , the state-of-the-art has substantially improved for EDS and AMR parsing, but stayed the same for UCCA. However, as mentioned in §6, remote edge detection for UCCA improved substantially, though it carries only a small weight in terms of overall scores due to the scarcity of remote edges. For EDS, the strongest results were obtained in the MRP 2019 official competition by SUDA– Alibaba (Zhang et al., 2019c). However, in the post-evaluation stage, they were outperformed by the Peking system (Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard"
2020.conll-shared.1,D19-1278,0,0.0153051,"Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets"
2020.conll-shared.1,N18-2020,1,0.822488,"ed on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency frame and, thus, correspond more closely to the n"
2020.conll-shared.1,P19-4007,0,0.0514516,"Missing"
2020.conll-shared.1,2020.acl-main.629,0,0.125349,"Missing"
2020.conll-shared.1,N18-1104,0,0.0512423,"tudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite limited training data in the case of UCCA and DRG. Furthermore, the evaluation sets for most of the frameworks comprise different text types and subject matters—offering some hope of robustness to"
2020.conll-shared.1,N18-1000,0,0.197513,"Missing"
2020.conll-shared.1,K19-2006,0,0.10434,"been included in the CoNLL 2009 Shared Task on Semantic Role Labeling (Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019),"
2020.conll-shared.1,S19-2002,0,0.0495413,"Missing"
2020.conll-shared.1,hajic-etal-2012-announcing,0,0.357504,"Missing"
2020.conll-shared.1,kingsbury-palmer-2002-treebank,0,0.452155,"o distinguish different types of the underlying DRS elements. logy broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 4 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 1. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 4 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
2020.conll-shared.1,J16-4009,1,0.877647,"d (d) increased crossfertilization of parsing approaches (§7). 2 tute ordered graphs. A natural way to visualize a bi-lexical dependency graph is to draw its edges as semicircles in the halfplane above the sentence. An ordered graph is called noncrossing if in such a drawing, the semicircles intersect only at their endpoints (this property is a natural generalization of projectivity as it is known from dependency trees). A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. Increased terminological uniformity and guidance in how to navigate this rich and diverse landscape are among the desirable side-effects of the MRP task series. The following paragraphs provide semi-formal definiti"
2020.conll-shared.1,P17-1104,1,0.878575,".78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRG"
2020.conll-shared.1,P19-1232,0,0.0175512,"Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Compara"
2020.conll-shared.1,P18-1035,1,0.856893,"e invited to develop parsing systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a"
2020.conll-shared.1,W19-6202,0,0.0118155,"sed factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by"
2020.conll-shared.1,S19-2001,1,0.83448,"Missing"
2020.conll-shared.1,2020.findings-emnlp.288,0,0.0119332,"with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general"
2020.conll-shared.1,P19-1450,0,0.182109,"istinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four"
2020.conll-shared.1,P18-1040,0,0.0323045,", summarization, or text generation. Maybe equally importantly, the MRP task design capitalizes on uniformity of representations and evaluation, enabling resource creators and parser developers to more closely (inter)relate representations and parsing approaches across a diverse range of semantic graph frameworks. This facilitates DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also m"
2020.conll-shared.1,W19-1203,0,0.0982514,"he UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets are language-neutral. The mismatch between German tokens and English lemmas of senses must be expected to add additional complexity to"
2020.conll-shared.1,2020.acl-main.607,0,0.0482936,"Missing"
2020.conll-shared.1,K19-2001,1,0.563644,"Missing"
2020.conll-shared.1,P18-1037,0,0.0412388,"tokens and English lemmas of senses must be expected to add additional complexity to German DRG parsing. Direct comparison to non-MRP results is impossible: we are using a new version of AMRbank. Gold-standard tokenization is not provided for any of the frameworks. We use the MRP scorer. However, general trends appear consistent with recent developments. Pretrained embeddings and crosslingual transfer help; but multi-task learning less so. There is yet progress to be made in sharing information between parsers for different frameworks and making better use of their overlap. Prior to MRP 2019, Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. Lyu et al. (2020) recently improved the latent alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on al"
2020.conll-shared.1,K19-2003,1,0.845577,"on marks in the left or right periphery of a normalized anchor. Assuming the string Oh no! as a hypothetical parser input, the following anchorings will all be considered equivalent: {h0 : 6i}, {h0 : 2i, h3 : 6i}, {h0 : 1i, h1 : 6i}, and {h0 : 5i}. 6 Six teams submitted parser outputs to the shared task within the official evaluation period. In addition, we received two submissions after the submission deadline, which we mark as ‘unofficial’. We further include results from an additional ‘reference’ system by one of the task co-organizers, namely EDS outputs from the grammar-based ERG parser (Oepen and Flickinger, 2019). Table 5 presents an overview of the participating systems and the tracks and frameworks they submitted results for. All official systems submitted results for the cross-framework track (across all frameworks), and additionally five of them submitted results to the cross-lingual track as well (where TJU-BLCU did not submit UCCA parser outputs in the cross-lingual track). We note that the shared task explicitly allowed partial submissions, in order to lower the bar for participation (which is no doubt substantial). Two of the teams—ISCAS and TJUBLCU—declined the invitation to submit a system d"
2020.conll-shared.1,P14-5010,0,0.0027367,"ces of ‘raw’ sentence strings and (b) in pre-tokenized, partof-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premium-quality morpho-syntactic dependency analyses were provided to participants, called the MRP 2020 companion parses. These parses were obtained using a prerelease of the ‘future’ UDPipe architecture (Straka, 2018; Straka and Strakov´a, 2020), trained on available gold-standard UD 2.x treebanks, for English augmented with conversions from PTB-style annotations in the WSJ and OntoNotes corpora (Hovy et al., 2006), using the UD-style CoreNLP 4.0 tokenizer (Manning et al., 2014) and jack-knifing where appropriate (to avoid overlap with the texts underlying the MRP semantic graphs). Table 4: Different tuple types per framework. on-line CodaLab infrastructure. Teams were allowed to make repeated submissions, but only the most recent successful upload to CodaLab within the evaluation period was considered for the official, primary ranking of submissions. Task participants were encouraged to process all inputs using the same general parsing system, but—owing to inevitable fuzziness about what constitutes ‘one’ parser—this constraint was not formally enforced. 5 Evaluatio"
2020.conll-shared.1,S15-2153,1,0.842966,"Missing"
2020.conll-shared.1,J93-2004,0,0.0699436,", as fully lexically anchored and wholly unanchored, respectively, leading to the categorization of mixed forms of anchoring as Flavor (1), and allow for the presence of ordered graphs, in principle at least, at all levels of the hierarchy.2 Meaning Representation Frameworks The shared task combines five distinct frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions. This section reviews the frameworks and presents English example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 4 are prewhere unanchored nodes for unexpressed material beyond the surface string can be postulated (Schuster and Manning, 2016). Whether or not these nodes occupy a well-defined position in the otherwise total order of basic UD nodes remains an open questi"
2020.conll-shared.1,S16-1166,0,0.0329712,"context of the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the su"
2020.conll-shared.1,S14-2008,1,0.896688,"Missing"
2020.conll-shared.1,S17-2090,0,0.0310415,"the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although mo"
2020.conll-shared.1,2020.conll-shared.4,0,0.136232,"(Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word"
2020.conll-shared.1,2020.conll-shared.8,0,0.415434,"Missing"
2020.conll-shared.1,P17-1186,0,0.154444,"Missing"
2020.conll-shared.1,2020.lrec-1.497,1,0.867008,"Missing"
2020.conll-shared.1,W19-1204,0,0.0450391,"Missing"
2020.conll-shared.1,W16-6401,0,0.0666192,"Missing"
2020.conll-shared.1,Q18-1043,1,0.868093,"Missing"
2020.conll-shared.1,2020.conll-shared.5,0,0.310999,"impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1"
2020.conll-shared.1,N18-2040,1,0.887249,"Missing"
2020.conll-shared.1,L16-1376,0,0.0817331,"Missing"
2020.conll-shared.1,D17-1129,1,0.843784,"contextualized token embeddings with XLM-R (Conneau et al., 2019) on the encoder side, and then on the decoder side, uses separate attention heads to predict the node labels, identify anchors for nodes, and predict edges between nodes, as well as edge labels. Because the label set for nodes is typically very large, rather than predicting the node labels directly, the PERIN system reduces the search space by predicting ‘relative rules’ that can be used to map surface token strings to node labels in meaning representation graphs, an idea that is similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that d"
2020.conll-shared.1,P19-1454,0,0.0201296,"extualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by the fact that two of the 2020 frameworks (PTG and DR"
2020.conll-shared.1,D18-1263,0,0.0141923,"ng systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction"
2020.conll-shared.1,2020.emnlp-main.196,0,0.450632,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.wmt-1.104,0,0.540185,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.lt4hala-1.20,0,0.0828616,"Missing"
2020.conll-shared.1,2020.conll-shared.3,1,0.64593,"Missing"
2020.conll-shared.1,W15-3502,1,0.865834,"arly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges fr"
2020.conll-shared.1,P19-1009,0,0.173641,"Missing"
2020.conll-shared.1,D19-1392,0,0.119567,"Missing"
2020.conll-shared.1,N18-1063,1,0.899522,"Missing"
2020.conll-shared.1,K19-2014,0,0.104365,"Missing"
2020.conll-shared.1,P18-1016,1,0.879706,"true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient),"
2020.conll-shared.2,W13-0101,0,0.514464,"Missing"
2020.conll-shared.2,hajic-etal-2012-announcing,0,0.215418,"Missing"
2020.conll-shared.2,E17-2039,1,0.896243,"Missing"
2020.conll-shared.2,S19-2001,0,0.0898366,"ve roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning of natural language sentences or texts. Several shared tasks on semantic parsing have been organized, and the target meaning representations of the shared tasks were predominantly encoded as directed labeled graphs:1 Semantic Dependency Graphs (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), and Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). Some of these graphs are presented in Figure 1. Recently, Oepen et al. (2019) packaged several meaning representation graphs in a uniform graph ∗ Part of the work was done while the author was at the University of Groningen. 1 Throughout the paper, we mean a directed labeled graph when simply talking about graphs, unless stated otherwise. 23 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 23–32 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics contrast-01 ARG2 _but_c TENSE pres PERF + R-HNDL act-02 polarity - L-HNDL ne"
2020.conll-shared.2,W19-1201,1,0.899402,"Missing"
2020.conll-shared.2,W13-2322,0,0.316416,"Missing"
2020.conll-shared.2,P18-1040,0,0.365872,"4 CTR CTR CTR CTR a2 a2senate senate bin bin Name Name senatesenate a2 a2 NameName c5 c5 a1 a1 b3 b3 b3 b3 a1 ref ref a1 ref ref con PRE PRE b4 b4 vote.v.01 vote.v.01 con senate.n.01 senate.n.01 a1 rol b5 NEG NEG b5 con AgentAgent a1 rol ref con ??2 a1 a2 ref a1 act.v.01 act.v.01 ??2 a2 a1 ??2 a1 ??2 senate.n.01 senate.n.01 c6 c6 a1 a1 ??2 PRE PRE a2 Agent Agent a2 c7 c7 a1 a1 ?? b4 ref ref b4 b5 b5 a1 NEG NEGact.v.01 act.v.01 c8 c8 2 a1 ??2 ??2 vote.v.01 vote.v.01 senatesen Name Name CTR CTR senate.n.01 senate. PRE NEG NEG act.v. contentcontentact.v.01 (c) L18 is the edge-centric encoding by Liu et al. (2018). B and C are represented as unlabeled nodes with B- and C-labeled incoming edges. R, O and argument positions (A) are encoded as labeled edges. Unlabeled nodes are introduced not only by B and r but also by B and C. (b) The BB∗ encoding largely follows Basile and Bos (2013) and incorporates several additional simplifications. The encoding is node-centric. B and C are encoded as labeled nodes while R, O and argument positions (A) as labeled edges. Only B and r are unlabeled nodes. Figure 3: Contrasting the existing graph representations of DRSs. The graphs encode the sample DRS from Figure 2."
2020.conll-shared.2,W13-2101,1,0.830951,"act.v.01 ??2 a2 a1 ??2 a1 ??2 senate.n.01 senate.n.01 c6 c6 a1 a1 ??2 PRE PRE a2 Agent Agent a2 c7 c7 a1 a1 ?? b4 ref ref b4 b5 b5 a1 NEG NEGact.v.01 act.v.01 c8 c8 2 a1 ??2 ??2 vote.v.01 vote.v.01 senatesen Name Name CTR CTR senate.n.01 senate. PRE NEG NEG act.v. contentcontentact.v.01 (c) L18 is the edge-centric encoding by Liu et al. (2018). B and C are represented as unlabeled nodes with B- and C-labeled incoming edges. R, O and argument positions (A) are encoded as labeled edges. Unlabeled nodes are introduced not only by B and r but also by B and C. (b) The BB∗ encoding largely follows Basile and Bos (2013) and incorporates several additional simplifications. The encoding is node-centric. B and C are encoded as labeled nodes while R, O and argument positions (A) as labeled edges. Only B and r are unlabeled nodes. Figure 3: Contrasting the existing graph representations of DRSs. The graphs encode the sample DRS from Figure 2. For brevity, the tense information is omitted from the DRS. Unlabeled nodes have a gray background. The shapes of nodes are not part of the graphs but simply help with reading to distinguish the types of symbols. The work by Power (1999) doesn’t aim to convert DRSs into grap"
2020.conll-shared.2,S16-1166,0,0.100358,"ing representations is that DRSs are very different from syntactic structures. DRSs have roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning of natural language sentences or texts. Several shared tasks on semantic parsing have been organized, and the target meaning representations of the shared tasks were predominantly encoded as directed labeled graphs:1 Semantic Dependency Graphs (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), and Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). Some of these graphs are presented in Figure 1. Recently, Oepen et al. (2019) packaged several meaning representation graphs in a uniform graph ∗ Part of the work was done while the author was at the University of Groningen. 1 Throughout the paper, we mean a directed labeled graph when simply talking about graphs, unless stated otherwise. 23 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 23–32 c Online, Nov. 19-20, 2020. 2020 Association for"
2020.conll-shared.2,S17-2090,0,0.0817584,"ntations is that DRSs are very different from syntactic structures. DRSs have roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning of natural language sentences or texts. Several shared tasks on semantic parsing have been organized, and the target meaning representations of the shared tasks were predominantly encoded as directed labeled graphs:1 Semantic Dependency Graphs (Oepen et al., 2014, 2015), Abstract Meaning Representation (May, 2016; May and Priyadarshi, 2017), and Universal Conceptual Cognitive Annotation (Hershcovich et al., 2019). Some of these graphs are presented in Figure 1. Recently, Oepen et al. (2019) packaged several meaning representation graphs in a uniform graph ∗ Part of the work was done while the author was at the University of Groningen. 1 Throughout the paper, we mean a directed labeled graph when simply talking about graphs, unless stated otherwise. 23 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 23–32 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics co"
2020.conll-shared.2,W08-2222,1,0.646327,"o-Abn-Ia1 10 4 Cor-Bo-Ab-Ia1 20 Cor-Bo-Abn 0.1 Cor-Bo-Ab 1.0 Cor-Bo-An 2.2 Cor-B!-An 3.5 Cl-Bo-Abn 2.2 Cl-Bo-Ab 2.0 Cl-Bo-An 4.8 Cl-B!-An 6.7 C!-B!-An 4.8 chLSTM↑ "" Table 4: The percentage of approximate (i.e., non-exact) matches w.r.t. the total non-null DRGs. Lower numbers are better as more graph matches corresponding to MCES are found. The total number of DRSs is 885. While converting DRSs into DRGs, 8-number of DRGs become null due to ill-formed DRSs and are excluded during calculating the percentages. Encoding with C• additionally renders C• -number of DRSs untranslatable. parser Boxer (Bos, 2008), which is used in the PMB to pack all annotations layers into DRS boxes. Boxer↓ is Boxer based on the NLP tools of the PMB pipeline6 , on the other hand, Boxer↑ is Boxer employing annotation layers output by MaChAmp (van der Goot et al., 2020). As the names suggest, Boxer↑ is a better model than Boxer↓ . The output DRSs are obtained by parsing the development set (885 documents) of the PMB v3.0.0.7 Evaluation of the models based on the DRSs of the dev set is given in Table 3. DRSs are scored with Counter (van Noord et al., 2018a), the clause matching tool for DRSs in clausal form.8 For MCES-b"
2020.conll-shared.2,L18-1267,1,0.864181,"Missing"
2020.conll-shared.2,Q18-1043,1,0.910083,"Missing"
2020.conll-shared.2,W13-0122,1,0.730526,"d from the Parallel Meaning Bank (PMB, Abzianidze et al., 2017). One such DRS is presented in Figure 2. The DRS signature is given in Table 1. The PMB incorporates several extensions to DRSs. On a micro level, the extensions aim to make DRSs language-neutral by disambiguating non-logical symbols with WordNet (Miller, 1995) synsets and VerbNet (Bonial et al., 2011) roles, where the VerbNet roles are used in combination with neo-Davidsonian event semantics (Parsons, 1990). On a macro level, presuppositions are modeled and explicitly represented following Van der Sandt (1992) and Projective DRT (Venhuizen et al., 2013) while discourse is analyzed following Segmented DRT (Asher and Lascarides, 2003) and flattened by treating discourse relations and DRS operators in a unified way. Due to these extensions, 3 Related Work There have been several approaches to represent DRSs as graphs. These representations are put side-by-side in Figure 3. 3 Note that t2 is in b4 because it has to be out of the scope of negation: there is a time t2 , and it is not the case that at t2 the Senate acts. 25 a2 1 1 a1 a1 c2 a1 c4 ??1 c6 a1 a2 ref v.01 c8 Name Agent PRE house ??2 a1 a1 ??2 bin b1 b1 bin CTR Name NEG content senate.n."
2020.conll-shared.2,2020.conll-shared.1,1,0.908406,"Missing"
2020.conll-shared.2,2020.conll-shared.3,0,0.229774,"Missing"
2020.conll-shared.2,K19-2001,1,0.857613,"Missing"
2020.conll-shared.2,2020.cl-3.3,0,0.171003,"based meaning representations in two aspects. First, DRSs are not inherently graphs. A DRS is more like a formula of predicate logic which is further organized in sub-formulas and governed with additional operations that account for co-reference and presupposition. That’s why DRSs are usually not considered as graphbased meaning representations. For example, DRT was not among the frameworks of the shared task on cross-framework meaning representation parsing (MRP 2019, Oepen et al., 2019) since the meaning representations at the shared task were ˇ all uniformly formatted as graphs. Zabokrtsk´ y et al. (2020) excluded DRSs when surveying sentence meaning representations as they “limit [themselves] to meaning representations whose backbone structure can be described as a graph over words (possibly with added non-lexical nodes) [. . . ]”. The second main contrast between DRSs and several of the graph-based meaning representations is that DRSs are very different from syntactic structures. DRSs have roots in formal semantics, and they are geared to account for negation, quantification, and semantic scope rather than for syntactic Introduction Graphs are a common data structure for representing meaning"
2020.conll-shared.2,S15-2153,1,0.928445,"Missing"
2020.conll-shared.2,S14-2008,1,0.925336,"Missing"
2020.conll-shared.2,oepen-lonning-2006-discriminant,1,0.79677,"Missing"
2020.iwpt-1.3,K18-2002,1,0.872508,"Missing"
2020.iwpt-1.3,N19-1423,0,0.204273,"sults from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated 2018 CoNLL Shared Task on Universal Dependency Parsing (Zeman et al., 2018). 3 Data The"
2020.iwpt-1.3,P18-2077,0,0.450072,"g its scope, is relevant for a large number of applications in natural language processing, and has been the subject of several contrastive research efforts (Morante and Blanco, 2012; Oepen et al., 2017; Fares et al., 2018). In this paper we cast NR as a graph parsing problem. More specifically, we represent negation cues and corresponding scopes as a bi-lexical graph and learn to predict this graph from the tokens. Under this representation, we may apply any dependency graph parser to the task of negation resolution. The specific parsing architecture that we use in this paper extends that of Dozat and Manning (2018). Contributions This work (a) rationally reconstructs the previous state of the art in negation resolution; (b) develops a novel approach to the problem based on general graph parsing techniques; (c) proposes and evaluates different ways of integrating ‘external’ grammatical information; (d) gauges the utility of morpho-syntactic preprocessing at different levels of accuracy; (e) shifts experimental focus (back) to a complete, end-toend perspective on the task; and (f) reflects on un14 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages"
2020.iwpt-1.3,P19-1267,0,0.0119054,"r good performance on the final evaluation set. This notion is further reinforced by the lack of significant difference in performance of our best systems, compared to S HERLOCK. For the S TANFORD -PARIS version of the data, even nearly three points of FN F1 (64.27 vs. 61.42) do not constitue a significant difference. The difficulty to confidently analyse the results is also illustrated by the somewhat erratic performance differences across different settings and runs. The NLP community has recently realized the importance of proper testing in favour of simple comparisons of benchmark scores (Gorman and Bedrick, 2019). This becomes even more pronounced when working with deep learning architectures, where model selection is more complicated (Moss et al., 2019) due to sensitivity to different random seeds. When working with smaller datasets such as ConanDoyle-neg, it is particularly important to thoroughly analyse the results before claiming that a new system improves the state of the art (Dror et al., 2017). Predicted Cues When we task our system to also predict cues, as in *SEM 2012, our best system outperforms Read et al. (2012) and Packard et al. (2014) on most measures. Our neural graph parsing approch"
2020.iwpt-1.3,K17-3002,0,0.0421719,"ping Interpolation constant L2 regularization 200 50 0 0.95 1 · 10−3 5 0.025 3 · 10−9 tion task of the 2017 edition of EPE, S TANFORD PARIS-06 (Schuster et al., 2017), uses enhanced Universal Dependencies (v1) and data from the Penn Treebank (Marcus et al., 1993), the Brown Corpus (Francis and Kuˇcera, 1985) and the GENIA treebank (Tateisi et al., 2005). In contrast to this, the best performing system for the 2018 edition, T URKU NLP (Kanerva et al., 2018), only uses the English training data provided by the co-located UD parsing shared task. Both systems use the parser and hyperparameters of Dozat et al. (2017), the winning submission of the CoNLL 2017 Shared Task on parsing Universal Dependencies. In the overview paper for the 2018 EPE shared task (Fares et al., 2018), the organizers report that the version of the S HERLOCK negation system that was used for EPE 2017 had a deficiency that could leak gold-standard scope and event annotations into system predictions, leading to potentially inflated scores.2 The EPE 2018 version of S HERLOCK corrected this problem and added automated hyperparameter tuning, which Fares et al. (2018) suggest largely offset the negative effect on overall scores from the b"
2020.iwpt-1.3,Q17-1033,0,0.012549,"at erratic performance differences across different settings and runs. The NLP community has recently realized the importance of proper testing in favour of simple comparisons of benchmark scores (Gorman and Bedrick, 2019). This becomes even more pronounced when working with deep learning architectures, where model selection is more complicated (Moss et al., 2019) due to sensitivity to different random seeds. When working with smaller datasets such as ConanDoyle-neg, it is particularly important to thoroughly analyse the results before claiming that a new system improves the state of the art (Dror et al., 2017). Predicted Cues When we task our system to also predict cues, as in *SEM 2012, our best system outperforms Read et al. (2012) and Packard et al. (2014) on most measures. Our neural graph parsing approch is clearly better at identifying the relevant scope tokens (ST), due to its pairwise classification approach, respectively gaining 5.32 and 2.29 points in FN F1 . This generally also results in better performance for matching complete scopes (SM). The system does however struggle with telling events and regular scopes apart, and is clearly outperformed by Read et al. (2012) on that measure (6."
2020.iwpt-1.3,2020.cl-1.5,0,0.167676,"Missing"
2020.iwpt-1.3,K18-2013,0,0.0849187,"Missing"
2020.iwpt-1.3,N13-1070,0,0.145481,"{ S O CUE N } E N N N N SO Figure 1: An example of how overlapping ConanDoyle-neg annotations are converted to flat sequences of labels in S HERLOCK. In this example, an in-scope token is labelled with N, a cue with CUE, a negated event with E, a negation stop with S, and an out-of-scope token with O. Illustration taken from Lapponi et al. (2017). 3.1 in-scope or out-of-scope using a conditional random field (CRF). Another CRF further classifies scope tokens as events, and a heuristic is applied to distribute scope tokens to their respective cues. The S HERLOCK system was subsequently used by Elming et al. (2013) to evaluate various dependency conversions, and similarly served as one of three reference ‘downstream’ applications in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used"
2020.iwpt-1.3,2020.lrec-1.704,0,0.139041,"gation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated 2018 CoNLL Shared Task on Universal Dependency Parsing (Zeman et al., 2018). 3 Data The negation data of *SEM 2012 consists of selected Sherlock Holmes stories from the works of Arthur Conan Doyle, and contains 3,644 sentences in the training set, 787 sentences in the development set, and 1,089 sentences in the evaluation set. The corpus annot"
2020.iwpt-1.3,P16-1047,0,0.0549824,"is applied to distribute scope tokens to their respective cues. The S HERLOCK system was subsequently used by Elming et al. (2013) to evaluate various dependency conversions, and similarly served as one of three reference ‘downstream’ applications in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) succe"
2020.iwpt-1.3,S12-1035,0,0.813225,"cribes the specific NR task that we address in this paper. In Section 4 we present our new encoding of negations and our parsing model, followed by the description of our experiments and results in Section 5. We discuss these results in Section 6 and summarize our findings in Section 7. 2 Related Work While there exist a variety of datasets that annotate negation (Jim´enez-Zafra et al., 2020), the BioScope (Szarvas et al., 2008) and Conan Doyle datasets (ConanDoyle-neg; Morante and Daelemans, 2012) are most commonly used for evaluation. The latter was created for the shared task at *SEM 2012 (Morante and Blanco, 2012), where competing systems needed to predict both negation cues (linguistic expressions of negation) and their corresponding scopes, i.e. the part of the utterance being negated. Cues can be simple negation markers (such as not or without), but may also consist of multiple words (i.e. neither . . . nor), or be mere affixes (i.e. infrequent or clueless). In contrast to other datasets, ConanDoyle-neg also annotates negated events that are part of the scopes. The analysis of negation is divided into two related sub-tasks, cue detection and scope resolution. While cue detection is mostly dependent"
2020.iwpt-1.3,morante-daelemans-2012-conandoyle,0,0.620195,"Missing"
2020.iwpt-1.3,W19-6202,1,0.752559,"dependents (D), and the nodes themselves (S), weighted by ~ l: layer-specific weight matrices W   l−1  ~ l = ReLU A ~W ~ l +A ~ >W ~ l +W ~l X ~ X H D S ~~ score(~hi , d~j ) = ~h> i U dj ~ corresponds to The inner dimension of the tensor U the number of negation graph labels plus a special N ONE label indicating the absence of an arc, and thus predicts arcs and labels jointly. 4.3 Adding External Graph Features Similarly to S HERLOCK, our neural model is able to process external morpho-syntactic or surfacesemantic analyses of the input sentence in the form of dependency graphs. Inspired by Kurtz et al. (2019), we extend the contextualized embeddings that are computed by our parser by information derived from the external graph. For this we use three approaches: (i) attaching the sum of heads; (ii) scaled attention on the heads; and (iii) Graph Convolutional Networks (Kipf and Welling, 2017). In the following, we view the external graph in ~ and the terms of its n × n adjacency matrix A ~ contextualized embeddings as an n × d matrix C. When applying the next layer l, each node is up~ l−1 from dated with respect to its representation X the previous layer, thus indirectly taking into account grandpar"
2020.iwpt-1.3,P19-1281,0,0.0123128,"ems, compared to S HERLOCK. For the S TANFORD -PARIS version of the data, even nearly three points of FN F1 (64.27 vs. 61.42) do not constitue a significant difference. The difficulty to confidently analyse the results is also illustrated by the somewhat erratic performance differences across different settings and runs. The NLP community has recently realized the importance of proper testing in favour of simple comparisons of benchmark scores (Gorman and Bedrick, 2019). This becomes even more pronounced when working with deep learning architectures, where model selection is more complicated (Moss et al., 2019) due to sensitivity to different random seeds. When working with smaller datasets such as ConanDoyle-neg, it is particularly important to thoroughly analyse the results before claiming that a new system improves the state of the art (Dror et al., 2017). Predicted Cues When we task our system to also predict cues, as in *SEM 2012, our best system outperforms Read et al. (2012) and Packard et al. (2014) on most measures. Our neural graph parsing approch is clearly better at identifying the relevant scope tokens (ST), due to its pairwise classification approach, respectively gaining 5.32 and 2.29"
2020.iwpt-1.3,S12-1042,0,0.564707,"ch as not or without), but may also consist of multiple words (i.e. neither . . . nor), or be mere affixes (i.e. infrequent or clueless). In contrast to other datasets, ConanDoyle-neg also annotates negated events that are part of the scopes. The analysis of negation is divided into two related sub-tasks, cue detection and scope resolution. While cue detection is mostly dependent on lexical or morphological features, relating cues to scopes is a structured prediction problem and will likely benefit from an analysis of morpho-syntactic or surface-semantic properties. The UiO2 system S HERLOCK (Lapponi et al., 2012), the winner of the open track of the *SEM 2012 shared task, uses morpho-syntactic parts of speech and syntactic dependencies to classify tokens as either Introduction Negation resolution (NR), the task of detecting negation and determining its scope, is relevant for a large number of applications in natural language processing, and has been the subject of several contrastive research efforts (Morante and Blanco, 2012; Oepen et al., 2017; Fares et al., 2018). In this paper we cast NR as a graph parsing problem. More specifically, we represent negation cues and corresponding scopes as a bi-lexi"
2020.iwpt-1.3,P14-1007,1,0.907349,"et al. (2017). 3.1 in-scope or out-of-scope using a conditional random field (CRF). Another CRF further classifies scope tokens as events, and a heuristic is applied to distribute scope tokens to their respective cues. The S HERLOCK system was subsequently used by Elming et al. (2013) to evaluate various dependency conversions, and similarly served as one of three reference ‘downstream’ applications in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using B"
2020.iwpt-1.3,P10-1052,0,0.04826,"Missing"
2020.iwpt-1.3,D14-1162,0,0.0830174,"Missing"
2020.iwpt-1.3,L18-1547,0,0.124555,"tup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated 2018 CoNLL Shared Task on Universal Dependency Parsing (Zeman et al., 2018). 3 Data The negation data of *SEM 2012 consists of selected Sherlock Holmes stories from the works of Arthur C"
2020.iwpt-1.3,N18-1202,0,0.0368131,"*SEM 2012 shared task on negation resolution, clearly outperforming each previously best system, albeit none of our results is statistically significant. We believe that our approach can be used to restructure other tasks as dependency-style graphs in similar fashion, and thus reuse existing systems as general purpose tools. Recasting the negation resolution task as a graph-parsing problem allows us to straightforwardly use a variety of such tools. With most of these now using neural networks, we can extend them to employ massive pre-trained models such as BERT (Devlin et al., 2019) or ELMo (Peters et al., 2018). This would allow us to leverage their general power into more specific tasks that have only limited data available. Significant Learning While the boxplots in Figures 3 and 4 show the same general trends as our particular systems in Tables 2 and 3, they also illustrate the considerable variance of performance between runs. Choosing the final system with regards to performance on the development sets may lead to state-of-the-art performance on the evaluation sets—this is the case for our best performing system using gold cues and additional syntax processed by a GCN, which performs more than"
2020.iwpt-1.3,D17-1159,0,0.0304333,"and dense. LSTM, and 100-dimensional GloVe (Pennington et al., 2014) embeddings. Based on the contextdependent embeddings, two feedforward neural networks (FNN) create specialized representations of each word as a potential head and dependent: ~hi = FNNh (~ci ) d~i = FNNd (~ci ) These new representations are then scored via a ~: bilinear model with weight tensor U Graph Convolutional Networks Graph Convolutional Networks (GCNs; Kipf and Welling, 2017) generalize convolutional networks to graph-structured data. While they were developed with graphs much larger than our negation graphs in mind, Marcheggiani and Titov (2017) showed their use~0 = C ~ fulness for semantic role labelling. With X at the first level, we compute, for each level l > 0, a combined representation of heads (H), dependents (D), and the nodes themselves (S), weighted by ~ l: layer-specific weight matrices W   l−1  ~ l = ReLU A ~W ~ l +A ~ >W ~ l +W ~l X ~ X H D S ~~ score(~hi , d~j ) = ~h> i U dj ~ corresponds to The inner dimension of the tensor U the number of negation graph labels plus a special N ONE label indicating the absence of an arc, and thus predicts arcs and labels jointly. 4.3 Adding External Graph Features Similarly to S HER"
2020.iwpt-1.3,S12-1041,1,0.867849,"9 73.10 82.37 85.40 67.02 – 57.63 – S TANFORD -PARIS no syntax sumoh scatt gcn 91.76 91.62 91.51 93.26 73.76 74.10 76.16 73.11 86.57 85.63 84.72 84.22 71.96 69.43 70.00 72.40 65.69 63.39 66.42 65.19 90.98◦ 91.05 90.98 92.68∗ 75.81 68.64 72.25 73.83 87.69 86.66 86.09 86.89 60.66 59.74 61.21 63.69 59.40 52.58 57.64 58.07 T URKU NLP no syntax sumoh scatt gcn 92.98 92.49 92.54 91.02 76.22∗ 73.45 76.60 72.46◦ 85.61 85.03 85.34 84.90 71.11 75.35 71.03 73.49 66.42 62.31 64.15 63.15 90.71 90.66 90.13 90.98 72.09 71.73 71.85 71.43 86.92 87.91 87.06 86.57 59.88 60.06 57.23 63.40 55.18 53.19 52.08 54.54 Read et al. (2012) Packard et al. (2014) Table 3: Results of our NR parser on the S TANFORD -PARIS and T URKU NLP versions of the ConanDoyle-neg development and evaluation sets when cues are predicted. The numerically best results are shown in bold. We test for significant differences between our gcn with syntax models for S TANFORD -PARIS and T URKU NLP and respective models using no additional inputs. Only the ∗-marked measures are significantly different from their ◦-marked counterparts. 19 with the winning system of the *SEM 2012 shared task by Read et al. (2012), and also with the MRS Crawler of Packard et"
2020.iwpt-1.3,J93-2004,0,0.0727284,"Missing"
2020.iwpt-1.3,D19-6221,0,0.0275757,"in the 2017 Extrinsic Parser Evaluation initiative (EPE; Oepen et al., 2017). The best results from this evaluation define the state of the art in NR. Deviating from the original *SEM 2012 setup, Packard et al. (2014) simplified the task to only evaluate the performance on finding scope tokens, assuming gold-standard information about negation cues. Fancellu et al. (2016, 2018) continued this trend, additionally treating each negation instance separately, and successfully used BiLSTM (bidirectional Long Short-Term Memory recurrent neural networks; Hochreiter and Schmidhuber, 1997). Recently, Sergeeva et al. (2019) used pre-trained transformers (Vaswani et al., 2017), namely BERT (Devlin et al., 2019), to further improve performance, albeit on a derivative of the original dataset (Liu et al., 2018). Using BERT in a two-stage sequence-labelling approach on the original ConanDoyle-neg corpus and other relevant negation corpora, Khandelwal and Sawant (2020) successfully improved previous results by a considerable margin. The 2018 follow-up to the EPE shared task (Fares et al., 2018) again used S HER LOCK to evaluate parsing performance, this time restricting itself to participating systems in the colocated"
2020.iwpt-1.3,W08-0606,0,0.216358,"n Doyle benchmark dataset, including a new top result for our best model. 1 Paper Structure In the following Section 2, we review selected related work on negation resolution. Section 3 describes the specific NR task that we address in this paper. In Section 4 we present our new encoding of negations and our parsing model, followed by the description of our experiments and results in Section 5. We discuss these results in Section 6 and summarize our findings in Section 7. 2 Related Work While there exist a variety of datasets that annotate negation (Jim´enez-Zafra et al., 2020), the BioScope (Szarvas et al., 2008) and Conan Doyle datasets (ConanDoyle-neg; Morante and Daelemans, 2012) are most commonly used for evaluation. The latter was created for the shared task at *SEM 2012 (Morante and Blanco, 2012), where competing systems needed to predict both negation cues (linguistic expressions of negation) and their corresponding scopes, i.e. the part of the utterance being negated. Cues can be simple negation markers (such as not or without), but may also consist of multiple words (i.e. neither . . . nor), or be mere affixes (i.e. infrequent or clueless). In contrast to other datasets, ConanDoyle-neg also a"
2020.iwpt-1.3,I05-2038,0,0.0495751,"Missing"
2020.iwpt-1.3,J12-2005,1,0.69606,"relationship between tokens and their cue(s), while being able to easily differentiate between regular scopes and events. An example for a negation graph is shown in Figure 2. We adopt a convention from dependency parsing and visualize negation graphs with their nodes laid out as the words of the respective sentence, and their arcs drawn above the nodes. When ~c1 , . . . , ~cn = BiLSTM(w ~ 1, . . . , w ~ n) 1 One main focus of the EPE task was the downstream evaluation of different syntactic representations; but the subtask of cue detection is relatively insensitive to grammatical structure (Velldal et al., 2012). We augment the input word embeddings w ~ i with additional part-of-speech tag and lemma embeddings, embeddings created by a character-based 16 Here, d is the size of the contextualized embeddings. In our case, where we merely want to ex~ is tract features from a given graph, the matrix A known and sparse; but the same scaled attention model could also be used in a multi-task setup to jointly learn to parse syntactico-semantic graphs ~ would be learned and negations, in which case A and dense. LSTM, and 100-dimensional GloVe (Pennington et al., 2014) embeddings. Based on the contextdependent"
2020.iwpt-1.3,K18-2001,0,0.0551834,"Missing"
2020.lrec-1.234,W13-2322,0,0.0891257,"tically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of speech, and (for verbs only, in the PSD case) frame or sense identifiers; jointly, these properties define a semantic predicate. Edges represent semantic argument roles: DM mostly uses overtly order-coded labels, e.g. ARG1, ARG2, etc. Abstractly similar, PSD labels like ACT(or), PAT(ient), or ADDR(essee) indicate ‘participant’ positions in an underlying valency frame. Regarding lexical anchoring, on the opposite end of the range of frameworks in the MRP 2019 shared task is Abstract Meaning Representation (AMR; Banarescu et al. (2013)), which by design does not spell out how nodes relate to sub-strings of the underlying parser input; Figure 2 shows the same example sentence in AMR. Without an explicit relation to the surface string, several of the ‘querying’ possible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 2: Sample unanchored Abstract Meaning Representation (AMR) graph for the same sentence as in Figure 1. 1903 dimensions of McDonald and Nivre (2011) will need to eith"
2020.lrec-1.234,W06-2920,0,0.111386,"tly top-performing semantic parsers. Finally, § 5. concludes the paper and discusses avenues for future research. 2. Background The following paragraphs establish relevant methodological and technological context for our work, out of necessity summarizing prior efforts in rather broad strokes. A Tale of Two Parsers One inspiration for this study is the contrastive error analysis of graph-based vs. transitionbased syntactic dependency parsers carried out by McDonald and Nivre (2007) and McDonald and Nivre (2011). Based on data from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006), they analyzed the performance of the two parser types in relation to a number of structural factors, such as sentence length, dependency length, and tree depth, as well as linguistic categories, notably parts of speech and dependency types. The analysis showed that, although the best graphbased and transition-based syntactic dependency parsers at the time achieved very similar accuracy on average, they had quite distinctive error profiles. Moreover, these differences could be explained by inherent strengths and weaknesses of the two algorithmic approaches. Thus, for exam1902 top ARG2 BV ARG1"
2020.lrec-1.234,K19-2007,0,0.338406,"these, the first two abstractly parallel the two families represented in the studies by McDonald and Nivre (2011), whereas composition-based parsing approaches are not found in syntactic parsing. We consider participating systems in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the transition-based systems in MRP 2019, the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the"
2020.lrec-1.234,N19-1423,0,0.087353,"l, but degraded more because of error propagation in greedy decoding. Conversely, graph-based parsers showed a more graceful degradation thanks to global optimization and exact decoding, but had a disadvantage for local structures because of a more restricted feature model. More recently, Kulmizev et al. (2019) replicated this analysis for neural graph-based and transition-based parsers and showed that, although the distinct error profiles are still discernible, the differences are now much smaller and are further reduced by the use of deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). MRP 2019 The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks (Oepen et al., 2019). For the first time, this task combined formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. The training and evaluation data for the task comprised five distinct approaches— which all encode core predicate–argument structure, among other things—to the representation of sentence meaning in the form of directed graphs, packaged in a u"
2020.lrec-1.234,K19-2006,0,0.286659,"s in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the transition-based systems in MRP 2019, the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM"
2020.lrec-1.234,W17-6810,0,0.0189096,"erforming overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM ‘feature extractor’ architecture of Kiperwasser and Goldberg (2016) is employed for predicting dependency trees, with input representations combining ELMo 2 We use framework-specific performance on PSD and DM, rather than the overall ranking across frameworks within the shared task, as the selection criterion, given that this pilot study is focused on comparing and analysing the results of parsing into these p"
2020.lrec-1.234,hajic-etal-2012-announcing,0,0.0969522,"Missing"
2020.lrec-1.234,W12-3602,1,0.781193,"niform abstract structure and serialization. This task design seeks to enable cross-framework comparison of different parsing approaches and to advance learning from complementary knowledge sources (e.g. via parameter sharing). The MRP 2019 competition received submissions from eighteen teams, and there will be a follow-up shared task, again hosted by CoNLL, in 2020. Figure 1 shows two example graphs for one sentence from the venerable Wall Street Journal (WSJ) corpus in the two bi-lexical MRP frameworks (of five total), DELPHIN MRS Bi-Lexical Dependencies (DM) of Oepen and Lønning (2006) and Ivanova et al. (2012), and Prague Semantic Dependencies (PSD) by Hajiˇc et al. (2012) and Miyao et al. (2014). The DM and PSD frameworks are bi-lexical in the MRP collection, characterized by a one-toone relation between graph nodes and surface tokens. But even within this limiting assumption, which makes these graphs formally somewhat similar to standard syntactic dependency trees, the examples in Figure 1 exhibit all the nontree properties sketched in § 1. above (reentrancies, multiple roots, and semantically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of speech, and (for verbs only,"
2020.lrec-1.234,Q16-1023,0,0.0230753,"1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM ‘feature extractor’ architecture of Kiperwasser and Goldberg (2016) is employed for predicting dependency trees, with input representations combining ELMo 2 We use framework-specific performance on PSD and DM, rather than the overall ranking across frameworks within the shared task, as the selection criterion, given that this pilot study is focused on comparing and analysing the results of parsing into these particular frameworks. 1905 Figure 4: Overall MRP precision, recall, and F1 by sentence length for DM (left) and PSD (right). (Peters et al., 2018), and BERT (Devlin et al., 2019) contextualised word embeddings. Additionally, a decomposition step into sub"
2020.lrec-1.234,P19-4002,1,0.844548,"eworks) their evaluation data is publicly available (Oepen et al., 2016). All statistics in this section are against the standard 3,359-sentence PSD and DM test set, comprising gold-standard graphs drawn from the WSJ and Brown corpora. Overall and component-wise MRP evaluation scores were computed using an instrumented version of the official scorer, the mtool Swiss Army knife of meaning representation.1 4.2. Parsing Systems Our choice of models for contrastive evaluation was motivated by the characterisation of systems into three broad families of approaches, as presented, amongst others, by Koller et al. (2019) and Oepen et al. (2019): transition-, factorisation-, and composition-based parsers. Of these, the first two abstractly parallel the two families represented in the studies by McDonald and Nivre (2011), whereas composition-based parsing approaches are not found in syntactic parsing. We consider participating systems in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the t"
2020.lrec-1.234,J16-4009,1,0.939361,"Missing"
2020.lrec-1.234,D19-1277,1,0.894859,"Missing"
2020.lrec-1.234,K19-2004,0,0.0119447,"omposition-based parsing approaches are not found in syntactic parsing. We consider participating systems in the MRP 2019 competition, and, within each family of approaches, choose the top-performing systems for the PSD 1 See https://github.com/cfmrp/mtool for details. Figure 3: Distribution of sentences by length (node count), binned to ten aggregates. and DM frameworks.2 Among the transition-based systems in MRP 2019, the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smalle"
2020.lrec-1.234,P19-1450,0,0.120028,"the best-performing parser is the HIT-SCIR parser (Che et al., 2019), which is also the top-performing overall parser; in the factorisation-based family, the SJTU-NICT system (Li et al., 2019) performs best on DM; and among the composition-based submissions, the Saarland system (Donatelli et al., 2019) obtains the best PSD results. Table 1 shows the absolute output quality (in terms of MRP precision, recall, and F1 ) and the rankings of these systems on the PSD evaluation data, reproducing the official shared task results presented by Oepen et al. (2019). The Saarland parser, an extension of Lindemann et al. (2019), uses a compositional approach, employing the Apply–Modify Algebra of Groschwitz et al. (2017) to build semantic graphs through highly constrained combinations of smaller graph fragments. A BiLSTM sequence labeling model is used for semantic tagging of word tokens, and the BiLSTM ‘feature extractor’ architecture of Kiperwasser and Goldberg (2016) is employed for predicting dependency trees, with input representations combining ELMo 2 We use framework-specific performance on PSD and DM, rather than the overall ranking across frameworks within the shared task, as the selection criterion, given"
2020.lrec-1.234,D07-1013,1,0.852393,"structures make the parsing task much more complex—often moving from techniques with polynomial worst-case complexity to problems that are in principle NP-hard. Among other things, meaning representations transcend syntactic trees in allowing nodes with in-degree greater than one (‘reentrancies’), multiple roots, and ignoring semantically ‘vacuous’ parts of the parser input. Besides greatly increased modeling and algorithmic complexity, meaning representation parsing also poses its own set of methodological challenges for parser evaluation and diagnostics. The contrastive studies initiated by McDonald and Nivre (2007) and McDonald and Nivre (2011) have been influential in comparing the performance of two core types of approaches to syntactic dependency parsing, i.e. different families of parsing approaches. In this work, we investigate to what degree these techniques can be transferred to meaning representation parsing, and how they can be adapted and extended to reflect the formal and linguistic † We acknowledge and thank (Zhang and Clark, 2008) for inspiring our title. differences in the nature of target representations. We develop the blueprint of a general framework for quantitative diagnostic evaluati"
2020.lrec-1.234,J11-1007,1,0.442505,"sk much more complex—often moving from techniques with polynomial worst-case complexity to problems that are in principle NP-hard. Among other things, meaning representations transcend syntactic trees in allowing nodes with in-degree greater than one (‘reentrancies’), multiple roots, and ignoring semantically ‘vacuous’ parts of the parser input. Besides greatly increased modeling and algorithmic complexity, meaning representation parsing also poses its own set of methodological challenges for parser evaluation and diagnostics. The contrastive studies initiated by McDonald and Nivre (2007) and McDonald and Nivre (2011) have been influential in comparing the performance of two core types of approaches to syntactic dependency parsing, i.e. different families of parsing approaches. In this work, we investigate to what degree these techniques can be transferred to meaning representation parsing, and how they can be adapted and extended to reflect the formal and linguistic † We acknowledge and thank (Zhang and Clark, 2008) for inspiring our title. differences in the nature of target representations. We develop the blueprint of a general framework for quantitative diagnostic evaluation and experimentally seek to"
2020.lrec-1.234,S14-2056,1,0.833406,"ork comparison of different parsing approaches and to advance learning from complementary knowledge sources (e.g. via parameter sharing). The MRP 2019 competition received submissions from eighteen teams, and there will be a follow-up shared task, again hosted by CoNLL, in 2020. Figure 1 shows two example graphs for one sentence from the venerable Wall Street Journal (WSJ) corpus in the two bi-lexical MRP frameworks (of five total), DELPHIN MRS Bi-Lexical Dependencies (DM) of Oepen and Lønning (2006) and Ivanova et al. (2012), and Prague Semantic Dependencies (PSD) by Hajiˇc et al. (2012) and Miyao et al. (2014). The DM and PSD frameworks are bi-lexical in the MRP collection, characterized by a one-toone relation between graph nodes and surface tokens. But even within this limiting assumption, which makes these graphs formally somewhat similar to standard syntactic dependency trees, the examples in Figure 1 exhibit all the nontree properties sketched in § 1. above (reentrancies, multiple roots, and semantically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of speech, and (for verbs only, in the PSD case) frame or sense identifiers; jointly, these properties define a semanti"
2020.lrec-1.234,oepen-lonning-2006-discriminant,1,0.643309,"ected graphs, packaged in a uniform abstract structure and serialization. This task design seeks to enable cross-framework comparison of different parsing approaches and to advance learning from complementary knowledge sources (e.g. via parameter sharing). The MRP 2019 competition received submissions from eighteen teams, and there will be a follow-up shared task, again hosted by CoNLL, in 2020. Figure 1 shows two example graphs for one sentence from the venerable Wall Street Journal (WSJ) corpus in the two bi-lexical MRP frameworks (of five total), DELPHIN MRS Bi-Lexical Dependencies (DM) of Oepen and Lønning (2006) and Ivanova et al. (2012), and Prague Semantic Dependencies (PSD) by Hajiˇc et al. (2012) and Miyao et al. (2014). The DM and PSD frameworks are bi-lexical in the MRP collection, characterized by a one-toone relation between graph nodes and surface tokens. But even within this limiting assumption, which makes these graphs formally somewhat similar to standard syntactic dependency trees, the examples in Figure 1 exhibit all the nontree properties sketched in § 1. above (reentrancies, multiple roots, and semantically vacuous surface tokens). DM and PSD nodes are labeled with lemmas, parts of sp"
2020.lrec-1.234,S14-2008,1,0.942226,"Missing"
2020.lrec-1.234,L16-1630,1,0.874805,"Missing"
2020.lrec-1.234,K19-2001,1,0.908652,"Missing"
2020.lrec-1.234,N18-1202,0,0.219124,"a richer feature model, but degraded more because of error propagation in greedy decoding. Conversely, graph-based parsers showed a more graceful degradation thanks to global optimization and exact decoding, but had a disadvantage for local structures because of a more restricted feature model. More recently, Kulmizev et al. (2019) replicated this analysis for neural graph-based and transition-based parsers and showed that, although the distinct error profiles are still discernible, the differences are now much smaller and are further reduced by the use of deep contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). MRP 2019 The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks (Oepen et al., 2019). For the first time, this task combined formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. The training and evaluation data for the task comprised five distinct approaches— which all encode core predicate–argument structure, among other things—to the representation of sentence meaning in the form of directed g"
2020.lrec-1.234,D08-1059,0,0.0609159,"meaning representation parsing also poses its own set of methodological challenges for parser evaluation and diagnostics. The contrastive studies initiated by McDonald and Nivre (2007) and McDonald and Nivre (2011) have been influential in comparing the performance of two core types of approaches to syntactic dependency parsing, i.e. different families of parsing approaches. In this work, we investigate to what degree these techniques can be transferred to meaning representation parsing, and how they can be adapted and extended to reflect the formal and linguistic † We acknowledge and thank (Zhang and Clark, 2008) for inspiring our title. differences in the nature of target representations. We develop the blueprint of a general framework for quantitative diagnostic evaluation and experimentally seek to validate this proposal through a small-scale pilot study. The remainder of the paper is structured as follows: In § 2., we present the relevant background, including previous studies in syntactic parsing that provide our point of departure and the 2019 shared task on meaning representation parsing. § 3. gives a review of established dimensions of contrastive diagnostic evaluation for syntactic dependency"
2021.acl-long.263,P15-1040,0,0.0606129,"Missing"
2021.acl-long.263,2020.acl-main.421,0,0.0150168,"1 the gains are more limited (3.3 pp/ 3.8 pp) and similarly for NSF1 and SF1 (3.6 pp/ 3.9 pp). The gains are 3394 NoReCFine MultiBEU MultiBCA MPQA DSUnis 57.0 (1.5) 75.7 (0.8) 71.7 (2.4) 38.5 (1.4) 44.5 (2.4) Table 6: Polarity F1 scores (unweighted and weighted) of models augmented with mBERT on the head-final setup. We report average and standard deviation over 5 runs. largest for the English datasets (MPQA, DSUnis ) followed by NoReCFine , and finally MultiBCA and MultiBEU . This corroborates the bias towards English and similar languages that has been found in multilingual language models (Artetxe et al., 2020; Conneau et al., 2020) and motivates the need for language-specific contextualized embeddings. 7.4 Analysis of polarity predictions Acknowledgements This work has been carried out as part of the SANT project (Sentiment Analysis for Norwegian Text), funded by the Research Council of Norway (grant number 270908). The computations were performed on resources provided by UNINETT Sigma2 - the National Infrastructure for High Performance Computing and Data Storage in Norway. References In this section we zoom in on polarity, in order to quantify how well models perform at predicting only polarity."
2021.acl-long.263,L18-1104,1,0.902863,"Missing"
2021.acl-long.263,D12-1091,0,0.015329,"entations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Appendix) and run all of our models five times with different random seeds and report the mean (standard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference between the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3 shows the results for all datasets. On NoReCFine , the baselines IMN, RACL, and RACL-BERT perform well at extracting targets (35.9, 45.6, and 47.2 F1 , respectively) and expressions (48.7/55.4/56.3), but struggle with the full targeted sentiment task (18.0/20.1/30.3). The graphbased models extract targets better (50.1/54.8) and have comparable scores for expressions (54.4/55.5). The holder extraction scores have a similar range (51.1/60.4). These patterns hold throughout the other datasets, where th"
2021.acl-long.263,W17-0237,1,0.844981,"ty (holder, target, expression, polarity). A true positive is defined as an exact match at graph-level, weighting the overlap in predicted and gold spans for each element, averaged across all three spans. For precision we weight the number of correctly predicted tokens divided by the total number of predicted tokens (for recall, we divide instead by the number of gold tokens). We allow for empty holders and targets. 3392 6 Experiments All sentiment graph models use token-level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository8 (Fares et al., 2017). We train all models for 100 epochs and keep the model that performs best regarding LF1 on the dev set (Targeted F1 for the baselines). We use default hyperparameters from Kurtz et al. (2020) (see Appendix) and run all of our models five times with different random seeds and report the mean (standard deviation shown as well in Table 8 in the Appendix). We calculate statistical difference between the best and second best models through a bootstrap with replacement test (Berg-Kirkpatrick et al., 2012). As there are 5 runs, we require that 3 of 5 be statistically significant at p < 0.05. Table 3"
2021.acl-long.263,P19-1048,0,0.110331,"s have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annotation process or any inter-annotator agreement. He et al. (2019) make use of this data and propose a multi-layer CNN (IMN) to c"
2021.acl-long.263,P11-1016,0,0.0618142,"entiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction ("
2021.acl-long.263,P16-1087,0,0.162601,"between these elements, and v) assigning polarity. Previous work on information extraction has used pipeline methods which first extract the holders, targets, and expressions (tasks i - iii) and subsequently predict their relations (task iv), mostly on the MPQA dataset (Wiebe et al., 2005). CRFs and a number of external resources (sentiment lexicons, dependency parsers, named-entity taggers) (Choi et al., 2006; Yang and Cardie, 2012) are strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and mult"
2021.acl-long.263,J16-4009,1,0.787534,"nes, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representatio"
2021.acl-long.263,2020.iwpt-1.3,1,0.851073,"rall resolution of sentiment, or do not take into account the inter-dependencies of the various sub-tasks. As such, we propose a unified approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations. Moreover, we cast sentiment analysis as a dependency graph parsing problem, where the sentiment expression is the root node, and the other elements have arcs which model the relationships between them. This methodology also enables us to take advantage of recent improvements in semantic dependency parsing (Dozat and Manning, 2018; Oepen et al., 2020; Kurtz et al., 2020) to efficiently learn a sentiment graph parser. This perspective also allows us to unify a number of approaches, including targeted, and opinion tuple mining. We aim to answer RQ1: whether graph-based approaches to structured sentiment outperform state-of-the-art sequence labeling approaches, and RQ2: how to best encode structured sentiment as parsing graphs. We perform experiments on five standard datasets in four languages (English, Norwegian, Basque, Catalan) and show that graph-based approaches outperform state-ofthe-art baselines on all datasets on several standard metrics, as well as our"
2021.acl-long.263,S12-1042,1,0.750098,"ationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. However, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to pre"
2021.acl-long.263,D19-5505,0,0.0118353,"(Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016) augment the ABSA datasets with sentiment expressions, but provide no details on the annotation process or any inter-annotator agreement. He et al. (201"
2021.acl-long.263,P09-1113,0,0.117439,"s, generally only include sentiment-bearing words (not phrases), and do not specify the relationship between target and expression. Finally, the recently proposed aspect sentiment triplet extraction (Peng et al., 2019; ?) attempts to extract targets, expressions and their polarity. However, the datasets used are unlikely to be adequate, as they augment available targeted datasets, but do not report annotation guidelines, procedure, or inter-annotator agreement. Graph parsing: Syntactic dependency graphs are regularly used in applications, supplying them with necessary grammatical information (Mintz et al., 2009; Cui et al., 2005; Bj¨orne et al., 2009; Johansson and Moschitti, 2012; Lapponi et al., 2012). The dependency graph structures used in these systems are predominantly restricted to trees. While trees are sufficient to encode syntactic dependencies, they are not expressive enough to handle meaning representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented"
2021.acl-long.263,D13-1171,0,0.0551436,"Missing"
2021.acl-long.263,S12-1035,0,0.0332557,"representations, that require nodes to have multiple incoming arcs, or having no incoming arcs at all (Kuhlmann and Oepen, 2016). While much of the early research on parsing these new structures (Oepen et al., 2014, 2015) focused on specialized decoding algorithms, Dozat and Manning (2018) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. 3 Datasets are shown in Table 1. The largest available structured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structu"
2021.acl-long.263,morante-daelemans-2012-conandoyle,0,0.0525443,"Missing"
2021.acl-long.263,N10-1120,0,0.0553484,"Missing"
2021.acl-long.263,2020.conll-shared.1,1,0.80928,"Missing"
2021.acl-long.263,S15-2082,0,0.0767941,"Missing"
2021.acl-long.263,S14-2004,0,0.0356346,"strong baselines. Given the small size of the training data and the complicated task, these techniques often still outperform neural models, such as BiLSTMs (Katiyar and Cardie, 2016). Transition-based end-toend approaches have shown some potential (Zhang et al., 2019). However, all of this work ignores the polarity classification subtask. Targeted sentiment analysis only concentrates on extracting sentiment targets (task ii) and classifying the polarity directed towards them (task iv) (Jiang et al., 2011; Mitchell et al., 2013). Recent shared tasks on Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014, 2015, 2016) also include target extraction and polarity classification subtasks. Joint approaches perform on par with pipeline methods (Li et al., 2019a) and multitask models can perform even better (He et al., 2019). Finally, pretrained language models (Devlin et al., 2019) can also lead to improvements on the ABSA data (Li et al., 2019b). End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment (tasks ii and v) and sentiment expression extraction (task i), without requiring the resolution of relationships between targets and expressions. Wang et al. (2016"
2021.acl-long.263,2020.acl-demos.14,0,0.0179855,"own in Table 8 in the Appendix. The implementation of the graph structure has a large effect on all metrics, although the specific results depend on the dataset. We plot the average effect of each implementation across all datasets in Figure 3, as well as each individual dataset (Figures 4–8 in the Appendix). +inlabel tends to improve results on the non-English datasets, consistently increasing target and expression extraction and targeted sentiment. It also generally improves the graph scores UF1 and LF1 on the non-English datasets. 9 We use SpaCy (Honnibal et al., 2020) for English, Stanza (Qi et al., 2020) for Basque and Catalan and UDPipe (Straka and Strakov´a, 2017) for Norwegian. Dep. edges has the strongest positive effect on the NSF1 and SF1 (an avg. 2.52 and 2.22 percentage point (pp) over Head-final, respectively). However, this average is pulled down by poorer performance on the English datasets. Removing these two, the average benefit is 5.2 and 4.2 for NSF1 and SF1 , respectively. On span extraction and targeted sentiment, however, Dep. edges leads to poorer scores overall. Dep. labels does not lead to any consistent improvements. These results indicate that incorporating syntactic de"
2021.acl-long.263,D13-1170,0,0.0568037,"Missing"
2021.acl-long.263,2020.acl-main.577,0,0.0253182,"l dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures. Using the parser’s ability to learn arbitrary dependency graphs, Kurtz et al. (2020) phrased the task of negation resolution (Morante and Blanco, 2012; Morante and Daelemans, 2012) as a graph parsing task. This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time. In a simpler fashion, Yu et al. (2020) exploit the parser of Dozat and Manning (2018) to predict spans of named entities. 3 Datasets are shown in Table 1. The largest available structured sentiment dataset is the NoReCFine dataset (Øvrelid et al., 2020), a multi-domain dataset of professional reviews in Norwegian, annotated for structured sentiment. MultiBEU and MultiBCA (Barnes et al., 2018) are hotel reviews in Basque and Catalan, respectively. MPQA (Wiebe et al., 2005) annotates news wire text in English. Finally, DSUnis (Toprak et al., 2010) annotate English reviews of online universities and e-commerce. In our experiments, we"
2021.acl-long.263,K17-3009,0,0.056707,"Missing"
2021.nodalida-main.4,K18-2005,0,0.104033,"rBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingual BERT model – mBERT – which was trained on data that also included Norwegian. Up until the release of the models described in the current paper, mBERT was the only BERT-instance that could be used for Norwegian.6 Another widely used architecture for contextualised LMs is Embeddings From Language Models or ELMo (Peters et al., 2018). The ElmoForManyLangs initiative (Che et al., 2018) trained and released monolingual ELMo models for a wide range of different languages, including Norwegian (with separate models for Bokm˚al and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was published at https://github.com/botxo/nordic_bert in th"
2021.nodalida-main.4,N19-1423,0,0.598307,"iling the training process, we present contrastive benchmark results on a suite of NLP tasks for Norwegian. For additional background and access to the data, models, and software, please see: http://norlm.nlpl.eu 1 Introduction In this work, we present NorLM, an ongoing community initiative and emerging collection of largescale contextualised language models for Norwegian. We here introduce the NorELMo and NorBERT models, that have been trained on around two billion tokens of running Norwegian text. We describe the training procedure and compare these models with the multilingual mBERT model (Devlin et al., 2019), as well as an additional Norwegian BERT model developed contemporaneously, with some interesting differences in training data and setup. We report results over a number of Norwegian benchmark datasets, addressing a broad range of diverse NLP tasks: part-of-speech tagging, negation resolution, sentence-level and fine-grained sentiment analysis and named entity recognition (NER). All the models are publicly available for download from the Nordic Language Processing Laboratory (NLPL) Vectors Repository1 with a CC BY 4.0 license. They are also accessible locally, together with the training and s"
2021.nodalida-main.4,P18-2077,0,0.012311,"ssifier with dropout, identical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the common Targeted F1 metric, which requires correctly extracting a target (strict) and its polarity. We"
2021.nodalida-main.4,N03-3010,0,0.337854,"Missing"
2021.nodalida-main.4,W17-0237,1,0.852813,"e recipe for recreating the environment on other HPC systems, may contribute to ‘democratising’ large-scale NLP research; if nothing else, it eliminates dependency on commercial cloud computing services. 4 Related work Large-scale deep learning language models (LM) are important components of current NLP systems. They are often based on BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and other contextualised architectures. A number of language-specific initiatives have in recent years released monolingual versions of these models for a number of languages (Fares et al., 2017; Kutuzov and Kuzmenko, 2017; Virtanen et al., 2019; de Vries et al., ˇ 2019; Ulˇcar and Robnik-Sikonja, 2020; Koutsikakis et al., 2020; Nguyen and Nguyen, 2020; Farahani et al., 2020; Malmsten et al., 2020). For our purposes, the most important such previous training effort is that of Virtanen et al. (2019) on creating a BERT model for Finnish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model,"
2021.nodalida-main.4,P18-1007,0,0.0130726,"e believe this to be extremely important for more linguistically-oriented studies, where it is critical to deal with words, not with arbitrarily fragmented pieces (even if they are well-performing in practical tasks). The vocabulary for the model is of size 30,000. It is much less than the 120,000 of mBERT, but it is compensated by these entities being almost exclusively Norwegian. The vocabulary was generated from raw text, without, e.g., separating punctuation from word tokens. This means one can feed raw text into NorBERT. For the vocabulary generation, we used the SentencePiece algorithm (Kudo, 2018) and Tokenizers library.10 The resulting Tokenizers model was converted to the standard BERT WordPiece format. The final vocabulary contains several thousand unused wordpiece slots which can be filled in with task-specific lexical entries for further finetuning by future NorBERT users. 6.1 Training technicalities NorBERT corresponds in its configuration to the Google’s Bert-Base Cased for English, with 12 layers and hidden size 768 (Devlin et al., 2019). We used the standard masked language modeling and next sentence prediction losses with the LAMB optimizer (You et al., 2020). The model was t"
2021.nodalida-main.4,2021.nodalida-main.3,0,0.260739,"ch language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was published at https://github.com/botxo/nordic_bert in the beginning of 2020. However, the vocabulary of this model seems to be broken, and to the best of our knowledge nobody has achieved any meaningful results with it. 6 TraM) project, has released a Norwegian BERT (Base, cased) model dubbed NB-BERT (Kummervold et al., 2021).7 The model is trained on the Colossal Norwegian Corpus, reported to comprise close to 18,5 billion words (109.1 GB of text). In raw numbers, this is about ten times more than the corpus we use for training the NorLM models. However, the vast majority of this is from OCR’ed historical sources, which is bound to introduce at least some noise. In Section 7 below, we demonstrate that in some NLP tasks, a language model trained on less (but arguably cleaner) data can outperform a model trained on larger but noisy corpora. 5 NorELMo NorELMo is a set of bidirectional recurrent ELMo language models"
2021.nodalida-main.4,2020.iwpt-1.3,1,0.691501,"ntical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the common Targeted F1 metric, which requires correctly extracting a target (strict) and its polarity. We also evaluate Labelle"
2021.nodalida-main.4,E17-3025,1,0.74015,"ing the environment on other HPC systems, may contribute to ‘democratising’ large-scale NLP research; if nothing else, it eliminates dependency on commercial cloud computing services. 4 Related work Large-scale deep learning language models (LM) are important components of current NLP systems. They are often based on BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) and other contextualised architectures. A number of language-specific initiatives have in recent years released monolingual versions of these models for a number of languages (Fares et al., 2017; Kutuzov and Kuzmenko, 2017; Virtanen et al., 2019; de Vries et al., ˇ 2019; Ulˇcar and Robnik-Sikonja, 2020; Koutsikakis et al., 2020; Nguyen and Nguyen, 2020; Farahani et al., 2020; Malmsten et al., 2020). For our purposes, the most important such previous training effort is that of Virtanen et al. (2019) on creating a BERT model for Finnish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingua"
2021.nodalida-main.4,2021.nodalida-main.30,1,0.712644,"models on the POS tagging of Bokm˚al (BM) and Nynorsk (NN) test sets in comparison with other large pretrained models for Norwegian. Running times in minutes are given for Bokm˚al. the POS tagging task, training the models for 20 epochs and keeping the model that performs best on the development data. For ELMo models, we used a BiLSTM with global max pooling, taking ELMo token embeddings from the top layer as an input. The evaluation metric is macro F1 . Negation detection Finally, the NoReCfine dataset has recently been annotated with negation cues and their corresponding in-sentence scopes (Mæhlum et al., 2021). The resulting dataset is dubbed NoReCneg .18 We use the same graph-based modeling approach as described for fine-grained sentiment above. We evaluate on the same metrics as in the *SEM 2012 shared task (Morante and Blanco, 2012): cue-level F1 (CUE), scope token F1 over individual tokens (ST), and the combined full negation F1 (FN). 7.2 Results We present the results for the various benchmarking tasks below. POS tagging As can be seen from Table 2, NorBERT outperforms mBERT on both tasks: on POS tagging for Bokm˚al by 5 percentage points and 1 percentage point for Nynorsk. NorBERT is almost o"
2021.nodalida-main.4,S12-1035,0,0.0391265,"r 20 epochs and keeping the model that performs best on the development data. For ELMo models, we used a BiLSTM with global max pooling, taking ELMo token embeddings from the top layer as an input. The evaluation metric is macro F1 . Negation detection Finally, the NoReCfine dataset has recently been annotated with negation cues and their corresponding in-sentence scopes (Mæhlum et al., 2021). The resulting dataset is dubbed NoReCneg .18 We use the same graph-based modeling approach as described for fine-grained sentiment above. We evaluate on the same metrics as in the *SEM 2012 shared task (Morante and Blanco, 2012): cue-level F1 (CUE), scope token F1 over individual tokens (ST), and the combined full negation F1 (FN). 7.2 Results We present the results for the various benchmarking tasks below. POS tagging As can be seen from Table 2, NorBERT outperforms mBERT on both tasks: on POS tagging for Bokm˚al by 5 percentage points and 1 percentage point for Nynorsk. NorBERT is almost on par with NB-BERT on POS tagging. NorELMo models are outperformed by NB-BERT and NorBERT, but are on par with mBERT in POS tagging. Note that their adaptation to the tasks (extracting token embeddings and learning a classifier) t"
2021.nodalida-main.4,2020.findings-emnlp.92,0,0.0467805,"Missing"
2021.nodalida-main.4,L16-1250,1,0.838941,". Below we first provide an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set. ELMo models were not fine-tuned, following the recommendations from Peters et al. (2019). Instead we trained a simple neural classifier (a feed forwar"
2021.nodalida-main.4,2020.lrec-1.618,1,0.9224,"resentations across all 3 layers) for all words. Then, these token embeddings are fed to a neural classifier with dropout, identical to the one we used for POS tagging earlier. This classifier is also trained for 20 epochs with early stopping and batch size 32. 14 https://github.com/ltgoslo/norne https://github.com/davidsbatista/ NER-Evaluation 15 Fine-grained sentiment analysis NoReCfine is a dataset16 comprising a subset of the Norwegian Review Corpus (NoReC; Velldal et al., 2018) annotated for sentiment holders, targets, expressions, and polarity, as well as the relationships between them (Øvrelid et al., 2020). We here cast the problem as a graph prediction task and train a graph parser (Dozat and Manning, 2018; Kurtz et al., 2020) to predict sentiment graphs. The parser creates token-level representations which is the concatenation of a word embedding, POS tag embedding, lemma embedding, and character embedding created by a character-based LSTM. We further augment these representations with contextualised embeddings from each model. Models are trained for 100 epochs, keeping the best model on development F1 . For span extraction (holders, targets, expressions), we evaluate token-level F1 , and the"
2021.nodalida-main.4,N18-1202,0,0.459415,"nish – FinBERT5 – as our training setup for creating NorBERT builds heavily on this; see Section 6 for more details. Many low-resource languages do not have dedicated monolingual large-scale language models, and instead resort to using a multilingual model, such as Google’s multilingual BERT model – mBERT – which was trained on data that also included Norwegian. Up until the release of the models described in the current paper, mBERT was the only BERT-instance that could be used for Norwegian.6 Another widely used architecture for contextualised LMs is Embeddings From Language Models or ELMo (Peters et al., 2018). The ElmoForManyLangs initiative (Che et al., 2018) trained and released monolingual ELMo models for a wide range of different languages, including Norwegian (with separate models for Bokm˚al and Nynorsk). However, these models were trained on very modestly sized corpora of 20 million words for each language (randomly sampled from Wikipedia dumps and Common Crawl data). In a parallel effort to that of the current paper, the AI Lab of the National Library of Norway, through their Norwegian Transformer Model (No5 https://github.com/TurkuNLP/FinBERT A BERT model trained on Norwegian data was pub"
2021.nodalida-main.4,W19-4302,0,0.0601483,"Missing"
2021.nodalida-main.4,2020.acl-demos.14,0,0.119749,"is important for BERT-like models, because one of their training tasks is next sentence prediction). In total, our training corpus comprises about two billion (1,907,072,909) word tokens in 203 million (202,802,665) sentences. We conducted the following pre-processing steps: 1. Wikipedia texts were extracted from the dumps using the segment wiki script ˇ uˇrek and Sojka, from the Gensim project (Reh˚ 2010). 2. For the news texts from Norwegian Aviskorpus, we performed de-tokenization and conversion to UTF-8 encoding, where required. 3. The resulting corpus was sentencesegmented using Stanza (Qi et al., 2020). We left blank lines between documents (and 4 https://www.nb.no/sprakbanken/ ressurskatalog/oai-nb-no-sbr-4/ sections in the case of Wikipedia) so that the ‘next sentence prediction’ task of BERT does not span between documents. 3 Prerequisites: software and computing Developing very large contextualised language models is no small challenge, both in terms of engineering sophistication and computing demands. Training ELMo- and in particular BERT-like models presupposes access to specialised hardware – graphical processing units (GPUs) – over extended periods of time. Compared to the original"
2021.nodalida-main.4,solberg-etal-2014-norwegian,1,0.845115,"mBERT and to the recently released NB-BERT model described in Section 4. Where applicable, we show separate evaluation results for Bokm˚al and Nynorsk. Below we first provide an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set"
2021.nodalida-main.4,2020.lrec-1.582,0,0.0422658,"Missing"
2021.nodalida-main.4,L18-1661,1,0.755579,"Missing"
2021.nodalida-main.4,W17-0201,1,0.823933,"an overview of the different tasks and the corresponding classifiers that we train, before turning to discuss the results. 7.1 Task descriptions We start by briefly describing each task and associated dataset, in addition to the architectures we use. The sentence counts for the different datasets and their train, dev. and test splits are provided in Table 1. Part-of-speech tagging The Norwegian Dependency Treebank (NDT) (Solberg et al., 2014) includes annotation of POS tags for both Bokm˚al and Nynorsk. NDT has also been converted to the Universal Dependencies format (Øvrelid and Hohle, 2016; Velldal et al., 2017) and this is the version we are using here (for UD 2.7) for predicting UPOS tags. We use a typical sequence labelling approach with the BERT models, adding a linear layer after the final token representations and taking the softmax to get token predictions. We fine-tune all parameters for 20 epochs, using a learning rate of 2e-5, a training batch size of 8, max length of 256, and keep the best model on the development set. ELMo models were not fine-tuned, following the recommendations from Peters et al. (2019). Instead we trained a simple neural classifier (a feed forward network with one hidd"
2021.nodalida-main.4,K18-2001,0,0.0327235,"Missing"
A00-2022,P99-1061,1,0.72281,"t be equal, but could stand in a subtype-supertype relationship. In addition, the feature structure subsumption test is potentially expensive since feature structures are large, typically containing hundreds of nodes. It is therefore an open question whether parsing systems using grammars of this type can gain any advantage from local ambiguity packing. The question is becoming increasingly important, though, as wide-coverage HPSG grammars are starting to be deployed in practical applications-for example for &apos;deep&apos; analysis in the VerbMobil speech-to-speech translation system (Wahlster, 1997; Kiefer, Krieger, Carroll, & Malouf, 1999). 1 In this paper we answer the question by demonstrating that (a) subsumption- and equivalence-based feature structure packing is applicable to large HPSG grammars, and (b) average complexity and time taken for the parsing task can be greatly reduced. In Section 2 we present a new, linear-time, bidirec1A significant body of work on efficient processing with such grammars has been building up recently, with investigations into efficient feature structure operations, abstractmachine-based compilation, CF backbone computation, and finite-state approximation of HPSGderivations, amongst others (F"
A00-2022,2000.iwpt-1.16,0,0.0412401,"Missing"
A00-2022,P99-1075,0,0.013827,"ro- and retroactive local ambiguity packing with large feature structures, and have provided strong empirical evidence that our approach can be applied beneficially to chart parsing with a large, broad-coverage HPSG of English. By comparison to previous work in unification-based parsing we have demonstrated that pro- and retroactive packing are well-suited to achieve optimal packing; furthermore, experimental results obtained with a publicly-available HPSG processing platform confirm that ambiguity packing can greatly reduce average parse complexity for this type of grammars. In related work, Miyao (1999) describes an approach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments. Although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. (1999), who report large speed-ups from the elimination of disjunction processing during unification. Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison. We intend to develop the approach pres"
A00-2022,2000.iwpt-1.19,1,0.831282,"o only one previously derived edge being invalidated. This, of course, is a function of the order in which edges are derived, i.e. the parsing strategy. All the results in Table 2 were obtained with a &apos;right corner&apos; strategy which aims to exhaust computation for any suffix of the input string before moving the input pointer to the left; this is achieved by means of a scoring function end - -start W - (where start and end are the vertices of the derivation that would result from the computation, and n is the total input length) that orders parser tasks in the agenda. However, we have observed (Oepen & Callmeier, 2000) that HPSG-type, highly lexicalized grammars benefit greatly from a bidirectional, &apos;key&apos;-driven, active parsing regime, since they often employ rules with underspecified arguments that are only instantiated by coreference with other daughters (where the &apos;key&apos; daughter is the linguistic head in many but not all constructions). This requirement and the general non-predictability of categories derived for any token substring (in particular with respect to unary rule applications), means that a particular parsing strategy may reduce retroactive packing but cannot avoid it in general. With pro- and"
A00-2022,P85-1018,0,0.227187,"s reflect the exponential increase in total numbers of analyses; the figures show that our packing scheme achieves a very significant speedup, even when unpacking time is included in the comparison. 5 Choosing the Grammar Restrictor and Parsing Strategy In order for the subsumption relation to apply meaningfully to HPSG signs, two conditions must be met. Firstly, parse tree construction must not be duplicated in the feature structures (by means of the HPSG DTRS feature) but be left to the parser (i.e. recorded in the chart); this is achieved in a standard way by feature structure restriction (Shieber, 1985) applied to all passive edges. Secondly, the processing of constraints that do not restrict the search space but build up new (often semantic) structure should be postponed, since they are likely to interfere with subsumption. For example, analyses that differ only with respect to PP attachment would have the same syntax, but differences in semantics may prevent them being packed. This problem can be overcome by using restriction to (temporarily) remove such (semantic) attributes from lexical entries and also from the rule set, before they are input to the parser in the initial parse forest co"
A00-2022,P91-1041,0,0.077682,"we use in a bottom-up, chart-based parsing algorithm incorporating novel, efficient accounting mechanisms to guarantee minimal chart size (Section 3). We present a full-scale evaluation of the techniques on a large corpus (Section 4), and complete the picture with an empirically-based discussion of grammar restrictors and parsing strategies (Section 5). 2 Efficient Subsumption and Equivalence Algorithms Our feature structure subsumption algorithm 2 assumes totally well-typed structures (Carpenter, 1992) and employs similar machinery to the quasi-destructive unification algorithm described by Tomabechi (1991). In particular, it uses temporary pointers in dag nodes, each pointer tagged with a generation counter, to keep track of intermediate results in processing; incrementing the generation counter invalidates all temporary pointers in a single operation. But whereas quasi-destructive unification makes two passes (determining whether the unification will be successful and then copying out the intermediate representation) the subsumption algorithm makes only one pass, checking reentrancies and type-supertype relationships at the same time. 3 The algorithm, shown in Figure 1, also simultaneously tes"
A00-2022,A92-1012,0,\N,Missing
A00-2022,1991.iwpt-1.19,0,\N,Missing
A00-2022,P89-1018,0,\N,Missing
adolphs-etal-2008-fine,callmeier-etal-2004-deepthought,1,\N,Missing
adolphs-etal-2008-fine,W01-1815,0,\N,Missing
adolphs-etal-2008-fine,2004.tmi-1.2,1,\N,Missing
adolphs-etal-2008-fine,J93-2004,0,\N,Missing
adolphs-etal-2008-fine,W07-1219,1,\N,Missing
adolphs-etal-2008-fine,W06-2714,0,\N,Missing
adolphs-etal-2008-fine,P99-1061,1,\N,Missing
adolphs-etal-2008-fine,P02-1056,1,\N,Missing
adolphs-etal-2008-fine,P03-1014,1,\N,Missing
adolphs-etal-2008-fine,P02-1035,0,\N,Missing
adolphs-etal-2008-fine,P04-1014,0,\N,Missing
adolphs-etal-2008-fine,waldron-etal-2006-preprocessing,1,\N,Missing
baldwin-etal-2004-road,copestake-flickinger-2000-open,1,\N,Missing
baldwin-etal-2004-road,C02-2025,1,\N,Missing
baldwin-etal-2004-road,P03-1059,1,\N,Missing
C02-2025,J97-4005,0,0.0188522,"d information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these pr"
C02-2025,W97-1502,0,0.122641,"nks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relatively easy to judge. All disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the annotated profile or (ii) dynamic prop"
C02-2025,P97-1003,0,0.0128863,"t the simplest end, we might look only at the lexical type sequence assigned to the words by each parse and rank the parse based on the likelihood of that sequence. These lexical types – the preterminals in the derivation – are essentially part-of-speech tags, but encode considerably finer-grained information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). The"
C02-2025,P01-1019,1,0.256862,"with an enhanced version of the grammar in an automated fashion, viz. by re-applying the disambiguating decisions on the corpus with an updated version of the grammar. Depth of Representation and Transformation of Information Internally, the [incr tsdb()] database records analyses in three different formats, viz. (i) as a derivation tree composed of identifiers of lexical items and constructions used to build the analysis, (ii) as a traditional phrase structure tree labeled with an inventory of some fifty atomic labels (of the type ‘S’, ‘NP’, ‘VP’ et al.), and (iii) as an underspecified MRS (Copestake, Lascarides, & Flickinger, 2001) meaning representation. While representation (ii) will in many cases be similar to the representation found in the Penn Treebank, representation (iii) subsumes the functor – argument (or tectogrammatical) structure advocated in the Prague Dependency Treebank or the German TiGer corpus. Most importantly, however, representation (i) provides all the information required to replay the full HPSG analysis (using the original grammar and one of the open-source HPSG processing environments, e.g., the LKB or PET, which already have been interfaced to [incr tsdb()]). Using the latter approach, users"
C02-2025,W00-1908,0,0.0559015,"Missing"
C02-2025,P99-1069,0,0.0373806,"including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection perfo"
C02-2025,2000.iwpt-1.19,1,0.696136,"SG framework and a generally-available broad-coverage grammar of English, the LinGO English Resource Grammar (Flickinger, 2000) as implemented with the LKB grammar development environment (Copestake, 2002). Unlike existing treebanks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relat"
C02-2025,W02-2030,1,0.780762,"ermining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection performance as percentage of test sentences for which the correct parse was highest ranked by the model. (We restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.) We examine four models: an HMM tagging model, a simple PCFG, a PCFG with grandparent annotation, and a hybrid model that combines predictions from the PCFG and the tagger. Thes"
C10-1155,W02-1503,0,0.0438619,"different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take additional features into account. The procedure to enable the data-driven parser to learn from the grammar-driven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al., 2008) and the English grammar developed within the ParGram project (Butt, Dyvik, King, Masuichi, & Rohrer, 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank—one gold standard and one with LFG annotation. We extend the gold standard treebank with additional information from the corresponding LFG analysis and train MaltParser on the enhanced data set. Table 2 shows the enhanced dependency representation of example (1) above, taken from the training data. For each token, the parsed data contains information on the word form, lemma, and part of speech (PoS), as well as on the head and dependency relation in columns 6 and 7. The added XLE i"
C10-1155,W01-0521,0,0.029999,"enn Treebank (PTB), converted to dependency format (Johansson & Nugues, 2007) and extended with XLE features, as described above. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8. PoS Tagging and Domain Variation Our parser is trained on financial news, and although stacking with a general-purpose LFG parser is ex1381 pected to aid domain portability, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea, 2001). MaltParser presupposes that inputs have been PoS tagged, leaving room for variation in preprocessing. On the one hand, we aim to make parser inputs maximally similar to its training data (i.e. the conventions established in the PTB); on the other hand we wish to benefit from specialized resources for the biomedical domain. The GENIA tagger (Tsuruoka et al., 2005) is particularly relevant in this respect (as could be the GENIA Treebank proper4 ). However, we found that GENIA tokenization does not match the PTB conventions in about one out of five sentences (for example wrongly splitting token"
C10-1155,W07-2416,0,0.0146943,"bove, taken from the training data. For each token, the parsed data contains information on the word form, lemma, and part of speech (PoS), as well as on the head and dependency relation in columns 6 and 7. The added XLE information resides in the F E A T S column, and in the XLE-specific head and dependency columns 8 and 9. Parser outputs, which in turn form the basis for our scope resolution rules discussed in Section 5, also take this same form. The parser employed in this work is trained on the Wall Street Journal sections 2 – 24 of the Penn Treebank (PTB), converted to dependency format (Johansson & Nugues, 2007) and extended with XLE features, as described above. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8. PoS Tagging and Domain Variation Our parser is trained on financial news, and although stacking with a general-purpose LFG parser is ex1381 pected to aid domain portability, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea, 2001). MaltParser presupposes that inputs have been PoS tagged, leavi"
C10-1155,W08-0607,0,0.0122864,"gest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the strong and weak points in current"
C10-1155,P07-1125,0,0.0132078,"he organizers further suggest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the stron"
C10-1155,W10-3006,0,0.331143,"approaches and technology. In this article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (of 24) on Task 1 and third (of 15) on Task 2, for an overall best average result (there appears to be very limited overlap among top performers for the two subtasks). 2 It turns out, in fact, that all the top-performing systems in Task 2 of the CoNLLShared Task rely on syntactic information provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the rol"
C10-1155,W09-1304,0,0.0614029,"anguage has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level, lexical information only. To this end, CoNLL 2010 enters largely uncharted territory, and it remains to be seen (a) whether syntactic analysis indeed is a necessary component in approaching this task and, more generally, (b) to what degree the specific task setup can inform us about the strong and weak points in current approaches and technology. In this article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (o"
C10-1155,P06-2079,0,0.0325861,"out this paper, angle brackets highlight hedge cues, and curly braces indicate the scope of a given cue, as annotated in BioScope. sentences containing uncertainty; the objective of Task 2 is learning to resolve the in-sentence scope of hedge cues (Farkas, Vincze, Mora, Csirik, & Szarvas, 2010). The organizers further suggest: This task falls within the scope of semantic analysis of sentences exploiting syntactic patterns [...]. The utility of syntactic information within various approaches to sentiment analysis in natural language has been an issue of some debate (Wilson, Wiebe, & Hwa, 2006; Ng, Dasgupta, & Arifin, 2006), and the potential contribution of syntax clearly varies with the specifics of the task. Previous work in the hedging realm has largely been concerned with cue detection, i.e. identifying uncertainty cues such as may in (1), which are predominantly individual tokens (Medlock & Briscoe, 2007; Kilicoglu & Bergler, 2008). There has been little previous work aimed at actually resolving the scope of such hedge cues, which presumably constitutes a somewhat different and likely more difficult problem. Morante and Daelemans (2009) present a machine-learning approach to this task, using token-level,"
C10-1155,nivre-etal-2006-maltparser,0,0.0404639,"ndency representation of example (1), with MaltParser and XLE annotations. unit possible. For evaluation purposes, the task organizers provided newly annotated biomedical articles, following the same general BioScope principles. The CoNLL 2010 evaluation data comprises 5,003 additional utterances (138,276 tokens), of which 790 are annotated as hedged. The data contains a total of 1033 cues, of which 87 are so-called multiword cues (i.e. cues spanning multiple tokens), comprising 1148 cue tokens altogether. Stacked Dependency Parsing For syntactic analysis we employ the open-source MaltParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar se"
C10-1155,P08-1108,0,0.032951,"ltiple tokens), comprising 1148 cue tokens altogether. Stacked Dependency Parsing For syntactic analysis we employ the open-source MaltParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combinat"
C10-1155,P09-2010,1,0.841272,"altParser (Nivre, Hall, & Nilsson, 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse his"
C10-1155,W10-3008,0,0.197988,"s article, we investigate the contribution of syntax to hedge resolution, by reflecting on our experience in the CoNLL 2010 task.2 Our CoNLL system submission ranked fourth (of 24) on Task 1 and third (of 15) on Task 2, for an overall best average result (there appears to be very limited overlap among top performers for the two subtasks). 2 It turns out, in fact, that all the top-performing systems in Task 2 of the CoNLLShared Task rely on syntactic information provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the role of syntactic informa"
C10-1155,W10-3007,1,0.834253,"formation provided by parsers, either in features for machine learning or as input to manually crafted rules (Morante, Asch, & Daelemans, 2010; Rei & Briscoe, 2010). 1379 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1379–1387, Beijing, August 2010 Abstracts Articles Total Sentences Hedged Sentences 11871 2670 14541 2101 519 2620 Cues Multi-Word Cues Tokens Cue Tokens 2659 668 3327 309634 68579 378213 364 84 448 3056 782 3838 Table 1: Summary statistics for the Shared Task training data. This article transcends our CoNLL system description (Velldal, Øvrelid, & Oepen, 2010) in several respects, presenting updated and improved cue detection results (§ 3 and § 4), focusing on the role of syntactic information rather than on machine learning specifics (§ 5 and § 6), providing an analysis and discussion of Task 2 errors (§ 7), and generally aiming to gauge the value of available annotated data and processing tools (§ 8). We present a hybrid, two-level approach for hedge resolution, where a statistical classifier detects cue words, and a small set of manually crafted rules operating over syntactic structures resolve scope. We show how syntactic information—produced"
C10-1155,W08-0606,0,0.470685,"result (in terms of both combined ranks and average F1 ) in the 2010 CoNLL Shared Task. 1 Background—Motivation Recent years have witnessed an increased interest in the analysis of various aspects of sentiment in natural language (Pang & Lee, 2008). The subtask of hedge resolution deals with the analysis of uncertainty as expressed in natural language, and the linguistic means (so-called hedges) by which speculation or uncertainty are expressed. Information of this kind is of importance for various mining tasks which aim at extracting factual data. Example (1), taken from the BioScope corpus (Vincze, Szarvas, Farkas, Móra, & Csirik, 2008), shows a sentence where uncertainty is signaled by the modal verb may.1 {The unknown amino acid hmayi be used by these species}. (1) The topic of the Shared Task at the 2010 Conference for Natural Language Learning (CoNLL) is hedge detection in biomedical literature—in a sense ‘zooming in’ on one particular aspect of the broader BioNLP Shared Task in 2009 (Kim, Ohta, Pyysalo, Kano, & Tsujii, 2009). It involves two subtasks: Task 1 is described as learning to detect 1 In examples throughout this paper, angle brackets highlight hedge cues, and curly braces indicate the scope of a given cue, as"
C10-1155,P09-1043,0,0.0301455,"s, we make our parser incorporate the predictions of a large-scale, general-purpose LFG parser—following the work of Øvrelid, Kuhn, and Spreyer (2009). A technique dubbed parser stacking enables the data-driven parser to learn, not only from gold standard treebank annotations, but from the output of another parser (Nivre & McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar setup employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang & Wang, 2009). The stacked parser combines two quite different approaches—data-driven dependency parsing and ‘deep’ parsing with a handcrafted grammar—and thus provides us with a broad range of different types of linguistic information for the hedge resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take additional features into account. The procedure to enable the data-driven parser to"
C10-1155,W09-1401,0,\N,Missing
C10-1155,W10-3001,0,\N,Missing
C12-2096,P12-2074,1,0.812441,"struct. We also suggest a formal, generalized definition of the SBD task that we feel better represents the practical problem, from the point of view of an NLP pipeline. Gold-Standard Data Sets Our aim is to produce data sets that are as faithful as possible to the original text form, including paragraph breaks. For some continuity with previous work, we use both WSJ and Brown Corpus data, which required some corpus archaeology and reconstruction. An alignment between the original text of the WSJ (last released in 1995 as LDC #95T07) and the annotations in the PTB has recently been published (Dridan and Oepen, 2012), and we thus start from raw, untokenised text and superimpose onto it 7,706 PTB gold-standard sentence boundaries (from Sections 3 – 6, the ‘classic’ SBD subset). Various versions of the Brown Corpus exist, but none correspond exactly to the original raw text. For our SBD tests, we combine sentence segmentation in the tagged Brown Corpus (distributed with the Natural Language Toolkit, NLTK) with the ‘raw’, so-called Bergen Form I3 as a reference for automatically reversing tokenisation, quote disambiguation, and some artifacts that appear only in the tagged version. As in previous work, we al"
C12-2096,N09-2061,0,0.439988,"Missing"
C12-2096,J06-4003,0,0.796347,"Missing"
C12-2096,A00-2035,0,0.222354,"Missing"
C12-2096,J02-3002,0,0.04498,"ficiency in terms of total processing time when segmenting the Brown Corpus as one single document. For high-throughput use cases, the comparatively ‘lean’, heuristic systems RASP and tokenizer—building on the Un∗ x (f)lex tool in the tradition of Grefenstette and Tapanainen (1994)—may have an advantage. Note that one tool offers two pre-packaged configurations: one for ‘general’ text, here dubbed LingPipe1 , and another tuned to biomedical literature and specifically the GENIA Corpus, LingPipe2 . We were unable to obtain implementations for the earlier studies of Palmer and Hearst (1997) and Mikheev (2002). Table 2 presents SBD performance levels for our nine systems and four corpora in a first, off-the-shelf experiment. Here, we sought to run each tool in its default configuration—often instantiating sample invocations or API calls, where available—and in a setup we consider representative for an NLP pipeline: processing each corpus as a contiguous text stream. For Punkt, we rely on the implementation and pre-trained model for English that ships with NLTK (Bird et al., 2009). Recall from § 3 above that our corpus preparation preserves in-text paragraph boundaries (as indicated in plain raw tex"
C12-2096,S12-1035,0,0.0210252,"erence for automatically reversing tokenisation, quote disambiguation, and some artifacts that appear only in the tagged version. As in previous work, we also occasionally corrected the segmentation (often related to paragraph-initial quote marks, which in the tagged data can occur as a ‘sentence’ by themselves), and restored punctuation to better match the original raw text—for a total of 57,275 sentences. We also used more recently released data, to explore a wider range of edited text types and introduce ‘fresh’ test data. The Conan Doyle Corpus (CDC) was used in the 2012 *SEM Shared Task (Morante and Blanco, 2012), and provides various Sherlock Holmes stories in both raw and segmented versions, albeit for only 5,692 sentences. Then, from quite a different domain, the GENIA Corpus, a collection of 16,392 sentences from biomedical research abstracts (Kim et al., 2003), also contains the required boundary annotation. The use of these additional corpora allows us to assess the generality of the various tools, which is particularly important as WSJ and Brown data has been central in much previous SBD development. Task Definition and Evaluation In much previous work, SBD was operationalized as a binary class"
C12-2096,J97-2002,0,0.104923,"rse indication of run-time efficiency in terms of total processing time when segmenting the Brown Corpus as one single document. For high-throughput use cases, the comparatively ‘lean’, heuristic systems RASP and tokenizer—building on the Un∗ x (f)lex tool in the tradition of Grefenstette and Tapanainen (1994)—may have an advantage. Note that one tool offers two pre-packaged configurations: one for ‘general’ text, here dubbed LingPipe1 , and another tuned to biomedical literature and specifically the GENIA Corpus, LingPipe2 . We were unable to obtain implementations for the earlier studies of Palmer and Hearst (1997) and Mikheev (2002). Table 2 presents SBD performance levels for our nine systems and four corpora in a first, off-the-shelf experiment. Here, we sought to run each tool in its default configuration—often instantiating sample invocations or API calls, where available—and in a setup we consider representative for an NLP pipeline: processing each corpus as a contiguous text stream. For Punkt, we rely on the implementation and pre-trained model for English that ships with NLTK (Bird et al., 2009). Recall from § 3 above that our corpus preparation preserves in-text paragraph boundaries (as indicat"
C12-2096,read-etal-2012-wesearch,1,0.653981,"dan and Oepen, 2012), or may just be owed to this (part of a larger) system not being as thoroughly engineered as the specialized RASP or tokenizer rule sets. 5 SBD for More Informal, User-Generated Content To extend our survey to rather different types of text—user-generated Web content (UGC)—we ran similar experiments on two recent corpora that come with gold-standard sentence boundary annotations: First, the WeScience Corpus of Ytrestøl et al. (2009) comprises some 12,000 sentences from Wikipedia, drawing on a sample of articles in the NLP domain. Second, the WeSearch Data Collection (WDC; Read et al., 2012) includes gold-standard annotations for two smaller samples of Web blogs, some 1,000 sentences each in the NLP and Linux domains, respectively (dubbed WNB and WLB in Table 4 below). As none of our SBD tools fully supports mark-up processing, we reduced the WeScience and WDC corpora into a pure text form, only keeping paragraph breaks from the original mark-up in a first experimental setup dubbed A in Table 4. A variant experiment, dubbed B below, aims to gauge the potential contribution of mark-up to SBD, where we insert additional paragraph boundaries around block elements like headings, pre-"
C12-2096,A97-1004,0,0.16828,"9.8 + 98.5 98.8 99.3 98.9 99.0 + 98.9 + WSJ + 94.8 98.7 + 90.9 + 98.5 99.1 98.3 99.0 99.2 + 99.2 + All 95.0 97.4 95.3 97.2 97.4 97.3 97.6 96.5 97.6 Table 3: Best-case SBD performance. Results prefixed with ‘+ ’ indicate forcing sentence boundaries at paragraph breaks, while the ‘A’ and ‘ L ’ prefixes mark substitution of Unicode quotes to ASCII or LATEX-style, respectively. Non-prefixed scores are repeated from Table 2 for ease of comparison. Among the supervised machine-learning tools, MxTerminator and OpenNLP show very similar results, confirming OpenNLP as essentially a reimplementation of Reynar and Ratnaparkhi (1997), if maybe with a slight edge over the original. Splitta, representing the most recent SBD study applying supervised learning, on the other hand, is outranked by OpenNLP by almost one full F1 point. It further shows comparatively larger drops on Brown and CDC, which taken together with its premium performance on WSJ could be suggestive of limited robustness to text variation (or over-tuning effects). Three of the rule-based tools, in this survey, show comparable behavior: LingPipe1 (the ‘generic’ variant), RASP, and tokenizer all deliver relatively good results across the board and show compar"
C12-2096,H89-2048,0,0.189833,"Missing"
C12-2096,2001.mtsummit-papers.66,0,0.0608878,"of the state of the art in SBD. In much NLP research, the ‘sentence’ (in a suitable interpretation; see below) is a foundational unit, for example in aligning parallel texts; PoS tagging; syntactic, semantic, and discourse parsing; or machine translation. Assuming gold-standard sentence boundaries (and possibly tokenisation)—as provided by standard data sets like the PTB—has been common practice for many isolated studies. However, strong effects of error propagation must be expected in standard NLP pipelines, for example of imperfect SBD into morphosyntactic, semantic, or discourse analysis (Walker et al., 2001; Kiss and Strunk, 2002). For these reasons, we aim to determine (a) what levels of performance can be expected from extant SBD techniques; (b) to which degree SBD performance is sensitive to variation in text types; and (c) whether there are relevant differences in observed behavior across different SBD approaches. Our own motivation in this work is twofold: First, working in the context of semi-automated parser adaptation to domain and genre variation, we would hope to encourage a shift of emphasis towards parsing as an end-to-end task, i.e. taking as its point of departure the running text"
C12-2096,J93-2004,0,\N,Missing
C14-2020,flickinger-etal-2010-wikiwoods,1,0.898793,"Missing"
C14-2020,W12-3602,1,0.882931,"Missing"
C14-2020,kouylekov-oepen-2014-semantic,1,0.841345,"Missing"
C14-2020,J93-2004,0,0.0566075,"Missing"
C14-2020,S14-2008,1,0.900513,"Missing"
C14-2020,oepen-lonning-2006-discriminant,1,0.863393,"Missing"
copestake-etal-2004-lexicon,copestake-flickinger-2000-open,1,\N,Missing
copestake-etal-2004-lexicon,villavicencio-etal-2004-multilingual,1,\N,Missing
copestake-etal-2004-lexicon,copestake-etal-2002-multiword,1,\N,Missing
copestake-etal-2004-lexicon,W02-1210,0,\N,Missing
D11-1037,P06-4020,0,0.051008,"Missing"
D11-1037,W08-1705,0,0.023705,"Missing"
D11-1037,cer-etal-2010-parsing,0,0.0398474,"Missing"
D11-1037,P05-1022,0,0.0267785,"ngs from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. English factored model which combines the preferences of unlexicalized PCFG phrase structures and of lexical dependencies, trained on sections 02–21 of the WSJ portion of the PTB. We chose Stanford Parser from among the state-of-the-art PTB-derived parsers for its support for grammatical relations as an alternate interface representation. Charniak&Johnson Reranking Parser (Charniak and Johnson, 2005) is a two-stage PCFG parser with a lexicalized generative model for the firststage, and a discriminative MaxEnt reranker for the second-stage. The models we evaluate are also trained on sections 02–21 of the WSJ. Top-50 readings were used for the reranking stage. The output constituent trees were then converted into Stanford Dependencies. According to Cer et al. (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical"
D11-1037,J07-4004,0,0.0417408,". (2010), this combination gives the best parsing accuracy in terms of Stanford dependencies on the PTB. Enju (Miyao et al., 2004) is a probabilistic HPSG parser, combining a hand-crafted core grammar with automatically acquired lexical types from the PTB.3 The model we evaluate is trained on the same material from the WSJ sections of the PTB, but the treebank is first semi-automatically converted into HPSG derivations, and the annotation is enriched with typed feature structures for each constituent. In addition to HPSG derivation trees, Enju also produces predicate argument structures. C&C (Clark and Curran, 2007) is a statistical CCG parser. Abstractly similar to the approach of Enju, the grammar and lexicon are automatically induced from CCGBank (Hockenmaier and Steedman, 2007), a largely automatic projection of (the WSJ portion of) PTB trees into the CCG framework. In addition to CCG derivations, the C&C parser can directly output a variant of grammatical relations. RASP (Briscoe et al., 2006) is an unlexicalized robust parsing system, with a hand-crafted “tag sequence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic"
D11-1037,W01-0713,0,0.0205191,"ive examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak"
D11-1037,flickinger-etal-2010-wikiwoods,1,0.827502,"omparable to what they report for the Brown Corpus (but not the WSJ portion of the PTB). 4.2 Annotation format We annotated up to two dependency triples per phenomenon instance, identifying the heads and dependents by the surface form of the head words in the sentence suffixed with a number indicating word position (see Table 2).6 Some strings contain more than one instance of the phenomenon they illustrate; in these cases, multiple sets of dependencies are We processed 900 million tokens of Wikipedia text using the October 2010 release of the ERG, following the work of the WikiWoods project (Flickinger et al., 2010). Using the top-ranked ERG deriva6 tion trees as annotations over this corpus and simAs the parsers differ in tokenization strategies, our evaluaple patterns using names of ERG-specific construc- tion script treats these position IDs as approximate indicators. 402 Item ID 1011079100200 1011079100200 1011079100200 Phenomenon absol absol absol Polarity 1 1 1 Dependency having-2|been-3|passed-4 ARG act-1 withdrew-9 MOD having-2|been-3|passed-4 carried+on-12 MOD having-2|been-3|passed-4 Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-"
D11-1037,P06-1111,0,0.012084,"c knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowled"
D11-1037,J07-3004,0,0.221976,"type “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1 Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between probabilistic and symbolic knowled"
D11-1037,W07-2416,0,0.109783,"with a hand-coded grammar at their core typically also incorporate an automatically trained probabilistic disambiguation component. 398 formal nature and the “granularity” of linguistic information (i.e. the number of distinctions assumed), encompassing variants of constituent structure, syntactic dependencies, or logical-form representations of semantics. Parser interface representations range between the relatively simple (e.g. phrase structure trees with a limited vocabulary of node labels as in the PTB, or syntactic dependency structures with a limited vocabulary of relation labels as in Johansson and Nugues (2007)) and the relatively complex, as for example elaborate syntactico-semantic analyses produced by the ParGram or DELPH - IN grammars. There tends to be a correlation between the methodology used in the acquisition of linguistic knowledge and the complexity of representations: in the creation of a mostly hand-crafted treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of ma"
D11-1037,N04-1013,0,0.0162081,"Missing"
D11-1037,P04-1061,0,0.0139015,"along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Pe"
D11-1037,J93-2004,0,0.0463107,"verage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic di"
D11-1037,H05-1066,0,0.0471313,"equence” grammar at its core. The parser thus analyses a lattice of PoS tags, building a parse forest from which the most probable syntactic trees and sets of corresponding grammatical relations can be extracted. Unlike other parsers in our mix, RASP did not build on PTB data in either its PoS tagging 3 This hand-crafted grammar is distinct from the ERG, despite sharing the general framework of HPSG. The ERG is not included in our evaluation, since it was used in the extraction of the original examples and thus cannot be fairly evaluated. 399 or syntactic disambiguation components. MSTParser (McDonald et al., 2005) is a datadriven dependency parser. The parser uses an edgefactored model and searches for a maximal spanning tree that connects all the words in a sentence into a dependency tree. The model we evaluate is the second-order projective model trained on the same WSJ corpus, where the original PTB phrase structure annotations were first converted into dependencies, as established in the CoNLL shared task 2009 (Johansson and Nugues, 2007). XLE/ParGram (Riezler et al., 2002, see also Cahill et al., 2008) applies a hand-built Lexical Functional Grammar for English and a stochastic parse selection mod"
D11-1037,C10-1094,0,0.115638,"Missing"
D11-1037,P04-1047,0,0.0766051,"Missing"
D11-1037,P06-1055,0,0.00651761,"4), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in"
D11-1037,P02-1035,0,0.037496,"anguage Processing, pages 397–408, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics unlabeled training data. A related dimension of variation is the type of representations manipulated by the parser. We briefly review some representative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH - IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all"
D11-1037,D09-1085,0,0.273521,"Missing"
D11-1037,P08-1039,0,0.028581,"Missing"
D11-1037,N10-1034,0,\N,Missing
D11-1037,J03-4003,0,\N,Missing
D18-1178,N18-1143,0,0.0457869,"Missing"
D18-1178,W16-1604,0,0.0341938,"Missing"
D18-1178,W15-0122,0,0.534904,"in the British National Corpus (Burnard, 2000) are part ´ S´eaghdha, 2008) – of noun–noun compounds (O and its relevance to other natural language processing (NLP) tasks such as question answering and information retrieval (Nakov, 2008), noun– noun compound interpretation has been the focus of much work, in theoretical linguistics (Li, 1972; Downing, 1977; Levi, 1978; Finin, 1980; Ryder, 1994), psycholinguistics (Gagn´e and Shoben, 1997; Marelli et al., 2017), and computational lin´ S´eaghdha guistics (Lauer, 1995; Nakov, 2007; O and Copestake, 2009; Girju et al., 2009; Kim and Baldwin, 2013; Dima and Hinrichs, 2015). In computational linguistics, noun–noun compound interpretation is, by and large, approached as an automatic classification problem. Hence several machine learning (ML) algorithms and models have been used to learn the semantics of nominal compounds, including Maximum Entropy (Tratz ´ and Hovy, 2010), Support Vector Machines (O S´eaghdha and Copestake, 2013) and Neural Networks (Dima and Hinrichs, 2015; Vered and Waterson, 2018). These models use information from lexical semantics such as WordNet-based features and distributional semantics such as word embeddings. Nonetheless, noun–noun comp"
D18-1178,P16-3011,1,0.651956,"e 2018 Conference on Empirical Methods in Natural Language Processing, pages 1488–1498 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics One of the primary motivations for using multitask learning is to improve generalization by “leveraging the domain-specific information contained in the training signals of related tasks” Caruana (1997). In this work, we show that TL and MTL can indeed be used as a kind of regularizer to learn to predict infrequent relations given a highly skewed distribution of relations from the noun–noun compound dataset of Fares (2016) which is especially well suited for TL and MTL experimentation as detailed in Section 3. Our contributions can be summarized as: 1. Through careful result analysis, we find that TL and MTL (mainly on the embedding layer) do improve the overall accuracy and the F1 scores of the less frequent relations in a highly skewed dataset, in comparison to a strong single-task learning baseline. 2. Even though our work focuses on TL and MTL, to the best of our knowledge, we are the first to report experimental results on the comparatively recent dataset of Fares (2016). 2 Related Work Noun–Noun Compound"
D18-1178,W17-0237,1,0.83664,"ing layer where the word embedding vectors are stored; the selected word embedding vectors are then fed to a fully connected hidden layer whose size is the same as the number of dimensions of the word embedding vectors. Finally, a softmax function is applied on the output layer and the most likely relation is selected. The compound’s constituents are represented using a 300-dimensional word embedding model trained on an English Wikipedia dump (dated February 2017) and English Gigaword Fifth Edition (Parker et al., 2011) using GloVe (Pennington et al., 2014). The embedding model was trained by Fares et al. (2017) who provide more details on the hyperparameters used to train the embedding model.5 When looking up a word in the embedding model, if the word is not found we check if the word is uppercased and look up the same word in lowercase. If a word is hyphenated and is not found in the embedding vocabulary, we split it on the hyphen and average the vectors of its parts (if they exist in the vocabulary). If after these steps the word is still not found, we use a designated vector for unknown words. Architecture and Hyperparameters Our choice of hyperparameters is motivated by several rounds of experim"
D18-1178,I05-1082,0,0.0285735,"ne-grained taxonomy of 43 relations. Others question the very assumption that noun–noun compounds are interpretable using a finite, predefined set of relations (Downing, 1977; Finin, 1980) and propose alternative paraphrasingbased approaches (Nakov, 2007; Shwartz and Dagan, 2018). We here focus on the approaches that cast the interpretation problem as a classification task over a finite predefined set of relations. A wide variety of machine learning models have been already applied to learn this task, including nearest neighbor classifiers using semantic similarity based on lexical resources (Kim and Baldwin, 2005), kernel-based methods like SVMs ´ S´eaghdha using lexical and relational features (O and Copestake, 2009), Maximum Entropy models with a relatively large selection of lexical and surface form features such as synonyms and affixes (Tratz and Hovy, 2010) and, most recently, neural networks either solely relying on word embeddings to represent noun–noun compounds (Dima and Hinrichs, 2015) or word embeddings and socalled path embeddings (which encode information about lemmas and part-of-speech tags, inter alia) in a combined paraphrasing and classification approach (Vered and Waterson, 2018). Of"
D18-1178,kingsbury-palmer-2002-treebank,0,0.365094,"Missing"
D18-1178,N15-1098,0,0.0631539,"Missing"
D18-1178,E17-1005,0,0.226126,"s of noun–noun compounds is not easily derivable from the compounds’ constituents (Vered and Waterson, 2018). Our work, in part, contributes to advancing NLP research on noun– noun compound interpretation through the use of transfer and multi-task learning. The interest in using transfer learning (TL) and multi-task learning (MTL) in NLP has surged over the past few years, showing ‘mixed’ results depending on the so-called main and auxiliary tasks involved, model architectures and datasets, among other things (Collobert and Weston, 2008; Mou et al., 2016; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). These ‘mixed’ results, coupled with the fact that neither TL nor MTL has been applied to noun–noun compounds interpretation before, motivate our extensive empirical study on the use of TL and MTL for compound interpretation, not only to supplement existing research on the utility of TL and MTL for semantic NLP tasks in general, but also to determine their benefits for compound interpretation in particular. 1488 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1488–1498 c Brussels, Belgium, October 31 - November 4, 2018."
D18-1178,P17-1186,0,0.0528941,"Missing"
D18-1178,meyers-etal-2004-annotating,0,0.0672502,"to learn to classify the semantic relations holding between each pair of compound constituents. The difficulty of this task, obviously, depends on the label set used and its distribution, among other things. For all the experiments presented in this paper, we adapt the noun–noun compounds dataset created by Fares (2016) which consists of compounds annotated with two different taxonomies of relations; in other words, for each noun–noun compound there are two distinct relations, drawing on different linguistic schools. The dataset was derived from existing linguistic resources, such as NomBank (Meyers et al., 2004) and the Prague Czech-English Dependency Treebank 2.0 (Hajiˇc et al., 2012, PCEDT). Our motivation for using this dataset is twofold: first, dual annotation with relations over the same underlying set of compounds maximally enables TL and MTL perspectives; second, alignment of two distinct annotation frameworks over the same data facilitates contrastive analysis and comparison across frameworks. More specifically, we use a subset of the dataset created by Fares (2016), by focusing on type-based instances of so-called two-word compounds.1 The original dataset by Fares (2016) also includes multi"
D18-1178,D14-1162,0,0.0810176,"ifying the indices of a compound’s constituents in the embedding layer where the word embedding vectors are stored; the selected word embedding vectors are then fed to a fully connected hidden layer whose size is the same as the number of dimensions of the word embedding vectors. Finally, a softmax function is applied on the output layer and the most likely relation is selected. The compound’s constituents are represented using a 300-dimensional word embedding model trained on an English Wikipedia dump (dated February 2017) and English Gigaword Fifth Edition (Parker et al., 2011) using GloVe (Pennington et al., 2014). The embedding model was trained by Fares et al. (2017) who provide more details on the hyperparameters used to train the embedding model.5 When looking up a word in the embedding model, if the word is not found we check if the word is uppercased and look up the same word in lowercase. If a word is hyphenated and is not found in the embedding vocabulary, we split it on the hyphen and average the vectors of its parts (if they exist in the vocabulary). If after these steps the word is still not found, we use a designated vector for unknown words. Architecture and Hyperparameters Our choice of h"
D18-1178,D16-1046,0,0.320843,"tic construction, is very productive and 2) the semantics of noun–noun compounds is not easily derivable from the compounds’ constituents (Vered and Waterson, 2018). Our work, in part, contributes to advancing NLP research on noun– noun compound interpretation through the use of transfer and multi-task learning. The interest in using transfer learning (TL) and multi-task learning (MTL) in NLP has surged over the past few years, showing ‘mixed’ results depending on the so-called main and auxiliary tasks involved, model architectures and datasets, among other things (Collobert and Weston, 2008; Mou et al., 2016; Søgaard and Goldberg, 2016; Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). These ‘mixed’ results, coupled with the fact that neither TL nor MTL has been applied to noun–noun compounds interpretation before, motivate our extensive empirical study on the use of TL and MTL for compound interpretation, not only to supplement existing research on the utility of TL and MTL for semantic NLP tasks in general, but also to determine their benefits for compound interpretation in particular. 1488 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages"
D18-1178,P07-3013,0,0.692326,"Missing"
D18-1178,E09-1071,0,0.0811582,"Missing"
D18-1178,P18-1111,0,0.015922,"pretation vary depending on the taxonomy of compound relations as well as the machine learning models and features used to learn those rela´ S´eaghdha (2007) defines tions. For example, O a coarse-grained set of relations (viz. six relations based on theoretical work by Levi (1978)), whereas Tratz and Hovy (2010) assume a considerably more fine-grained taxonomy of 43 relations. Others question the very assumption that noun–noun compounds are interpretable using a finite, predefined set of relations (Downing, 1977; Finin, 1980) and propose alternative paraphrasingbased approaches (Nakov, 2007; Shwartz and Dagan, 2018). We here focus on the approaches that cast the interpretation problem as a classification task over a finite predefined set of relations. A wide variety of machine learning models have been already applied to learn this task, including nearest neighbor classifiers using semantic similarity based on lexical resources (Kim and Baldwin, 2005), kernel-based methods like SVMs ´ S´eaghdha using lexical and relational features (O and Copestake, 2009), Maximum Entropy models with a relatively large selection of lexical and surface form features such as synonyms and affixes (Tratz and Hovy, 2010) and,"
D18-1178,P16-2038,0,0.131782,"Missing"
D18-1178,P10-1070,0,0.495055,"g single-task learning baseline. 2. Even though our work focuses on TL and MTL, to the best of our knowledge, we are the first to report experimental results on the comparatively recent dataset of Fares (2016). 2 Related Work Noun–Noun Compound Interpretation Existing approaches to noun–noun compound interpretation vary depending on the taxonomy of compound relations as well as the machine learning models and features used to learn those rela´ S´eaghdha (2007) defines tions. For example, O a coarse-grained set of relations (viz. six relations based on theoretical work by Levi (1978)), whereas Tratz and Hovy (2010) assume a considerably more fine-grained taxonomy of 43 relations. Others question the very assumption that noun–noun compounds are interpretable using a finite, predefined set of relations (Downing, 1977; Finin, 1980) and propose alternative paraphrasingbased approaches (Nakov, 2007; Shwartz and Dagan, 2018). We here focus on the approaches that cast the interpretation problem as a classification task over a finite predefined set of relations. A wide variety of machine learning models have been already applied to learn this task, including nearest neighbor classifiers using semantic similarit"
D18-1178,N18-2035,0,0.175578,"n, 1997; Marelli et al., 2017), and computational lin´ S´eaghdha guistics (Lauer, 1995; Nakov, 2007; O and Copestake, 2009; Girju et al., 2009; Kim and Baldwin, 2013; Dima and Hinrichs, 2015). In computational linguistics, noun–noun compound interpretation is, by and large, approached as an automatic classification problem. Hence several machine learning (ML) algorithms and models have been used to learn the semantics of nominal compounds, including Maximum Entropy (Tratz ´ and Hovy, 2010), Support Vector Machines (O S´eaghdha and Copestake, 2013) and Neural Networks (Dima and Hinrichs, 2015; Vered and Waterson, 2018). These models use information from lexical semantics such as WordNet-based features and distributional semantics such as word embeddings. Nonetheless, noun–noun compound interpretation remains one of the more difficult NLP problems because: 1) noun–noun compounding, as a linguistic construction, is very productive and 2) the semantics of noun–noun compounds is not easily derivable from the compounds’ constituents (Vered and Waterson, 2018). Our work, in part, contributes to advancing NLP research on noun– noun compound interpretation through the use of transfer and multi-task learning. The in"
flickinger-etal-2010-wikiwoods,zhang-kordoni-2008-robust,0,\N,Missing
flickinger-etal-2010-wikiwoods,2004.tmi-1.2,1,\N,Missing
flickinger-etal-2010-wikiwoods,A00-2022,1,\N,Missing
flickinger-etal-2010-wikiwoods,W97-1502,0,\N,Missing
flickinger-etal-2010-wikiwoods,adolphs-etal-2008-fine,1,\N,Missing
flickinger-etal-2014-towards,J93-2004,0,\N,Missing
flickinger-etal-2014-towards,flickinger-etal-2010-wikiwoods,1,\N,Missing
flickinger-etal-2014-towards,P82-1014,0,\N,Missing
flickinger-etal-2014-towards,D12-1035,0,\N,Missing
flickinger-etal-2014-towards,D11-1037,1,\N,Missing
flickinger-etal-2014-towards,P13-1042,0,\N,Missing
flickinger-etal-2014-towards,kouylekov-oepen-2014-semantic,1,\N,Missing
flickinger-etal-2014-towards,D13-1161,0,\N,Missing
flickinger-etal-2014-towards,oepen-lonning-2006-discriminant,1,\N,Missing
flickinger-etal-2014-towards,adolphs-etal-2008-fine,1,\N,Missing
I05-1015,W02-1503,0,0.0307772,"a set of algorithmic refinements, we present two novel techniques: the integration of subsumption-based local ambiguity factoring, and a procedure to selectively unpack the generation forest according to a probability distribution given by a conditional, discriminative model. 1 Introduction A number of wide-coverage precise bi-directional NL grammars have been developed over the past few years. One example is the LinGO English Resource Grammar (ERG) [1], couched in the HPSG framework. Other grammars of similar size and coverage also exist, notable examples using the LFG and the CCG formalisms [2,3]. These grammars are used for generation from logical form input (also termed tactical generation or realization) in circumscribed domains, as part of applications such as spoken dialog systems [4] and machine translation [5]. Grammars like the ERG are lexicalist, in that the majority of information is encoded in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of vari"
I05-1015,2004.tmi-1.2,1,0.599826,"ion given by a conditional, discriminative model. 1 Introduction A number of wide-coverage precise bi-directional NL grammars have been developed over the past few years. One example is the LinGO English Resource Grammar (ERG) [1], couched in the HPSG framework. Other grammars of similar size and coverage also exist, notable examples using the LFG and the CCG formalisms [2,3]. These grammars are used for generation from logical form input (also termed tactical generation or realization) in circumscribed domains, as part of applications such as spoken dialog systems [4] and machine translation [5]. Grammars like the ERG are lexicalist, in that the majority of information is encoded in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of variables associated with predicates and their semantic roles. For these sorts of grammars and ‘flat’ semantic inputs, lexically-driven approaches to realization – such as Shake-and-Bake [6], bag generation from logical form [7],"
I05-1015,C92-2117,0,0.0138245,"og systems [4] and machine translation [5]. Grammars like the ERG are lexicalist, in that the majority of information is encoded in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of variables associated with predicates and their semantic roles. For these sorts of grammars and ‘flat’ semantic inputs, lexically-driven approaches to realization – such as Shake-and-Bake [6], bag generation from logical form [7], chart generation [8], and constraint-based generation [9] – are highly suitable. Alternative approaches based on semantic head-driven generation and more recent variants [10,11] would work less well for lexicalist grammars since these approaches assume a hierarchically structured input logical form. Similarly to parsing with large scale grammars, realization can be computationally expensive. In his presentation of chart generation, Kay [8] describes one source of potential inefficiency and proposes an approach for tackling it. However, Kay does not repor"
I05-1015,P96-1027,0,0.557912,"e ERG are lexicalist, in that the majority of information is encoded in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of variables associated with predicates and their semantic roles. For these sorts of grammars and ‘flat’ semantic inputs, lexically-driven approaches to realization – such as Shake-and-Bake [6], bag generation from logical form [7], chart generation [8], and constraint-based generation [9] – are highly suitable. Alternative approaches based on semantic head-driven generation and more recent variants [10,11] would work less well for lexicalist grammars since these approaches assume a hierarchically structured input logical form. Similarly to parsing with large scale grammars, realization can be computationally expensive. In his presentation of chart generation, Kay [8] describes one source of potential inefficiency and proposes an approach for tackling it. However, Kay does not report on a verification of his approach with an actual grammar."
I05-1015,P01-1028,0,0.0134666,"ority of information is encoded in lexical entries (or lexical rules) as opposed to being represented in constructions (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of variables associated with predicates and their semantic roles. For these sorts of grammars and ‘flat’ semantic inputs, lexically-driven approaches to realization – such as Shake-and-Bake [6], bag generation from logical form [7], chart generation [8], and constraint-based generation [9] – are highly suitable. Alternative approaches based on semantic head-driven generation and more recent variants [10,11] would work less well for lexicalist grammars since these approaches assume a hierarchically structured input logical form. Similarly to parsing with large scale grammars, realization can be computationally expensive. In his presentation of chart generation, Kay [8] describes one source of potential inefficiency and proposes an approach for tackling it. However, Kay does not report on a verification of his approach with an actual grammar. Carroll et al. [12]  Dan Flickinger"
I05-1015,J90-1004,0,0.245091,"ns (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of variables associated with predicates and their semantic roles. For these sorts of grammars and ‘flat’ semantic inputs, lexically-driven approaches to realization – such as Shake-and-Bake [6], bag generation from logical form [7], chart generation [8], and constraint-based generation [9] – are highly suitable. Alternative approaches based on semantic head-driven generation and more recent variants [10,11] would work less well for lexicalist grammars since these approaches assume a hierarchically structured input logical form. Similarly to parsing with large scale grammars, realization can be computationally expensive. In his presentation of chart generation, Kay [8] describes one source of potential inefficiency and proposes an approach for tackling it. However, Kay does not report on a verification of his approach with an actual grammar. Carroll et al. [12]  Dan Flickinger and Ann Copestake contributed a lot to the work described in this paper. We also thank Berthold Crysmann, Jan Tore Lønni"
I05-1015,W02-2106,0,0.0615891,"ns (i.e. rules operating on phrases). The semantic input to the generator for such grammars, often, is a bag of lexical predicates with semantic relationships captured by appropriate instantiation of variables associated with predicates and their semantic roles. For these sorts of grammars and ‘flat’ semantic inputs, lexically-driven approaches to realization – such as Shake-and-Bake [6], bag generation from logical form [7], chart generation [8], and constraint-based generation [9] – are highly suitable. Alternative approaches based on semantic head-driven generation and more recent variants [10,11] would work less well for lexicalist grammars since these approaches assume a hierarchically structured input logical form. Similarly to parsing with large scale grammars, realization can be computationally expensive. In his presentation of chart generation, Kay [8] describes one source of potential inefficiency and proposes an approach for tackling it. However, Kay does not report on a verification of his approach with an actual grammar. Carroll et al. [12]  Dan Flickinger and Ann Copestake contributed a lot to the work described in this paper. We also thank Berthold Crysmann, Jan Tore Lønni"
I05-1015,P99-1061,1,0.805095,"dreds or thousands of edges may be produced for non-trivial input semantics, but there are only a relatively small number of logical variables. Indexing edges on these variables involves bookkeeping that turns out not to be worthwhile in practice; logical bit vector operations on edge coverage take negligible time, and these serve to filter out the majority of edge combinations with incompatible indices. The remainder are filtered out efficiently before unification is attempted by a check on which rules can dominate which others, and the quick-check, as developed for unification-based parsing [14]. For the quick-check, it turns out that the same set of feature paths that most frequently lead to unification failure in parsing also work well in generation. 3 We therefore have four operations on bit vectors representing EP coverage (C) in chart edges: concatenation of edges e1 and e2 → e3 : C(e3 ) = OR(C(e1 ), C(e2 )); can edges e1 and e2 combine? AND(C(e1 ), C(e2 )) = 0; do edges e1 and e2 cover the same EPs? C(e1 ) = C(e2 ); do edges e1 , . . . , en cover all input EPs? NOT(OR(C(e1 ), . . . , C(en )) = 0. • • • • 170 J. Carroll and S. Oepen 3.2 Local Ambiguity Factoring In chart parsing"
I05-1015,P89-1018,0,0.233993,"= OR(C(e1 ), C(e2 )); can edges e1 and e2 combine? AND(C(e1 ), C(e2 )) = 0; do edges e1 and e2 cover the same EPs? C(e1 ) = C(e2 ); do edges e1 , . . . , en cover all input EPs? NOT(OR(C(e1 ), . . . , C(en )) = 0. • • • • 170 J. Carroll and S. Oepen 3.2 Local Ambiguity Factoring In chart parsing with context free grammars, the parse forest (a compact representation of the full set of parses) can only be computed in polynomial time if sub-analyses dominated by the same non-terminal and covering the same segment of the input string are ‘packed’, or factored into a single unitary representation [15]. Similar benefits accrue for unification grammars without a context free backbone such as the LinGO ERG, if the category equality test is replaced by feature structure subsumption [16]4 ; also, feature structures representing the derivation history need to be restricted out when applying a rule [17]. The technique can be applied to chart realization if the input span is expressed as coverage of the input semantics. For example, with the input of Figure 1, the two phrases in (2) below would have equivalent feature structures, and we pack the one found second into the one found first, which the"
I05-1015,A00-2022,1,0.90155,"(C(e1 ), . . . , C(en )) = 0. • • • • 170 J. Carroll and S. Oepen 3.2 Local Ambiguity Factoring In chart parsing with context free grammars, the parse forest (a compact representation of the full set of parses) can only be computed in polynomial time if sub-analyses dominated by the same non-terminal and covering the same segment of the input string are ‘packed’, or factored into a single unitary representation [15]. Similar benefits accrue for unification grammars without a context free backbone such as the LinGO ERG, if the category equality test is replaced by feature structure subsumption [16]4 ; also, feature structures representing the derivation history need to be restricted out when applying a rule [17]. The technique can be applied to chart realization if the input span is expressed as coverage of the input semantics. For example, with the input of Figure 1, the two phrases in (2) below would have equivalent feature structures, and we pack the one found second into the one found first, which then acts as the representative edge for all subsequent processing. (2) young Polish athlete |Polish young athlete We have found that packing is crucial to efficiency: realization time is"
I05-1015,P85-1018,0,0.2231,"h context free grammars, the parse forest (a compact representation of the full set of parses) can only be computed in polynomial time if sub-analyses dominated by the same non-terminal and covering the same segment of the input string are ‘packed’, or factored into a single unitary representation [15]. Similar benefits accrue for unification grammars without a context free backbone such as the LinGO ERG, if the category equality test is replaced by feature structure subsumption [16]4 ; also, feature structures representing the derivation history need to be restricted out when applying a rule [17]. The technique can be applied to chart realization if the input span is expressed as coverage of the input semantics. For example, with the input of Figure 1, the two phrases in (2) below would have equivalent feature structures, and we pack the one found second into the one found first, which then acts as the representative edge for all subsequent processing. (2) young Polish athlete |Polish young athlete We have found that packing is crucial to efficiency: realization time is improved by more than an order of magnitude for inputs with more than 500 realizations (see Section 4). Changing pac"
I05-1015,W02-2030,0,0.328106,"edges would be created that extended that phrase without the negation being present. Section 4 shows this technique results in dramatic improvements in realization efficiency. 3.5 Selective Unpacking The selective unpacking procedure outlined in this section allows us to extract a small set of n-best realizations from the generation forest at minimal cost. The global rank order is determined by a conditional Maximum Entropy (ME) model – essentially an adaptation of recent HPSG parse selection work to the realization ranking task [19]. We use a similar set of features to Toutanova and Manning [20], but our procedure differs from theirs in that it applies the stochastic model before unpacking, in a guided search through the generation forest. Thus, we avoid enumerating all candidate realizations. Unlike Malouf and van Noord [21], on the other hand, we avoid an approximative beam search during forest creation and guarantee to produce exactly the n-best realizations (according to the ME model). Further looking at related parse selection work, our procedure is probably most similar to those of Geman and Johnson [22] and Miyao and 5 Implementing collect-semantic-vars() can be efficient: sea"
I05-1015,P02-1036,0,0.060532,"nking task [19]. We use a similar set of features to Toutanova and Manning [20], but our procedure differs from theirs in that it applies the stochastic model before unpacking, in a guided search through the generation forest. Thus, we avoid enumerating all candidate realizations. Unlike Malouf and van Noord [21], on the other hand, we avoid an approximative beam search during forest creation and guarantee to produce exactly the n-best realizations (according to the ME model). Further looking at related parse selection work, our procedure is probably most similar to those of Geman and Johnson [22] and Miyao and 5 Implementing collect-semantic-vars() can be efficient: searching for Skolem constants throughout the full structure, it does a similar amount of computation as a single unification. 172 J. Carroll and S. Oepen 1 → 2 →    2 3 5 6   4 3  5 7     8 6 8 7 9 6 9 7    6 → 10 11 4 →  Fig. 2. Sample generator forest and sub-node decompositions: ovals in the forest (on the left) indicate packing of edges under subsumption, i.e. edges 4 , 7 , 9 , and 11 are not in the generator chart proper. During unpacking, there will be multiple ways of instantiating a chart edge,"
I11-1028,P09-1006,0,0.050545,"Missing"
I11-1028,P02-1035,0,0.1593,"Missing"
I11-1028,P05-1041,1,0.936429,"new parse forest, and checking inter-annotator agreement on the overlap, we annotated the remaining sentences. All accuracy figures we report are over the data set of 669 trees complete at the time of experimentation. 3.3 4 Blazing In §2, we reviewed work that uses linguistic information from superficially incompatible formalisms for treebanking or parse selection. Our experiments here use syntactic information from the GTB to partially disambiguate the parse forest produced by the ERG. We do this by disallowing certain candidate ERG trees on the basis of GTBderived information, and we follow Tanaka et al. (2005) in denoting this process “blazing”.1 As detailed below, we can use this partially disambiguated forest: (1) to train parse selection models; and (2) to reduce treebanking effort, abstractly similarly to Tanaka et al. (2005). The goal is not to apply all constraints from the GTB to the ERG parse trees; rather, we want to apply the minimal amount of constraints possible, while still sufficiently restricting the parse forest for our target application. We call the set of trees remaining after blazing silver trees, to represent the fact that they are not gold standard, but are generally of better"
I11-1028,W10-3007,1,0.800767,"scriminants, which are easier to identify. This process happens with all discriminants for a sentence simultaneously, so it is possible to rule out all parse trees. This may indicate that none of the candidate parses are desirable, or that the imperfect blazing process is not completely successful. The blazing module is given the GTB XML source for the tree, and a set of discriminants, each of which includes the name of the rule or lexical Biomedical parsing setup We parsed sentences using the ERG with the PET parser (Callmeier, 2000), which uses POS tags to constrain unknown words. Following Velldal et al. (2010), we primarily use the biomedically trained GENIA tagger (Tsuruoka et al., 2005), but defer to TnT (Brants, 2000) for tagging nominal elements, because it makes a useful distinction between common and proper nouns. Biomedical text poses a unique set of challenges, mostly relating to named entities, such as proteins, DNA and cell lines. To address this, we used the GENIA tagger as a named-entity (NE) recogniser, treating named entities as atomic lex1 248 Which is a term in forestry: marking trees for removal. NP entry, as well as the corresponding character span in the source tree. It applies s"
I11-1028,P94-1034,0,0.142853,"Missing"
I11-1028,W07-2207,1,0.929339,"Missing"
I11-1028,A00-1031,0,0.578191,"it is possible to rule out all parse trees. This may indicate that none of the candidate parses are desirable, or that the imperfect blazing process is not completely successful. The blazing module is given the GTB XML source for the tree, and a set of discriminants, each of which includes the name of the rule or lexical Biomedical parsing setup We parsed sentences using the ERG with the PET parser (Callmeier, 2000), which uses POS tags to constrain unknown words. Following Velldal et al. (2010), we primarily use the biomedically trained GENIA tagger (Tsuruoka et al., 2005), but defer to TnT (Brants, 2000) for tagging nominal elements, because it makes a useful distinction between common and proper nouns. Biomedical text poses a unique set of challenges, mostly relating to named entities, such as proteins, DNA and cell lines. To address this, we used the GENIA tagger as a named-entity (NE) recogniser, treating named entities as atomic lex1 248 Which is a term in forestry: marking trees for removal. NP entry, as well as the corresponding character span in the source tree. It applies some pre-configured transformations to the GTB tree, and examines each discriminant for whether it should be ruled"
I11-1028,W02-1503,0,0.354299,"Missing"
I11-1028,W97-1502,0,0.655547,"Missing"
I11-1028,W11-2927,1,0.695034,"rsing using a pure WeScience model. Other configurations used models trained from the same training sentence parse forest, setting a pseudo-gold tree either randomly, selftrained (best from a WeScience model), or blazing (highest-ranked of the silver trees, other silver trees discarded). The gold WeScience data is also used for training. Significance figures are against “WeSc only”, ( *: p &lt; 0.05; ***:p &lt; 0.001), and “Self-train”, ( ††: p &lt; 0.01; †††: p &lt; 0.001) Experimental Configuration how ‘right’ or ‘wrong’ the top analysis is. To supplement AccN , we use Elementary Dependency Match (EDM: Dridan and Oepen (2011)). This is based on triples extracted from the semantic output of the parser, providing a more granular measure of the quality of the analyses. We use the EDMN A configuration that is arguably the most compatible with other dependency-based parser evaluation, although we make no claims of direct comparability. We create a parse forest by parsing 10747 sentences from a GTB subset not overlapping with the test corpus, using the WeScience model to determine the top 500 parses. The best-performing method we found to create parse selection models from this parse forest was to apply the blazing conf"
I11-1028,W01-0521,0,0.056251,"s stochastically adapted to a new domain, using unlabelled data from the new domain (Daum´e III and Marcu, 2006); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, we see two main use cases. The first uses an existing treebank to create a treebank for some completely different linguistic framework, generally to induce a grammar in that framework. Xia (1999) presents work on transforming Penn Treebank (PTB) trees into Lexicalized Tree Adj"
I11-1028,hockenmaier-steedman-2002-acquiring,0,0.18136,"n option, however, for finegrained tasks which require an expert understanding of a theory or domain, such as syntactic treebanking or discourse annotation. Two main approaches have been adopted to efficiently create new resources: (1) domain adaptation, where a trained model from one domain is stochastically adapted to a new domain, using unlabelled data from the new domain (Daum´e III and Marcu, 2006); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, w"
I11-1028,W02-2018,0,0.0442808,"Missing"
I11-1028,P06-1043,0,0.197318,"06); and (2) annotation projection, where the labels in a pre-existing resource are semi-automatically translated into an independent formalism, e.g. in translating the PTB into the CCG formalism (Hockenmaier and Steedman, 2002). This paper looks at both of these approaches: domain adaptation from unannotated 2 Related Work Domain adaptation is an active research area, triggered by the observation that parsers trained on one domain show decreased performance when used in other domains (Gildea, 2001). Much domain-adaptation work involves some small amount of in-domain data to tune a model, but McClosky et al. (2006) showed “self-training” using unannotated in-domain data could achieve significant improvements in parser accuracy. In parsing-related research that has used annotated data, but in an incompatible format, we see two main use cases. The first uses an existing treebank to create a treebank for some completely different linguistic framework, generally to induce a grammar in that framework. Xia (1999) presents work on transforming Penn Treebank (PTB) trees into Lexicalized Tree Adjoining Grammar (LTAG) structures. The work of Hockenmaier and Steedman (2002) is roughly parallel, but targets Combina"
I11-1028,W05-0603,0,0.0611187,"Missing"
I11-1028,I05-2038,0,\N,Missing
J12-2005,A00-1031,0,0.0143254,"8, Number 2 exhibited GENIA tokenization problems. Our pre-processing approach thus deploys a cascaded ﬁnite-state tokenizer (borrowed and adapted from the open-source English Resource Grammar: Flickinger [2002]), which aims to implement the tokenization decisions made in the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993)—much like GENIA, in principle—but more appropriately treating corner cases like the ones noted here. 2.2 PoS Tagging and Lemmatization For part-of-speech (PoS) tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant tokenizer) and TnT (Brants 2000), which operates on pre-tokenized inputs but in its default model is trained on ﬁnancial news from the Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy provided by GENIA in the biomedical domain, while using our improved tokenization and producing inputs to the parsers that as much as possible resemble the conventions used in the original training data for the (dependency) parser (the Penn Treebank, once again). To this effect, for the vast majority of tokens we can align the GENIA tokenization with our own, and in these cases we typically use GENIA PoS tags"
J12-2005,W02-1503,0,0.0392941,"f linguistic information to draw upon for the speculation resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classiﬁers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take into account additional features. The procedure to enable the data-driven parser to learn from the grammar-driven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al. 2008) and the English grammar developed within the ParGram project (Butt et al. 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank—one gold-standard and one with LFG annotation. We extend the gold-standard treebank with additional information from the corresponding LFG analysis and train MaltParser on the enhanced data set. For a description of the parse model features and the dependency substructures proposed by XLE for each word token, see Nivre and McDonald (2008). For further background on the conversion and training procedures, see Øvrelid, Kuhn, and Spreyer (2009). Table 5 shows the enhanced dependency re"
J12-2005,W10-3110,0,0.168931,"Missing"
J12-2005,W01-0521,0,0.0163915,"urrent set of scope rules, or annotation of parallel constructions may in some cases differ in subtle ways (see Section 6.1.5). The overﬁtting effects caused by the data dependencies introduced by the various GENIA-based domain adaptation steps, as described in Section 2.3, must also be taken into account. 6.1.4 PoS Tagging and Domain Variation. As mentioned in Section 6.1.1, an advantage of stacking with a general-purpose LFG parser is that it can be expected to aid domain portability. Nonetheless, substantial differences in domain and genre are bound to negatively affect syntactic analysis (Gildea 2001), and our parser is trained on ﬁnancial news. MaltParser presupposes that inputs have been PoS tagged, however, leaving room for variation in preprocessing. In this article we have aimed, on the one hand, to make parser inputs conform as much as possible to the conventions established in its PTB training data, while on the other hand taking advantage of specialized resources for the biomedical domain. To assess the impact of improved, domain-adapted inputs on our scope resolution rules, we contrast two conﬁgurations: Running the parser in the exact same manner as Øvrelid, Kuhn, and Spreyer (20"
J12-2005,W07-2416,0,0.0153954,"e XLE analysis (Features). Id Form PoS Features XHead XDep Head DepRel 1 2 3 4 5 6 7 8 9 10 11 The unknown amino acid may be used by these species . DT JJ JJ NN MD VB VBN IN DT NNS . _ degree:attributive degree:attributive pers:3|case:nom|num:sg|ntype:common mood:ind|subcat:MODAL|tense:pres|clauseType:decl _ subcat:V-SUBJ-OBJ|vtype:main|passive:+ _ deixis:proximal num:pl|pers:3|case:obl|common:count|ntype:common _ 4 4 4 3 0 7 5 9 10 7 0 SPECDET ADJUNCT ADJUNCT SUBJ ROOT PHI XCOMP PHI SPECDET OBL-AG PUNC 4 4 4 5 0 5 6 7 10 8 5 NMOD NMOD NMOD SBJ ROOT VC VC LGS NMOD PMOD P to dependency format (Johansson and Nugues 2007) and extended with XLE features, as described previously. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8, which is lower than the current stateof-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE. 6.1.2 Rule Overview. Our sco"
J12-2005,P99-1069,0,0.0433025,"English Resource Grammar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to a syntactic constituent and secondly, that we can automatically learn a ranking function that selects the correct constituent. Our ranking approach to scope resolution is abstractly related to statistical parse selection, and in particular work on discriminative parse selection for uniﬁcation based grammars, such as those by Johnson et al. (1999), Riezler et al. (2002), Malouf and van Noord (2004), and Toutanova et al. (2005). The overall goal is to learn a function for ranking syntactic structures, based on training data that annotates which tree(s) are correct and incorrect for each sentence. In our case, however, rather than discriminating between complete analyses for a given sentence, we want to learn a ranking function over candidate subtrees (i.e., constituents) within a parse (or possibly even within several parses). Figure 3 presents an example derivation tree that represents a complete HPSG analysis. Starting from the cue an"
J12-2005,W10-3010,0,0.271449,"extended on this system by also adding syntactic features, resulting in the top performing system of the CoNLL-2010 Shared Task at the scope-level (corresponding to the second subtask). It is interesting to note that all the top performers use various types of syntactic information in their scope resolution systems: The output from a dependency parser (MaltParser) (Morante, van Asch, and Daelemans 2010; Velldal, Øvrelid, and Oepen 2010), a tag sequence grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010). The majority of systems perform classiﬁcation at the token level, using some variant of machine learning with a BIO classiﬁcation scheme and a post-processing step to assemble the full scope (Farkas et al. 2010), although several of the top performers employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, Øvrelid, and Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010). 5. Identifying Speculation Cues We now turn to look at the details of our own system, starting in this section with describing a simple yet effective approach to identifying"
J12-2005,W04-3103,0,0.271137,"Missing"
J12-2005,I11-1028,1,0.819467,"carry over to any downstream components using this information. For the experiments described in this article, GENIA supplies lemmas for the n-gram features used by the cue classiﬁers, as well as PoS tags used in the input to both the dependency parser and the Head-driven Phrase Structure Grammar (HPSG) parser (which in turn provide the inputs to our various scope resolution components). For the HPSG parser, a subset of the GENIA corpus was also used as part of the training data for estimating an underlying statistical parse selection model, producing n-best lists of ranked candidate parses (MacKinlay et al. 2011). When reporting ﬁnal test results on the full papers (BSP or BSE) or the clinical reports (BSR), no such dependencies between information sources exists. It does mean, however, that we can reasonably expect to see some extra drop in performance when going from development results on data that includes the BioScope abstracts to the test results on these other data sets. 372 Velldal et al. Rules, Rankers, and the Role of Syntax 3. Evaluation Measures In this section we seek to clarify the type of measures we will be using for evaluating both the cue detection components (Section 3.1) and the sc"
J12-2005,J93-2004,0,0.0406023,"Missing"
J12-2005,D08-1017,0,0.00926869,".1.1) and quantifying the effect of using a domain-adapted PoS tagger (Section 6.1.4). 6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source MaltParser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-dr"
J12-2005,P07-1125,0,0.0169926,"a given system. The sequence of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence for a different system so that the sign-test can be applied as above. Note that our modiﬁed scorer for negation is available from our Web page of supplemental materials,2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our ﬁnal model conﬁgurations. 4. Related Work on Speculation Labeling Although there exists a body of earlier work on identifying uncertainty on the sentence level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the task of resolving the in-sentence scope of speculation cues was ﬁrst pioneered by Morante and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al. 2010) entered largely uncharted territory and contributed to an increased interest for this task. Virtually all systems for resolving speculation scope implement a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem,"
J12-2005,morante-2010-descriptive,0,0.138902,"ules applied directly without modiﬁcations achieve 48.67 and 56.25. In order to further improve on these results, we introduce a few new rules to account speciﬁcally for negation. The general rule machinery is identical to the speculation scope rules described in Section 6.1: The rules are triggered by the part of speech of the cue and operate over the dependency representations output by the stacked dependency parser described in Section 6.1.1. In developing the rules we consulted the BioScope guidelines (Vincze et al. 2008), as well as a descriptive study of negation in the BioScope corpus (Morante 2010). 401 Computational Linguistics Volume 38, Number 2 Table 13 Additional dependency-based scope rules for negation, with information source (MaltParser or XLE), organized by PoS of the cue. PoS DT NN NNnone VB RBvb RBother Description Source Determiners scope over their head node and its descendants Nouns scope over their descendants none take scope over entire sentence if subject and otherwise over its descendants Verbs scope over their descendants Adverbs with verbal head scope over the descendants of the lexical verb Adverbs scope over the descendants of the head M M M M M, X M, X 7.2.1 Rule"
J12-2005,W09-1304,0,0.0634709,"d biomedical articles for evaluation purposes, constituting an additional 5,003 utterances. This latter data set (also detailed in Table 1) will be used for held-out testing of our speculation models. We will be using the following abbreviations when referring to the various parts of the data: BSA (BioScope abstracts), BSP (full papers), BSE (the heldout evaluation data), and BSR (clinical reports). Note that, when we get to the negation task we will be using the original version of the BioScope data. Furthermore, as BSE does not annotate negation, we instead follow the experimental set-up of Morante and Daelemans (2009b) for the negation task, reporting 10-fold cross validation on BSA and held-out testing on BSP and BSR. 2.1 Tokenization The BioScope data (and other data sets in the CoNLL-2010 Shared Task), are provided sentence-segmented only, and otherwise non-tokenized. Unsurprisingly, the GENIA tagger (Tsuruoka et al. 2005) has a central role in our pre-processing set-up. We found that its tokenization rules are not always optimally adapted for the type of text in BioScope, however. For example, GENIA unconditionally introduces token boundaries for some punctuation marks that can also occur token-intern"
J12-2005,W09-1105,0,0.644974,"(2009b) Cue classiﬁer & Scope Rules + Ranking 66.31 69.30 65.27 72.89 65.79 71.05 BSP Held-out Morante et al. (2009b) Cue classiﬁer & Scope Rules + Ranking 42.49 58.58 39.10 68.09 40.72 62.98 BSR Held-out Scope Level Data Morante et al. (2009b) Cue classiﬁer & Scope Rules + Ranking 74.03 89.62 70.54 89.41 72.25 89.52 405 Computational Linguistics Volume 38, Number 2 To some degree, some of the differences are to be expected, perhaps, at least with respect to BSP. For example, the BSP evaluation represents a held-out setting for both the cue and scope component in the machine learned system of Morante and Daelemans (2009b). While also true for our cue classiﬁer and subtree ranker, it is not strictly speaking the case for the dependency rules, and so the potential effect of any overﬁtting during learning might be less visible. The small set of manually deﬁned rules are general in nature, targeting the general syntactic constructions expressing negation, as shown in Table 13. In addition to being based on the BioScope annotation guidelines, however, both the abstracts and the full papers were consulted for patterns, and the fact that rule development has included intermediate testing on BSP (although mostly dur"
J12-2005,D08-1075,0,0.0736588,"Missing"
J12-2005,W10-3006,0,0.343028,"Missing"
J12-2005,nivre-etal-2006-maltparser,0,0.110423,"Missing"
J12-2005,P08-1108,0,0.0459012,"the effect of using a domain-adapted PoS tagger (Section 6.1.4). 6.1.1 Stacked Dependency Parsing. For syntactic analysis we use the open-source MaltParser (Nivre, Hall, and Nilsson 2006), a platform for data-driven dependency parsing. For improved accuracy and portability across domains and genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-driven dependency parsing an"
J12-2005,C10-1155,1,0.869645,"Missing"
J12-2005,N07-1051,0,0.0167452,"system output of Morante and Daelemans (2009b), however, we also computed cue-level scores for their system. Morante and Daelemans (2009b) identify cues using a small list of unambiguous cue words compiled from the abstracts in combination with applying a decision tree classiﬁer to the remaining words. Their features record information about neighboring word forms, PoS, and chunk information from GENIA. Zhu et al. (2010) train an SVM to classify tokens according to a BIO-scheme using surface-oriented n-gram features in addition to various syntactic features extracted using the Berkley parser (Petrov and Klein 2007) trained on the GENIA treebank. Looking at the results in Table 12, we see that the performance of our cue classiﬁer compares favorably with the systems of both Morante and Daelemans (2009b) and Zhu et al. (2010), achieving a higher cue-level F1 across all data sets (with differences in classiﬁer decisions with respect to Morante and Daelemans [2009b] being statistically signiﬁcant for all of them). For the 10-fold run, the biggest difference concerns token-level precision, where both the system of Zhu et al. (2010) and our own achieves a substantially higher score than that of Morante and Dae"
J12-2005,W10-3008,0,0.0167739,"ask: as a sequence labeling task and using only token-level, lexical information. Morante, van Asch, and Daelemans (2010) then extended on this system by also adding syntactic features, resulting in the top performing system of the CoNLL-2010 Shared Task at the scope-level (corresponding to the second subtask). It is interesting to note that all the top performers use various types of syntactic information in their scope resolution systems: The output from a dependency parser (MaltParser) (Morante, van Asch, and Daelemans 2010; Velldal, Øvrelid, and Oepen 2010), a tag sequence grammar (RASP) (Rei and Briscoe 2010), as well as constituent analysis in combination with dependency triplets (Stanford lexicalized parser) (Kilicoglu and Bergler 2010). The majority of systems perform classiﬁcation at the token level, using some variant of machine learning with a BIO classiﬁcation scheme and a post-processing step to assemble the full scope (Farkas et al. 2010), although several of the top performers employ manually constructed rules (Kilicoglu and Bergler 2010; Velldal, Øvrelid, and Oepen 2010) or even combinations of machine learning and rules (Rei and Briscoe 2010). 5. Identifying Speculation Cues We now tur"
J12-2005,P02-1035,0,0.0191491,"ar (ERG; Flickinger [2002]), a general-purpose, wide-coverage grammar couched in the framework of an HPSG (Pollard and Sag 1987, 1994). The approach rests on two main assumptions: Firstly, that the annotated scope of a speculation cue corresponds to a syntactic constituent and secondly, that we can automatically learn a ranking function that selects the correct constituent. Our ranking approach to scope resolution is abstractly related to statistical parse selection, and in particular work on discriminative parse selection for uniﬁcation based grammars, such as those by Johnson et al. (1999), Riezler et al. (2002), Malouf and van Noord (2004), and Toutanova et al. (2005). The overall goal is to learn a function for ranking syntactic structures, based on training data that annotates which tree(s) are correct and incorrect for each sentence. In our case, however, rather than discriminating between complete analyses for a given sentence, we want to learn a ranking function over candidate subtrees (i.e., constituents) within a parse (or possibly even within several parses). Figure 3 presents an example derivation tree that represents a complete HPSG analysis. Starting from the cue and working through the t"
J12-2005,P08-1033,0,0.0127087,"nce of boolean values that results (FP = 0, TP = 1) can be directly paired with the corresponding sequence for a different system so that the sign-test can be applied as above. Note that our modiﬁed scorer for negation is available from our Web page of supplemental materials,2 together with the system output (in XML following the BioScope DTD) for all end-to-end runs with our ﬁnal model conﬁgurations. 4. Related Work on Speculation Labeling Although there exists a body of earlier work on identifying uncertainty on the sentence level, (Light, Qiu, and Srinivasan 2004; Medlock and Briscoe 2007; Szarvas 2008), the task of resolving the in-sentence scope of speculation cues was ﬁrst pioneered by Morante and Daelemans (2009a). In this sense, the CoNLL-2010 Shared Task (Farkas et al. 2010) entered largely uncharted territory and contributed to an increased interest for this task. Virtually all systems for resolving speculation scope implement a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem, putting emphasi"
J12-2005,W10-3002,0,0.723173,"a two-stage architecture: First there is a component that identiﬁes the speculation cues and then there is a component for resolving the in-sentence scopes of these cues. In this section we provide a brief review of previous work on this problem, putting emphasis of the best performers from the two corresponding subtasks of the CoNLL-2010 Shared Task, cue detection (Task 1) and scope resolution (Task 2). 4.1 Related Work on Identifying Speculation Cues The top-ranked system for Task 1 in the ofﬁcial CoNLL-2010 Shared Task evaluation approached cue identiﬁcation as a sequence labeling problem (Tang et al. 2010). Similarly to the decision-tree approach of Morante and Daelemans (2009a), Tang et al. (2010) set out to label tokens according to a BIO-scheme; indicating whether they are at the Beginning, Inside, or Outside of a speculation cue. In the “cascaded” system architecture of Tang et al. (2010), the predictions of both a Conditional Random Field (CRF) sequence classiﬁer and an SVM-based Hidden Markov Model (HMM) are both combined in a second CRF. In terms of the overall approach, namely, viewing the problem as a sequence labeling task, Tang et al. (2010) are actually representative of the majorit"
J12-2005,W10-3007,1,0.912943,"Missing"
J12-2005,W08-0606,0,0.622666,"ce in this respect, where the topic was speculation detection for the domain of biomedical research literature ∗ University of Oslo, Department of Informatics, PB 1080 Blindern, 0316 Oslo, Norway. E-mail: {erikve,liljao,jread,oe}@ifi.uio.no. Submission received: 5 April 2011; revised submission received: 30 September 2011; accepted for publication: 2 December 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 2 (Farkas et al. 2010). This particular area has been the focus of much current research, triggered by the release of the BioScope corpus (Vincze et al. 2008)—a collection of scientiﬁc abstracts, full papers, and clinical reports with manual annotations of words that signal speculation or negation (so-called cues), as well as of the scopes of these cues within the sentences. The following examples from BioScope illustrate how sentences are annotated with respect to speculation. Cues are here shown using angle brackets, with braces corresponding to their annotated scopes: (1) {The speciﬁc role of the chromodomain is unknown} but chromodomain swapping experiments in Drosophila {suggest that they {might be protein interaction modules}} [18]. (2)"
J12-2005,W10-3003,0,0.0796274,"Missing"
J12-2005,P09-1043,0,0.0183721,"nd genres, we make our parser incorporate the predictions of a large-scale, general-purpose Lexical-Functional Grammar parser. A technique dubbed parser stacking enables the data-driven parser to learn from the output of another parser, in addition to gold-standard treebank annotations (Martins et al. 2008; Nivre and McDonald 2008). This technique has been shown to provide signiﬁcant improvements in accuracy for both English and German (Øvrelid, Kuhn, and Spreyer 2009), and a similar set-up using an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang 2009). The stacked parser used here is identical to the parser described in Øvrelid, Kuhn, and Spreyer (2009), except for the preprocessing in terms of tokenization and PoS tagging, which is performed as detailed in Sections 2.1–2.2. The parser combines two quite different approaches—data-driven dependency parsing and “deep” parsing with a hand-crafted grammar—and thus provides us with a broad range of different types of linguistic information to draw upon for the speculation resolution task. MaltParser is based on a deterministic parsing strategy in combination with treebank-induced classiﬁers for"
J12-2005,P11-2033,0,0.0199828,"nt|ntype:common _ 4 4 4 3 0 7 5 9 10 7 0 SPECDET ADJUNCT ADJUNCT SUBJ ROOT PHI XCOMP PHI SPECDET OBL-AG PUNC 4 4 4 5 0 5 6 7 10 8 5 NMOD NMOD NMOD SBJ ROOT VC VC LGS NMOD PMOD P to dependency format (Johansson and Nugues 2007) and extended with XLE features, as described previously. Parsing uses the arc-eager mode of MaltParser and an SVM with a polynomial kernel. When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a labeled accuracy score of 89.8, which is lower than the current stateof-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE. 6.1.2 Rule Overview. Our scope resolution rules take as input a parsed sentence that has been further tagged with speculation cues. We assume the default scope to start at the cue word and span to the end of the sentence (modulo punctuation), and this scope also provides the baseline when evaluating our rules. In developing the rules, we made use of the information provided by the guidelines for scope annotation in the"
J12-2005,D10-1070,0,0.0588464,"Missing"
J12-2005,W07-2207,1,\N,Missing
J12-2005,W10-3001,0,\N,Missing
J12-2005,E99-1043,0,\N,Missing
J16-4009,W13-0101,0,0.0858078,"d arguably increasing) levels of abstraction over the surface signal and its syntactic structure, viz. (a) Combinatory Categorial Grammar word–word dependencies (CCD); (b) Semantic Dependency Parsing targets from SemEval 2014 and 2015 (SDP); (c) the Elementary Dependency Structures (EDS) of Oepen and Lønning (2006); and (d) Abstract Meaning Representation (AMR; Banarescu et al. 2013). Additional candidate graph banks for inclusion in a community-maintained on-line catalogue are, for example, the Groningen Meaning Bank (GMB; Basile et al. 2012), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013), as well as combinations of layers of annotations from the Penn Treebank (PTB; Marcus, Santorini, and Marcinkiewicz 1993) and OntoNotes (Hovy et al. 2006) ecosystems. Also, recent work on “deeper” syntax (Ballesteros et al. 2015) and the Universal Dependencies initiative (de Marneffe et al. 2014) push towards increasing use of non-tree structures. There are multiple linguistic and formal differences between these resources. Most importantly, CCD and SDP represent bilexical dependencies, where graph nodes correspond to surface lexical units (words or tokens). In contrast, EDS and AMR take the"
J16-4009,W13-2322,0,0.562349,"er graph banks that are generally available (through the Linguistic Data Consortium) and have already been applied in training and evaluation of data-driven parsers. To capture relevant variation, this selection represents different (and arguably increasing) levels of abstraction over the surface signal and its syntactic structure, viz. (a) Combinatory Categorial Grammar word–word dependencies (CCD); (b) Semantic Dependency Parsing targets from SemEval 2014 and 2015 (SDP); (c) the Elementary Dependency Structures (EDS) of Oepen and Lønning (2006); and (d) Abstract Meaning Representation (AMR; Banarescu et al. 2013). Additional candidate graph banks for inclusion in a community-maintained on-line catalogue are, for example, the Groningen Meaning Bank (GMB; Basile et al. 2012), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013), as well as combinations of layers of annotations from the Penn Treebank (PTB; Marcus, Santorini, and Marcinkiewicz 1993) and OntoNotes (Hovy et al. 2006) ecosystems. Also, recent work on “deeper” syntax (Ballesteros et al. 2015) and the Universal Dependencies initiative (de Marneffe et al. 2014) push towards increasing use of non-tree structures. There are"
J16-4009,N10-1138,0,0.0187963,"al language parsing traditionally have been trees, in the formal sense that every node is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central role of feature stru"
J16-4009,de-marneffe-etal-2014-universal,0,0.056497,"Missing"
J16-4009,P15-1149,0,0.0311655,"Missing"
J16-4009,P14-1134,0,0.441658,"the formal sense that every node is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central role of feature structures in unification-based grammar formalisms arguably ma"
J16-4009,P10-1035,0,0.0194341,"actic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank (LDC2005 T13), the strings of the venerable PTB Wall Street Journal (WSJ) corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bilexical dependency triples, which we term CCD. The latter “include most semantically relevant non-anaphoric local and long-range dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. Although CCD has mainly been used for contrastive parser evaluation (Clark and Curran 2007, Fowler and Penn 2010; among others), there is current work that views each set of triples 820 Kuhlmann and Oepen Towards a Catalogue of Linguistic Graph Banks as a directed graph and parses directly into these target representations (Du, Sun, and Wan 2015). SDP 2014 and 2015: DM and PSD. For the SDP tasks at SemEval, Oepen et al. (2014, 2015) prepared aligned sets of semantic dependency graphs over the same WSJ text by reduction (i.e., lossy conversion) of independently developed syntactico-semantic treebanks into bilexical semantic dependencies. SDP (LDC2016 T10) comprises multiple linguistic frameworks, but for"
J16-4009,hajic-etal-2012-announcing,0,0.23683,"Missing"
J16-4009,N06-2015,0,0.0149941,"CD); (b) Semantic Dependency Parsing targets from SemEval 2014 and 2015 (SDP); (c) the Elementary Dependency Structures (EDS) of Oepen and Lønning (2006); and (d) Abstract Meaning Representation (AMR; Banarescu et al. 2013). Additional candidate graph banks for inclusion in a community-maintained on-line catalogue are, for example, the Groningen Meaning Bank (GMB; Basile et al. 2012), Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport 2013), as well as combinations of layers of annotations from the Penn Treebank (PTB; Marcus, Santorini, and Marcinkiewicz 1993) and OntoNotes (Hovy et al. 2006) ecosystems. Also, recent work on “deeper” syntax (Ballesteros et al. 2015) and the Universal Dependencies initiative (de Marneffe et al. 2014) push towards increasing use of non-tree structures. There are multiple linguistic and formal differences between these resources. Most importantly, CCD and SDP represent bilexical dependencies, where graph nodes correspond to surface lexical units (words or tokens). In contrast, EDS and AMR take the form of semantic networks (or conceptual graphs), where nodes represent concepts and there need not be an explicit mapping to surface linguistic forms. In"
J16-4009,W12-3602,1,0.575347,"y into these target representations (Du, Sun, and Wan 2015). SDP 2014 and 2015: DM and PSD. For the SDP tasks at SemEval, Oepen et al. (2014, 2015) prepared aligned sets of semantic dependency graphs over the same WSJ text by reduction (i.e., lossy conversion) of independently developed syntactico-semantic treebanks into bilexical semantic dependencies. SDP (LDC2016 T10) comprises multiple linguistic frameworks, but for our pilot comparison we focus on two sets of target representations that are not derivative of the PTB, viz. (a) DELPH-IN MRS-Derived Dependencies (DM; Oepen and Lønning 2006, Ivanova et al. 2012) and (b) Prague Semantic Dependencies (PSD; Hajiˇc et al. 2012, Miyao, Oepen, and Zeman 2014). Both are rooted in general theories of grammar—Head-Driven Phrase Structure Grammar (Pollard and Sag 1994) and Prague Functional Generative Description (FGD; Sgall, Hajiˇcová, and Panevová 1986), respectively—and there are numerous current reports on parsing into these target representations. Elementary Dependency Structures (EDS). The DM bilexical dependencies originally derive from the underspecified logical forms of Copestake et al. (2005), which Oepen and Lønning (2006), by elimination of scope c"
J16-4009,W13-1810,0,0.245433,"Missing"
J16-4009,Q15-1040,1,0.810581,"Missing"
J16-4009,P06-2066,1,0.360948,"Missing"
J16-4009,P98-1116,0,0.0297687,"Representation (AMR). Unlike the bilexical dependency graphs of CCD, DM, and PSD, AMR eschews explicit syntactic derivations and consideration of the syntax–semantics interface; it rather seeks to directly annotate “whole-sentence logical meanings” (Banarescu et al. 2013). Node labels in AMR name abstract concepts, which in large part draw on the ontology of OntoNotes predicate senses and corresponding semantic roles. Nodes are not overtly related to surface lexical units, and thus are unordered. Although AMR has its roots in semantic networks and earlier knowledge representation approaches (Langkilde and Knight 1998), largerscale manual AMR annotation is a recent development only. We sample two variants of AMR, viz. (a) the graphs as annotated in AMRBank 1.0 (LDC2014 T12), and (b) a normalized version that we call AMR−1 , where so-called “inverse roles” (like ARG0-of) are reversed. Such inverted edges are frequently used in AMR in order to render the graph as a single rooted structure, where the root is interpreted as the top-level focus.1 In Section 3, we map this interpretation to our concept of top nodes for both AMR and AMR−1 . Flanigan et al. (2014) published the first parser targeting AMR, and the s"
J16-4009,J93-2004,0,0.0578282,"Missing"
J16-4009,S14-2082,0,0.0382965,"is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central role of feature structures in unification-based grammar formalisms arguably marks an exception to this c"
J16-4009,S14-2056,1,0.928126,"Missing"
J16-4009,S14-2008,1,0.7848,"Missing"
J16-4009,C08-1095,0,0.0552776,"epresentations in natural language parsing traditionally have been trees, in the formal sense that every node is reachable from a distinguished root node by exactly one directed path. With a gradual shift of emphasis from more surfaceoriented, morpho-syntactic target representations in parsing towards “deeper,” more semantic analyses, there is increasing interest in processing structures where characteristic properties of trees like the unique root, connectedness, or lack of reentrancies can be relaxed. Some recent parsing work targets graph-structured representations more general than trees (Sagae and Tsujii 2008, Das et al. 2010, Jones, Goldwater, and Johnson 2013, Flanigan et al. 2014, Martins and Almeida 2014; among others). This development is made possible by ongoing efforts to annotate deeper syntacticosemantic analyses at scale, and typically such annotations either directly take the form of directed graph structures, or can be interpreted as such under moderate transformations. In computational linguistics and in particular in natural language parsing, however, there is less of an established tradition of using general graphs than in, say, theoretical computer science (although the central rol"
J16-4009,S15-1031,0,0.269151,"Missing"
J16-4009,J07-4004,0,\N,Missing
J16-4009,C98-1112,0,\N,Missing
J16-4009,P13-1091,0,\N,Missing
J16-4009,J07-3004,0,\N,Missing
J16-4009,P13-2131,0,\N,Missing
J16-4009,basile-etal-2012-developing,0,\N,Missing
J16-4009,D15-1136,0,\N,Missing
J16-4009,oepen-lonning-2006-discriminant,1,\N,Missing
K16-2002,J12-2005,1,0.934737,"o an overall account of discourse structure and of having annotation decisions concentrate on the individual instances of discourse relations, rather than on their interactions. Previous work on this task has usually broken it down into a set of sub-problems, which are solved in a pipeline architecture (roughly: identify connectives, then arguments, then discourse senses; Lin et al., 2014). While adopting a similar pipeline approach, the OPT discourse parser also builds on and extends a method that has previously achieved state-of-the-art results for the detection of speculation and negation (Velldal et al., 2012; Read 3 Relation Identification Explicit Connectives Our classifier for detecting explicit discourse connectives extends the work by Velldal et al. (2012) for identifying expressions of speculation and negation. The approach treats the set of connectives observed in the training data as a closed class, and ‘only’ attempts to disambiguate occurrences of these token sequences in new data. Connectives can be single- or multitoken sequences (e.g. ‘as’ vs. ‘as long as’). In cases 20 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 20–26, c Berlin, Germ"
K16-2002,K15-2002,0,0.285154,"r sense classifier described has been developed specifically for OPT. The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a ‘classic’ pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and ‘editing’ of syntactic constituents for argument identification, and an ensemble of classifiers to assign discourse senses. With an end-toend performance of 27.77 F1 on the English ‘blind’ test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F1 points, with particularly good results for the argument identification sub-tasks. 1 2 System Architecture Our system overview is shown in Figure 1. The individual modules interface through JSON files which resemble the desired output files of the Task. Each module adds the information specified for it. We will describe them here in thematic blocks, while the exact order of the modules can be seen in the figure. Relation identification (§3) includes the detection of explicit discourse connectives and the stipulation of non-explicit relations. Our argument identification mod"
K16-2002,K15-2001,0,0.136259,"21.6 48.1 27.8 F1 91.8 52.4 75.2 44.0 34.5 64.6 76.4 52.0 21.9 48.2 27.8 Table 3: Per-component breakdown of system performance, compared to top performers in 2015/16. 6 optimizing the primal objective and setting the error penalty term C to 0.3. Experimental Results Overall Results Table 3 summarizes OPT system performance in terms of the metrics computed by the official scorer for the Shared Task, against both the WSJ and ‘blind’ test sets. To compare against the previous state of the art, we include results for the top-performing systems from the 2015 and 2016 competitions (as reported by Xue et al., 2015, and Xue et al., 2016, respectively). Where applicable, best results (when comparing F1 ) are highlighted for each sub-task and -metric. The highlighting makes it evident that the OPT system is competitive to the state of the art across the board, but particularly so on the argument identification sub-task and on the ‘blind’ test data: In terms of the WSJ test data, OPT would have ranked second in the 2015 competition, but on the ‘blind’ data it outperforms the previous state of the art on all but one metric for which contrastive results are provided by Xue et al.. Where earlier systems tend"
K16-2002,P09-2004,0,0.792028,"ve heads only, these are the unit of disambiguation in OPT. Disambiguation is performed as point-wise (‘per-connective’) classification using the support vector machine implementation of the SVMlight toolkit (Joachims, 1999). Tuning of feature configurations and the error-to-margin cost parameter (C) was performed by ten-fold cross validation on the Task training set. candidate configurations against the development data. The model used in the system submission includes n-grams of up to three preceding and following positions, full feature conjunction for the ‘self’ and ‘parent’ categories of Pitler & Nenkova (2009), but limited conjunctions involving their ‘left’ and ‘right’ sibling categories, and none of the ‘connected context’ features suggested by Wang & Lan (2015). This model has some 1.2 million feature types. Non-Explicit Relations According to the PDTB guidelines, non-explicit relations must be stipulated between each pair of sentences iff four conditions hold: two sentences (a) are adjacent; (b) are located in the same paragraph; and (c) are not yet ‘connected’ by an explicit connective; and (d) a coherence relation can be inferred or an entity-based relation holds between them. We proceed stra"
K16-2002,prasad-etal-2008-penn,0,\N,Missing
K16-2002,K15-2003,0,\N,Missing
K16-2002,S12-1041,1,\N,Missing
K16-2002,K16-2001,0,\N,Missing
K18-2002,J13-3002,1,0.895149,"negation metric used in the EPE context—counting as true positives only perfectly retrieved full scopes, including an exact match on negated events. and PoS tags. Conversely, the opinion holder extraction and reranking modules make central use of structural information, i.e. paths and topological properties in one or more syntactico-semantic dependency graph(s). In the EPE context, we evaluated how well the participating systems extract the three types of structures mentioned above: expressions, holders, and polarities. In each case, soft-boundary precision and recall measures were computed (Johansson and Moschitti, 2013; Johansson, 2017). Furthermore, for the detailed analysis we evaluated the opinion holder extractor separately, using goldstandard opinion expressions. We refer to this task as in-vitro holder extraction, and this score is used for the overall ranking of submissions when averaging F1 scores across the three EPE downstream applications. The reason for highlighting this score is that it is the one most strongly affected by the design of the dependency representation. Participating Teams Nine teams participated in EPE 2017, in the order of overall rank: Stanford– Paris (Schuster et al., 2017), S"
K18-2002,W09-1401,0,0.358818,"generated to a large degree from syntactic dependency parses. All classification tasks are implemented using the SVMmulticlass classifier (Joachims, 1999). TEES has been developed using corpora from the Biomedical Natural Language Processing (BioNLP) domain, in particular the event corpora from the BioNLP Shared Tasks. These tasks define their own annotation schemes and provide standardized evaluation services. In the context of the EPE challenge we use the BioNLP 2009 GENIA corpus and its associated evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the depend"
K18-2002,J16-4009,1,0.850677,"ation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribute–value matrix (or ‘feature structure’), for example to encode lemma and part of speech information. Each graph can optionally designate one or more ‘top’ nodes, broadly interpreted as the root-level head or highest-scoping predicate (Kuhlmann and Oepen, 2016). In principle, this notion of dependency representations is broad in that it allows nodes that do not correspond to (full) surface tokens, partial or full overlap of nodes, as well as graphs that transcend fully connected rooted trees. Participating teams in the original EPE 2017 initiative did in fact take advantage of all these degrees of freedom, whereas in connection to the 2018 UD parsing task such variation is excluded by design. Biological Event Extraction The Turku Event Extraction System (TEES) (Björne, 2014) is a program developed for the automated extraction of events, complex rela"
K18-2002,S12-1042,1,0.849881,"on tasks. The first step is entity detection where each token in the sentence is predicted as an entity node or as negative. In the second step of edge detection, argument edges are predicted for all valid, directed pairs of nodes. In the third, unmerging step, overlapping events are ‘pulled apart’ by duplicating trigger nodes. In the optional fourth step of modifier detection, binary modifiers (such as speculation or negation) can be predicted for the detected events. All of the classification steps in the TEES system Negation Resolution The EPE negation resolution system is called Sherlock (Lapponi et al., 2012, 2017) and implements the perspective on negation defined by Morante and Daelemans (2012) through the creation of the Conan Doyle Negation Corpus for the Shared Task of the 2012 Joint Conference on Lexical and Computational Semantics (*SEM 2012). Negation instances are annotated as tri-partite structures: Negation cues can be full tokens (e.g. not), multi-word expressions (by no means), or sub-tokens (un in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as"
K18-2002,P10-1052,0,0.0518508,"in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as negated events or states, provided that the sentence is factual and the events in question did not take place. In the EPE context, gold-standard negation cues are provided, because this sub-task has been found relatively insensitive to grammatical structure (Velldal et al., 2012). Sherlock approaches negation resolution as a sequence labeling problem, using a Conditional Ran23 dom Field (CRF) classifier (Lavergne et al., 2010). The token-wise negation annotations contain multiple layers of information. Tokens may or may not be negation cues and they can be either in or out of scope for a specific cue; in-scope tokens may or may not be negated events. Moreover, multiple negation instances may be (partially or fully) overlapping. Before presenting the CRF with the annotations, Sherlock ‘flattens’ all negation instances in a sentence, assigning a six-valued extended ‘begin– inside–outside’ labeling scheme. After classification, hierarchical (overlapping) negation structures are reconstructed using a set of post-proces"
K18-2002,de-marneffe-etal-2006-generating,0,0.18263,"Missing"
K18-2002,W12-3602,1,0.835691,"sity of Washington (Peng et al., 2017). These teams submitted 49 distinct runs that encompassed many different families of dependency representations, various approaches to preprocessing and parsing, and variable types and volumes of training data. The dependency representations employed by the participants varied from more syntactically oriented schemes—e.g. Stanford Basic (de Marneffe et al., 2006), CoNLL 2008–style (Surdeanu et al., 2008), and UD—to more semantically oriented representations, such as the Deep Syntactic Structures of Ballesteros et al. (2015), DELPH-IN MRS Dependencies (DM; Ivanova et al., 2012), or Enju Predicate–Argument Structures (PAS; Miyao, 2006). The teams also employed wildly variable volumes of training data, ranging from around 200,000 tokens (the English UD treebanks) to 1,7 million tokens (combining the venerable Wall Street Journal, Brown, and GENIA treebanks). Opinion Analysis The system by Johansson and Moschitti (2013) marks up expressions of opinion and emotion in a pipeline comprised of three separate classification steps, combined with endto-end reranking; it was previously generalized and adapted for the EPE framework by Johansson (2017). The system is based on th"
K18-2002,P13-2017,0,0.136318,"Missing"
K18-2002,S12-1035,0,0.0462694,"of the classifier include different combinations of token-level observations, such as surface forms, part-of-speech tags, lemmas, and dependency labels. In addition, we extract both token and dependency distance to the nearest cue, together with the full shortest dependency path. Standard evaluation measures from the original shared task include scope tokens (ST), scope match (SM), event tokens (ET), and full negation (FN) F1 scores. ST and ET are token-level scores for inscope and negated event tokens, respectively, where a true positive is a correctly retrieved token of the relevant class (Morante and Blanco, 2012). FN is the strictest of these measures and the primary negation metric used in the EPE context—counting as true positives only perfectly retrieved full scopes, including an exact match on negated events. and PoS tags. Conversely, the opinion holder extraction and reranking modules make central use of structural information, i.e. paths and topological properties in one or more syntactico-semantic dependency graph(s). In the EPE context, we evaluated how well the participating systems extract the three types of structures mentioned above: expressions, holders, and polarities. In each case, soft"
K18-2002,I05-2038,0,0.173019,"ted evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the dependencies combined wit the text span and a single part of speech for the tokens; lemmas are not used. The term (bi-lexical) dependency representation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribut"
K18-2002,J12-2005,1,0.852735,"tances are annotated as tri-partite structures: Negation cues can be full tokens (e.g. not), multi-word expressions (by no means), or sub-tokens (un in unfortunate); for each cue, its scope is defined as the possibly discontinuous sequence of (sub-)tokens affected by the negation. Additionally, a subset of in-scope tokens can be marked as negated events or states, provided that the sentence is factual and the events in question did not take place. In the EPE context, gold-standard negation cues are provided, because this sub-task has been found relatively insensitive to grammatical structure (Velldal et al., 2012). Sherlock approaches negation resolution as a sequence labeling problem, using a Conditional Ran23 dom Field (CRF) classifier (Lavergne et al., 2010). The token-wise negation annotations contain multiple layers of information. Tokens may or may not be negation cues and they can be either in or out of scope for a specific cue; in-scope tokens may or may not be negated events. Moreover, multiple negation instances may be (partially or fully) overlapping. Before presenting the CRF with the annotations, Sherlock ‘flattens’ all negation instances in a sentence, assigning a six-valued extended ‘beg"
K18-2002,P17-1186,0,0.022577,"Missing"
K18-2002,K18-2001,0,0.121439,"Missing"
K19-2001,W13-0101,1,0.863351,"or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanchored. The UCCA graph for the running example (see the bottom of Figure 2) includes a single scene, whose main relation is the Process (P) evoked by apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such t"
K19-2001,D15-1198,0,0.0255627,"9 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating"
K19-2001,K19-2008,0,0.403223,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,D17-1130,0,0.100948,"Missing"
K19-2001,W13-2322,0,0.179348,"aphs need not be rooted trees: Argument sharing across units will give rise to reentrant nodes much like in the other frameworks. For example, technique in Figure 2 is both a Participant in the scene evoked by similar and a Center in the parent unit. UCCA in principle also supports implicit (unexpressed) units which do not correspond to any tokens, but these are currently excluded from parsing evaluation and, thus, suppressed in the UCCA graphs distributed in the context of the shared task. Abstract Meaning Representation Finally, the shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchor6 DM PSD EDS UCCA AMR Flavor 0 0 1 1 2 TRAIN Text Type Sentences Tokens newspaper 35,656"
K19-2001,S16-1176,0,0.0127804,"ition-based, as well as sequence-to-sequence systems. Existing 19 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target"
K19-2001,basile-etal-2012-developing,0,0.0638169,"Missing"
K19-2001,W15-0128,1,0.854139,"ing example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first recast as Elementary Dependency Structures (EDS; Oepen and Lønning, 2006; see below) and then further simplified into pure bi-lexical semantic dependencies, dubbed DELPH-IN MRS Bi-Lexical Dependencies (or DM). As a Flavor (0) framework, graph nodes in DM are restricted to surface tokens. But DM graphs are neither lexica"
K19-2001,D16-1134,1,0.82449,"y Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior"
K19-2001,P17-1112,0,0.291422,"vich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment a"
K19-2001,P13-2131,0,0.502571,"range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design shies away from such formal purit"
K19-2001,K19-2006,0,0.10424,"Missing"
K19-2001,K19-2013,0,0.128351,"Missing"
K19-2001,W11-2927,1,0.84568,"quely determined as the character range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design sh"
K19-2001,P12-2074,1,0.629774,"and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid overlap of morpho-syntactic training data with the texts underlying the semantic graphs of the shared task, we performed five-fold jack-knifing on the WSJ and EWT corpora. For compatibility with the majority of the training data, the ‘raw’ input strings for the MRP semantic graphs were tokenized using the PTB-style REPP rules of Dridan and Oepen (2012) and input to UDPipe in pre-tokenized form. Whether as merely a source of state-of-the-art PTBstyle tokenization, or as a vantage point for approaches to meaning representation parsing that start from explicit syntactic structure, the optional morpho-syntactic companion data offers community value in its own right. respectively; see Table 2. The shared task has, for the first time, repackaged the five graph banks into a uniform and normalized abstract representation with a common serialization format. The common interchange format for semantic graphs implements the abstract model of Kuhlmann a"
K19-2001,K19-2007,0,0.24925,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,K19-2015,0,0.0373872,"Missing"
K19-2001,P18-1038,0,0.0751759,"recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment as a preprocessing step during training. They developed their own rule-based alignment method, complemented by Pourdamghani et al. (2014),"
K19-2001,K19-2016,0,0.133819,"Missing"
K19-2001,S14-2080,0,0.0289716,"lti-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results"
K19-2001,S15-2154,0,0.216771,"Missing"
K19-2001,N18-2020,1,0.780781,"iversal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanch"
K19-2001,S16-1186,0,0.0968157,"Missing"
K19-2001,N18-1104,0,0.0451061,"iding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by usi"
K19-2001,P14-1134,0,0.12208,"of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into the sum of the scores of its nodes and edges. A popular choice for predicting the edge is to feed the output of an LSTM encoder to a biaffine classifier to predict if an edge exists between a pair of nodes as well as the label of the edge (SJTU– NICT, SUDA–Alibaba, Hitachi, and JBNU), with slight variations as to the input to the LSTM encoder. Due to the difference in anchoring betw"
K19-2001,S15-2161,0,0.0179913,"and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text"
K19-2001,kingsbury-palmer-2002-treebank,0,0.377337,"iyadarshi, 2017). The AMR example graph in Figure 3 has a topology broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 3 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 2. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 3 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
K19-2001,hajic-etal-2012-announcing,0,0.575983,"Missing"
K19-2001,P19-4002,1,0.832307,"listed. 12 tial submissions declined the invitation to submit a system description for publication in the shared task proceedings (and one team asked to remain anonymous), such that only limited information is available about these parsers, and they will not be considered in further detail in §7. Finally, based on input by task participants, Table 4 also provides an indication of which submissions employed multi-task learning (MTL) and a high-level characterization of the overall parsing approach. The distinction between transition-, factorization-, and composition-based architectures follows Koller et al. (2019) and is discussed in more detail in §7 below. In some submissions there can of course be elements of more than one of these high-level architecture types. Also, not all of the teams who indicate the use of multi-task learning actually apply it across different semantic graph frameworks, but in some cases rather to multiple sub-tasks within the parsing architecture for a single framework.11 The main task results are summarized in Table 6, showing average MRP scores across frameworks, broken down by the different component pieces (see §5 above). These cross-framework averages can only be meaning"
K19-2001,P17-1104,1,0.901739,"e, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BEL in the version of TUPA used in the shared task. CUHK developed a transition-based parser with a general transition system suited for all five frameworks, by including a variable-arity R ESOLVE action. F"
K19-2001,P17-1014,0,0.0341005,"ly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the s"
K19-2001,P18-1035,1,0.82103,"rticipants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; or Stanovsky and Dagan, 2018). The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the training and evaluation data for the task, packaged in a uniform graph abstraction and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of extra training data,"
K19-2001,K19-2011,0,0.145598,"3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by the symbol “∦”), arrived after the closing deadli"
K19-2001,S19-2001,1,0.68669,"ng representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-level analysis beyond surfac"
K19-2001,J16-4009,1,0.942174,"ng (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning 1 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 1–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2001 tations across frameworks, the MRP 2019 shared task is regrettably limited to parsing English for the time being. 2 A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. The following paragraphs provide semi-formal definitions of core graph-theoretic concepts that can be meaningfully applied across the range of frameworks represented in the shared task. Hierarchy of Formal Flavors"
K19-2001,K19-2002,1,0.70568,"ing parser outputs for all target frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission,"
K19-2001,K19-2010,0,0.116608,"Missing"
K19-2001,K19-2004,0,0.0682072,"Missing"
K19-2001,W12-3602,1,0.783003,"NNS n rice NN n ADDR.m ACT ADDR.m PAT RSTR technique NN as IN p conj ADDR.m PAT similar JJ ARG2 ADDR.m EXT be almost VBZ RB ev-w218f2 impossible JJ RSTR apply other VB JJ ev-w119f2 CONJ.m APPS.m crop NNS as IN APPS.m cotton NN CONJ.m soybean NNS CONJ.m and CC rice NN Figure 1: Bi-lexical semantic dependencies for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first"
K19-2001,W19-3304,1,0.80829,"dicated in each cell: ShanghaiTech, for example, ranks much higher in the framework-specific SDP metric than in the official MRP ranks. These divergences likely reflect the more limited scope of the SDP approach to scoring, which essentially only considers labeled edges (and top nodes, as a pseudo-edge) but ignores node labels, properties, and anchors (which all used to be provided as part of the parser inputs in the original SDP parsing tasks; see §3 above). trastive, phenomena-oriented studies would likely be called for, as for example the comparison of parsing accuracies for EDS vs. AMR by Lin and Xue (2019). 7 Overview of Approaches The participating systems in the shared task have approached this multi-meaning representation task in a variety of ways, which we characterize into three broad families of approaches: transition-, factorization-, or composition-based architectures. Transition-Based Architectures In these parsing system, the meaning representation graph is generated via a series of actions, in a process that is very similar to dependency tree parsing, with the difference being that the actions for graph parsing need to allow reentrancies, as well as (possibly) non-token nodes, labels"
K19-2001,S19-2002,0,0.0756992,"Missing"
K19-2001,P19-1450,0,0.171664,"used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results on PSD. Lindemann et al. (2019) trained a composition-based parser on DM, PAS, PSD, AMR and EDS, using the Apply–Modify algebra, on which the Saarland submission to the shared task is based. They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. UCCA parsing was first tackled by Hershcovich et al. (2017), who used a neural transition-based parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared t"
K19-2001,S17-2156,0,0.033905,"Missing"
K19-2001,P18-1037,0,0.260682,"le anchoring in the parsing system has a significant impact on parser performance. Some of the participating systems follow early approaches in AMR parsing and use a separate ‘alignment’ model to provide hard anchorings and then proceed with the rest of the parsing process (e.g. the HIT-SCIR system) assuming the alignments are already in place. Other submissions use a soft alignment component that is trained jointly with other components of their systems. For example, the Amazon and the SUDA– Alibaba parsers jointly model anchoring, node detection, and edge detection, adopting the approach of Lyu and Titov (2018), while the SJTU–NICT system uses a sequence-to-sequence model with a pointer-generator network to predict the concepts in AMR, following Zhang et al. (2019a). That sequence-to-sequence model is trained jointly with other components of their system. Benefits of Multi-Task Learning Another research question the shared task seeks to advance is whether and how multi-task learning (MTL) helps with multi-framework meaning representation parsing. The term, in fact, seems to be applied somewhat variably in the system descriptions. In one sense, it is equated with traditional joint learning, where dif"
K19-2001,W03-3017,0,0.384782,"t and a buffer for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the n"
K19-2001,J93-2004,0,0.0674913,"DM are restricted to surface tokens. But DM graphs are neither lexically fully covering nor rooted trees, i.e. some tokens do not contribute to the graph, and for some nodes there are multiple incoming edges. In the example DM graph in Figure 1, technique semantically depends on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacusection reviews the frameworks and presents example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 3 are presented in order of (arguably) increasing ‘abstraction’ from the surface string, i.e. ranging from ordered Flavor (0) to unanchored Flavor (2). Two of the frameworks in the shared task present simplifications into bi-lexical semantic dependencies (i."
K19-2001,S16-1166,0,0.537725,"‘balkanization’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate targe"
K19-2001,W17-7306,0,0.0913721,"Missing"
K19-2001,S17-2090,0,0.515681,"tion’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-l"
K19-2001,K19-2003,1,0.784391,"rget frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission, the extra training data is li"
K19-2001,P13-2017,0,0.074515,"Missing"
K19-2001,S15-2153,1,0.900924,"Missing"
K19-2001,E06-1011,0,0.0181086,"of the Saarland parser, the lexical items are produced by a BiLSTM-based supertagger, and the best derivation is selected in a tree dependency parsing process where the edge between a head and its argument or modifier is labeled with the derivation operation. In the case of the Peking system, the SHRG rules are extracted with a context-free parser, and the derivation is scored by a sum of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into t"
K19-2001,S14-2008,1,0.915565,"Missing"
K19-2001,S14-2056,1,0.708772,"g covert quantifiers (e.g. on bare nominals, labeled udef q3 ), the two-place such+as p relation, as well as the implicit conj(unction) relation (which reflects recursive decomposition of the coordinate structure Prague Semantic Dependencies Another instance of simplification from richer syntacticosemantic representations into Flavor (0) bi-lexical semantic dependencies is the reduction of tectogrammatical trees (or t-trees) from the linguistic school of Functional Generative Description (FGD; Sgall et al., 1986; Hajiˇc et al., 2012) into what are called Prague Semantic Dependencies (or PSD). Miyao et al. (2014) sketch the nature of this conversion, which essentially collapses empty (or generated, in FGD terminology) t-tree nodes with corresponding surface nodes and forward-projects incoming dependencies onto all members of paratactic constructions, e.g. the appositive and coordinate structures in the bottom of Figure 1. The PSD graph for our running example has many of the same dependency edges as the DM one (albeit using a different labeling scheme and inverse directionality in a few cases), but it analyzes the predicative copula as semantically contentful and does not treat almost as ‘scoping’ ove"
K19-2001,K19-2009,0,0.0353841,"Missing"
K19-2001,N12-2006,0,0.0317062,"parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014),"
K19-2001,P18-1173,0,0.0339442,"Missing"
K19-2001,N18-1135,0,0.164463,"Missing"
K19-2001,K19-2012,1,0.876098,"Missing"
K19-2001,W15-3502,1,0.870363,"y apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 3 has a topology broa"
K19-2001,D14-1048,0,0.0260749,"quences of ‘raw’ sentence strings and (b) in pretokenized, part-of-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premiumquality English morpho-syntactic analyses were provided to participants, described in more detail below. These parser outputs are referred to as the MRP 2019 morpho-syntactic companion trees. Additional companion data available to participants includes automatically generated reference anchorings (commonly called ‘alignments’ in AMR parsing) for the AMR graphs in the training data, obtained from the JAMR and ISI tools of Flanigan et al. (2016) and Pourdamghani et al. (2014), respectively. Because some of the semantic graph banks involved in the shared task had originally been released by the Linguistic Data Consortium (LDC), the training data was made available to task participants by the LDC under no-cost evaluation licenses. Upon completion of the competition, all task data (including system submissions and evaluation results) are being prepared for general release through the LDC, while those subsets that are copyright-free will also become available for direct, open-source download. Additional Resources For reasons of comparability and fairness, the shared t"
K19-2001,P18-1016,1,0.787871,"hnique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Elementary Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of"
K19-2001,L16-1376,0,0.145661,"Missing"
K19-2001,I05-2038,0,0.0146077,"he full novel have long served as a common reference point for AMR, and gold-standard DM and EDS graphs could be converted from the ERS inter-annotator agreement study by Bender et al. (2015). For PSD and UCCA, the 100-sentence subset used for MRP evaluation has been annotated specifically for the shared task. 5 See http://svn.nlpl.eu/mrp/2019/public/ resources.txt for the full list of seventeen generally available third-party resources, including a broad range of large English corpora and distributed word representations. 8 Corpus, as well as to the PTB-style annotations of the GENIA Corpus (Tateisi et al., 2005). This conversion targets Universal Dependencies (UD; McDonald et al., 2013; Nivre, 2015) version 2.x, so that the resulting gold-standard annotations could be concatenated with the UD English Web Treebank (Silveira et al., 2014), for a total of 2.2 million tokens annotated with lemmas, Universal and PTBstyle parts of speech, and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid ove"
K19-2001,silveira-etal-2014-gold,0,0.0451563,"Missing"
K19-2001,N18-2040,1,0.872943,"Missing"
K19-2001,P19-1446,0,0.0232282,"Missing"
K19-2001,D17-1129,1,0.913091,"Missing"
K19-2001,N15-1040,1,0.925482,"Missing"
K19-2001,K19-2005,0,0.0368897,"Missing"
K19-2001,W03-3023,0,0.0220925,"r for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BE"
K19-2001,P19-1009,0,0.3054,"Missing"
K19-2001,D19-1392,0,0.220016,"Missing"
K19-2001,K19-2014,0,0.380663,"rov 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by t"
K19-2003,K19-2008,0,0.0425329,"al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, in the sense that they do not incorporate manually curated linguistic knowledge (beyond finite-state tokenization rules, maybe) but rather learn all their parameters exclusively from the shared task training data. By and large, the data-driven parsers are competitive to the ERG, in particular the SJTU–NICT and HIT-SCIR systems for DM, and the Peking parser for EDS. For some structural types of graph components (tops and edges), the ERG is in fact outperformed by some submissions"
K19-2003,W15-0128,1,0.876913,"all utterances in standard corpora, including newspaper text, the English Wikipedia, or bio-medical research literature (Flickinger et al., 2017). Parsing times for these data sets measure in seconds per sentence, time comparable to human production or comprehension. Second, since around 2001 the ERG has been accompanied by a selection of development corIntroduction Two of the target representations in the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) achieve simplification of ERS into labeled directed graphs by elimination of most of the information regarding scope underspecificati"
K19-2003,I05-1015,1,0.518071,". (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treebank is built exclusively from ERG analyses, i.e. full HPSG syntactico-semantic signs. Annotation in Redwoods amounts to disambiguation among the candidate analyses derived by the grammar (identifying the intended parse) and, of course, analytical validation of the final result. To make this task practical, a specialized tree selection tool extracts a set of what are called discriminants from the complete set of analyses. Discriminants encode contrasts among alternate analyses—for example whether to treat a word like crop as nominal or verbal, or where to attach a preposition"
K19-2003,W97-1502,0,0.137023,"changes in the underlying grammar, as well as ‘picking up’ analyses for previously out-of-scope inputs and new development corpora. Since mid-2016, the current version of Redwoods (dubbed Ninth Growth, corresponding to ERG release 1214) encompasses gold-standard analyses for some 85,400 utterances (or close to 1.3 million tokens) of running text from half a dozen different genres and domains, including the first 22 sections of the venerable Wall Street Journal (WSJ) text in the Penn Treebank (PTB; Marcus et al., 1993). the most comprehensive such effort, complementing the original proposal by Carter (1997) with the notion of dynamic treebanking, in two senses of this term. First, different views can be projected from the multi-stratal HPSG analyses at the core of the treebank, highlighting subsets of the syntactic or semantic properties of each analysis, e.g. HPSG derivations, more conventional phrase structure trees, full logical-form meaning representations, and various variable-free forms of semantic dependency graphs—including EDS and DM. Second, the dynamic treebank is extended and refined over time. As the grammar (the core repository of knowledge about derivation and composition) evolves"
K19-2003,K19-2007,0,0.116833,"nouns and adjectives) typically enable the grammar to derive complete syntactico-semantic analyses for 85 – 95 percent of all utterances in standard corpora, including newspaper text, the English Wikipedia, or bio-medical research literature (Flickinger et al., 2017). Parsing times for these data sets measure in seconds per sentence, time comparable to human production or comprehension. Second, since around 2001 the ERG has been accompanied by a selection of development corIntroduction Two of the target representations in the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012)"
K19-2003,K19-2016,0,0.0976457,"feature configuration of Zhang et al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, in the sense that they do not incorporate manually curated linguistic knowledge (beyond finite-state tokenization rules, maybe) but rather learn all their parameters exclusively from the shared task training data. By and large, the data-driven parsers are competitive to the ERG, in particular the SJTU–NICT and HIT-SCIR systems for DM, and the Peking parser for EDS. For some structural types of graph components (tops and edges), the ERG is in fac"
K19-2003,J93-2004,0,0.0670511,"of the treebank has been produced, manually validating and updating existing analyses to reflect changes in the underlying grammar, as well as ‘picking up’ analyses for previously out-of-scope inputs and new development corpora. Since mid-2016, the current version of Redwoods (dubbed Ninth Growth, corresponding to ERG release 1214) encompasses gold-standard analyses for some 85,400 utterances (or close to 1.3 million tokens) of running text from half a dozen different genres and domains, including the first 22 sections of the venerable Wall Street Journal (WSJ) text in the Penn Treebank (PTB; Marcus et al., 1993). the most comprehensive such effort, complementing the original proposal by Carter (1997) with the notion of dynamic treebanking, in two senses of this term. First, different views can be projected from the multi-stratal HPSG analyses at the core of the treebank, highlighting subsets of the syntactic or semantic properties of each analysis, e.g. HPSG derivations, more conventional phrase structure trees, full logical-form meaning representations, and various variable-free forms of semantic dependency graphs—including EDS and DM. Second, the dynamic treebank is extended and refined over time."
K19-2003,K19-2001,1,0.788811,"Missing"
K19-2003,flickinger-etal-2014-towards,1,0.841676,"es for 85 – 95 percent of all utterances in standard corpora, including newspaper text, the English Wikipedia, or bio-medical research literature (Flickinger et al., 2017). Parsing times for these data sets measure in seconds per sentence, time comparable to human production or comprehension. Second, since around 2001 the ERG has been accompanied by a selection of development corIntroduction Two of the target representations in the 2019 Shared Task on Cross-Framework Meaning Representation Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) achieve simplification of ERS into labeled directed graphs by elimination of most of the information regarding"
K19-2003,2000.iwpt-1.19,1,0.558011,"re needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treebank is built exclusively from ERG"
K19-2003,P82-1014,0,0.708302,"een under continuous development for multiple decades now, as part of the world-wide Deep Linguistic Processing with HPSG Initiative (DELPH-IN; http://delph-in.net). First, the LinGO English Resource Grammar (ERG; Flickinger, 2000) is an implementation of the grammatical theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) for English, i.e. a computational grammar that can be used for parsing and generation. Development of the ERG started in 1993, building conceptually on earlier work on unification-based grammar engineering for English at Hewlett Packard Laboratories (Gawron et al., 1982). The ERG has continuously evolved through a series of R&D projects (and a small handful of commercial applications) and today allows the grammatical analysis of running text across domains and genres. The handbuilt ERG lexicon of some 38,000 lemmata (for 27,000 distinct citation forms) aims for complete coverage of function words and open-class words with ‘non-standard’ syntactic properties (e.g. argument structure). Built-in support for light-weight named entity recognition and an unknown word mechanism combining statistical PoS tagging and on-the-fly lexical instantiation for ‘standard’ ope"
K19-2003,A00-2022,1,0.48413,"led in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treebank is built exclusively from ERG analyses, i.e. full HPSG syntactico-semantic signs. Annotation in Redwoods amounts to disambiguation among the candidate analyses derived by the grammar (identifying the intended parse) and, of course, analytic"
K19-2003,W12-3602,1,0.948803,"tion Parsing (MRP 2019; Oepen et al., 2019) derive from the framework dubbed English Resource Semantics (ERS; Flickinger et al., 2014; Bender et al., 2015). ERS instantiates the designer logic for scopally underspecified meaning representation called Minimal Recursion Semantics (MRS; Copestake et al., 2005); in and of themselves, ERS terms are logic- rather than graph-based, i.e. require conversion into graph-structured representations of meaning in the context of the MRP shared task. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) achieve simplification of ERS into labeled directed graphs by elimination of most of the information regarding scope underspecification and, in the case of DM, further reduction into pure bi-lexical graphs. Oepen et al. (2019) provide additional background on these representations. This paper gives some linguistic and technical background on ERS parsing (§2), summarizes the processes used in deriving EDS and DM graphs for the MRP evaluation data 40 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 40–44 c Hong Kong, November 3, 2019. 201"
K19-2003,P99-1069,0,0.647476,"y to mostly automatically update the Redwoods treebank, to for example add detail to the linguistic analyses or apply targeted error correction while minimizing any loss of manual input from previous annotation cycles. Although we can by no means quantify precisely the effort devoted to ERG and Redwoods development to date, we estimate that in excess of thirty person years have been accumulated between 1993 and 2019. The original motivation for treebanking ERG analyses was to enable training discriminative parse ranking models, i.e. a conditional probability distribution over ERG derivations (Johnson et al., 1999). For this purpose, the treebank must disambiguate at the same level of granularity as is maintained in the grammar, i.e. encode its exact linguistic distinctions. Furthermore, to train discriminative (i.e. conditional) stochastic models, both the intended as well as the dispreferred analyses are needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 20"
K19-2003,P99-1061,0,0.0220515,"he dispreferred analyses are needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest according to a discriminative parse ranking model in the tradition of Johnson et al. (1999) and Toutanova et al. (2005). Despite the non-local nature of features (of ERG derivations) used in parse ranking, the selective unpacking procedure of Carroll and Oepen (2005) The Redwoods treeban"
K19-2003,W07-2207,1,0.51326,"esulting graphs are guaranteed to reflect the composition algebra of the ERG, recursively building larger fragments of meaning from smaller parts. 4 Experimental Results Parsing accuracies for PET and the ERG are summarized in Table 1, for both the DM (top) and EDS (bottom) evaluation graphs. The table compares ERG parsing results to a selection of ‘real’ submissions to the shared task, viz. the top performers within each framework and for the task For parsing the MRP evaluation data, we applied ERG release 1214 with its bundled WSJ parse ranking model, which uses the feature configuration of Zhang et al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zha"
K19-2003,P06-4014,1,0.427681,"Johnson et al., 1999). For this purpose, the treebank must disambiguate at the same level of granularity as is maintained in the grammar, i.e. encode its exact linguistic distinctions. Furthermore, to train discriminative (i.e. conditional) stochastic models, both the intended as well as the dispreferred analyses are needed. 3 Parsing with the ERG There are several highly engineered implementations of the DELPH-IN feature structure reference formalism; for our experiments we used the PET parser of Callmeier (2002), as bundled in the open-source distribution of DELPH-IN resources called LOGON (Lønning and Oepen, 2006).1 At its core, PET is a classic, agenda-driven chart parser (Kay, 1986), synthesizing a large body of algorithm design for efficient feature structure manipulation and unification-based parsing by among others Tomabechi (1995), Malouf et al. (2000), Erbach (1991), Kiefer et al. (1999), and Oepen and Callmeier (2000). The parser achieves exact inference by constructing the complete parse forest, factoring local ambiguity under feature structure subsumption (a technique termed retroactive packing by Oepen and Carroll, 2000) and subsequently enumerating n-best full derivations from the forest ac"
K19-2003,K19-2014,0,0.0946981,"00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, in the sense that they do not incorporate manually curated linguistic knowledge (beyond finite-state tokenization rules, maybe) but rather learn all their parameters exclusively from the shared task training data. By and large, the data-driven parsers are competitive to the ERG, in particular the SJTU–NICT and HIT-SCIR systems for DM, and the Peking parser for EDS. For some structural types of graph components (tops and edges), the ERG is in fact outperformed by some submissions, whereas it holds at times commanding"
K19-2003,W02-2018,0,0.00929824,"smaller parts. 4 Experimental Results Parsing accuracies for PET and the ERG are summarized in Table 1, for both the DM (top) and EDS (bottom) evaluation graphs. The table compares ERG parsing results to a selection of ‘real’ submissions to the shared task, viz. the top performers within each framework and for the task For parsing the MRP evaluation data, we applied ERG release 1214 with its bundled WSJ parse ranking model, which uses the feature configuration of Zhang et al. (2007) and was trained on Sections 00– 20 of the Redwoods Ninth Growth using the Maximum Entropy estimation toolkit of Malouf (2002). We use the LOGON distribution as of August 2019 2 The ERS-to-EDS converter of Oepen and Lønning (2006) is part of the LOGON distribution, as is the converter of Ivanova et al. (2012) for further simplification to bi-lexical DM. Exact command-line incantations for all tools and their parameterization are specified as part of the submission archive in the MRP 2019 data release. 42 overall: HIT-SCIR (Che et al., 2019), Peking (Chen et al., 2019)3 , SJTU–NICT (Bai and Zhao, 2019), and SUDA–Alibaba (Zhang et al., 2019). In contrast to the ERG parser, all of these systems are purely data-driven, i"
kouylekov-oepen-2014-semantic,W12-3602,1,\N,Missing
kouylekov-oepen-2014-semantic,J93-2004,0,\N,Missing
kouylekov-oepen-2014-semantic,flickinger-etal-2010-wikiwoods,1,\N,Missing
kouylekov-oepen-2014-semantic,C00-2157,0,\N,Missing
kouylekov-oepen-2014-semantic,W08-1301,0,\N,Missing
kouylekov-oepen-2014-semantic,chiarcos-2012-generic,0,\N,Missing
L16-1630,D11-1031,0,0.0192561,"linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where subst"
L16-1630,W13-2322,0,0.201241,"ns as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer t"
L16-1630,W08-2222,0,0.0201912,"and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collec"
L16-1630,Q15-1040,1,0.86691,"banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was perf"
L16-1630,J93-2004,0,0.0550061,"ies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “includ"
L16-1630,S14-2082,0,0.0694129,"information. We envision that general availability of a standardized and comprehensive set of semantic dependency graphs and associated tools will stimulate more research in this sub-area of semantic parsing. To date, reported ‘parsing success’ 7 To seek to relate these different approaches to the encoding of lexical valency, one can multiply out the DM frame identifiers with verb lemmata, which yields a count of some 4,600 distinct combinations, i.e. slightly less than the set of observed sense distinctions in PSD. measures in terms of dependency F1 range between the high seventies for PSD (Martins & Almeida, 2014) and high eighties to low nineties for CCD, DM, and PAS (Du et al., 2015; Miyao et al., 2014). Such variation may in principle be owed to differences in the number and complexity of linguistic distinctions made, to homogeneity and consistency of training and test data, and of course to the cumulative effort that has gone into pushing the state of the art on individual target representations. A deeper understanding of these parameters, as well as of contentful vs. superficial linguistic differences across frameworks, will be a prerequisite to judging the relative suitability of different resour"
L16-1630,J07-4004,0,0.0382917,"bank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannot"
L16-1630,copestake-flickinger-2000-open,1,0.723309,"al trees), and with corresponding ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic propert"
L16-1630,S14-2056,1,0.955729,"s). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional manual annotation was performed). CCD: Combinatory Categorial Grammar Dependencies Hockenmaier & Steedman (2007) construct CCGbank from a combination of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Jo"
L16-1630,P15-1149,0,0.063476,"s of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Treebank (PTB; Marcus et al., 1993), though the connection is arguably more direct for CCD and PAS than for PSD (where substantial additional"
L16-1630,flickinger-etal-2014-towards,1,0.859802,"ing ‘companion’ syntactic analyses from a broad variety of frameworks and sources, comprising gold-standard annotations as well as analyses delivered by state-of-the-art statistical parsers. 1 Domain- and application-independence and lexicalization set these target representations apart from other strands of semantic parsing, into immediately actionable query languages in the tradition of Zelle & Mooney (1996), on the one hand, or into representations whose primitives need not be surface lexical units, on the other hand, as for example English Resource Semantics (Copestake & Flickinger, 2000; Flickinger et al., 2014), the Discourse Representation Structures of Bos (2008), or Abstract Meaning Representation (Banarescu et al., 2013). Furthermore, the release package includes system submissions and scores from two parsing competitions against several of our target representations, viz. the Broad-Coverage Semantic Dependency Parsing (SDP) tasks at recent Semantic Evaluation Exercises (Oepen et al., 2014, 2015), together with Java and Python tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks a"
L16-1630,P10-1035,0,0.0195635,"n of careful interpretation of the syntactic annotations in the PTB with additional, manually curated lexical and constructional knowledge. In CCGbank, the strings of 3991 the PTB Wall Street Journal (WSJ) Corpus are annotated with pairs of (a) CCG syntactic derivations and (b) sets of semantic bi-lexical dependency triples. The latter “include most semantically relevant non-anaphoric local and longrange dependencies” and are suggested by the CCGbank creators as a proxy for predicate–argument structure. While these have mainly been used for contrastive parser evaluation (Clark & Curran, 2007; Fowler & Penn, 2010; inter alios), recent parsing work as mentioned above views each set of triples as a directed graph and parses directly into these target representations. Our CCD graphs combine the CCGbank dependency triples with information gleaned from the CCG syntactic derivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBan"
L16-1630,hajic-etal-2012-announcing,1,0.917423,"Missing"
L16-1630,S15-2153,1,0.925074,"Missing"
L16-1630,S14-2008,1,0.928327,"Missing"
L16-1630,oepen-lonning-2006-discriminant,1,0.921801,"rivations, notably the part of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). To"
L16-1630,J07-3004,0,0.0404021,"hon tools to read, manipulate, and score these graphs. We intend this overview paper to document relevant formal and (some of the) linguistic properties of these graph banks and to encourage broader use of this standardized collection for improved comparability and replicability; we refer to this new public resource as SDP 2016. 2. Varieties of Semantic Dependency Graphs The earlier SDP tasks comprised three distinct target representations, dubbed DM, PAS, and PSD (see below for details). SDP 2016 derives an additional collection of semantic dependency graphs, which we term CCD, from CCGbank (Hockenmaier & Steedman, 2007). Dependencies of this type have been used as the target representations in some recent parsing work (Auli & Lopez, 2011; Du et al., 2015; Kuhlmann & Jonsson, 2015), but the exact procedure of extracting these graphs from CCGbank has yet to be standardized. The following paragraphs briefly summarize the linguistic genesis of each representation, with particular emphasis on CCD, because the other three have already been introduced by Oepen et al. (2014) and Miyao et al. (2014). With the exception of the DM graphs, all representations for English, to some degree, build on the venerable Penn Tree"
L16-1630,W12-3602,1,0.888177,"of speech and lexical category associated with each token (interpreted as its argument frame), and the identity of the lexical head of the derivation, which becomes the semantic top node. DM: DELPH-IN MRS Bi-Lexical Dependencies These semantic dependency graphs originate in a manual reannotation, dubbed DeepBank2 , of Sections 00–21 of the WSJ Corpus with syntactico-semantic analyses from the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). Native ERG semantics take the form of underspecified logical forms, which Oepen et al. (2002); Oepen & Lønning (2006); and Ivanova et al. (2012) map onto the DM bi-lexical semantic dependencies in a twostep conversion pipeline.3 For this target representation, top nodes designate the highest-scoping (non-quantificational) predicate in the graph, e.g. the scopal adverb almost in Figure 1 below. PAS: Enju Predicate–Argument Structures The Enju Treebank4 is derived from automatic HPSG-style reannotation of the PTB (Miyao, 2006). Our PAS graphs stem from the Enju Treebank, without contentful conversion, and from the application of the same basic techniques to the Penn Chinese Treebank (CTB; Xue et al., 2005). Top nodes in this representat"
lapponi-etal-2014-road,J06-4003,0,\N,Missing
lapponi-etal-2014-road,P12-2074,1,\N,Missing
lapponi-etal-2014-road,P01-1040,0,\N,Missing
oepen-lonning-2006-discriminant,2004.tmi-1.2,1,\N,Missing
oepen-lonning-2006-discriminant,W97-1502,0,\N,Missing
oepen-lonning-2006-discriminant,W02-1503,0,\N,Missing
oepen-lonning-2006-discriminant,C02-2025,1,\N,Missing
P05-1041,H91-1060,0,0.0486623,"Missing"
P05-1041,brants-2000-inter,0,0.0246506,"When broken down by pairs of annotators and sets of 1,000 items each, which have been annotated in strict sequential order, F scores in Table 2 confirm that: (a) inter-annotator agreement is stable, all three annotators appear to have performed equally (well); (b) with growing experience, there is a slight increase in F scores over time, particularly when taking into account that set E exhibits a noticeably higher average ambiguity rate (1208 parses per item) than set D (820 average parses); and (c) Hinoki inter-annotator agreement compares favorably to results reported for the German NeGra (Brants, 2000) and Spanish Cast3LB (Civit et al., 2003) treebanks, both of which used manual mark-up seeded from automated POS tagging and chunking. Compared to the 92.43 per cent labeled F score reported by Brants (2000), Hinoki achieves an ‘error’ (i.e. disagreement) rate of less than half, even though our structures are richer in information and should probably be contrasted with the ‘edge label’ F score for NeGra, which is 88.53 per cent. At the same time, it is unknown to what extent results are influenced by differences in text genre, i.e. average sentence length of our dictionary definitions is notic"
P05-1041,W97-1502,0,0.318075,"panese and building an ontology from the results. Arguably the most common method in building a treebank still is manual annotation, annotators (often linguistics students) marking up linguistic properties of words and phrases. In some semi-automated treebank efforts, annotators are aided by POS taggers or phrase-level chunkers, which can propose mark-up for manual confirmation, revision, or extension. As computational grammars and parsers have increased in coverage and accuracy, an alternate approach has become feasible, in which utterances are parsed and the annotator selects the best parse Carter (1997); Oepen et al. (2002) from the full analyses derived by the grammar. We adopted the latter approach. There were four main reasons. The first was that we wanted to develop a precise broad-coverage grammar in tandem with the treebank, as part of our research into natural language understanding. Treebanking the output of the parser allows us to immediately identify problems in the grammar, and improving the grammar directly improves the quality of the treebank in a mutually beneficial feedback loop (Oepen et al., 2004). The second reason is that we wanted to annotate to a high level of detail, ma"
P05-1041,C02-2025,1,0.878669,"ding an ontology from the results. Arguably the most common method in building a treebank still is manual annotation, annotators (often linguistics students) marking up linguistic properties of words and phrases. In some semi-automated treebank efforts, annotators are aided by POS taggers or phrase-level chunkers, which can propose mark-up for manual confirmation, revision, or extension. As computational grammars and parsers have increased in coverage and accuracy, an alternate approach has become feasible, in which utterances are parsed and the annotator selects the best parse Carter (1997); Oepen et al. (2002) from the full analyses derived by the grammar. We adopted the latter approach. There were four main reasons. The first was that we wanted to develop a precise broad-coverage grammar in tandem with the treebank, as part of our research into natural language understanding. Treebanking the output of the parser allows us to immediately identify problems in the grammar, and improving the grammar directly improves the quality of the treebank in a mutually beneficial feedback loop (Oepen et al., 2004). The second reason is that we wanted to annotate to a high level of detail, marking not only depend"
P05-1041,W02-1210,0,0.0124955,"sons. The first was that we wanted to develop a precise broad-coverage grammar in tandem with the treebank, as part of our research into natural language understanding. Treebanking the output of the parser allows us to immediately identify problems in the grammar, and improving the grammar directly improves the quality of the treebank in a mutually beneficial feedback loop (Oepen et al., 2004). The second reason is that we wanted to annotate to a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. By using a Japanese grammar (JACY: Siegel and Bender, 2002) based on a monostratal theory of grammar (HPSG: Pollard and Sag, 1994) we could simultaneously annotate syntactic and semantic structure without overburdening the annota330 Proceedings of the 43rd Annual Meeting of the ACL, pages 330–337, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics tor. The third reason was that we expected the use of the grammar to aid in enforcing consistency — at the very least all sentences annotated are guaranteed to have well-formed parses. The flip side to this is that any sentences which the parser cannot parse remain unannotated, at least u"
P06-4014,I05-1015,1,0.894998,"s a quadruple: [ CONTEXT : ] INPUT [ ! FILTER ] → English Generation Realization of post-transfer in LOGON builds on the pre-existing LinGO English Resource Grammar (ERG; Flickinger, 2000) and LKB generator (Carroll, Copestake, Flickinger, & Poznanski, 1999). The ERG already produced MRS outputs with good coverage in several domains. In LOGON, it has been refined, adopted to the new domain, and semantic representations revised in light of cross-linguistic experiences from MT. Furthermore, chart generation efficiency and integration with stochastic realization have been substantially improved (Carroll & Oepen, 2005). Table 1 summarizes (exhaustive) generator performance on a segment of the LOGON OUTPUT where each of the four components, in turn, is a partial MRS, i.e. triplet of a top handle, bag of EPs, and handle constraints. Left-hand side components are unified against an input MRS M and, when successful, trigger the rule application; elements of M matched by INPUT are replaced with the OUTPUT component, respecting all variable bindings established during unification. The optional CONTEXT and FILTER components serve to condition rule application (on the presence or absence of specific aspects of M ),"
P06-4014,maegaard-etal-2006-kunsti,0,0.0307324,"Missing"
P06-4014,oepen-lonning-2006-discriminant,1,0.81149,"ords of edited, running Norwegian text was gathered and translated by three professional translators. Three quarters of the material are available for system development and also serve as training data for machine learning approaches. Using the discriminant-based Redwoods approach to treebanking (Oepen, Flickinger, Toutanova, & Manning, 2004), a first 5,000 English reference translations were hand-annotated and released to the public.1 In on-going work on adapting the Redwoods approach to (Norwegian) LFG, we are working to treebank a sizable text segment (Ros´en, Smedt, Dyvik, & Meurer, 2005; Oepen & Lønning, 2006). 4 Implementation Figure 2 presents the main components of the LOGON prototype, where all component communication is in terms of sets of MRSs and, thus, can easily be managed in a distributed and (potentially) parallel client – server set-up. Both the analysis and generation grammars ‘publish’ their interface to transfer—i.e. the inventory and synopsis of semanParse Selection The XLE analyzer includes support for stochastic parse selection models, assigning likelihood measures to competing analyses 1 See ‘http://www.delph-in.net/redwoods/’ for the LinGO Redwoods treebank in its latest release"
P06-4014,P02-1035,0,0.0136283,"t potentially related, semantic predicates. Likewise, the SEM-I incorporates some ontological information, e.g. a classification of temporal entities, though crucially only to the extent that is actually grammaticized in the language proper. development corpus: realizations average at a little less than twelve words in length. After addition of domain-specific vocabulary and a small amount of fine-tuning, the ERG provides adequate analyses for close to ninety per cent of the LOGON reference translations. For about half the test cases, all outputs can be generated in less than one cpu second. (Riezler et al., 2002). Using a trial LFG treebank for Norwegian (of less than 100 annotated sentences), we have adapted the tools for the current LOGON version and are now working to train on larger data sets and evaluate parse selection performance. Despite the very limited amount of training so far, the model already appears to pick up on plausible, albeit crude preferences (as regards topicalization, for example). Furthermore, to reduce fan-out in exhaustive processing, we collapse analyses that project equivalent MRSs, i.e. syntactic distinctions made in the grammar but not reflected in the semantics. End-to-E"
P06-4014,2005.mtsummit-osmtw.3,1,0.885732,"Missing"
P06-4014,2005.mtsummit-papers.15,1,0.840857,"pectable) BLEU score of 0.61; averaging over the entire corpus, i.e. counting inputs with processing errors as a zero contribution, the BLEU score drops to 0.21. Realization Ranking At an average of more than fifty English realizations per input MRS (see Table 1), ranking generator outputs is a vital part of the LOGON pipeline. Based on a notion of automatically derived symmetric treebanks, we have trained comprehensive discriminative, log-linear models that (within the LOGON domain) achieve up to 75 per cent exact match accuracy in picking the most likely realization among competing outputs (Velldal & Oepen, 2005). The bestperforming models make use of configurational (in terms of tree topology) as well as of stringlevel properties (including local word order and constituent weight), both with varied domains of locality. In total, there are around 300,000 features with non-trivial distribution, and we combine the MaxEnt model with a traditional language model trained on a much larger corpus (the BNC). The latter, more standard approach to realization ranking, when used in isolation only achieves around 50 per cent accuracy, however. 3 Stochastic Components To deal with competing hypotheses at all proce"
P06-4014,2004.tmi-1.2,1,\N,Missing
P06-4014,2005.mtsummit-papers.22,1,\N,Missing
P12-2074,P05-1022,0,0.0118609,"o scripts that may be available on request (Tateisi & Tsujii, 2006). 4 The original WSJ text was last included with the 1995 release of the PTB (LDC #95T07) and required alignment with the treebank, with some manual correction so that the same text is represented in both raw and parsed formats. 379 Tokenization Method tokenizer.sed CoreNLP C&J parser Differing Sentences 3264 1781 2597 Levenshtein Distance 11168 3717 4516 Table 1: Quantitative view on tokenization differences. PTB tokenizer.sed script; (b) the tokenizer from the Stanford CoreNLP tools5 ; and (c) tokenization from the parser of Charniak & Johnson (2005). Table 1 shows quantitative differences between each of the three methods and the PTB, both in terms of the number of sentences where the tokenization differs, and also in the total Levenshtein distance (Levenshtein, 1966) over tokens (for a total of 49,208 sentences and 1,173,750 gold-standard tokens). Looking at the differences qualitatively, the most consistent issue across all tokenization methods was ambiguity of sentence-final periods. In the treebank, final periods are always (with about 10 exceptions) a separate token. If the sentence ends in U.S. (but not other abbreviations, oddly),"
P12-2074,N06-2015,0,0.0171545,"similar to that used in PTB, except . . . Most exceptions are to do with hyphenation, or special forms of named entities such as chemical names or URLs. None of the documentation with extant data sets is sufficient to fully reproduce the tokenization.3 The CoNLL 2008 Shared Task data actually provided two forms of tokenization: that from the PTB (which many pre-processing tools would have been trained on), and another form that splits (most) hyphenated terms. This latter convention recently seems to be gaining ground in data sets like the Google 1T n-gram corpus (LDC #2006T13) and OntoNotes (Hovy et al., 2006). Clearly, as one moves towards a more application- and domaindriven idea of ‘correct’ tokenization, a more transparent, flexible, and adaptable approach to stringlevel pre-processing is called for. 3 A Contrastive Experiment To get an overview of current tokenization methods, we recovered and tokenized the raw text which was the source of the (Wall Street Journal portion of the) PTB, and compared it to the gold tokenization in the syntactic annotation in the treebank.4 We used three common methods of tokenization: (a) the original 2 See http://www.cis.upenn.edu/~treebank/ tokenization.html fo"
P12-2074,J93-2004,0,0.0420576,"n) could be left to downstream analysis, where a tagger or parser, for example, could be expected to accept non-disambiguated quote marks (so-called straight or typewriter quotes) and disambiguate as Arguably, even in an overtly ‘separating’ language like English, there can be token-level ambiguities that ultimately can only be resolved through parsing (see § 3 for candidate examples), and indeed Waldron et al. (2006) entertain the idea of downstream processing on a token lattice. In this article, however, we accept the tokenization conventions and sequential nature of the Penn Treebank (PTB; Marcus et al., 1993) as a useful point of reference— primarily for interoperability of different NLP tools. Still, we argue, there is remaining work to be done on PTB-compliant tokenization (reviewed in§ 2), both methodologically, practically, and technologically. In § 3 we observe that state-of-the-art tools perform poorly on re-creating PTB tokenization, and move on in § 4 to develop a modular, parameterizable, and transparent framework for tokenization. Besides improvements in tokenization accuracy and adaptability to diverse use cases, in § 5 we further argue that each token object should unambiguously link b"
P12-2074,C10-1155,1,0.636732,"f ‘correct’ tokenization, a more transparent, flexible, and adaptable approach to stringlevel pre-processing is called for. 3 A Contrastive Experiment To get an overview of current tokenization methods, we recovered and tokenized the raw text which was the source of the (Wall Street Journal portion of the) PTB, and compared it to the gold tokenization in the syntactic annotation in the treebank.4 We used three common methods of tokenization: (a) the original 2 See http://www.cis.upenn.edu/~treebank/ tokenization.html for available ‘documentation’ and a sed script for PTB-style tokenization. 3 Øvrelid et al. (2010) observe that tokenizing with the GENIA tagger yields mismatches in one of five sentences of the GENIA Treebank, although the GENIA guidelines refer to scripts that may be available on request (Tateisi & Tsujii, 2006). 4 The original WSJ text was last included with the 1995 release of the PTB (LDC #95T07) and required alignment with the treebank, with some manual correction so that the same text is represented in both raw and parsed formats. 379 Tokenization Method tokenizer.sed CoreNLP C&J parser Differing Sentences 3264 1781 2597 Levenshtein Distance 11168 3717 4516 Table 1: Quantitative vie"
P12-2074,P11-4002,0,0.0134359,"for English. 5 Characterization for Traceability Tokenization, and specifically our notion of generalized tokenization which allows text normalization, involves changes to the original text being analyzed, rather than just additional annotation. As such, full traceability from the token objects to the original text is required, which we formalize as ‘characterization’, in terms of character position links back to the source.8 This has the practical benefit of allowing downstream analysis as direct (stand-off) annotation on the source text, as seen for example in the ACL Anthology Searchbench (Schäfer et al., 2011). With our general regular expression replacement rules in REPP, making precise what it means for a token to link back to its ‘underlying’ substring requires some care in the design and implementation. Definite characterization links between the string before (I) and after (O) the application of a single rule can only be established in certain positions, viz. (a) spans not matched by the rule: unchanged text in O outside the span matched by the left-hand side regex of the rule can always be linked back to I; and (b) spans caught by a regex capture group: capture groups represent the same text"
P12-2074,waldron-etal-2006-preprocessing,0,0.0250725,"ing the conversion of a sequence of characters (a string) to a sequence of token objects.1 1 Obviously, some of the normalization we include in the tokenization task (in this generalized interpretation) could be left to downstream analysis, where a tagger or parser, for example, could be expected to accept non-disambiguated quote marks (so-called straight or typewriter quotes) and disambiguate as Arguably, even in an overtly ‘separating’ language like English, there can be token-level ambiguities that ultimately can only be resolved through parsing (see § 3 for candidate examples), and indeed Waldron et al. (2006) entertain the idea of downstream processing on a token lattice. In this article, however, we accept the tokenization conventions and sequential nature of the Penn Treebank (PTB; Marcus et al., 1993) as a useful point of reference— primarily for interoperability of different NLP tools. Still, we argue, there is remaining work to be done on PTB-compliant tokenization (reviewed in§ 2), both methodologically, practically, and technologically. In § 3 we observe that state-of-the-art tools perform poorly on re-creating PTB tokenization, and move on in § 4 to develop a modular, parameterizable, and"
P13-3005,H05-1066,0,0.326013,"Missing"
P13-3005,D12-1133,0,0.110153,"ntly perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats Malt (Nivre et al., 2007): transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005): graph-based dependency parser with global near-exhaustive search. Bohnet and Nivre (2012) parser: transitionbased dependency parser with joint tagger that implements global learning and beam search. (b) the noun phrase is headed by the noun rather than by determiner; (c) prepositions or subordinating conjunctions, rather than their NP or clause arguments, serve as the head in prepositional phrase or subordinated clauses. Therefore we can expect (a) Malt and MST to have fewer errors on coordination structures parsing SB and CD than parsing DT, because SB and CD choose the first conjunct as the head and DT chooses the coordinating conjunction as the head; (b,c) no significant differ"
P13-3005,W06-2932,0,0.0192522,"s (Table 1, Gold PTB tags). The Bohnet and Nivre (2012) parser outperforms Malt on CD and DT and MST on SB, CD and DT with PTB tags even though it does not receive gold PTB tags during test phase but predicts them (Table 2, Predicted PTB tags). This is explained by the fact that the Bohnet and Nivre (2012) parser implements a novel approach to parsing: beam-search algorithm with global structure learning. MST “loses” more than Malt when parsing SB with gold supertags (Table 1, Gold supertags). This parser exploits context features “POS tag of each intervening word between head and dependent” (McDonald et al., 2006). Due to the far larger size of the supertag set compared to the PTB tagset, such features are sparse and have low frequencies. This leads to the lower scores of parsing accuracy for MST. For the Bohnet and Nivre (2012) parser the complexity of supertag prediction has significant negative influence on the attachment and labeling accuracies (Table 2, Predicted supertags). The addition of gold PTB tags as a feature lifts the performance of the Bohnet and Nivre (2012) parser to the level of performance of Malt and MST on CD with gold supertags and Malt on SB with gold supertags (compare Table 2,"
P13-3005,de-marneffe-etal-2006-generating,0,0.0690973,"Missing"
P13-3005,P05-1067,0,0.0149815,"comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: t"
P13-3005,N10-1115,0,0.0202642,"ford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats Malt (Nivre et al., 2007): transition-based dependency parser with local learning and greedy search. MST (McDonald et al., 2005): graph-based dependenc"
P13-3005,C12-1147,0,0.230962,"igurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT is rooted in the formal grammar theory HPSG and is independent from any specific treebank. For DT we gain more expressivity from the underlying linguistic theory, which challenges parsing with statistical tools. The structural analysis of the schemes in Ivanova et al. (2012) leads to the hypothesis that CD and DT are more similar to each other than SB to DT. We recompute similarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. In this paper we focus on practical issues of data representation for dependency parsing. We carry out an exper"
P13-3005,W12-3602,1,0.940897,"ies in all configurations in order to determine how parsers, dependency representations and grammatical tagging methods interact with each other in application to automatic syntactic analysis. SB and CD are derived automatically from phrase structures of Penn Treebank to accommodate the needs of fast and accurate dependency parsing, whereas DT is rooted in the formal grammar theory HPSG and is independent from any specific treebank. For DT we gain more expressivity from the underlying linguistic theory, which challenges parsing with statistical tools. The structural analysis of the schemes in Ivanova et al. (2012) leads to the hypothesis that CD and DT are more similar to each other than SB to DT. We recompute similarities on a larger treebank and check whether parsing results reflect them. The paper has the following structure: an overview of related work is presented in Section 2; treebanks, tagsets, dependency schemes and parsers used in the experiments are introduced in Section 3; analysis of parsing results is discussed in Section 4; conclusions and future work are outlined in Section 5. In this paper we focus on practical issues of data representation for dependency parsing. We carry out an exper"
P13-3005,P12-2020,0,0.0159754,"tream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST 2 Related work Schwartz et al. (2012) investigate which dependency r"
P13-3005,W07-2416,0,0.0449829,"ats (Stanford Dependencies, CoNLL-X, and Enju PAS). Intristic evaluation results show that all parsers have the highest accuracies with the CoNLL-X format. 3 3.1 3.3 In this work we extract DeepBank data in the form of bilexical syntactic dependencies, DELPH-IN Syntactic Derivation Tree (DT) format. We obtain the exact same sentences in Stanford Basic (SB) format from the automatic conversion of the PTB with the Stanford parser (de Marneffe et al., 2006) and in the CoNLL Syntactic Dependencies (CD) representation using the LTH Constituentto-Dependency Conversion Tool for Penn-style Treebanks (Johansson and Nugues, 2007). SB and CD represent the way to convert PTB to bilexical dependencies; in contrast, DT is grounded in linguistic theory and captures decisions taken in the grammar. Figure 1 demonstrates the differences between the formats on the coordination structure. According to Schwartz et al. (2012), analysis of coordination in SB and CD is easier for a statistical parser to learn; however, as we will see in section 4.3, DT has more expressive power distinguishing structural ambiguities illustrated by the classic example old men and women. Data and software Treebanks For the experiments in this paper we"
P13-3005,W06-1634,0,0.0112175,"emes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser. 1 Introduction Dependency parsing is one of the mainstream research areas in natural language processing. Dependency representations are useful for a number of NLP applications, for example, machine translation (Ding and Palmer, 2005), information extraction (Yakushiji et al., 2006), analysis of typologically diverse languages (Bunt et al., 2010) and parser stacking (Øvrelid et al., 2009). There were several shared tasks organized on dependency parsing (CoNLL 2006–2007) and labeled dependencies (CoNLL 2008–2009) and there were a number of attempts to compare various dependencies intrinsically, e.g. (Miyao et al., 2007), and extrinsically, e.g. (Wu et al., 2012). In this paper we focus on practical issues of data representation for dependency parsing. The central aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsi"
P13-3005,P04-1061,0,0.0534082,"tral aspects of our discussion are (a) three dependency formats: two ‘classic’ representations for dependency parsing, namely, Stanford Basic (SB) and CoNLL Syntactic Dependencies (CD), and bilexical dependencies from the HPSG English Resource Grammar (ERG), so-called DELPH-IN Syntactic Derivation Tree (DT), proposed recently by Ivanova et al. (2012); (b) three state-of-the art statistical parsers: Malt (Nivre et al., 2007), MST 2 Related work Schwartz et al. (2012) investigate which dependency representations of several syntactic structures are easier to parse with supervised versions of the Klein and Manning (2004) parser, ClearParser (Choi and Nicolov, 2009), MST Parser, Malt and the Easy First Non-directional parser (Goldberg and Elhadad, 2010). The results imply that all parsers consistently perform better when (a) coordination has one of the conjuncts as the head rather than the coordinating conjunction; 31 Proceedings of the ACL Student Research Workshop, pages 31–37, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics A , B and C A , B and C A, B and C Figure 1: Annotation of coordination structure in SB, CD and DT (left to right) dependency formats Malt (Nivre et al"
P13-3005,W11-2922,0,0.0383567,"Missing"
P13-3005,J93-2004,0,0.0444152,"Missing"
P13-3005,C10-1088,0,\N,Missing
P13-3005,W07-2218,0,\N,Missing
P14-1007,S12-1040,0,0.56099,"of negation and called for systems to analyze negation—detecting cues (affixes, words, or phrases that express negation), resolving their scopes (which parts of a sentence are actually negated), and identifying the negated event or property. The task organizers designed and documented an annotation scheme (Morante and Daelemans, 2012) and applied it to a little more than 100,000 tokens of running text by the novelist Sir Arthur Conan Doyle. While the task and annotations were framed from a semantic perspective, only one participating system actually employed explicit compositional semantics (Basile et al., 2012), with results ranking in the middle of the 12 participating systems. Conversely, the bestperforming systems approached the task through machine learning or heuristic processing over syntactic and linguistically relatively coarse-grained representations; see § 2 below. Example (1), where hi marks the cue and {} the in-scope elements, illustrates the annotations, including how negation inside a noun phrase can scope over discontinuous parts of the sentence.1 In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the automated analysis of negation. Unlike the vast majority of part"
P14-1007,J12-2001,0,0.0330887,"of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date. 1 Introduction (1) Recently, there has been increased community interest in the theoretical and practical analysis of what Morante and Sporleder (2012) call modality and negation, i.e. linguistic expressions that modulate the certainty or factuality of propositions. Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining (Vincze et al., 2008) or sentiment analysis (Lapponi et al., 2012). Owing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Con"
P14-1007,oepen-lonning-2006-discriminant,1,0.347561,"encoded in the Shared Task annotations is not concerned with the relative scope of quantifiers and negation, such as the two possible readings of (2) represented informally below:5 (2) mentary prediction includes a predicate symbol, a label (or ‘handle’, prefixed to predicates with a colon in Fig. 1), and one or more argument positions, whose values are semantic variables. Eventualities (ei ) in MRS denote states or activities, while instance variables (x j ) typically correspond to (referential or abstract) entities. All EPs have the argument position ARG0, called the distinguished variable (Oepen and Lønning, 2006), and no variable is the ARG0 of more than one nonquantifier EP. The arguments of one EP are linked to the arguments of others either directly (sharing the same variable as their value), or indirectly (through socalled ‘handle constraints’, where =q in Fig. 1 denotes equality modulo quantifier insertion). Thus a well-formed MRS forms a connected graph. In addition, the grammar links the EPs to the elements of the surface string that give rise to them, via character offsets recorded in each EP (shown in angle brackets in Fig. 1). For the purposes of the present task, we take a negation cue as o"
P14-1007,S12-1041,1,0.505351,"epresentations provided by a general-purpose deep parser. Our contributions are three-fold: Theoretically, we correlate the structures at play in the Morante and Daelemans (2012) view on negation with formal semantic analyses; methodologically, we demonstrate how to approach the task in terms of underspecified, logical-form semantics; and practically, our combined system retroactively ‘wins’ the 2012 *SEM Shared Task. In the following sections, we review related work (§ 2), detail our own setup (§ 3), and present and discuss our experimental results (§ 4 and § 5, respectively). 2 Related Work Read et al. (2012) describe the best-performing submission to Task 1 of the 2012 *SEM Conference. They investigated two approaches for scope resolution, both of which were based on syntactic constituents. Firstly, they created a set of 11 heuristics that describe the path from the preterminal of a cue to the constituent whose projection is predicted to match the scope. Secondly they trained an SVM ranker over candidate constituents, generated by following the path from a cue to the root of the tree and describing each candidate in terms of syntactic properties along the path and various surface features. Both a"
P14-1007,P07-2009,0,0.0293845,"Missing"
P14-1007,W08-0606,0,0.641248,"m an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date. 1 Introduction (1) Recently, there has been increased community interest in the theoretical and practical analysis of what Morante and Sporleder (2012) call modality and negation, i.e. linguistic expressions that modulate the certainty or factuality of propositions. Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of statements, such as for example text mining (Vincze et al., 2008) or sentiment analysis (Lapponi et al., 2012). Owing to its immediate utility in the curation of scholarly results, the analysis of negation and so-called hedges in bio-medical research literature has been the focus of several workshops, as well as the Shared Task at the 2011 Conference on Computational Language Learning (CoNLL). {The German} was sent for but professed to {know} hnothingi {of the matter}. In this work, we return to the 2012 *SEM task from a deliberately semantics-centered point of view, focusing on the hardest of the three sub-problems: scope resolution.2 Where Morante and Dae"
P14-1007,P13-1166,0,0.0206515,"Missing"
P14-1007,P05-3003,0,0.0325545,"y as a fall-back in system combination as described in § 3.4 below. Scopal information in MRS analyses delivered by the ERG fixes the scope of operators—such as negation, modals, scopal adverbs (including subordinating conjunctions like while), and clauseembedding verbs (e.g. believe)—based on their position in the constituent structure, while leaving the scope of quantifiers (e.g. a or every, but also other determiners) free. From these underspecified representations of possible scopal configurations, a scope resolution component can spell out the full range of fully-connected logical forms (Koller and Thater, 2005), but it turns out that such enumeration is not relevant here: the notion of scope encoded in the Shared Task annotations is not concerned with the relative scope of quantifiers and negation, such as the two possible readings of (2) represented informally below:5 (2) mentary prediction includes a predicate symbol, a label (or ‘handle’, prefixed to predicates with a colon in Fig. 1), and one or more argument positions, whose values are semantic variables. Eventualities (ei ) in MRS denote states or activities, while instance variables (x j ) typically correspond to (referential or abstract) ent"
P14-1007,S12-1035,0,0.381171,"Missing"
P14-1007,morante-daelemans-2012-conandoyle,0,\N,Missing
P14-1007,L14-1000,0,\N,Missing
P19-4002,W13-2322,0,0.637356,"ive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the sub"
P19-4002,C16-1056,0,0.056597,"Missing"
P19-4002,basile-etal-2012-developing,0,0.0527628,"hen apply dynamic programming to search for the best graph, as well as transition-based methods, which learn to make individual parsing decisions for each token in the sentence. Some neural techniques also make use of an encoder-decoder architecture, as in neural machine translation. • Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013); • Graph-Based Minimal Recursion Semantics (EDS and DMRS; Oepen and Lønning, 2006; Copestake, 2009); • Abstract Meaning Representation (AMR; Banarescu et al., 2013); • Non-Graph Representations: Discourse Representation Structures (DRS; Basile et al., 2012); Compositionality Semantic parsers also differ with respect to whether they assume that the graph-based semantic representations are constructed compositionally. Some approaches follow standard linguistic practice in assuming that the graphs have a latent compositional structure and try to reconstruct it explicitly or implicitly during parsing. Others are more agnostic and simply predict the edges of the target graph without regard to such linguistic assumptions. • Contrastive review of selected examples across frameworks; • Availability of training and evaluation data; shared tasks; state-of"
P19-4002,P14-1134,0,0.0312129,"state-of-the-art empirical results. (4) Parsing into Semantic Graphs Structural information Finally, semantic parsers differ with respect to how structure information is represented. Some model the target graph directly, whereas others use probability models that score a tree which evaluates to the target graph (e.g. a syntactic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexan"
P19-4002,C04-1180,0,0.239598,"Missing"
P19-4002,P18-1170,1,0.908645,"Missing"
P19-4002,P17-1112,0,0.504659,"enabling lexical decomposition (e.g. of causatives or comparatives). Frameworks instantiating this flavor of semantic graphs include Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the su"
P19-4002,hajic-etal-2012-announcing,0,0.136582,"Missing"
P19-4002,S16-1167,0,0.0477873,"tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this graph flavor include CCG word–word dependencies (CCD; Hockenmaier and Steedman, 2007), Enju Predicate– Argument Structures (PAS; Miyao and Tsujii, 7 3 Processing Semantic Graphs three hours of presentation. The references below are illustrative of the content in each block; in the tutorial itself, we will present one or two approaches per block in detail while treating others more superficially. The creation of large-scale, high-quality semantic graph banks has driven research on semantic parsing, where a system is trained to map from natura"
P19-4002,J94-1007,0,0.785093,"Missing"
P19-4002,P17-1104,0,0.0797035,"ed methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the"
P19-4002,S15-2153,1,0.918102,"Missing"
P19-4002,P18-1035,0,0.283731,"satives or comparatives). Frameworks instantiating this flavor of semantic graphs include Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013; featured in a SemEval 2019 task) and two variants of ‘reducing’ the underspecified logical forms of Flickinger (2000) and Copestake et al. (2005) into directed graphs, viz. Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) and Dependency Minimal Recursion Semantics (DMRS; Copestake, 2009). All three frameworks serve as target representations in recent parsing research (e.g. Buys and Blunsom, 2017; Chen et al., 2018; Hershcovich et al., 2018). Type (2) Finally, our framework review will include Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in our hierarchy of graph flavors is considered unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release"
P19-4002,S14-2008,1,0.898615,"Missing"
P19-4002,J07-3004,0,0.0229765,"in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this graph flavor include CCG word–word dependencies (CCD; Hockenmaier and Steedman, 2007), Enju Predicate– Argument Structures (PAS; Miyao and Tsujii, 7 3 Processing Semantic Graphs three hours of presentation. The references below are illustrative of the content in each block; in the tutorial itself, we will present one or two approaches per block in detail while treating others more superficially. The creation of large-scale, high-quality semantic graph banks has driven research on semantic parsing, where a system is trained to map from natural-language sentences to graphs. There is now a dizzying array of different semantic parsing algorithms, and it is a challenge to keep trac"
P19-4002,W12-3602,1,0.935246,"almost every Semantic Evaluation (SemEval) exercise since 2014. These shared tasks were based on a variety of different corpora with graph-based meaning annotations (graph banks), which differ both in their formal properties and in the facets of meaning they aim to represent. The relevance of this tutorial is to clarify this landscape 6 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 6–11 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2008), DELPH-IN MRS Bi-Lexical Dependencies (DM; Ivanova et al., 2012) and Prague Semantic Dependencies (PSD; a simplification of the tectogrammatical structures of Hajiˇc et al., 2012). Tutorial slides and additional materials are available at the following address: https://github.com/cfmrp/tutorial 2 Semantic Graph Banks In the first part of the tutorial, we will give a systematic overview of the available semantic graph banks. On the one hand, we will distinguish graph banks with respect to the facets of natural language meaning they aim to represent. For instance, some graph banks focus on predicate–argument structure, perhaps with some extensions for polari"
P19-4002,P10-5006,0,0.0128625,"cipants will be enabled to identify genuine content differences between frameworks as well as to tease apart more superficial variation, for example in terminology or packaging. Furthermore, major current processing techniques for semantic graphs will be reviewed against a highlevel inventory of families of approaches. This part of the tutorial will emphasize reflections on codependencies with specific graph flavors or frameworks, on worst-case and typical time and space complexity, as well as on what guarantees (if any) are obtained on the wellformedness and correctness of output structures. Kate and Wong (2010) suggest a definition of semantic parsing as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” This view brings along a tacit expectation to map (more or less) directly from a linguistic surface form to an actionable encoding of its intended meaning, e.g. in a database query or even programming language. In this tutorial, we embrace a broader perspective on semantic parsing as it has come to be viewed commonly in recent years. We will review graph-based meaning representations that ai"
P19-4002,P17-1186,0,0.115805,"Missing"
P19-4002,P17-1014,0,0.0545964,"nderspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philolo"
P19-4002,P18-1171,0,0.187888,"ng of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philology, Computer Scienc"
P19-4002,Q15-1040,0,0.0475384,"ical results. (4) Parsing into Semantic Graphs Structural information Finally, semantic parsers differ with respect to how structure information is represented. Some model the target graph directly, whereas others use probability models that score a tree which evaluates to the target graph (e.g. a syntactic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD"
P19-4002,C08-1095,0,0.198112,"odels. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic par"
P19-4002,J16-4009,1,0.825702,"icate–argument structure, perhaps with some extensions for polarity or tense, whereas others capture (some) scopal phenomena. Furthermore, while the graphs in most graph banks do not have a precisely defined model theory in the sense of classical linguistic semantics, there are still underlying intuitions about what the nodes of the graphs mean (individual entities and eventualities in the world vs. more abstract objects to which statements about scope and presupposition can attach). We will discuss the different intuitions that underly different graph banks. On the other hand, we will follow Kuhlmann and Oepen (2016) in classifying graph banks with respect to the relationship they assume between the tokens of the sentence and the nodes of the graph (called anchoring of graph fragments onto input sub-strings). We will distinguish three flavors of semantic graphs, which by degree of anchoring we will call type (0) to type (2). While we use ‘flavor’ to refer to formally defined sub-classes of semantic graphs, we will reserve the term ‘framework’ for a specific linguistic approach to graph-based meaning representation (typically cast in a particular graph flavor, of course). Type (1) A more general form of an"
P19-4002,D18-1263,0,0.0131,"guities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for AMR using neural supertagging and dependency in the context of a compositional model. • Translation-based methods (Konstas et al., 2017; Peng et al., 2018; Stanovsky and Dagan, 2018); • Cross-framework parsing and multi-task learning (Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018); • Cross-lingual parsing methods (Evang and Bos, 2016; Damonte and Cohen, 2018; Zhang et al., 2018); Stephan Oepen Department of Informatics, University of Oslo, Norway oe@ifi.uio.no https://www.mn.uio.no/ifi/ english/people/aca/oe/ • Contrastive discussion across frameworks, approaches, and languages. (5) Outlook: Applications of Semantic Graphs 5 Content Breadth Stephan Oepen studied Linguistics, German and Russian Philology, Computer Science, and Computational Linguis"
P19-4002,P19-1450,1,0.642351,"tic derivation tree or a term over a graph algebra). This choice interacts with the compositionality dimension, in that tree-based models for graph parsing go together well with compositional models. • Parser evaluation: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he"
P19-4002,N15-1040,0,0.0436829,"on: graph similarity; 4 • Factorization-based methods (Flanigan et al., 2014; Kuhlmann and Jonsson, 2015; Peng et al., 2017; Dozat and Manning, 2018); quantifying semantic • Parsing sub-tasks: segmentation, concept identification, relation detection, structural validation; • Composition-based methods (Callmeier, 2000; Bos et al., 2004; Artzi et al., 2015; Groschwitz et al., 2018; Lindemann et al., 2019; Chen et al., 2018); Tutorial Structure We have organized the content of the tutorial into the following blocks, which add up to a total of 8 • Transition-based methods (Sagae and Tsujii, 2008; Wang et al., 2015; Buys and Blunsom, 2017; Hershcovich et al., 2017); Alexander Koller received his PhD in 2004, with a thesis on underspecified processing of semantic ambiguities using graph-based representations. His research interests span a variety of topics including parsing, generation, the expressive capacity of representation formalisms for natural language, and semantics. Within semantics, he has published extensively on semantic parsing using both grammar-based and neural approaches. His most recent work in this field (Groschwitz et al., 2018) achieved state-of-the-art semantic parsing accuracy for A"
P19-4002,S16-1166,0,0.0365445,"unanchored, in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic f"
P19-4002,D18-1194,0,0.126774,"Missing"
P19-4002,S17-2090,0,0.0283489,"in that the correspondence between nodes and tokens is not explicitly annotated. The AMR framework deliberately backgrounds notions of compositionality and derivation. At the same time, AMR frequently invokes lexical decomposition and represents some implicitly expressed elements of meaning, such that AMR graphs quite generally appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for semantic parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). Type (0) The strongest form of anchoring is obtained in bi-lexical dependency graphs, where graph nodes injectively correspond to surface lexical units (tokens). In such graphs, each node is directly linked to a specific token (conversely, there may be semantically empty tokens), and the nodes inherit the linear order of their corresponding tokens. This flavor of semantic graphs was popularized in part through a series of Semantic Dependency Parsing (SDP) tasks at the SemEval exercises in 2014–16 (Oepen et al., 2014, 2015; Che et al., 2016). Prominent linguistic frameworks instantiating this"
P19-4002,J08-1002,0,0.0102092,", and it is a challenge to keep track of their respective strengths and weaknesses. Different parsing approaches are, of course, more or less effective for graph banks of different flavors (and, at times, even specific frameworks). We will discuss these interactions in the tutorial and organize the research landscape on graph-based semantic parsing along three dimensions. (1) Linguistic Foundations: Layers of Sentence Meaning (2) Formal Foundations: Graphs Labeled Directed (3) Meaning Representation Frameworks and Graph Banks • Bi-Lexical semantic dependencies (Hockenmaier and Steedman, 2007; Miyao and Tsujii, 2008; Hajiˇc et al., 2012; Ivanova et al., 2012; Che et al., 2016); Decoding strategy Semantic parsers differ with respect to the type of algorithm that is used to compute the graph. These include factorizationbased methods, which factorize the score of a graph into parts for smaller substrings and can then apply dynamic programming to search for the best graph, as well as transition-based methods, which learn to make individual parsing decisions for each token in the sentence. Some neural techniques also make use of an encoder-decoder architecture, as in neural machine translation. • Universal Co"
read-etal-2012-wesearch,flickinger-etal-2010-wikiwoods,1,\N,Missing
read-etal-2012-wesearch,bird-etal-2008-acl,0,\N,Missing
read-etal-2012-wesearch,P00-1061,0,\N,Missing
read-etal-2012-wesearch,D08-1050,0,\N,Missing
read-etal-2012-wesearch,W00-0901,0,\N,Missing
read-etal-2012-wesearch,I11-1100,0,\N,Missing
read-etal-2012-wesearch,W07-2202,0,\N,Missing
read-etal-2012-wesearch,W11-2925,0,\N,Missing
read-etal-2012-wesearch,P11-4002,0,\N,Missing
read-etal-2012-wesearch,W10-0508,0,\N,Missing
S12-1041,P05-1022,0,0.0956249,"be due to annotation errors (insuperable, unhappily, endless, listlessly). Among the FNs, two are due to MWCs not covered by our heuristics (e.g., no more), with the remainder concerning affixes. 3 Constituent-Based Scope Resolution During the development of our scope resolution system we have pursued both a rule-based and datadriven approach. Both are rooted in the assumption that the scope of negations corresponds to a syntactically meaningful unit. Our starting point here will be the syntactic analyses provided by the task organizers (see Figure 1), generated using the reranking parser of Charniak and Johnson (2005). However, as alignment between scope annotations and syntactic units is not straightforward for all cases, we apply several exception rules that ‘slacken’ the requirements for alignment, as discussed in Section 3.1. In Sections 3.2 and 3.3 we detail our rule-based and data-driven approaches, respectively. Note that the predictions of the rule-based component will be incorporated as features in the learned model, similarly to the set-up described by Read et al. (2011). Section 3.4 details the post-processing we apply to handle cases of discontinuous scope, beRB//VP/SBAR if SBARWH* RB//VP/S RB"
S12-1041,S12-1035,0,0.263085,"(UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall. 1 (1) There was no answer. Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) hosts a shared task on resolving negation (Morante and Blanco, 2012). This involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these cues, and (iii) identifying negated (and factual) events. This paper describes a system submitted by the Language Technology Group at the University of Oslo (UiO). Our starting point is the negation system developed by Velldal et al. (2012) for the domain of biomedical texts, an SVM-based system for classifying cues and ranking syntactic constituents to resolve cue scopes. However, we extend and adapt this system in several important respects, such as in terms of the underlying lingui"
S12-1041,morante-daelemans-2012-conandoyle,0,0.137264,"nt is the negation system developed by Velldal et al. (2012) for the domain of biomedical texts, an SVM-based system for classifying cues and ranking syntactic constituents to resolve cue scopes. However, we extend and adapt this system in several important respects, such as in terms of the underlying linguistic formalisms that are used, the textual domain, handling of morphological cues and discontinuous scopes, and in that the current system also identifies negated events. The data sets used for the shared task include the following, all based on negation-annotated Conan Doyle (CD) stories (Morante and Daelemans, 2012): a training set of 3644 sentences (hereafter We describe two different system configurations, both of which were submitted for the closed track (hence we can only make use of the data provided by the task organizers). The systems only differ with respect to how they were optimized. In the first configuration, (hereafter I), all components in the pipeline had their parameters tuned by 10-fold cross-validation across CDTD. The second configuration (II) is tuned against CDD using CDT for training. The rationale for this strategy is to guard against possible overfitting effects that could result"
S12-1041,J12-2005,1,0.589072,"Oslo, Department of Informatics {jread,erikve,liljao,oe}@ifi.uio.no Abstract referred to as CDT), a development set of 787 sentences (CDD), and a held-out evaluation set of 1089 sentences (CDE). We will refer to the combination of CDT and CDD as CDTD. An example of an annotated sentence is shown in (1) below, where the cue is marked in bold, the scope is underlined, and the event marked in italics. This paper describes the first of two systems submitted from the University of Oslo (UiO) to the 2012 *SEM Shared Task on resolving negation. Our submission is an adaption of the negation system of Velldal et al. (2012), which combines SVM cue classification with SVM-based ranking of syntactic constituents for scope resolution. The approach further extends our prior work in that we also identify factual negated events. While submitted for the closed track, the system was the top performer in the shared task overall. 1 (1) There was no answer. Introduction The First Joint Conference on Lexical and Computational Semantics (*SEM 2012) hosts a shared task on resolving negation (Morante and Blanco, 2012). This involves the subtasks of (i) identifying negation cues, (ii) identifying the in-sentence scope of these"
S14-2008,D12-1133,0,0.0149346,"ained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given in the LF column. In cases where a team submitted two runs to a track, only the highestranked score is included in the t"
S14-2008,W06-2920,0,0.101128,"em in comparison to other sub-tasks in computational language analysis, introduce the semantic dependency target representations used, reflect on high-level commonalities and differences between these representations, and summarize the task setup, participating systems, and main results. 1 Background and Motivation Syntactic dependency parsing has seen great advances in the past decade, in part owing to relatively broad consensus on target representations, and in part reflecting the successful execution of a series of shared tasks at the annual Conference for Natural Language Learning (CoNLL; Buchholz & Marsi, 2006; Nivre et al., 2007; inter alios). From this very active research area accurate and efficient syntactic parsers have developed for a wide range of natural languages. However, the predominant data structure in dependency parsing to date are trees, in the formal sense that every node in the dependency graph is reachable from a distinguished root node by exactly one directed path. (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans, and rice. Semantically, technique arguably is dependent on the determiner (the quantificational locus), the modifier simil"
S14-2008,de-marneffe-etal-2006-generating,0,0.0702807,"Missing"
S14-2008,oepen-lonning-2006-discriminant,1,0.758648,"antic dependency graphs originate in a manual re-annotation of Sections 00– 21 of the WSJ Corpus with syntactico-semantic analyses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czec"
S14-2008,W09-1201,1,0.855031,"Missing"
S14-2008,J05-1004,0,0.129381,"ependency graphs for Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in o"
S14-2008,P06-1055,0,0.0124328,"e of the gold-standard syntactic or semantic analyses of the SDP 2014 test data, i.e. were directly or indirectly trained or otherwise derived from WSJ Section 21. This restriction implies that typical off-the-shelf syntactic parsers had to be re-trained, as many datadriven parsers for English include this section of the PTB in their default training data. To simplify participation in the open track, the organizers prepared ready-to-use ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in two formats, viz. PTB-style phrase structure trees obtained from the parser of Petrov et al. (2006) and Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). 6 Submissions and Results From 36 teams who had registered for the task, test runs were submitted for nine systems. Each team submitted one or two test runs per track. In total, there were ten runs submitted to the closed track and nine runs to the open track. Three teams submitted to both the closed and the open track. The main results are summarized and ranked in Table 4. The ranking is based on the average LF score across all three target representations, which is given i"
S14-2008,hajic-etal-2012-announcing,1,0.772691,"Missing"
S14-2008,W12-3602,1,0.885947,"yses derived from the LinGO English Resource Grammar (ERG; Flickinger, 2000). Among other layers of linguistic annotation, this resource— dubbed DeepBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical ("
S14-2008,C08-1095,0,0.478753,".27 to 75.89 and the corresponding scores across systems are 88.64 for PAS, 84.95 for DM, and 67.52 for PCEDT. While these scores are consistently higher than in the closed track, the differences are small. In fact, for each of the three teams that submitted to both tracks (Alpage, Potsdam, and Priberam) improvements due to the use of additional resources in the open track do not exceed two points LF. 7 dencies), while the others apply post-processing to recover non-tree structures. The second strategy is to use a parsing algorithm that can directly generate graph structures (in the spirit of Sagae & Tsujii, 2008; Titov et al., 2009). In many cases such algorithms generate restricted types of graph structures, but these restrictions appear feasible for our target representations. The last approach is more machine learning–oriented; they apply classifiers or scoring methods (e.g. edge-factored scores), and find the highest-scoring structures by some decoding method. It is difficult to tell which approach is the best; actually, the top three systems in the closed and open tracks selected very different approaches. A possible conclusion is that exploiting existing systems or techniques for dependency par"
S14-2008,P10-5006,0,0.0693315,"ple inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another difference to common interpretations of SRL is that the SDP 2014 task definition does not encompass predicate disambiguation, a design decision in part owed to our goal to focus on parsing-oriented, i.e. structural, analysis, and in part to lacking consensus on sense inventories for all content words. Finally, a third closely related area of much current interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to most work in this tradition, our SDP target representations aim to be task- and domainindependent, though at least part of this generality comes at the expense of ‘completeness’ in the above sense; i.e. there are aspects of sentence meaning that arguably remain implicit. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure"
S14-2008,J93-2004,0,0.0590496,"t of multiple predicates (i.e. have more than one incoming arc), and it will often be desirable to leave nodes corresponding to semantically vacuous word classes unattached (with no incoming arcs). Thus, Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP 2014),1 seeks to stimulate the dependency parsing community to move towards more general graph processing, to thus enable a more direct analysis of Who did What to Whom? For English, there exist several independent annotations of sentence meaning over the venerable Wall Street Journal (WSJ) text of the Penn Treebank (PTB; Marcus et al., 1993). These resources constitute parallel semantic annotations over the same common text, but to date they have not been related to each other and, in fact, have hardly been applied for training and testing of datadriven parsers. In this task, we have used three different such target representations for bi-lexical semantic dependencies, as demonstrated in Figure 1 below for the WSJ sentence: Task 8 at SemEval 2014 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure"
S14-2008,P07-1031,0,0.0115637,"ersely, in PCEDT the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.7 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase–internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PCEDT, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PCEDT annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely ass"
S14-2008,meyers-etal-2004-annotating,0,0.0465249,"Example (1). uous preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node reentrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. In addition to its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002). In much previous work, however, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena— for example negation and other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—typically remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inferencebased techniques. In contrast, we require parsers to identify all semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Another"
S14-2008,S14-2056,1,0.893424,"pBank by Flickinger et al. (2012)— includes underspecified logical-form meaning representations in the framework of Minimal Recursion Semantics (MRS; Copestake et al., 2005). Our DM target representations are derived through a two-step ‘lossy’ conversion of MRSs, first to variable-free Elementary Dependency Structures (EDS; Oepen & Lønning, 2006), then to ‘pure’ bi-lexical form—projecting some construction semantics onto word-to-word dependencies (Ivanova et al., 2012). In preparing our gold-standard DM graphs from DeepBank, the same conversion pipeline was used as in the system submission of Miyao et al. (2014). For this target representation, top nodes designate the highest-scoping (nonquantifier) predicate in the graph, e.g. the (scopal) degree adverb almost in Figure 1.2 NNP NNP VBZ NNP . − − + − − + − + − − arg1 arg2 _ compound _ _ _ _ ARG1 _ ARG2 _ Table 1: Tabular SDP data format (showing DM). texts from the PTB, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). PCEDT bi-lexical dependencies in this task have been extracted from the t-trees. The specifics of the PCE"
S14-2008,C10-1011,0,\N,Missing
S14-2008,S14-2080,0,\N,Missing
S14-2008,S14-2082,0,\N,Missing
S14-2008,J02-3001,0,\N,Missing
S14-2008,W15-0128,1,\N,Missing
S14-2008,D07-1096,0,\N,Missing
S14-2008,cinkova-2006-propbank,0,\N,Missing
S14-2056,D11-1037,1,0.860866,"ing data is positively biased towards our ensemble members.13 But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of labeled vs. unlabeled F1 scores, (a) the PCEDT parser is comparatively stronger at recovering semantic dependency structure than at assigning labels, and (b) about the same appears to be the case for the best-performing Priberam system (on this target representation). by the"
S14-2056,hajic-etal-2012-announcing,0,0.189879,"Missing"
S14-2056,A00-2022,1,0.412002,"er most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ 1 The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. conversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Par"
S14-2056,J93-2004,0,0.0466436,"plied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) f"
S14-2056,S14-2082,0,0.0922872,"rees can contain generated nodes, which represent elided words and do not correspond to any surface to5 Results and Reflections Seeing as our ‘in-house’ parsers are not directly trained on the semantic dependency graphs provided for the Task, but rather are built from additional linguistic resources, we submitted results from the parsing pipelines sketched in Sections 2 to 4 above to the open SDP track. Table 1 summarizes parser performance in terms of labeled and unlabeled F1 (LF and UF)12 and fullsentence exact match (LM and UM), comparing to the best-performing submission (dubbed Priberam; Martins and Almeida, 2014) to this track. Judging by the official SDP evaluation metric, average labeled F1 over the three representations, our ensemble ranked last among six participating 10 The system was able to output the following functors (ordered in the descending order of their frequency in the system output): RSTR, PAT, ACT, CONJ.member, APP, MANN, LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP, MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND, TSIN, REG, DIR2, CNCS, and TTILL. 11 In the SDP context, the target representation derived from the PCEDT is called by the same name as the original treebank; but note th"
S14-2056,S14-2008,1,0.87444,"Missing"
S14-2056,H05-1066,0,0.0955956,"Missing"
S14-2056,J05-1004,0,0.0249374,"ere used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for Engli"
S14-2056,W07-2207,1,0.819624,"ge numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ 1 The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. conversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Parsing System Enju Predicate–Argument Structures (PAS) are derived from the automatic HPSG-style annotation of the PTB, which was primarily used for the development of the Enj"
S14-2056,J08-1002,1,0.798501,"used for the development of the Enju parsing system4 (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank (Miyao and Tsujii, 2008).5 The PAS data set is an extraction of predicate– argument structures from the Enju HPSG treebank. The Enju parser outputs results in ‘readyto-use’ formats like phrase structure trees and predicate–argument structures, as full HPSG analyses are not friendly to users who are not familiar with the HPSG theory. The gold-standard PAS target data in the Task was developed using this function; the conversion program from full HPSG analyses to predicate–argument structures was applied to the Enju Treebank. Predicate–argument structures (PAS) represent word-to-word semantic dependencies, such as sema"
S14-2056,W12-3602,1,\N,Missing
S14-2056,D07-1096,0,\N,Missing
S14-2056,oepen-lonning-2006-discriminant,1,\N,Missing
S14-2056,W13-5707,1,\N,Missing
S15-2153,D12-1133,0,0.036307,"semantic dependencies distributed for the task. Systems in the open track, on the other hand, could use additional resources, such as a syntactic parser, for example—provided that they make sure to not use any tools or resources that encompass knowledge of the gold-standard syntactic or semantic analyses of the SDP 2015 test data.11 To simplify participation in the open track, the organizers prepared ready-touse ‘companion’ syntactic analyses, sentence- and token-aligned to the SDP data, in the form of Stanford Basic syntactic dependencies (de Marneffe et al., 2006) produced by the parser of Bohnet and Nivre (2012). Finally, to more directly gauge the the contributions of syntactic structure on the semantic dependency parsing problem, an idealized gold track was introduced in SDP 2015. For this track, gold-standard syntactic companion files were provided in a varity of formats, viz. (a) Stanford Basic dependencies, derived from the PTB, (b) HPSG syntactic dependencies in the form called DM by Ivanova et al. (2012), derived from DeepBank, and (c) HPSG syntactic dependencies derived from the Enju Treebank. 6 Submissions and Results From almost 40 teams who had registered for the task, twelve teams obtaine"
S15-2153,cinkova-2006-propbank,1,0.772792,".6993 .5743 .6719 − .5630 .5675 .5490 − Table 2: Pairwise F1 similarities, including punctuation (upper right diagonals) or not (lower left). Frame or sense distinctions are a new property in SDP 2015 and currently are only available for the English DM and PSD data. Table 1 reveals a stark difference in granularity: DM limits itself to argument structure distinctions that are grammaticized, e.g. causative vs. inchoative contrasts or differences in the arity or coarse semantic typing of argument frames; PSD, on the other hand, draws on the much richer sense inventory of the EngValLex database (Cinková, 2006). Accordingly, the two target representations represent quite different challenges for the predicate disambiguation sub-task of SDP 2015. Finally, in Table 2 we seek to quantify pairwise structural similarity between the three representations in terms of unlabeled dependency F1 (dubbed UF in Section 5 below). We provide four variants of this metric, (a) taking into account the directionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discardin"
S15-2153,de-marneffe-etal-2006-generating,0,0.0580061,"Missing"
S15-2153,S14-2080,0,0.42538,"ably because the additional dependency parser they used was trained on data from the target domain. 7 Overview of Approaches Table 5 shows a summary of the tracks in which each submitted system participated, and Table 6 shows an overview of approaches and additionally used resources. All the teams except In-House submitted results for cross-lingual data (Czech and Chinese). Teams except Lisbon also tackled with predicate disambiguation. Only Turku participated in the Gold track. The submitted teams explored a variety of approaches. Riga and Peking relied on the graph-to-tree transformation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) tra"
S15-2153,hajic-etal-2012-announcing,1,0.91158,"Missing"
S15-2153,W12-3602,1,0.93906,"ctionality of edges or not and (b) including edges involving punctuation marks or not. On this view, DM and PAS are structurally much closer to each other than either of the two is to PSD, even more so when discarding punctuation. While relaxing the comparison to ignore edge directionality also increases similarity scores for this pair, the effect is much more pronounced when comparing either to PSD. This suggests that directionality of semantic dependencies is a major source of diversion between DM and PAS on the one hand, and PSD on the other hand. Linguistic Comparison Among other aspects, Ivanova et al. (2012) categorize a range of syntactic and semantic dependency annotation schemes according to the role that functional elements take. In Figure 1 and the discussion of Table 1 above, we already observed that PAS differs from the other representations in integrating into the graph auxiliaries, the infinitival marker, the case-marking preposition introducing the argument of apply (to), and most punctuation marks;9 while these (and other functional elements, e.g. complementizers) are analyzed as semantically vacuous in DM and PSD, they function as predicates in PAS, though do not always serve as ‘loca"
S15-2153,P10-5006,0,0.0554855,"R-arg ACT-arg PAT-arg RSTR EXT RSTR CONJ.m APPS.m ADDR-arg APPS.m CONJ.m A similar technique is almost impossible to apply to other crops , such as cotton , soybeans _ _ _ ev-w218f2 _ _ _ ev-w119f2 _ _ _ _ _ _ _ _ _ CONJ.m and _ rice . _ _ (c) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PSD). Figure 1: Sample semantic dependency graphs for Example (1). sentence’ semantic dependencies, i.e. compute a representation that integrates all content words in one structure. Finally, a third related area of much interest is often dubbed ‘semantic parsing’, which Kate and Wong (2010) define as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” In contrast to much work in this tradition, our SDP target representations aim to be task- and domain-independent. 2 Target Representations We use three distinct target representations for semantic dependencies. As is evident in our running example (Figure 1), showing what are called the DM, PAS, and PSD semantic dependencies, there are contentful differences among these annotations, and there is of course not one obvious (o"
S15-2153,J93-2004,0,0.0679778,"stitute of Informatics, Tokyo Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics • Stanford University, Center for the Study of Language and Information ♠ ◦ sdp-organizers@emmtee.net Abstract more general graph processing, to thus enable a more direct analysis of Who did What to Whom? Extending the very similar predecessor task SDP 2014 (Oepen et al., 2014), we make use of three distinct, parallel semantic annotations over the same common texts, viz. the venerable Wall Street Journal (WSJ) and Brown segments of the Penn Treebank (PTB; Marcus et al., 1993) for English, as well as comparable resources for Chinese and Czech. Figure 1 below shows example target representations, bi-lexical semantic dependency graphs in all cases, for the WSJ sentence: Task 18 at SemEval 2015 defines BroadCoverage Semantic Dependency Parsing (SDP) as the problem of recovering sentence-internal predicate–argument relationships for all content words, i.e. the semantic structure constituting the relational core of sentence meaning. In this task description, we position the problem in comparison to other language analysis sub-tasks, introduce and compare the semantic de"
S15-2153,S14-2082,0,0.0634842,"ormation of Du et al. (2014) as a basis. This method converts semantic dependency graphs into tree structures. Training data of semantic dependency 12 Please see the task web page at the address indicated above for full labeled and unlabeled scores. Team In-House Lisbon Minsk Peking Riga Turku Closed X X X X Open X X X Cross-Lingual Predicate Disambiguation Gold X X X X X X X X X X X Table 5: Summary of tracks in which submitted systems participated Team Approach Resources In-House Lisbon Minsk Peking Riga Turku grammar-based parsing (Miyao et al., 2014) graph parsing with dual decomposition (Martins & Almeida, 2014) transition-based dependency graph parsing in the spirit of Titov et al. (2009) (Du et al., 2014) extended with weighted tree approximation, parser ensemble (Du et al., 2014)’s graph-to-tree transformation, Mate, C6.0, parser ensemble sequence labeling for argument detection for each predicate, SVM classifiers for top node recognition and sense prediction ERG & Enju companion — — — companion Table 6: Overview of approaches and additional resources used (if any). graphs are converted into tree structures, and wellestablished parsing methods for tree structures are applied to converted structure"
S15-2153,meyers-etal-2004-annotating,0,0.0555434,"the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-i p:e-u-i n:x n:x _ n:"
S15-2153,S14-2056,1,0.858363,"ndencies, there are contentful differences among these annotations, and there is of course not one obvious (or even objective) truth. Advancing in-depth comparison of representations and underlying design decisions, in fact, is among the moand other scopal embedding, comparatives, possessives, various types of modification, and even conjunction—often remain unanalyzed in SRL. Thus, its target representations are partial to a degree that can prohibit semantic downstream processing, for example inference-based techniques. 916 tivations for the SDP task series. Please see Oepen et al. (2014) and Miyao et al. (2014) for additional background. DM: DELPH-IN MRS-Derived Bi-Lexical Dependencies These semantic dependency graphs originate in a manual re-annotation, dubbed DeepBank, of Sections 00–21 of the WSJ Corpus and of selected parts of the Brown Corpus with syntacticosemantic analyses of the LinGO English Resource Grammar (Flickinger, 2000; Flickinger et al., 2012). For this target representation, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g. the (scopal) adverb almost in Figure 1.3 PAS: Enju Predicate–Argument Structures The Enju Treebank and parser4 are derived f"
S15-2153,S14-2008,1,0.363094,"Missing"
S15-2153,J05-1004,0,0.359612,"s preposition marking the deep object of apply can be argued to not have a semantic contribution of their own. Besides calling for node re-entrancies and partial connectivity, semantic dependency graphs may also exhibit higher degrees of non-projectivity than is typical of syntactic dependency trees. Besides its relation to syntactic dependency parsing, the task also has some overlap with Semantic Role Labeling (SRL; Gildea & Jurafsky, 2002).2 However, we require parsers to identify ‘full2 In much previous SRL work, target representations typically draw on resources like PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004), which are limited to argument identification and labeling for verbal and nominal predicates. A plethora of semantic phenomena—for example negation 915 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 915–926, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics top BV ARG3 ARG2 ARG1 ARG1 ARG1 ARG1 ARG1 mwe ARG2 conj _and_c A similar technique is almost impossible to apply to other crops , such as cotton, soybeans and rice . q:i-h-h a_to:e-i n:x _ a:e-h a_for:e-h-i _ v_to:e-i-p-i _ a:e-i n:x _ p:e-u-"
S15-2153,P07-1031,0,0.0187644,"ersely, in PSD the last coordinating conjunction takes all conjuncts as its arguments (in case there is no overt conjunction, a punctuation mark is used instead); additional conjunctions or punctuation marks are not connected to the graph.10 A linguistic difference between our representations that highlights variable granularities of analysis and, relatedly, diverging views on the scope of the problem can be observed in Figure 2. Much noun phrase– internal structure is not made explicit in the PTB, and the Enju Treebank from which our PAS representation derives predates the bracketing work of Vadas and Curran (2007). In the four-way nominal compounding example of Figure 2, thus, PAS arrives at a strictly left-branching tree, and there is no attempt at interpreting semantic roles among the members of the compound either; PSD, on the other hand, annotates both the actual compound-internal bracketing and the assignment of roles, e.g. making stock the PAT(ient) of investment. In this spirit, the PSD annotations could be directly paraphrased along the lines of plans by employees for investment in stocks. In a middle position between the other two, DM disambiguates the bracketing but, by design, merely assigns"
W00-1601,H91-1036,0,\N,Missing
W00-1601,C96-2120,1,\N,Missing
W00-1601,J97-3004,0,\N,Missing
W00-1601,J87-1004,0,\N,Missing
W00-1601,J93-4001,0,\N,Missing
W00-1601,J93-1002,1,\N,Missing
W00-1601,P94-1040,1,\N,Missing
W00-1601,A92-1022,0,\N,Missing
W00-1607,copestake-flickinger-2000-open,0,\N,Missing
W01-1512,W99-0802,0,0.127485,"been used for teaching. Besides the LKB, typed feature structure environments have been used at many universities, though unlike the systems cited above, most have only been used with small grammars and may not scale up. Hands on courses using various systems have been run at many recent summer schools including ESSLLI 99 (using the Xerox XLE, see Butt et al, 1999) and ESSLLI 97 and the 1999 LSA summer school (both using ConTroll, see Hinrichs and Meurers, 1999). Very little seems to have been formally published describing experiences in teaching with grammar development environments, though Bouma (1999) describes material for teaching a computational linguistics course that includes exercises using the Hdrug unification-based enviroment to extend a grammar. Despite this rich variety of tools, we believe that the LKB system has a combination of features which make it distinctive and give it a useful niche in teaching. The most important points are that its availability as open source, combined with scale and efficiency, allow advanced projects to be supported as well as introductory courses. As far as we are aware, it is the only system freely available with a broad coverage grammar that supp"
W01-1512,copestake-flickinger-2000-open,1,0.684868,"tware University of Groningen, CSLI, Stanford University Ventura Hall, Postbus 716, 9700 AS Groningen, 110 Pioneer Way Stanford, USA The Netherlands Mountain View, USA danf@csli.stanford.edu malouf@let.rug.nl Abstract We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students. 1 Overview of the LKB system The LKB system is a grammar development environment that is distributed as part of the open source LinGO tools (http://wwwcsli.stanford.edu/˜aac/lkb.html and http://lingo.stanford.edu, see also Copestake and Flickinger, 2000). It is an open-source grammar development environment implemented in Common Lisp, distributed not only as source but also as a standalone application that can be run on Linux, Solaris and Windows (see the website for specific requirements). It will also run under Macintosh Common Lisp, but for this a license is required. The LKB includes a parser, generator, support for large-scale inheritance hierarchies (including the use of defaults), various tools for manipulating semantic representations, a rich set of graphical tools for analyzing and debugging grammars, and extensive on-line documentat"
W02-1502,C00-1004,0,0.00894855,"system developed at Tokyo University (Makino, Yoshida, Torisawa, & Tsujii, 1998), and a parallel processing system developed in Objective C at Delft University (The Netherlands; van Lohuizen, 2002). As part of the matrix package, sample configuration files and documentation will be provided for at least some of these additional platforms. Existing pre-processing packages can also significantly reduce the effort required to develop a new grammar, particularly for coping with the morphology/syntax interface. For example, the ChaSen package for segmenting Japanese input into words and morphemes (Asahara & Matsumoto, 2000) has been linked to at least the LKB and PET systems. Support for connecting implementations of language-specific pre-processing packages of this kind will be preserved and extended as the matrix develops. Likewise, configuration files are included to support generation, at least within the LKB, provided that the grammar conforms to certain assumptions about semantic representation using the Minimal Recursion Semantics framework. Finally, a methodology is under development for constructing and using test suites organized around a typology of linguistic phenomena, using the implementation platf"
W02-1502,1995.tmi-1.2,1,0.665663,"t constituent structure (in one view, Norwegian exhibits a VSO topology in the main clause). The user groups have suggested refinements and extensions of the basic inventory, and it is expected that general solutions, as they are identified jointly, will propagate into the existing grammars too. 3 A Detailed Example As an example of the level of detail involved in the grammar matrix, in this section we consider the analysis of intersective and scopal modification. The matrix is built to give Minimal Recursion Semantics (MRS; Copestake et al., 2001; Copestake, Flickinger, Sag, & Pollard, 1999; Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995) representations. The two English examples in (1) exemplify the difference between intersective and scopal modification:1 (1) a. Keanu studied Kung Fu on a spaceship. b. Keanu probably studied Kung Fu. The MRSs for (1a-b) (abstracting away from agreement information) are given in (2) and (3). The MRSs are ordered tuples consisting of a top handle (h1 in both cases), an instance or event variable (e in both cases), a bag of elementary predications (eps), and a bag of scope constraints (in these cases, QEQ constraints or ‘equal modulo quantifiers’). In a well-formed MRS, the handles can be 1 Th"
W02-1502,P01-1019,1,0.872623,"t we foresee and an evaluation methodology for the matrix proper. 2 Preliminary Development of Matrix We have produced a preliminary version of the grammar matrix relying heavily on the LinGO project’s English Resource Grammar, and to a lesser extent on the Japanese grammar developed jointly between DFKI Saarbr¨ucken (Germany) and YY Technologies (Mountain View, CA). This early version of the matrix comprises the following components:  Types defining the basic feature geometry and technical devices (e.g., for list manipulation).  Types associated with Minimal Recursion Semantics (see, e.g., Copestake, Lascarides, & Flickinger, 2001), a meaning representation language which has been shown to be wellsuited for semantic composition in typed feature structure grammars. This portion of the grammar matrix includes a hierarchy of relation types, types and constraints for the propagation of semantic information through the phrase structure tree, a representation of illocutionary force, and provisions for grammar rules which make semantic contributions.  General classes of rules, including derivational and inflectional (lexical) rules, unary and binary phrase structure rules, headed and non-headed rules, and head-initial and he"
W02-1502,P98-2132,0,0.049922,"Missing"
W02-1502,2000.iwpt-1.19,1,0.705383,"ecting implementations of language-specific pre-processing packages of this kind will be preserved and extended as the matrix develops. Likewise, configuration files are included to support generation, at least within the LKB, provided that the grammar conforms to certain assumptions about semantic representation using the Minimal Recursion Semantics framework. Finally, a methodology is under development for constructing and using test suites organized around a typology of linguistic phenomena, using the implementation platform of the [incr tsdb()] profiling package (Oepen & Flickinger, 1998; Oepen & Callmeier, 2000). These test suites will enable better communication about current coverage of a given grammar built using the matrix, and serve as the basis for identifying additional phenomena that need to be addressed cross-linguistically within the matrix. Of course, the development of the typology of phenomena is itself a major undertaking for which a systematic cross-linguistic approach will be needed, a discussion of which is outside the scope of this report. But the intent is to seed this classification scheme with a set of relatively coarse-grained phenomenon classes drawn from the existing grammars,"
W02-1502,C02-2025,1,0.16955,"t are important to sync to (e.g., changes that affect MRS outputs, fundamental changes to important analyses), (ii) develop a methodology for communicating changes in the matrix, their motivation and their implementation to the user community, and (iii) develop tools for semi-automating resynching of existing grammars to upgrades of the matrix. These tools could use the type hierarchy to predict where conflicts are likely to arise and bring these to the engineer’s attention, possibly inspired by the approach under development at CSLI for the dynamic maintenance of the LinGO Redwoods treebank (Oepen et al., 2002). Finally, while initial development of the matrix has been and will continue to be highly centralized, we hope to provide support for proposed matrix improvements from the user community. User feedback will already come in the form of case studies for the library as discussed in Section 5 above, but also potentially in proposals for modification of the matrix drawing on experiences in grammar development. In order to provide users with some cross-linguistic context in which to develop and evaluate such proposals themselves, we intend to provide some sample matrix-derived grammars and correspo"
W02-1502,W02-1210,1,0.532935,"necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding. 1 Introduction The past decade has seen the development of wide-coverage implemented grammars representing deep linguistic analysis of several languages in several frameworks, including Head-Driven Phrase Structure Grammar (HPSG), LexicalFunctional Grammar (LFG), and Lexicalized Tree Adjoining Grammar (LTAG). In HPSG, the most extensive grammars are those of English (Flickinger, 2000), German (M¨uller & Kasper, 2000), and Japanese (Siegel, 2000; Siegel & Bender, 2002). Despite being couched in the same general framework and in some cases being written in the same formalism and consequently being compatible with the same parsing and generation software, these grammars were developed more or less independently of each other. They each represent between 5 and 15 person years of research efforts, and comprise 35–70,000 lines of code. Unfortunately, most of that research is undocumented and the accumulated analyses, best practices for grammar engineering, and tricks of the trade are only available through painstaking inspection of the grammars and/or consultati"
W02-1502,C98-2128,0,\N,Missing
W02-1508,C00-1004,0,0.015174,"ammarians shifted their engineering focus on ‘tightening’ the grammar, i.e. the elimination of spurious ambiguity and overgeneration (see Siegel & Bender, 2002, for details on the grammar). Another view on grammar evolution is presented in Figure 3, depicting the ‘size’ of the Japanese grammar over the same five-month development cycle. Although measuring the size of 2 Quantifying input complexity for Japanese is a nontrivial task, as the count of the number of input words would depend on the approach to string segmentation used in a specific system (the fairly aggressive tokenizer of ChaSen, Asahara & Matsumoto, 2000, in our case); to avoid potential for confusion, we report input complexity in the (overtly systemspecific) number of lexical items stipulated by the grammar instead: around 50 and 80, on average, for the ‘banking’ and ‘trading’ data sets, respectively (as of February 2002). Grammar Size 10200 10000 9800 9600 9400 9200 9000 8800  •••••• • ••• •  • ••• • •• • •••   •••   •   •         • ••• •• ••  ••••••  ••••   ••    •• • (generated   by [incr tsdb()] at 30-jun-2002 (16:09 h)) •• • • — types  — rules 106 104 102 100 98 96 94 92 90"
W02-1508,P98-2132,0,0.02331,"on the following components and modules: • test and reference data stored with annotations in a structured database; annotations can range from minimal information (unique test item identifier, item origin, length et al.) to fine-grained linguistic classifications (e.g. regarding grammaticality and linguistic phenomena presented in an item), as they are represented in the TSNLP test suites, for example (Oepen, Netter, & Klein, 1997); • tools to browse the available data, identify suitable subsets and feed them through the analysis component of processing systems like the LKB and PET, LiLFeS (Makino, Yoshida, Torisawa, & Tsujii, 1998), TRALE (Penn, 2000), PAGE (Uszkoreit et al., 1994), and others; • the ability to gather a multitude of precise and fine-grained (grammar) competence and (system) performance measures—like the number of readings obtained per test item, various time and memory usage statistics, ambiguity and non-determinism metrics, and salient properties of the result structures—and store them in a uniform, platform-independent data format as a competence and performance profile; and • graphical facilities to inspect the resulting profiles, analyze system competence (i.e. grammatical coverage and overgenerati"
W02-1508,2000.iwpt-1.19,1,0.834837,"mmar development cannot be isolated from measurements of system resource consumption and overall performance (specific properties of a grammar may trigger idiosyncrasies or software bugs in a particular version of the processing system); therefore, and to enable exchange of reference points and comparability of experiments, grammarians and system developers alike should use the same, homogenuous set of relevant parameters. 3 Integrated Competence and Performance Profiling The integrated competence and performance profiling methodology and associated engineering platform, dubbed [incr tsdb()] (Oepen & Callmeier, 2000)1 and reviewed in the remainder of this sec1 See ‘http://www.coli.uni-sb.de/itsdb/’ for the (draft) [incr tsdb()] user manual, pronunciation rules, and instructions on obtaining and installing the package. tion, was designed to meet all of the requirements identified in the DFKI – YY case study. Generally speaking, the [incr tsdb()] environment is an integrated package for diagnostics, evaluation, and benchmarking in practical grammar and system engineering. The toolkit implements an approach to grammar development and system optimization that builds on precise empirical data and systematic ex"
W02-1508,W02-1210,1,0.838186,"overall average of readings assigned to each sentence varies around relatively small numbers. For the moderately complex email data2 the grammar often assigns less than ten analyses, rarely more than a few dozens. However, not surprisingly the addition of grammatical coverage comes with a sharp increase in ambiguity (which may indicate overgeneration): the graphs in Figure 2 clearly show that, once coverage on the ‘trading’ data was above eighty per cent, grammarians shifted their engineering focus on ‘tightening’ the grammar, i.e. the elimination of spurious ambiguity and overgeneration (see Siegel & Bender, 2002, for details on the grammar). Another view on grammar evolution is presented in Figure 3, depicting the ‘size’ of the Japanese grammar over the same five-month development cycle. Although measuring the size of 2 Quantifying input complexity for Japanese is a nontrivial task, as the count of the number of input words would depend on the approach to string segmentation used in a specific system (the fairly aggressive tokenizer of ChaSen, Asahara & Matsumoto, 2000, in our case); to avoid potential for confusion, we report input complexity in the (overtly systemspecific) number of lexical items s"
W02-1508,C94-1072,1,0.847612,"ored with annotations in a structured database; annotations can range from minimal information (unique test item identifier, item origin, length et al.) to fine-grained linguistic classifications (e.g. regarding grammaticality and linguistic phenomena presented in an item), as they are represented in the TSNLP test suites, for example (Oepen, Netter, & Klein, 1997); • tools to browse the available data, identify suitable subsets and feed them through the analysis component of processing systems like the LKB and PET, LiLFeS (Makino, Yoshida, Torisawa, & Tsujii, 1998), TRALE (Penn, 2000), PAGE (Uszkoreit et al., 1994), and others; • the ability to gather a multitude of precise and fine-grained (grammar) competence and (system) performance measures—like the number of readings obtained per test item, various time and memory usage statistics, ambiguity and non-determinism metrics, and salient properties of the result structures—and store them in a uniform, platform-independent data format as a competence and performance profile; and • graphical facilities to inspect the resulting profiles, analyze system competence (i.e. grammatical coverage and overgeneration) and performance (e.g. cpu time and memory usage,"
W02-1508,C96-2120,1,\N,Missing
W02-1508,P94-1040,0,\N,Missing
W02-1508,C98-2128,0,\N,Missing
W06-1661,W98-1426,0,0.0670737,"iven the semantics s, is defined as Table 1: A small example set of generator outputs using the ERG. Where the input semantics is no specified for aspects of information structure (e.g. requesting foregrounding of a specific entity), paraphrases include all grammatically legitimate topicalizations. Other choices involve, for example, the optionality of complementizers and relative pronouns, permutation of (intersective) modifiers, and lexical and orthographic alternations. 2.1 Language Models The use of n-gram language models is the most common approach to statistical selection in generation (Langkilde & Knight, 1998; and White (2004); inter alios). In order to better assert the relative performance of the discriminative models and the structural features we present below, we also apply a trigram model to the ranking problem. Using the freely available CMU SLM Toolkit (Clarkson & Rosenfeld, 1997), we trained a trigram model on an unannotated version of the British National Corpus (BNC), containing roughly 100 million words (using Witten-Bell discounting and back-off). Given such a model pn , the score of a realization ri with surface form wik1 = (wi1 ; : : : ; wik ) is then computed as X F (s; r ) = p (w"
W06-1661,I05-1015,1,0.874424,"support vector machine (SVM). These are all models that have proved popular within the NLP community, but it is usually only the first of these three that has been applied to the task of ranking in sentence generation. The latter two models that we present here go beyond the surface information used by the n-gram model, and are trained on a symmetric treebank with features defined over the full HPSG analyses of competing realizations. Furthermore, such discriminative models are suitable for ‘on-line’ use within our generator—adopting the technique of selective unpacking from a packed forest (Carroll & Oepen, 2005)—which means our hybrid realizer obviates the need for exhaustive enumeration of candidate outputs. The present results extend our earlier work (Velldal, Oepen, & Flickinger, 2004)—and the related work of Nakanishi, Miyao, & Tsujii (2005)—to an enlarged data set, more feature types, and additional learners. The rest of this paper is structured as follows. Section 2 first gives a general summary of the various statistical models we will be considering, as well as the measures used for evaluating them. We then go on to define the task we are aiming to solve in terms of treebank data and feature"
W06-1661,W02-2018,0,0.0246851,"n of si . Fol(11) w  (s; r ) = i X j;k (s ; r )(s; r ) j k i 2.4 Evaluation Measures The models presented in this paper are evaluated with respect to two simple metrics: exact match accuracy and word accuracy. The exact match measure simply counts the number of times that the model assigns the highest score to a string that exactly matches a corresponding ‘gold’ or reference sentence (i.e. a sentence that is marked as preferred in the treebank). This score is discounted appropriately in the case of ties between preferred and non-preferred candidates. 1 We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. 519 Jotunheimen if several realizations are given the top rank by the model. We also include the exact match accuracy for the five best candidates according to the models (see the n-best columns of Table 6). The simple measure of exact match accuracy offers a very intuitive and transparent view on model performance. However, it is also in some respects too harsh as an evaluation measure in our setting since there"
W06-1661,W05-1510,0,0.245041,"Missing"
W06-1661,1995.tmi-1.2,0,0.0748515,"cal generator is part of a larger, semantic transfer MT system. 1 Introduction This paper describes the application of several different statistical models for the task of realization ranking in tactical generation, i.e. the problem of choosing among multiple paraphrases that are generated for a given meaning representation. The specific realization component we use is the opensource chart generator of the Linguistic Knowledge Builder (LKB; Carroll, Copestake, Flickinger, & Poznanski, 1999; Carroll & Oepen, 2005). Given a meaning representation in the form of Minimal Recursion Semantics (MRS; Copestake, Flickinger, Malouf, Riehemann, & Sag, 1995), the generator outputs English realizations in accordance with the HPSG LinGO English Resource Grammar (ERG; Flickinger, 2002). As an example of generator output, a sub-set of alternate realizations that are produced for a single input MRS is shown in Figure 1. For the two data sets considered in this paper, the average number of realizations produced by the generator is 85.7 and 102.2 (the maximum numbers are 4176 and 3408, respectively). Thus, there is immediate demand for a principled way of choosing a single output among the generated candidates. For this task we train and test three dif"
W06-1661,N04-1021,0,0.0357401,"uly 2006. 2006 Association for Computational Linguistics Remember that dogs must be on a leash. Remember dogs must be on a leash. On a leash, remember that dogs must be. On a leash, remember dogs must be. A leash, remember that dogs must be on. A leash, remember dogs must be on. Dogs, remember must be on a leash. 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, & Riezler, 1999; Malouf & Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y (si ) = fr1 ; : : : ; rm g be the set of realizations licensed by the grammar for a semantic representation si . Now, let our (positive) training data be given as Xp = fx1 ; : : : ; xN g where each xi is a pair (si ; rj ) for which rj 2 Y (si ) and rj is annotated in the treebank as being a correct realization of si . Note that we might have sev"
W06-1661,W02-2030,0,0.046085,"Missing"
W06-1661,P99-1069,0,0.129327,"he 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 517–525, c Sydney, July 2006. 2006 Association for Computational Linguistics Remember that dogs must be on a leash. Remember dogs must be on a leash. On a leash, remember that dogs must be. On a leash, remember dogs must be. A leash, remember that dogs must be on. A leash, remember dogs must be on. Dogs, remember must be on a leash. 2.2 Maximum Entropy Models Maximum entropy modeling provides a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection (e.g. Johnson, Geman, Canon, Chi, & Riezler, 1999; Malouf & Noord, 2004) and reranking for machine translation (e.g. Och et al., 2004). A model is specified by a set of real-valued feature functions that describe properties of the data, and an associated set of learned weights that determine the contribution of each feature. Let us first introduce some notation before we go on. Let Y (si ) = fr1 ; : : : ; rm g be the set of realizations licensed by the grammar for a semantic representation si . Now, let our (positive) training data be given as Xp = fx1 ; : : : ; xN g where each xi is a pair (si ; rj ) for which rj 2 Y (si ) and rj is annotat"
W07-1204,W00-1320,0,0.129455,"SEM-L2 + SP). The definitions sentences are harder syntactically, and thus get more of a boost from the semantics. The semantics still improve performance for the example sentences. The semantic class based sense features used here are based on manual annotation, and thus show an upper bound on the effects of these features. This is not an absolute upper bound on the use of sense information — it may be possible to improve further through feature engineering. The learning curves (Fig 7) have not yet flattened out. We can still improve by increasing the size of the training data. 5 Discussion Bikel (2000) combined sense information and parse information using a subset of SemCor (with WordNet senses and Penn-II treebanks) to produce a combined model. This model did not use semantic dependency relations, but only syntactic dependencies augmented with heads, which suggests that the deeper structural semantics provided by the HPSG parser is important. Xiong et al. (2005) achieved only a very minor improvement over a plain syntactic model, using features based on both the correlation between predicates and their arguments, and between predicates and the hypernyms of their arguments (using HowNet)."
W07-1204,W04-2206,1,0.833629,"ypernym class for h988:land vehiclei is h706:inanimatei, h2003:motioni is h1236:human activityi and h4:humani is unchanged. So we used h706:inanimatei and h1236:human activityi to make features in the same way as Table 3. An advantage of these underspecified semantic classes is that they are more robust to errors in word sense disambiguation — fine grained sense distinctions can be ignored. 3.2.4 Valency Dictionary Compatability The last kind of semantic information we use is valency information, taken from the Japanese side of the Goi-Taikei Japanese-English valency dictionary as extended by Fujita and Bond (2004).This valency dictionary has detailed information about the argument properties of verbs and adjectives, including subcategorization and selectional restrictions. A simplified entry of the Japanese side for unten-suru “drive” is shown in Figure 6. Each entry has a predicate and several case-slots. Each case-slot has information such as grammatical function, case-marker, case-role (N1, N2, . . . ) and semantic restrictions. The semantic restrictions are defined by the Goi-Taikei’s semantic classes. On the Japanese side of Goi-Taikei’s valency dictionary, there are 10,146 types of verbs giving 1"
W07-1204,N06-2015,0,0.0183902,"move upward (⊃ sky rocket) frequently appear together, which suggests that using word senses and their hypernyms as features may be useful However, to date, there have been few combinations of sense information together with symbolic grammars and statistical models. We hypothesize that one of the reasons for the lack of success is that there has been no resource annotated with both 2 The Hinoki Corpus There are now some corpora being built with the syntactic and semantic information necessary to investigate the use of semantic information in parse selection. In English, the OntoNotes project (Hovy et al., 2006) is combining sense tags with the Penn treebank. We are using Japanese data from the Hinoki Corpus consisting of around 95,000 dictionary definition and example sentences (Bond et al., 2007) annotated with both syntactic parses and senses from the same dictionary. 2.1 Syntactic Annotation Syntactic annotation in Hinoki is grammar based corpus annotation done by selecting the best parse (or parses) from the full analyses derived by a broadcoverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dep"
W07-1204,P03-1054,0,0.00508812,"antic features. Results are given for syntactic features, dependency relations and the use of semantic classes. 1 Introduction In this paper we investigate the use of semantic information in parse selection. Recently, significant improvements have been made in combining symbolic and statistical approaches to various natural language processing tasks. In parsing, for example, symbolic grammars are combined with stochastic models (Oepen et al., 2004; Malouf and van Noord, 2004). Much of the gain in statistical parsing using lexicalized models comes from the use of a small set of function words (Klein and Manning, 2003). Features based on general relations provide little improvement, presumably because the data is too sparse: in the Penn treebank standardly used to train and test statistical parsers stocks and skyrocket never appear together. However, the superordinate concepts capital (⊃ stocks) and move upward (⊃ sky rocket) frequently appear together, which suggests that using word senses and their hypernyms as features may be useful However, to date, there have been few combinations of sense information together with symbolic grammars and statistical models. We hypothesize that one of the reasons for the"
W07-1204,W02-2018,0,0.310709,"Missing"
W07-1204,oepen-lonning-2006-discriminant,1,0.824955,"le in each semantic relation. For most types of relations, the distinguished variable corresponds to the main index (ARG0 in the examples above), e.g. an event variable for verbal relations and a referential index for nominals. Assuming further that, by and large, there is a unique relation for each semantic variable for which it serves as the main index (thus assuming, for example, that adjectives and adverbs have event variables of their own, which can be motivated in predicative usages at least), an MRS can be broken down into a set of basic dependency tuples of the form shown in Figure 4 (Oepen and Lønning, 2006). All predicates are indexed to the position of the word or words that introduced them in the input sentence (<start:end>). This allows us to link them to the sense annotations in the corpus. 3.2.1 Basic Semantic Dependencies The basic semantic model, SEM-Dep, consists of features based on a predicate and its arguments taken from the elementary dependencies. For example, consider the dependencies for densha ya jidoushawo unten suru hito “a person who drives a train or car” given in Figure 4. The predicate unten “drive” has two arguments: ARG 1 hito “person” and ARG 2 jidousha “car”. From these"
W07-1204,W02-1210,0,0.0338149,"information in parse selection. In English, the OntoNotes project (Hovy et al., 2006) is combining sense tags with the Penn treebank. We are using Japanese data from the Hinoki Corpus consisting of around 95,000 dictionary definition and example sentences (Bond et al., 2007) annotated with both syntactic parses and senses from the same dictionary. 2.1 Syntactic Annotation Syntactic annotation in Hinoki is grammar based corpus annotation done by selecting the best parse (or parses) from the full analyses derived by a broadcoverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. As the grammar is based on a monostratal theory of grammar (HPSG: Pollard and Sag, 1994), annotation by manual disambiguation determines syntactic and semantic structure at the same time. Using a grammar 25 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32, c Prague, Czech Republic, June, 2007. 2007 Association for Computational Linguistics helps treebank consistency — all sentences annotated are guaranteed to have well-formed parses"
W07-1204,I05-1007,0,\N,Missing
W07-1204,J97-4005,0,\N,Missing
W07-1204,J08-1002,0,\N,Missing
W07-1204,W06-1670,0,\N,Missing
W07-1204,P08-1037,0,\N,Missing
W07-1204,P02-1035,0,\N,Missing
W07-1204,P99-1069,0,\N,Missing
W07-2207,J97-4005,0,0.122736,"Missing"
W07-2207,P89-1018,0,0.22569,"4), who use an HPSG grammar comparable to the ERG and GG, non-local ME features, and a two-phase parse forest creation and unpacking approach. However, their unpacking phase uses a beam search to find a good (single) candidate for the best parse; in contrast— for ME models containing the types of non-local features that are most important for accurate parse selection—we avoid an approximative search and efficiently identify exactly the n-best parses. When parsing with context free grammars, a (single) parse can be retrieved from a parse forest in time linear in the length of the input string (Billot & Lang, 1989). However, as discussed in Section 2, when parsing with a unification-based grammar and packing under feature structure subsumption, the cross-product of some local ambiguities may not be globally consistent. This means that additional unifications are required at unpacking time. In principle, when parsing with a pathological grammar with a high rate of failure, extracting a single consistent parse from the forest could take exponential time (see Lang (1994) for a discussion of this issue with respect to Indexed Grammars). In the case of GG, a high rate of unification failure in unpacking is d"
W07-2207,N03-1016,0,0.0171498,"properties. This lack of monotonicity in the scores associated with sub-trees, on the one hand, is beneficial, in that performing a greedy best-first search becomes practical: in contrast, with PCFGs and their monotonically decreasing probabilities on larger sub-trees, once the parser finds the first full tree the chart necessarily has been instantiated almost completely. On the other hand, the same property prohibits the application of exact best-first techniques like A∗ search, because there is no reliable future cost estimate; in this respect, our set-up differs fundamentally from that of Klein & Manning (2003) and related PCFG parsing work. Using the unnormalized sum of ME 51 weights on a partial solution as its agenda score, effectively, means that sub-trees with low scores ‘sink’ to the bottom of the agenda; highly-ranked partial constituents, in turn, instigate the immediate creation of larger structures, and ideally the bottom-up agenda-driven search will greedily steer the parser towards full analyses with high scores. Given its heuristic nature, this procedure cannot guarantee that its n-best list of results corresponds to the globally correct rank order, but it may in practice come reasonabl"
W07-2207,J98-2004,0,0.0238923,"ple packed forest: given two ways of decomposing 6 , there will be three candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees. crafted grammars and inputs of average complexity the approach can perform reasonably well. Another mode of operation is to organize the parser’s search according to an agenda (i.e. priority queue) that assigns numeric scores to parsing moves (Erbach, 1991). Each such move is an application of the fundamental rule of chart parsing, combining an active and a passive edge, and the scores represent the expected ‘figure of merit’ (Caraballo & Charniak, 1998) of the resulting structure. Assuming a parse selection model of the type sketched in Section 2, we can determine the agenda priority for a parsing move according to the (unnormalized) ME score of the derivation (sub-)tree that would result from its successful execution. Note that, unlike in probabilistic context-free grammars (PCFGs), ME scores of partial trees do not necessarily decrease as the tree size increases; instead, the distribution of feature weights is in the range (−∞, +∞), centered around 0, where negative weights intuitively correspond to dis-preferred properties. This lack of m"
W07-2207,I05-1015,1,0.317703,"n-best list of results; and (c) a two-phase approach, where a complete packed for48 Proceedings of the 10th Conference on Parsing Technologies, pages 48–59, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics est is created and combined with a specialized graph search procedure to selectively enumerate results in (globally) correct rank order. Although conceptually simple, the second technique has not previously been evaluated for HPSG parsing (to the best of our knowledge). The last of these techniques, which we call selective unpacking, was first proposed by Carroll & Oepen (2005) in the context of chart-based generation. However, they only provide an account of the algorithm for local ME properties and assert that the technique should generalize to larger contexts straightforwardly. This paper describes this generalization of selective unpacking, in its application to parsing, and demonstrates that the move from features that resemble a context-free domain of locality to features of, in principle, arbitrary context size can indeed be based on the same algorithm, but the required extensions are non-trivial. The structure of the paper is as follows. Section 2 summarizes"
W07-2207,P04-1014,0,0.0216039,"algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature nonlocality; in addition, compared with agendadriven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.† 1 Background—Motivation Technology for natural language analysis using linguistically precise grammars has matured to a level of coverage and efficiency that enables parsing of large amounts of running text. Research groups working within grammatical frameworks like CCG (Clark & Curran, 2004), LFG (Riezler et al., 2002), and HPSG (Malouf & van Noord, 2004; Oepen, Flickinger, Toutanova, & Manning, 2004; Miyao, Ninomiya, & Tsujii, 2005) have successfully integrated broad-coverage computational grammars with sophisticated statistical parse selection models. The former delineate the space of possible analyses, while the latter provide a probability distribu† The first author warmly acknowledges the guidance of his PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grateful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Erik Velldal for many discussions and their suppo"
W07-2207,P02-1036,0,0.0182332,"a parse forest. The algorithm of Carroll & Oepen (2005) and the final one of Huang & Chiang (2005) are essentially equivalent, and turn out to be reformulations of an approach originally described by Jim´enez & Marzal (2000) (although expressed there only for grammars in Chomsky Normal Form). In this paper we have considered ME properties that extend beyond immediate dominance relations, extending up to 4 levels of grandparenting. Previous work has either assumed properties that are restricted to the minimal parse fragments (i.e. subtrees of depth one) that make up the packed representation (Geman & Johnson, 2002), or has taken a more relaxed approach by allowing non-local propConfiguration greedy best-first selective, no caching selective, with cache Unifications (#) 5980 5535 4915 Copies (#) 1447 1523 1522 Hypotheses (#) – 1245 382 Space (kbyte) 9202 27188 27176 Unpack (ms) – 70 10 Total (ms) 400 410 350 Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting. All statistics are averages over the 1941 items that complete within the resource bounds in all three configurations. Unification, Copies, Unpack, and Total have the same interpretation a"
W07-2207,W05-1506,0,0.00687881,"zation of hyper-parameters for individual configurations would moderately improve model performance, especially for higher-order grandparenting levels with large numbers of features. 57 8 Discussion The approach to n-best parsing described in this paper takes as its point of departure recent work of Carroll & Oepen (2005), which describes an efficient algorithm for unpacking n-best trees from a forest produced by a chart-based sentence generator and containing local ME properties with associated weights. In an almost contemporaneous study, but in the context of parsing with treebank grammars, Huang & Chiang (2005) develop a series of increasingly efficient algorithms for unpacking n-best results from a weighted hypergraph representing a parse forest. The algorithm of Carroll & Oepen (2005) and the final one of Huang & Chiang (2005) are essentially equivalent, and turn out to be reformulations of an approach originally described by Jim´enez & Marzal (2000) (although expressed there only for grammars in Chomsky Normal Form). In this paper we have considered ME properties that extend beyond immediate dominance relations, extending up to 4 levels of grandparenting. Previous work has either assumed properti"
W07-2207,A00-2023,0,0.0153214,"number of hypotheses that need to be considered is doubled; as an immediate consequence, there can be up to eight distinct lexicalized variants for the decomposition 1 → h 4 3 i further up in the tree. It may look as if combinatorics will cross-multiply throughout the tree—in the worst case returning us to an exponential number of hypotheses—but this is fortunately not the case: regarding the external bi-grams of 1 , node 6 no longer participates in its left- or rightmost periphery, so variation internal to 6 is not a multiplicative factor at this level. This is essentially the observation of Langkilde (2000), and her bottom-up factoring of n-gram computation is easily incorporated into our top-down selective unpacking control structure. At the point where hypothesizeedge() invokes itself recursively (line 23 in Figure 3), its return value is now a set of lexicalized alternates, and hypothesis creation (in line 26) can take into account the local cross-product of all such alternation. 54 Including additional properties from non-local subtrees (for example higher-order n-grams and head lexicalization) is a straightforward extension of this scheme, replacing our per-edge left- and rightmost peripher"
W07-2207,W02-2018,0,0.0722633,"in “space” between exhaustive and selective unpacking. Also, the difference in “unifications” and “copies” indicates that with our selective unpacking algorithm, these expensive operations on typed feature structures are significantly reduced. In return for increased processing time (and marginal loss in coverage) when using grandparenting features, Table 3 shows some large improvements in parse selection accuracy (although the picture is less clear-cut at higher-order levels of grandparenting5 ). A balance point between efficiency 5 The models were trained using the open-source TADM package (Malouf, 2002), using default hyper-parameters for all configurations, viz. a convergence threshold of 10−8 , variance of the prior of 10−4 , and frequency cut-off of 5. It is likely that ≤ 15 words Configuration GP greedy best-first exhaustive unpacking 0 0 0 1 2 3 4 0 0 0 1 2 3 4 selective unpacking greedy best-first exhaustive unpacking &gt; 15 words selective unpacking Unifications (#) 1845 2287 1912 1913 1914 1914 1914 25233 39095 17489 17493 17493 17495 17495 Copies (#) 527 795 589 589 589 589 589 5602 15685 4422 4421 4421 4422 4422 Space (kbyte) 2328 8907 8109 8109 8109 8110 8110 24646 80832 33326 33318"
W07-2207,P02-1035,0,0.0871292,"ts a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature nonlocality; in addition, compared with agendadriven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy.† 1 Background—Motivation Technology for natural language analysis using linguistically precise grammars has matured to a level of coverage and efficiency that enables parsing of large amounts of running text. Research groups working within grammatical frameworks like CCG (Clark & Curran, 2004), LFG (Riezler et al., 2002), and HPSG (Malouf & van Noord, 2004; Oepen, Flickinger, Toutanova, & Manning, 2004; Miyao, Ninomiya, & Tsujii, 2005) have successfully integrated broad-coverage computational grammars with sophisticated statistical parse selection models. The former delineate the space of possible analyses, while the latter provide a probability distribu† The first author warmly acknowledges the guidance of his PhD advisors, Valia Kordoni and Hans Uszkoreit. We are grateful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Erik Velldal for many discussions and their support. We thank Ron Kaplan, Mar"
W07-2207,P99-1069,0,0.140483,"t be globally consistent. Assume for example that, in Figure 2, edges 6 and 8 subsume 7 and 9 , respectively; combining 7 and 9 into the same tree during unpacking can in principle fail. Thus, unpacking effectively needs to deterministically replay unifications, but this extra expense in our experience is negligible when compared to the decreased cost of constructing the forest under subsumption. In Section 3 we argue that this very property, in addition to increasing parsing efficiency, interacts beneficially with parse selection and on-demand enumeration of results in rank order. Following (Johnson et al., 1999), a conditional ME model of the probabilities of trees {t1 . . . tn } for a string s, and assuming a set of feature functions {f1 . . . fm } with corresponding weights {λ1 . . . λm }, is defined as: P exp j λj fj (ti ) P p(ti |s) = Pn (1) k=1 exp j λj fj (tk ) 2 This property of parse forests is not a prerequisite of the chart parsing framework. The basic CKY procedure (Kasami, 1965), for example, as well as many unification-based adaptations (e.g. the Core Language Engine; Moore & Alshawi, 1992) merely record the local category of each edge, which is sufficient for the recognition task and si"
W07-2207,J08-1002,0,\N,Missing
W07-2207,J07-4004,0,\N,Missing
W07-2207,P08-1067,0,\N,Missing
W07-2207,W01-1812,0,\N,Missing
W09-1204,adolphs-etal-2008-fine,1,0.829188,"e, rejects the conventional assumptions underlying the PTB (and derived tools). It opts for an analysis of punctuation akin to affixation (rather than as standalone tokens), does not break up contracted negated auxiliaries, and splits hyphenated words like illadvised into two tokens (the hyphen being part of the first component). Thus, a string like Don’t you! in the CoNLL data is tokenized as the four-element sequence hdo, n’t, you, !i,2 whereas the ERG analysis has only two leaf nodes: hdon’t, you!i. Fortunately, the DELPH-IN toolchain recently incorporated a mechanism called chart mapping (Adolphs et al., 2008), which allows one to map flexibly from ‘external’ input to grammar-internal assumptions, while keeping track of external token identities and their contributions to the final analysis. The February 2009 release of the ERG already had this machinery in place (with the goal of supporting extant, PTB-trained PoS taggers in pre-processing input to the deep parser), and we found that only a tiny number of additional chart mapping rules was required to ‘fix up’ CoNLL-specific deviations from the PTB tradition. With the help of the original developers, we created new chart mapping configurations for"
W09-1204,burchardt-etal-2006-salsa,0,0.127541,"Missing"
W09-1204,W09-1201,1,0.865296,"Missing"
W09-1204,kawahara-etal-2002-construction,0,0.0194058,"Missing"
W09-1204,W02-2018,0,0.00915684,"mprovement observed after using the HPSG features. Therefore, we did not include it in the final submission. 5 Semantic Role Labeling The semantic role labeling component used in the submitted system is similar to the one described by Zhang et al. (2008). Since predicates are indicated in the data, the predicate identification module is removed from this year’s system. Argument identification, argument classification and predicate classification are the three sub-components in the pipeline. All of them are MaxEnt-based classifiers. For parameter estimation, we use the open source TADM system (Malouf, 2002). The active features used in various steps of SRL are fine tuned separately for different languages using development datasets. The significance of feature types varies across languages and datasets. SYN SRL Closed ood Closed ood Open ood ca 82.67 67.34 - zh 73.63 73.20 - cs 75.58 71.29 78.28 77.78 - en 87.90 81.50 77.85 67.07 78.13 (↑0.28) 68.11 (↑1.04) de 84.57 75.06 62.95 54.87 64.31 (↑1.36) 58.42 (↑3.55) ja 91.47 64.71 65.95 (↑1.24) - es 82.69 67.81 68.24 (↑0.43) - Table 2: Summary of System Performance on Multiple Languages In the open challenge, two groups of extra features from HPSG pa"
W09-1204,J05-1004,0,0.0711747,"Missing"
W09-1204,W02-1210,0,0.017282,"atively detailed, hand-coded linguistic knowledge— including lexical argument structure and the linking of syntactic functions to thematic arguments—and are intended as general-purpose resources, applicable to both parsing and generation. Semantics in DELPH-IN is cast in the Minimal Recursion Semantics framework (MRS; Copestake, Flickinger, Pollard, & Sag, 2005), essentially predicate – argument structures with provision for underspecified scopal relations. For the 2009 ‘open’ task, we used the DELPH-IN grammars for English (ERG; Flickinger, 2000), German (GG; Crysmann, 2005), Japanese (JaCY; Siegel & Bender, 2002), and Spanish (SRG; Marimon, Bel, & Seghezzi, 2007). The grammars vary in their stage of development: the ERG comprises some 15 years of continuous development, whereas work on the SRG only started about five years ago, with GG and JaCY ranging somewhere inbetween. 3.1 Overall Setup We applied the DELPH-IN grammars to the CoNLL data using the PET parser (Callmeier, 2002) running 1 See http://www.delph-in.net for background. 32 it through the [incr tsdb()] environment (Oepen & Carroll, 2000), for parallelization and distribution. Also, [incr tsdb()] provides facilities for (re-)training the Max"
W09-1204,W08-2121,0,0.100771,"Missing"
W09-1204,taule-etal-2008-ancora,0,0.0657859,"Missing"
W09-1204,W07-2207,1,0.835309,"with the TADM software, using tenfold cross-validation and exact match ranking accuracy (against the binarized training distribution) to optimize estimation hyper-parameters 3.3 Deep Parsing Features HPSG parsing coverage and average cpu time per input for the four languages with DELPH-IN grammars are summarized in Table 1. The PoS-based unknown word mechanism was active for all grammars but no other robustness measures (which tend to lower the quality of results) were used, i.e. only complete spanning HPSG analyses were accepted. Parse times are for 1-best parsing, using selective unpacking (Zhang, Oepen, & Carroll, 2007). HPSG parsing outputs are available in several different forms. We investigated two types of structures: syntactic derivations and MRS meaningrepresentations. Representative features were extracted from both structures and selectively used in the statistical syntactic dependency parsing and semantic role labeling modules for the ‘open’ challenge. 3 We also experimented with using DA scores directly as empirical probabilities in the training distribution (or some function of DA, to make it fall off more sharply), but none of these methods seemed to further improve parse selection performance."
W09-1204,W08-2126,1,0.789115,"Cluster of Multimodal Computing and Interaction for the support of the work. The second author is funded by the PIRE PhD scholarship program. Participation of the third author in this work was supported by the University of Oslo, as part of its research partnership with the Center for the Study of Language and Information at Stanford University. Our deep parsing experimentation was executed on the TITAN HPC facilities at the University of Oslo. 31 performance. This makes the task a nice testbed for the cross-fertilization of various language processing techniques. As an example of such work, Zhang et al. (2008) have shown in the past that deep linguistic parsing outputs can be integrated to help improve the performance of the English semantic role labeling task. But several questions remain unanswered. First, the integration only experimented with the semantic role labeling part of the task. It is not clear whether syntactic dependency parsing can also benefit from grammar-based parsing results. Second, the English grammar used to achieve the improvement is one of the largest and most mature hand-crafted linguistic grammars. It is not clear whether similar improvements can be achieved with less deve"
W10-3007,J93-2004,0,0.0437243,"E P ). our own tokenization to a GENIA token, we rely on TnT annotation only. In the merging of annotations across components, and also in downstream processing we have found it most convenient to operate predominantly in terms of characterization, i.e. sub-strings of the raw input that need not align perfectly with token boundaries. lems. Our pre-processing approach therefore deploys a home-grown, cascaded finite-state tokenizer (borrowed and adapted from the opensource English Resource Grammar; Flickinger (2000)), which aims to implement the tokenization decisions made in the Penn Treebank (Marcus et al., 1993) – much like GENIA, in principle – but properly treating corner cases like the ones above. Synchronized via characterization, this tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre a"
W10-3007,W09-1304,0,0.272407,"Missing"
W10-3007,P08-1108,0,0.00733183,", 1993) – much like GENIA, in principle – but properly treating corner cases like the ones above. Synchronized via characterization, this tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ t"
W10-3007,P81-1022,0,0.0609688,"Missing"
W10-3007,W03-3017,0,0.0121603,"ice, as well as more semantic properties detailing, e.g., subcategorization frames, semantic conceptual categories such as human, time and location, etc., resides in the F E A T S column. The parser outputs, which in turn form the basis for our scope resolution rules discussed in Section 4, also take this same form. The parser employed in this work is trained on the Wall Street Journal sections 2 – 24 of the Penn Treebank, converted to dependency format (Johansson and Nugues, 2007) and extended with XLE features, as described above. Parsing is performed using the arc-eager mode of MaltParser (Nivre, 2003) and an SVM with a polynomial kernel. When tested using 10-fold cross-validation on this data set, the parser achieves a labeled accuracy 3 Identifying Hedge Cues For the task of identifying hedge cues, we developed a binary maximum entropy (MaxEnt) classifier. The identification of cue words is used for (i) classifying sentences as certain/uncertain (Task 1), and (ii) providing input to the syntactic rules that we later apply for resolving the in-sentence scope of the cues (Task 2). We also report evaluation scores for the sub-task of cue detection in isolation. As annotated in the training d"
W10-3007,A00-1031,0,0.0383377,"ich may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich feature representation of the parse history in order to guide parsing and may easily be extended to take into account new features of the PoS Tagging and Lemmatization For PoS tagging and lemmatization, we combine (with its built-in, occasionally deviant tokenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default model is trained on financial news from the Penn Treebank. Our general goal here is to take advantage of the higher PoS accuracy provided by GENIA in the biomedical domain, while using our improved tokenization and producing inputs to the parsing stage (see Section 2.3 below) that as much as possible resemble the conventions used in the original training data for the parser – the Penn Treebank, once again. To this effect, for the vast majority of tokens we can align the GENIA tokenization with our own, and in these cases we typically"
W10-3007,W02-1503,0,0.121758,"icles Total 11871 2670 14541 Hedged Cues Sentences 2101 519 2620 2659 668 3327 Multi-Word Cues Tokens Cue Tokens 364 84 448 309634 68579 378213 3056 782 3838 Table 2: Some descriptive figures for the shared task training data. Token-level counts are based on the tokenization described in Section 2.1. parse history. score of 89.8 (Øvrelid et al., 2010). Parser stacking The procedure to enable the data-driven parser to learn from the grammardriven parser is quite simple. We parse a treebank with the XLE platform (Crouch et al., 2008) and the English grammar developed within the ParGram project (Butt et al., 2002). We then convert the LFG output to dependency structures, so that we have two parallel versions of the treebank – one gold standard and one with LFG-annotation. We extend the gold standard treebank with additional information from the corresponding LFG analysis and train the data-driven dependency parser on the enhanced data set. See Øvrelid et al. (2010) for details of the conversion and training of the parser. Table 1 shows the enhanced dependency representation of the English sentence The unknown amino acid may be used by these species, taken from the training data. For each token, the par"
W10-3007,P09-2010,1,0.868874,"his tokenization is then enriched with the output of no less than two PoS taggers, as detailed in the next section. 2.2 2.3 Dependency Parsing with LFG Features For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It"
W10-3007,W10-3001,0,0.124802,"its tokens like ‘3,926.50’, ‘methlycobamide:CoM’, or ‘Ca(2+)’. Conversely, GENIA fails to isolate some kinds of opening single quotes, because the quoting conventions assumed in BioScope differ from those used in the GENIA Corpus; furthermore, it mis-tokenizes LATEX-style n- and m-dashes. On average, one in five sentences in the CoNLL training data exhibited GENIA tokenization probThe CoNLL-2010 shared task1 comprises two sub-tasks. Task 1 is described as learning to detect sentences containing uncertainty, while the object of Task 2 is learning to resolve the in-sentence scope of hedge cues (Farkas et al., 2010). Paralleling this two-fold task definition, the architecture of our system naturally decomposes into two main steps. First, a maximum entropy (MaxEnt) classifier is applied to automatically detect cue words. For Task 1, a given sentence is labeled as uncertain if it contains a word classified as a cue. For Task 2, we then go on to determine the scope of the identified cues using a set of manually crafted rules operating on dependency representations. For both Task 1 and Task 2, our system participates in the stricter category of ‘closed’ or ‘indomain’ systems. This means that we do not use an"
W10-3007,W08-0606,0,0.562357,"oken- and sentence-level F-scores, the effect of incrementally including a larger percentage of training data into the 10-fold cycles. (As described also for the other development results, while we are training on both the articles and the abstracts, we are testing only on the articles.) Model Development, Data Sets and Evaluation Measures hedge cues (possibly spanning multiple tokens). These latter scores are computed using the official shared task scorer script. While the training data made available for the shared task consisted of both abstracts and full articles from the BioScope corpus (Vincze et al., 2008), the test data were pre-announced to consist of biomedical articles only. In order to make the testing situation during development as similar as possible to what could be expected for the held-out testing, we only tested on sentences taken from the articles part of the training data. When developing the classifiers we performed 10-fold training and testing over the articles, while always including all sentences from the abstracts in the training set as well. Table 2 provides some basic descriptive figures summarizing the training data. As can be seen in Table 3, we will be reporting precisio"
W10-3007,W02-2018,0,0.0202726,"e task of determining whether a cue word forms part of a larger multi-word cue, is performed by a separate post-processing step, further described in Section 3.2. 3.1 Maximum Entropy Classification In the MaxEnt framework, each training example – in our case a paired word and label hwi , yi i – is represented as a feature vector f (wi , yi ) = fi ∈ <d . Each dimension or feature function fij can encode arbitrary properties of the data. The particular feature functions we are using for the cue identification are described under Section 3.4 below. For model estimation we use the TADM3 software (Malouf, 2002). For feature extraction and model tuning, we build on the experimentation environment developed by Velldal (2008) (in turn extending earlier work by Oepen et al. 3 Toolkit for Advanced Discriminative Modeling; available from http://tadm.sourceforge.net/. 50 90 (2004)). Among other things, its highly optimized feature handling – where the potentially expensive feature extraction step is performed only once and then combined with several levels of feature caching – make it computationally feasible to perform large-scale ‘grid searches’ over different configurations of features and model paramet"
W10-3007,P09-1043,0,0.0954061,"atures For syntactic parsing we employ a data-driven dependency parser which incorporates the predictions from a large-scale LFG grammar. A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard treebank annotations (Nivre and McDonald, 2008). This technique has been shown to provide significant improvements in accuracy for both English and German (Øvrelid et al., 2009), and a similar approach employing an HPSG grammar has been shown to increase domain independence in data-driven dependency parsing (Zhang and Wang, 2009). For our purposes, we decide to use a parser which incorporates analyses from two quite different parsing approaches – data-driven dependency parsing and “deep” parsing with a handcrafted grammar – providing us with a range of different types of linguistic features which may be used in hedge detection. We employ the freely available MaltParser (Nivre et al., 2006), which is a languageindependent system for data-driven dependency parsing.2 It is based on a deterministic parsing strategy in combination with treebank-induced classifiers for predicting parse transitions. It supports a rich featur"
W10-3007,nivre-etal-2006-maltparser,0,\N,Missing
W10-3007,W07-2416,0,\N,Missing
W11-2927,P04-1032,0,0.0603599,"Missing"
W11-2927,H91-1060,0,0.12682,"ncy Matching Stephan Oepen Rebecca Dridan Department of Informatics NICTA Victoria Research Laboratory Universitetet i Oslo Dept. of Computer Science and Software Engineering oe@ifi.uio.no University of Melbourne rdridan@csse.unimelb.edu.au Abstract this purpose, we distinguish three broad classes of information that contribute to meaning: class 1 core functor – argument structure, whether syntactic or semantic class 2 predicate information, such as the lemma, word category, and sense class 3 properties of events and entities, such as tense, number, and gender The widely-used PARSEVAL metric (Black et al., 1991) evaluates phrase structure, which covers none of these classes directly. Dependency-based evaluation schemes, such as those used by MaltParser (Nivre et al., 2004) and MSTParser (McDonald et al., 2005) evaluate class 1 surface information. The annotation used in the Briscoe and Carroll (2006) DepBank for parser evaluation also describes just class 1 syntactic information, although the relationships are different to those that MaltParser or MSTParser produce. The annotation of the original King et al. (2003) PARC700 DepBank does describe all three classes of information, but again in terms of"
W11-2927,P06-2006,0,0.0495495,"oad classes of information that contribute to meaning: class 1 core functor – argument structure, whether syntactic or semantic class 2 predicate information, such as the lemma, word category, and sense class 3 properties of events and entities, such as tense, number, and gender The widely-used PARSEVAL metric (Black et al., 1991) evaluates phrase structure, which covers none of these classes directly. Dependency-based evaluation schemes, such as those used by MaltParser (Nivre et al., 2004) and MSTParser (McDonald et al., 2005) evaluate class 1 surface information. The annotation used in the Briscoe and Carroll (2006) DepBank for parser evaluation also describes just class 1 syntactic information, although the relationships are different to those that MaltParser or MSTParser produce. The annotation of the original King et al. (2003) PARC700 DepBank does describe all three classes of information, but again in terms of syntactic rather than semantic properties. A common element between all the dependency types above is the use of grammatical relations to describe class 1 information. That is, the dependencies are usually labels like SUBJ, OBJ, MOD, etc. While these grammatical functions allow one to describe"
W11-2927,H05-1066,0,0.114441,"Missing"
W11-2927,J07-4004,0,0.0301468,"RG1 <0:2&gt; <0:2&gt; <13:16&gt; <20:26&gt; <13:16&gt; <13:16&gt; NAME NAME NAME NAME NUM PERS PRONTYPE MOOD SF TENSE NUM PERS MOOD SF TENSE roles. One particular configuration we have found useful is to assign zero weight to the PROP triples and only evaluate ARGs and NAMEs. While the class 3 information is useful for applications such as machine translation, and ideally would be evaluated, some applications don’t make use of this information, and so, in certain scenarios, it makes sense to ignore these triples in evaluation. This configuration produces a metric broadly similar to the CCG dependencies used by Clark and Curran (2007) and also to the predicate argument structures produced by the Enju parser (Miyao and Tsujii, 2008), in terms of the information classes included, although the CCG dependencies again encode syntactic rather than semantic structure. ARG2 pronoun q pron persuade v of proper q named leave v 1 NAME NAME GEND “He” “He” “Kim” “leave.” “Kim” “Kim” m sg 3 std pron indicative prop past sg 3 indicative prop-or-ques untensed 5 Analysis To get some idea of the numeric range of the different EDM configurations, we parsed a section of the SemCor corpus (Miller et al., 1994) using the English Resource Gramma"
W11-2927,H94-1046,0,0.0104315,"o the CCG dependencies used by Clark and Curran (2007) and also to the predicate argument structures produced by the Enju parser (Miyao and Tsujii, 2008), in terms of the information classes included, although the CCG dependencies again encode syntactic rather than semantic structure. ARG2 pronoun q pron persuade v of proper q named leave v 1 NAME NAME GEND “He” “He” “Kim” “leave.” “Kim” “Kim” m sg 3 std pron indicative prop past sg 3 indicative prop-or-ques untensed 5 Analysis To get some idea of the numeric range of the different EDM configurations, we parsed a section of the SemCor corpus (Miller et al., 1994) using the English Resource Grammar (ERG: (Flickinger, 2000)), and then calculated the average F1 -score for each rank, as ranked by the statistical model packaged with the ERG. Figure 3 shows the relative differences between five configurations: all triples together (EDM), the NAME, ARG and PROP triple types separately (EDMN , EDMA and EDMP , respectively) and measuring just the NAME and ARG types together (EDMN A ). We can see that all configurations show approximately the same trends, and maintain their relative order. EDMP is consistently higher, which follows from the fact that many of th"
W11-2927,E09-1001,0,0.104936,"ed grammars as our test environment to evaluate. The traditional accuracy metric for PET has been sentence accuracy which requires an exact match against the very fine-grained gold analysis, but arguably this harsh metric supplies insuffi2 At the same time, we wish to focus parser evaluation on information determined solely by grammatical analysis, i.e. all contributions to interpretation by syntax, and only those. For these reasons, the task of semantic role labeling (SRL) against PropBank-style target representations (Kingsbury et al., 2002) is too far removed from parser evaluation proper; Copestake (2009) elaborates this argument. 3 In more recent work, Copestake (2009) shows how essentially the same reduction can be augmented with information about the underspecified scope hierarchy, so as to yield so-called Dependency MRS (which unlike EDs facilitates bidirectional conversion from and to the original MRS). 226 h h1 , h3 :pron<0:2&gt; (ARG0 x 4 {PERS 3 , NUM sg , GEND m, PRONTYPE std pron}), h5 :pronoun q<0:2&gt;(ARG0 x 4 , RSTR h6 , BODY h7 ), h8 : persuade v of<3:12&gt;(ARG0 e2 {SF prop, TENSE past , MOOD indicative}, ARG1 x 4 , ARG2 x 10 , ARG3 h9 ), h11 :proper q<13:16&gt; (ARG0 x 10 {PERS 3 , NUM sg"
W11-2927,J08-1002,0,0.0135779,"NUM PERS MOOD SF TENSE roles. One particular configuration we have found useful is to assign zero weight to the PROP triples and only evaluate ARGs and NAMEs. While the class 3 information is useful for applications such as machine translation, and ideally would be evaluated, some applications don’t make use of this information, and so, in certain scenarios, it makes sense to ignore these triples in evaluation. This configuration produces a metric broadly similar to the CCG dependencies used by Clark and Curran (2007) and also to the predicate argument structures produced by the Enju parser (Miyao and Tsujii, 2008), in terms of the information classes included, although the CCG dependencies again encode syntactic rather than semantic structure. ARG2 pronoun q pron persuade v of proper q named leave v 1 NAME NAME GEND “He” “He” “Kim” “leave.” “Kim” “Kim” m sg 3 std pron indicative prop past sg 3 indicative prop-or-ques untensed 5 Analysis To get some idea of the numeric range of the different EDM configurations, we parsed a section of the SemCor corpus (Miller et al., 1994) using the English Resource Grammar (ERG: (Flickinger, 2000)), and then calculated the average F1 -score for each rank, as ranked by"
W11-2927,W04-2407,0,0.0432271,"Missing"
W11-2927,2004.tmi-1.2,1,0.864624,"Missing"
W11-2927,W03-2401,0,\N,Missing
W11-2927,oepen-lonning-2006-discriminant,1,\N,Missing
W12-3210,P11-1051,0,0.0365451,"Missing"
W12-3210,P11-3015,0,0.0256761,"of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on"
W12-3210,W09-3011,0,0.0464009,"Missing"
W12-3210,W12-3211,1,0.623569,"he goal of a high-quality, rich-text version of the ACL Anthology as a corpus—making available both the original text and logical document structure. Although many of the subtasks sketched above did not find volunteers in this round, the Contributed Task, in our view, is an on-going, long-term community endeavor. Results to date, if nothing else, confirm the general suitability of (a) using TEI P5 markup as a shared target representation and (b) exploiting the complementarity of OCR-based techniques (Schäfer & Weitz, 2012), on the one hand, and direct interpretation of born-digital PDF files (Berg et al., 2012), on the other hand. Combining these approaches has the potential to solve the venerable challenges that stem from inhomogeneous sources in the ACL Anthology—e.g. scanned, older papers and digital newer papers, generated from a broad variety of typesetting tools. However, as of mid-2012 there still is no ready-touse, high-quality corpus that could serve as a shared starting point for the range of Anthology-based NLP activities sketched in Section 1 above. In fact, we remain slightly ambivalent about our recommendations for utilizing the current state of affairs and expected next steps—as we wo"
W12-3210,bird-etal-2008-acl,0,0.419296,"ACL Anthology has become the open access collection2 of scientific papers in the area of Computational Linguistics and Language Technology. It contains conference and workshop proceedings and the journal Computational Linguistics (formerly the American Journal of Computational Linguistics). As of Spring 2012, the ACL Anthol1 The term born-digital means natively digital, i.e. prepared electronically using typesetting systems like LATEX, OpenOffice, and the like—as opposed to digitized (or scanned) documents. 2 http://aclweb.org/anthology ogy comprises approximately 23,000 papers from 46 years. Bird et al. (2008) started collecting not only the PDF documents, but also providing the textual content of the Anthology as a corpus, the ACL Anthology Reference Corpus3 (ACL-ARC). This text version was generated fully automatically and in different formats (see Section 2.2 below), using off-theshelf tools and yielding somewhat variable quality. The main goal was to provide a reference corpus with fixed releases that researchers could use and refer to for comparison. In addition, the vision was formulated that manually corrected groundtruth subsets could be compiled. This is accomplished so far for citation li"
W12-3210,councill-etal-2008-parscit,0,0.0849057,"Missing"
W12-3210,P11-4020,0,0.0230337,"thology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaravi"
W12-3210,W11-2841,0,0.0142443,"k related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In"
W12-3210,W11-0502,0,0.0211568,"thology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaravi"
W12-3210,W12-3207,0,0.0339626,"Missing"
W12-3210,I11-1070,1,0.812466,"been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011),"
W12-3210,W11-1107,0,0.0120029,"2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et"
W12-3210,W12-3208,0,0.0287857,"analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, tracking authors over topical subfields, identifying epochs and analyzing the evolution of subfields. Sim et al. (2012) use a citation analysis to identify the changing factions within the field. Vogel & Jurafsky (2012) use topic models to explore the research topics of men and women in the ACL Anthology Network. Gupta & Rosso (2012) look for evidence of text reuse in the ACL Anthology. Most of these and related works would benefit from section (heading) information, and partly the approaches already used ad hoc solutions to gather this information from the existing plain text versions. Rich text markup (e.g. italics, tables) could also be used for linguistic, multilingual example extraction in the spirit of the ODIN project (Xia & Lewis, 2008; Xia et al., 2009). 3 Target Text Encoding To select encoding elements we adopt the TEI P5 Guidelines (TEI Consortium, 2012). The TEI encoding scheme was developed with the intentio"
W12-3210,I11-1001,0,0.0125413,"research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, tracking authors over topical subfields, identifyi"
W12-3210,D08-1038,0,0.016477,"Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, tracking authors over topical subfields, identifying epochs and analyz"
W12-3210,W11-1516,0,0.0134834,"Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, track"
W12-3210,W10-1202,0,0.0273898,". Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, tracking authors over topi"
W12-3210,W12-3213,0,0.0230421,"(biblFull). For example: We follow the PPI extraction method of &lt;ref target=&quot;#BI39&quot;&gt;Sætre et al. (2007)&lt;/ref&gt;, which is based on SVMs ... &lt;div type=&quot;bib&quot;&gt; &lt;head&gt;References&lt;/head&gt; &lt;listBibl&gt; &lt;bibl xml:id=&quot;BI39&quot;&gt; R. Sætre, K. Sagae, and J. Tsujii. 2007. Syntactic features for protein-protein interaction extraction. In &lt;hi rend=&quot;italic&quot;&gt;LBM 2007 short papers&lt;/hi&gt;. &lt;/bibl&gt; &lt;/listBibl&gt; &lt;/div&gt; A citation extraction and linking tool that is known to deliver good results on ACL Anthology papers (and even comes with CRF models trained on this corpus) is ParsCit (Councill et al., 2008). In this volume, Nhat & Bysani (2012) provide an implementation for this task using ParsCit and discuss possible further improvements. 4.4 Subtask 4: De-hyphenation Both paperXML and PDFExtract output contain soft hyphenation indicators at places where the original paper contained a line break with hyphenation. In paperXML, they are represented by the Unicode soft hyphen character (in contrast to normal dashes that also occur). PDFExtract marks hyphenation from the original text using a special element. However, both tools make errors: In some cases, the hyphens are in fact hard hyphens. The idea of this task is to combine both s"
W12-3210,C08-1087,0,0.016893,"e aim here is to get an overview and distribution of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keywor"
W12-3210,P10-1057,0,0.0167397,"logy as corpus/dataset. The aim here is to get an overview and distribution of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and"
W12-3210,P11-1110,0,0.0129629,"y other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et"
W12-3210,C10-2092,0,0.0143315,"Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL"
W12-3210,C10-1101,0,0.0155046,"ting on having used the ACL Anthology as corpus/dataset. The aim here is to get an overview and distribution of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering relat"
W12-3210,W09-3607,0,0.0665836,"n was generated fully automatically and in different formats (see Section 2.2 below), using off-theshelf tools and yielding somewhat variable quality. The main goal was to provide a reference corpus with fixed releases that researchers could use and refer to for comparison. In addition, the vision was formulated that manually corrected groundtruth subsets could be compiled. This is accomplished so far for citation links from paper to paper inside the Anthology for a controlled subset. The focus thus was laid on bibliographic and bibliometric research and resulted in the ACL Anthology Network (Radev et al., 2009) as a public, manually corrected citation database. What is currently missing is an easy-to-process XML variant that contains high-quality running text and logical markup from the layout, such as section headings, captions, footnotes, italics etc. In principle this could be derived from LATEX source files, but unfortunately, these are not available, and furthermore a considerable amount of papers have been typeset with various other word processing software. Here is where the ACL 2012 Contributed Task starts: The idea is to combine OCR and PDFBoxlike born-digital text extraction methods and re"
W12-3210,W12-3203,0,0.063806,"ˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, tracking authors over topical subfields, identifying epochs and analyzing the evolution of subfields. Sim et al. (2012) use a citation analysis to identify the changing factions within the field. Vogel & Jurafsky (2012) use topic models to explore the research topics of men and women in the ACL Anthology Network. Gupta & Rosso (2012) look for evidence of text reuse in the ACL Anthology. Most of these and related works would benefit from section (heading) information, and partly the approaches already used ad hoc solutions to gather this information from the existing plain text versions. Rich text markup (e.g. italics, tables) could also be used for linguistic, multilingual example extraction in the spirit of t"
W12-3210,W12-3206,1,0.843146,"Missing"
W12-3210,N06-1050,0,0.0293622,"additionally contains manually-corrected citation graphs, author and affiliation data for most of the Anthology (papers until 2009). 2.4 Publications with the ACL Anthology as a Corpus We did a little survey in the ACL Anthology of papers reporting on having used the ACL Anthology as corpus/dataset. The aim here is to get an overview and distribution of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro"
W12-3210,W12-3204,0,0.047359,"patterns for the identification of glossary sentences. In this workshop several authors have used the ACL Anthology to analyze the history of computational linguistics. Radev & Abu-Jbara (2012) examine research trends through the citing sentences in the ACL Anthology Network. Anderson et al. (2012) use the ACL Anthology to perform a peoplecentered analysis of the history of computational linguistics, tracking authors over topical subfields, identifying epochs and analyzing the evolution of subfields. Sim et al. (2012) use a citation analysis to identify the changing factions within the field. Vogel & Jurafsky (2012) use topic models to explore the research topics of men and women in the ACL Anthology Network. Gupta & Rosso (2012) look for evidence of text reuse in the ACL Anthology. Most of these and related works would benefit from section (heading) information, and partly the approaches already used ad hoc solutions to gather this information from the existing plain text versions. Rich text markup (e.g. italics, tables) could also be used for linguistic, multilingual example extraction in the spirit of the ODIN project (Xia & Lewis, 2008; Xia et al., 2009). 3 Target Text Encoding To select encoding ele"
W12-3210,W06-0804,0,0.0222859,"additionally contains manually-corrected citation graphs, author and affiliation data for most of the Anthology (papers until 2009). 2.4 Publications with the ACL Anthology as a Corpus We did a little survey in the ACL Anthology of papers reporting on having used the ACL Anthology as corpus/dataset. The aim here is to get an overview and distribution of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Ro"
W12-3210,E09-1099,0,0.0134225,"the changing factions within the field. Vogel & Jurafsky (2012) use topic models to explore the research topics of men and women in the ACL Anthology Network. Gupta & Rosso (2012) look for evidence of text reuse in the ACL Anthology. Most of these and related works would benefit from section (heading) information, and partly the approaches already used ad hoc solutions to gather this information from the existing plain text versions. Rich text markup (e.g. italics, tables) could also be used for linguistic, multilingual example extraction in the spirit of the ODIN project (Xia & Lewis, 2008; Xia et al., 2009). 3 Target Text Encoding To select encoding elements we adopt the TEI P5 Guidelines (TEI Consortium, 2012). The TEI encoding scheme was developed with the intention of being applicable to all types of natural language, and facilitating the exchange of textual data among researchers across discipline. The guidelines are implemented in XML; we currently use inline markup, but stand-off annotations have also been applied (Ba´nski & Przepiórkowski, 2009). We use a subset of the TEI P5 Guidelines as not all elements were deemed necessary. This pro90 cess was made easier through Roma5 , an online to"
W12-3210,W11-2843,0,0.0139021,"06a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction patterns for the identification"
W12-3210,I08-1069,0,0.0129123,"nalysis to identify the changing factions within the field. Vogel & Jurafsky (2012) use topic models to explore the research topics of men and women in the ACL Anthology Network. Gupta & Rosso (2012) look for evidence of text reuse in the ACL Anthology. Most of these and related works would benefit from section (heading) information, and partly the approaches already used ad hoc solutions to gather this information from the existing plain text versions. Rich text markup (e.g. italics, tables) could also be used for linguistic, multilingual example extraction in the spirit of the ODIN project (Xia & Lewis, 2008; Xia et al., 2009). 3 Target Text Encoding To select encoding elements we adopt the TEI P5 Guidelines (TEI Consortium, 2012). The TEI encoding scheme was developed with the intention of being applicable to all types of natural language, and facilitating the exchange of textual data among researchers across discipline. The guidelines are implemented in XML; we currently use inline markup, but stand-off annotations have also been applied (Ba´nski & Przepiórkowski, 2009). We use a subset of the TEI P5 Guidelines as not all elements were deemed necessary. This pro90 cess was made easier through R"
W12-3210,W10-0402,1,0.858122,"nt NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are address"
W12-3210,C10-2145,0,0.0132565,"ting on having used the ACL Anthology as corpus/dataset. The aim here is to get an overview and distribution of the different NLP research tasks that have been pursued using the ACL Anthology as dataset. There are probably other papers outside the Anthology itself, but these have not been looked at. The pioneers working with the Anthology as corpus are Ritchie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering relat"
W12-3210,W11-2842,0,0.0151029,"ie et al. (2006a, 2006b). They did work related to citations which also forms the largest topic cluster of papers applying or using Anthology data. Later papers on citation analysis, summarization, classification, etc. are Qazvinian et al. (2010), AbuJbara & Radev (2011), Qazvinian & Radev (2010), Qazvinian & Radev (2008), Mohammad et al. (2009), Athar (2011), Schäfer & Kasterka (2010), and Dong & Schäfer (2011). Text summarization research is performed in Qazvinian & Radev (2011) and Agarwal et al. (2011a, 2011b). The HOO (“Help our own”) text correction shared task (Dale & Kilgarriff, 2010; Zesch, 2011; Rozovskaya et al., 2011; Dahlmeier et al., 2011) aims at developing automated tools and techniques that assist authors, e.g. non-native speakers of English, in writing (better) scientific publications. Classification/Clustering related publications are Muthukrishnan et al. (2011) and Mao et al. (2010). Keyword extraction and topic models based on Anthology data are addressed in Johri et al. (2011), Johri et al. (2010), Gupta & Manning (2011), Hall et al. (2008), Tu et al. (2010) and Daudaraviˇcius (2012). Reiplinger et al. (2012) use the ACL Anthology to acquire and refine extraction pattern"
W12-3210,P11-4002,1,\N,Missing
W12-3210,W12-3202,0,\N,Missing
W12-3210,W12-3201,0,\N,Missing
W12-3210,W12-3212,1,\N,Missing
W12-3210,W10-4236,0,\N,Missing
W12-3211,W12-3210,1,0.734631,"ol. Sections Identify section header styles by compiling a list of styles that are either larger than or have some emphasis on the body text style, and have instances with evidence of section numbering (e.g. 1.1, (1a)). Infer the nesting level of each section header style from its order of occurrence in the document; a section heading will always appear earlier than a subsection heading, for instance. Having identified the different components in the document, these are used to create a logical hierarchical representation following the TEI P5 Guidelines (TEI Consortium, 2012) as introduced by Schäfer et al. (2012). Title, abstract, floaters, and figures are separated from the main text. The body of the document is then collated into a tree of section elements, with headers and body text. Body text is collected by combining consecutive text blocks that have identical styles, before inferring paragraphs on the basis of indented initial lines. Dehyphenation is tackled using a combination of a lexicon and a set of orthographic rules. 6 Discussion—Outlook PDFExtract provides a fresh and open-source take on the problem of high-quality content and structure extraction from born-digital PDFs. Unlike existing i"
W12-3211,W12-3212,0,\N,Missing
W12-3602,branco-etal-2010-developing,0,0.0137066,"Specifically, the Deep Linguistic Processing with HPSG Initiative (DELPH-IN1 ) has produced both manually and automatically annotated resources making available comparatively fine-grained syntactic and semantic analyses in the framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994). For English, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take"
W12-3602,P05-1067,0,0.0176179,"ilar information can take quite different forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonym"
W12-3602,flickinger-etal-2010-wikiwoods,1,0.847029,"Missing"
W12-3602,W09-1201,0,0.0731308,"Missing"
W12-3602,W07-2416,0,0.121834,"e in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities,"
W12-3602,P03-1054,0,0.00636733,"he representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices with respect to head status are largely substantive. The dependency relations employed for this representation are PropBank semantic roles, such as A0 (proto-agent), A1 (protopatient), and various modifier roles. Stanford Basic Dependencies (SB) The Stanford Dependency scheme, a popular alternative to CoNLL-style syntactic dependencies (CD), was originally provided as an additional output format for the Stanford parser (Klein & Manning, 2003). It is a result of a conversion from PTB-style phrase structure trees (be they gold standard or automatically produced)—combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives (Marneffe et al., 2006). The so-called basic format provides a dependency graph which conforms to the criteria listed above, and the heads are largely content rather than function words. The grammatical relations are organized in a hierarchy, rooted in the generic relation ‘dependent’ and containing 56 different relations (Marneffe & Man"
W12-3602,J93-2004,0,0.0479705,"pendency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities, frameworks, and languages. Moreover, many of the gold-standard dependency banks were created by automated conversion from pre-existing constituency treebanks— notably the venerable Penn Treebank for English (PTB; Marcus et al., 1993)—and there exist several conversion toolkits which convert from constituent structures to dependency structures. This conversion is not always trivial, and the outputs can differ notably in choices concerning head status, relation inventories, and formal graph properties of the resulting depedency structure. Incompatibilty of representations and differences in the ‘granularity’ of linguistic information hinder the evaluation of parsers across communities (Sagae et al., 2008). In this paper, we pursue theoretical as well as practical goals. First, we hope to shed more light on commonalities and"
W12-3602,marimon-2010-spanish,0,0.0134134,"ep Linguistic Processing with HPSG Initiative (DELPH-IN1 ) has produced both manually and automatically annotated resources making available comparatively fine-grained syntactic and semantic analyses in the framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994). For English, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take as 1 See http:/"
W12-3602,de-marneffe-etal-2006-generating,0,0.00979249,"on are PropBank semantic roles, such as A0 (proto-agent), A1 (protopatient), and various modifier roles. Stanford Basic Dependencies (SB) The Stanford Dependency scheme, a popular alternative to CoNLL-style syntactic dependencies (CD), was originally provided as an additional output format for the Stanford parser (Klein & Manning, 2003). It is a result of a conversion from PTB-style phrase structure trees (be they gold standard or automatically produced)—combining ‘classic’ head finding rules with rules that target specific linguistic constructions, such as passives or attributive adjectives (Marneffe et al., 2006). The so-called basic format provides a dependency graph which conforms to the criteria listed above, and the heads are largely content rather than function words. The grammatical relations are organized in a hierarchy, rooted in the generic relation ‘dependent’ and containing 56 different relations (Marneffe & Manning, 2008), largely based on syntactic functions. sb-hd_mc_c { e12 sp-hd_n_c hd-cmp_u_c v_prd_is_le d_-_sg-nmd_le 1 :_a_q( BV x 6 ) e9 :_similar_a_to(ARG1 x 6 ) x 6 :_technique_n_1 e12 :_almost_a_1(ARG1 e3 ) e3 :_impossible_a_for(ARG1 e18 ) e18 :_apply_v_to(ARG2 x 6 , ARG3 x 19 ) 2"
W12-3602,W08-1301,0,0.126868,"Missing"
W12-3602,J11-1007,0,0.0165324,"ral of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) for a range of different languages. These data sets have enabled rigorous evaluation of parsers and have spurred considerable progress in the field of data-driven dependency parsing (McDonald & Nivre, 2011). Despite widespread use, dependency grammar does not represent a unified grammatical framework and there are large representational differences across communities, frameworks, and languages. Moreover, many of the gold-standard dependency banks were created by automated conversion from pre-existing constituency treebanks— notably the venerable Penn Treebank for English (PTB; Marcus et al., 1993)—and there exist several conversion toolkits which convert from constituent structures to dependency structures. This conversion is not always trivial, and the outputs can differ notably in choices conc"
W12-3602,J05-1004,0,0.0583595,"ST collection were obtained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices wit"
W12-3602,W04-2705,0,0.0333196,"tained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designated root node, the graph is not connected, and the graph is not acyclic. The choices with respect to head stat"
W12-3602,D09-1001,0,0.0165104,"nt forms across frameworks. We further seek to shed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this"
W12-3602,read-etal-2012-wesearch,1,0.863524,"Missing"
W12-3602,oepen-lonning-2006-discriminant,1,0.569818,"ries. In Section 4 below, we convert DELPH-IN derivations into syntactic bilexical dependencies. DELPH-IN Minimal Recursion Semantics (DM) As part of the full HPSG sign, the ERG also makes available a logical-form representation of propositional semantics in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005). While MRS proper utilizes a variant of predicate calculus that affords underspecification of scopal relations, for our goal of projecting semantic forms onto bilexical dependencies, we start from the reduction of MRS into the Elementary Dependency Structures (EDS) of Oepen & Lønning (2006), as shown in Figure 2. EDS is a lossy (i.e. non-reversible) conversion from MRS into a variable-free dependency graph; graph nodes (one per line in Figure 2) correspond to elementary predications from the original logical form and are connected by arcs labeled with MRS argument indices: ARG1, ARG2, etc. (where BV is reserved for what is the bound variable of a quantifier in the full MRS).6 Note that, while EDS already brings us relatively close to the other formats, there are graph nodes that do not correspond to individual words from our running example, for example the underspecified quanti"
W12-3602,P09-2010,1,0.844405,"glish, the so-called LinGO Redwoods Treebank (Oepen et al., 2004) contains gold-standard annotations for some 45,000 utterances in five broad genres and domains; comparable resources exist for Japanese (Bond et al., 2004) and are currently under construction for Portuguese and Spanish (Branco et al., 2010; Marimon, 2010). We develop an automated, parameterizable conversion procedure for these resources that maps HPSG analyses into either syntactic or semantic bilexical dependencies. Similar conversion procedures have recently been formulated for functional structures within the LFG framework (Øvrelid et al., 2009; Cetinoglu et al., 2010). In the design of this unidirectional (i.e. lossy) mapping, we apply and corroborate the cross-framework observations made in the more linguistic part of this study. The paper has the following structure: Section 2 introduces the corpus and annotations we take as 1 See http://www.delph-in.net for background. 3 our point of departure; Section 3 contrasts analyses of select linguistic phenomena by example; and Section 4 develops an automated conversion from HPSG analyses to bilexical dependencies. 2 The Multi-Annotated PEST Corpus At the 2008 Conference on Computational"
W12-3602,W08-2121,0,0.0449557,"syntactic or semantic bilexical relations. For English, syntactic dependencies in the PEST collection were obtained by converting PTB trees with the PennConverter software (Johansson & Nugues, 2007), which relies on head finding rules (Magerman, 1994; Collins, 1999) and the functional anno3 Among others, annotations in the Prague Dependency format would be interesting to compare to, but currently these are unfortunately not among the formats represented in the PEST corpus. 4 CoNLL PropBank Semantics (CP) For the 2008 CoNLL shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), the PropBank and NomBank annotations ‘on top’ of the PTB syntax (Palmer et al., 2005; Meyers et al., 2004) were converted to bilexical dependency form. This conversion was based on the dependency syntax already obtained for the same data set (CD, above) and heuristics which identify the semantic head of an argument with its syntactic head.The conversion further devotes special attention to arguments with several syntactic heads, discontinuous arguments, and empty categories (Surdeanu et al., 2008). The representation does not adhere to the formal constraints posed above; it lacks a designate"
W12-3602,J09-3003,0,0.0079248,"hed light on the representational ‘distance’ between pure bilexical dependencies, on the one hand, and full-blown logical-form propositional semantics, on the other hand. Furthermore, we propose a fully automated conversion procedure from (logical-form) meaning representation to bilexical semantic dependencies.† 1 Introduction—Motivation Dependency representations have in recent years received considerable attention from the NLP community, and have proven useful in diverse tasks such as Machine Translation (Ding & Palmer, 2005), Semantic Search (Poon & Domingos, 2009), and Sentiment Analysis (Wilson et al., 2009). Dependency representations are often claimed to be more ‘semantic’ in spirit, in the sense that they directly express predicate–argument relations, i.e. Who did What to Whom? Several of the shared tasks of the Conference on Natural Language Learning (CoNLL) in the past years have focused on datadriven dependency parsing—producing both syntactic (Nivre et al., 2007) and semantic dependencies (Hajiˇc et al., 2009)—and have made available gold † We are indebted to Emily Bender, Rebecca Dridan, and four anonymous reviewers for their feedback on this work. standard data sets (dependency banks) fo"
W12-3602,J03-4003,0,\N,Missing
W12-3602,D07-1096,0,\N,Missing
W13-5633,flickinger-etal-2010-wikiwoods,1,0.912826,"ging and parsing (e.g. italics may indicate foreign language words and links may offer clues to constituent structure). However, exactly which markup elements are relevant for which tasks in natural language processing remains to be explored. In order to answer comprehensively, it is important to normalise the disparate types of markup (such as LATEX, WikiText, or XML) so that they can be treated generally. For such normalisation we provide options that allow a user to specify how different elements of markup should be treated, including a configurable mapping option. Following previous work (Flickinger et al., 2010; Solberg, 2012), we have used this option to map certain source elements into an abstraction of these markup languages dubbed the Grammar Markup Language (described in Section 3). In Section 4 we evaluate the tool using two collections of text from blogs, and provide an error analysis. We then summarise our work and outline the future directions for improvements in Section 5. 2 A Generic Approach to Segmenting Text with Inline Markup In this section we describe our generic approach to sentence boundary detection in text with inline markup. The approach is generic in that it is (a) applicable"
W13-5633,I11-1100,0,0.0214878,"Missing"
W13-5633,P11-2008,0,0.0249146,"Missing"
W13-5633,J03-3001,0,0.0138476,"ry detection using any external segmenter, while producing segments containing normalised markup, with an account of how to recreate the original form. Keywords: Accountability, Markup, Normalisation, Sentence Boundary Detection, Traceability. ∗ Work carried out at the University of Oslo. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 365 of 474] 1 Introduction Attention in Natural Language Processing (NLP) research increasingly focuses on text from the World Wide Web, the so-called ‘Web as Corpus’ (Kilgarriff and Grefenstette, 2003). This fertile source of language use has enriched the field by greatly expanding the genres and styles of text being analysed by standard NLP techniques, including much information available as user-generated content. One outcome of this recent attention is a host of research showing the degradation of performance on web text when using tools trained on, for example, the venerable Penn Treebank (Marcus et al., 1993). Techniques such as domain adaptation (Foster et al., 2011) and normalisation (Gimpel et al., 2011) have been shown to reduce the impact on performance of tools such as statistica"
W13-5633,C12-2096,1,0.665077,"the following sections, we look at a different technique for handling issues at this often-overlooked stage. While the differences in style and register can have an effect on sentence segmentation, the major impact is from the markup (such as HTML or WikiText) that is present in web-sourced text. Reuse of standard tools is generally desirable, since we can take advantage of previous tuning on plain text, but using tools designed for plain text on text with markup can result in a major loss in recall of sentence boundaries. This occurs when markup elements obscure sentence-ending punctuation (Read et al., 2012b). For instance, in the example given in Figure 1 the sentence terminating full-stop is attached to the closing tag. Thus, any tool that operates under the assumption that full-stops must have whitespace to the right will fail to detect this boundary. To avoid this problem, markup could be—and often is—stripped and discarded during processing. However, another frequently ignored aspect of preprocessing is accountability: the recording of actions taken that alter the text, in order to preserve the link to the original data. Many applications of NLP augment the original document with new inform"
W13-5633,read-etal-2012-wesearch,1,0.484484,"the following sections, we look at a different technique for handling issues at this often-overlooked stage. While the differences in style and register can have an effect on sentence segmentation, the major impact is from the markup (such as HTML or WikiText) that is present in web-sourced text. Reuse of standard tools is generally desirable, since we can take advantage of previous tuning on plain text, but using tools designed for plain text on text with markup can result in a major loss in recall of sentence boundaries. This occurs when markup elements obscure sentence-ending punctuation (Read et al., 2012b). For instance, in the example given in Figure 1 the sentence terminating full-stop is attached to the closing tag. Thus, any tool that operates under the assumption that full-stops must have whitespace to the right will fail to detect this boundary. To avoid this problem, markup could be—and often is—stripped and discarded during processing. However, another frequently ignored aspect of preprocessing is accountability: the recording of actions taken that alter the text, in order to preserve the link to the original data. Many applications of NLP augment the original document with new inform"
W13-5633,P11-4002,0,0.0124387,"the sentence terminating full-stop is attached to the closing tag. Thus, any tool that operates under the assumption that full-stops must have whitespace to the right will fail to detect this boundary. To avoid this problem, markup could be—and often is—stripped and discarded during processing. However, another frequently ignored aspect of preprocessing is accountability: the recording of actions taken that alter the text, in order to preserve the link to the original data. Many applications of NLP augment the original document with new information. For example, the ACL Anthology Searchbench (Schäfer et al., 2011) highlights search hits in text from scientific papers—such annotations would also be useful in HTML and other source documents. When unilaterally throwing away markup, it becomes difficult to trace back to the original source and hence one loses some of the utility of the annotations from downstream processing. Alternatively, in corpus creation, discarding information without record can unnecessarily limit future unforeseen uses of the corpus, whereas full accountability leaves open possibilities that cannot be predicted at creation time. To facilitate the accountability of preprocessing when"
W13-5633,J93-2004,0,\N,Missing
W13-5642,heid-etal-2010-corpus,0,0.0279013,"e, users are presented with the Galaxy workspace, as shown in Figure 1. The left panel displays the installed processing tools; clicking on tool-names displays general information and configuration options in the center panel. In order to be integrated in the LAP tool-chain, existing tools are ‘wrapped’ inside scripts that decode the LAP-internal format, present the tool itself with its expected input and finally re-encode the output so that it is compatible with the next processing step. For LAP’s system-internal representation, we are at the moment looking into both TCF (Text Corpus Format, Heid et al., 2010) (the format used within WebLicht, which comes with a full, albeit closed-source API) and our own in-house JSON-based LTON format (Language Technology Object Notation) which is still currently under development. Additionally, the wrapper handles the submission of the job to the Abel cluster, a shared resource for research computing boasting more than 600 machines, totaling more than 10.000 cores (CPUs).4 The HPC connection is a very important feature, as Language technology can be computationally quite expensive, often involving sub-problems where known best solutions 3 For more information ab"
W13-5707,adolphs-etal-2008-fine,1,0.821468,"al., 2009). Table 1 provides exact sentence, token, and type counts for these data sets. Tokenization Conventions A relevant peculiarity of the DeepBank and Redwoods annotations in this context is the ERG approach to tokenization. Three aspects in Figure 1 deviate from the widely used PTB conventions: (a) hyphens (and slashes) introduce token boundaries; (b) whitespace in multi-word lexical units (like ad hoc, of course, or Mountain View) does not force token boundaries; and (c) punctuation marks are attached as ‘pseudo-affixes’ to adjacent words, reflecting the rules of standard orthography. Adolphs et al. (2008) offer some linguistic arguments for this approach to tokenization, but for our purposes it suffices to note that these differences to PTB tokenization may in part counter-balance each other, but do increase the types-per-tokens ratio somewhat. This property of the DeepBank annotations, arguably, makes English look somewhat similar to languages with moderate inflectional morphology. To take advantage of the finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on"
W13-5707,H91-1060,0,0.216164,"ency parsing), instantiating a comparatively diverse range of domains and genres (Oepen et al., 2004). Adding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al"
W13-5707,D12-1133,0,0.176439,"bserve the strongest correspondence between DT and CoNLL (at a Jaccard index of 0.49, compared to 0.32 for DT and Stanford, and 0.43 between CoNLL and Stanford). posed to its developers until the grammar and disambiguation model were finalized and frozen for this release. Ivanova et al. (2013) complement this comparison of dependency schemes through an empirical assesment in terms of ‘parsability’, i.e. accuracy levels available for the different target representations when training and testing a range of state-of-the-art parsers on the same data sets. In their study, the dependency parser of Bohnet and Nivre (2012), henceforth B&N, consistently performs best for all schemes and output configurations. Furthermore, parsability differences between the representations are generally very small. Based on these observations, we conjecture that DT is as suitable a target representation for parser comparison as any of the others. Furthermore, two linguistic factors add to the attractiveness of DT for our study: it is defined in terms of a formal (and implemented) theory of grammar; and it makes available more finegrained lexical categories, ERG lexical types, than is common in PTB-derived dependency banks. Cross"
W13-5707,P06-2006,0,0.0126994,"dding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they showed that the loss of accu"
W13-5707,W97-1502,0,0.0106444,"s (e.g. subject–head or head–complement: sbhd_mc_c and hd-cmp_u_c, respectively; see below for more details on unary rules). Preterminals are labeled with fine-grained lexical categories, dubbed ERG lexical types, that augment common parts of speech with additional information, for example argument structure or the distinction between count, mass, and proper nouns. In total, the ERG distinguishes about 250 construction types and 1000 lexical types. DeepBank annotations were created by combining the native ERG parser, dubbed PET (Callmeier, 2002), with a discriminant-based tree selection tool (Carter, 1997; Oepen et al., 2004), thus making it possible for annotators to navigate the large space of possible analyses efficiently, identify and validate the intended reading, and record its full HPSG analysis in the treebank. Owing to this setup, DeepBank in its current version 1.0 lacks analyses for some 15 percent of the WSJ sentences, for which either the ERG parser failed to suggest a set of candidates (within certain bounds on time and memory usage), or the annotators found none of the available parses acceptable.3 Furthermore, DeepBank annotations to date only comprise the first 21 sections of"
W13-5707,cer-etal-2010-parsing,0,0.0534845,"Missing"
W13-5707,P07-1032,0,0.0143626,"n in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they showed that the loss of accuracy due to the mapping process can swamp any actual parser differences. As long as heuristic conversion is required before evaluation, cross-framework comparison inevitably includes a level of fuzziness. An alternative approach is possible when there is enough data available in a particular representation, and convers"
W13-5707,W08-1301,0,0.0449293,"Missing"
W13-5707,D13-1120,1,0.827329,"arser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior to forest construction yields greatly improved efficiency at a moderate accuracy loss. Her lexical pruning model is trained on DeepBank 00–19 too, hence compatible with our setup. We include the bestperforming configuration of Dridan (2013) in our experiments, a variant henceforth referred to as ERGe . Unlike the other parsers in our study, PET internally operates over an ambiguous token lattice, and there is no easy interface to feed the parser pre-tokenized inputs. W"
W13-5707,I11-1100,0,0.0285816,"Missing"
W13-5707,P10-1035,0,0.0184554,"rocess can swamp any actual parser differences. As long as heuristic conversion is required before evaluation, cross-framework comparison inevitably includes a level of fuzziness. An alternative approach is possible when there is enough data available in a particular representation, and conversion (if any) is deterministic. Cer et al. (2010) used Stanford Dependencies (de Marneffe & Manning, 2008) to evaluate a range of statistical parsers. Pre- or post-converting from PTB phrase structure trees to the Stanford dependency scheme, they were able to evaluate a large number of different parsers. Fowler and Penn (2010) formally proved that a range of Combinatory Categorial Grammars (CCGs) are context-free. They trained the PCFG Berkeley parser on CCGBank, the CCG annotation of the PTB WSJ text (Hockenmaier & Steedman, 2007), advancing the state of the art in terms of supertagging accuracy, PARSEVAL measures, and CCG dependency accuracy. In other words, a specialized CCG parser is not necessarily more accurate than the generalpurpose Berkeley parser; this study, however, fails to also take parser efficiency into account. In related work for Dutch, Plank and van Noord (2010) suggest that, intuitively, one sho"
W13-5707,W01-0521,0,0.0808273,"Missing"
W13-5707,J07-3004,0,0.0193412,"h is possible when there is enough data available in a particular representation, and conversion (if any) is deterministic. Cer et al. (2010) used Stanford Dependencies (de Marneffe & Manning, 2008) to evaluate a range of statistical parsers. Pre- or post-converting from PTB phrase structure trees to the Stanford dependency scheme, they were able to evaluate a large number of different parsers. Fowler and Penn (2010) formally proved that a range of Combinatory Categorial Grammars (CCGs) are context-free. They trained the PCFG Berkeley parser on CCGBank, the CCG annotation of the PTB WSJ text (Hockenmaier & Steedman, 2007), advancing the state of the art in terms of supertagging accuracy, PARSEVAL measures, and CCG dependency accuracy. In other words, a specialized CCG parser is not necessarily more accurate than the generalpurpose Berkeley parser; this study, however, fails to also take parser efficiency into account. In related work for Dutch, Plank and van Noord (2010) suggest that, intuitively, one should expected that a grammar-driven system can be more resiliant to domain shifts than a purely data-driven parser. In a contrastive study on parsing into Dutch syntactic dependencies, they substantiated this e"
W13-5707,P13-3005,1,0.838136,"petitions (Nivre et al., 2007) and the ‘basic’ variant of Stanford Dependencies. They observe that the three dependency representations are broadly comparable in granularity and that there are substantial structural correspondences between the schemes. Measured as average Jaccard similarity over unlabeled dependencies, they observe the strongest correspondence between DT and CoNLL (at a Jaccard index of 0.49, compared to 0.32 for DT and Stanford, and 0.43 between CoNLL and Stanford). posed to its developers until the grammar and disambiguation model were finalized and frozen for this release. Ivanova et al. (2013) complement this comparison of dependency schemes through an empirical assesment in terms of ‘parsability’, i.e. accuracy levels available for the different target representations when training and testing a range of state-of-the-art parsers on the same data sets. In their study, the dependency parser of Bohnet and Nivre (2012), henceforth B&N, consistently performs best for all schemes and output configurations. Furthermore, parsability differences between the representations are generally very small. Based on these observations, we conjecture that DT is as suitable a target representation fo"
W13-5707,W12-3602,1,0.908199,"hrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG analyses recorded in DeepBank, Zhang and Wang (2009) and Ivanova et al. (2012) define a reduction into bi-lexical syntactic dependencies, which they call Derivation TreeDerived Dependencies (DT). Through application of the converter of Ivanova et al. (2012) to DeepBank, we can thus obtain a DT-annotated version of the standard WSJ text, to train and test a data-driven dependency and phrase structure parser, respectively, and to compare parsing results to a hybrid, grammar-driven HPSG parser. Furthermore, we can draw on a set of additional corpora annotated in the same HPSG format (and thus amenable to conversion for both phrase structure and dependency parsing), instant"
W13-5707,W03-2401,0,0.0464787,"en et al., 2004). Adding this data to our setup for additional cross-domain testing, we seek to document not only what trade-offs apply in terms of dependency accuracy vs. parser efficiency, but also how these trade-offs are affected by domain and genre variation, and, more generally, how resilient the different approaches are to variation in parser inputs. 63 2 Related Work Comparing between parsers from different frameworks has long been an area of active interest, ranging from the original PARSEVAL design (Black et al., 1991), to evaluation against ‘formalism-independent’ dependency banks (King et al., 2003; Briscoe & Carroll, 2006), to dedicated workshops (Bos et al., 2008). Grammatical Relations (GRs; Briscoe & Carroll, 2006) have been the target of a number of benchmarks, but they require a heuristic mapping from ‘native’ parser outputs to the target representations for evaluation, which makes results hard to interpret. Clark and Curran (2007) established an upper bound by running the mapping process on gold-standard data, to put into perspective the mapped results from their CCG parser proper. When Miyao et al. (2007) carried out the same experiment for a number of different parsers, they sh"
W13-5707,J93-2004,0,0.042212,"reduction in linguistic detail. 2 In contrast, much earlier work on cross-framework comparison involved post-processing parser outputs in form and content, into a target representation for which gold-standard annotations were available. In § 2 below, we argue that such conversion inevitably introduces blur into the comparison. ited to purely data-driven (or statistical) parsers, i.e. systems where linguistic knowledge is exclusively acquired through supervised machine learning from annotated training data. For English, the venerable Wall Street Journal (WSJ) portion of the Penn Treebank (PTB; Marcus et al., 1993) has been the predominant source of training data, for phrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG a"
W13-5707,D07-1013,0,0.0391918,"nd (c) the distribution of LAS over different lexical categories. Among the different dependency types, we observe that the notion of an adjunct is difficult for all three parsers. One of the hardest dependency labels is hdn-aj (post-adjunction to a nominal head), the relation employed for relative clauses and prepositional phrases attaching to a nominal head. The most common error for this relation is verbal attachment. It has been noted that dependency parsers may exhibit systematic performance differences with respect to dependency length (i.e. the distance between a head and its argument; McDonald & Nivre, 2007). In our experiments, we find that the parsers perform comparably on longer dependency arcs (upwards of fifteen words), with ERGa constantly showing the highest accuracy, and Berkeley holding a slight edge over B&N as dependency length increases. In Figure 3, one can eyeball accuracy levels per lexical category, where conjunctions (c) and various types of prepositions (p and pp) are the most difficult for all three parsers. That the DT analysis of coordination is challenging is unsurprising. Schwartz et al. 68 CB SC VM (2012) show that choosing conjunctions as heads in coordinate structures is"
W13-5707,A00-2022,1,0.62018,"epBank annotations, arguably, makes English look somewhat similar to languages with moderate inflectional morphology. To take advantage of the finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on our parser comparison. PET: Native HPSG Parsing The parser most commonly used with the ERG is called PET (Callmeier, 2002), a highly engineered chart parser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior"
W13-5707,P06-1055,0,0.114573,"Missing"
W13-5707,W10-2105,0,0.0233349,"Missing"
W13-5707,C12-1147,0,0.0515236,"Missing"
W13-5707,W11-2923,0,0.0197785,"enization mismatches local to some sub-segment of the input will not ‘throw off’ token correspondences in other parts of the string.5 We will refer to this character-based variant of the standard CoNLL metrics as LASc and UASc . 4 PCFG Parsing of HPSG Derivations Formally, the HPSG analyses in the DeepBank and Redwoods treebanks transcend the class of contextfree grammars, of course. Nevertheless, one can pragmatically look at an ERG derivation as if it were a context-free phrase structure tree. On this view, standard, off-the-shelf PCFG parsing techniques are applicable to the ERG treebanks. Zhang and Krieger (2011) explore this space experimentally, combining the ERG, Redwoods (but not DeepBank), and massive collections of automatically parsed text. Their study, however, does not consider parser efficiency.6 . In contrast, our goal is to reflect on practical tradeoffs along multiple dimensions. We therefore focus on Berkeley, as one of the currently best-performing (and relatively efficient) PCFG engines. Due to its ability to internally rewrite node labels, this parser should be expected to adapt well also to ERG derivations. Compared to the phrase structure annotations in the PTB, there are two struct"
W13-5707,W07-2207,1,0.846594,"finegrained ERG lexical categories, most of our experiments assume ERG tokenization. In two calibration experiments, however, we also investigate the effects of tokenization differences on our parser comparison. PET: Native HPSG Parsing The parser most commonly used with the ERG is called PET (Callmeier, 2002), a highly engineered chart parser for unification grammars. PET constructs a complete parse forest, using subsumption-based ambiguity factoring (Oepen & Carroll, 2000), and then extracts from the forest n-best lists of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–19 of DeepBank and otherwise used the default configuration (which corresponds to the environment used by the DeepBank and Redwoods developers), which is optimized for accuracy. This parser, performing exact inference, we will call ERGa . In recent work, Dridan (2013) augments ERG parsing with lattice-based sequence labeling over lexical types and lexical rules. Pruning the parse chart prior to forest construction yields greatly improved efficiency at a moderate accuracy loss. Her lexical pruning model is trained on DeepBank 00–"
W13-5707,P09-1043,0,0.474619,"ce of training data, for phrase structure and dependency parsers alike. Two recent developments make it possible to broaden the range of parsing approaches that can be assessed empirically on the task of deriving bi-lexical syntactic dependencies. Flickinger et al. (2012) make available another annotation layer over the same WSJ text, ‘deep’ syntacto-semantic analyses in the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard & Sag, 1994; Flickinger, 2000). This resource, dubbed DeepBank, is available since late 2012. For the type of HPSG analyses recorded in DeepBank, Zhang and Wang (2009) and Ivanova et al. (2012) define a reduction into bi-lexical syntactic dependencies, which they call Derivation TreeDerived Dependencies (DT). Through application of the converter of Ivanova et al. (2012) to DeepBank, we can thus obtain a DT-annotated version of the standard WSJ text, to train and test a data-driven dependency and phrase structure parser, respectively, and to compare parsing results to a hybrid, grammar-driven HPSG parser. Furthermore, we can draw on a set of additional corpora annotated in the same HPSG format (and thus amenable to conversion for both phrase structure and de"
W13-5715,H91-1060,0,0.195465,"omewhat ambivalently (since we experience these ‘tweaks’ as obstacles to transparency and replicability), we mimic the standard practice of discarding triples labelled TOP (the root category) and - NONE - (empty elements, absent in most parser outputs), as well as truncating labels at the first hyphen (i.e. ignoring function tags); furthermore, we allow the commonly accepted equivalence of ADVP and PRT, even though we have been unable to trace the origins of this convention. However, we do not follow the convention of ignoring tokens tagged as punctuation, for two reasons. First, Black et al. (1991) suggested ignoring punctuation for independently developed, hand-built parsers, to reduce variation across linguistic theories; in evaluating statistical parsers trained directly on PTB structures, however, it is natural to include syntactic information related to punctuation in the evaluation—as is common practice in data-driven dependency parsing since the 2007 CoNLL Shared Task. Second, the evalb technique of identifying punctuation nodes merely by PoS tags is problematic in the face of tagging errors; and even if one were to ‘project’ gold-standard PoS tags onto parser outputs, there woul"
W13-5715,P05-1022,0,0.0158613,"aty et al. (2012), that is specific to their situation of allowing ambiguous morphological segmentation only within pre-segmented tokens. In our more general case, we consider enumerating all possible tokenisations of a sentence (or document) neither practical or desirable, nor do we wish to enforce token breaks at whitespace, since different tokenisation conventions such as those described by Fares et al. (2013) allow mid-token spaces. 4 Experimental Setup To evaluate end-to-end parsing, we selected three commonly used phrase structure parsers: the Charniak and Johnson reranking parser (C&J; Charniak & Johnson, 2005), the Berkeley parser (Petrov, Barrett, Thibaux, & Klein, 2006), and the Stanford parser (Klein & Manning, 2003). For each parser, we used the recommended pre-trained English model, and mostly default settings.3 All parsers tokenise their in3 We used the -accurate setting for the Berkeley parser, to improve coverage on our out-of-domain data. Full configuration details and other background, including an open-source implementation of our evaluation framework will be distributed through a companion website. put by default and the Stanford parser also includes a sentence splitting component. Sinc"
W13-5715,P12-2074,1,0.835149,"can lead to two incorrect sentence spans. 130 erately less accurate for the edited text, but radically under-segments the messier web text, returning 12914 sentences, compared to 14954 from tokenizer. For tokenisation accuracy, there’s very little difference between Berkeley and Stanford, which is not surprising, since the Berkeley parser tokeniser was built using the Stanford code. The in-built tokeniser in the C&J parser however is significantly less accurate. While the drop in F1 doesn’t seem too large, examining the sentence accuracy shows a much bigger effect. This echoes the findings of Dridan and Oepen (2012) which demonstrate that the built-in tokeniser of the C&J parser is well below state-ofthe-art. For completeness, we also looked at their best performing tokeniser (REPP) on tokenizersegmented sentences, adding those token accuracy numbers to Table 2. Here we see that relative performance of the better tokenisers varies with domain. As Dridan and Oepen (2012) found, REPP outperforms Stanford over Wall Street Journal text, but the performance difference over Brown is slight, and on EWTB, the advantage is to the Stanford tokeniser. We re-parsed using the best tokeniser and sentence segmenter for"
W13-5715,W01-0521,0,0.0359277,"Missing"
W13-5715,J06-4003,0,0.0789919,"(1986), and calculate the shortest edit script with consideration of the most commonly accepted mappings. This enables an efficient alignment that is even robust to occasional missing analyses (due to parse failure). Phrase structure is not the only annotation that can be represented as a labelled span. Many standard NLP tasks, including part-of-speech (PoS) tagging, tokenisation, and sentence segmentation can also be viewed this way. For PoS tagging, the parallel is obvious. Tokenisation and sentence segmentation have previously been evaluated in a variety of manners (Palmer & Hearst, 1997; Kiss & Strunk, 2006; Read et al., 2012), but are also amenable to being seen as span labelling, allowing us to use a consistent generalised evaluation framework. Thus, we represent each annotation as a triple consisting of span start and end positions, together with a label. This is particularly useful for our present purpose of evaluating the effect of these pre-processing tasks on phrase structure parsing. Figure 2 shows all such triples that can be extracted from our running example.2 3 Previous Work A character-based version of evalb, dubbed SParseval, was earlier used for parser evaluation over the output f"
W13-5715,P03-1054,0,0.019066,"pre-segmented tokens. In our more general case, we consider enumerating all possible tokenisations of a sentence (or document) neither practical or desirable, nor do we wish to enforce token breaks at whitespace, since different tokenisation conventions such as those described by Fares et al. (2013) allow mid-token spaces. 4 Experimental Setup To evaluate end-to-end parsing, we selected three commonly used phrase structure parsers: the Charniak and Johnson reranking parser (C&J; Charniak & Johnson, 2005), the Berkeley parser (Petrov, Barrett, Thibaux, & Klein, 2006), and the Stanford parser (Klein & Manning, 2003). For each parser, we used the recommended pre-trained English model, and mostly default settings.3 All parsers tokenise their in3 We used the -accurate setting for the Berkeley parser, to improve coverage on our out-of-domain data. Full configuration details and other background, including an open-source implementation of our evaluation framework will be distributed through a companion website. put by default and the Stanford parser also includes a sentence splitting component. Since both the C&J and Berkeley parsers require sentence-segmented text, we pre-segmented for those parsers using to"
W13-5715,J93-2004,0,0.0475591,"ain data. Full configuration details and other background, including an open-source implementation of our evaluation framework will be distributed through a companion website. put by default and the Stanford parser also includes a sentence splitting component. Since both the C&J and Berkeley parsers require sentence-segmented text, we pre-segmented for those parsers using tokenizer, the top-performing, lightweight, rule-based segmenter in the recent survey of Read et al. (2012). For parser evaluation, the standard test data is Section 23 of the venerable WSJ portion of the Penn Treebank (PTB, Marcus et al., 1993). In addition to this data, we also use the full Brown portion of the PTB (often used for experiments in domain variation) and the relatively new English Web Treebank (EWTB, LDC #2012T13), which comprises several web genres. To evaluate document parsing, we start from the raw, running text. In the case of the EWTB this was provided with the treebank; for the other corpora, we use the raw texts distributed by Read et al. (2012). Corpora sizes are included in Table 1. Our reference implementation for evaluation instantiates the triple-based framework outlined in § 2 above. For compatibility with"
W13-5715,P06-1043,0,0.0192805,"viable task, leading to parser accuracies only slightly below the state of the art over gold tokens for the (potentially overfitted) WSJ23 , given the right preprocessors. Furthermore, by actually evaluating the end-toend pipeline, as well as the performance, in-situ, of the various preprocessors we have discovered that parser accuracy can actually be improved by matching the tokenisation to the expectations of the parser, rather than merely using the tokenisation of the treebank. This subtle tuning of annotation style, rather than domain could even help explain previous cross-domain results (McClosky, Charniak, & Johnson, 2006) that showed better accuracy from not using labelled Brown data for parser training. 6 Conclusions and Outlook Through this work, we hope to call the syntactic analysis research community to arms and initiate a shift of emphasis towards practical use cases and document parsing—with the aim of mitigating the risks of overestimating parser performance and over-tuning to individual treebank idiosyncrasies. Our triple-based proposal for Robust Evaluation of Syntactic Analysis synthesises earlier work and existing metrics into a uniform framework that (a) encompasses the complete analysis pipeline"
W13-5715,J97-2002,0,0.123049,"ence algorithm of Myers (1986), and calculate the shortest edit script with consideration of the most commonly accepted mappings. This enables an efficient alignment that is even robust to occasional missing analyses (due to parse failure). Phrase structure is not the only annotation that can be represented as a labelled span. Many standard NLP tasks, including part-of-speech (PoS) tagging, tokenisation, and sentence segmentation can also be viewed this way. For PoS tagging, the parallel is obvious. Tokenisation and sentence segmentation have previously been evaluated in a variety of manners (Palmer & Hearst, 1997; Kiss & Strunk, 2006; Read et al., 2012), but are also amenable to being seen as span labelling, allowing us to use a consistent generalised evaluation framework. Thus, we represent each annotation as a triple consisting of span start and end positions, together with a label. This is particularly useful for our present purpose of evaluating the effect of these pre-processing tasks on phrase structure parsing. Figure 2 shows all such triples that can be extracted from our running example.2 3 Previous Work A character-based version of evalb, dubbed SParseval, was earlier used for parser evaluat"
W13-5715,P06-1055,0,0.0158636,"uation of allowing ambiguous morphological segmentation only within pre-segmented tokens. In our more general case, we consider enumerating all possible tokenisations of a sentence (or document) neither practical or desirable, nor do we wish to enforce token breaks at whitespace, since different tokenisation conventions such as those described by Fares et al. (2013) allow mid-token spaces. 4 Experimental Setup To evaluate end-to-end parsing, we selected three commonly used phrase structure parsers: the Charniak and Johnson reranking parser (C&J; Charniak & Johnson, 2005), the Berkeley parser (Petrov, Barrett, Thibaux, & Klein, 2006), and the Stanford parser (Klein & Manning, 2003). For each parser, we used the recommended pre-trained English model, and mostly default settings.3 All parsers tokenise their in3 We used the -accurate setting for the Berkeley parser, to improve coverage on our out-of-domain data. Full configuration details and other background, including an open-source implementation of our evaluation framework will be distributed through a companion website. put by default and the Stanford parser also includes a sentence splitting component. Since both the C&J and Berkeley parsers require sentence-segmented"
W13-5715,C12-2096,1,0.901405,"e the shortest edit script with consideration of the most commonly accepted mappings. This enables an efficient alignment that is even robust to occasional missing analyses (due to parse failure). Phrase structure is not the only annotation that can be represented as a labelled span. Many standard NLP tasks, including part-of-speech (PoS) tagging, tokenisation, and sentence segmentation can also be viewed this way. For PoS tagging, the parallel is obvious. Tokenisation and sentence segmentation have previously been evaluated in a variety of manners (Palmer & Hearst, 1997; Kiss & Strunk, 2006; Read et al., 2012), but are also amenable to being seen as span labelling, allowing us to use a consistent generalised evaluation framework. Thus, we represent each annotation as a triple consisting of span start and end positions, together with a label. This is particularly useful for our present purpose of evaluating the effect of these pre-processing tasks on phrase structure parsing. Figure 2 shows all such triples that can be extracted from our running example.2 3 Previous Work A character-based version of evalb, dubbed SParseval, was earlier used for parser evaluation over the output from speech recognise"
W13-5715,roark-etal-2006-sparseval,0,0.0147523,"ion framework. Thus, we represent each annotation as a triple consisting of span start and end positions, together with a label. This is particularly useful for our present purpose of evaluating the effect of these pre-processing tasks on phrase structure parsing. Figure 2 shows all such triples that can be extracted from our running example.2 3 Previous Work A character-based version of evalb, dubbed SParseval, was earlier used for parser evaluation over the output from speech recognisers, which cannot be expected (or easily forced) to adhere to gold standard sentence and token segmentation (Roark et al., 2006). As word recognition errors can lead to very different parse yields, this approach required a separate, external word alignment step, far more complicated than our in-built alignment stage. Parsing morphologically rich languages is another area where expecting gold standard parse yields is unrealistic. In this case, parse yields are often morpheme sequences, ambiguously derived from the input strings. Tsarfaty et al. (2012) propose an eval2 Note that, at this point, we include several elements of the gold-standard annotation of Figure 1 that are commonly suppressed in parser evaluation, notab"
W13-5715,P12-2002,0,0.0634283,"er used for parser evaluation over the output from speech recognisers, which cannot be expected (or easily forced) to adhere to gold standard sentence and token segmentation (Roark et al., 2006). As word recognition errors can lead to very different parse yields, this approach required a separate, external word alignment step, far more complicated than our in-built alignment stage. Parsing morphologically rich languages is another area where expecting gold standard parse yields is unrealistic. In this case, parse yields are often morpheme sequences, ambiguously derived from the input strings. Tsarfaty et al. (2012) propose an eval2 Note that, at this point, we include several elements of the gold-standard annotation of Figure 1 that are commonly suppressed in parser evaluation, notably the root category TOP, the empty node at position h11, 11i, and the final punctuation mark. In § 4 below, we return to some of these and reflect on the common practice of ignoring parts of the gold standard, against our goal of ‘realistic’ parser evaluation. 128 1: h 0, 4, POS : PRP i 2: h 0, 4, TOK i 3: h 5, 8, POS : VBD i 4: h 5, 8, TOK i 5: h 8, 11, POS : RB i 6: h 8, 11, TOK i 7: h 11, 11, POS :- NONE - i 8: h 11, 11,"
W13-5715,P09-1043,0,0.0333551,"Missing"
W15-0126,S14-2081,1,0.886398,"ns and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at the semantic dependency graphs"
W15-0126,C10-1011,0,0.057073,"g and untrimming, since conservative trimming does not increase the label sets. 5 Graph Parsing We proceed to evaluate our tree approximations in graph parsing. Here, our previously outlined parsing pipeline is applied: training graphs are converted to trees using different pre-processing approximations, parsers are trained and applied on test data, outputs are converted to graphs and evaluated against the gold standard graphs. We observe the labeled F1 scores (LF) and exact matches (LM). Experiment Setup For dependency tree parsing, we use the mate-tools state-of-the-art graphbased parser of Bohnet (2010). As in the shared task, we experiment in two tracks: the open track, and the closed track. In the closed track, for training the parser, we use only the features available in the SDP training data, i.e., word forms, parts of speech and lemmas. In the open track, we also pack additional features from the SDP companion dataset – automatic dependency and phrase-based parses of the SDP training and testing sets – as well as the Brown clustering features (Brown et al., 1992). For top node detection, we use a sequence tagger based on conditional random fields (CRFs). To guess the top nodes in the c"
W15-0126,J92-4003,0,0.089862,"and exact matches (LM). Experiment Setup For dependency tree parsing, we use the mate-tools state-of-the-art graphbased parser of Bohnet (2010). As in the shared task, we experiment in two tracks: the open track, and the closed track. In the closed track, for training the parser, we use only the features available in the SDP training data, i.e., word forms, parts of speech and lemmas. In the open track, we also pack additional features from the SDP companion dataset – automatic dependency and phrase-based parses of the SDP training and testing sets – as well as the Brown clustering features (Brown et al., 1992). For top node detection, we use a sequence tagger based on conditional random fields (CRFs). To guess the top nodes in the closed track, we use words and POS tags as features, while we add the companion syntactic features in the open track. 5.1 Results The evaluation results are listed in Table 3. The overall performance of our basic DFS tree approximation parser is identical to the one of Agi´c and Koller (2014) in the closed track. In the open track, however, we improve by 2-3 points in LF due to better top node detection, and improved tree parser accuracy due to the introduction of additio"
W15-0126,S14-2080,0,0.0199245,"ork, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree app"
W15-0126,C96-1058,0,0.12084,"a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to wi"
W15-0126,W12-3602,1,0.861041,"aphs from SDP 2014. The three SDP annotation layers over WSJ text stem from different semantic representations, but all result in directed acyclic graphs (DAGs) for describing sentence semantics. The three representations can be characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while DM HPSG annotations were manual, the annotations for training Enju were automatically constructed from the Penn Treebank bracketings by Miyao et al. (2004). 3. PCEDT originates from the English part of the Prague Czech–English Dependency Treebank. In this project, the WSJ part of PTB was translated into Czech, and both sides w"
W15-0126,S14-2068,0,0.0196154,"n the SDP evaluation framework. This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of t"
W15-0126,J93-2004,0,0.0542364,"Missing"
W15-0126,P13-2109,0,0.0284804,"y. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored"
W15-0126,S14-2082,0,0.0349885,"between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also"
W15-0126,S14-2056,1,0.866199,"he single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at the semantic dependency graphs from SDP 2014. The three SDP annotation layers over WSJ text stem from different semantic representations, but all result in directed acyclic graphs (DAGs) for describing sentence semantics. The three representations can be characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while D"
W15-0126,J08-1002,0,0.0147916,"characterized as follows (Oepen et al., 2014; Miyao et al., 2014). 1. DM semantic dependencies stem from the gold-standard HPSG annotations of the WSJ text, as provided by the LinGO English Resource Grammar (ERG; Flickinger, 2000; Flickinger et al., 2012). The resource was converted to bi-lexical dependencies in preparation for the task by Oepen and Lønning (2006) and Ivanova et al. (2012) by a two-step lossy conversion. 2. PAS bi-lexical dependencies are also derived from HPSG annotations of PTB, which were originally aimed at providing a training set for the wide-coverage HPSG parser Enju (Miyao and Tsujii, 2008). As noted in the task description, while DM HPSG annotations were manual, the annotations for training Enju were automatically constructed from the Penn Treebank bracketings by Miyao et al. (2004). 3. PCEDT originates from the English part of the Prague Czech–English Dependency Treebank. In this project, the WSJ part of PTB was translated into Czech, and both sides were manually in accordance with the Prague-style rules for tectogrammatical analysis (Cinkov´a et al., 2009). The dataset is postprocessed by the task organizers to match the requirements for bi-lexical dependencies. Nodes in SDP"
W15-0126,P05-1013,0,0.0536753,"wo types of edge removal, which we name deletion and trimming. In deletion, it is not possible to reconstruct the removed edge in post-processing, i.e. the removed edge is permanently lost. In trimming, by contrast, the removed edge can be reconstructed – or untrimmed – in post-processing, either deterministically or with a certain success rate. In the shared task, a number of systems approached trimming through label overloading. In label overloading, a deletion of one edge is recorded in another kept edge, similar to encoding non-projective dependency arcs in pseudo-projective tree parsing (Nivre and Nilsson, 2005). In post-processing, the information stored in overloaded labels is used to attempt edge untrimming. We proceed to explore several ways of performing tree approximations, which include a mixture of edge removals via deletion and trimming. 3.1 Baselines Three baselines are used in this research. We re-implement the official SDP shared task baseline, and the local edge flipping and depth-first flipping systems of Agi´c and Koller (2014). OFFICIAL : The official baseline tree approximation only performs deletions and artificial edge insertions to satisfy the dependency tree constraints. No trimm"
W15-0126,S14-2008,1,0.883555,"evaluation framework. This system implements a tree approximation that strikes a good and linguistically plausible empirical balance between loss minimization and parsing accuracy. Outline We provide a detailed account of the properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of t"
W15-0126,S14-2012,0,0.0192021,"properties of SDP graphs (§2), introduce approaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy,"
W15-0126,C08-1095,0,0.0793274,"roaches to tree approximations (§3) and evaluate them (§4). We use this linguistic insight to produce a linguistically motivated tree approximation–based parsing framework, which we evaluate as the top-performing nonvoting parser based on tree approximations on the SDP data (§5). Related work In the SDP 2014 campaign, Kuhlmann (2014) adapted the tree parsing algorithm of Eisner (1996), while Thomson et al. (2014) implement a novel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outp"
W15-0126,S14-2034,0,0.235629,"ovel approach to graph parsing. Martins and Almeida (2014) adapt TurboParser (Martins et al., 2013) for graph processing, and Ribeyre et al. (2014) utilize the parser of Sagae and Tsujii (2008). Graph parsing by tree approximations and post-processing was most notably performed by the topperforming system of the competition, the one by Du et al. (2014). Their tree approximations are obtained by depth-first and breadth-first graph traversals, possibly changing the edge directions in the direction of the traversal. However, this was not sufficient to win the competition, since two other teams – Schluter et al. (2014) and Agi´c and Koller (2014) – also implemented a similar approach and scored in the mid range. For overall premium accuracy, Du et al. (2014) applied voting over the outputs of 17 different tree approximation–based systems, which arguably makes for a computationally inefficient resulting system. The single top-performing tree approximation system was the one by Schluter et al. (2014), which is closely followed by Agi´c and Koller (2014). The latter one are the only to provide some linguistic insight into the SDP graphs. 2 Semantic Dependency Graphs In this section, we take a closer look at th"
W15-0126,S14-2027,0,0.217765,"Missing"
W15-0126,oepen-lonning-2006-discriminant,1,\N,Missing
W15-0128,adolphs-etal-2008-fine,1,0.866719,"Missing"
W15-0128,P98-1013,0,0.191125,"tic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot, in itself, include information t"
W15-0128,W13-2322,0,0.651957,"emantics either for parsing or for generation. 4 Benefits of Compositionality We are concerned here with the goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with bett"
W15-0128,basile-etal-2012-developing,0,0.204582,"ture, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot, in itself, include information that is not compositional, but as we will develop further in §6 below, it is possible to have the best of both worlds, adding non-compositional information as additional annotation layers over grammar-produced semantic representations. 4.1 Comprehensiv"
W15-0128,P11-1059,0,0.0201877,"ication, rather than a strong claim about lexical meaning. Another kind of non-compositional meaning layer is that which requires some sort of further computation over linguistic structure. This can be seen as purely monotonic addition of further constraints on underspecified meaning representations, but it is not compositional in the sense that it is never (strictly) constrained by grammatical structure. In this category, we find quantifier scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of"
W15-0128,W97-1502,0,0.546635,"ee and ERS for each of these sentences are made available as the Redwoods Treebank; at the end of 2014, the current version of Redwoods encompasses gold-standard ERG analyses for 85,000 utterances (∼1.5 million tokens) of running text from half a dozen different genres and domains. In more detail, the task of annotation for a sentence consists of making binary decisions about the set of discriminants each of which partitions the parse forest into two: all of the analyses which employ the particular rule or lexical entry, and the rest of the analyses which do not. This method, originating with Carter 1997, enables the human annotator to rapidly discard analyses in order to isolate the intended analysis, or to conclude that the correct analysis is unavailable. As a reference point for speed of annotation using this method, an expert treebanker using the current ‘1214’ version of the ERG annotated 2400 sentences (37,200 words) from the Brown corpus in 1400 minutes, for an average rate of 1.7 sentences per minute.3 Annotations produced by this method of choosing among the candidate analyses licensed by a grammar will thus record those components of sentence meaning which are constrained by the gr"
W15-0128,P01-1019,1,0.580225,"nnotation. As Szabó (2013) points out, there are many different interpretations of the principle of compositionality in the literature. Since we are concerned with annotation, the issue is compositionality of meaning representations (rather than denotation, for instance). In order to ask which aspects of meaning are compositional, we provide the following working definition:1 1 In Szabó’s terms, our definition of compositionality is local, distributive, and language-bound and furthermore consistent with the rule-to-rule principle. It is also consistent with the notion of compositionality from Copestake et al. (2001) and implemented in the ERG, which furthermore adds the constraint that the function for determining meanings of complex expressions must be monotonic in the sense that it cannot remove or overwrite any information contributed by the constituents. 240 (1) A meaning system (or subsystem) is compositional if: • it consists of a finite (but possibly very large) number of arbitrary atomic symbol-meaning pairings; • it is possible to create larger symbol-meaning pairings by combining the atomic pairings through a finite set of rules; • the meaning of any non-atomic symbol-meaning pairing is a funct"
W15-0128,W11-2927,1,0.716728,"Missing"
W15-0128,flickinger-etal-2010-wikiwoods,1,0.898233,"Missing"
W15-0128,P82-1014,0,0.754755,"fits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 1 Introduction Kate and Wong (2010) define ‘semantic parsing’ as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (Woods et al., 1972; Gawron et al., 1982; Alshawi, 1992, inter alios). What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-specific meaning representations (e.g. the RoboCup or GeoQuery formal language) and (b) a lack of emphasis on natural language syntax, i.e. a tacit expectation to map (more or less) directly from a linguistic surface form to an abstract representation of its meaning. This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics (Quine, 1960;"
W15-0128,J03-1004,0,0.0864457,"Missing"
W15-0128,W07-1501,0,0.0215267,"entions. Some of the information we would like to see in such annotations is grammatically constrained, and we have argued that representations of those aspects of meaning are best built compositionally. However, there are further aspects of meaning which are closely tied to the linguistic signal but are not constrained by sentencelevel grammar (or only partially so constrained). We agree here with Basile et al. (2012) and Banarescu et al. (2013) that a single resource that combines multiple different types of semantic annotations, all applied to the same text, will be most valuable (see also Ide and Suderman 2007). However, just because some aspects of the desired representations cannot be created in a grammar-based fashion does not mean that what can be done with a grammar has no value. To get the best of both worlds, one should start from grammar-derived semantic annotations and then either add further layers of annotation (e.g. word sense, coreference) or, should larger paraphrase sets be desired, systematically simplify aspects of the grammar-derived representations, effectively ‘bleaching’ some of the contrasts. In moving from the current state of the art towards more comprehensive representations"
W15-0128,P10-5006,0,0.0583505,"al itself. We further argue that compositional construction of such sentence meaning representations affords better consistency, more comprehensiveness, greater scalability, and less duplication of effort for each new NLP application. For concreteness, we describe one well-tested grammar-based method for producing sentence meaning representations which is efficient for annotators, and which exhibits many of the above benefits. We then report on a small inter-annotator agreement study to quantify the consistency of semantic representations produced via this grammar-based method. 1 Introduction Kate and Wong (2010) define ‘semantic parsing’ as “the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.” At this level of generality, semantic parsing has been a cornerstone of NLU from its early days, including work seeking to support dialogue systems, database interfaces, or machine translation (Woods et al., 1972; Gawron et al., 1982; Alshawi, 1992, inter alios). What distinguishes most current work in semantic parsing from such earlier landmarks of old-school NLU is (a) the use of (highly) taskand domain-"
W15-0128,kingsbury-palmer-2002-treebank,0,0.223431,"e goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§4.1), produced with better consistency (§4.2) and greater scalability (§4.3). The drawback to a grammar-based approach is that it cannot"
W15-0128,P98-1116,0,0.0854027,"equire the computation of semantics either for parsing or for generation. 4 Benefits of Compositionality We are concerned here with the goal of designing task-independent semantic representations and deploying them at scale to create a large sembank including diverse genres. Such representations can be created compositionally, where the content and internal structure of the representations is constrained by syntactic structure, or non-compositionally, where annotators encode their understanding of a sentence directly. The latter category is exemplified by Abstract Meaning Representation (AMR; Langkilde and Knight 1998; Banarescu et al. 2013). In the former category, we find both manual annotation projects, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998), which annotate semantic information with reference to syntactic structure, and grammar-based annotation initiatives such as the Redwoods Treebank (Oepen et al., 2004), TREPIL (Rosén et al., 2005), and the Groningen Meaning Bank (Basile et al., 2012). We argue here that a grammar-based, compositional approach is critical to achieving this long-range goal, in particular because it supports more comprehensive representations (§"
W15-0128,I11-1028,1,0.818925,"to abstract away from task-irrelevant details of linguistic expression, task-independent representations only have that luxury when the variation is truly (sentence) meaning preserving. A task-independent semantic representation should capture exactly the meaning encoded in the linguistic signal itself, as it is is not possible to know, a priori, which parts of that sentence meaning will be critical to determining speaker meaning in any given application. 3 This rate is roughly consistent with an earlier experiment using the same Redwoods treebanking method where annotation times were noted: MacKinlay et al. (2011) report a somewhat slower mean annotation time by an expert annotator of 0.6 sentences per minute, but this difference can be attributed to the greater average sentence length (and hence increased number of discriminants to be determined) for that biomedical corpus: 23.4 tokens compared with 15.5 for the Brown data. 243 h h1 , h h1 , h4 :personh0 : 6i(ARG0 x 5 ), h6 :_no_qh0 : 6i(ARG0 x 5 , RSTR h7 , BODY h8 ), h2 :_eat_v_1h7 : 11i(ARG0 e3 , ARG1 x 5 , ARG2 i 9 ) { h1 =q h2 , h7 =q h4 } i h4 :_every_qh0 : 5i(ARG0 x 6 , RSTR h7 , BODY h5 ), h8 :_person_n_1h6 : 12i(ARG0 x 6 ), h2 :_fail_v_1h13 :"
W15-0128,P97-1013,0,0.0300129,"resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally built-up meanings for morphologically complex words. Semi-prod"
W15-0128,J94-1007,0,0.713929,"act representation of its meaning. This approach risks conflating a distinction that has long played an important role in the philosophy of language and theoretical linguistics (Quine, 1960; Grice, 1968), viz. the contrast between those aspects of meaning that are determined by the linguistic signal alone (called ‘timeless’, ‘conventional’, ‘standing’, or ‘sentence’ meaning), on the one hand, and aspects of meaning that are particular to a context of use (‘utterer’, ‘speaker’, or ‘occasion’ meaning, or ‘interpretation’), on the other hand. Relating this tradition to computational linguistics, Nerbonne (1994, p. 134) notes: Linguistic semantics does not furnish a characterization of the interpretation of utterances in use, which is what one finally needs for natural language understanding applications—rather, it (mostly) provides a characterization of conventional content, that part of meaning determined by linguistic form. Interpretation is not determined by 239 Proceedings of the 11th International Conference on Computational Semantics, pages 239–249, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics form, however, nor by its derivative content. In order to interpre"
W15-0128,oepen-lonning-2006-discriminant,1,0.897769,"Missing"
W15-0128,W04-2319,0,0.0143789,"negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally built-up meanings for morphologically complex words. Semi-productive morphological processes and frozen or lexicalized complex forms complica"
W15-0128,W13-0122,0,0.0367255,"scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology supports compositionally bu"
W15-0128,W08-0606,0,0.0207628,"As this level of processing concerns relationships both within and across sentences, it is clearly not compositional with respect to sentence grammar. We consider it an open question whether there are compositional processes at higher levels of structure that constrain these aspects of meaning in analogous ways, but we note that in presupposition processing at least, a notion of defeasibility is required (Asher and Lascarides, 2011). Finally, there are semantic annotations that attempt to capture what speakers are trying to do with their speech acts. This includes tasks like hedge detection (Vincze et al., 2008) and the annotation of social acts such as authority claims and alignment moves (Morgan et al., 2013) or the pursuit of power in dialogue (Swayamdipta and Rambow, 2012). While in some cases there are keywords that have a strong association with particular categories in these annotation schemes, these aspects of meaning are clearly not anchored in the structure of sentences but rather relate to the goals that speakers have in uttering sentences. Lacking a firm link to the structure of sentences, they do not appear to be compositional. We have seen in this (necessarily brief) section that existi"
W15-0128,W13-0505,0,0.01847,"tegory, we find quantifier scope ambiguity resolution (e.g. Higgins and Sadock 2003), coreference resolution (e.g. Hobbs 1979), and the determination of the focus of negation (e.g. Blanco and Moldovan 2011). All of these build on partial constraints provided by the grammar, but in all cases, the interpretation of particular sentences in context will correspond to one (or a subset) of the possibilities allowed by the grammar. The next layer of meaning annotation to consider corresponds to discourse processing. This includes the calculation of presupposition projection (e.g. Van der Sandt 1992; Zaenen and Karttunen 2013; Venhuizen et al. 2013), coherence relations/rhetorical structure (e.g. Marcu 1997), and the annotation of discourse moves/adjacency pairs (Shriberg et al., 2004). These aspects of meaning clearly build on information provided during sentence-level processing, including lexically determined veridicality contexts (e.g. (2a) vs. (2b)) as well as discourse connectives. In both cases, the grammatical structure links embedded clauses to the relevant lexical predicates. 2 We assume here that word sense is a property of roots, rather than fully inflected forms. Productive derivational morphology sup"
W15-0128,C98-1112,0,\N,Missing
W15-0128,C98-1013,0,\N,Missing
W17-0237,P14-1023,0,0.0167,"vide indicative empirical results for a first set of embeddings made available in the repository (Section 3). Using an interactive web application, users are also able to explore and compare different pretrained models on-line (Section 4). 2 Motivation and background Over the last few years, the field of NLP at large has seen a huge revival of interest for distributional semantic representations in the form of word vectors 1 Our repository in several respects complements and updates the collection of Wikipedia-derived corpora and pretrained word embeddings published by Al-Rfou et al. (2013). (Baroni et al., 2014). In particular, the use of dense vectors embedded in low-dimensional spaces, socalled word embeddings, have proved popular. As recent studies have shown that beyond their traditional use for encoding word-to-word semantic similarity, word vectors can also encode other relational or ‘analogical’ similarities that can be retrieved by simple vector arithmetic, these models have found many new use cases. More importantly, however, the interest in word embeddings coincides in several ways with the revived interest in neural network architectures: Word embeddings are now standardly used for providi"
W17-0237,Q17-1010,0,0.0313544,"Missing"
W17-0237,P10-2043,0,0.034064,"Missing"
W17-0237,P15-1033,0,0.00750661,"Missing"
W17-0237,N16-2002,0,0.0261349,"Missing"
W17-0237,C16-1262,0,0.0223234,"Missing"
W17-0237,J15-4004,0,0.0532269,"Missing"
W17-0237,D14-1181,0,0.0154571,"Missing"
W17-0237,E17-3025,1,0.895986,"Missing"
W17-0237,N16-1030,0,0.00679306,"Missing"
W17-0237,W14-1618,0,0.0607022,"Missing"
W17-0237,Q15-1016,0,0.0582115,"Missing"
W17-0237,P14-5010,0,0.00549961,"Missing"
W17-0237,D14-1162,0,0.113148,"Missing"
W17-0237,P16-2067,0,0.0327556,"Missing"
W17-0237,P05-1077,0,0.0183182,"Missing"
W17-0237,D13-1170,0,0.0109549,"Missing"
W17-0237,W11-4631,1,0.893413,"Missing"
W17-0808,W14-5204,1,0.928633,"er Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed and implemented framework-specific representations. These designs a"
W17-0808,lapponi-etal-2014-road,1,0.860158,"Marc Verhagen♦ ♣ Technische Universität Darmstadt, Department of Computer Science ♠ Vassar College, Department of Computer Science ♥ University of Oslo, Department of Informatics ♦ Brandeis University, Linguistics and Computational Linguistics Abstract representation—a uniform framework-internal convention—with mappings from tool-specific input and output formats. Specifically, we will take an in-depth look at how the results of morphosyntactic analysis are represented in (a) the DKPro Core component collection1 (Eckart de Castilho and Gurevych, 2014), (b) the Language Analysis Portal2 (LAP; Lapponi et al. (2014)), and (c) the Language Application (LAPPS) Grid3 (Ide et al., 2014a). These three systems all share the common goal of facilitating the creation of complex NLP workflows, allowing users to combine tools that would otherwise need input and output format conversion in order to be made compatible. While the programmatic interface of DKPro Core targets more technically inclined users, LAP and LAPPS are realized as web applications with a point-andclick graphical interface. All three have been under active development for the past several years and have—in contemporaneous, parallel work— designed"
W17-0808,P12-2074,1,0.837847,"pendencyStructure which can bind multiple dependency relations together and thus supports multiple parallel dependency structures even within a single LIF view. Media–Tokenization Mismatches Tokenizers may apply transformations to the original input text that introduce character offset mismatches with the normalized output. For example, some Penn Treebank–compliant tokenizers normalize different conventions for quotation marks (which may be rendered as straight ‘typewriter’ quotes or in multicharacter LATEX-style encodings, e.g. &quot; or ``) into opening (left) and closing (right) Unicode glyphs (Dridan and Oepen, 2012). To make such normalization accessible to downstream processing, it is insufficient to represent tokens as only a region (sub-string) of the underlying linguistic signal. In LXF, the string output of tokenizers is recorded in the annotations encapsulated with each token node, which is in turn linked to a region recording its character offsets in the original media. LIF (which is largely inspired by ISO LAF, much like LXF) also records the token string and its character offsets in the original medium. LIF supports this via the word property on tokens. DKPro Core has also recently started intro"
W17-0808,W14-5201,1,0.897235,"Missing"
W17-0808,heid-etal-2010-corpus,0,0.0249296,"Lacking interface standardization, thus, severely limits interoperability. The frameworks surveyed in this work address interoperability by means of a common 2 Terminological Definitions A number of closely interrelated concepts apply to the discussion of design choices in the repre1 https://dkpro.github.io/dkpro-core https://lap.clarino.uio.no 3 https://www.lappsgrid.org 4 There are, of course, additional designs and workflow frameworks that we would ultimately hope to include in this comparison, as for example the representations used by CONCRETE, WebLicht, and FoLiA (Ferraro et al., 2014; Heid et al., 2010; van Gompel and Reynaert, 2013), to name just a few. However, some of these frameworks are at least abstractly very similar to representatives in our current sample, and also for reasons of space we need to restrict this in-depth comparison to a relatively small selection. 2 67 Proceedings of the 11th Linguistic Annotation Workshop, pages 67–75, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics root nsubj det aux nn DT The the NNP Olympic Olympic neg NNP Committee Committee VBZ does do RB n’t not xcomp VB regret regret VBG choosing choose ORGANIZATION NNP China"
W17-0808,ide-etal-2014-language,1,0.90873,"Missing"
