2016.gwc-1.22,W14-0130,1,0.822236,"kta as a separate character before search. 3.3.2 Morphological Analysis Before searching in the databases the word is first passed to a morphological analyzer to obtain its root form. We use Hindi Morph Analyzer (Bahuguna et al., 2014) to return the root form of the input word for Hindi language, since by principle, WordNet only contains root forms of the words. Due to non availability of other language Morphological Analyzers, we may not be able to include them in the search process. Though, in the future, we can use a fully automated version of the “Human mediated TRIE base generic stemmer”(Bhattacharyya et al., 2014) for obtaining root forms for other languages later. 3.3.3 Handling Multiple Root forms Figure 2: Devanagari Keyboard tion using Google Transliteration API5 , and a JavaScript based online keyboard (Figure: 2) for input of Hindi Unicode characters. Transliteration for a native user is very convenient. In case, the user does not know the right combination of keys then the keyboard for Devanagari is provided. These two methods ensure that all words can be easily entered for searching. Thereafter, by touching / clicking on “Search”, the synsets with all relevant information are retrieved. 3.3 Sea"
2020.acl-srw.37,D11-1033,0,0.301577,"a selection, pre-training and fine-tuning. 3.1 Data Pre-processing Blindly pre-training a NMT model on vast amounts of monolingual data belonging to the assisting languages and LOI might improve translation quality slightly. However, divergences between the languages, especially their scripts (Hermjakob et al., 2018) and also the distributions of data between different training phases is known to impact the final result. Motivated by past works on using related languages (Dabre et al., 2017), orthography mapping/unification (Hermjakob et al., 2018; Chu et al., 2012) and data selection for MT (Axelrod et al., 2011), we propose to improve the efficacy of pre-training by reducing data and language divergence. 3.1.1 Script Mapping Previous research has shown that enforcing shared orthography (Sennrich et al., 2016b; Dabre et al., 2015) has a strong positive impact on translation. Following this, we propose to leverage existing script mapping rules1 or script unification mechanisms to, at the very least, maximize the possibility of cognate sharing and thereby bringing the assisting language closer to the LOI. This should strongly impact languages such as Hindi, Punjabi and Bengali belonging to the same fami"
2020.acl-srw.37,chu-etal-2012-chinese,1,0.614354,"ASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a v"
2020.acl-srw.37,Y15-1033,1,0.921893,"languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses. 2 Assisting languages Proposed Metho"
2020.acl-srw.37,Y17-1038,1,0.933618,"velopment of methods like BERT (Devlin et al., 2018). Song et al. (2019) recently proposed MASS, a new state-of-the-art NMT pre-training task that jointly trains the encoder and the decoder. Our approach builds on the initial idea of MASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target langu"
2020.acl-srw.37,W11-2123,0,0.0396903,"g a denoised auto-encoder model. The NMT model is pre-trained with the MASS task, until convergence, jointly for both the source and target languages. Thereafter training is resumed on the parallel corpus, a step known as fine-tuning (Zoph et al., 2016). 4 Experimental Settings We conducted experiments on Japanese–English (Ja–En) translation in a variety of simulated lowresource settings using the “similar” assisting lan281 guage pairs Chinese (Zh) and French (Fr) and the “distant” assisting language pairs Russian (Ru) and Arabic (Ar). 4.1 as linguistically). 3. Data selection: We used KenLM (Heafield, 2011) to train 5-gram LMs on in-domain data for LM scoring based data selection and use ASPEC dev set for length distribution based data selection. Datasets We used the official ASPEC Ja–En parallel corpus (Nakazawa et al., 2016) provided by WAT 20192 . The official split consists of 3M, 1790 and 1872 train, dev and test sentences respectively. We sampled parallel corpora from the top 1M sentences for fine-tuning. Out of the remaining 2M sentences, we used the En side of the first 1M and the Ja side of the next 1M sentences as monolingual data for language modeling for data selection. We used Commo"
2020.acl-srw.37,P18-4003,0,0.115289,"er (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Target languages Data Selection Mapping Mixed data Pre-train Pre-trained model Parallel data Target languages Fine-tune NMT model Figure 1: An overview of our proposed method consisting of script mapping, data selection, pre-training and fine-tuning 2. Empirical evaluation: We make a comparison of existing and proposed techniques in a variety of corpora settings to verify our hypotheses. 2 Assisting languages Proposed Method: Using Assisting Languages We propose a novel monolingual pre-training meth"
2020.acl-srw.37,D18-2012,0,0.0403888,"t languages (script-wise as well 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/index. html#task.html 3 http://data.statmt.org/ngrams/ 4 http://data.statmt.org/news-commentary/v14/ 5 https://github.com/fxsjy/jieba 6 https://www.nltk.org 5 Results and Analysis 5.1 Training and Evaluation Settings We used the tensor2tensor framework (Vaswani et al., 2018) 7 , version 1.14.0., with its default “transformer big” setting. We created a shared sub-word vocabulary using Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl. We used SentencePiece (Kudo and Richardson, 2018) and obtained a vocabulary with the size of roughly 64k . We used this vocabulary in all experiments except unrelated language experiment where Arabic and Russian were used instead of Chinese and French data. We combined monolingual data of assisting languages and languages of interest (LOI; Japanese and English) for pre-training. When mixing datasets of different sizes, we always oversampled the smaller datasets to match the size of the largest. For all pre-training models, we saved checkpoints every 1000 steps and for all fine-tuning models, we saved checkpoints every 200 steps. We used earl"
2020.acl-srw.37,N18-2084,0,0.0301046,"ever, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S2S specific pre-training task for NMT and obtained new state-of-the-art results in low-resource settings. MASS assumes that a large amount of monolingual data is available for the languages involved but some language pairs may lack both parallel and monolingual corpora and are “truly low-resource” and challenging. Fortu"
2020.acl-srw.37,P16-1009,0,0.233853,"corpora, we were able to improve Japanese–English translation quality by up to 8.5 BLEU in lowresource scenarios. 1 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is known to give state-of-the-art (SOTA) translations for language pairs with an abundance of parallel corpora. However, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S"
2020.acl-srw.37,P16-1162,0,0.411738,"corpora, we were able to improve Japanese–English translation quality by up to 8.5 BLEU in lowresource scenarios. 1 Introduction Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is known to give state-of-the-art (SOTA) translations for language pairs with an abundance of parallel corpora. However, most language pairs are resource poor (Russian–Japanese, Marathi–English) as they lack large parallel corpora and the lack of bilingual training data can be compensated by by monolingual corpora. Although it is possible to utilise the popular back-translation method (Sennrich et al., 2016a), it is time-consuming to backtranslate a large amount of monolingual data. Furthermore, poor quality backtranslated data tends to be of little help. Recently, another approach has gained popularity where the NMT model is pre-trained through tasks that only require monolingual data (Song et al., 2019; Qi et al., 2018). Pre-training using models like BERT (Devlin et al., 2018) have led to new state-of-the-art results in text understanding. However, BERT-like sequence models were not designed to be used for NMT which is sequence to sequence (S2S). Song et al. (2019) recently proposed MASS, a S"
2020.acl-srw.37,D18-2010,1,0.834981,"selection. We used Common Crawl3 monolingual corpora for pre-training. To train LMs for data-selection of the assisting languages corpora, we used news commentary datasets 4 . While this data selection step for the assisting languages won’t minimize the domain difference from the parallel corpus, it can help in filtering noisy sentences. In this paper we consider the ASPEC and news commentary data as in-domain and the rest of the pre-training data as out-of-domain. 4.2 Data Pre-processing 1. Normalization and Initial Filtering: We applied NFKC normalization to data of all languages. Juman++ (Tolmachev et al., 2018) for Ja tokenization, jieba5 for Zh tokenization and NLTK6 tokenization for other languages. We filtered out all sentences from the pre-training data that contain fewer than 3 and equal or more than 80 tokens. For Chinese data, we filtered out sentences containing fewer than 30 percent Chinese words or more than 30 percent English words. 2. Script Mapping: Chinese is the only assisting language that can be mapped to Japanese reliably. We converted Chinese to Japanese script to make them more similar by using the mapping table from (Chu et al., 2012) and the mapping approaches mentioned in the"
2020.acl-srw.37,W18-1819,0,0.0277583,"om (Chu et al., 2012) and the mapping approaches mentioned in the previous section. French and English are written using the Roman alphabet and do not need any script mapping. We did not perform script mapping for Arabic and Russian to show the impact of using distant languages (script-wise as well 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/index. html#task.html 3 http://data.statmt.org/ngrams/ 4 http://data.statmt.org/news-commentary/v14/ 5 https://github.com/fxsjy/jieba 6 https://www.nltk.org 5 Results and Analysis 5.1 Training and Evaluation Settings We used the tensor2tensor framework (Vaswani et al., 2018) 7 , version 1.14.0., with its default “transformer big” setting. We created a shared sub-word vocabulary using Japanese and English data from ASPEC mixing with Japanese, English, Chinese and French data from Common Crawl. We used SentencePiece (Kudo and Richardson, 2018) and obtained a vocabulary with the size of roughly 64k . We used this vocabulary in all experiments except unrelated language experiment where Arabic and Russian were used instead of Chinese and French data. We combined monolingual data of assisting languages and languages of interest (LOI; Japanese and English) for pre-train"
2020.acl-srw.37,P19-1583,0,0.0136871,"ine ∈ TargetFile do 6 TargetD[len(Line)]+ = 1 ; 9 10 ically the monolingual corpus). When selecting monolingual data of languages of interest, we can first calculate the length distribution of parallel data as target distribution (the ratio of all lengths in T argetF ile) and we fill the length distribution by selecting sentences from monolingual data of same language. As a result, the monolingual data and parallel data have similar length distribution. Data Selection Often, the pre-training monolingual data and the fine-tuning parallel data belong to different domains. (Axelrod et al., 2011; Wang and Neubig, 2019) have shown that proper data selection can reduce the differences between the natures of data between different training domains and phases. In this paper we experiment with (a) Scoring monolingual sentences using a language model (LM) and selecting the highest scoring ones and (b) Selecting monolingual sentences to match the sentence length distribution of the development set sentences in the parallel corpus. 1. LM based data selection: We use a language model trained on corpora belonging to the domain that the fine-tuning data belongs to. We use this sort monolingual sentences according to L"
2020.acl-srw.37,D16-1163,0,0.199846,"resource language translation. Pre-training has enjoyed great success in other NLP tasks with the development of methods like BERT (Devlin et al., 2018). Song et al. (2019) recently proposed MASS, a new state-of-the-art NMT pre-training task that jointly trains the encoder and the decoder. Our approach builds on the initial idea of MASS, but focuses on complementing the potential scarcity of monolingual corpora for the languages of interest using relatively larger monolingual corpora of other (assisting) languages. On the other hand, leveraging multilingualism involves cross-lingual transfer (Zoph et al., 2016) which solves the low-resource issue by using data from different language pairs. Dabre et al. (2017) showed the importance of transfer learning between languages belonging to the same language family but corpora might not always be available in a related language. A mapping between Chinese and Japanese characters (Chu et al., 2012) was shown to be useful for Chinese–Japanese dictionary construction (Dabre et al., 2015). Mappings between scripts or unification of scripts (Hermjakob et al., 2018) can artificially increase the similarity between languages which motivates most of our work. 3 Targ"
2020.acl-srw.37,D19-1632,0,\N,Missing
2020.acl-srw.37,D19-1146,1,\N,Missing
2020.coling-main.119,P17-1042,0,0.0201093,"res (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular sim"
2020.coling-main.119,P18-1073,0,0.0179705,"imedia Dumps; as on April 22, 2020 Additional Monolingual Corpus JNU Sanskrit Proses Corpus Indic NLP Library FastText - GitHub 1387 uses the supervised method named MUSE (Conneau et al., 2017)10 which utilizes a manually curated bilingual lexicon11 for alignments. We use Hindi as a pivot language due to the ease of computation and availability of resources (Corpora and WordNet size). We use the monolingual models described above and train 13 cross-lingual word embedding models (thirteen language pairs over 100 dimensions) using this approach. The second cross-lingual methodology uses VecMap (Artetxe et al., 2018), which utilizes the monolingual models created above. VecMap uses an optional normalization feature while it builds the mappings between any two monolingual models. It performs orthogonal transformation and maps semantically related words, similar to MUSE, which was used in our first approach for building cross-lingual models. Additionally, it also reduces the dimensions of the embeddings models, which, is optional. We train it using the same hyperparameters as described above, for consistency while evaluating. We used the supervised approach for training these models as well, and the trainin"
2020.coling-main.119,D16-1162,0,0.0258029,"an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approximately around 1M tokens (Arthur et al., 2016). Our detected cognate list size varies from 930 cognates (Hi-Te) to 15834 (Hi-Mr). Due to the addition of more parallel instances to the corpus, the vocabulary size for NMT increases. Hence, we experiment further by varying the BPE merges, in a close range, to the optimal merge point obtained earlier. We report the results of the best optimal merge setting, for both NMT-BPE Baseline model and the cognate injected NMT-BPE model, in"
2020.coling-main.119,N09-3008,0,0.0125337,"nza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word re"
2020.coling-main.119,Q17-1010,0,0.0429109,"any other script to the Devanagari script. We perform Unicode transliteration using Indic NLP Library8 to convert scripts for Bn, As, Or, Gu, Pa, Ml, Ta, Kn and Te to Devanagari for standardization. Hi, Mr, Ko, Ne, and Sa are already based on the Devanagari script. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size o"
2020.coling-main.119,bojar-etal-2014-hindencorp,0,0.028968,"Missing"
2020.coling-main.119,D18-2029,0,0.0251243,"l dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular similarity distinguishes nearly parallel vectors much better as small changes in vector values yield considerable distances. For each word pair vector and its context vectors, we compute the ‘word-pair similarity’ and ‘contextual similarity’. We use arccos to obtain angular cosine similarity (asim) among vectors ‘u’ and ‘v’, as shown below:     u.v asim(u, v) = 1 − arccos /π (3) kukkvk Each candidate word-pair generates a score i.e., score1, and the average of scores among all words in the context dictionary generates another score i.e., s"
2020.coling-main.119,P14-2017,0,0.106733,"a. Link: Data, code and models This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information"
2020.coling-main.119,P15-2071,0,0.0199277,"(Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation e"
2020.coling-main.119,P19-4007,0,0.0420076,"Missing"
2020.coling-main.119,D18-1027,0,0.0152337,"e two similarity scores (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and thei"
2020.coling-main.119,E17-1113,0,0.0425306,"Missing"
2020.coling-main.119,jha-2010-tdil,0,0.0142001,"Missing"
2020.coling-main.119,N10-1103,0,0.043284,"be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or"
2020.coling-main.119,2019.gwc-1.51,1,0.808977,", Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic simil"
2020.coling-main.119,2020.lrec-1.378,1,0.798533,"ages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 17021 15726 14097 21710 9235 3363 936 3478 4103 11894 2560 11295 Non-Cognates 16119 15057 15983 15166 23029 8976 4005 1084 4101 3810 13027 1918 9826 Table 1: Number of cognates and non-cognates for each language pair in the dataset. Hi-Ne* and Hi-Ko* were generated via replicating their approach (Kanojia et al., 2020). Language Hi Bn Gu Mr Pa Sa Ml Ta Te Ne As Kn Ko Or Corpus Size 48142K 1564K 439K 520K 505K 553K 495K 909K 1023K 706K 504K 159K 214K 744K STTR (n=1000) 0.5821 0.5437 0.4587 0.6108 0.4314 0.5350 0.7339 0.6411 0.4950 0.4883 0.5968 0.5338 0.5614 0.4160 Table 2: Corpus Statistics where corpus size is the approximate number of lines, and STTR is the moving average type-token ratio on a windows of 1000 sentences. 3 Dataset and Experimental Setup In this section, we describe our primary dataset for the cognate detection task. We also describe the datasets used for building cross-lingual word embeddi"
2020.coling-main.119,P17-4012,0,0.0362403,"once the learning rate falls below 0.001. We perform our experiments with the feature sets (Orthographic (WLS), Phonetic (PVS), and three different cross-lingual embeddings based feature sets) described above for all the thirteen language pairs. We also perform an ablation test with various feature sets and report the results for the best feature combination in the next section. The results of our classification task can be seen in Table 3 and are discussed in the next section, in detail. 4.5 Cognate-aware Neural Machine Translation (NMT) Task For the NMT task, we use the OpenNMT-Py toolkit (Klein et al., 2017) to perform our experiments. We use a Bidirectional RNN Encoder-Decoder architecture with attention (Bahdanau et al., 2014). We choose three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best bas"
2020.coling-main.119,N03-2016,0,0.112806,"ginated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in French can translate to"
2020.coling-main.119,A00-2038,0,0.178355,"logies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (20"
2020.coling-main.119,N01-1014,0,0.277746,"Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual word embeddings. Cross-lingual w"
2020.coling-main.119,2005.mtsummit-papers.40,0,0.203812,"s that have originated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in Fr"
2020.coling-main.119,W12-0216,0,0.0171469,"tion methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Prev"
2020.coling-main.119,N01-1020,0,0.147349,"ative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language"
2020.coling-main.119,J99-1003,0,0.0606347,"ion 4 presents the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection"
2020.coling-main.119,K19-1011,0,0.0223535,"Missing"
2020.coling-main.119,mulloni-pekar-2006-automatic,0,0.0637471,"the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly i"
2020.coling-main.119,W97-1102,0,0.46133,"entations for each token. 4 Approaches We use various approaches to perform the cognate detection task viz. baseline cognate detection approaches like orthographic similarity-based, phonetic similarity-based, phonetic vectors with SiameseCNN based proposed by Rama (2016), and Recurrent neural network-based approach proposed by Kanojia et al. (2019b). We use the same hyperparameters and architectures, as discussed in these papers. We describe each of these feature sets in this section. 4.1 Weighted Lexical Similarity (WLS) The Normalized Edit Distance (NED) approach computes the edit distance (Nerbonne and Heeringa, 1997) for all word pairs in our dataset. Each of the operations has unit cost (except that substitution of a character by itself has zero cost), so NED is equal to the minimum number of operations to transform ‘word a’ to ‘word b’. We use a similarity score provided by NED, which is calculated as (1 - NED Score). We combine NED with q-gram distance (Shannon, 1948) for a better similarity score. The qgrams (‘n-grams’) are simply substrings of length q. This distance measure has been applied previously for various spelling correction approaches (Owolabi and McGregor, 1988; Kohonen, 1978). Kanojia et"
2020.coling-main.119,P02-1040,0,0.108173,"e three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best baseline model with an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approx"
2020.coling-main.119,N18-1202,0,0.0510587,"pt. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5"
2020.coling-main.119,P19-1493,0,0.0254159,"ransliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5 Link: Link: 7 Link: 8 Link: 9"
2020.coling-main.119,N18-2063,0,0.0356884,"Missing"
2020.coling-main.119,C16-1097,0,0.0946439,"work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual wor"
2020.coling-main.119,W99-0626,0,0.196426,"put. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for c"
2020.coling-main.119,W19-4720,0,0.0231209,"lingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic similarity arises for non-identical cognates, based on the reaction time from human annotators. Similarly, Merlo and Andueza Rodriguez (2019) show that cross-lingual models exhibit the semantic properties of for bilingual lexicons despite their structural simplicities, which leads us to perform our investigation for low-resource Indian languages. Uban et al. (2019) discuss the semantic change in languages by studying the change in cognate words across Romance languages using cross-lingual similarity. All of the previous approaches discussed above, lack the use of an appropriate cross-lingual similarity-based measure and do not work well for Indian languages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 1702"
2020.coling-main.376,D17-1209,0,0.0287748,"Missing"
2020.coling-main.376,2020.acl-main.147,0,0.0221429,"ng strategies using RNN namely parallel, hierarchical and mixed, are tried to integrate the dependency information. Wu et al. (2018) used dependency information of both the source and the target languages. As their model needs multiple encoders and decoders, so it is not worthy for use under low-resource condition. In the work of Zhang et al. (2019), the authors employed a supervised encoder-decoder dependency parser and used the outputs from the encoder as a syntax-aware representations of words, which in turn, are concatenated to the input embeddings of the translation model. Most recently, Bugliarello and Okazaki (2020) proposed dependency-aware self-attention in the Transformer that needs no extra parameter. For a pivot word, its self-attention scores with other words are weighted considering their distances from the dependency parent of the pivot word. Apart from the above works, there are some studies which use the factors in the target side (Burlot et al., 2017; Garc´ıa-Mart´ınez et al., 2016a; Garc´ıa-Mart´ınez et al., 2016b). In general, their approach is to predict the roots and other morphological tags of the target words instead of producing the surface forms. Additionally, a morphological analyzer"
2020.coling-main.376,W17-4703,0,0.042239,"Missing"
2020.coling-main.376,P17-1177,0,0.0183396,"anguage Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and fou"
2020.coling-main.376,P16-1078,0,0.020133,"s having diverse morphological variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a"
2020.coling-main.376,J82-2005,0,0.664999,"Missing"
2020.coling-main.376,D17-1012,0,0.0119381,"iza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers r"
2020.coling-main.376,U16-1001,0,0.0175246,"ords due to the inflectional nature of the language. Hence, for morphologically rich languages, ideally, the use of language specific knowledge should improve the translation quality. To the best of our knowledge, there are not an adequate amount of research works on effectively incorporating arbitrary syntactic information into NMT. One possible reason could be that in high resource scenario the network learns from the large amount of training data to handle the problem caused by polysemy and morphological variants. In this direction, the notable works are done by (Sennrich and Haddow, 2016; Hoang et al., 2016; Li et al., 2018). Sennrich and Haddow (2016) incorporated several features at the source side by employing a separate embedding matrix for each component of a source token including the word and its associated features. Finally, all embeddings are concatenated to enrich the representation. Inspired by this work, Hoang et al. (2016) developed a method to process feature sequences of the source sentence by separate recurrent neural networks (RNNs) and combined the output of all RNNs using a hybrid global-local attention strategy. Li et al. (2018) proposed a complex RNN architecture to model so"
2020.coling-main.376,P17-4012,0,0.0292268,"p 15 512 15 15 15 Table 2: Embedding dimensions of the components. Baseline models Proposed models Base Concat Add Linear Self-rel Word-rel en-bg 4.97 5.56 4.66 4.89 6.10 6.25 en-fi 25.59 23.75 22.02 24.26 26.26 26.01 en-hi 18.54 20.69 15.45 20.65 21.27 21.63 en-id 27.93 27.99 24.78 27.17 30.41 26.53 en-khm 22.88 23.53 21.65 23.42 24.76 25.13 en-ms 32.40 32.92 30.45 32.64 34.71 33.20 en-my 13.93 14.92 11.86 13.79 16.53 15.62 en-vi 24.99 26.50 22.78 25.36 27.74 27.66 Table 3: BLEU scores of the models for all reference language pairs. Hyperparameters: We use the OpenNMT PyTorch implementation (Klein et al., 2017) to build our models and mostly follow the Transformer-base hyperparameter setting mentioned there4 . There are 6 layers in each of the encoder and the decoder stacks. The number of multi-heads used is 8. The dimension of the fully-connected-feed-forward network is 2, 048. Total number of training steps is set to 200, 000 and after each 10, 000 steps validation checking is performed. We use the early-stopping strategy in training. If the validation accuracy does not improve for 5 consecutive validation checking steps, then training stops. Following (Sennrich and Haddow, 2016), we keep the dime"
2020.coling-main.376,P17-1064,0,0.0608027,"sis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent differen"
2020.coling-main.376,D15-1166,0,0.23826,") word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017; Kitaev et al., 2020) is known to give state-of-the-art translation quality for language pairs having abundance of parallel corpora. In case of resource poor scenarios, additional translation knowledge is acquired either through transfer learning in the form of pre-trained model parameters or by supplying external monolingual corpora. However, exploiting linguistic information effectively in low-resource conditions is still an under-researched field. Annotating the source side with various syntactic features e.g. part-of-speech (POS), lemma, dependency labels etc. can he"
2020.coling-main.376,P14-5010,0,0.00251552,"ymbols. Linguistic Features Used: We use three linguistic features of the source language in our experiments. They are - (i) lemma, (ii) POS tag and (iii) dependency label. For morphologically rich languages where roots have multiple variants, there tagging the raw text with these three features helps to disambiguate homonymy and polysemy. In particular, if experiments are done at subword-level, then annotating each subword with the word-level features is expected to feed into the model’s performance. As the source side is fixed to English, we annotate the English data using Stanford CoreNLP (Manning et al., 2014) toolkit. The vocabulary sizes of the three features are 26, 414; 43 and 45 respectively. Subword Tags: All experiments are done at subword-level to reduce the out-of-vocabulary cases during inference. We segment the datasets into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016) technique keeping the number of merge operations to be 10, 000. Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word. Hence, following Sennrich and Haddow (2016), we add an extra feature to each subword in the sourc"
2020.coling-main.376,P02-1040,0,0.110315,"ation checking steps, then training stops. Following (Sennrich and Haddow, 2016), we keep the dimension of the final embedding which is fed to the Transformer, comparable across the models without and with using features so that the number of model parameters does not influence the performance. In Table 2 we list the embedding dimensions of the subword and its features for all experimental settings. Inference is done keeping beam size equal to 5. We carry out our experiments using single GPU with the specification of 32 GB Tesla V100-SXM2. 4.1 Results BLEU Scores: We present the BLEU scores5 (Papineni et al., 2002) of our proposed methods and the baselines in Table 3. The scores are computed after undoing the BPE segmentation of the translations. For en-fi, en-id, en-ms, en-my and en-vi, the self relevance checking strategy yields the best results (26.26, 30.41, 34.71, 16.53 and 27.74 respectively). For en-bg, en-hi and en-khm, the wordbased relevance method outperforms others (6.25, 21.63 and 25.13 respectively). Compared to the base configuration, maximum improvement is obtained for en-hi (18.54 → 21.63) and minimum for en-fi (25.59 → 26.26). Compared to (Sennrich and Haddow, 2016) i.e. the concat com"
2020.coling-main.376,W16-2209,0,0.275768,"shared by different root words due to the inflectional nature of the language. Hence, for morphologically rich languages, ideally, the use of language specific knowledge should improve the translation quality. To the best of our knowledge, there are not an adequate amount of research works on effectively incorporating arbitrary syntactic information into NMT. One possible reason could be that in high resource scenario the network learns from the large amount of training data to handle the problem caused by polysemy and morphological variants. In this direction, the notable works are done by (Sennrich and Haddow, 2016; Hoang et al., 2016; Li et al., 2018). Sennrich and Haddow (2016) incorporated several features at the source side by employing a separate embedding matrix for each component of a source token including the word and its associated features. Finally, all embeddings are concatenated to enrich the representation. Inspired by this work, Hoang et al. (2016) developed a method to process feature sequences of the source sentence by separate recurrent neural networks (RNNs) and combined the output of all RNNs using a hybrid global-local attention strategy. Li et al. (2018) proposed a complex RNN arch"
2020.coling-main.376,P16-1162,0,0.0584049,"elps to disambiguate homonymy and polysemy. In particular, if experiments are done at subword-level, then annotating each subword with the word-level features is expected to feed into the model’s performance. As the source side is fixed to English, we annotate the English data using Stanford CoreNLP (Manning et al., 2014) toolkit. The vocabulary sizes of the three features are 26, 414; 43 and 45 respectively. Subword Tags: All experiments are done at subword-level to reduce the out-of-vocabulary cases during inference. We segment the datasets into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016) technique keeping the number of merge operations to be 10, 000. Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word. Hence, following Sennrich and Haddow (2016), we add an extra feature to each subword in the source side in addition to the three linguistic features stated above. Every subword is annotated with one of the four markers - B (beginning), I (inside), E (end), S (single). The annotation is done with the help of the script provided in the corresponding url3 . Table 1 depicts the structure of a samp"
2020.coling-main.376,D16-1159,0,0.0269826,"logical variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of trai"
2020.coling-main.376,W18-6459,0,0.0831158,"y validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent different types of syntax"
2020.coling-main.376,N19-1118,0,0.0608803,"ng the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent different types of syntax. This work gives the"
2020.coling-tutorials.3,Q19-1007,1,0.817811,"can be combined. We will see how MT models for new languages can be rapidly adapted from pre-trained MNMT models. Additionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2"
2020.coling-tutorials.3,2020.findings-emnlp.445,1,0.721245,"or Applied Researcher in the MT team at Microsoft India, where he is involved in building MT systems for multiple Indian languages. His research interests span in different aspects of machine translation, particularly: multilingual models, low-resource translation and translation involving related languages. More broadly, he is interested in different multilingual, cross-lingual and multi-task NLP approaches. He is passionate about building software and resources for NLP in Indian languages. He is the developer of the Indic NLP Library (Kunchukuttan, 2020), co-developer of the IndicNLP-Suite (Kakwani et al., 2020) and a co-founder of the AI4Bharat-NLP Initiative, a community initiative to build Indian language NLP technologies. He has published papers on MT and multilingual learning at ACL, NAACL, EMNLP, TACL and IJCNLP. He has been a member of the organizing committees for COLING 2012 and Workshop on Asian Translation. He received his Ph.D from the Indian Institute of Technology Bombay. References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively Multilingual Neural Machine Translation. In NAACL. Maruan Al-Shedivat and Ankur Parikh. 2019. Consistency by agreement in zero-shot neural machi"
2020.coling-tutorials.3,W18-6325,0,0.0229673,"3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 20"
2020.coling-tutorials.3,N19-1387,1,0.778287,"2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Fu"
2020.coling-tutorials.3,D18-1103,0,0.0161762,"ameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion"
2020.coling-tutorials.3,I17-2050,0,0.0220177,", 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterj"
2020.coling-tutorials.3,W18-2711,0,0.020594,"cmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. from IIT Bombay, India and his Ph.D. from Kyoto University, Japan. He is a post-doctoral researcher at NICT which is Japan’s natio"
2020.coling-tutorials.3,D18-1039,0,0.0186918,"ned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-re"
2020.coling-tutorials.3,W18-6327,0,0.0175966,"ntative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 20"
2020.coling-tutorials.3,P19-1297,0,0.0197742,"Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. fro"
2020.coling-tutorials.3,P16-1009,0,0.0502196,"tionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer"
2020.coling-tutorials.3,P16-1162,0,0.0231572,"tionally, we will spend some time on multi-source NMT which leverages multilingual redundancy in terms of input in order to yield high quality translations. We will end the tutorial with a discussion on possible future directions that we believe that MNMT research should take. 3 Tutorial Outline Some representative papers are mentioned against tutorial sections. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer"
2020.coling-tutorials.3,D18-1326,0,0.0261679,"tions. 1. Introduction (15 min) • Why MNMT? • Motivating multilingual NLP • Cross-lingual embeddings (Conneau et al., 2018; Jawanpuria et al., 2019) • Tutorial roadmap 2. Basics of NMT (20 min) (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Sennrich et al., 2016a; Vaswani et al., 2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30"
2020.coling-tutorials.3,P19-1579,0,0.0106457,"earning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre recei"
2020.coling-tutorials.3,N16-1004,0,0.00916043,"2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source translation (15 min) (Zoph and Knight, 2016; Dabre et al., 2017a) • Missing sentences (Nishimura et al., 2018) • Hybrid multi-source systems • Post-editing (Chatterjee et al., 2017) 8. Future directions (15 min) 9. Summary and conclusion (10 min) Total Time: 180 minutes (excluding break). Type of the Tutorial: Cutting-edge. Pre-requisites: Familiarity with sequence to sequence learning. 4 Tutorial Instructors Dr. Raj Dabre Post-Doctoral Researcher, NICT, Kyoto, Japan email: raj.dabre@nict.go.jp website: https://prajdabre.wixsite.com/prajdabre Raj Dabre received his M.Tech. from IIT Bombay, India and his Ph.D. from Kyoto University, Jap"
2020.coling-tutorials.3,D16-1163,0,0.0194072,"2017) • Architectures (RNN/Transformer) • Pre-processing and training • Decoding (beam-search, reranking) 3. Multi-way translation (45min) (Firat et al., 2016a; Johnson et al., 2017) • Prototypical architectures • Controlling parameter sharing (Sachan and Neubig, 2018; Platanios et al., 2018; Wang et al., 2018) • Addressing language divergence (V´azquez et al., 2018; Gu et al., 2018) • Training protocols (Tan et al., 2019; Lakew et al., 2018) • Massively multilingual models (Aharoni et al., 2019; Bapna et al., 2019) 4. – Coffee Break – 17 5. Transfer learning (30min) • Fine-tuning approaches (Zoph et al., 2016; Firat et al., 2016b) • Utilizing language relatedness (Dabre et al., 2017b; Kocmi and Bojar, 2018) • Lexical and syntactic transfer (Nguyen and Chiang, 2017; Murthy et al., 2019) • Rapid adaption of MT models (Neubig and Hu, 2018; Gheini and May, 2019) 6. Zero-shot/zero-resource learning (30 min) (Johnson et al., 2017; Firat et al., 2016b; Cheng et al., 2017) • Pivoting strategies • Modified training objectives (Al-Shedivat and Parikh, 2019) • Teacher-student learning (Chen et al., 2017) • Unsupervised learning (Lample et al., 2018; Xia et al., 2019; Sen et al., 2019) 7. Multi-source transla"
2020.lrec-1.449,abdelali-etal-2014-amara,0,0.066042,", 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel"
2020.lrec-1.449,P17-1042,0,0.0121265,"necessarily hold for every language pair. We thus encourage researchers to try iterative refinement of training data in their own experimental settings. 5.6. Indirect Assessment of the Created In-Domain Data In this section, we evaluate the superiority of our sentence alignment method, presented in Section 3.2. (henceforth, MT+CS), over other methods, extrinsically, through MT performance. The following two similarity measures were additionally implemented and tested. Unsupervised: Cosine similarity over the cross-lingual sentence embeddings, learned by an unsupervised method, called VecMap (Artetxe et al., 2017).19 18 Compare the pairs (A1, A2), (A2, A3), (A1, A10), (B2, B3), (C3, C4), (C10, C11), (D3, D4), (D10, D11), and (A1, E14). 19 3646 https://github.com/artetxem/vecmap # of aligned lines Unsupervised MT+BLEU MT+CS (B16 in Table 6) 40,452 42,672 40,770 BLEU Ja→En En→Ja 4.0 2.8 6.2 4.3 3.4 6.4 Table 9: BLEU scores achieved with only Coursera parallel data extracted by different similarity measures. Acknowledgments This work was carried out when Haiyue Song was taking up an internship at NICT, Japan. A part of this work was conducted under the program “Research and Development of Enhanced Multili"
2020.lrec-1.449,L18-1528,0,0.0299661,"Missing"
2020.lrec-1.449,2012.eamt-1.60,0,0.254145,"lecture subtitles into other languages, including English, is also an important challenge. The TraMOOC project (Kordoni et al., 2015) aims at improving the accessibility of European languages through MT. They focus on collecting translations of lecture subtitles and constructing MT systems for eleven European and BRIC languages. However, the amount of parallel resources involving other languages, such as Chinese and Japanese, are still quite low. Subtitle translation falls under spoken language translation. Past studies in spoken language translation mainly focused on subtitles for TED talks (Cettolo et al., 2012). Even though the parallel data in this domain should be exploitable for lectures translation to some degree, university lectures are devoted mainly for educational purposes, and the subtle differences in domains may hinder translation quality. To obtain high-quality parallel data, professional translators are typically employed to translate. However, the cost is often very high and thus using this way to produce large quantities of parallel data is economically infeasible, especially for universities and non-profit organizations. In the case of online lectures and talks, subtitles are often t"
2020.lrec-1.449,W19-5435,0,0.0614599,"Missing"
2020.lrec-1.449,P17-2061,1,0.798899,"l., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services ("
2020.lrec-1.449,D19-1146,1,0.865738,"TC→C flow. Bold indicates the initial training, and red-, blue-, and grey-colored cells mean inflation , deflation , and replacement of training data, respectively. into multiple stages where each stage uses data from different domains to maximize the impact of the domain-specific training data. As we have larger parallel corpora from other domains, such as TED (0.2M pairs; non-educational spoken domain) and ASPEC (3.0M pairs; scientific domain), we can leverage domain adaptation techniques, such as finetuning and mixed fine-tuning (Chu et al., 2017). Furthermore, Imankulova et al. (2019) and Dabre et al. (2019) showed that training in multiple stages where each stage contains different proportions of various types of training data leads to the best results. Following them, we decided to conduct an extensive experiment with multistage training with different proportions of training data from different domains at each stage. 5.2. Datasets As in the previous section, we performed Juman++ and NLTK tokenization for Japanese and English, respectively. Henceforth, we refer to the ASPEC training data of 1.0 million lines as “A,” the TED training data of 0.2 million lines as “T,” and the Coursera training da"
2020.lrec-1.449,L18-1545,0,0.0167994,"he best possible sentence alignments for Coursera data were already found, owing to our algorithm, similarity measure, and/or the initial MT system trained only on ASPEC and TED. • Leveraging out-of-domain data through multistage training is invaluable. • Gradually inflating the data starting from out-ofdomain corpus and adding the in-domain corpus at the end should give the best possible translation quality. One peculiarity of our results is that the BLEU scores for the Ja→En task were significantly higher than the En→Ja task, which is a reversal of a general tendency for this language pair (Imamura and Sumita, 2018; Nakazawa et al., 2019), even though the BLEU scores in different languages are not directly comparable. Table 7 gives a comparison of three different translation tasks. Upon manual investigation, we identified that the En→Ja translations in TED and Coursera tasks tend to be much shorter than the reference translation, receiving around 0.7 brevity penalty. When we tuned the length penalty for decoding on the development set, we observed 0.5 to 1.0 point BLEU gains on the test set Iterative Refinement of Aligned Data Having obtained a better MT system than the initial one, we can iterate the w"
2020.lrec-1.449,W19-6613,1,0.842963,"1) sub-paths of the A→AT→ATC→TC→C flow. Bold indicates the initial training, and red-, blue-, and grey-colored cells mean inflation , deflation , and replacement of training data, respectively. into multiple stages where each stage uses data from different domains to maximize the impact of the domain-specific training data. As we have larger parallel corpora from other domains, such as TED (0.2M pairs; non-educational spoken domain) and ASPEC (3.0M pairs; scientific domain), we can leverage domain adaptation techniques, such as finetuning and mixed fine-tuning (Chu et al., 2017). Furthermore, Imankulova et al. (2019) and Dabre et al. (2019) showed that training in multiple stages where each stage contains different proportions of various types of training data leads to the best results. Following them, we decided to conduct an extensive experiment with multistage training with different proportions of training data from different domains at each stage. 5.2. Datasets As in the previous section, we performed Juman++ and NLTK tokenization for Japanese and English, respectively. Henceforth, we refer to the ASPEC training data of 1.0 million lines as “A,” the TED training data of 0.2 million lines as “T,” and"
2020.lrec-1.449,W17-3204,0,0.0132128,"https://github.com/shyyhs/CourseraParallelCorpusMining http://mooc.org 4 https://www.coursera.org 5 https://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addr"
2020.lrec-1.449,W15-4935,0,0.065899,"Missing"
2020.lrec-1.449,L16-1003,0,0.0411224,"Missing"
2020.lrec-1.449,L18-1236,0,0.0116466,"ic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel document"
2020.lrec-1.449,2015.iwslt-evaluation.11,0,0.0224968,"model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpor"
2020.lrec-1.449,D17-1156,0,0.0170107,"ing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services (Behnke et al., 2018). However, this can be extremely timeconsuming if not expensive. Previous research (Abdelali et 2 al., 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and R"
2020.lrec-1.449,L16-1350,1,0.819808,"data. Dataset English Mean / Median / s.d. Japanese Mean / Median / s.d. ASPEC TED Coursera 25.4 / 23 / 11.4 20.4 / 17 / 13.9 21.1 / 19 / 11.1 27.5 / 20 / 12.0 19.8 / 16 / 14.1 22.2 / 20 / 11.8 Table 2: BLEU score for Ja→En on TED test set. A, T, and AT respectively stand for ASPEC training set, TED training set, and their balanced mixture. “→” indicates that the model trained on the left-hand side data is fine-tuned on the right-hand side data. domain. However, given its small size, it can lead to only an unreliable MT system. Therefore, we decided to use a larger out-of-domain ASPEC corpus (Nakazawa et al., 2016)13 to build a better MT system. Table 1 gives the statistics of the ASPEC and TED corpora that we used to train our initial MT system. We compared fine-tuning and mixed fine-tuning approaches proposed by Chu et al. (2017). When performing mixed fine-tuning on the concatenation of both two corpora, the TED corpus was oversampled to match the size of the ASPEC corpus. We trained our NMT models using tensor2tensor with its default hyper-parameters. Refer to Section 5.3. for further details on training configurations. So far, we do not have a test set for the target domain, i.e., Coursera. We ther"
2020.lrec-1.449,P02-1040,0,0.109811,"o translate one side into the other language using an MT system (Sennrich and Volk, 2010). To train such a system, we can leverage any existing parallel data in related or even distant domains. The MT system should generate translations as accurately as possible. In practice, domain adaptation techniques (Chu et al., 2017) are most useful in training an accurate MT system. 3.2.2. Similarity Measure The key component in the DP algorithm is the matching function, i.e., similarity measure in our context. Existing methods, such as that in Sennrich and Volk (2010), used sentence-level BLEU scores (Papineni et al., 2002) of machine-translated source sentence against the actual target language sentence as their similarity score: formally, Sim BLEU (fi , ej ) = BLEU (MT (fi ), ej ), (1) where fi and ej are the i-th sentence in the source document and the j-th sentence in the target document, respectively. However, due to the lack of in-domain data, MT system can give only translations of low quality and thus the BLEU scores can be misleading, especially for distant language pairs, such as Japanese and English. An alternative way is to directly compute cosine similarity of a given sentence pair (Bouamor and Sajj"
2020.lrec-1.449,N16-1110,0,0.0201253,"Missing"
2020.lrec-1.449,2010.amta-papers.14,0,0.10192,"such as German, Polish, and Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the"
2020.lrec-1.449,W11-4624,0,0.0319982,"nd Russian. Using automatic alignment methods are more desirable, because they can help extract parallel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domai"
2020.lrec-1.449,P16-1009,0,0.220059,"2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translation"
2020.lrec-1.449,D19-1168,0,0.0189494,"erage similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japanese–English setting."
2020.lrec-1.449,N19-1209,0,0.0109303,"arallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006; Miceli Barone et al., 2017; Thompson et al., 2019). Furthermore, the domain divergence between the out-of- and in-domain corpora is another issue. 3. Parallel Corpus Alignment Extracting parallel data usable for MT involves crawling documents and aligning translation pairs in the corpora. To align translations, one can use crowdsourcing services (Behnke et al., 2018). However, this can be extremely timeconsuming if not expensive. Previous research (Abdelali et 2 al., 2014) focused on collecting data from AMARA platform (Jansen et al., 2014). They usually aim at European and BRIC languages, such as German, Polish, and Russian. Using automatic"
2020.lrec-1.449,tiedemann-2012-parallel,0,0.0507603,"://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over-fitting which can be addressed by strong regularization techniques (Hinton and Salakhutdinov, 2006; Chelba and Acero, 2006"
2020.lrec-1.449,L16-1559,0,0.333293,"data, professional translators are typically employed to translate. However, the cost is often very high and thus using this way to produce large quantities of parallel data is economically infeasible, especially for universities and non-profit organizations. In the case of online lectures and talks, subtitles are often translated by crowdsourcing (Behnke et al., 2018) which involves non-professional translators. The resulting translation can thus be often inaccurate and quality control is indispensable. There are many automatic ways to find parallel sentences from roughly parallel documents (Tiedemann, 2016).1 In particular, MT-based approaches are quite desirable because of their simplicity and it is possible to use existing translation models to extract additional parallel data. However, using an MT system trained on data from another domain can give unreliable translations which can lead to parallel data of low quality. In this paper, we propose a new method which combines machine translation and similarities of sentence vector representations to automatically align sentences between roughly aligned document pairs. As we are interested in educational lectures translation, we focus on extractin"
2020.lrec-1.449,C73-2019,0,0.51231,"Missing"
2020.lrec-1.449,C94-2175,0,0.762098,"at can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel documents (top-left), such as those available from Coursera, and out-of-domain parallel sentences (bottom-right). We give details about the way we pre"
2020.lrec-1.449,W18-1819,0,0.0295138,"ection 4.4., we decided to focus on the training schedule A→AT→ATC→TC→C, where in each stage we use datasets more similar with indomain dataset (Wang et al., 2019a), and thoroughly evaluated all of its sub-paths. We also used T and AC for some contrastive experiments. 5.3. Settings for MT We created a shared sub-word vocabulary for Japanese and English from ASPEC and TED training set using BPE (Sennrich et al., 2016b) with roughly 32k merge operations. This vocabulary was used for all experiments, even when a model is trained only on C. We trained NMT models using the tensor2tensor framework (Vaswani et al., 2018)16 with its default “transformer big” setting, such as dropout=0.2, attention dropout=0.1, optimizer=adam with beta1=0.9, beta2=0.997. We used eight Tesla V100 32GB GPUs with batch size of 4,096 sub-word tokens, and early-stopping on approximate BLEU score computed on the development set: the training process stops when the score shows no gain larger than 0.1 for 10,000 steps. When fine-tuning the model on a different dataset, we resumed the training process from the last checkpoint in the previous stage. In the decoding step, we used the average of the last 10 checkpoints, and decoded the tes"
2020.lrec-1.449,P19-1116,0,0.0265469,"pairs in the descending order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection proc"
2020.lrec-1.449,P19-1583,0,0.0198191,"allel sentences that are orders of magnitude larger than those that can be obtained by manual translation, including crowdsourcing. Although the quality of the extracted parallel sentences might be low, relying on comparable corpora can help address quality issues (Wołk, 2015) where one can use time-stamp to roughly align corresponding documents (Abdelali et al., 2014; Tiedemann, 2016). In order to obtain high-quality parallel data from these documents, MT-based methods (Sennrich and Volk, 2010; Sennrich and Volk, 2011; Liu et al., 2018) and similarity-based methods (Bouamor and Sajjad, 2018; Wang and Neubig, 2019) can be combined with dynamic programming (Utsuro et al., 1994) for fast and accurate sentence alignment. The LASER tool (Chaudhary et al., 2019)8 offers another way to align sentence pairs automatically in an unsupervised fashion. Our Framework for Mining Coursera Parallel Corpus This section describes our general framework to compile a parallel corpus in the educational lectures domain, relying on Coursera. Figure 1 gives an overview of our framework, where we assume the availability of in-domain parallel documents (top-left), such as those available from Coursera, and out-of-domain parallel"
2020.lrec-1.449,P19-1123,0,0.0830708,"ding order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japan"
2020.lrec-1.449,P19-1624,0,0.094315,"ding order of the average similarity of all aligned sentence pairs within each document pair. We then subject these sorted and sentence aligned pairs to human evaluation using Algorithm 1 in order to obtain high-quality test and development sets, where the target volume of each set (volume) and document-level comparability (ratio) are the two parameters. We use the remaining sentence aligned document pairs for training. Our test, development, and training sets are all constructed at the document level and thus our corpora can be used to evaluate document-level translation (Voita et al., 2019; Wang et al., 2019b; Tan et al., 2019). 4. 20 Cleaning Documents Our framework is mostly language independent. The only language specific processes are tokenization and language mismatch detection. We first segmented the both English and Japanese paragraphs with full-stop (“.”), exclamation (“!”), and question marks (“?”) in Latin encoding and their full-width counterparts in UTF-8 followed by a space or the end of line. Then, we tokenized Japanese and English sentences, using Juman++ (Tolmachev et al., 2018)11 and NLTK,12 respectively. Algorithm 2 shows our rule-based language detection procedure for the Japan"
2020.lrec-1.449,D16-1163,0,0.0201952,"eral domains. 2.2. https://github.com/shyyhs/CourseraParallelCorpusMining http://mooc.org 4 https://www.coursera.org 5 https://iversity.org 6 https://wit3.fbk.eu/mt.php?release=2017-01-ted-test 7 https://www.opensubtitles.org 3 2.3. Domain Adaptation for Neural Machine Translation At present, neural machine translation (NMT) is known to give higher quality of translation. To train a sequence-tosequence model (Sutskever et al., 2014), attention-based model (Bahdanau et al., 2015) or self-attention based model (Vaswani et al., 2017), we need a large parallel corpus for high-quality translation (Zoph et al., 2016; Koehn and Knowles, 2017). In the case of the news domain, there are many corpora, e.g., News Commentary (Tiedemann, 2012), containing large number of parallel sentences that enable high-quality translation. In contrast, for educational lectures translation, only relatively small datasets are available. Transfer learning through fine-tuning an out-ofdomain model on the in-domain data (Luong and Manning, 2015; Sennrich et al., 2016a; Zoph et al., 2016; Chu et al., 2017) is the most common way to overcome the lack of data. However, approaches based on fine-tuning suffer from the problem of over"
2020.lrec-1.454,P05-1066,0,0.299925,"Missing"
2020.lrec-1.454,D19-1146,1,0.794451,"dels, code and annotated data as resources for reproducibility and public use. 2. Related Work Pre-training based approaches are essentially transfer learning approaches where we leverage an external source of data to train a model whose components can be used for NLP tasks which do not have abundant data. In the context of NMT, cross-lingual transfer (Zoph et al., 2016) was shown to be most effective to improve Hausa-English translation when a pre-trained French-English NMT model was fine-tuned on Hausa-English data. While this work focused on strongly pre-training the English side decoder, (Dabre et al., 2019) showed that pre-training the encoder is also useful through experiments on fine-tuning an English–Chinese model on a small multi-parallel English–X (7 Asian languages) data. All these works rely on bilingual corpora but our focus is on leveraging monolingual corpora that are orders of magnitude larger than bilingual corpora. Pre-trained models such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018), XLNET (Yang et al., 2019) and GPT (Radford, 2018) have proved very useful for 2 https://github.com/Mao-KU/JASS tasks such as Text Understanding, but have a limited application to NMT, as th"
2020.lrec-1.454,N19-1423,0,0.182544,"monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-sp"
2020.lrec-1.454,W17-4123,0,0.030112,"show that linguistically motivated pre-training can be complementary to MASS. Our research is motivated by previous research (Kawahara et al., 2017) for Japanese NLP which showed that linguistic annotations from the syntactic analyzers such as Juman (Morita et al., 2015) and KNP (Kurohashi et al., 1994) are extremely important. Pre-ordering consists of pre-processing a sentence so that its word-order is more similar to that of its expected translation. It has been a popular technique for Statistical Machine Translation since the early work of (Collins et al., 2005). Although initial research (Du and Way, 2017) had concluded that pre-ordering had limited usefulness for NMT, it has been shown more recently that it can improve translation quality, especially in the case of low-resource languages. (Murthy et al., 2019) showed that pre-ordering English to Indic language word order is beneficial when performing transfer learning via fine-tuning. (Zhou et al., 2019) showed that leveraging structural knowledge for creating the psuedo Japanese-ordered English by pre-ordering English from SVO to SOV improves Japanese–English translation. Our work will try to incorporate similar ideas directly in the pre-trai"
2020.lrec-1.454,D18-1045,0,0.0356188,"Missing"
2020.lrec-1.454,W18-2703,0,0.0338408,"the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentia"
2020.lrec-1.454,W14-7008,0,0.0272325,"We expect that this will let the system learn the structure of Japanese language, as well as prepare it for the reordering operation it will have to perform when translating to a language with different grammar. 4.2.1. Bunsetsu-based Reordering We first define here a simple process for re-ordering a (typically SOV) Japanese sentence into a “SVO-ordered Japanese” pseudo-sentence. We will then use this reordered sentence in section 4.2.2. for our BRSS pre-training. There exist several previous works about reordering a SOVordered sentence to a SVO-ordered sentence (Katz-Brown and Collins, 2008; Hoshino et al., 2014). In our case, 3685 : Bunsetsu (a) Origin (b) MASS (c) BMASS : Chunking Signal Token : Token ラブライブ は 、 三 つ の プロジェクト に よって 構成 さ れて いる 。 Src. ラブライブ は 、 三 [M] [M] [M] [M] [M] [M] [M] れて いる 。 Tgt. [M] [M] [M] [M] つ の プロジェクト に よって 構成 [M] さ [M] [M] [M] Src. ラブライブ は [M] [M] [M] プロジェクト に よって [M] [M] [M] [M] [M] Tgt. [M] [M] [M] 三 つ の [M] [M] [M] 構成 さ れて いる 。 Src. ラブライブ は 、 構成 いる よって 三 つ の 。 Tgt. ラブライブ は 、 三 つ の Src. LoveLive (theme) , make Tgt. LoveLive (theme) , three _ 、 さ れて プロジェクト に (d) BRSS English version of BRSS プロジェクト _ (passive) of _ project に よって based on 構成 project based make さ れて on three"
2020.lrec-1.454,W19-6613,1,0.911441,"ives, which we dub MASS+JASS in the following sections. 5. Experimental Settings In this section, we evaluate our pre-training methods on 4 translation directions: Japanese-to-English (Ja-En), English-to-Japanese (En-Ja), Japanese-to-Russian (Ja-Ru) and Russian-to-Japanese (Ru-Ja). Specifically, we monitor the performance of our pre-training methods on both simulated low-resource and high-resource scenarios involving ASPEC Japanese–English translation (Nakazawa et al., 2015). We also test our methods on a realistic low-resource scenario involving News Commentary Japanese–Russian translation3 (Imankulova et al., 2019). 5.1. Datasets and Pre-processing from the official WMT monolingual training data5 for pretraining. Each side of the parallel data used in fine-tuning is also incorporated into the monolingual data for pretraining. Specifically, for Japanese and English, 3M sentences from each side of the parallel data is added to the monolingual data while for Japanese and Russian, 10K sentences from each side of the parallel data is also used in pre-training. This results in 50M monolingual sentences for Japanese and English, and 45M monolingual sentences for Japanese and Russian. Given that our pre-trainin"
2020.lrec-1.454,W17-6301,1,0.851346,"er. Pre-training schemes more suitable to NMT have been proposed by (Lample and Conneau, 2019), (Ren et al., 2019) and (Song et al., 2019). In particular, (Song et al., 2019) obtained state-of-the-art results with their “MASS” pre-training scheme. MASS allows for the simultaneous pre-training of the encoder and decoder and hence is the most useful for NMT. However, MASS does not consider the linguistic properties of language when pretraining whereas our objective is to show that linguistically motivated pre-training can be complementary to MASS. Our research is motivated by previous research (Kawahara et al., 2017) for Japanese NLP which showed that linguistic annotations from the syntactic analyzers such as Juman (Morita et al., 2015) and KNP (Kurohashi et al., 1994) are extremely important. Pre-ordering consists of pre-processing a sentence so that its word-order is more similar to that of its expected translation. It has been a popular technique for Statistical Machine Translation since the early work of (Collins et al., 2005). Although initial research (Du and Way, 2017) had concluded that pre-ordering had limited usefulness for NMT, it has been shown more recently that it can improve translation qu"
2020.lrec-1.454,W04-3250,0,0.359057,"cantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks. Keywords: pre-training, neural machine translation, bunsetsu, low resource 1. Introduction Encoder-decoder based neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015), and in particular, the Transformer model (Vaswani et al., 2017) have led to a large jump in the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of th"
2020.lrec-1.454,D15-1276,1,0.94624,"al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et al., 1994; Morita et al., 2015). On the other hand, large parallel corpora involving Japanese exist only for a few language pairs and domains. As such it is critical to leverage both monolingual data and the syntactic analyses of Japanese for optimal translation quality. Our pre-training approach is inspired by MASS, but with more linguistically motivated tasks. In particular, we add syntactic constraints to the sentence-masking process of MASS and dub the resulting task BMASS1 . We also add a linguistically-motivated reordering task that we dub BRSS (Bunsetsu Reordering Sequence to Sequence). We combine these two tasks to"
2020.lrec-1.454,N19-1387,0,0.0964144,"P tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works such as (Sennrich and Haddow, 2016; Murthy et al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et al., 1994; Morita et"
2020.lrec-1.454,W15-5001,1,0.762132,"e” monolingual data with MASS objective. We can also consider using Japanese monolingual data with a combination of BMASS, BRSS and MASS objectives, which we dub MASS+JASS in the following sections. 5. Experimental Settings In this section, we evaluate our pre-training methods on 4 translation directions: Japanese-to-English (Ja-En), English-to-Japanese (En-Ja), Japanese-to-Russian (Ja-Ru) and Russian-to-Japanese (Ru-Ja). Specifically, we monitor the performance of our pre-training methods on both simulated low-resource and high-resource scenarios involving ASPEC Japanese–English translation (Nakazawa et al., 2015). We also test our methods on a realistic low-resource scenario involving News Commentary Japanese–Russian translation3 (Imankulova et al., 2019). 5.1. Datasets and Pre-processing from the official WMT monolingual training data5 for pretraining. Each side of the parallel data used in fine-tuning is also incorporated into the monolingual data for pretraining. Specifically, for Japanese and English, 3M sentences from each side of the parallel data is added to the monolingual data while for Japanese and Russian, 10K sentences from each side of the parallel data is also used in pre-training. This"
2020.lrec-1.454,L16-1350,1,0.853945,"[BM ASS] or [RSS] and a language token [Ja], [En], or [Ru]. This ensures that the model learns to distinguish between different pre-training objectives and languages. 5.2. For the NMT model, we experiment with a Transformer (Vaswani et al., 2017) having 6 layers for both the encoder and the decoder. We implement our approaches on top of the OpenNMT9 transformer implementation. We use both the monolingual data and parallel data for pretraining and the parallel data for fine-tuning. Refer to Table 1 for an overview. 5.1.1. Parallel Data We use scientific abstracts domain ASPEC parallel corpus (Nakazawa et al., 2016) for Japanese–English translation and the news commentary domain JaRuNC parallel corpus (Imankulova et al., 2019) for Japanese–Russian translation. 5.1.2. Monolingual data We use monolingual data containing 22M Japanese, 22M English and 22M Russian sentences randomly sub-sampled from Common Crawl dataset and News crawl4 dataset 3 Neither Japanese nor Russian are low-resource languages, but Ja-Ru can be regarded as a low-resource language pair because of the limited amount of the parallel data. 4 The pre-training will be very effective if the domains of the pre-training and fine-tuning dataset"
2020.lrec-1.454,P02-1040,0,0.1071,"ent our pre-training methods and fine-tuning using the Transformer-big setting, which consists of a 6-layer encoder and a 6-layer decoder, with the length of 1024 for hidden size, the length of 4096 for feedforward size, dropout rate of 0.3 and attention heads of 16. A learning-rate of 10−4 is used both for pre-training and fine-tuning, and all the pre-training tasks are implemented on 8 TITAN X (Pascal) GPU cards until convergence with a batch-size of 2048 for each GPU while single GPU is used for fine-tuning. The checkpoint with the highest accuracy is selected for fine-tuning. We use BLEU (Papineni et al., 2002) to implement the evaluation. We do early stopping if no improvement on development-set within 5 checkpoints, and the checkpoint with the best BLEU performance on development-set is selected for evaluation. For multi-task pre-training, data is randomly shuffled so that even in each mini-batch, different pre-training objectives will appear, corresponding to a real joint pre-training. We evaluate the statistical significance of our BLEU scores by bootstrap resampling (Koehn, 2004). 5.3. We compare these models with baselines which do not use pre-training. 6. Results & Analysis We now give the re"
2020.lrec-1.454,N18-1202,0,0.0704212,"re much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works su"
2020.lrec-1.454,N18-2084,0,0.0259756,". This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence mo"
2020.lrec-1.454,D19-1071,0,0.118197,"Missing"
2020.lrec-1.454,W16-2209,0,0.153824,"(Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works such as (Sennrich and Haddow, 2016; Murthy et al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et"
2020.lrec-1.454,P16-1009,0,0.464907,"e led to a large jump in the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-lik"
2020.lrec-1.454,P16-1162,0,0.49472,"e led to a large jump in the quality of automatic translation over previous approaches such as Statistical Machine Translation (Koehn, 2004). One of the drawbacks of NMT is that it requires large parallel corpora for training robust and high quality translation models. This strongly limits its usefulness for many language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-lik"
2020.lrec-1.454,D16-1160,0,0.0258231,"1 ≤ len(x) and len(x) is the number of tokens in sentence x. We denote by xC the masked sequence where tokens in positions from p1 to p2 , p3 to p4 and so on until pn to pn+1 in x are replaced by a special token [M ]. x!C is the invert masked sequence where tokens in positions other than the aforementioned fragments are replaced by the mask token [M ]. MASS is a pre-training objective that predicts the masked fragments in x using an encoder-decoder model where xC is the input to the encoder and x!C is the reference for the decoder. The log likelihood objective function is: based pre-training (Zhang and Zong, 2016). It is a combination of two sub-methods, BMASS (Bunsetsu-based MAsked Sequence to Sequence pre-training) and BRSS (Bunsetsu Reordering Sequence to Sequence pre-training). 4.1. In MASS, a NMT model is trained by making it predict random parts of a sentence given their context. Instead of random parts we are interested in making the model predict a set of bunsetsus given the contextual bunsetsus. We expect this will let the model learn about the important concept of bunsetsu, as well as focus its training on predicting meaningful subsequences instead of random ones. More precisely, we propose B"
2020.lrec-1.454,D19-1143,0,0.0909798,"se methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Another way to overcome the scarcity of parallel data is to provide the model with more “linguistic knowledge”, such as language-specific information. Works such as (Sennrich and Haddow, 2016; Murthy et al., 2019; Zhou et al., 2019) have shown that such information could improve results. However, because NMT models are end-to-end sequence to sequence models, the manner in which such linguistic hints should be provided is not always clear. In this paper, we argue that pre-training provides an ideal framework both for leveraging monolingual data and improving NMT models with linguistic information. Our setting focuses on the translation between language pairs involving Japanese. Japanese is a language for which very high quality syntactic analyzers have been developed (Kurohashi et al., 1994; Morita et al., 2015). On the o"
2020.lrec-1.454,D16-1163,0,0.430581,"language pairs and domains for which no such large corpora exist. The most popular way to solve this issue is to leverage monolingual corpora, which are much easier to obtain (as compared to parallel corpora) for most languages and domains. This can be done either by backtranslation (Sennrich et al., 2016a; Hoang et al., 2018; Edunov et al., 2018) or by pre-training. Pre-training consists in initializing some or all of the parameters of the model through tasks that only require monolingual data. One can pre-train the word embeddings of the model (Qi et al., 2018) or the encoder and decoders (Zoph et al., 2016). Pre-training has recently become the focus of much research after the success of methods such as BERT (Devlin et al., 2019), ELMO (Peters et al., 2018) or GPT (Radford, 2018) in many NLP tasks. However, these methods were not designed to be used for NMT models in the sense that BERT-like models are essentially language models and not sequence to sequence models. (Song et al., 2019) have obtained new state-of-theart results for NMT in low-resource settings by addressing these issues and providing a pre-training method for sequence to sequence models: MASS (MAsked Sequence to Sequence). Anothe"
2020.lrec-1.454,W14-7001,1,\N,Missing
2020.ngt-1.3,I17-1001,0,0.0669869,"Missing"
2020.ngt-1.3,2000.eamt-1.5,0,0.626159,"Missing"
2020.ngt-1.3,W18-6301,0,0.0510352,"Missing"
2020.ngt-1.3,P02-1040,0,0.106902,"he fact that vanilla and our tied-multi models have identical shapes when n and m for encoder and decoder layers are specified. the same number of examples during training,6 we trained the models for 300k iterations, with 1 GPU for the vanilla models and 2 GPUs with batch size halved for our tied-multi model. We averaged the last 10 checkpoints saved every after 1k updates, decoded the test sentences, fixing a beam size7 of 4 and length penalty of 0.6, and post-processed the decoded results using the detokenizer.perl and detruecase.perl in Moses. We evaluated our models using the BLEU metric (Papineni et al., 2002) implemented in sacreBLEU (Post, 2018).8 We also present the time consumed to translate the test data, which includes times for the model instantiation, loading the checkpoints, sub-word splitting and indexing, decoding, and subword de-indexing and merging, whereas times for detokenization are not taken into account. Note that we did not use any development data for two reasons. First, we train all models for the same number of iterations. Second, we use checkpoint averaging before decoding, which does not require development data unlike early stopping. the one for the vanilla model, when perf"
2020.ngt-1.3,N19-1423,0,0.0388521,"he independence of target labels (layer combinations) for a given input sentence allows for ties, the model is able to predict multiple layer combinations for the same input sentence. We implemented the model f with a multi-head self-attention neural network inspired by Vaswani et al. (2017). The number of layers and attention heads are optimized during a hyper-parameter search, while the feed-forward layer dimensionality is fixed to 2,048. Input sequences of tokens are mapped to their corresponding embeddings, initialized by the embedding table of the tied-multi NMT model. Similarly to BERT (Devlin et al., 2019), a specific token is prepended to input sequence before being fed to the classifier. This token is finally fed during the forward pass to the output linear layer for sentence classification. The output linear layer has K dimensions, allowing to output as many logits as the number of layer combinations in the tied-multi NMT model. Finally, a sigmoid function outputs probabilities for each layer combination among the K possible combinations. The parameters θ of the model f are learned using mini-batch stochastic gradient descent with Nesterov momentum (Sutskever et al., 2013) and the loss funct"
2020.ngt-1.3,W16-2341,0,0.115507,"Missing"
2020.ngt-1.3,W18-6319,0,0.0137795,"ve identical shapes when n and m for encoder and decoder layers are specified. the same number of examples during training,6 we trained the models for 300k iterations, with 1 GPU for the vanilla models and 2 GPUs with batch size halved for our tied-multi model. We averaged the last 10 checkpoints saved every after 1k updates, decoded the test sentences, fixing a beam size7 of 4 and length penalty of 0.6, and post-processed the decoded results using the detokenizer.perl and detruecase.perl in Moses. We evaluated our models using the BLEU metric (Papineni et al., 2002) implemented in sacreBLEU (Post, 2018).8 We also present the time consumed to translate the test data, which includes times for the model instantiation, loading the checkpoints, sub-word splitting and indexing, decoding, and subword de-indexing and merging, whereas times for detokenization are not taken into account. Note that we did not use any development data for two reasons. First, we train all models for the same number of iterations. Second, we use checkpoint averaging before decoding, which does not require development data unlike early stopping. the one for the vanilla model, when performing decoding with the 6 encoder and"
2020.ngt-1.3,D18-1457,0,0.0231562,"coder-decoder layer combinations. Section 4 describes our efforts towards designing and evaluating a mechanism for dynamically selecting encoderdecoder layer combinations prior to decoding. Section 5 describes two orthogonal extensions to our model aiming at further model compression and speeding-up of decoding. The paper ends with Section 6 containing conclusion and future work. 2 Related Work There are studies that exploit multiple layers simultaneously. Wang et al. (2018) fused hidden representations of multiple layers in order to improve the translation quality. Belinkov et al. (2017) and Dou et al. (2018) attempted to identify which layer can generate useful representations for different natural language processing tasks. Unlike them, we make all layers of the encoder and decoder usable for decoding with any encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that ties parameters between layers (Dabre and Fujita, 2019) and/or between the encoder and decoder within a single model (Xia et al., 2019; Dabre"
2020.ngt-1.3,K16-1029,0,0.0613098,"Missing"
2020.ngt-1.3,D16-1139,0,0.308824,"ny encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that ties parameters between layers (Dabre and Fujita, 2019) and/or between the encoder and decoder within a single model (Xia et al., 2019; Dabre and Fujita, 2019). Parameter tying leads to compact models, but they usually suffer from drops in inference quality. In this paper, we counter such drops with knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016; Freitag et al., 2017). This approach utilizes smoothed data or smoothed training signals instead of the actual training data. A model with a large number of parameters and high per1 Rather than casting the encoder-decoder model into a single column model with (N + M ) layers. 25 formance provides smoothed distributions that are then used as labels for training small models instead of one-hot vectors. As one of the aims in this work is model size reduction, it is related to a growing body of work that addresses the computational requirement reduction. Pruning of pre-trained models (See et al."
2020.ngt-1.3,N04-1022,0,0.101292,"wn in Figure 2, we tackled an advanced problem: dynamic selection of one layer combination prior to decoding.11 4.1 Method We formalize the encoder-decoder layer combination selection with a supervised learning approach 10 We measured the collapsed time for a fair comparison, assuming that all vanilla models were trained on a single GPU one after another, even though one may be able to use multiple GPUs to train the 36 vanilla models in parallel. 11 This is the crucial difference from two post-decoding processes: translation quality estimation (Specia et al., 2010) and n-best-list re-ranking (Kumar and Byrne, 2004). 28 the macro Fβ implemented following (4). where the objective is to minimize the following loss function (2). arg min θ 1 X L(f (si ; θ), tik ), |S |i LiFβ (2) s ∈S  = 1 − (1 + β 2 ) ·  P ·R , (β 2 · P ) + R (4) P P where µ/ k yˆki , R = µ/ k yki , and µ = P iP = yk · yki ). k (ˆ The final loss function is the linear interpolation of LBCE averaged over the K classes and LFβ with parameter λ, both averaged over the batch: λ × LBCE + (1 − λ) × LFβ . We tune α, β, and λ during the classifier hyper-parameter search based on the validation loss. where si is the i-th input sentence (1 ≤ i ≤ |S|"
2020.ngt-1.3,C18-1255,0,0.0209797,"on 2 briefly reviews related work for compressing neural models. Section 3 covers our method that ties multiple models by softmaxing all encoder-decoder layer combinations. Section 4 describes our efforts towards designing and evaluating a mechanism for dynamically selecting encoderdecoder layer combinations prior to decoding. Section 5 describes two orthogonal extensions to our model aiming at further model compression and speeding-up of decoding. The paper ends with Section 6 containing conclusion and future work. 2 Related Work There are studies that exploit multiple layers simultaneously. Wang et al. (2018) fused hidden representations of multiple layers in order to improve the translation quality. Belinkov et al. (2017) and Dou et al. (2018) attempted to identify which layer can generate useful representations for different natural language processing tasks. Unlike them, we make all layers of the encoder and decoder usable for decoding with any encoder-decoder layer combination. In practical scenarios, we can save significant amounts of time by choosing shallower encoder and decoder layers for inference. Our method ties the parameters of multiple models, which is orthogonal to the work that tie"
2020.ngt-1.3,P18-1166,0,0.0400003,"Missing"
2020.wat-1.1,L18-1548,1,0.762787,"emselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training, development and test data. Par ticipants of this taks must get a copy of BSD corpus by themselves. 2.12 IITB Hindi–English task In this task we use IIT Bombay EnglishHindi Cor pus (Kunchukuttan et al., 2018) which contains EnglishHindi parallel corpus as well as mono lingual Hindi corpus collected from a variety of sources and corpora (Bojar et al., 2014). This cor pus had been developed at the Center for Indian Language Technology, IIT Bombay over the years. The corpus is used for mixed domain tasks hi↔en. The statistics for the corpus are shown in Table 13. 3 4.1 Tokenization We used the following tools for tokenization. 4.1.1 For ASPEC, JPC, TDDC, JIJI, ALT, UCSY, ECCC, and IITB • Juman version 7.024 for Japanese segmenta tion. • Stanford Word Segmenter version 201401 0425 (Chinese Penn"
2020.wat-1.1,E06-1031,0,0.0814841,"ipants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcomes. The scores are thus comparable only wi"
2020.wat-1.1,W17-5701,1,0.728933,"on to documentlevel evaluation. 2.11 Documentlevel Translation Task In WAT2020, we set up 2 documentlevel transla tion tasks: ParaNatCom and BSD. 20 21 http://www2.nict.go.jp/astrec-att/member/ mutiyama/paranatcom/ https://github.com/nlabmpg/Flickr30kEntJP 7 Lang hien hi Train 1,609,682 – Dev 520 – Test 2,507 – Mono – 45,075,279 systems were published on the WAT web page.23 We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017. The base line systems are shown in Tables 16, 17, and 18. SMT baseline systems are described in the WAT 2017 overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online transla tion systems were operated by the organizers. We note that these RBMT companies and online trans lation companies did not submit themselves. Be cause our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. Table 13: Statistics for IITB Corpus. “Mono” indicates monolingual Hindi corpus. For the first year, WAT uses BSD Corpus 22 (The Business Scene Dialogue corpus) for the dataset including training"
2020.wat-1.1,Y18-3001,1,0.753826,"raining, develop ment, and test data of the KhmerEnglish transla tion tasks are listed in Table 6. 1 4 http://opus.nlpl.eu/ Lang.pair Ja↔Ru Ja↔En Ru↔En Table 8: task. Partition train development test train development test train development test #sent. 12,356 486 600 47,082 589 600 82,072 313 600 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 21k / 16k 22k / 17k 1.61M / 1.83M 7.8k / 8.4k 15k / 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k 2.7 Indic Multilingual Task In 2018, we had organized an Indic languages task (Nakazawa et al., 2018) but due to lack of reli able evaluation corpora we discontinued it in WAT 2019. However, in 2020, high quality publicly available evaluation (and training) corpora became available which motivated us to relaunch the task. The Indic task involves mixed domain corpora for evaluation consisting of various articles composed by Indian Prime Minister. The languages involved are Hindi (Hi), Marathi (Mr), Tamil (Ta), Telugu (Te), Gujarati (Gu), Malayalam (Ml), Bengali (Bg) and English (En). English is either the source or the target language during evaluation leading to a total of 14 translation dir"
2020.wat-1.1,W12-5611,1,0.853557,"Missing"
2020.wat-1.1,P11-2093,0,0.0137734,"eq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 30 https://github.com/tensorflow/ tensor2tensor https://taku910.github.io/mecab/ 12 (separated by 1000 batches) and performed decod ing with a beam of size 4 and a length penalty of 0.6. 4.2.3 Before the calculation of the automatic evalua tion scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmenta tion, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model33 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.34 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 20140616 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.35 For Korean segmentation, we used mecabko.36 For Myanmar and Khmer segmen tations, we used myseg.py37 and kmseg.py38 . For English and Russian tokenizations, we used tokenizer.perl39 in the Moses toolkit. For Indonesian and Malay tokenizations, we used tokenizer.perl as same as the Engl"
2020.wat-1.1,2020.lrec-1.518,1,0.770784,") which are not publicly available at the time of WAT 2020. Note that phrasetoregion an notation is not included in the test data. There are two settings of submission: with and without resource constraints. In the constrained setting, external resources such as additional data and pretrained models (with external data) are not allowed to use, except for pretrained convo lutional neural networks (for visual analysis) and basic linguistic tools such as taggers, parsers, and morphological analyzers. As the baseline system to compute the Pairwise score, we implement the textonly model in (Nishihara et al., 2020) under the constrained setting. 2.11.1 Documentlevel Scientific Paper Translation Traditional ASPEC translation tasks are sentence level and the translation quality of them seem to be saturated. We think it’s high time to move on to documentlevel evaluation. For the first year, we use ParaNatCom 21 (Parallel EnglishJapanese ab stract corpus made from Nature Communications articles) for the development and test sets of the Documentlevel Scientific Paper Translation sub task. We cannot provide documentlevel training corpus, but you can use ASPEC and any other ex tra resources. 2.11.2 Do"
2020.wat-1.1,P02-1040,0,0.120093,"ase tophrase and phrasetoregion annotations avail able in the training dataset. 5 5.2 Automatic Evaluation System The automatic evaluation system receives transla tion results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each sub mission: Automatic Evaluation 5.1 Procedure for Calculating Automatic Evaluation Score • Human Evaluation: whether or not they sub mit the results for human evaluation; We evaluated translation results by three met rics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.31 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2020 web page.32 All scores for each task were calculated using the corresponding reference translations. 33 http://www.phontron.com/kytea/model.html http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 35 http://nlp.stanford.edu/software/segmenter"
2020.wat-1.1,2020.lrec-1.462,0,0.0440733,"Missing"
2020.wat-1.1,2006.amta-papers.25,0,0.152126,"conducted pairwise evaluation for participants’ systems submitted for human evaluation. The sub mitted translations were evaluated by a profes sional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 4). Additional Automatic Scores in MultiModal and UFAL EnOdia Tasks For the multimodal task and UFAL EnOdia task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (this time calculated by Moses scorer43 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer44 on the can didate and reference before scoring. For all error metrics, i.e. metrics where better scores are lower, we reverse the score by taking 1 − x and indicate this by prepending “n” to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all met ric scores by 100 for better readability. These additional scores document again, that BLEU implementations (and the underlying tok enization schemes) heavily vary in their outcome"
2020.wat-1.1,J82-2005,0,0.692397,"Missing"
2020.wat-1.1,L16-1249,1,0.823126,"pur pose of this task was to test the feasibility of multi domain multilingual solutions for extremely low resource language pairs and domains. Naturally the solutions could be onetomany, manytoone or manytomany NMT models. The domains in question are Wikinews and IT (specifically, Soft ware Documentation). The total number of evalu ation directions are 16 (8 for each domain). There is very little clean and publicly available data for these domains and language pairs and thus we en couraged participants to not only utilize the small Asian Language Treebank (ALT) parallel corpora (Thu et al., 2016) but also the parallel corpora from OPUS1 . The ALT dataset contains 18,088, 1,000 and 1,018 training, development and test ing sentences. As for corpora for the IT domain we only provided evaluation (dev and test sets) 2016), consisting of twenty thousand Khmer English parallel sentences from news articles. • The ECCC corpus consists of 100 thousand KhmerEnglish parallel sentences extracted from document pairs of KhmerEnglish bi lingual records in Extraordinary Chambers in the Court of Cambodia, collected by National Institute of Posts, Telecoms & ICT, Cambo dia. The ALT corpus has been"
2020.wat-1.9,Y18-3013,0,0.018333,"hile we use the multilingual NMT approach proposed by (Johnson et al., 2017) for multilingualism and the derivative multidomain NMT approach by (Chu et al., 2017) we refer readers to (Dabre et al., 2020) and (Chu and Wang, 2018) for overviews on multilingualism and domain-adaptation, respectively. We do not describe the multilingual or multi-domain NMT modeling techniques in this paper as they are the same as described in Johnson et al. (2017) and Chu et al. (2017). Specific to WAT, multilingual multi-domain approaches have been shown to improve translation quality for low-resource languages (Banerjee et al., 2018; Dabre et al., 2018; Philip et al., 2018). 3 Train Dev Test Hi 254,242 2,016 2,073 Language Ms 18,088 158,472 506,739 1,000 2,023 2,050 1,018 2,037 2,050 Id Th 74,497 2,049 2,050 Table 1: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are N-way parallel. part of the IT domain). The languages involved are Thai (Th), Hindi (Hi), Malay (Ms), Indonesian (Id) and English (En). The Indic task involves mixed domain corpora for evaluation (various articles composed by Indian Prime Minister) and involves the lan"
2020.wat-1.9,2020.wat-1.20,0,0.257076,"omain NMT model. The desired models could be one-to-many, many-to-one or many-tomany. English is either the source or the target language for both tasks. 3.2 Datasets We used some corpora from the many listed in the official task descriptions12 . In particular the following parallel corpora were used: NICT-SAP Task: We used parallel corpora from the Asian Language Treebank (ALT) (Thu et al., 2016), KDE, GNOME and Ubuntu. The last three corpora were taken from OPUS3 . Where the ALT corpus is for the ALT domain test set domain4 , the other three are for the IT or Software Documentation domain5 (Buschbeck and Exel, 2020). Detailed statistics for corpora can be found in Table 1. Indic Task: We used the filtered PM India dataset provided by organizers6 and the CVIT-PIB Experiments In this section we describe the tasks, datasets, implementation details, evaluation methodology and actual models trained. 3.1 Domain ALT IT ALT IT ALT IT 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ NICT-SAP-Task/ 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/ indic-multilingual/ 3 http://opus.nlpl.eu/ 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/ NICT-SAP-Task/altsplits-sap-nict.zip 5 Software Domain Evaluation Splits 6 http://lotus.kuee.kyoto-u.ac.jp/W"
2020.wat-1.9,P17-2061,1,0.922577,"how well a many-to-many model would perform in the case of the two sub-tasks we chose. The many-to-many models we trained used the Transformer (Vaswani et al., 2017) architecture using the simple black-box token-prepending technique (Johnson et al., 2017). In essence we simply concatenated the individual parallel corpora while prepending an artificial token such as 2xx where xx indicates the target language. Typically, the smallest corpus in the multilingual dataset is oversampled to match the size of the largest one but we tried settings with and without oversampling. Furthermore, following (Chu et al., 2017) we additionally prepended source sentences with tokens such as 2dom where dom indicates the domain of the corpus. We only did this when we knew that the test (and train) sets would involve multiple domains. An evaluation of our models showed that their performance is not consistent because they sometimes outperform the one-to-many and manyto-one models and sometimes underperform them. Furthermore the performance also depends on the language pair. As a secondary observation, we noticed that when parallel corpora sizes are not too different, oversampling smaller corpora negatively affects the f"
2020.wat-1.9,C18-1111,0,0.0205912,"s of the predecessor of NMT aka statistical machine translation (SMT) (Koehn et al., 2007) which involved error compounding as the input sentences were processed by multiple components that were prone to making mistakes. Another advantage of NMT is that its inner working is non-symbolic which enables it to incorporate multiple languages without any need to modify the basic architecture. While we use the multilingual NMT approach proposed by (Johnson et al., 2017) for multilingualism and the derivative multidomain NMT approach by (Chu et al., 2017) we refer readers to (Dabre et al., 2020) and (Chu and Wang, 2018) for overviews on multilingualism and domain-adaptation, respectively. We do not describe the multilingual or multi-domain NMT modeling techniques in this paper as they are the same as described in Johnson et al. (2017) and Chu et al. (2017). Specific to WAT, multilingual multi-domain approaches have been shown to improve translation quality for low-resource languages (Banerjee et al., 2018; Dabre et al., 2018; Philip et al., 2018). 3 Train Dev Test Hi 254,242 2,016 2,073 Language Ms 18,088 158,472 506,739 1,000 2,023 2,050 1,018 2,037 2,050 Id Th 74,497 2,049 2,050 Table 1: The NICT-SAP task"
2020.wat-1.9,2020.coling-tutorials.3,1,0.871368,"a does not necessarily give the best translation quality for the language pair associated with that pair. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) is an end-to-end machine translation (MT) modeling approach that is known to give state-of-the-art translations for a variety of language pairs. Although, it is known to work particularly well for language pairs with an abundance of parallel corpora it tends to perform rather poorly for language pairs that lack large parallel corpora. Fortunately, multilingual neural machine translation (MNMT) (Dabre et al., 2020) methods can be applied in order to significantly improve the translation quality for such language pairs. The underlying reason for improvement is that sharing parameters among several language pairs enables transfer learning which is proven to improve translation quality regardless of the language pair. For the 7th Workshop on Asian Translation (WAT, 2020), our team (NICT-5) decided to focus on the Indic languages task and the NICT-SAP 98 Proceedings of the 7th Workshop on Asian Translation, pages 98–102 c December 4, 2020. 2020 Association for Computational Linguistics 2 Related Work Split"
2020.wat-1.9,Y18-3003,1,0.723584,"ngual NMT approach proposed by (Johnson et al., 2017) for multilingualism and the derivative multidomain NMT approach by (Chu et al., 2017) we refer readers to (Dabre et al., 2020) and (Chu and Wang, 2018) for overviews on multilingualism and domain-adaptation, respectively. We do not describe the multilingual or multi-domain NMT modeling techniques in this paper as they are the same as described in Johnson et al. (2017) and Chu et al. (2017). Specific to WAT, multilingual multi-domain approaches have been shown to improve translation quality for low-resource languages (Banerjee et al., 2018; Dabre et al., 2018; Philip et al., 2018). 3 Train Dev Test Hi 254,242 2,016 2,073 Language Ms 18,088 158,472 506,739 1,000 2,023 2,050 1,018 2,037 2,050 Id Th 74,497 2,049 2,050 Table 1: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are N-way parallel. part of the IT domain). The languages involved are Thai (Th), Hindi (Hi), Malay (Ms), Indonesian (Id) and English (En). The Indic task involves mixed domain corpora for evaluation (various articles composed by Indian Prime Minister) and involves the languages Hindi (Hi), M"
2020.wat-1.9,P07-2045,0,0.0251681,"s used in research as well as engineering applications. While the initial architectures used recurrent neural networks, the more recent architectures use self-attention and feed-forward networks (Vaswani et al., 2017) which enable faster training and decoding. The main advantage of NMT is that the translation models are small (can fit on low-memory and low-computation devices) and the training approach is end-to-end (rather than modular). Large translation models and modular (multiple components) design were the key features of the predecessor of NMT aka statistical machine translation (SMT) (Koehn et al., 2007) which involved error compounding as the input sentences were processed by multiple components that were prone to making mistakes. Another advantage of NMT is that its inner working is non-symbolic which enables it to incorporate multiple languages without any need to modify the basic architecture. While we use the multilingual NMT approach proposed by (Johnson et al., 2017) for multilingualism and the derivative multidomain NMT approach by (Chu et al., 2017) we refer readers to (Dabre et al., 2020) and (Chu and Wang, 2018) for overviews on multilingualism and domain-adaptation, respectively."
2020.wat-1.9,P02-1040,0,0.107094,"nerated where xx is one of en, bg, hi, mr, ml, ta, te, gu, th, ms, id as applicable. Additionally for the NICT-SAP task, we prepend the source sentences with a token like 2it or 2alt to distinguish between the IT and Wikinews domains. As for the models trained, we trained transformer big models on single Tesla V100 GPUs using the hyperparameter settings corresponding to “transformer big single gpu”. Some important hyperparameters are: 6-layer encoder and decoder models with 16 attention heads, 1024-4096 hidden-filter sizes. We trained the models till convergence on development set BLEU score (Papineni et al., 2002). The development set BLEU score is the average of the BLEU scores of individual language pairs pairs. Evaluation on development set is done every 1000 batches (of 2048 tokens) and training stops when the BLEU score does not improve for 10 consecutive evaluations. Before evaluation, the model parameters are saved as a checkpoint and the last 10 checkpoints are averaged to give a single model which is then decoded using a beam of size 4 and a length penalty of 0.69 . We trained a total of 4 models, 2 models per task; one with oversampling the smaller corpora to match the size of the largest cor"
2020.wat-1.9,P16-1009,0,0.0342666,"st of our translations were unable to beat those of the organizers’. However, there might be a simple explanation for this. Note that the Indic task involves almost twice as many translation directions as the NICT-SAP task. We used big transformer models for both tasks and so, the problem is not representation capacity but rather a lack of it. This lack of capacity likely comes from our naive multilingual solution coupled with rather small parallel corpora for each language pair. Future work will focus on expanding the amount of parallel corpora via popular techniques such as backtranslation (Sennrich et al., 2016). Another observation, just as in several cases of the ALT domain translations of the NICT-SAP task, we see that using oversampling is detrimental to translation quality. 5 have mixed performance where they sometimes outperform one-to-many or many-to-one models (organzier’s models) and sometimes do not. Furthermore we observed that our models trained without oversampling smaller corpora tended to perform better than their counterparts that used oversampling of smaller corpora. This is especially true when parallel corpora for those pairs/domains contain approximately 100,000 sentences or fewer"
2020.wat-1.9,L16-1249,0,0.123419,"and involves the languages Hindi (Hi), Marathi (Mr), Tamil (Ta), Telugu (Te), Gujarati (Gu), Malayalam (Ml), Bengali (Bg) and English (En). For both tasks, the objective was to train a single multilingual and multi-domain NMT model. The desired models could be one-to-many, many-to-one or many-tomany. English is either the source or the target language for both tasks. 3.2 Datasets We used some corpora from the many listed in the official task descriptions12 . In particular the following parallel corpora were used: NICT-SAP Task: We used parallel corpora from the Asian Language Treebank (ALT) (Thu et al., 2016), KDE, GNOME and Ubuntu. The last three corpora were taken from OPUS3 . Where the ALT corpus is for the ALT domain test set domain4 , the other three are for the IT or Software Documentation domain5 (Buschbeck and Exel, 2020). Detailed statistics for corpora can be found in Table 1. Indic Task: We used the filtered PM India dataset provided by organizers6 and the CVIT-PIB Experiments In this section we describe the tasks, datasets, implementation details, evaluation methodology and actual models trained. 3.1 Domain ALT IT ALT IT ALT IT 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ NICT-SAP-Task/ 2 ht"
2020.wmt-1.61,P17-2061,1,0.934704,"nly on SD. However, its impact on ELR settings is uncertain. Given that only few thousands of domain-specific sentences are available, training large NMT models tends to over-fit on the small corpora while compact NMT models will only lead to pseudo-targets of poor quality, both preventing the generation of useful distilled corpora. It is certainly possible to search for an optimal model size. However, it will involve a time-consuming hyper-parameter search, while the result may be specific to given corpora. 2.2 Transfer Learning Transfer learning (TL) can be in the form of domain adaptation (Chu et al., 2017) or cross-lingual or multilingual transfer (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019; Johnson et al., 2017; Dabre et al., 2020) using helping bilingual corpora. Assume that L1 –L2 is an ELR language pair and L3 –L4 is a helping pair. The given parallel corpora 493 for the two pairs may belong to different domains. Typically, pre-training a model on the larger L3 –L4 corpus and then fine-tuning (“ft”) it on the smaller L1 –L2 corpus is known to give the best translation quality for the L1 –L2 pair (Zoph et al., 2016; Chu et al., 2017; Dabre et al., 2019), regardless of the numb"
2020.wmt-1.61,D19-1146,1,0.843886,"-specific sentences are available, training large NMT models tends to over-fit on the small corpora while compact NMT models will only lead to pseudo-targets of poor quality, both preventing the generation of useful distilled corpora. It is certainly possible to search for an optimal model size. However, it will involve a time-consuming hyper-parameter search, while the result may be specific to given corpora. 2.2 Transfer Learning Transfer learning (TL) can be in the form of domain adaptation (Chu et al., 2017) or cross-lingual or multilingual transfer (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019; Johnson et al., 2017; Dabre et al., 2020) using helping bilingual corpora. Assume that L1 –L2 is an ELR language pair and L3 –L4 is a helping pair. The given parallel corpora 493 for the two pairs may belong to different domains. Typically, pre-training a model on the larger L3 –L4 corpus and then fine-tuning (“ft”) it on the smaller L1 –L2 corpus is known to give the best translation quality for the L1 –L2 pair (Zoph et al., 2016; Chu et al., 2017; Dabre et al., 2019), regardless of the number of model parameters. However, without careful regularization, this will definitely lead to the L1"
2020.wmt-1.61,N19-1423,0,0.017733,"ts size matches to the L3 –L4 corpus. Also, we can prepend the source sentences with two artificial tokens, one indicating the domain of the corpus (Chu et al., 2017), and another indicating the target language into which we want to translate (Johnson et al., 2017). Note that when L2 and L4 are the same, the target language tokens are unnecessary. If L1 and L3 are also the same, then we are essentially performing domain adaptation. translation (Sennrich et al., 2016), where target language monolingual data are translated into pseudosource sentences. Recently, pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020) has been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we consider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on"
2020.wmt-1.61,2020.ngt-1.12,0,0.0224982,"ation (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our proposed method. 2.3 3 Mixed Training (“mxt”): Directly train on the concatenated corpus. Mixed Fine-Tuning (“mxft”): First train on the L3 –L4 corpus as in “ft,” but perform finetuning on the concatenated corpus. Other Related Work Some recent work tackled efficient NMT modeling in low-resource settings (Goyal et al., 2020; Gordon and Duh, 2020). Whereas they focus on applications of TL for compact models as this paper, there are some key differences between them and ours. Gordon and Duh (2020) focus on lowresource settings, but our low-resource data are significantly smaller than theirs. Second, whereas they use distillation twice and TL once, we recommend distillation once and TL twice. Finally, they do not examine cross-lingual TL for model compression. Goyal et al. (2020) focus on cross-lingual learning, but their approaches are centered more on leveraging orthographic or linguistic similarity, whereas we make no efforts towards"
2020.wmt-1.61,2020.acl-srw.22,0,0.127717,"essive model binarization (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our proposed method. 2.3 3 Mixed Training (“mxt”): Directly train on the concatenated corpus. Mixed Fine-Tuning (“mxft”): First train on the L3 –L4 corpus as in “ft,” but perform finetuning on the concatenated corpus. Other Related Work Some recent work tackled efficient NMT modeling in low-resource settings (Goyal et al., 2020; Gordon and Duh, 2020). Whereas they focus on applications of TL for compact models as this paper, there are some key differences between them and ours. Gordon and Duh (2020) focus on lowresource settings, but our low-resource data are significantly smaller than theirs. Second, whereas they use distillation twice and TL once, we recommend distillation once and TL twice. Finally, they do not examine cross-lingual TL for model compression. Goyal et al. (2020) focus on cross-lingual learning, but their approaches are centered more on leveraging orthographic or linguistic similarity, whereas we m"
2020.wmt-1.61,D16-1139,0,0.342272,"y but needs more computation and is unacceptable in a lowlatency real-time application where faster decoding is as valuable as if not more valuable than translation quality. Consequently, neural models that are compact and fast are extremely important and a growing body of research known as neural model efficiency focuses on this issue. One of the most popular techniques to train efficient models is knowledge distillation (Hinton et al., 2015) which relies on transferring the knowledge learned by a large model (called teacher) into a smaller model (called student). Sequence distillation (SD) (Kim and Rush, 2016) is a special case of knowledge distillation for sequence-to-sequence models, such as those used for NMT. Not only does it help in the training of compact and fast models with high translation quality, it sometimes helps in eliminating the need for beam search which further increases decoding speed. SD relies on the creation of distilled parallel corpora by translating the training source sentences into the target language by using a large model. The distilled corpora are simplified representations of how the large model sees the original corpora and their quality will have a direct impact on"
2020.wmt-1.61,L18-1548,0,0.0295442,"of different TL settings on the quality of distilled ELR corpora and hence the compact models trained. #2. With ELR and helping corpora: To determine the settings using both ELR and helping corpora that give compact models with highest possible translation quality. 4.1 Datasets We experimented with the ELR Vietnamese– English (Vi–En) and Hindi–English (Hi–E) pairs from the Asian Languages Treebank (ALT) with 18,088 training, 1,000 development, and 1,018 test sentence pairs. As for the helping corpora, we used the training part of the IWSLT 2015 Vietnamese– English3 and the IITB Hindi–English (Kunchukuttan et al., 2018),4 consisting of 133k and 1.5M lines, respectively. We chose large as well as small helping corpora in order to determine the impact of helping corpora sizes on the model training. 4.2 Implementation Details We used the Transformer model for our experiments (Vaswani et al., 2017) because it gives the state-of-the-art results for NMT. We made necessary changes to the code in the tensor2tensor v1.14 implementation of the Transformer in order to construct joint sub-word vocabularies as well as to handle oversampling. Tensor2tensor has its own default sub-word vocabulary learning method which we u"
2020.wmt-1.61,2020.lrec-1.454,1,0.826073,"so, we can prepend the source sentences with two artificial tokens, one indicating the domain of the corpus (Chu et al., 2017), and another indicating the target language into which we want to translate (Johnson et al., 2017). Note that when L2 and L4 are the same, the target language tokens are unnecessary. If L1 and L3 are also the same, then we are essentially performing domain adaptation. translation (Sennrich et al., 2016), where target language monolingual data are translated into pseudosource sentences. Recently, pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020) has been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we consider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on distillation using unsupervised method"
2020.wmt-1.61,P17-1079,0,0.0217168,"sider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on distillation using unsupervised methods (Sun et al., 2020). Independent of the application of TL, there exist methods for speeding up NMT, such as weight pruning (See et al., 2016) where model weights close to zero are pruned out, quantization (Lin et al., 2016) where weights are represented by faster to process integers instead of floating point numbers, aggressive model binarization (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our proposed method. 2.3 3 Mixed Training (“mxt”): Directly train on the concatenated corpus. Mixed Fine-Tuning (“mxft”): First train on the L3 –L4 corpus as in “ft,” but perform finetuning on the concatenated corpus. Other Related Work Some recent work tackled efficient NMT modeling in low-resource settings (Goyal et al., 2020; Gordon and Duh, 2020). Whereas they focus on applications of TL for compact models as t"
2020.wmt-1.61,P02-1040,0,0.10643,"ich further increases decoding speed. SD relies on the creation of distilled parallel corpora by translating the training source sentences into the target language by using a large model. The distilled corpora are simplified representations of how the large model sees the original corpora and their quality will have a direct impact on the translation quality of compact models trained with them. While SD is known to perform extremely well for high-resource settings, its direct application to extremely low-resource (ELR) settings will not work due to over-fitting. Table 1 gives the BLEU scores (Papineni et al., 2002) for Vietnamese– English (Vi–En) and Hindi–English (Hi–En) translation tasks in the Asian Languages Treebank (ALT) (Riza et al., 2016),1 where Transformer Base models (Vaswani et al., 2017) with 1, 2, 3, and 6 encoder and decoder layers were trained on the ALT training data of 18k sentence pairs. It is clear that there is a huge performance gap between the 1 http://www2.nict.go.jp/astrec-att/member/mutiyama/ ALT/ALT-Parallel-Corpus-20191206.zip 492 Proceedings of the 5th Conference on Machine Translation (WMT), pages 492–502 c Online, November 19–20, 2020. 2020 Association for Computational Li"
2020.wmt-1.61,K16-1029,0,0.0536086,"Missing"
2020.wmt-1.61,P16-1009,0,0.0423866,"ng the following two methods on both corpora has been studied: Prior to concatenating two corpora, the L1 –L2 corpus is typically oversampled so that its size matches to the L3 –L4 corpus. Also, we can prepend the source sentences with two artificial tokens, one indicating the domain of the corpus (Chu et al., 2017), and another indicating the target language into which we want to translate (Johnson et al., 2017). Note that when L2 and L4 are the same, the target language tokens are unnecessary. If L1 and L3 are also the same, then we are essentially performing domain adaptation. translation (Sennrich et al., 2016), where target language monolingual data are translated into pseudosource sentences. Recently, pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020) has been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we con"
2020.wmt-1.61,2020.acl-srw.37,1,0.833832,"our distilled corpora can be used in situations where quick deployment of compact and fast NMT models is important. 5.1.2 Domain Adaptation vs. Cross-Lingual Transfer Our experiment revealed that cross-lingual training is definitely a viable alternative. For instance, in Vi→En translation, the best BLEU score was achieved when the helping direction was also Vi→En. When the helping direction was Hi→En, these improvements were much smaller. Nevertheless, it is clear that cross-lingual training is useful when domain adaptation is not possible. Work on script mapping to improve the quality of TL (Song et al., 2020; Goyal et al., 2020) indicates that our cross-lingual distillation procedure might give better results if we mapped Hi to Vi or vice-versa. We leave this for future work. Consider two hypothetical settings for Vi→En translation, where we used the reversed, En→Vi and En→Hi, helping directions to generate distilled corpora for Vi→En translation. When using En→Vi as the helping direction, the BLEU scores of greedy search with 1-, 2-, and 3-layer models improved by 4.3, 4.4, and 5.6 BLEU points, respectively. These improvements are approximately 1.0 BLEU points lower than those obtained in the do"
2020.wmt-1.61,2020.acl-main.324,0,0.0244345,"s been proven to significantly improve the translation quality of ELR settings. Approaches involving helping monolingual data are usually more time-consuming than those that use helping bilingual corpora. Furthermore, given that our approach already needs a reasonable amount of time due to the application of TL and forwardtranslation of the source sentences of the parallel corpora for distilling them, we consider that such approaches should be used when no more gains can be obtained from helping bilingual corpora. We refer interested readers to work on distillation using unsupervised methods (Sun et al., 2020). Independent of the application of TL, there exist methods for speeding up NMT, such as weight pruning (See et al., 2016) where model weights close to zero are pruned out, quantization (Lin et al., 2016) where weights are represented by faster to process integers instead of floating point numbers, aggressive model binarization (Courbariaux et al., 2017), and binary code prediction softmax (Oda et al., 2017) where the softmax is sped up by making it predict a binary code representing words instead of one-hot vectors. We expect these methods to further speed up the models obtained using our pro"
2020.wmt-1.61,D16-1163,0,0.0166132,"thousands of domain-specific sentences are available, training large NMT models tends to over-fit on the small corpora while compact NMT models will only lead to pseudo-targets of poor quality, both preventing the generation of useful distilled corpora. It is certainly possible to search for an optimal model size. However, it will involve a time-consuming hyper-parameter search, while the result may be specific to given corpora. 2.2 Transfer Learning Transfer learning (TL) can be in the form of domain adaptation (Chu et al., 2017) or cross-lingual or multilingual transfer (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019; Johnson et al., 2017; Dabre et al., 2020) using helping bilingual corpora. Assume that L1 –L2 is an ELR language pair and L3 –L4 is a helping pair. The given parallel corpora 493 for the two pairs may belong to different domains. Typically, pre-training a model on the larger L3 –L4 corpus and then fine-tuning (“ft”) it on the smaller L1 –L2 corpus is known to give the best translation quality for the L1 –L2 pair (Zoph et al., 2016; Chu et al., 2017; Dabre et al., 2019), regardless of the number of model parameters. However, without careful regularization, this will defini"
2021.mtsummit-research.10,D19-1146,1,0.842567,"., 2017) seeks to mitigate overconfident predictions but is not known to work well Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di ) represents the softmax function produci"
2021.mtsummit-research.10,N19-1423,0,0.0320029,"not known to work well Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di ) represents the softmax function producing the probability distribution, where X and Y&lt;i indicate t"
2021.mtsummit-research.10,P17-2012,0,0.0209806,"r this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand, model regularization techniques place constraints on the learning of model parameters in order to aid the model to learn robust representations that positively impact model performance. Among existing model regularization methods, dropout (Srivastava et al., 2014) is most commonly used and is known to be effective regardless of the size of data. Label smoothing (Szegedy Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 114 et al., 2016) is another effective approach that uses smoothed label vecto"
2021.mtsummit-research.10,N16-1101,0,0.0323529,"max entropy maximization (Pereyra et al., 2017) seeks to mitigate overconfident predictions but is not known to work well Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di )"
2021.mtsummit-research.10,W17-3204,0,0.0278434,"known to give state-of-the-art results for a large variety of language pairs. NMT for high-resource language pairs is straightforward: choose an NMT architecture and implementation, and train a model on all existing data by minimizing the softmax cross-entropy loss, i.e., cross-entropy between the softmax distribution and the label distribution typically represented with a one-hot vector. In contrast, for low-resource language pairs, this does not work well due to the inability of neural networks to generalize from small amounts of data. One reason for this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand, model regularization tec"
2021.mtsummit-research.10,2020.lrec-1.454,1,0.83534,"he 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 115 for NMT. Our method is intended to complement these techniques, i.e., label smoothing and softmax entropy maximization, and not necessarily replace them. Existing methods effective for low-resource language pairs include data augmentation via back-translating additional monolingual data (Sennrich et al., 2016), exploitation of multilingualism (Firat et al., 2016; Zoph et al., 2016; Dabre et al., 2019), and pre-training on monolingual data (Devlin et al., 2019; Song et al., 2019; Mao et al., 2020). These require more training time and resources, while ours does not. 3 Softmax Tempering Softmax tempering (Hinton et al., 2015) consists of two tiny changes in the implementation of the training phase of any neural model used for classification. Assume that Di ∈ RV is the logit output of the decoder for the i-th word prediction in the target language sentence, Yi , where V stands for the target vocabulary size, and that Pi = P (Yi |Y&lt;i , X) = softmax (Di ) represents the softmax function producing the probability distribution, where X and Y&lt;i indicate the given source sentence and the past"
2021.mtsummit-research.10,D17-1156,0,0.0145084,"max tempering is also used in model calibration (Guo et al., 2017; Kumar and Sarawagi, 2019), where the temperature coefficient is optimized on the development set in order to penalize overconfident predictions, which is a common practice in low-resource settings. While model calibration is performed after a model is trained, we use softmax tempering during training. We regard softmax tempering as a regularization technique, since it adds noise to NMT model training. Thus, it is related to techniques, such as LN regularization (Ng, 2004), dropout (Srivastava et al., 2014), and tuneout (Miceli Barone et al., 2017). The most important aspect of our method is that it is only applied at the softmax layer whereas other regularization techniques add noise to several parts of the entire model. Label smoothing (Szegedy et al., 2016), which is known to help low-resource NMT (Sennrich and Zhang, 2019), is highly related to our idea, where the key difference is that label smoothing affects the label distributions whereas softmax tempering affects the softmax distributions. On a related note, softmax entropy maximization (Pereyra et al., 2017) seeks to mitigate overconfident predictions but is not known to work w"
2021.mtsummit-research.10,P02-1040,0,0.10997,"or v1.14.4 For “Transformer Base” and “Transformer Big” models, we used the hyper-parameter settings in transformer base single gpu and transformer big single gpu, respectively. Label smoothing of 0.1 was used. We used the internal sub-word tokenization mechanism of tensor2tensor with separate source and target language vocabularies of size 8,192 and 32,768 for low-resource and high-resource settings, respectively. We trained our models for each of the softmax temperature values, 1.0 (default softmax), 1.2, 1.4, 1.6, 1.8, 2.0, 3.0, 4.0, 5.0, and 10.0. We used early-stopping on the BLEU score (Papineni et al., 2002) for the development set which was evaluated every 1k iterations. Our early-stopping mechanism halts training when the BLEU score does not improve over 10 consecutive evaluation steps. For decoding, we averaged the final 10 checkpoints, and evaluated beam search and greedy search. Note that the training time temperature coefficient was used during decoding as well. If this is not done then the softmax distributions will be extremely sharp and beam search will collapse to greedy search. 4.3 Evaluation Criteria We evaluated translation quality of each model using BLEU (Papineni et al., 2002) pro"
2021.mtsummit-research.10,W18-6319,0,0.0218523,"set which was evaluated every 1k iterations. Our early-stopping mechanism halts training when the BLEU score does not improve over 10 consecutive evaluation steps. For decoding, we averaged the final 10 checkpoints, and evaluated beam search and greedy search. Note that the training time temperature coefficient was used during decoding as well. If this is not done then the softmax distributions will be extremely sharp and beam search will collapse to greedy search. 4.3 Evaluation Criteria We evaluated translation quality of each model using BLEU (Papineni et al., 2002) provided by SacreBLEU (Post, 2018).5 The optimal temperature (Topt ) for the tempered model was determined based on greedy-search BLEU score on the development set, given that beam- and greedy-search score improvements are almost always correlated. We therefore used these optimal temperature models to perform beam search, where the beam width (among 2, 4, 6, 8, 10, and 12) and length penalty (among 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, and 1.4) were tuned on the development set. We performed statistical significance testing6 to determine if differences in BLEU are significant. 4.4 Results in Low-Resource Settings Table 1 sho"
2021.mtsummit-research.10,P16-1009,0,0.11837,"ralize from small amounts of data. One reason for this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand, model regularization techniques place constraints on the learning of model parameters in order to aid the model to learn robust representations that positively impact model performance. Among existing model regularization methods, dropout (Srivastava et al., 2014) is most commonly used and is known to be effective regardless of the size of data. Label smoothing (Szegedy Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 114 et al., 2016) is another e"
2021.mtsummit-research.10,P19-1021,0,0.215095,"learn robust representations that positively impact model performance. Among existing model regularization methods, dropout (Srivastava et al., 2014) is most commonly used and is known to be effective regardless of the size of data. Label smoothing (Szegedy Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 114 et al., 2016) is another effective approach that uses smoothed label vectors as opposed to onehot label vectors. Previous work on NMT has shown that label smoothing is very effective in low-resource settings (Sennrich and Zhang, 2019) and we believe that this deserves further study. We thus focus on a technique that does not need additional data and can complement dropout and label smoothing in an extremely low-resource situation. In this paper, we propose to apply softmax tempering (Hinton et al., 2015) to the training of NMT models. Softmax tempering is realized by dividing the pre-softmax logits with a positive real number greater than 1.0. This leads to a smoother softmax probability distribution, which is then used to compute the cross-entropy loss. Softmax tempering has been devised and used regularly in knowledge di"
2021.mtsummit-research.10,D16-1163,0,0.0605124,"tion models and is known to give state-of-the-art results for a large variety of language pairs. NMT for high-resource language pairs is straightforward: choose an NMT architecture and implementation, and train a model on all existing data by minimizing the softmax cross-entropy loss, i.e., cross-entropy between the softmax distribution and the label distribution typically represented with a one-hot vector. In contrast, for low-resource language pairs, this does not work well due to the inability of neural networks to generalize from small amounts of data. One reason for this is over-fitting (Zoph et al., 2016; Koehn and Knowles, 2017), where the softmax distribution (sparse vector) ends up resembling the label distribution (one-hot vector). There are several solutions that address this issue, of which the two most effective ones are transfer learning and model regularization. Transfer learning can sometimes be considered as data regularization and comes in the form of monolingual or cross-lingual (multilingual) fashion (Zoph et al., 2016; Song et al., 2019), pseudo-parallel data generation (back-translation) (Sennrich et al., 2016), or multi-task learning (Eriguchi et al., 2017). On the other hand"
2021.mtsummit-research.17,N18-1118,0,0.0179542,"d translations (Xiong et al., 2019; Voita et al., 2019). However, post-processing generated translations may Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 203 lead to higher latency, which is counter-intuitive in a simultaneous translation scenario. On the other hand, context-aware models leverage additional context during translation. For example, Tiedemann and Scherrer (2017) proposed to simply concatenate the previous sentences in both the source and target side to the input to the system. Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use separate context encoder for a few previous source sentences. Similarly, we also use a separate context encoder to extract document-level information. However, we incorporate document-level information into SNMT in order to improve translation quality, where only information from the source sentence is insufﬁcient during translation. 3 Methods 3.1 Background: Wait-k Simultaneous NMT The most straightforward approach for SNMT is the wait-k approach (Ma et al., 2019) with a ﬁxed policy. As tokens are fed to the encoder one at a time, we have to rely on a unidirectional"
2021.mtsummit-research.17,2020.emnlp-main.184,0,0.0302809,"he wait-k SNMT model, Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 202 the standard encoder and decoder are replaced with their SNMT equivalents, which are a unidirectional encoder and a modiﬁed decoder, respectively. The decoder can only look at i + k − 1 encoder tokens when predicting the ith token. We are aware of a previous work that has shown that using an image as an additional modality can help improve translation quality in a wait-k setting when k is a small value around 1 to 4 (Imankulova et al., 2020; Caglayan et al., 2020). The additional image modality provides the model with a form of context which helps disambiguate hard-to-translate phenomena, especially when needed information is not available yet during translation. An additional image modality may not always be available, and thus, taking advantage of the context in the form of previously seen sentences is the only viable option. Research in document-level NMT has already proven that context from neighbouring sentences can help enhance representations and thereby improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2"
2021.mtsummit-research.17,2021.acl-long.505,0,0.0266501,"(2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens are available. However, their model for simultaneous translation relies only on the source sentence. This research concentrates on the wait-k approach leveraging document-level information from previous context sentences. Document-level NMT leverages context beyond the current sentence in order to improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Voita et al., 2018, 2019; Zheng et al., 2020b; Fernandes et al., 2021). Document-level NMT models can be implemented as a post-processing model or context-aware model. The post-processing models use an additional module to use context on generated translations (Xiong et al., 2019; Voita et al., 2019). However, post-processing generated translations may Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 203 lead to higher latency, which is counter-intuitive in a simultaneous translation scenario. On the other hand, context-aware models leverage additional context during translation. For"
2021.mtsummit-research.17,D14-1140,0,0.0647058,"Missing"
2021.mtsummit-research.17,E17-1099,0,0.0137945,"le to utilize context effectively and sometimes suffer due to the provided context. This opens up the possibility of research into better mechanisms for leveraging context more effectively. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. Mainly, SNMT can mostly be implemented with ﬁxed or adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to READ another source word or WRITE a target word in one model (Grissom II et al., 2014; Matsubara et al., 2000; Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Zheng et al., 2019a,c, 2020a) focus on mechanisms that determine the optimal number of source language tokens to wait for before generating the next target language token. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens are available. However, their model for simultaneous translation relies only on the source sentence. This research concentrates on the wait-k approach leveraging document-level information from previous context sentences. Documen"
2021.mtsummit-research.17,2020.wmt-1.70,1,0.837371,"However, in the case of the wait-k SNMT model, Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 202 the standard encoder and decoder are replaced with their SNMT equivalents, which are a unidirectional encoder and a modiﬁed decoder, respectively. The decoder can only look at i + k − 1 encoder tokens when predicting the ith token. We are aware of a previous work that has shown that using an image as an additional modality can help improve translation quality in a wait-k setting when k is a small value around 1 to 4 (Imankulova et al., 2020; Caglayan et al., 2020). The additional image modality provides the model with a form of context which helps disambiguate hard-to-translate phenomena, especially when needed information is not available yet during translation. An additional image modality may not always be available, and thus, taking advantage of the context in the form of previously seen sentences is the only viable option. Research in document-level NMT has already proven that context from neighbouring sentences can help enhance representations and thereby improve translation quality (Tiedemann and Scherrer, 2017; Jean et a"
2021.mtsummit-research.17,D16-1139,0,0.0289766,"a weighted representation, the attention, A. Given the context encoding Ec (Xc ) we additionally compute the context attention Ac . We combine A and Ac into Acomb , the context augmented attention, using a simple gating mechanism as Acomb = α ∗ A + (1 − α) ∗ Ac where α = sigmoid(Wcomb ∗ [A : Ac ]). [:] indicates concatenation of representations along the hidden layer axis. Wcomb is the weight matrix of size 1 This means that the memory requirements will increase, but we believe that this is an acceptable trade-off if translation quality improves. Furthermore, we can use sequence distillation Kim and Rush (2016) to compress these models, which have a smaller memory footprint Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 204 Figure 1: A simpliﬁed overview of our simultaneous document level NMT model which uses previous source sentences as context. [2h, h] where h is the model’s hidden size. α is a weight that can help interpolate A and Ac to determine the balance between them. 3.2.2 CA: Context Attention Based Context Incorporation This method is same as the one in Voita et al. (2018). Where the multi-source approach inv"
2021.mtsummit-research.17,W04-3250,0,0.0451252,"CA. 5.1.3 Non-contextual versus Contextual Full-Sentence models Next, when context sentences are used for full sentence translation for Russian→English, the quality for when up to 1, 2, and 3 previous sentences as context are used is 35.2, 35.5, and 35.7, respectively. Compared to a baseline score of 34.9, the improvements are 0.3, 0.6, and 0.8 BLEU. Similarly, for English→Russian, compared to a baseline score of 26.7, using up to 1, 2, and 3 previous sentences as context lead to translation quality improvements of 0.3, 0.5, and 0.5, respectively. We performed statistical signiﬁcance testing (Koehn, 2004) which showed that all improvements are signiﬁcant5 at p &lt; 0.05. This shows that context certainly helps in a spoken language domain, and as the number of context sentences grows, the translation quality also grows steadily. 5.1.4 Non-contextual versus Contextual SNMT models Comparing the wait-k non-contextual model against contextual models using up to N context sentences shows that, once again, context is helpful in an SNMT setting. When using up to 3 context sentences, for wait-k values of 1, 2, 4, 6 and 8, the BLEU score improvements over their non-contextual counterparts are 0.6, 0.5, 0.8"
2021.mtsummit-research.17,W18-6326,0,0.0443223,"Missing"
2021.mtsummit-research.17,P19-1289,0,0.210683,"translation models are expected to present translated words or phrases as they are generated. Furthermore, waiting for the entire source language sentence adds to the latency, and therefore an optimal solution is to have a model that can start generating target language words right after the ﬁrst few source language words are available for translation. This is known as simultaneous NMT (SNMT) and is known for its poor translation quality, especially in lowresource settings. The concept of waiting for k words or tokens before generating target language words or tokens is known as wait-k SNMT (Ma et al., 2019). In this paper, we work with the Transformer architecture as the standard NMT model, consisting of a bidirectional encoder and unidirectional decoder. The decoder is able to attend to all source language tokens when generating target language tokens. However, in the case of the wait-k SNMT model, Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 202 the standard encoder and decoder are replaced with their SNMT equivalents, which are a unidirectional encoder and a modiﬁed decoder, respectively. The decoder can only l"
2021.mtsummit-research.17,P15-1020,0,0.0194136,"while SNMT models deﬁnitely beneﬁt from context, they are unable to utilize context effectively and sometimes suffer due to the provided context. This opens up the possibility of research into better mechanisms for leveraging context more effectively. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. Mainly, SNMT can mostly be implemented with ﬁxed or adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to READ another source word or WRITE a target word in one model (Grissom II et al., 2014; Matsubara et al., 2000; Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Zheng et al., 2019a,c, 2020a) focus on mechanisms that determine the optimal number of source language tokens to wait for before generating the next target language token. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens are available. However, their model for simultaneous translation relies only on the source sentence. This research concentrates on the wait-k approach leveraging docume"
2021.mtsummit-research.17,W17-4811,0,0.111975,"around 1 to 4 (Imankulova et al., 2020; Caglayan et al., 2020). The additional image modality provides the model with a form of context which helps disambiguate hard-to-translate phenomena, especially when needed information is not available yet during translation. An additional image modality may not always be available, and thus, taking advantage of the context in the form of previously seen sentences is the only viable option. Research in document-level NMT has already proven that context from neighbouring sentences can help enhance representations and thereby improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017). The simplest documentlevel NMT architecture involves using an additional encoder that encodes the context sentences, following which the encoded context is used to augment the representation of the sentence to be translated (Zhang et al., 2018). Just like using an image as a modality helps enrich the encoding of the sentence with additional disambiguation information, the context sentences might also contain such useful information. We already know that in an SNMT setting, due to partial sentences being translated, the amount of context available to the"
2021.mtsummit-research.17,D19-1081,0,0.0170588,"n the source sentence. This research concentrates on the wait-k approach leveraging document-level information from previous context sentences. Document-level NMT leverages context beyond the current sentence in order to improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Voita et al., 2018, 2019; Zheng et al., 2020b; Fernandes et al., 2021). Document-level NMT models can be implemented as a post-processing model or context-aware model. The post-processing models use an additional module to use context on generated translations (Xiong et al., 2019; Voita et al., 2019). However, post-processing generated translations may Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 203 lead to higher latency, which is counter-intuitive in a simultaneous translation scenario. On the other hand, context-aware models leverage additional context during translation. For example, Tiedemann and Scherrer (2017) proposed to simply concatenate the previous sentences in both the source and target side to the input to the system. Jean et al. (2017); Bawden et al. (2018); Zhang et al. (2018) use separate"
2021.mtsummit-research.17,P18-1117,0,0.181478,"ext target language token. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens are available. However, their model for simultaneous translation relies only on the source sentence. This research concentrates on the wait-k approach leveraging document-level information from previous context sentences. Document-level NMT leverages context beyond the current sentence in order to improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Voita et al., 2018, 2019; Zheng et al., 2020b; Fernandes et al., 2021). Document-level NMT models can be implemented as a post-processing model or context-aware model. The post-processing models use an additional module to use context on generated translations (Xiong et al., 2019; Voita et al., 2019). However, post-processing generated translations may Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 203 lead to higher latency, which is counter-intuitive in a simultaneous translation scenario. On the other hand, context-aware models"
2021.mtsummit-research.17,D17-1301,0,0.0544229,"et al., 2020). The additional image modality provides the model with a form of context which helps disambiguate hard-to-translate phenomena, especially when needed information is not available yet during translation. An additional image modality may not always be available, and thus, taking advantage of the context in the form of previously seen sentences is the only viable option. Research in document-level NMT has already proven that context from neighbouring sentences can help enhance representations and thereby improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017). The simplest documentlevel NMT architecture involves using an additional encoder that encodes the context sentences, following which the encoded context is used to augment the representation of the sentence to be translated (Zhang et al., 2018). Just like using an image as a modality helps enrich the encoding of the sentence with additional disambiguation information, the context sentences might also contain such useful information. We already know that in an SNMT setting, due to partial sentences being translated, the amount of context available to the decoder is limited, and thus leveragin"
2021.mtsummit-research.17,D18-1049,0,0.0784019,"y may not always be available, and thus, taking advantage of the context in the form of previously seen sentences is the only viable option. Research in document-level NMT has already proven that context from neighbouring sentences can help enhance representations and thereby improve translation quality (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017). The simplest documentlevel NMT architecture involves using an additional encoder that encodes the context sentences, following which the encoded context is used to augment the representation of the sentence to be translated (Zhang et al., 2018). Just like using an image as a modality helps enrich the encoding of the sentence with additional disambiguation information, the context sentences might also contain such useful information. We already know that in an SNMT setting, due to partial sentences being translated, the amount of context available to the decoder is limited, and thus leveraging the context sentences should signiﬁcantly boost SNMT translation quality. This motivated us to combine document-level NMT with SNMT leading to document-level SNMT. Our document-level SNMT architecture is simple, where we have a sentence encoder"
2021.mtsummit-research.17,D19-1137,0,0.0154149,"lps improve translation slightly in both settings but not by a large margin. We then perform a statistical and manual analysis of the translations where we observe that while SNMT models deﬁnitely beneﬁt from context, they are unable to utilize context effectively and sometimes suffer due to the provided context. This opens up the possibility of research into better mechanisms for leveraging context more effectively. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. Mainly, SNMT can mostly be implemented with ﬁxed or adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to READ another source word or WRITE a target word in one model (Grissom II et al., 2014; Matsubara et al., 2000; Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Zheng et al., 2019a,c, 2020a) focus on mechanisms that determine the optimal number of source language tokens to wait for before generating the next target language token. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens a"
2021.mtsummit-research.17,P19-1582,0,0.0187773,"lps improve translation slightly in both settings but not by a large margin. We then perform a statistical and manual analysis of the translations where we observe that while SNMT models deﬁnitely beneﬁt from context, they are unable to utilize context effectively and sometimes suffer due to the provided context. This opens up the possibility of research into better mechanisms for leveraging context more effectively. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. Mainly, SNMT can mostly be implemented with ﬁxed or adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to READ another source word or WRITE a target word in one model (Grissom II et al., 2014; Matsubara et al., 2000; Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Zheng et al., 2019a,c, 2020a) focus on mechanisms that determine the optimal number of source language tokens to wait for before generating the next target language token. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens a"
2021.mtsummit-research.17,D19-1144,0,0.0126071,"lps improve translation slightly in both settings but not by a large margin. We then perform a statistical and manual analysis of the translations where we observe that while SNMT models deﬁnitely beneﬁt from context, they are unable to utilize context effectively and sometimes suffer due to the provided context. This opens up the possibility of research into better mechanisms for leveraging context more effectively. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. Mainly, SNMT can mostly be implemented with ﬁxed or adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to READ another source word or WRITE a target word in one model (Grissom II et al., 2014; Matsubara et al., 2000; Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Zheng et al., 2019a,c, 2020a) focus on mechanisms that determine the optimal number of source language tokens to wait for before generating the next target language token. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with ﬁxed policy, where the decoder starts generating the target language tokens the moment k source language tokens a"
2021.mtsummit-research.17,N16-1004,0,0.0200433,"(Ec ) leading to context encoding Ec (Xc ) which ′ is then used for translation along with E(X) as Y = D(E(X), Ec (Xc )). It is a common practice to share the parameters of the sentence and context encoders. A key component of document-level NMT is the incorporation of Ec (Xc ) into the framework by combining it with E(X). This paper considers two simple approaches, which we dub as “multi-source” (MS) and “context-attention” (CA). 3.2.1 MS: Multi-Source Based Context Incorporation This method treats the context as an additional source of information similar to the setting in multi-source NMT (Zoph and Knight, 2016; Dabre et al., 2017). In multi-source NMT, the decoder is modiﬁed to attend to multiple source sentences, and this approach should help incorporate context into the decoding process. For vanilla NMT, the cross attention mechanism of the decoder takes in E(X) and produces a weighted representation, the attention, A. Given the context encoding Ec (Xc ) we additionally compute the context attention Ac . We combine A and Ac into Acomb , the context augmented attention, using a simple gating mechanism as Acomb = α ∗ A + (1 − α) ∗ Ac where α = sigmoid(Wcomb ∗ [A : Ac ]). [:] indicates concatenation"
2021.wat-1.1,2020.acl-main.703,0,0.187967,"news-commentary corpus.14 This year we also encouraged participants to use any corpora from WMT 202015 and WMT 202116 involving Japanese, Russian, and English as long as it did not belong to the news commentary domain to prevent any test set sentences from being unintentionally seen during training. 2,049 2,050 Table 5: The NICT-SAP task corpora splits. The corpora belong to two domains: wikinews (ALT) and software documentation (IT). The Wikinews corpora are Nway parallel. also encouraged the use of monolingual corpora expecting that it would be for pre-trained NMT models such as BART/MBART (Lewis et al., 2020; Liu et al., 2020). In Table 5 we give statistics of the aforementioned corpora which we used for the organizer’s baselines. Note that the evaluation corpora for both domains are created from documents and thus contain document level meta-data. Participants were encouraged to use document level approaches. Note that we do not exhaustively list6 all available corpora here and participants were not restricted from using any corpora as long as they are freely available. 2.7 Partition train development test train development test train development test 8 http://www.phontron.com/kftt/ https://data"
2021.wat-1.1,C18-2019,0,0.0200002,"task and 4 systems for the Japanese→ English.74 On the whole, all the submitted systems are basically lexical-constraint-aware NMT models with lexically constrained decoding method, where the restricted target vocabulary is concatenated into source sentences and, during the beam search at inference time, the models generate translation outputs containing the target vocabulary. We observed that these techniques boost the final translation performance of the NMT models in the restricted translation task. For human evaluation, we conducted the sourcebased direct assessment (Cettolo et al., 2017; Federmann, 2018) and source-based contrastive assessment (Sakaguchi and Van Durme, 2018; Federmann, 2018), to have the top-ranked systems of each team appraised by bilingual human annotators. In the human evaluation campaign, we also include the human reference data. Table 20 reports the final automatic evaluation score and the human evaluation results. In both tasks, the systems from the team “NTT” are the most highly evaluated in all the submitted systems in the final score and the human evaluation, consistently. We also note that our designed automation metric is well correlated Flickr30kEnt-JP Japanese↔En"
2021.wat-1.1,W18-1819,0,0.0552279,"Missing"
2021.wat-1.1,2007.mtsummit-papers.63,0,0.0610664,"om OPUS. We Test set II : A pair of test and reference sentences and context data that are articles including test sentences. The references were automatically extracted from English newswire sentences and manually selected. Therefore, the quality of the references of test set II is better than that of test set I. The statistics of JIJI Corpus are shown in Table 2. The definition of data use is shown in Table 3. Participants submit the translation results of one or more of the test data. The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from (Utiyama and Isahara, 2007). 2.5 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2021 consists of two corpora, the ALT corpus and UCSY corpus. • The ALT corpus is one part from the Asian Language Treebank (ALT) project (Riza et al., 2016), consisting of twenty thousand Myanmar-English parallel sentences from news articles. • The UCSY corpus (Yi Mon Shwe Sin and Khin Mar Soe, 2018) is constructed by the NLP Lab, University of Computer Studies, 3 http://opus.nlpl.eu/ http://www.statmt.org/wmt20/ 5 Software Domain Evaluation Splits 4 3 Task Use Training Test set I Japanese to English Test"
2021.wat-1.23,2020.coling-tutorials.3,1,0.900853,"icantly outperforms a very strong multilingual baseline that does not rely on any pre-training. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have a shared target language. Cross-lingual transfer despite i"
2021.wat-1.23,D19-1146,1,0.851644,"leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have"
2021.wat-1.23,N16-1101,0,0.132548,"quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have a shared target language. Cross-lingu"
2021.wat-1.23,W18-2703,0,0.0177418,"o vanilla multilingual training which does not rely on monolingual corpora. Another reason for focusing on utilizing monolingual corpora is that they are extremely abundant when compared to parallel corpora and they contain a large amount of language modeling information. In this regard, back-translation and multilingual pre-training are two of the most reliable methods. While back-translation is easy to use, it involves the translation of millions of monolingual sentences and quite often it is necessary to perform multiple iterations of the back-translation process to yield the best results (Hoang et al., 2018) which means that it is quite resource intensive. This leaves us with multilingual pre-training using methods such as BART/MBART (Liu et al., 2020; Lewis et al., 2020) which we use for developing our translation system. The advantage of BART/MBART is that we need to pre-train these models once and then fine-tune not only for machine translation but also for any natural language generation task such as summarization (Shi et al., 2021). These models can be upgraded to include additional language pairs in the future by simply resuming pre-training (Tang et al., 2020). In this paper, we describe o"
2021.wat-1.23,D10-1092,0,0.0372315,"r each language pair. Different from most previous works, instead of decoding a single final model, we choose a particular model for a language pair with the highest development set BLEU score for that pair. Therefore, we treat multilingualism as a way get a (potentially) different model per language pair leading to the best BLEU scores for that pair and not as a way to get a single model that gives the best performance for each language pair. For evaluation, as we have mentioned before, we use BLEU (Papineni et al., 2002) as the primary evaluation metric. WAT also uses metrics such as RIBES (Isozaki et al., 2010), AM-FM (Zhang et al., 2021) and human evaluation (Nakazawa et al., 2019, 2020, 2021). All these metrics focus on different aspects of translations and may lead to different rankings for submissions, however this multi-metric evaluation helps us understand that there may not be one perfect model. To avoid confusing the reader with a clutter of scores, we only show BLEU scores and we refer the reader to the evaluation page where all scores and rankings10 can be seen11 . 4.4 Models Trained We trained the following models: 7 https://github.com/prajdabre/yanmtt We choose SPM because SPM can work w"
2021.wat-1.23,N19-1388,0,0.0197266,"l corpora collection spanning corpora for N language pairs Lisrc − Litgt for i ∈ [1, N ]. The sizes of the parallel corpora are typically different, often radically different, in which case it is important to balance corpora sizes to prevent the model from focusing too much on some language pairs. Johnson et al. (2017) showed that training by oversampling smaller corpora to match the size of the largest corpus is the best approach. However, since then newer corpora balancing approaches have been proposed and the most recent effective method is known as the temperature based sampling approach (Aharoni et al., 2019). Suppose that the size of the ith corpus is si which means the probability of sampling a sentence P pair from each si corpus is pi = S where S = i si . Using this default sampling probability is biased towards larger corpora so first the probability values are tempered using a temperature T . The resultant probabilities pti are obtained as follows: 1 pti pT = Pi 1 T j pj (1) When T = 1, pti = pi and when T = ∞, pti = N1 . Aharoni et al. (2019) showed that a value of T = 5 works well in practice which is what we use in our experiments. During training, sentence pairs are sampled from each corp"
2021.wat-1.23,D18-2012,0,0.0138469,"e methods mentioned in Section 3 in our in-house toolkit which we make publicly available7 . This toolkit is based on the HuggingFace transformers library (Wolf et al., 2020) v4.3.2. Note that the MBART implementation in the library shares the encoder embedding, decoder embedding and decoder softmax projection layers. We implement denoising, temperature based data sampling and multilingual training ourselves. We also use the HuggingFace transformer tokenizer library to train tokenizers. These tokenizers are wrappers around Byte Pair Encoding (BPE) (Gage, 1994) or SentencePiece (SPM) (Kudo and Richardson, 2018) models and we choose8 the latter as opposed to the former which is used by the original MBART implementation. 4.3 Training and Evaluation We first trained a tokenizer with a joint vocabulary size of 64,000 sub-words which is learned on the IndicCorp monolingual data. We consider this vocabulary size to be sufficient for all languages. For pre-training, we use hyperparameters corresponding to the “transformer big” (Vaswani et al., 2017) with a few exceptions such as dropout of 0.1, positional embeddings instead of positional encodings and a maximum learning rate of 0.001. When performing batch"
2021.wat-1.23,P16-1162,0,0.0840457,"lingual baseline that does not rely on any pre-training. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have a shared target language. Cross-lingual transfer despite its simplicity and effectiveness relies on"
2021.wat-1.23,2020.acl-main.703,0,0.487975,"n Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have a shared target language. Cross-lingual transfer despite its simplicity and effectiveness relies on shared source or target languages for effective transfer and thus"
2021.wat-1.23,2020.tacl-1.47,0,0.332722,"ing. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have a shared target language. Cross-lingual transfer despite its simplicity and effectiveness relies on shared source or target languages for effectiv"
2021.wat-1.23,2020.lrec-1.454,1,0.914403,"nslation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is often better to have a shared target language. Cross-lingual transfer despite its simplicity and effectiveness relies on shared source or target languages for effective transfer and thus depending on method"
2021.wat-1.23,P02-1040,0,0.112509,"train till convergence on the global development set BLEU score, an average of BLEU scores for each language pair. Different from most previous works, instead of decoding a single final model, we choose a particular model for a language pair with the highest development set BLEU score for that pair. Therefore, we treat multilingualism as a way get a (potentially) different model per language pair leading to the best BLEU scores for that pair and not as a way to get a single model that gives the best performance for each language pair. For evaluation, as we have mentioned before, we use BLEU (Papineni et al., 2002) as the primary evaluation metric. WAT also uses metrics such as RIBES (Isozaki et al., 2010), AM-FM (Zhang et al., 2021) and human evaluation (Nakazawa et al., 2019, 2020, 2021). All these metrics focus on different aspects of translations and may lead to different rankings for submissions, however this multi-metric evaluation helps us understand that there may not be one perfect model. To avoid confusing the reader with a clutter of scores, we only show BLEU scores and we refer the reader to the evaluation page where all scores and rankings10 can be seen11 . 4.4 Models Trained We trained the"
2021.wat-1.23,D16-1163,0,0.137849,"ingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014) is known to give state-of-the-art translations for a variety of language pairs. NMT is known to perform poorly for language pairs for which parallel corpora are scarce. This happens due to lack of translation knowledge as well as due to overfitting which is inevitable in a low-resource setting. Fortunately, transfer learning via cross-lingual transfer (Zoph et al., 2016; Dabre et al., 2019), multilingualism (Firat et al., 2016; Dabre et al., 2020), back-translation (Sennrich et al., 2016) or monolingual pre-training (Liu et al., 2020; Lewis et al., 2020; Mao et al., 2020) can significantly improve translation quality in a low-resource situation. Cross-lingual transfer learning involves pretraining a model using a parallel corpus for a resource-rich language pair XX − Y Y and then fine-tuning on a parallel corpus for a resource-poor language pair AA − BB. Naturally the improvements in translation quality will be impacted by if XX = AA or Y Y = BB 1 and it is"
C12-2023,C94-1087,0,0.0143373,"Missing"
C12-2023,J95-3006,0,0.464557,"Missing"
C12-2023,W10-3604,1,0.854743,"Missing"
D19-1146,2012.eamt-1.60,0,0.0681834,"and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson"
D19-1146,P17-2061,1,0.921873,"whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular,"
D19-1146,P15-1166,0,0.17506,"al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more difficult. Dong et al. (2015) have shown that a one-to-many model trained on middle-sized parallel data (200k sentence pairs) can improve the translation quality over a one-to-one model. However, it is unclear whether this works for much resource-poorer, more distant, and more diverse language pairs. Using pseudo-parallel data is a potential solution, but for most resource-poor languages, the amount of available clean and in-domain monolingual data are limited. It is also unclear what the real reason behind improvements in translation is: increase in the training data or multilingualism. This paper focuses on (a) improvin"
D19-1146,N16-1101,0,0.0294968,"Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed"
D19-1146,N18-1032,0,0.0267464,"9. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these appr"
D19-1146,D18-1398,0,0.0280459,"9. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these appr"
D19-1146,L18-1545,0,0.0789616,"from the overlapping vocabularies of Chinese and Japanese, the performance of other languages also improves despite no vocabulary overlap. Consequently, we believe that translation into a low-resource target language might benefit when the language is also target side of the helping corpus but multistage fine-tuning does not require such overlap and instead shows optimal performance when it leverages multilingualism during stage-wise tuning. In the future, we will test the above hypotheses thoroughly via some controlled experiments by using, for instance, larger scale multi-parallel corpora (Imamura and Sumita, 2018) and varieties of helping corpora. 5.2 How Does Multilingualism Help? The ALT corpus is multi-parallel and merely comprises of the same sentences in multiple languages. Although we used seven target languages, we did not introduce new English sentences to the source side. Table 1 shows that some languages are better translated (En-Vi) than others (En-Bn). We speculate that the representations for En-Vi might be better learned than those for En-Bn. When learning all language pairs jointly, the representations of sentences with the same meaning tend to be similar. As such, the representations fo"
D19-1146,W19-6613,1,0.78878,"from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular, we consider exploiting two types of corpora. One is a small-scale multi-parallel corpus, En-YY1 · · ·-YYN , consisting of English and N target languages of interest. T"
D19-1146,Q17-1024,0,0.0864411,"Missing"
D19-1146,2005.mtsummit-papers.11,0,0.188451,"Japanese, Khmer, Malay, and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, wherea"
D19-1146,I17-2050,0,0.0225962,"cs 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked on one-tomany NMT, whereas Firat et al. (2016) and Johnson et al. (2017) have worked on multilingual and multi-way NMT. These studies have focused on training a multilingual model from scratch, but we explore transfer learning for one-to-many NMT. Fine-tuning based transfer learning has been studied for transferring proper parameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multi"
D19-1146,P02-1040,0,0.104338,"tter than the 1-to-1 “En-YY” model (#1), the best model without the multi-parallel corpus (#2–#4), and the strongest baselines (#5–#6). the BLEU score on the development set (simply concatenated unlike training data) did not vary by more than 0.1 BLEU over 10 checkpoints. Instead of choosing the model with the best development set BLEU, we averaged the last 10 checkpoints saved every after 1,000 updates, following Vaswani et al. (2017), and decoded the test sets with a beam size of 4 and a length penalty, α, of 0.6 consistently across all the models. 4.3 Results Table 1 gives the BLEU scores (Papineni et al., 2002) for all the configurations. Among the seven configurations, irrespective of the external parallel corpus for En-XX, the three-stage fine-tuned model (#7) achieved the highest BLEU scores for all the seven target languages. Results for #1 demonstrate that NMT systems trained on 18k parallel sentences can achieve only poor results for Bn and Ja, whereas reasonably high BLEU scores (> 20) are achieved for the other target languages. Introducing a large external En-XX parallel corpus improved the translation quality consistently and significantly for all the seven target languages,14 irrespective"
D19-1146,P16-1009,0,0.0870722,"arameters (Zoph et al., 2016; Gu et al., 2018b), lexical (Zoph et al., 2016; Nguyen and Chiang, 2017; Gu et al., 2018a; Lakew et al., 2018), and syntactic (Gu et al., 2018a; Murthy et al., 2018) knowledge from a resource-rich language pair to a resource-poor language pair. On the other hand, Chu et al. (2017) proposed a more robust training approach for domain adaptation, called mixed fine-tuning, which uses a mixture of data from different domains. Imankulova et al. (2019) proposed a multistage fine-tuning method which combines finetuning techniques for domain adaptation and backtranslation (Sennrich et al., 2016). Unlike us, these approaches do not try to combine multilingualism with multistage fine-tuning using only parallel corpora involving a large number of languages. 3 Multistage Fine-Tuning for NMT This paper focuses on translation from English (En) to N different languages. In particular, we consider exploiting two types of corpora. One is a small-scale multi-parallel corpus, En-YY1 · · ·-YYN , consisting of English and N target languages of interest. The other is a relatively larger helping parallel corpus, En-XX, where XX indicates the helping target language which needs not be one of the tar"
D19-1146,L16-1561,0,0.0220411,"s (Bengali, Filipino, Indonesian, Japanese, Khmer, Malay, and Vietnamese) in the Asian Language Treebank (ALT) corpus (Riza et al., 2016),3 as a result of soft division of labor: training of a strong English encoder, domain adaptation, and tuning of the decoder for each target language. 2 Adding a new language does not add any new translation knowledge. Such corpora deserve more attention, since adding a new language to it leads to parallel corpora between the new language and all the other languages in the corpora. 3 There exist other multi-parallel corpora, such as those for United Nations (Ziemski et al., 2016), Europarl (Koehn, 2005), Ted Talks (Cettolo et al., 2012), ILCI (Jha, 2010), and the Bible (Christodouloupoulos and Steedman, 2015). 1410 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1410–1416, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 Related Work En-XX In this paper, we address (a) the importance of multilingualism (b) via one-to-many NMT (c) through robust fine-tuning for transfer learning. Dong et al. (2015) have worked o"
D19-1146,N16-1004,0,0.0202007,"and German–English (Bojar et al., 2018), it performs poorly for resource-poor pairs (Zoph et al., 2016; Riza et al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more difficult. Dong et al. (2015) have shown that a one-to-many model trained on middle-sized parallel data (200k sentence pairs) can improve the translation quality over a one-to-one model. However, it is unclear whether this works for much resource-poorer, more distant, and more diverse language pairs. Using pseudo-parallel data is a potential solution, but for most resource-poor languages, the amount of available clean and in-domain monolingual data are limited. It is also unclear what the real reason behind improvem"
D19-1146,D16-1163,0,0.450445,"tent-wise redundancy thus exhibiting the true power of multilingualism. Even when the helping target language is not one of the target languages of our concern, our multistage finetuning can give 3–9 BLEU score gains over a simple one-to-one model. 1 Introduction Encoder-decoder based neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) allows for end-to-end training of a translation model. While NMT is known to perform well for resource-rich language pairs, such as French– English and German–English (Bojar et al., 2018), it performs poorly for resource-poor pairs (Zoph et al., 2016; Riza et al., 2016). Translating from a resource-poor language into English is typically easier than the other direction, because large parallel corpora between English and other languages can be used for transfer learning (Zoph et al., 2016) and many-to-one modeling.1 Additionally, large English monolingual data can be back-translated to obtain pseudoparallel corpora for augmenting the performance of such NMT models. 1 Note that “many-to-one” does not mean multi-source machine translation (Zoph and Knight, 2016). Translating from English into a resource-poor language is substantially more di"
D19-5201,E06-1031,0,0.0437063,"ion web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 http://lotus.kuee.kyoto-u.ac.jp/WAT/ km-en-d"
D19-5201,W16-4601,1,0.763733,"Missing"
D19-5201,W19-6613,1,0.928331,"Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each particJaRuNC Corpus For the Russian↔Japanese task we asked participants to use the JaRuNC corpus5 (Imankulova et al., 2019) which belongs to the news commentary domain. This dataset was manually aligned and cleaned and is trilingual. It can be used to evaluate Russian↔English 6 http://www.phontron.com/kftt/ https://datarepository.wolframcloud.com/ resources/Japanese-English-Subtitle-Corpus 8 https://wit3.fbk.eu/ 9 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 10 https://cms.unov.org/UNCorpus/ 11 https://translate.yandex.ru/corpus?lang=en 12 http://lotus.kuee.kyoto-u.ac.jp/WAT/ News-Commentary/news-commentary-v14.en-ru. filtered.tar.gz 7 4 http://ufal.mff.cuni.cz/~ramasamy/parallel/ html/ 5 https://github.com/aizhanti/JaR"
D19-5201,W17-5701,1,0.779618,"Missing"
D19-5201,D10-1092,0,0.105912,"t of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for each task were calculated using the corresponding reference translations. The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provid"
D19-5201,W14-7001,1,0.794275,"t 400 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 • Open innovation platform Due to the fixed and open test data, we can repeatedly evaluate translation systems on the same dataset over years. WAT receives submissions at any time; i.e., there is no submission deadline of translation results w.r.t automatic evaluation of translation quality. Introduction The Workshop on Asian Translation (WAT) is an open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2018 (Nakazawa et al., 2014, 2015, 2016, 2017, 2018), WAT2019 brings together machine 2 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/ 3 https://ufal.mff.cuni.cz/hindi-visual-genome/ wat-2019-multimodal-task 1 One paper was withdrawn post acceptance and hence only 6 papers will be in the proceedings. 1 Proceedings of the 6th Workshop on Asian Translation, pages 1–35 Hong Kong, China, November 4, 2019. ©2019 Association for Computational Linguistics Lang JE JC Train 3,008,500 672,315 Dev 1,790 2,090 DevTest 1,784 2,148 Lang zh-ja ko-ja en-ja Test 1,812 2,107 Lang zh-ja ko-ja en-ja Table 1: Statistics for ASPEC Dataset"
D19-5201,W15-5001,1,0.855752,"Missing"
D19-5201,P17-4012,0,0.147541,"MT RBMT Other Other ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ja-hi ✓ ✓ ✓ ✓ EnTam ta-en en-ta ✓ ✓ IITB en-hi hi-ja ✓ ✓ ✓ ✓ TDDC ja-en ✓ hi-en ✓ ✓ NewsCommentary ru-ja ja-ru JIJI ja-en en-ja ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ALT en-my km-en ✓ ✓ Multimodal en-hi ✓ my-en ✓ ✓ en-km ✓ model) for Chinese segmentation. • The Moses toolkit for English and Indonesian tokenization. • Mecab-ko16 for Korean segmentation. • Indic NLP Library17 for Indic language segmentation. • The tools included in the ALT corpus for Myanmar and Khmer segmentation. • subword-nmt18 for all languages. 3.3.1 NMT with Attention We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems of NMT with attention (System ID: NMT). We used the following OpenNMT configuration. • • • • • • • • When we built BPE-codes, we merged source and target sentences and we used 100,000 for s option. We used 10 for vocabulary-threshold when subword-nmt applied BPE. 3.2.2 The default values were used for the other system parameters. For EnTam, News Commentary • The Moses toolkit for English and Russian only for the News Commentary data. 3.3.2 Transformer (Tensor2Tensor) For the News Commentary and English↔Tamil tasks, we used tensor2tensor’s20 im"
D19-5201,W18-1819,0,0.026575,"multilingual model. As for the English↔Tamil task, we train separate baseline models for each translation direction with 32,000 separate sub-word vocabularies. • Mecab19 for Japanese segmentation. • The EnTam corpus is not tokenized by any external toolkits. • Both corpora are further processed by tensor2tensor’s internal pre/postprocessing which includes sub-word segmentation. 3.2.3 For Multi-Modal Task • Hindi Visual Genome comes untokenized and we did not use or recommend any specific external tokenizer. 3.3.3 Transformer (OpenNMT-py) For the Multimodal task, we used the Transformer model (Vaswani et al., 2018) as implemented in OpenNMT-py (Klein et al., 2017) and used the “base” model with default parameters for the multi-modal task baseline. We have generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. • The standard OpenNMT-py sub-word segmentation was used for pre/postprocessing for the baseline system and each participant used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_freque"
D19-5201,P11-2093,0,0.0158555,"tion system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure 2, the system requires participants to provide the following information for each submission: • Human Evaluation: whether or not they submit the results for human evaluation; Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model23 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.24 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.25 For Korean segmentation, we used mecab-ko.26 For Myanmar and Khmer segmentations, we used myseg.py27 and kmseg.py28 . For English and Russian tokenizations, we used tokenizer.perl29 in the Moses toolkit. For Hindi and Tamil tokenizations, we used Indic NLP Library.30 The detailed procedu"
D19-5201,P02-1040,0,0.106619,"used what they wanted. 3.3 encoder_type = brnn brnn_merge = concat src_seq_length = 150 tgt_seq_length = 150 src_vocab_size = 100000 tgt_vocab_size = 100000 src_words_min_frequency = 1 tgt_words_min_frequency = 1 Baseline NMT Methods We used the following NMT with attention for most of the tasks. We used Transformer (Vaswani et al., 2017) (Tensor2Tensor)) for the News Commentary and English↔Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score 16 We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES https://bitbucket.org/eunjeon/mecab-ko/ https://bitbucket.org/anoopk/indic_nlp_library 18 https://github.com/rsennrich/subword-nmt 19 https://taku910.github.io/mecab/ 17 20 9 https://github.com/tensorflow/tensor2tensor 4.2 Automatic Evaluation System (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.22 All scores for e"
D19-5201,W16-2342,0,0.0144454,"on results that participants permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyot"
D19-5201,J82-2005,0,0.731235,"Missing"
D19-5201,D19-5224,0,0.0224104,"Missing"
D19-5201,W15-3049,0,0.0158524,"s permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-da"
D19-5201,W12-5611,1,0.827974,"/ 17k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 3.5k / 3.8k 3.5k / 3.8k 144k / 74k 3.2k / 2.3k 5.6k / 3.8k translation quality as well but this is beyond the scope of this years sub-task. Refer to Table 10 for the statistics of the in-domain parallel corpora. In addition we encouraged the participants to use out-of-domain parallel corpora from various sources such as KFTT,6 JESC,7 TED,8 ASPEC,9 UN,10 Yandex11 and Russian↔English news-commentary corpus.12 EnTam Corpus For Tamil↔English translation task we asked the participants to use the publicly available EnTam mixed domain corpus4 (Ramasamy et al., 2012). This corpus contains training, development and test sentences mostly from the news-domain. The other domains are Bible and Cinema. The statistics of the corpus are given in Table 9. 2.10 #sent. 12,356 486 600 47,082 589 600 82,072 313 600 Table 10: In-Domain data for the Russian– Japanese task. Table 9: Data for the Tamil↔English task. 2.9 Partition train development test train development test train development test 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation r"
D19-5201,2006.amta-papers.25,0,0.0857626,"hed are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page.32 4.3 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer33 ), characTER (Wang et al., 2016), chrF3 (Popović, 2015), TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006). Except for chrF3 and characTER, we ran Moses tokenizer34 on the candidate and reference before scoring. For all error metrics, i.e. metrics where better 21 http://www.kecl.ntt.co.jp/icl/lirg/ribes/index. html 22 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/ 23 http://www.phontron.com/kytea/model.html 24 http://code.google.com/p/mecab/downloads/ detail?name=mecab-ipadic-2.7.0-20070801.tar.gz 25 http://nlp.stanford.edu/software/segmenter. shtml 26 https://bitbucket.org/eunjeon/mecab-ko/ 27 http://lotus.kuee.kyoto-u.ac.jp/WAT/ my-en-data/wat2019.my-en.zip 28 htt"
D19-5201,2007.mtsummit-papers.63,0,0.678495,",000 2,000 Dev 2,000 2,000 2,000 Table 2: Statistics for JPC • Domain and language pairs WAT is the world’s first workshop that targets scientific paper domain, and Chinese↔Japanese and Korean↔Japanese language pairs. In the future, we will add more Asian languages such as Vietnamese, Thai and so on. 2 Train 1,000,000 1,000,000 1,000,000 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from Utiyama and Isahara (2007). Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the 2.2 JPC JPO Patent Corpus (JPC) for the patent tasks was constructed by the Japan Patent Office (JPO) in collaboration with NICT. The corpus consists of Chinese-Japanese, KoreanJapanese and English-Japanese patent descriptions whose International Patent Classi2 Disclosure Period 2016-01-01 to 2017-12-31 2018-01-01 to 2018-06-30 Train 1,089,346 (614,817) 314,649 (218,495) Dev Texts Items 1,153 2,"
D19-5207,D14-1179,0,0.011769,"Missing"
D19-5207,W19-6613,1,0.88935,"pora for L1, L2 and L3 into all other languages. (b) a. Fine-tune the previous model using all in-domain parallel and pseudoparallel corpora. This stage-wise division of training ensures that the model focuses on a specific domain per training step and relies on multilingualism to address the scarcity of data. The resultant model used for back-translation leads to an inflation in good quality in-domain data which should substantially increase translation performance. In our experiments, L1 is Russian, L2 is Japanese and L3 is English. Multilingual Multi-stage Training with Back-translation In Imankulova et al. (2019), we proposed leveraging multilingualism via multiple training stages. Although we explain the idea in detail below, we urge the readers to read this paper for minute details regarding implementation and datapreprocessing. Assume that our language pair of interest is L1– L2 for which we have very little data. We have the following types of helping data: large L1–L3 and L2–L3 out-of-domain parallel corpora, small L1–L3 and L2–L3 in-domain parallel corpora and in-domain monolingual corpora that are slightly larger than the in-domain parallel corpora. In order to train robust NMT models we do the"
D19-5207,D16-1163,0,0.0336808,"be the Transformer which is the state-of-the-art NMT model we used for our experiments. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2019, we participated as team “NICT5” and worked on Russian–Japanese and English– Tamil translation. The techniques we focused on for each translation task can be summarized as below: 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural model that consists of two components, the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors of high dimensi"
D19-5207,W18-6325,0,0.019379,"imilar L3 and L2 allow for better transfer learning. As such, we used Hindi as the helping language, L3 for which L2 is Tamil because both are Indian languages. In theory, Tamil should benefit more from Dravidian languages but there is no large helping corpus involving a Dravidian language. It is reasonable to expect improvements in translation by fine-tuning a L3→L1 model on L2→L1 data because of the additional target language monolingual data that helps improve the decoderside language model. However, previous research has shown that this works even if the translation direction is reversed (Kocmi and Bojar, 2018). As such, we also experiment with fine-tuning a L1→L3 model on L1→L2 data with the expectation that the encoder representations will be improved. 2.3 4. Use robust multilingual model for backtranslation and final model training: (a) Use the previous model to backtranslate all in-domain monolingual corpora for L1, L2 and L3 into all other languages. (b) a. Train a multilingual model for L1↔L2, L1↔L3 and L2↔L3 using all in-domain parallel and pseudo-parallel corpora. 5. Repeat N 1 times: (a) Use the previous model to backtranslate all in-domain monolingual corpora for L1, L2 and L3 into all oth"
D19-5207,P07-2045,0,0.00456996,"to the submissions of other WAT participants, kindly refer to the overview paper (Nakazawa et al., 2019). 2 NMT Models and Approaches We will first describe the Transformer which is the state-of-the-art NMT model we used for our experiments. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2019, we participated as team “NICT5” and worked on Russian–Japanese and English– Tamil translation. The techniques we focused on for each translation task can be summarized as below: 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural m"
D19-5207,W12-5611,0,0.048395,"Missing"
I17-5004,Q17-1024,0,0.035587,"Missing"
I17-5004,P16-1009,0,0.014262,"TRA) Overview of implementations of the NMT models and the frameworks: KNMT1 , Lamtram2 , Open NMT3 , Nematus4 , Tensor2Tensor5 . Coffee Break ( 30 min) 3. Practical NMT ( 45 min) The objective of this part of the tutorial is to augment the audience’s understanding of NMT with various practical ideas that can help improve the quality and speed of NMT as well as showcase the many black box applications of NMT. 5. Summary and Conclusion 3 • Preprocessing and management of rare words. • Subword units to enable infinite vocabulary. • BPE (Byte Pair Encoding) and its impact on translation quality (Sennrich et al., 2016b). • Using monolingual corpora to improve NMT (Gülçehre et al., 2015; Sennrich et al., 2016a). • Training and Translation search. • Optimization algorithms (ADAM etc). • Residual connections. • Training schedules for optimal results (ADAM –&gt; SGD –&gt; annealing –&gt; early stopping). About the Speakers • Raj Dabre: Graduate School of Informatics, Kyoto University, Japan (raj@nlp.ist.i.kyotou.ac.jp) Raj Dabre is a 3rd year PhD student at Kyoto University. His research interests center on natural language processing, particularly neural machine translation for low resource languages and domain adapta"
I17-5004,P16-1162,0,0.0400068,"TRA) Overview of implementations of the NMT models and the frameworks: KNMT1 , Lamtram2 , Open NMT3 , Nematus4 , Tensor2Tensor5 . Coffee Break ( 30 min) 3. Practical NMT ( 45 min) The objective of this part of the tutorial is to augment the audience’s understanding of NMT with various practical ideas that can help improve the quality and speed of NMT as well as showcase the many black box applications of NMT. 5. Summary and Conclusion 3 • Preprocessing and management of rare words. • Subword units to enable infinite vocabulary. • BPE (Byte Pair Encoding) and its impact on translation quality (Sennrich et al., 2016b). • Using monolingual corpora to improve NMT (Gülçehre et al., 2015; Sennrich et al., 2016a). • Training and Translation search. • Optimization algorithms (ADAM etc). • Residual connections. • Training schedules for optimal results (ADAM –&gt; SGD –&gt; annealing –&gt; early stopping). About the Speakers • Raj Dabre: Graduate School of Informatics, Kyoto University, Japan (raj@nlp.ist.i.kyotou.ac.jp) Raj Dabre is a 3rd year PhD student at Kyoto University. His research interests center on natural language processing, particularly neural machine translation for low resource languages and domain adapta"
I17-5004,D14-1179,0,0.0156838,"Missing"
I17-5004,N16-1101,0,0.0130943,") The objective of this part of the tutorial is to bring the audience up to speed with the current SOTA (state-of-the-art) NMT models and advancements. We plan to enumerate the most important ones and thereby provide the audience members a roadmap to understanding the big picture. • Facebook’s CNN (Convolutional Neural Network) based NMT model (Gehring et al., 2017). • Google’s Transformer (Vaswani et al., 2017) that replies purely on attention and feedforward networks (SOTA for WMT tasks). • Results and speedup in training achieved by these architectures. • Multilingual Multiway NMT (MLNMT) (Firat et al., 2016) and Zero Shot NMT (Johnson et al., 2016). • Other advances (search-guided, latent graph, pointer networks) (EXTRA) Overview of implementations of the NMT models and the frameworks: KNMT1 , Lamtram2 , Open NMT3 , Nematus4 , Tensor2Tensor5 . Coffee Break ( 30 min) 3. Practical NMT ( 45 min) The objective of this part of the tutorial is to augment the audience’s understanding of NMT with various practical ideas that can help improve the quality and speed of NMT as well as showcase the many black box applications of NMT. 5. Summary and Conclusion 3 • Preprocessing and management of rare words. •"
L16-1468,J93-2003,0,0.0626585,"Missing"
L16-1468,D14-1179,0,0.0370302,"Missing"
L16-1468,2012.eamt-1.7,1,0.859034,"parallel sentences extracted by the baseline system. The models trained on the seed parallel corpus are used for producing the NN features for training and testing the classifier. We tried the use of both the two types of models to score the parallel sentence candidates for extraction. 3. Experiments We evaluated classification accuracy, and conducted extraction and translation experiments on Chinese-Japanese data to verify the effectiveness of our proposed NN features. In all our experiments, we preprocessed the data by segmenting Chinese and Japanese sentences using a segmenter proposed by Chu et al. (2012) and JUMAN (Kurohashi et al., 1994) respectively. 3.1. Data The seed parallel corpus we used is the Chinese-Japanese section of the Asian Scientific Paper Excerpt Corpus (ASPEC),5 containing 680k sentences pairs (18.2M Chinese and 21.8M Japanese tokens, respectively). Also, we downloaded Chinese6 (20120921) and Japanese7 (20120916) Wikipedia database dumps. We used an opensource Python script8 to extract and clean the text from the dumps. Since the Chinese dump is mixed of Traditional and Simplified Chinese, we converted all Traditional Chinese to Simplified Chinese using a conversion table pu"
L16-1468,chu-etal-2014-constructing,1,0.784656,"Missing"
L16-1468,Y15-1033,1,0.847103,"ning “+NN-ASPEC” and “+NN-WIKI” are also greatly different (680k versus 126k). As NMT is sensitive to the quality and quantity of the training data (Bahdanau et al., 2014), “+NN-ASPEC” outperformed “+NN-WIKI”. 4. Related Work Several studies have exploited the NN features for SMT. (Sutskever et al., 2014) used the NN features to rerank the N-best list of a SMT system, which achieved a BLEU score that is close to the previous state of the art. (Cho et al., 2014) scored the phrase pairs of a SMT system with a neural translation model, and used the scores as additional NN features for decoding. (Dabre et al., 2015) used the NN features for a pivot-based SMT system for dictionary construction. In contrast, we score the sentence pairs of with a neural translation model, and use the scores as NN features for parallel sentence extraction from comparable corpora. 5. Conclusion In this paper, we incorporated the NN features for parallel sentence extraction from comparable corpora for the first time. Experimental results verified the effectiveness of NN features for this task. As future work, we plan to address the domain problem of “+NN-ASPEC” by a NN based sentence selection method. Namely, we train NN model"
L16-1468,P07-2045,0,0.00628691,"Missing"
L16-1468,J05-4003,0,0.739012,"o be viewed as bilingual language models, they can be used to generate scores for candidate translations as neural network (NN) features for reranking the n-best lists produced by a statistical machine translation (SMT) system, whose quality rivals the state of the art (Sutskever et al., 2014). Comparable corpora are a set of monolingual corpora that describe roughly the same topic in different languages. Although they are not exact translation equivalents of each other, there are a large amount of parallel sentences contained in the comparable texts. The task of parallel sentence extraction (Munteanu and Marcu, 2005) is to identify truly parallel sentences from the erroneous ones from comparable corpora. Intuitively, because the NN features give a measure of the bilingual similarity of a sentence pair, they could be helpful for this task. However, this assumption has not been verified previously. In this paper, we incorporate the NN features into a robust parallel sentence extraction system (Chu et al., 2014), which consists of a parallel sentence candidate filter and a binary classifier for parallel sentence identification. The NN features are naturally used as additional features for the classifier. Exp"
N15-1125,W07-0702,0,0.0286083,"9) for a large number (over 25) of languages (other than Indian) including Japanese and Hindi (Japanese to Hindi translation being our focus) of approximately 30000 lines. We chose this setting because we feel that this multilingual approach is especially important for lowresource language pairs. Typically system combination methods like linear interpolation are used to combine the direct and pivot phrase tables by modifying the probabilities of phrase pairs leading to the modification of the underlying distribution which affects the resultant translation quality. The Multiple Decoding Paths (Birch and Osborne, 2007) (MDP) feature has been used 4 The construction of a multilingual corpus has already the benefit that each new language added to it will allow direct translation with a SMT system for N new language pairs. 1193 • Most works focus on obtaining pivot based phrase tables on relatively larger corpora than the ones used for the direct phrase table. We use the same corpora sizes for the pivot as well as direct tables. • We verify that Multiple Decoding Paths (MDP) feature of Moses is much more effective than plain linear interpolation, especially when more pivot languages are used together. • We sho"
N15-1125,P10-1137,1,0.846116,"ments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and experiments conducted. 3 Description of Languages, Corpora and Experiments We first describe the pivot languages and the corpora we use. We follow this with a description of the triangulation method which we use to construct phrase tables using the pivot languages, the methods used to combine the constructed tables and then the experiments that use them. 1194 3"
N15-1125,I13-1167,0,0.0314773,"Missing"
N15-1125,W09-0431,0,0.0211025,"as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval."
N15-1125,P07-2045,0,0.00823489,"thods by: Firstly, combining the pivot based phrase tables into a single table using equation 5 (using the ratio of BLEU scores as interpolation weights) followed by using this table to support the direct phrase table by MDP. Note that the right way would be to use the BLEU scores on the tuning set but our objective was to show that even in the best case scenario (also called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) us"
N15-1125,W07-0733,0,0.0240646,"translation system. Reordering tables are supplementary and can usually be replaced by a simple distortion model. Major problems arise when source-pivot and pivot-target corpora belong to different domains leading to rather poor quality translations. Even if the individual corpora are large, one will run into domain adaptation problems. In such a scenario the availability of a small size multilingual corpus of a few thousand lines belonging to a single domain can be beneficial. The setting of this paper is: to combine two source-target phrase tables of different domains for domain adaptation (Koehn and Schroeder, 2007) but not so extensively in a pivot language scenario, especially when multiple pivots are involved (7 in our case). Our work is different from other previous works in the following ways: • We work on a realistic low resource setting for translation between Japanese and Hindi in which we use small sized multilingual corpora containing translations of a sentence in multiple languages. 1. We suppose the existence of a multilingual corpus with sentences aligned across N 4 different languages. • We focus on the impact of using a relatively large number of pivot languages (7 to be precise) to improv"
N15-1125,W04-3250,0,0.0268447,"the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (source) to tgt (target) direct phrase table. 2. For piv in Pivot Languages Set; the set of pivot languages to be used (Tables 1 and 2): 7 By Oracle sc"
N15-1125,kunchukuttan-etal-2014-shata,1,0.889032,"Missing"
N15-1125,D09-1141,0,0.0177672,"ce this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by"
N15-1125,2001.mtsummit-papers.46,0,0.15184,"rks to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and experiments conducted. 3 Description of Languages, Corpora and Experiments We fi"
N15-1125,P03-1021,0,0.0135287,"ur objective was to show that even in the best case scenario (also called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (sour"
N15-1125,P02-1040,0,0.0937469,"so called Oracle7 scenario) this method is still inferior compared to only using the MDP method. 3.5 Descriptions of Experiments Our experiments were centered around Phrase Based SMT (PBSMT). We used the open source Moses decoder (Hoang et al., 2007) package (including Giza++) for word alignment, phrase table extraction and decoding for sentence translation. We also used the Moses scripts for linear and fillup interpolation along with the multiple decoding paths (MDP) setting (by modifying the moses.ini files). We performed MERT (Och, 2003) based tuning using the MIRA algorithm. We used BLEU (Papineni et al., 2002) as our evaluation criteria and the bootstrapping method (Koehn, 2004) for significance testing. For the sake of comparison with previous methods, we experimented with sentence translation strategy (Utiyama and Isahara, 2007) using 10 as the n-best list size for intermediate and target language translations. The experiments we performed are given below. Each experiment involves either the creation of a phrase tables or combination of phrase tables. We tune, test and evaluate these tables or combinations. 1. A src (source) to tgt (target) direct phrase table. 2. For piv in Pivot Languages Set;"
N15-1125,N09-2056,0,0.0204827,"hen combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related fi"
N15-1125,P14-2125,0,0.0121014,"s had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider English as a pivot in our experiments. None of the above works focus on the utilization and impact of more than 2 pivots in their experiments which was one of our main objectives. Related to multilingual translation are works by Habash and Hu (2009), El Kholy et al. (2013), Salloum et al. (2014) and Koehn et al. (2009). Work on multi source translation (Och and Ney, 2001) which is complementary to our work must also be noted. In the related field of information retrieval, pivot languages were employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001) (Kishida and Kando, 2003). Chinnakotla et al. (2010) retrieved feedback terms from documents written in the pivot languages (after translating back from the pivot), and augmented source queries leading to improvements in information retrieval. We now talk about the languages, corpora and ex"
N15-1125,E12-1055,0,0.131834,"ture work. 2 Related Work Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. This is one of the first works to use MDP in the pivot based SMT scenario. (Paul et al., 2013) and (Paul et al., 2009) showed that English is not the best pivot language for many language pairs, including Japanese and Hindi. This was reason enough for us to not consider Eng"
N15-1125,N07-1061,0,0.736099,"he parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages. To remedy this scenario, an intermediate resource rich language can be exploited. Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot1 ) and one between that pivot and the target language. Using the methods developed for Pivot Based SMT (Wu and Wang, 2007) (Utiyama and Isahara, 2007) one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system3 ) can be supported by the pivot based source-target system to significantly improve the translation quality. Note that in this paper we use the terms ”translation system” and ”phrase table” interchangeably since the phrase table is the We present our work on leveraging multilingual parallel corpora of small sizes f"
N15-1125,P07-1108,0,0.621386,"ted to the size of the parallel corpus, it is impossible to achieve the same level of translation quality as that in the case of resource rich languages. To remedy this scenario, an intermediate resource rich language can be exploited. Although, finding a direct parallel corpus between source and target languages might be difficult, there are higher odds of finding a pair of parallel corpora: one between the source language and an intermediate resource rich language (henceforth called pivot1 ) and one between that pivot and the target language. Using the methods developed for Pivot Based SMT (Wu and Wang, 2007) (Utiyama and Isahara, 2007) one can use the source-pivot and pivot-target parallel corpora to develop a source-target translation system (henceforth called as pivot based system 2 ) . Moreover, if there exists a small source-target parallel corpus then the resulting system (henceforth called as direct system3 ) can be supported by the pivot based source-target system to significantly improve the translation quality. Note that in this paper we use the terms ”translation system” and ”phrase table” interchangeably since the phrase table is the We present our work on leveraging multilingual paral"
N15-1125,P09-1018,0,0.362193,"airs being acquired that impact translation quality. Section 2 contains the related work. Section 3 begins with a basic description about the languages involved, followed by the corpora details and the experimental methodology. Section 4 consists of results, observations and discussions. The paper ends with conclusions and future work. 2 Related Work Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to get a source-target phrase table. They then combine the pivoted and direct tables by linear interpolation whose weights were manually specified. There is a method to automatically learn the weights (Sennrich, 2012) but it requires reference phrase pairs not easily available in resource constrained scenarios like ours. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots ha"
N15-1125,2009.mtsummit-papers.7,0,\N,Missing
P17-2061,W04-3250,0,0.160581,"43 20.01 37.66 35.79 33.74 34.61 37.57 37.23 37.77 Table 2: Domain adaptation results (BLEU-4 scores) for WIKI-CJ using ASPEC-CJ. 5 Results Tables 1 and 2 show the translation results on the Chinese-to-English and Chinese-to-Japanese tasks, respectively. The entries with SMT and NMT are the PBSMT and NMT systems, respectively; others are the different methods described in Section 3. In both tables, the numbers in bold indicate the best system and all systems that were not significantly different from the best system. The significance tests were performed using the bootstrap resampling method (Koehn, 2004) at p < 0.05. We can see that without domain adaptation, the SMT systems perform significantly better than the NMT system on the resource poor domains, i.e., IWSLT-CE and WIKI-CJ; while on the resource rich domains, i.e., NTCIR-CE and ASPECCJ, NMT outperforms SMT. Directly using the SMT/NMT models trained on the out-of-domain data to translate the in-domain data shows bad performance. With our proposed “Mixed fine tuning” domain adaptation method, NMT significantly outperforms SMT on the in-domain tasks. Comparing different domain adaptation methods, “Mixed fine tuning” shows the best perfor6"
P17-2061,D14-1179,0,0.0132915,"Missing"
P17-2061,P07-2045,0,0.00716769,"1 2.55 16.12 16.56 15.02 16.41 18.77 18.10 17.65 test 2013 14.67 7.29 4.74 2.85 17.12 17.54 15.96 16.80 18.63 17.97 17.94 average 14.31 7.87 4.33 2.60 16.41 16.34 14.97 15.82 18.01 17.43 17.11 Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE. System WIKI-CJ SMT WIKI-CJ NMT ASPEC-CJ SMT ASPEC-CJ NMT Fine tuning Multi domain Multi domain w/o tags Multi domain + Fine tuning Mixed fine tuning Mixed fine tuning w/o tags Mixed fine tuning + Fine tuning For performance comparison, we also conducted experiments on phrase based SMT (PBSMT). We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments. For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively. In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. For both MT systems, we preprocessed the data as follows. For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b). For English, we tokeniz"
P17-2061,L16-1468,1,0.872655,"Missing"
P17-2061,W16-5407,1,0.90265,"ag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain was automatically extracted (Chu et al., 2016a). We observed that “mixed fine tuning” works significantly better than methods that use fine tuning ∗ This work was done when the first author was a researcher of Japan Science and Technology Agency. 385 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 385–391 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2061 Source-Target (out-of-domain) NMT e control the politeness of NMT translations. The overview of this method is shown in the dotted section in Fig"
P17-2061,2015.iwslt-evaluation.11,0,0.147063,"em without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpu"
P17-2061,W16-4616,1,0.50204,"Missing"
P17-2061,D16-1046,0,0.0226074,"hat combines the best of existing approaches and show that it is effective. We can further fine tune the multi domain model on the in-domain data, which is named as “multi domain + fine tuning.” • To the best of our knowledge this is the first work on an empirical comparison of various domain adaptation methods. 2 3.3 Related Work The proposed mixed fine tuning method is a combination of the above methods (shown in Figure 2). The training procedure is as follows: Fine tuning has also been explored for various NLP tasks using neural networks such as sentiment analysis and paraphrase detection (Mou et al., 2016). Tag based NMT has also been shown to be effective for control the politeness of translations (Sennrich et al., 2016a) and multilingual NMT (Johnson et al., 2016). Besides fine tuning and multi domain NMT using tags, another direction of domain adaptation for NMT is using in-domain monolingual data. Either training an in-domain recurrent neural network (RNN) language model for the NMT decoder (G¨ulc¸ehre et al., 2015) or generating synthetic data by back translating target in-domain monolingual data (Sennrich et al., 2016b) have been studied. 3 1. Train an NMT model on out-of-domain data till"
P17-2061,W15-5001,1,0.85268,"Missing"
P17-2061,Q17-1024,0,0.0315585,"Missing"
P17-2061,P03-1021,0,0.0134846,"i domain w/o tags Multi domain + Fine tuning Mixed fine tuning Mixed fine tuning w/o tags Mixed fine tuning + Fine tuning For performance comparison, we also conducted experiments on phrase based SMT (PBSMT). We used the Moses PBSMT system (Koehn et al., 2007) for all of our MT experiments. For the respective tasks, we trained 5-gram language models on the target side of the training data using the KenLM toolkit6 with interpolated KneserNey discounting, respectively. In all of our experiments, we used the GIZA++ toolkit7 for word alignment; tuning was performed by minimum error rate training (Och, 2003), and it was re-run for every experiment. For both MT systems, we preprocessed the data as follows. For Chinese, we used KyotoMorph8 for segmentation, which was trained on the CTB version 5 (CTB5) and SCTB (Chu et al., 2016b). For English, we tokenized and lowercased the sentences using the tokenizer.perl script in Moses. Japanese was segmented using JUMAN9 (Kurohashi et al., 1994). For NMT, we further split the words into subwords using byte pair encoding (BPE) (Sennrich et al., 2016c), which has been shown to be effective for the rare word problem in NMT. Another motivation of using sub-word"
P17-2061,N16-1005,0,0.0989293,"Missing"
P17-2061,P16-1009,0,0.516582,"al with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain"
P17-2061,P16-1162,0,0.519258,"al with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted resource poor corpus (Chinese-to-Japanese translation): Using the ASPEC data (scientific domain; resource rich) (Nakazawa et al., 2016) to improve the translation quality for the Wiki data (resource poor). The parallel corpus of the latter domain"
P17-2061,D16-1163,0,0.0622938,"shortcomings. 1 Introduction One of the most attractive features of neural machine translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. However, it is reported that NMT works better than SMT only when there is an abundance of parallel corpora. In the case of low resource domains, vanilla NMT is either worse than or comparable to SMT (Zoph et al., 2016). Domain adaptation has been shown to be effective for low resource NMT. The conventional domain adaptation method is fine tuning, in which an out-of-domain model is further trained on indomain data (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016). However, fine tuning • Manually created resource poor corpus (Chinese-to-English translation): Using the NTCIR data (patent domain; resource rich) (Goto et al., 2013) to improve the translation quality for the IWSLT data (TED talks; resource poor) (Cettolo et al., 2015). • Automatically extracted r"
P17-2061,W14-7001,1,\N,Missing
P17-2061,2015.iwslt-evaluation.1,0,\N,Missing
W14-0126,N13-1088,1,0.885924,"Missing"
W14-0126,N03-1032,0,0.0718943,"Missing"
W14-0126,C12-3033,1,\N,Missing
W14-5103,W10-3604,1,0.760804,"corhttp://en.wikipedia.org/wiki/Hindi 3 pora, translation suffers from data pages sparsity. Also, Dhttp://en.wikipedia.org/wiki/Eighth_Schedule_to_the_Constitution S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, 11–19, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) the translation of morphemes does not merely involve independent dictionary substitution but require looking at neighboring morphemes. An important aspect of our work involved handling of participial forms known as Krudantas and Akhyatas (Bhosale et al, 2011; Bapat et al. 2010) which are derivatives of verbs. Consider: “मी धावलयानंतर असलेला वयायाम करत sentence but Marathi can place this either before or after. The sentence “He says that he comes home at 8” is written in Hindi as: “वह कहता है की वह आठ बजे घर आता है ” {vaha kahta hai ki vaha aath baje ghar aata hai} but in Marathi as “तो घरी आठ वाजता येतो असे तो महणतो” {to ghari aath vaj-ta ye-to ase to mhana-to} or “तो आहे ” {mi dhava-lya-nantar asa-le-la vyayam kaमहणतो की तो आठ वाजता घरी येतो” {to mhana-to ra-ta aahe} {I am doing the exercises that come after running} in which “धावलयानंतर” and ki to aath vaj-ta"
W14-5103,D07-1091,0,0.0273067,"Missing"
W14-5103,P02-1040,0,0.106348,"Missing"
W14-5103,W06-3102,0,0.0677239,"Missing"
W14-5113,D07-1091,0,0.0395601,"Missing"
W14-5113,P02-1040,0,0.0937335,"is because French is much closer to Creole than English. 3.5 Testing and Results For the purpose of testing an additional 142 sentence triplets were created, one each for English, French and Creole. These are sentences with more than 10 words per sentence on an average. The number of OOV’s was more in these sentences. Extra 142 simple (short) sentence pairs for English and Creole were created to verify our hypothesis mentioned above. These contained an average of 5 words per sentence with relatively lesser number of OOV’s compared to the earlier sentences. We evaluated the quality using BLEU (Papineni et al. 2002). The scores are given in Table 4 below. In the table “easy” indicates that the simple (short) sentence pairs were used for testing whereas “hard” indicates that the longer ones were used for testing. For the Creole to English, Creole to French and French to Creole systems the pivot language mechanism was not applicable since effective pivot languages were not available. Language Pair En-Cr (direct, hard) En-Cr (direct, easy) En-Cr (bridge (N=1), hard) En-Cr (bridge (N=10), hard) En-Cr (bridge (N=1), easy) En-Cr (bridge (N=10), easy) Cr-En (direct, hard) Cr-En (direct, easy) Fr-Cr (direct, har"
W14-5113,2008.iwslt-papers.1,0,0.019091,"Proc. of the 11th Intl. Conference on Natural Language Processing, pages 82–88, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 1.1 Related Work Sukhoo et al. (2014) had developed a basic English Creole SMT system with a very small amount of parallel corpus (10000-13000 lines including many dictionary words). They manage to get simple sentences translated with reasonable quality but fail when longer sentences (more than five words) are tested. This corpus, after considerable augmentation was used in our experiments. Significant work has been done in using bridge languages (Bertoldi et al. (2008), Utiyama et al. (2007)) to improve translation quality. In many of these works, they develop translation systems from languages A to C using B, where A-B and B-C are resource rich, by either synthesizing new phrase tables or modifying existing corpora. In our case, we had a huge English French corpus but a very small French Creole corpus. Moreover no linguistic processing modules for Creole exist, which would help in reducing data sparsity. Thus we decided to adopt the “transfer method” described by Wu et al. (2009). The remainder of the paper involves a chapter on Linguistic phenomena in Mau"
W14-5113,N07-1061,0,0.0225958,"f RAM. The services for each language pair are hosted using mosesserver. The EnglishFrench system took the maximum training time; a total of 24 hours, including the time for tuning. All the other systems took around 5 minutes of training. Due to lack of corpora we did not perform tuning for these systems. For each system the target side corpus was used for generating the language model. For creole we used the monolingual corpus for language modelling. Tuning was done using MERT. 3.4 Using French as a bridge language We used the “transfer method” or “sentence translation strategy”, proposed by Utiyama and Isahara (2007) and described by Wu and Wang (2009), to translate from English to Creole using French; wherein the first translated from English to French using the En-Fr system and then from French to Creole using the Fr-Cr system. This method is only applicable when either both the En-Fr and Fr-Cr systems or only the En-Fr system is of high quality. The main idea is that French is close to Creole and thus an SMT system built on a small corpus would suffice for good translations. As long as the English-French SMT system gives good translations the resulting Creole translations can be expected to be good. Th"
W14-5113,P09-1018,0,0.0221165,"re hosted using mosesserver. The EnglishFrench system took the maximum training time; a total of 24 hours, including the time for tuning. All the other systems took around 5 minutes of training. Due to lack of corpora we did not perform tuning for these systems. For each system the target side corpus was used for generating the language model. For creole we used the monolingual corpus for language modelling. Tuning was done using MERT. 3.4 Using French as a bridge language We used the “transfer method” or “sentence translation strategy”, proposed by Utiyama and Isahara (2007) and described by Wu and Wang (2009), to translate from English to Creole using French; wherein the first translated from English to French using the En-Fr system and then from French to Creole using the Fr-Cr system. This method is only applicable when either both the En-Fr and Fr-Cr systems or only the En-Fr system is of high quality. The main idea is that French is close to Creole and thus an SMT system built on a small corpus would suffice for good translations. As long as the English-French SMT system gives good translations the resulting Creole translations can be expected to be good. The Moses decoder allows for n-best tr"
W14-5113,J93-2003,0,\N,Missing
W14-5113,P07-2045,0,\N,Missing
W14-5113,N03-1017,0,\N,Missing
W14-5126,W04-2215,0,0.0323745,"ously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP tasks such as POS tagging and chunking, but not much work has gone in the direction of corpora generation aid for Machine Translation (MT). The few similar works that we did find are noted below. PolyPhraZ (Hajlaoui and Boitet, 2004) is one such tool which helps in visualizing, editing and evaluating MT systems on parallel corpora. CasualConc (Imao, 2008) is a parallel concordancer which generates keyword in context concordance lines, word clusters, collocation analysis, and word counts. MemoQ (Kilgray, 2006) and Trados (SDL, 2007) are also Computer Aided Translation (CAT) D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 162–166, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen syst"
W14-5126,P07-2045,0,0.00501907,"to ensure time and cost effectiveness of the process. Traditional method of parallel corpus creation 162 involves manual translation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text"
W14-5126,J03-1002,0,0.00655842,"slation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP ta"
W14-5126,E03-1016,0,0.0455033,"Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen systems which are commercially available with features like Translation memory, and Term Extraction. Wordfast is CAT system having just one free version “WordFast Anywhere”. We studied and used the system, but found the interface less intuitive, and hard to use. “WordFast Anywhere” also has an integrated MT system which provides translations via Microsoft Bing and an integrated MT system. Another system we came across is a web based text corpora development system (Yablonsky, 2003) that focuses on the development of UML-specifications, architecture and implementations of DBMS tools. None of the above mentioned systems provide a word alignment visualization, which can be corrected manually, and saved to provide perfect phrase tables later. 3 Parallel Corpora Management System Parallel Corpora Management System (PaCMan) (Figure: 1) is a platform-independent web-based workbench for managing all the processes involved in the generation of good quality parallel corpora. Along with covering the procedural / managerial aspects of the parallel corpora generation process, this t"
W14-5126,E03-1063,0,\N,Missing
W14-5126,kunchukuttan-etal-2014-shata,1,\N,Missing
W15-5006,P05-1022,0,0.129705,"Missing"
W15-5006,N12-1047,0,0.0279359,"2014), which makes use of our dependency parses in order to capture non-local reorderings. 3.2 • Forest parse scores • Number of content/function words aligned to content/function words • Number of times a subtree is inserted in a position (left or right of parent) that is not the most common in the training data • Number of examples sharing the same information used to create an initial hypothesis • Similarity between source and input word embeddings (Mikolov et al., 2013) Forest Input The optimal weights for each feature are as before estimated using the implementation of k-best batch MIRA (Cherry and Foster, 2012) included in Moses. We found that the quality of the source-side dependency parsing had an important impact 3 http://kheaﬁeld.com/code/kenlm/ 57 3.4 Reranking translation in the n-best list. These features were added to those used in the ﬁrst round of tuning, then one ﬁnal iteration of tuning was run. The tuning algorithm and settings were the same as for standard tuning. This retuning step was added in order to ﬁnd an optimal combination of the additional features with related features such as sentence length and the score given by the 5-gram language model used inside the decoder. A ﬁnal rer"
W15-5006,J07-2003,0,0.0620462,"X2, morphological forms of “be”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the"
W15-5006,D11-1047,1,0.850442,"view Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and sco"
W15-5006,C12-1120,1,0.883803,"Missing"
W15-5006,P14-2024,0,0.0236533,"ble combination of hypotheses. This linear model is based on a linear combination of both local features (local to each translation hypothesis) and non-local features (such as a 5-gram language model score of the ﬁnal translation). Despite our already relatively large set of dense features, we found there were a number of cases where these features were not enough to diﬀerentiate between good and bad translation hypotheses. This year we have added ten new features, now reaching a total of 52, a selection of which are shown below: Improvements from WAT2014 3.1 Alignment Based on the ﬁndings of Neubig and Duh (2014), we experimented with supervised alignment using Nile (Riesa et al., 2011) as part of our translation framework. We found that using supervised alignments made a considerable improvement to translation quality. Since Nile supports only constituency parses, we also perform constituency parsing for source and target languages for generating bidirectional word alignments. For the initial alignments for Nile, we use the alignments generated from the model described in last year’s system description (Richardson et al., 2014), which makes use of our dependency parses in order to capture non-local r"
W15-5006,W11-2123,0,0.0137398,"”, and the optional insertion of “at”. simple if i, s and t have the same structure and are perfectly aligned, but again this is not typically the case. A consequence is that we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-term"
W15-5006,N15-3009,0,0.0231441,"Missing"
W15-5006,2011.iwslt-evaluation.24,0,0.0153808,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,P06-1055,0,0.256442,"Missing"
W15-5006,D12-1107,0,0.0117256,"n-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the decoder to make choices between various morphological variations of a wo"
W15-5006,P05-1034,0,0.166664,"Missing"
W15-5006,N06-1023,1,0.726271,"Missing"
W15-5006,W08-0402,0,0.0128267,"hat we will sometimes have several possible insertion positions for each non-terminal. The choice of insertion position is again made during combination. 1 The score of a combination will be the sum of the local scores of each translation hypothesis. 2 H = set of translation hypotheses 56 will force us to prune the search space. This pruning is done eﬃciently through a variation of cube-pruning (Chiang, 2007). We use KenLM3 (Heaﬁeld, 2011) for computing the target language model score. Decoding is made more eﬃcient by using some of the more advanced features of KenLM such as state-reduction ((Li and Khudanpur, 2008), (Heaﬁeld et al., 2011)) and rest-cost estimations(Heaﬁeld et al., 2012). Compared with the original cube-pruning algorithm, our decoder is designed to handle an arbitrary number of non-terminals. In addition, as we have seen in Section 2.1, the translation hypotheses we initially extract from examples are ambiguous in term of which target word is going to be used and which will be the ﬁnal position of each non-terminal. In order to handle such ambiguities, we use a lattice-based internal representation that can encode them eﬃciently (see Figure 3). This lattice representation also allows the"
W15-5006,D11-1046,0,0.0924047,"er of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full context of the example to assign features and scores to each translation hypothesis. The main drawback of our approach is that it can be computationally more expensive to retrieve arbitrarily large matchings in the example database online than it is to match precomputed rules. We use the techniques described in Cromières and Kurohashi (2011) to perform this step as eﬃciently as possible. Once we have found an example translation (s, t) for which s partially matches i, we proceed to extract a translation hypothesis from it. A translation hypothesis is deﬁned as a generic translation rule for a part p of the input sentence that is represented as a targetlanguage treelet, with non-terminals representing the insertion positions for the translations of other parts of the sentence. A translation hypothesis is created from a translation example as follows: Figure 1: The translation pipeline can be roughly divided in 3 steps. Step 1 is t"
W15-5006,Y12-1033,1,0.720835,"Missing"
W15-5006,D07-1104,0,0.0236985,"g. We believe that 2 System Overview Figure 1 shows the basic structure of the KyotoEBMT translation pipeline. The training process begins with parsing and aligning parallel sentences from the training corpus. The alignments are then used to build an example database (‘translation mem54 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 54‒60, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). more faithful to the original EBMT approach advocated by Nagao (1984). It has already been proposed for phrase-based (CallisonBurch et al., 2005), hierarchical (Lopez, 2007), and syntax-based (Cromières and Kurohashi, 2011) systems. It does not however, seem to be very commonly integrated in syntax-based MT. This approach has several beneﬁts. The ﬁrst is that we are not required to impose a limit on the size of translation hypotheses. Systems extracting rules in advance typically restrict the size and number of extracted rules for fear of becoming unmanageable. In particular, if an input sentence is the same or very similar to one of our translation examples, we will be able to retrieve a perfect translation. A second advantage is that we can make use of the full"
W15-5944,H05-1085,0,0.0339948,"similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) the limitation of small sized corpora.Paul et al. (2013) discusses criteria to be considered for selection of good pivot language. Use of sourceside segmentation as pre-processing technique has been demonstrated by (Kunchukuttan et al., 2014). Goldwater and McClosky (2005) investigates several methods for incorporating morphological information to achieve better translation from Czech to English. Most of the pivot strategies mentioned above focus on the situation of resource-poor languages where direct translation is either very poor or not available. Our approach, like Dabre et al. (2014), tries to employ pivot strategy to help improve the performance of existing SMT systems. To the best of our knowledge, our work is the first attempt to integrate word segmentation with pivot-based SMT. 3 Our System We propose a system which integrates word segmentation with t"
W15-5944,D07-1091,0,0.0548085,"orpus Pivot-Target Corpus Morfessor SourceMorphPivotMorph PivotMorphTarget SMT Training SMT Training SourceMorphPivotMorph Phrase table PivotMorphTarget Phrase table Triangulation SourceMorphTarget Phrase table Tune And Test Figure 1: Integration of word segmentation with triangulation 3.4 Multiple Decoding Paths We use the triangulated phrase table to supplement the direct phrase table. In order to integrate these two phrase tables, we use the multiple decoding paths (MDP) feature provided by the 1 https://github.com/anoopkunchukuttan/indic_nlp_library Moses decoder. Multiple decoding paths (Koehn and Hoang, 2007) allows us to lookup multiple translation models for hypothesis at decoding time, and the choice of best hypothesis at decoding time based on available evidence. We use MDP to combine one or more pivot-based MT systems with the direct MT system. This constitutes our final decoding system. We preferred this option over offline linear interpolation of phrase tables since the framework can dynamically consider phrases from multiple phrase tables and wouldn’t need any hyperparameter tuning. 4 Experiments The aim of experiments is to study impact of pivot strategies and word segmentation, separatel"
W15-5944,N03-1017,0,0.0188284,"ively. Using these two models, we induce a source-target model. The two important components to be calculated are - 1) phrase translation probability and 2) lexical weight. The Phrase translation probability is estimated by marginalizing over all possible pivot phrase, along with the assumption that the target phrases are independent of the source phrase given the pivot phrase. The phrase translation probability can be calculated as shown below: ( ) ϕ s∥t = ∑ p  ( ) ϕ (s∥ p) ϕ p∥t (1) Where, s, p, t are phrases in languages Ls , Lp , Lt respectively. The Lexical Weight, according to Koehn et al. (2003), depends on - 1) word alignment information a in a phrase pair (s, t) and 2) lexical translation probability w(s|t). 304 Lexical weight can be modeled using following equation, ( ) pw f∥e, a = n ∏ ∑ 1 w (fi ∥ej ) ∥j |(i, j) ∈ a∥ ∀(i,j)∈a i=1 (2) Wu and Wang (2009) discuss in detail about alignments information and lexical translation probability. 3.2 Word segmentation We use unsupervised word segmentation as preprocessing technique. For this purpose, Morfessor (Virpioja et al., 2013) is used. It performs morphological segmentation of words of a natural language, based solely on raw text dat"
W15-5944,P02-1040,0,0.0925006,"g, 500 sentences are used for tuning and 2000 sentences are used for testing. For the experiments, we use phrase-based SMT training and 5-gram SRILM language model. Tuning is done using the MERT algorithm. The triangulated MT systems use default distance based reordering while direct systems use wbe-msdbidirectional-fe-allff model 4.2 Experimental Setup We trained various phrase based SMT systems by combining the basic systems mentioned in Section 3. We use a threshold of 0.001 for phrase translation probability to manage size of triangulated phrase table. The performance metric used is BLEU (Papineni et al., 2002). The following are the configurations we experimented with: 1. DIR: MT system trained on direct SourceTarget corpus. 2. DIR_Morph: DIR system with source-text word-segmented. 3. PIVOT: MT system based on triangulated phrase table of Source-Target using a single 305 Pivot language. 4. PIVOT_Morph: PIVOT system with both Source and Pivot texts segmented. 5. PIVOT_SourceMorph: PIVOT system with only Source text segmented. 6. DIR+PIVOT: MT system based on integration of DIR and PIVOT phrase tables using MDP. 7. DIR_Morph+PIVOT_Morph: MT system based on integration of DIR_Morph and PIVOT_Morph usi"
W15-5944,N07-1061,0,0.0200742,"t work done in the field. Section 3 explains the design of our system in detail. Section 4 describes the experimental setup. Results of the experiments are discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task. 2 Related Work There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English. In Utiyama and Isahara (2007), the authors propose the use of pivot language through - phrase translation (phrase table creation) and sentence translation. Wu and Wang (2007) compare three pivot strategies viz. - phrase translation (i.e. triangulation), transfer method and synthetic method. Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, Ind"
W15-5944,P07-1108,0,0.453884,"re discussed in Section 5. Section 6 includes concluding remarks on the mal-hin translation task. 2 Related Work There is substantial amount on pivot-based SMT. De Gispert and Marino (2006) discuss translation tasks between Catalan and English with the use of Spanish as a pivot language. Pivoting is done using two techniques: pipelining of source-pivot and pivot-target SMT systems and direct translation using a synthesized Catalan-English. In Utiyama and Isahara (2007), the authors propose the use of pivot language through - phrase translation (phrase table creation) and sentence translation. Wu and Wang (2007) compare three pivot strategies viz. - phrase translation (i.e. triangulation), transfer method and synthetic method. Nakov and Ng (2012) try to exploit the similarity between resource-poor languages and resource-rich languages for the translation task. Dabre et al. (2014) used multiple decoding paths (MDP) to overcome D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 303–307, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) the limitation of small sized corpora.Paul et al. (2013) discusses criteria to be consid"
W15-5944,P09-1018,0,0.0175912,", along with the assumption that the target phrases are independent of the source phrase given the pivot phrase. The phrase translation probability can be calculated as shown below: ( ) ϕ s∥t = ∑ p  ( ) ϕ (s∥ p) ϕ p∥t (1) Where, s, p, t are phrases in languages Ls , Lp , Lt respectively. The Lexical Weight, according to Koehn et al. (2003), depends on - 1) word alignment information a in a phrase pair (s, t) and 2) lexical translation probability w(s|t). 304 Lexical weight can be modeled using following equation, ( ) pw f∥e, a = n ∏ ∑ 1 w (fi ∥ej ) ∥j |(i, j) ∈ a∥ ∀(i,j)∈a i=1 (2) Wu and Wang (2009) discuss in detail about alignments information and lexical translation probability. 3.2 Word segmentation We use unsupervised word segmentation as preprocessing technique. For this purpose, Morfessor (Virpioja et al., 2013) is used. It performs morphological segmentation of words of a natural language, based solely on raw text data. Morfessor uses probabilistic machine learning methods to do the task of segmentation. The trained models for word segmentation of Indian languages are available to use1 . 3.3 Integrating word segmentation with Triangulation In our system, we use both phrase table"
W16-2349,P14-1129,0,0.0953135,"Missing"
W16-2349,P07-2045,0,0.00486754,"act that are able to beat the state of art systems by a reasonable margin. (Mikolov et al., 2010) showed that the word embeddings obtained using a simple feed-forward neural network give better results for word similarity tasks compared to those given by the embeddings obtained using GLOVE(Pennington et al., 2014). Furthermore, (Devlin et al., 2014) have shown that using a Neural Network based Lexical Translation Model can help boost the quality of Statistical Machine Translation. (Bahdanau et al., 2014) showed that it is possible to perform end to end MT whose quality surpasses that of Moses(Koehn et al., 2007) by using a combination of Recurrent Neural Networks (RNNs) and dictionary based unknown word substitution. In particular we wanted to test the capabilities of Recurrent Neural Networks augmented with an Attention Based Mechanism for this task. They are easy to design, implement and test due to the availability of NN frameworks like Chainer 1 , Torch 2 , Tensorflow 3 etc. Since Chainer provides a lot of useful functionality and enables rapid prototyping we decided to use it to implement our system. In this paper we describe our system we designed and implemented for the crosslingual pronoun pr"
W16-2349,D14-1162,0,0.0853875,"oss-Lingual Pronoun Translation System Raj Dabre Yevgeniy Puzikov Graduate School of Informatics Graduate School of Informatics Kyoto University, Japan Kyoto University, Japan prajdabre@gmail.com puzikov@nlp.ist.i.kyoto-u.ac.jp Fabien Cromieres JST, Japan Kyoto University fabien@nlp.ist.i.kyoto-u.ac.jp Abstract that are able to beat the state of art systems by a reasonable margin. (Mikolov et al., 2010) showed that the word embeddings obtained using a simple feed-forward neural network give better results for word similarity tasks compared to those given by the embeddings obtained using GLOVE(Pennington et al., 2014). Furthermore, (Devlin et al., 2014) have shown that using a Neural Network based Lexical Translation Model can help boost the quality of Statistical Machine Translation. (Bahdanau et al., 2014) showed that it is possible to perform end to end MT whose quality surpasses that of Moses(Koehn et al., 2007) by using a combination of Recurrent Neural Networks (RNNs) and dictionary based unknown word substitution. In particular we wanted to test the capabilities of Recurrent Neural Networks augmented with an Attention Based Mechanism for this task. They are easy to design, implement and test due to"
W16-2349,W15-2501,0,\N,Missing
W16-2349,W16-2345,0,\N,Missing
W17-5714,C16-2064,1,0.872421,"Missing"
W17-5714,W16-4616,1,0.879551,"Missing"
W17-5714,W17-3206,0,0.0819212,". Which is why all results presented in this paper are related to the LSTM-based model. Such feed-forward models probably have high potentials for the future, as they are more computationally efficient and do obtain state-of-the-art results on certain language directions. But, currently, we do not find that they should be necessarily preferred to recurrent architectures. 2.2 Direct connection from previous word to attention model There is an interesting flaw in the original architecture of the model (as well as in the model described in (Bahdanau et al., 2015)). This is briefly mentioned3 in (Goto and Tanaka, 2017), but we will expand on the details a bit more here. The attention mechanism computes the current context using only the previous decoder state as input. But the previous decoder state has been itself computed before the previously generated target word was selected. Therefore, when computing the current context, the attention mechanism is totally unaware of the previously generated word. Intuitively, this seems wrong: the attention should certainly depend on the previously generated word. Therefore, we add another input to the attention model: the previous word embedding. To be precise, re-us"
W17-5714,W11-2123,0,0.0728444,"Missing"
W17-5714,W16-2316,0,0.0291316,"semble, having 3 layers on the encoder and 2 on the decoder. 3.5 Averaging and Ensembling It is well known that using an ensemble of several independently trained models can boost NMT performances by several BLEU points. We did this in the same way as was described in (Cromi`eres et al., 2016). On top of ensembling independently trained models, we had found it useful to also make an ensemble with the parameters of the same model corresponding respectively to the best loss, best dev BLEU and last obtained during the training process (a practice which we will call here selfensemble). Following (Junczys-Dowmunt et al., 2016), we tried to compute averaged parameters instead of ensembling models. We found this to work surprisingly well. We observed only nonsignificant BLEU drops (by about 0.1 BLEU). But with the benefit that the averaged model has the same time and space complexity as a single model, while an ensemble of N models has N times the time and space complexity of a single model. We therefore switched to this averaging approach instead of the self-ensemble approach10 . 4 Zh → Ja Submission 1 corresponds to an ensemble of 5 models, three of them having 2 layers for encoders and decoders, and two of them ha"
W17-5714,P15-1002,0,0.0345332,"or sizes shown here are the ones suggested in the original paper. We use this general architecture for our model, but the single LSTMs are replaced by stacks of LSTMs. We also add a connection from the target embedding to the attention model, as suggested by (Goto and Tanaka, 2017), which was not in the original model (see section 2.2) We used subword segmentation for all target languages, so as to reduce the target vocabulary size. This makes the translation process more efficient memory-wise and computation-wise, while mostly avoiding the need for unknown-word replacement tricks such as in (Luong et al., 2015). The subword segmentation was done using the BPE algorithm (Sennrich et al., 2015) 6 . For the Japanese-Chinese language pair, we learned a joint segmentation (as suggested in (Sennrich et al., 2015)). We used a character equivalence map (Chu et al., 2013) to maximize the number of common characters between Japanese and Chinese when learning the joint segmentation. The joint segmentation was aimed at producing a vocabulary size of about 40,000 words for both the source and target vocabulary. For the Japanese-English language pair, we did not use a joint segmentation. We created a BPE model of"
W17-5714,W16-4601,1,0.753152,"ne for NMT. Our Kyoto-NMT system largely relies on an implementation of this model, with small modifications. Kyoto-NMT is implemented using the Chainer1 toolkit (Tokui et al., 2015). We make this implementation available under a GPL license.2 Introduction This paper describes our experiments for the WAT 2017 shared translation task. For more details refer to the overview paper (Nakazawa et al., 2017). This translation task contains several subtasks, but we focused on the ASPEC dataset, for the Japanese-English and Japanese-Chinese language pairs. Following up on our findings during WAT 2016 (Nakazawa et al., 2016) that our Neural Machine Translation system yielded significantly better results than our Example-Based Machine Translation system, we only experimented with NMT this year. Our improvements are actually quite incremental, with only small changes in the model architectures, model sizes, training and decoding approaches. Together, these small changes, however, allow us to improve over our past year’s results by several BLEU points, leading to the best official results for the Japanese-Chinese pair. In terms of pairwise human evaluation scores we have the best official results for all language di"
W17-5714,C16-1029,1,0.799847,"of our models, as well as the pre-processing we applied to the data. where eij is the unnormalized attention coefficient on source word j when decoding target word at step i, si−1 is the decoder state at step i − 1, and hj is the encoding of source word j. The matrices Wa and Ua , and the vector va are the parameters of the alignment model. We replace this equation with: 3.1 Preprocessing As a first preprocessing step, English sentences were tokenized and lowercased. Both Japanese sentences and Chinese sentences were automatically segmented, respectively with JUMAN5 (Kurohashi, 1994) and SKP (Shen et al., 2016). eij = vaT tanh(Wa ·si−1 +Ua ·hj +Xa ·Ey−1 ) (2) 4 3 tion The author had previously mentioned this to us in private communications. 5 147 but see section 4.2.1 for our attempt at system combinahttp://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN Figure 1: The structure of a NMT system with attention, as described in (Bahdanau et al., 2015) (but with LSTMs instead of GRUs). The notation “<1000>” means a vector of size 1000. The vector sizes shown here are the ones suggested in the original paper. We use this general architecture for our model, but the single LSTMs are replaced by stacks of LSTMs."
W19-5313,P18-4020,0,0.0305142,"Missing"
W19-5313,P07-2045,0,0.0106799,"Missing"
W19-5313,W18-6419,1,0.860496,"algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a equal contribution 168 Proceedings of the Fourth Conference on Machine Translation (WMT), Volu"
W19-5313,P02-1040,0,0.103864,"was set to 2048. The number of attention heads in each encoder and decoder layer was set to eight. During training, the value of label smoothing was set to 0.1, and the attention dropout and residual dropout were set to 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warm-up steps of 16,000. All NMT models for ZH↔EN tasks were consistently trained on four P100 GPUs. We validated the model with an interval of 5,000 batches on the development set and selected the best model according to BLEU (Papineni et al., 2002) score on the newsdev2018 data set. We performed the following training run independently for five times to obtain the models for ensembling. First, an initial model was trained on the provided parallel data and used to generate pseudo-parallel data through back-translation. A new model was then trained from scratch on the mixture of the original parallel data and the pseudo-parallel data. The new model was further 12 13 Results Table 2 shows the results of ZH↔EN tasks. It is obvious that the back-translation, fine-tuning, and ensemble methods are greatly effective for the ZH↔EN tasks. In part"
W19-5313,Y17-1038,1,0.807473,"ansfer Learning In addition to the approaches in Section 3.1, we also use fine-tuning for transfer learning. Zoph et al. (2016) proposed to train a robust L3→L1 parent model using a large L3–L1 parallel corpus and then fine-tune it on a small L2–L1 corpus to obtain a robust L2→L1 child model. The underlying assumption is that the pre-trained L3→L1 model contains prior probabilities for translation into L1. The prior information is divided into two parts: language modeling information (strong prior) and cross-lingual information (weak or strong depending on the relationship between L3 and L2). Dabre et al. (2017) have shown that linguistically similar L3 and L2 allow for better transfer learning. As such, we transliterate L3 to L2 before pre-training a parent model. This could help in faster convergence, ensure cognate overlap, and potentially lead to a better translation quality. In this participation, we used Hindi as the helping language, L3. Results Refer to rows 1 and 2 of Table 1 for the various automatic evaluation scores. For Kazakh→English our submitted system achieved a cased BLEU score of 26.2 placing our system at 3rd rank out of 9 primary systems. On the other hand, our English→Kazakh per"
W19-5313,P16-1009,0,0.456958,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,N16-1101,0,0.279052,", 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transfo"
W19-5313,P16-1162,0,0.806805,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,D16-1163,0,0.143248,"English corpus. Chinese↔English translation can benefit from back-translation, model ensembling, and fine-tuning based on the development data. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which l"
W19-5362,D14-1179,0,0.0698778,"Missing"
W19-5362,P17-2061,1,0.912568,"English text that be leveraged for this purpose. In this paper, we describe the systems for Japanese↔English translation, that we developed and submitted for WMT 2019 under the team name “NICT”. In particular our observations can be summarized as follows: In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest. 1 Japanese↔English translation dramatically fails given the limited amount of noisy training data. Fine-Tuning is simple but has over-fitting risks. Mixed-Fine-Tuning is a simple but effective way of performing domain adaptation via fine tuning where one does not have to worry"
W19-5362,D16-1163,0,0.164342,"but effective way of performing domain adaptation via fine tuning where one does not have to worry about the possibility of quick over-fitting. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models that are robust Kindly refer to the task overview paper (Li et al., 2019) for additional details a"
W19-5362,Q17-1024,0,0.078108,"Missing"
W19-5362,P07-2045,0,0.00751066,"isy training data. Fine-Tuning is simple but has over-fitting risks. Mixed-Fine-Tuning is a simple but effective way of performing domain adaptation via fine tuning where one does not have to worry about the possibility of quick over-fitting. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models"
W19-5362,W19-5303,0,0.0403835,"in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models that are robust Kindly refer to the task overview paper (Li et al., 2019) for additional details about the task, an analysis of the results and comparisons of all submitted systems which we do not include in this paper. 2 Approaches We used domain adaptation approaches on top of the transformer model. 2.1 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural model that consists of two components: the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors. The decoder, on the other hand, produces the target word sequence by predicti"
W19-5362,D18-1050,0,0.423737,"atistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). One such resource-poor scenario is the translation of noisy sentences which are often found on social media like Reddit, Facebook, Twitter etc. There are two main problems: (a) The type of noise (spelling mistakes, code switching, random characters, emojis) in the text is unpredictable (b) Scarcity of training data to capture all noise phenomena. One of the first works on dealing with noisy translation led to the development of the MTNT (Michel and Neubig, 2018) test suite for testing MT models that are robust Kindly refer to the task overview paper (Li et al., 2019) for additional details about the task, an analysis of the results and comparisons of all submitted systems which we do not include in this paper. 2 Approaches We used domain adaptation approaches on top of the transformer model. 2.1 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-to-sequence neural model that consists of two components: the encoder and the decoder. The encoder converts the input word sequenc"
W19-5362,P11-2093,0,0.0312858,"Computational Linguistics input sequence representations. The reader is encouraged to read the original paper (Vaswani et al., 2017) for a deeper understanding. 2.2 avoid out-of-memory errors. We tried a naive paragraph splitting method where we split a paragraphs into sentences and keep the splits if there are an equal number of sentences. Upon manual investigation we found out that this splitting leads to correct splits most of the times. As a result, the number of training sentences for English→Japanese translation increases to 10,060 pairs. We pre-processed the Japanese text using KyTea (Neubig et al., 2011). Other than this, we do not perform any pre-processing. Mixed Fine Tuning for Domain Adaptation The fastest way to adapt an out-of-domain model to an in-domain task is to first train a L1→L2 model on the large out-of-domain data and then fine tune it on the small in-domain data. However, given that NMT models overfit quickly on small data (Zoph et al., 2016), it is important to consider learning rate modification, regularization and sophisticated training schedules. All this can be avoided by performing Mixed-Fine-Tuning (MFT) (Chu et al., 2017) where the out-of-domain model is fine-tuned on"
W19-5362,W18-6319,0,0.0242931,"io thereby ensuring that the model sees an equal number of training examples from both domains. 2 https://github.com/tensorflow/ tensor2tensor 534 Task BLEU BLEU cased IGNORE BLEU (11b) English→Japanese Japanese→English 11.1 8.1 11.1 7.4 11.1 8.1 IGNORE BLEU-cased (11b) 11.1 7.4 IGNORE BLEU-cased-norm BEER 2.0 11.1 7.8 0.354 0.352 Table 1: Results for Japanese↔English translation for the robustness task. Approach Bidirectional FT Bidirectional MFT Ja→En 9.6 9.2 En→Ja 10.5 13.4 the training data. We did not use this test set for training or tuning. The BLEU scores are obtained using SacreBLEU (Post, 2018). We can see that while the performance of Japanese to English slightly degrades (not statistically significant), English to Japanese translation improves by approximately 2 BLEU points. As such MFT is either comparable to or significantly better than regular fine-tuning and was the reason why we chose it for the final submission. Table 2: BLEU scores on the non-blind test set for Japanese–English translation. We show that MFT is either comparable to or significantly better than regular fine-tuning. because the model had converged sufficiently by 150,000 iterations. We then used this model to"
W19-5428,P18-1073,0,0.149925,"-final-and heuristic. We also trained MSLR (monotone, swap, discontinuousleft, discontinuous-right) lexicalized reordering model. We trained one 4-gram language models on the entire monolingual data concatenated to the target side of the parallel data using LMPLZ 1 https://marian-nmt.github.io/, 1.6.0 2 NVIDIA® Tesla® P100 16Gb. 3 https://github.com/moses-smt/ mosesdecoder/ Unsupervised SMT We also built an SMT system, without any supervision, i.e., using only but all the provided monolingual data for training. We chose unsupervised SMT (USMT) over unsupervised NMT (UNMT) since previous work (Artetxe et al., 2018b) has shown that USMT slightly outperforms UNMT and that we expect USMT to work well for this language pair that involves only very few word reorderings. We built USMT systems using a framework similar to the one proposed in Marie and Fujita (2018b). The first step of USMT is the induction of a phrase table from the monolingual corpora. We first collected phrases of up to six tokens from the monolingual News Crawl corpora version 209 using word2phrase.4 As phrases, we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpor"
W19-5428,D18-1399,0,0.300481,"-final-and heuristic. We also trained MSLR (monotone, swap, discontinuousleft, discontinuous-right) lexicalized reordering model. We trained one 4-gram language models on the entire monolingual data concatenated to the target side of the parallel data using LMPLZ 1 https://marian-nmt.github.io/, 1.6.0 2 NVIDIA® Tesla® P100 16Gb. 3 https://github.com/moses-smt/ mosesdecoder/ Unsupervised SMT We also built an SMT system, without any supervision, i.e., using only but all the provided monolingual data for training. We chose unsupervised SMT (USMT) over unsupervised NMT (UNMT) since previous work (Artetxe et al., 2018b) has shown that USMT slightly outperforms UNMT and that we expect USMT to work well for this language pair that involves only very few word reorderings. We built USMT systems using a framework similar to the one proposed in Marie and Fujita (2018b). The first step of USMT is the induction of a phrase table from the monolingual corpora. We first collected phrases of up to six tokens from the monolingual News Crawl corpora version 209 using word2phrase.4 As phrases, we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpor"
W19-5428,N12-1047,0,0.035276,"thetic sentence pairs, obtained through back-translating (Sennrich et al., 2016a) the first 5M sentences from the monolingual corpora, to the original parallel data for training. We performed NMT decoding with an ensemble of a total of four models according to the best BLEU (Papineni et al., 2002) scores on the development data produced by four independent training runs using the same training parameters. 3.2 Table 2: Parameters of Marian used for training our NMT systems. (Heafield et al., 2013). Our systems used the default distortion limit of 6. We tuned the SMT model weights with KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 iterations. SMT 3.3 We trained SMT systems using Moses.3 Word alignments and phrase tables were obtained from the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag-final-and heuristic. We also trained MSLR (monotone, swap, discontinuousleft, discontinuous-right) lexicalized reordering model. We trained one 4-gram language models on the entire monolingual data concatenated to the target side of the parallel data using LMPLZ 1 https://"
W19-5428,P13-2121,0,0.0488158,"Missing"
W19-5428,P18-4020,0,0.033502,"Missing"
W19-5428,P07-2045,0,0.0139968,"ry close languages with large monolingual data, and to compare it with supervised MT systems trained on large bilingual data. We participated under the team name “NICT.” All our systems were constrained, i.e., we used only the parallel and monolingual data provided by the organizers to train and tune the MT systems. For both translation directions, we trained supervised neural MT (NMT) and statistical MT (SMT) systems, and combined them through n-best list reranking using different informative features as 2.2 Tokenization, Truecasing, and Cleaning We used the tokenizer and truecaser of Moses (Koehn et al., 2007). The truecaser was trained on one million tokenized lines extracted randomly from the monolingual data. Truecasing was then performed on all the tokenized data. For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Note that we did not perform any punctuation normalization. Table 1 presents the statistics of the parallel and monolingual data, respectively, after preprocessing. 208 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task P"
W19-5428,W17-3204,0,0.0207584,"ation systems, instead of using only either forward (Marie and Fujita, 2018b) or backward translations (Artetxe et al., 2018b). We report on the performance of the systems obtained after the fourth refinement step. 4 4.1 Generation of n-best Lists We first produced the six 100-best lists of translation hypotheses generated by four NMT leftto-right models individually, by their ensemble, and by one right-to-left model. Unlike Moses, Marian must use a beam of size k to produce a kbest list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 2017). Consequently, we also produced with Marian the 12-best lists and merged them with Marian’s 100-best lists to obtain lists containing up to 112 hypotheses,6 or up to 672 hypotheses after merging all the lists produced by NMT. In this way, we make sure that we still have hypotheses of good quality in the lists despite using a larger beam size. We also generated 100-best translation hypotheses with SMT.7 Finally, we merged the lists produced by Marian and Moses. 4.2 Reranking Framework and Features We rescored all the hypotheses in the resulting lists with a reranking framework using SMT and NM"
W19-5428,W18-1811,1,0.872248,"data using LMPLZ 1 https://marian-nmt.github.io/, 1.6.0 2 NVIDIA® Tesla® P100 16Gb. 3 https://github.com/moses-smt/ mosesdecoder/ Unsupervised SMT We also built an SMT system, without any supervision, i.e., using only but all the provided monolingual data for training. We chose unsupervised SMT (USMT) over unsupervised NMT (UNMT) since previous work (Artetxe et al., 2018b) has shown that USMT slightly outperforms UNMT and that we expect USMT to work well for this language pair that involves only very few word reorderings. We built USMT systems using a framework similar to the one proposed in Marie and Fujita (2018b). The first step of USMT is the induction of a phrase table from the monolingual corpora. We first collected phrases of up to six tokens from the monolingual News Crawl corpora version 209 using word2phrase.4 As phrases, we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase"
W19-5428,P02-1040,0,0.115857,"encoding (BPE) (Sennrich et al., 2016b) using 30k operations. BPE segmentations were jointly learned on the training parallel data for the source and target languages. All our NMT systems were consistently trained on 4 GPUs,2 with the parameters for Marian listed in Table 2. To improve translation quality, we added 5M synthetic sentence pairs, obtained through back-translating (Sennrich et al., 2016a) the first 5M sentences from the monolingual corpora, to the original parallel data for training. We performed NMT decoding with an ensemble of a total of four models according to the best BLEU (Papineni et al., 2002) scores on the development data produced by four independent training runs using the same training parameters. 3.2 Table 2: Parameters of Marian used for training our NMT systems. (Heafield et al., 2013). Our systems used the default distortion limit of 6. We tuned the SMT model weights with KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 iterations. SMT 3.3 We trained SMT systems using Moses.3 Word alignments and phrase tables were obtained from the tokenized parallel data using mgiza. Source-to-target and target-to-source"
W19-5428,P16-1009,0,0.0357656,"mizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --tied-embeddings --exponential-smoothing 84.69M 68,284 171.15M Table 1: Statistics of our preprocessed data. 3 3.1 MT Systems NMT For our NMT systems, we adopt the Transformer architecture (Vaswani et al., 2017). We chose Marian (Junczys-Dowmunt et al., 2018)1 to train our NMT systems since it supports state-of-the-art features and is one of the fastest NMT frameworks publicly available. In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 30k operations. BPE segmentations were jointly learned on the training parallel data for the source and target languages. All our NMT systems were consistently trained on 4 GPUs,2 with the parameters for Marian listed in Table 2. To improve translation quality, we added 5M synthetic sentence pairs, obtained through back-translating (Sennrich et al., 2016a) the first 5M sentences from the monolingual corpora, to the original parallel data for training. We performed NMT decoding with an ensemble of a total of four models according to the best BLEU (Papineni et al., 2002) scores on the d"
W19-6613,D18-1399,0,0.237594,"lel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sente"
W19-6613,N12-1047,0,0.0289999,"s) = P exp(β cos(emb(t),emb(s)) , where emb(·) stands 0 t0 exp(β cos(emb(t ),emb(s)) for a bilingual embedding of a given phrase, obtained through averaging bilingual embeddings of constituent words learned from the two monolingual data using fastText17 and vecmap.18 For each of the retained phrase pair, p(s|t) was computed analogously. We also computed lexical translation probabilities relying on those learned from the given small parallel corpus. Up to four phrase tables were jointly exploited by the multiple decoding path ability of Moses. Weights for the features were tuned using KB-MIRA (Cherry and Foster, 2012) on the development set; we took the best weights after 15 iterations. Two hyper-parameters, namely, k for the number of pivot-based phrase pairs per source phrase and d for distortion limit, were determined by a grid search on k ∈ {10, 20, 40, 60, 80, 100} and d ∈ {8, 10, 12, 14, 16, 18, 20}. In contrast, we used predetermined hyper-parameters for phrase table induction from monolingual data, following 15 https://github.com/facebookresearch/ UnsupervisedMT 16 https://code.google.com/archive/p/ word2vec/ 17 https://fasttext.cc/ 18 https://github.com/artetxem/vecmap Proceedings of MT Summit XVI"
W19-6613,P17-2061,1,0.92136,"odel trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the parents and children models should be trained on similar domains; otherwise, one has to take into account an additional problem of domain adaptation (Chu et al., 2017). In this paper, we work on a linguistically distant and thus challenging language pair Japanese↔Russian (Ja↔Ru) which has only 12k lines of news domain parallel corpus and hence is extremely resource-poor. Furthermore, the amount of indirect in-domain parallel corpora, i.e., Ja↔En and Ru↔En, are also small. As we demonstrate in Section 4, this severely limits the performance of prominent low-resource techniques, such as multilingual modeling, back-translation, and pivotbased PBSMT. To remedy this, we propose a novel multistage fine-tuning method for NMT that combines multilingual modeling (Jo"
W19-6613,J82-2005,0,0.634941,"Missing"
W19-6613,N16-1101,0,0.0717709,"system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En)."
W19-6613,D07-1103,0,0.0524556,"u and Ru→Ja. Cascade: 2-step decoding using the source-toEnglish and English-to-target systems. Synthesize: Obtain a new phrase table from synthetic parallel data generated by translating English side of the target–English training parallel data to the source language with the English-to-source system. Triangulate: Compile a new phrase table combining those for the source-to-English and English-to-target systems. Among these three, triangulation is the most computationally expensive method. Although we had filtered the component phrase tables using the statistical significance pruning method (Johnson et al., 2007), triangulation can generate an enormous number of phrase pairs. To reduce the computational cost during decoding and the negative effects of potentially noisy phrase pairs, we retained for each source phrase s only the k-best translations t according to the forward translation probability φ(t|s) calculated from the conditional probabilities in the component models as defined in Utiyama and Isahara (2007). For each of the retained phrase pairs, we also calculated the backward translation probability, φ(s|t), and lexical translation probabilities, φlex (t|s) and φlex (s|t), in the same manner a"
W19-6613,Q17-1024,0,0.0503006,"Missing"
W19-6613,W18-6325,0,0.124224,"rd alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the parents and children mode"
W19-6613,W17-3204,0,0.502618,"more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, m"
W19-6613,P07-2045,0,0.0227712,"eudo-parallel data. Our approach, which combines domain adaptation, multilingualism, and back-translation, helps improve the translation quality by more than 3.7 BLEU points, over a strong baseline, for this extremely low-resource scenario. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-ric"
W19-6613,C18-1054,0,0.0296836,"the poor quality of the bilingual word embeddings used to extract the phrase table, as envisaged in Section 3. None of pivot-based approaches with unidirectional NMT models could even remotely rival the M2M Transformer NMT model (b3). 4.4 Augmentation with Back-translation Given that the M2M Transformer NMT model (b3) achieved best results for most of the translation directions and competitive results for the rest, we further explored it through back-translation. We examined the utility of pseudo-parallel data for all the six translation directions, unlike the work of Lakew et al. (2017) and Lakew et al. (2018), which concentrate only on the zero-shot language pair, and the work of Niu et al. (2018), which compares only uni- or bi-directional models. We investigated whether each translation direction in M2M models will benefit from pseudoparallel data and if so, what kind of improvement takes place. Dublin, Aug. 19-23, 2019 |p. 133 ID System #1–#10 Ja∗→Ru and/or Ru∗→Ja Ja∗→En and/or En∗→Ja Ru∗→En and/or En∗→Ru All Pseudo 12k→82k 47k→82k 82k All of the above Parallel data Ja↔Ru Ja↔En 12k→82k 47k→82k×2 12k→82k×2 47k→82k 12k→82k×2 47k→82k×2 12k→82k 47k→82k Ru↔En 82k×2 82k×2 82k 82k Total size of traini"
W19-6613,P10-2041,0,0.0550434,"• 22.85 22.77 • 23.09 En→Ru 16.92 17.30 17.20 • 17.89 16.76 16.68 16.80 • 17.73 17.13 17.26 17.30 Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six “X∗→Y” columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and “• ” indicates statistical significance of the improvement. First, we selected sentences to be backtranslated from in-domain monolingual data (Table 3), relying on the score proposed by Moore and Lewis (2010) via the following procedure. 1. For each language, train two 4-gram language models, using KenLM: an in-domain one on all the Global Voices data, i.e., both parallel and monolingual data, and a general-domain one on the concatenation of Global Voices, IWSLT, and Tatoeba data. 2. For each language, discard sentences containing OOVs according to the in-domain language model. 3. For each translation direction, select the T best monolingual sentences in the news domain, according to the difference between cross-entropy scores given by the in-domain and general-domain language models. Whereas Niu"
W19-6613,W18-2710,0,0.413744,"s opposed to PBSMT. Transfer learning approaches (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018) work well when a large helping parallel corpus is available. This restricts one of the source or the target languages to be English which, in our case, is not possible. Approaches involving bi-directional NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual w"
W19-6613,P02-1040,0,0.104547,".70 0.19 3.72 2.02 Ru→Ja 1.86 1.61 4.29 1.96 0.87 8.35 4.45 Ja→En 2.41 6.18 5.15 4.36 6.48 10.24 8.19 En→Ja 7.83 8.81 7.55 7.97 10.63 12.43 10.27 Ru→En 18.42 19.60 14.24 20.70 22.25 22.10 22.37 En→Ru 13.64 15.11 10.86 16.24 16.03 16.92 16.52 Table 5: BLEU scores of baseline systems. Bold indicates the best BLEU score for each translation direction. tuned by a linear search on the BLEU score for the development set. Similarly to PBSMT, we also evaluated “Cascade” and “Synthesize” methods with unidirectional NMT models. 4.3 Results We evaluated MT models using case-sensitive and tokenized BLEU (Papineni et al., 2002) on test sets, using Moses’s multi-bleu.perl. Statistical significance (p &lt; 0.05) on the difference of BLEU scores was tested by Moses’s bootstraphypothesis-difference-significance.pl. Tables 5 and 6 show BLEU scores of all the models, except the NMT systems augmented with back-translations. Whereas some models achieved reasonable BLEU scores for Ja↔En and Ru↔En translation, all the results for Ja↔Ru, which is our main concern, were abysmal. Among the NMT models, Transformer models (b∗) were proven to be better than RNMT models (a∗). RNMT models could not even outperform the uni-directional PB"
W19-6613,P16-1009,0,0.406018,"is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the difficulty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called"
W19-6613,N09-2024,0,0.0303021,"nal NMT modeling is shown to drasti1 News domain translation is also the most competitive tasks in WMT indicating its importance. 2 http://opus.nlpl.eu/News-Commentary-v11. php 3 https://github.com/aizhanti/JaRuNC Dublin, Aug. 19-23, 2019 |p. 129 cally improve low-resource translation (Niu et al., 2018). However, like most other, this work focuses on translation from and into English. Remaining options include (a) unsupervised MT (Artetxe et al., 2018; Lample et al., 2018; Marie and Fujita, 2018), (b) parallel sentence mining from non-parallel or comparable corpora (Utiyama and Isahara, 2003; Tillmann and Xu, 2009), (c) generating pseudo-parallel data (Sennrich et al., 2016), and (d) MT based on pivot languages (Utiyama and Isahara, 2007). The linguistic distance between Japanese and Russian makes it extremely difficult to learn bilingual knowledge, such as bilingual lexicons and bilingual word embeddings. Unsupervised MT is thus not promising yet, due to its heavy reliance on accurate bilingual word embeddings. Neither does parallel sentence mining, due to the difficulty of obtaining accurate bilingual lexicons. Pseudo-parallel data can be used to augment existing parallel corpora for training, and pre"
W19-6613,N07-1061,0,0.47734,"rtance of sharing news between different language speakers. Moreover, news domain is one of the most challenging tasks, Proceedings of MT Summit XVII, volume 1 Ru Ja En #sent. X X X X X X 913 173 276 0 4 287 1 1,654 X X X Total X X X test 600 - Usage development 313 173 276 - Table 1: Manually aligned News Commentary data. due to large presence of out-of-vocabulary (OOV) tokens and long sentences.1 To establish and evaluate existing methods, we also involved English as the third language. As direct parallel corpora are scarce, involving a language such as English for pivoting is quite common (Utiyama and Isahara, 2007). There has been no clean held-out parallel data for Ja↔Ru and Ja↔En news translation. Therefore, we manually compiled development and test sets using News Commentary data2 as a source. Since the given Ja↔Ru and Ja↔En data share many lines in the Japanese side, we first compiled tri-text data. Then, from each line, corresponding parts across languages were manually identified, and unaligned parts were split off into a new line. Note that we have never merged two or more lines. As a result, we obtained 1,654 lines of data comprising trilingual, bilingual, and monolingual segments (mainly senten"
W19-6613,D16-1163,0,0.0944142,"ing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although NMT can be significantly better than PBSMT in resourcerich scenarios, PBSMT performs better in lowresource scenarios (Koehn and Knowles, 2017). c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Only by exploiting cross-lingual transfer learning techniques (Firat et al., 2016; Zoph et al., 2016; Kocmi and Bojar, 2018), can the NMT performance approach PBSMT performance in lowresource scenarios. However, such methods usually require an NMT model trained on a resource-rich language pair like French↔English (parent), which is to be fine-tuned for a low-resource language pair like Uzbek↔English (child). On the other hand, multilingual approaches (Johnson et al., 2017) propose to train a single model to translate multiple language pairs. However, these approaches are effective only when the parent target or source language is relatively resource-rich like English (En). Furthermore, the p"
W19-6613,P03-1010,0,\N,Missing
W19-6613,P07-1092,0,\N,Missing
W19-6613,L16-1350,0,\N,Missing
Y15-1033,D14-1179,0,0.0191739,"Missing"
Y15-1033,D07-1103,0,0.657787,"ntences and 4.5M terms) parallel data via pivot-based SMT. We generate a large pivot translation model using the Ja-En and En-Zh parallel data. Moreover, a small direct Ja-Zh translation model is generated using small-scale Ja-Zh parallel data. (680k sentences and 561k terms). Both the direct and pivot translation models are used to translate the Ja terms in the Ja-En dictionaries to Zh and the Zh terms in the Zh-En dictionaries to Ja to construct a large-scale Ja-Zh dictionary (about 3.6M terms). We address the noisy nature of pivoting large phrase tables by statistical significance pruning (Johnson et al., 2007). In addition, we exploit linguistic knowledge of common Chinese characters (Chu et al., 2013) shared in Ja-Zh to further improve the translation model. Large-scale experiments on scientific domain data indicate that our proposed method achieves high quality dictionaries which we manually verify to have a high quality. Reranking the n-best list produced by the SMT decoder is known to help improve the translation quality given that good quality features are used (Och et al., 2004). In this paper, we use bilingual neural network language model features for reranking the n-best list produced by t"
Y15-1033,P07-2045,0,0.143247,"o be a possible way of constructing a dictionary for the language pairs that have scarce parallel data (Tsunakawa et al., 2009; Chu et al., 2015). The assumption of this method is that there is a pair of large-scale parallel data: one between the source language and an intermediate resource rich language (henceforth called pivot), and one between that pivot and the target language. We can use the source-pivot and pivot-target parallel data to develop a source-target term1 translation model for dictionary construction. Pivot-based SMT uses the log linear model as conventional phrase-based SMT (Koehn et al., 2007) does. This method can address the data sparseness problem of directly merging the source-pivot and pivot-target terms, because it can use the portion of terms to generate new terms. Small-scale experiments in (Tsunakawa et al., 2009) showed very low 1 In this paper, we call the entries in the dictionary terms. A term consists of one or multiple tokens. accuracy of pivot-based SMT for dictionary construction.2 This paper presents our study to construct a largescale Japanese-Chinese (Ja-Zh) scientific dictionary, using large-scale Japanese-English (Ja-En) (49.1M sentences and 1.4M terms) and En"
Y15-1033,D09-1141,0,0.0502113,"iplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007) has shown to be quite effective in a bilingual scenario, it has never been considered in a pivot language scenario. (Tsunakawa et al., 2009) was the first work that constructs a dictionary"
Y15-1033,P02-1040,0,0.092256,"ioned in Section 4. The following scores are reported: • BS+RRCBLEU: Using character BLEU to rerank the n-best list. • BS+RRWBLEU: Using word BLEU to rerank the n-best list. • BS+RRSVM: Using SVM to rerank the n-best list. This is followed by substituting the OOVs with the character level translations using the learned neural translation models (which we label as +OOVsub). 5.2.3 Evaluation Criteria Following (Tsunakawa et al., 2009), we evaluated the accuracy on the test set using three metrics: 1 best, 20 best and Mean Reciprocal Rank (MRR)(Voorhees, 1999). In addition, we report the BLEU-4 (Papineni et al., 2002) scores that were computed on the word level. 5.2.4 Results of Automatic Evaluation Table 3 shows the evaluation results. We also show the percentage of OOV terms,11 and the accuracy with and without OOV terms respectively. In general, we can see that Pivot performs better than Direct, because the data of Ja-En and En-Zh is larger than that of Ja-Zh. Direct+Pivot shows better performance than either method. Different pruning methods show different performances, where Pr:P-T improves the accuracy, while the other two not. To understand the reason for this, we also investigated the statistics of"
Y15-1033,E12-1055,0,0.0162567,"loped a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked well since the pivots had substantial similarity to the source languages. They used the multiple decoding paths (MDP) feature of the phrase-based SMT toolkit Moses (Koehn et al., 2007) to combine multiple tables which avoids interpolation. The issue of noise introduced by pivoting has not been seriously addressed and although statistical significance pruning (Johnson et al., 2007"
Y15-1033,P14-2042,1,0.828997,"AS LCAS title ISTIC pc ASPEC Size 3,588,800 22,610,643 19,905,978 3,013,886 6,090,535 1,070,719 1,562,119 680,193 shows the statistics of the bilingual dictionaries used for training. • Parallel corpora: the scientific Ja-En, En-Zh and Ja-Zh corpora we used were also provided by JST and ISTIC, containing 49.1M , 8.7M and 680k sentence pairs respectively. Table 2 shows the statistics of parallel corpora used for training. Among which ISTIC pc was provided by ISTIC, and the others were provided by JST. 5.2.1 In our experiments, we segmented the Chinese and Japanese data using a tool proposed by Shen et al. (2014) and JUMAN (Kurohashi et al., 1994) respectively. For decoding, we used Moses (Koehn et al., 2007) with the default options. We trained a word 5-gram language model on the Zh side of all the En-Zh and Ja-Zh training data (14.4M sentences) using the SRILM toolkit10 with interpolated Keneser-Ney discounting. Tuning was performed by minimum error rate training which also provides us with the n-best lists used to learn reranking weights. As a baseline, we compared following three methods for training the translation model: • Direct: Only use the Ja-Zh data to train a direct Ja-Zh model. Table 2: S"
Y15-1033,N07-1061,0,0.02745,"character based neural MT to eliminate the out-of-vocabulary (OOV) terms, which further improves the quality. The rest of this paper is structured as follows: Section 2 reviews related work. Section 3 presents our dictionary construction using pivot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weig"
Y15-1033,P07-1108,0,0.0645188,"Missing"
Y15-1033,P09-1018,0,0.0175042,"ot-based SMT with significance pruning. Section 4 describe the bilingual neural language model features using a parallel corpus and the constructed dictionary for reranking the n-best list. Experiments and results are described in Section 5, and we conclude this paper in Section 6. 2 Related Work Many studies have been conducted for pivot-based SMT. Utiyama and Isahara (2007) developed a method (sentence translation strategy) for cascading a source-pivot and a pivot-target system to translate from source to target using a pivot language. Since this results in multiplicative error propagation, Wu and Wang (2009) developed a method (triangulation) in which they combined the source-pivot and pivot-target phrase tables to obtain a source-target phrase table. They then combine the pivoted and direct tables (using source-target parallel corpora) by linear interpolation whose weights were manually specified. There is a method to automatically learn the interpolation weights (Sennrich, 2012) but it requires reference phrase pairs which are not easily available. Work on translation from Indonesian to English using Malay and Spanish to English using Portuguese (Nakov and Ng, 2009) as pivot languages worked we"
Y17-1038,D14-1179,0,0.0151463,"Missing"
Y17-1038,N03-1017,0,0.0488231,"of child models. We empirically show that using a parent model with the source language falling in the same or linguistically similar language family as the source language of the child model is the best. 1 Introduction One of the most attractive features of Neural Machine Translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, phrase tables and complicated decoding algorithms which are a characteristic of Phrase Based Statistical Machine Translation (PBSMT) systems (Koehn et al., 2003). It is reported that NMT works better than PBSMT only when there is an abundance of parallel corpora. In the case of low resource languages like Hausa, vanilla NMT is either worse than or comparable to PBSMT (Zoph et al., 2016). However, it is possible to use a previously trained X-Y model (parent model; X-Y being the resource rich language pair where X and Y represent the source and target languages respectively) to initialize the 282 parameters of a Z-Y model (child model; Z-Y Hideto Kazawa Google Japan, Tokyo, Japan kazawa@google.com being the resource poor language pair) leading to signif"
Y17-1038,P02-1040,0,0.103947,"ntion for the various baselines and transfer learning experiments. We used an in house NMT system developed using the Tensorflow (Abadi et al., 2015) framework so as to exploit multiple GPUs to speed up training. To ensure replicability we use the same NMT model design as in the original work (Zoph et al., 2016). In order to enable infinite vocabulary we use the word piece model (WPM) (Schuster and Nakajima, 2012) as a segmentation model which is closely related to the Byte Pair Encoding (BPE) based segmentation approach (Sennrich et al., 2016). We evaluate our models using the standard BLEU (Papineni et al., 2002) metric2 on the detokenized translations of the test set. However we report the only the difference between the BLEU scores of the transferred and the baseline models since our focus is not on the BLEU scores themselves but rather the improvement by using transfer learning and on observing the language relatedness phenomenon. Baseline models are simply ones trained from scratch by initializing the model parameters with random values. 4.1 Languages The set of parent languages (and abbreviations) we considered is: Hindi (Hi), Indonesian (Id), Turkish (Tr), Russian (Ru), German (De) and French (F"
Y17-1038,P16-1162,0,0.0287342,"experiments were performed using an encoder-decoder NMT system with attention for the various baselines and transfer learning experiments. We used an in house NMT system developed using the Tensorflow (Abadi et al., 2015) framework so as to exploit multiple GPUs to speed up training. To ensure replicability we use the same NMT model design as in the original work (Zoph et al., 2016). In order to enable infinite vocabulary we use the word piece model (WPM) (Schuster and Nakajima, 2012) as a segmentation model which is closely related to the Byte Pair Encoding (BPE) based segmentation approach (Sennrich et al., 2016). We evaluate our models using the standard BLEU (Papineni et al., 2002) metric2 on the detokenized translations of the test set. However we report the only the difference between the BLEU scores of the transferred and the baseline models since our focus is not on the BLEU scores themselves but rather the improvement by using transfer learning and on observing the language relatedness phenomenon. Baseline models are simply ones trained from scratch by initializing the model parameters with random values. 4.1 Languages The set of parent languages (and abbreviations) we considered is: Hindi (Hi)"
Y17-1038,D16-1163,0,0.120808,"e most attractive features of Neural Machine Translation (NMT) (Bahdanau et al., 2015; Cho et al., 2014; Sutskever et al., 2014) is that it is possible to train an end to end system without the need to deal with word alignments, phrase tables and complicated decoding algorithms which are a characteristic of Phrase Based Statistical Machine Translation (PBSMT) systems (Koehn et al., 2003). It is reported that NMT works better than PBSMT only when there is an abundance of parallel corpora. In the case of low resource languages like Hausa, vanilla NMT is either worse than or comparable to PBSMT (Zoph et al., 2016). However, it is possible to use a previously trained X-Y model (parent model; X-Y being the resource rich language pair where X and Y represent the source and target languages respectively) to initialize the 282 parameters of a Z-Y model (child model; Z-Y Hideto Kazawa Google Japan, Tokyo, Japan kazawa@google.com being the resource poor language pair) leading to significant improvements (Zoph et al., 2016) for the latter. This paper is about an empirical study of transfer learning for NMT for low resource languages. Our main focus is on translation to English for the following low resource la"
Y18-3001,Y18-3013,1,0.888887,"Missing"
Y18-3001,Y18-3003,1,0.889659,"Missing"
Y18-3001,Y18-3005,0,0.0459044,"Missing"
Y18-3001,P17-4012,0,0.0426364,". 3 each participant’s system. That is, the specific baseline system was the standard for human evaluation. At WAT 2018, we adopted a neural machine translation (NMT) with attention mechanism as a baseline system except for the IITB tasks. We used a phrasebased statistical machine translation (SMT) system, which is the same system as that at WAT 2017, as the baseline system for the IITB tasks. The NMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page.5 We used OpenNMT (Klein et al., 2017) as the implementation of the baseline NMT systems. In addition to the NMT baseline systems, we have SMT baseline systems for the tasks that started at last year or before last year. The baseline systems are shown in Tables 8, 9, and 10. SMT baseline systems are described in the previous WAT overview paper (Nakazawa et al., 2017). The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online tra"
Y18-3001,Y18-3002,0,0.0375601,"Missing"
Y18-3001,P07-2045,0,0.0120426,"in frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeC"
Y18-3001,W04-3250,0,0.312277,"Missing"
Y18-3001,Y18-3007,0,0.0326187,"Missing"
Y18-3001,Y18-3014,0,0.0304859,"Missing"
Y18-3001,W14-7001,1,0.458489,"ion (WAT2018) including Ja↔En, Ja↔Zh scientific paper translation subtasks, Zh↔Ja, K↔Ja, En↔Ja patent translation subtasks, Hi↔En, My↔En mixed domain subtasks and Bn/Hi/Ml/Ta/Te/Ur/Si↔En Indic languages multilingual subtasks. For the WAT2018, 17 teams participated in the shared tasks. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated. 1 Introduction The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2017 (Nakazawa et al., 2014; Nakazawa et al., 2015; Nakazawa et al., 2016; Nakazawa et al., 2017), WAT2018 brings together machine translation researchers and users to try, evaluate, share and discuss brand-new ideas of machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 5th WAT, we adopted new translation subtasks with Myanmar ↔ EnSadao Kurohashi Kyoto University kuro@i.kyoto-u.ac.jp glish mixed domain corpus1 and Bengali/Hindi/Malayalam/Tamil/Telugu/Urdu/Sinhalese ↔ English OpenSubtitles corpus2 in addition to the subtasks at WAT2017. WAT is the uniq"
Y18-3001,W16-4601,1,0.938773,"Missing"
Y18-3001,P11-2093,0,0.0504246,"leu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0 (Kurohashi et al., 1994), KyTea 0.4.6 (Neubig et al., 2011) with full SVM model13 and MeCab 0.996 (Kudo, 2005) with IPA dictionary 2.7.0.14 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter (Tseng, 2005) version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.15 For Korean segmentation, we 11 http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html 12 lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/ 13 http://www.phontron.com/kytea/model. html 14 http://code.google.com/p/mecab/ downloads/detail?name=mecab-ipadic-2.7. 0-20070801.tar.gz 15 http://nlp.stanford.ed"
Y18-3001,Y18-3011,0,0.141475,"Missing"
Y18-3001,P02-1040,0,0.119424,"://bitbucket.org/anoopk/indic_nlp_ library 10 https://github.com/rsennrich/ subword-nmt • tgt vocab size = 100000 • src words min frequency = 1 • tgt words min frequency = 1 The default values were used for the other system parameters. For many to one, one to many, and many to many multilingual NMT (Johnson et al., 2017), we add &lt;2XX> tags, which indicate the target language (XX is replaced by the language code), to the head of the source language sentences. 4 Automatic Evaluation 4.1 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015). BLEU scores were calculated using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). RIBES scores were calculated using RIBES.py version 1.02.4.11 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2018 web page.12 All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanes"
Y18-3001,Y18-3010,0,0.0585011,"Missing"
Y18-3001,Y18-3012,0,0.0444067,"Missing"
Y18-3001,2007.mtsummit-papers.63,0,0.0425147,"itute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for ja↔en subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for ja↔zh subtasks. The statistics for each corpus are shown in Table 1. 2.1.1 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million JapaneseEnglish scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from (Utiyama and Isahara, 2007). Each sentence pair is accompanied by a similarity score that are calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the frequency and occurrence ratios for the training data, are described in the README file of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts that exclude the sentences in the training data. Each dataset consists of 400 documents and contains sentences in each field at the same rate. The document ali"
Y18-3001,Y18-3017,0,0.0591037,"Missing"
Y18-3001,Y18-3006,1,0.868302,"Missing"
Y18-3003,D14-1179,0,0.016896,"Missing"
Y18-3003,P17-2061,1,0.937207,"ish-Japanese translation, UCSY MyanmarEnglish translation and Indic multilingual translation directions. The techniques we focused on for each translation task can be summarized as below: • For the ASPEC translation tasks, we mostly relied on multilingual Transformer (Vaswani et al., 2017) models and experimented with Recurrently Stacked NMT (RS-NMT) (Dabre and Fujita, 2018) models in order to determine the trade-off between compactness of models and the loss in their performance. • For the UCSY Myanmar-English translation task, we tried domain adaptation techniques such as Mixed Fine Tuning (Chu et al., 2017) since the the final objective was to achieve high quality translation for a low-resource domain (ALT). • For the Indic multilingual task, we explored the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on"
Y18-3003,P07-2045,0,0.00542266,"perience a large loss in translation quality despite having significantly fewer parameters compared to the vanilla NMT models. An interesting observation is that systems with the best BLEU might not be the best in terms of human evaluation. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). Although vanilla NMT is significantly better than PBSMT in resource-rich scenarios, PBSMT performs better in resource-poor scenarios (Zoph et Anoop Kunchukuttan Microsoft AI and Research, India ankunchu@microsoft.com Eiichiro Sumita NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan eiichiro.sumita@nict.go.jp al., 2016). By exploiting transfer learning techniques, the performance of NMT approaches can be improved substantially. For WAT 2018, we participated as team “NICT5” and worked on ASPEC Chinese-Japanese and English-Japanese translation, UCSY MyanmarEnglish translation an"
Y18-3003,N15-3017,1,0.843788,"ingle model to translate from English to all the Indic languages. This is essentially the reverse of the XX-En model. We also trained this model for 500k iterations. gle model to translate from all the Indic languages to English and vice versa. Unlike the previous multilingual models, we trained this model only for 180k iterations due to lack of time. • Multilingual Shared Indic Script XX-En model: This model is similar to the XX-En model except that the scripts for all the Indic languages are mapped to a common script. We used Devanagari as the common script, and used the Indic NLP Library5 (Kunchukuttan et al., 2015) for script conversion. As such, this increases the chance of vocabulary sharing. Because the training corpus diversity is significantly reduced we trained this model for 100k iterations because it is technically equivalent 5 • Multilingual XX-YY model: We trained a sinhttps://github.com/anoopkunchukuttan/ indic_nlp_library/ 957 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 Task Bengali-English Bengali-English Bengali-English Bengali-English Hindi-Englis"
Y18-3003,Y18-3001,1,0.838941,"d the feasibility of bilingual, N -to-1, 1-to-N and N to-N way translation models. We also tried an approach where we mapped the scripts of all Indic languages to a common script (Devanagari) to see if it helps improve the performance of a multilingual model. For additional details of how our submissions are ranked relative to the submissions of other WAT 952 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 participants, kindly refer to the overview paper (Nakazawa et al., 2018). 2 NMT Models and Approaches We will first describe the Transformer which is the state-of-the-art NMT model we used for our experiments. 2.1 The Transformer The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a sequence-tosequence neural model that consists of two components, the encoder and the decoder. The encoder converts the input word sequence into a sequence of vectors of high dimensionality. The decoder, on the other hand, produces the target word sequence by predicting the words using a combination of the previously predicted word and relevant p"
Y18-3003,D16-1163,0,0.0754938,"Missing"
