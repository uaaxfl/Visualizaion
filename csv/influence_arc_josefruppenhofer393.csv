2007.sigdial-1.5,reidsma-etal-2006-annotating,0,0.072425,"Missing"
2007.sigdial-1.5,P04-1085,0,0.0839832,"Missing"
2007.sigdial-1.5,W06-1639,0,0.133431,"Missing"
2007.sigdial-1.5,W05-0308,1,0.769752,"Missing"
2007.sigdial-1.5,H05-1044,1,0.118163,"of work on the Sentiment category. By contrast, little work has been done on the Arguing category. We first define and annotate these opinion types in AMI meetings. We then perform inter-annotator agreement studies to verify if the two categories can be reliably detected. We develop an Arguing lexicon as a new knowl26 Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, pages 26–34, c Antwerp, September 2007. 2007 Association for Computational Linguistics edge source for automatically recognizing the Arguing category. We use previously developed lexicons for Sentiment detection (Wilson et al., 2005; Stone et al., 1966) to evaluate their portability to multi-party meetings. Previous efforts in recognizing opinions (or subjectivity) in monologic texts have focussed on knowledge from lexico-syntactic sources. While these have proven useful, we believe that in the conversational genre, reliably recognizing opinion expressions in utterances is a complex discourse task. Thus, we explore the novel use of dialog features for opinion recognition in combination with a lexicon. We find that this combination of knowledge sources shows promising results. The rest of the paper is organized as follows"
2020.lrec-1.566,C18-1139,0,0.0135655,"on on the GermEval dataset mostly adds labels for temporal categories (DATE, DUR, TIME) and numeric categories (ORDINAL, CARDINAL,MON, QUANT, PERC). 5. Experiments To establish baseline scores for tagging performance using our scheme, we experimented with two systems that model the task of NER as a sequence labeling problem. The one for which we report results here is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Our second system uses the character-based contextual string embeddings, provided by the the flair library (Akbik et al., 2018; Akbik et al., 2019). Since the flair tagger was consistently outperformed by the BERT-based system, we do not report results for it here for lack of space. Transformers, of which BERT is an example, have recently pushed the state of the art for many NLP applications by ID 1 2 3 4 5 avg. Acc. (all) 98.19 98.23 98.16 98.20 98.17 98.19 Acc. 86.18 86.94 86.22 86.57 86.10 86.40 Prec. Rec. (non-O) 85.88 83.25 85.58 83.63 85.13 83.06 85.24 82.79 85.99 83.17 85.56 83.18 F1 84.55 84.60 84.08 83.99 84.56 84.36 Table 4: Results for NER sequence tagging with BERT on the German CoNLL-2003 data. TAG LOC M"
2020.lrec-1.566,N19-4010,0,0.0149201,"ataset mostly adds labels for temporal categories (DATE, DUR, TIME) and numeric categories (ORDINAL, CARDINAL,MON, QUANT, PERC). 5. Experiments To establish baseline scores for tagging performance using our scheme, we experimented with two systems that model the task of NER as a sequence labeling problem. The one for which we report results here is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Our second system uses the character-based contextual string embeddings, provided by the the flair library (Akbik et al., 2018; Akbik et al., 2019). Since the flair tagger was consistently outperformed by the BERT-based system, we do not report results for it here for lack of space. Transformers, of which BERT is an example, have recently pushed the state of the art for many NLP applications by ID 1 2 3 4 5 avg. Acc. (all) 98.19 98.23 98.16 98.20 98.17 98.19 Acc. 86.18 86.94 86.22 86.57 86.10 86.40 Prec. Rec. (non-O) 85.88 83.25 85.58 83.63 85.13 83.06 85.24 82.79 85.99 83.17 85.56 83.18 F1 84.55 84.60 84.08 83.99 84.56 84.36 Table 4: Results for NER sequence tagging with BERT on the German CoNLL-2003 data. TAG LOC MISC ORG PER Prec. 84."
2020.lrec-1.566,benikova-etal-2014-nosta,0,0.0440065,"Missing"
2020.lrec-1.566,N19-1423,0,0.064446,"t ORGpart] [Christian Wulff PER] Fine: [VW-Aufsichtrat TITLE] [Christian Wulff PER] Finally, we see that the fine-grained annotation on the GermEval dataset mostly adds labels for temporal categories (DATE, DUR, TIME) and numeric categories (ORDINAL, CARDINAL,MON, QUANT, PERC). 5. Experiments To establish baseline scores for tagging performance using our scheme, we experimented with two systems that model the task of NER as a sequence labeling problem. The one for which we report results here is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Our second system uses the character-based contextual string embeddings, provided by the the flair library (Akbik et al., 2018; Akbik et al., 2019). Since the flair tagger was consistently outperformed by the BERT-based system, we do not report results for it here for lack of space. Transformers, of which BERT is an example, have recently pushed the state of the art for many NLP applications by ID 1 2 3 4 5 avg. Acc. (all) 98.19 98.23 98.16 98.20 98.17 98.19 Acc. 86.18 86.94 86.22 86.57 86.10 86.40 Prec. Rec. (non-O) 85.88 83.25 85.58 83.63 85.13 83.06 85.24 82.79 85.99 83.17 85.56 83.18 F1"
2020.lrec-1.566,E03-1068,0,0.2808,"Missing"
2020.lrec-1.566,gravier-etal-2012-etape,0,0.0328382,"Missing"
2020.lrec-1.566,C96-1079,0,0.344869,"Hierarchy (ENEH) with 200 categories in three layers.2 The ENEH intends to be domaingeneral and includes, for instance, paths from the Name-root such as Name→Facility→Line→{Railroad, Road, Canal, Water Route, Tunnel, Bridge} as well as Name→Product→Rule→{Rule Other, Treaty, Law}. Besides the Name root, the inventory has roots for Time (e.g. Timex other Spring semester) and Numex (numerical) expressions (e.g. Frequency twice, five times), which are not NEs but which are of interest for downstream applications. 2 https://nlp.cs.nyu.edu/ene/ In fact, the first Named Entity tag set introduced by Grishman and Sundheim (1996) already included categories for percentages, time and monetary expressions. By contrast, the work of Leitner et al. (2019) is more narrowly focused. They annotate German data from the legal domain with 19 fine-grained classes that can be mapped to 7 coarse classes.10 of the 19 fine categories are lawrelated and 9 are domain independent. Similarly, in work on entity recognition in traffic-related events, Schiersch et al. (2018) expand the classical 4-category NE inventory with domain general subtypes (e.g. ORG-COM(mercial) for businesses), domain-specific subtypes (e.g. LocationStop for public"
2020.lrec-1.566,W11-0411,0,0.0242263,"uld never occur in our data. On the other hand, we want more detail on some of the classic four categories as well as add some custom ones. Against this background, we take the categories used by the OntoNotes project (Weischedel et al., 2013) as a starting point. It recognizes 10 types of named entities and 7 types of what it calls values, such as MONEY and PERCENT. We add some categories and adjust and expand the definitions so they suit our overall inventory and account for linguistic facts of German as needed. The most comparable scheme to ours is the QUAERO scheme of Rosset et al. (2011; Grouin et al. (2011) that was used by the ETAPE evaluation campaign for French (Gravier et al., 2012) for NER annotation on a corpus of TV broadcast speech. It is hierarchical with 32 subcategories under 7 supercategories, whereas our scheme with 31 labels is flat. The schemes differ somewhat in where they add detail but they have many similar or overlapping categories. 3. 3.1. Data Israel corpus The main dataset we work with is the “Israel-Korpus: Wiener in Jerusalem”, or ISW corpus for short. It consists of transcriptions of biographic interviews of Israeli citizens who emigrated from Vienna, Austria, during th"
2020.lrec-1.566,W03-0419,0,0.0651763,"ddings with positional information and selfattention. The representations are trained in two different task setups, i.e. by predicting masked words based on their left and right context and by classifying two sentences based on how probable it is that the second one immediately succeeds the first one in a text document. As a result, the learned embeddings encode information about the left and right context for each word which makes them superior to most previous representations. Devlin et al. (2019) have proposed a BERT architecture for sequence tagging on the CoNLL-2003 NER shared task data (Sang and Meulder, 2003). The model uses the pretrained BERT embeddings for initialization and then finetunes the representations by adding a simple classification layer on top of the pre-trained BERT model and jointly optimizing the model parameters on the downstream task. Each BERT model provides its own tokenization which splits longer words into sub-tokens. The sequence tagger uses only the first sub-token as the input to the classifier, which then predicts a label for each token. In our experiments, we use the HuggingFace transformers library (Wolf et al., 2019) that provides pre-trained transformer models for d"
2020.lrec-1.566,L18-1703,0,0.0292337,"imes), which are not NEs but which are of interest for downstream applications. 2 https://nlp.cs.nyu.edu/ene/ In fact, the first Named Entity tag set introduced by Grishman and Sundheim (1996) already included categories for percentages, time and monetary expressions. By contrast, the work of Leitner et al. (2019) is more narrowly focused. They annotate German data from the legal domain with 19 fine-grained classes that can be mapped to 7 coarse classes.10 of the 19 fine categories are lawrelated and 9 are domain independent. Similarly, in work on entity recognition in traffic-related events, Schiersch et al. (2018) expand the classical 4-category NE inventory with domain general subtypes (e.g. ORG-COM(mercial) for businesses), domain-specific subtypes (e.g. LocationStop for public transit stops) and new domain-specific toplevel types (e.g. for Distance expressions). 2.3. NER on speech As our work is focused on biographic narratives, we do not require a large and deep hierarchy of NEs, many of which would never occur in our data. On the other hand, we want more detail on some of the classic four categories as well as add some custom ones. Against this background, we take the categories used by the OntoNo"
2020.lrec-1.566,schmidt-2014-database,0,0.0182056,"re domain differences. Keywords: Named Entity Recognition, spoken language, German, oral history corpora 1. Introduction While Named Entity Recognition (NER) is typically envisioned in service of NLP tasks such as information extraction, question answering, automatic translation, etc (Jurafsky, 2000), we are interested in it also from a corpus linguistic and digital humanities perspective. The Datenbank f¨ur Gesprochenes Deutsch (DGD; ‘Database for Spoken German’) that is hosted by the Leibniz Institute for the German Language is a repository of, and a platform for research on, spoken German (Schmidt, 2014). It contains a large, continuously growing collection of currently 34 variational and conversational corpora, totalling more than 4.000 hours of audiovisual material, which are used to address a wide variety of research questions. As a first step in adding a layer of shallow semantic analysis to these spoken corpora, we want to provide NER tags. Out of all the corpora in the database, we chose to begin with the ISW corpus, which contains transcripts of German-language biographic narrative interviews with Austrian-born emigrants to Israel. The ISW corpus is part of a series of three interrelat"
2020.lrec-1.566,sekine-nobata-2004-definition,0,0.278706,"Missing"
2020.lrec-1.606,E17-2092,0,0.0227357,"ould a supporter of X feel after reading this tweet?) The possible stance labels were: • positiv (positive) (1) • weder positiv noch negativ (neither positive nor negative) (0) • negativ (negative) (-1) • nicht lesbar / trifft nicht zu (cannot read / does not apply) (x) Annotators were given the tweet, its location, and the profile picture, name and description of the user who posted it. If a tweet was a response to another tweet, that tweet was shown also. Annotators were instructed to use this context to label more ambiguous tweets. Our prompt is inspired by the reader-perspective prompt in Buechel and Hahn (2017). The prompt is designed to capture subtle stance cues that the writer may not have included consciously. To compensate for the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017), we instructed annotators to imagine the perspective of a proponent of the target, as in Card et al. (2015). After annotation we obtained final labels with MultiAnnotator Competence Estimation (MACE) (Hovy et al., 2013). MACE can be used to remove the least reliable annotators and to obtain a reliable majority vote even in quite unfavourable circumstances.7 Out of 28 original annotators, we remov"
2020.lrec-1.606,P15-2072,0,0.348457,"“to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less"
2020.lrec-1.606,D16-1148,0,0.368455,"Missing"
2020.lrec-1.606,D19-1664,0,0.179967,"588/data/ AOSUY6. In compliance with Twitter usage guidelines, we provide tweet ids rather than full tweets with their texts. 2 Published along with the main GTTC. 4924 ing politicians with a doctoral degree • evidence for the status-indicating function of naming in a language other than English and for an academic degree instead of a professional title • evidence that the status-indicating function of naming and titling is weaker in left-leaning than in rightleaning discourse 2. Related Work The framing of entities is a fairly new topic covered in only a handful of papers (Card et al., 2016; Fan et al., 2019). The only currently existing dataset for entity framing is the BASIL dataset (Fan et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance"
2020.lrec-1.606,D18-1393,0,0.138455,"eality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less positively is in the use of names and titl"
2020.lrec-1.606,L16-1591,0,0.289025,"pects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” towards that topic (Entman, 1993, p. 52). For example, a text may discuss the topic of immigration primarily in an economic frame that focuses on the need for workers, or in a cultural frame that focuses on issues of diversity and integration. Most work on framing in computational linguistics has focused on the framing of issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). In previous work (van den Berg et al., 2019), we introduced entity framing as the presentation of an entity that (intentionally or not) promotes a particular viewpoint towards that entity. Our goal is to understand how bias and stance are expressed in computer-mediated discourse about political topics, in light of rising concern that discussions about politics on social media are less civil and objective than discussions on traditional platforms (Persily, 2017; Ott, 2017; Dang-Xuan et al., 2013). One area in which entities can be framed more or less positively is in the"
2020.lrec-1.606,N13-1132,0,0.027577,"er tweet, that tweet was shown also. Annotators were instructed to use this context to label more ambiguous tweets. Our prompt is inspired by the reader-perspective prompt in Buechel and Hahn (2017). The prompt is designed to capture subtle stance cues that the writer may not have included consciously. To compensate for the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017), we instructed annotators to imagine the perspective of a proponent of the target, as in Card et al. (2015). After annotation we obtained final labels with MultiAnnotator Competence Estimation (MACE) (Hovy et al., 2013). MACE can be used to remove the least reliable annotators and to obtain a reliable majority vote even in quite unfavourable circumstances.7 Out of 28 original annotators, we removed 3 annotators for being unreliable as judged by MACE. To measure agreement, we used Krippendorff’s alpha (Krippendorff, 2018), which is suitable for multi-coders ordinal annotation (Antoine et al., 2014). The 25 competent annotators had an agreement of 0.62. This is a bit higher than the 0.58 alpha value for stance annotation in van den Berg et al. (2019) (also 3 classes), and than the alpha value of 0.57 obtained"
2020.lrec-1.606,P11-1016,0,0.0327561,"n et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We h"
2020.lrec-1.606,S16-1003,0,0.0303821,"ch annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We hypothesised that the rel"
2020.lrec-1.606,S17-2088,0,0.02895,"es prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presidents (van den Berg et al., 2019). We hypothesised that the relation between naming and stance would depend on which naming function was more dominant: (i) marking of relative status (based on e.g. age or professional role) or (ii) marking of relative solidarity, also r"
2020.lrec-1.606,W10-0214,0,0.0113763,"taset for entity framing is the BASIL dataset (Fan et al., 2019), which annotates framing segments and their polarity towards political entities in news articles. BASIL is not suitable for studying the impact of naming and titling on entity framing, as journalistic style guides prescribe certain naming conventions to ensure objectivity both in English (Siegal and Connolly, 1999) and in German (Raue, 2012). We therefore work with tweets. More common than datasets for entity framing are datasets annotated for explicit stance. These are typically tagged for stance towards products and companies (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016) where titling might play a lesser role. Datasets which do cover person entities typically include them as a subset among other target types such as companies, institutions and topics (Rosenthal et al., 2017; Amig´o et al., 2012; Amig´o et al., 2013; Amig´o et al., 2014), with the exception of Taddy (2013) which only has person entity targets. On these datasets, no studies were conducted on the use of names and titles. In previous work, we presented a dataset to examine the use of names and titles in English tweets mentioning presi"
2020.lrec-1.606,W19-2101,1,0.288319,"Missing"
2020.lrec-1.616,P98-1013,0,0.781603,"Missing"
2020.lrec-1.616,D14-1082,0,0.0310392,"old standard training data, several of our features also require a textual corpus to perform pattern recognition and to compare word frequencies. We use Amazon Product Review Data (Jindal and Liu, 2008), a corpus of 5.8 million product reviews, as it has previously been shown to be a good fit for polarity shifter classification (Schulder et al., 2017). To prepare the corpus for use in our features, we lemmatise it and merge particle verbs to be represented as a single token (e. g. tear down). To determine syntactic dependency relations within the corpus, we parse it using the Stanford Parser (Chen and Manning, 2014). 4. Methodology To automatically classify shifting directions, we train an SVM multi-class classifier, using the SVMmulticlass implementation by Tsochantaridis et al. (2005). We train the classifier using features that utilise existing linguistic resources as well as patterns in textual data. In addition we present several baselines. 4.1. Baselines We define two majority classifiers and a word embedding classifier as baselines. All baselines were also tested for their use as classifier features during exploratory experiments, but as each resulted in decreased performance for the best classifi"
2020.lrec-1.616,D14-1125,0,0.0407248,"Missing"
2020.lrec-1.616,W14-2618,0,0.0408975,"Missing"
2020.lrec-1.616,P11-1144,0,0.0629714,"Missing"
2020.lrec-1.616,N10-1138,0,0.0754665,"Missing"
2020.lrec-1.616,P13-2022,0,0.0663663,"Missing"
2020.lrec-1.616,N19-1423,0,0.0130406,"labeled as ‘affects both’, but adjectives receive the label ‘affects positives’. This is a stronger baseline than BASELINEmaj , as it takes into account the label distributions of individual parts of speech as observed in Table 2. Word Embedding: For BASELINEembed we train an SVM classifier on the dimensions of a word embedding.3 Word embeddings are vector spaces that represent semantic similarity based on distributional similarity. As embedding we use the 500-dimensional Word2Vec (Mikolov et al., 2013) embedding of the Amazon Product 3 Classifiers using contextualised embeddings, e. g. BERT (Devlin et al., 2019), present no advantage for our task, as lexical classification involves no context. In addition, the small number of 540 training items precludes the use of most other deep learning classifiers. 5012 Review Data corpus created by Schulder et al. (2017).4 Each dimension is treated as a weighted binary feature. Our expectation is that as shifting directionality is a semantic phenomenon, it may be encoded in specific embedding dimensions. Shifters that share shifting direction would be expected to be close to each other on those dimensions. 4.2. Task-specific Features The following features are d"
2020.lrec-1.616,P16-1191,0,0.0451842,"Missing"
2020.lrec-1.616,N09-1002,0,0.0770587,"Missing"
2020.lrec-1.616,W13-3514,0,0.0679004,"Missing"
2020.lrec-1.616,P14-1145,0,0.0605981,"Missing"
2020.lrec-1.616,W06-0301,0,0.0848014,"Missing"
2020.lrec-1.616,I17-1063,1,0.195101,"arity shifter’ or ‘negative polarity shifter’, where positive and negative refer to the polarity that the shifted expression receives. We found that in practice this terminology could cause confusion as to whether the prior or shifted polarity was being referred to. It is also unclear how to use it in cases where shifting results in a neutral polarity expression (Taboada et al., 2011). We therefore choose to instead mark shifters as ‘affects positive polarity’, ‘affects negative polarity’ or ‘affects both polarities’. While multiple resources exist that identify polarity shifters for English (Schulder et al., 2017; Schulder et al., 2018b; Schulder et al., forthcoming) and German (Schulder et al., 2018a), none of them specify their shifting direction. As 5010 a result, the polarities of sentences such as (8), (10), (12) and (14) would erroneously be assumed to have shifted. To prevent such mistakes, we introduce a supervised classifier for shifting directions that can enhance available shifter lexica. It labels each shifter as exactly one of three types: shifters that only affect positive polarities, only negative ones or shifters that can affect both. Our contributions are the following: 1. We design a"
2020.lrec-1.616,C18-1213,1,0.883501,"Missing"
2020.lrec-1.616,L18-1222,1,0.880408,"Missing"
2020.lrec-1.616,D13-1170,0,0.00637511,"olarity shifters among them. They also observed that some shifters would only affect specific polarities and took this into account. While individual negation words are more frequent than individual polarity shifters, Schulder et al. (2018b) showed that overall shifters are at least as frequent, even when only considering verbal shifters. However, so far research that concerns itself with compositional polarity has mostly focussed on negation words (see the survey by Wiegand et al. (2010)). This is at least in part due to the lack of resources that would help identify polarity shifters. While Socher et al. (2013) showed that negation words can be learned implicitly from labeled data, this fails for polarity shifters due to the relative low frequency of individual shifter words compared to negation words (Schulder et al., 2017). This is a general problem for implicit negation learning, even for current state of the art classifiers (Schulder et al., forthcoming). 1 Dataset Verbs Nouns Adj. Total Gold Standard 304 107 129 540 Bootstrapped 676 793 512 1,981 Total 980 900 641 2,521 Table 1: Number of polarity shifters in the English shifter lexicon (Schulder et al., forthcoming), grouped by part of speech."
2020.lrec-1.616,J11-2001,0,0.0164586,"e]+ ]+ . (13) Let us [amendshifter that [problem]− ]+ . (14) We can [amend the [solution]+ ]+ to improve its clarity. Wilson et al. (2005) specify shifting directions by marking the shifter as ‘general polarity shifter’, ‘positive polarity shifter’ or ‘negative polarity shifter’, where positive and negative refer to the polarity that the shifted expression receives. We found that in practice this terminology could cause confusion as to whether the prior or shifted polarity was being referred to. It is also unclear how to use it in cases where shifting results in a neutral polarity expression (Taboada et al., 2011). We therefore choose to instead mark shifters as ‘affects positive polarity’, ‘affects negative polarity’ or ‘affects both polarities’. While multiple resources exist that identify polarity shifters for English (Schulder et al., 2017; Schulder et al., 2018b; Schulder et al., forthcoming) and German (Schulder et al., 2018a), none of them specify their shifting direction. As 5010 a result, the polarities of sentences such as (8), (10), (12) and (14) would erroneously be assumed to have shifted. To prevent such mistakes, we introduce a supervised classifier for shifting directions that can enhan"
2020.lrec-1.616,W10-3111,1,0.734343,"olarity classifier that takes into account that the lexical polarity of words is affected by a number of contextual phenomena, polarity shifters among them. They also observed that some shifters would only affect specific polarities and took this into account. While individual negation words are more frequent than individual polarity shifters, Schulder et al. (2018b) showed that overall shifters are at least as frequent, even when only considering verbal shifters. However, so far research that concerns itself with compositional polarity has mostly focussed on negation words (see the survey by Wiegand et al. (2010)). This is at least in part due to the lack of resources that would help identify polarity shifters. While Socher et al. (2013) showed that negation words can be learned implicitly from labeled data, this fails for polarity shifters due to the relative low frequency of individual shifter words compared to negation words (Schulder et al., 2017). This is a general problem for implicit negation learning, even for current state of the art classifiers (Schulder et al., forthcoming). 1 Dataset Verbs Nouns Adj. Total Gold Standard 304 107 129 540 Bootstrapped 676 793 512 1,981 Total 980 900 641 2,521"
2020.lrec-1.616,H05-1044,0,0.684915,"[did notnegation [pass the exam]+ ]− . (2) Peter [failedshifter to [pass the exam]+ ]− . + − (3) Peter’s [failureshifter to [pass the exam] ] (4) Peter’s [failedshifter [attempt to pass the exam]+ ]− Many polarity shifters can affect both positive and negative expressions. In (5), the verbal shifter destroy shifts a positive polar expression to negative, while in (6) it shifts from negative to positive. (5) It [destroyedshifter their [hopes]+ ]− . (6) The medication will [destroyshifter the [cancer]− ]+ . Other shifters, however, are unidirectional and only shift polarities in one direction (Wilson et al., 2005). The verbal shifter to risk, for example, shifts only positive polar expressions like good health in (7), while the polarity of negative polar expressions like war in (8) remains unaffected. Similarly, the adjectival shifter antiquated shifts the positive noun ideal in (9), but not the negative noun stereotype in (8). (8) Their actions [risk a [war]− ]− . (9) The “American dream” is an [antiquatedshifter [ideal]+ ]− . (10) Women belonging in the kitchen is an [antiquated [stereotype]− ]− . Conversely there are shifters that only affect negative expressions but not positive ones, such as recou"
2020.lrec-1.645,N18-1090,0,0.154878,"ices, taking into account a number of issues whose classification was partially inspired by the list of topics from the Special Track on the Syntac5 http://www.spmrl.org/sancl-posters2014. html 6 This is not to say that there is no conventional, well-punctuated data on social media. For instance, many corporations and institutions employ social media managers who adhere to common editing standards. Conversely, some sentence boundaries in canonical written language are also ambiguous, e.g. in headings, tables 5242 Name ATDT (UD) Hi-En-CS TwitterAAE (TAAE) Reference (Albogamy and Ramsay, 2017) (Bhat et al., 2018) (Blodgett et al., 2018) Source Twitter Twitter Twitter TWITTIRÒ-UD (TWRO) DWT W2.0 Foreebank (Frb) Tweebank (Twb) Tweebank2 (Twb2) TDT xUGC ITU tweeDe Postwita-UD (Pst) FSMB EWT SDT CWT GUM (Cignarella et al., 2019) (Daiber and Van Der Goot, 2016) (Foster et al., 2011) (Kaljahi et al., 2015) (Kong et al., 2014) (Liu et al., 2018) (Luotolahti et al., 2015) (Martínez Alonso et al., 2016) (Pamay et al., 2015) (Rehbein et al., 2019) (Sanguinetti et al., 2018) (Seddah et al., 2012) (Silveira et al., 2014) (Wang et al., 2017) (Wang et al., 2014) (Zeldes, 2017) Twitter Twitter Twitter, sport forums"
2020.lrec-1.645,P04-3031,0,0.214172,"l constructs occurring in social media. For instance, (sequences of) hashtags and URLs are separated out into ‘sentences’ of their own whenever they occur at the beginning or at the end of a tweet and do not have any syntactic function. The above segmentation policies notwithstanding, tweeDe still features the use of parataxis for juxtaposed clauses that are not separated by punctuation. A third option besides not segmenting and segmenting manually is, of course, to segment automatically. In the spirit of maintaining a real-world scenario, Frb split their forum data into sentences using NLTK (Bird and Loper, 2004), with no post-corrections. Accordingly, the resource contains instances where multiple sentences are merged into one sentence due to punctuation errors such as a comma being used instead of a full stop, as in Example 1. Conversely, there are cases where a single sentence is split over multiple lines, resulting in multiple sentences (Example 2) that are not rejoined. (1) Combofix will start, When it is scanning don’t move the mouse cursor inside the box, can cause freezing. (2) I’m sure the devs. can give you more details on this Tokenization Tokenization problems in informal text include case"
2020.lrec-1.645,P18-1131,0,0.0314012,"Missing"
2020.lrec-1.645,W19-7803,0,0.0239954,"n ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding difficult by means of the"
2020.lrec-1.645,W16-1714,1,0.893259,"Missing"
2020.lrec-1.645,L16-1667,1,0.88063,"Missing"
2020.lrec-1.645,L16-1248,0,0.0199681,"246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the"
2020.lrec-1.645,N13-1037,0,0.108812,"generated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013), and that continue to make its 1 https://noisy-text.github.io/ This phrase is used here in a broader sense to indicate all those orthographic, lexical as well as structural choices adopted by a user, often for expressive purposes. 2 treatment a difficult task. The availability of ad hoc training resources remaining an essential factor for the analysis of these texts, in the last decade, numerous resources of this type have been developed. A good proportion of these have been annotated according to the UD scheme (Nivre et al., 2016), a dependency-based format which has achieved great popularit"
2020.lrec-1.645,N10-1060,0,0.0742547,"e popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some"
2020.lrec-1.645,P11-2008,0,0.520555,"Missing"
2020.lrec-1.645,D14-1108,0,0.30934,"e last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UG"
2020.lrec-1.645,lacheret-etal-2014-rhapsodie,0,0.0291573,"inustria’s education’ 5246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions"
2020.lrec-1.645,W16-5403,0,0.0247332,"nn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described"
2020.lrec-1.645,N18-1088,0,0.414107,"ade it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foste"
2020.lrec-1.645,W15-4301,1,0.715109,"l media, treebanks, Universal Dependencies, annotation guidelines, UGC 1. Introduction The immense popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as we"
2020.lrec-1.645,J93-2004,0,0.07288,"the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding difficult by means of the current UD scheme. Elliptical structures and missing elements In constituency-based treebanks of canonical texts such as the Penn Treebank (Marcus et al., 1993) the annotation of empty elements results from the need to keep traces of movement and long-distance dependencies, usually marked with traces and co-indexations at the lexical level in addition to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject"
2020.lrec-1.645,W16-3905,1,0.923501,"resources comprise texts from discussion forums of any kind. Only two treebanks consist of texts from different sub-domains, i.e. blogs, reviews, emails, newsgroups and question answers (EWT), and Wikinews, Wikivoyage, wikiHow, Wikipedia, interviews, Creative Commons fiction and Reddit (GUM), and one is made up of generic data automatically crawled from the web (TDT). Syntactic frameworks As regards the formalism adopted to represent the syntactic structure, dependencies are by far the most used paradigm, especially among the treebanks created from 2014 onward. As also pointed out by Martínez Alonso et al. (2016), a dependency-based annotation lends 3 A more complete table with additional information on the surveyed treebanks can be found here: http://di.unito.i t/webtreebanks. 4 https://developer.twitter.com/en/develop er-terms/agreement-and-policy#c-respect-use rs-control-and-privacy 5241 Phenomenon Lang Attested example Standard form Ergographic phenomena (encoding simplification) GA Leigh aris! Léigh arís! ˙ TR Istanbuldaki agaclar Istanbul’daki a˘gaçlar EN ppl people TR slm selam EN Happy Birthday 2 me Happy Birthday to me TR n zmn ne zaman FR je sé je sais GA gura míle go raibh míle FR tous mes"
2020.lrec-1.645,N13-1039,0,0.155761,"Missing"
2020.lrec-1.645,L18-1106,0,0.0219631,"1. Introduction The immense popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall"
2020.lrec-1.645,W15-1302,1,0.787281,"accurate cross-lingual studies, all switched tokens should be (consistently) lemmatized if the language is known to annotators. Otherwise the surface form should be used, allowing for more comprehensive lemmatization at a later date. Disfluencies Similar to spoken language, UGC often contains disfluencies such as repetitions, fillers or aborted sentences. This might be surprising, given that UGC does not pose the same pressure on cognitive processing that online spoken language production does. In UGC, however, what seems to be a performance error has in fact a completely different function (Rehbein, 2015). Here, repetitions, self-repair and hesitation markers are often used with humorous intent (Example 12) (12) “Le proposte per l’education di Confindustria” ‘The proposals for the Confinustria’s education’ 5246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity res"
2020.lrec-1.645,L16-1376,0,0.0124556,"on to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject and object of the right hand-side conjunct are annotated with the orphan or remnant13 relations (Schuster et al., 2017). Even though the Enhanced UD scheme proposes to include a ghost-token (Schuster and Manning, 2016) which will be the actual governor of the right hand-side conjuncts , nothing is prescribed regarding treatment of ellipsis without 13 det/nmod Figure 1: Pathological example with two contesting structures from two different readings of the token “-2” surrounded by at least 2 elided elements. (Adapted to UD2.5 from (Martínez Alonso et al., 2016)) Du Hengst! äh, hängst. You stallion! uh, hang2.P s.Sg . “You stallion! uh, you’re stalled.” 5. fixed det This treatment, however, loses information whenever the reparandum does not have the same grammatical function as the repair, which is sometimes t"
2020.lrec-1.645,W17-0416,0,0.0155036,"nce dependencies, usually marked with traces and co-indexations at the lexical level in addition to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject and object of the right hand-side conjunct are annotated with the orphan or remnant13 relations (Schuster et al., 2017). Even though the Enhanced UD scheme proposes to include a ghost-token (Schuster and Manning, 2016) which will be the actual governor of the right hand-side conjuncts , nothing is prescribed regarding treatment of ellipsis without 13 det/nmod Figure 1: Pathological example with two contesting structures from two different readings of the token “-2” surrounded by at least 2 elided elements. (Adapted to UD2.5 from (Martínez Alonso et al., 2016)) Du Hengst! äh, hängst. You stallion! uh, hang2.P s.Sg . “You stallion! uh, you’re stalled.” 5. fixed det This treatment, however, loses information when"
2020.lrec-1.645,C12-1149,1,0.52101,"ic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013), and that continue to make its 1 https://noisy-text.github.io/ This phrase is used here in a broader sense to indicate all those orthographic, lexical as well as structural choices adopted by a user, often for expressive purposes. 2 treatment a difficult task. The availability of ad hoc training resources remaining an essential factor for the analysis of these texts, in the last decade, numerous resources of this type have been developed. A good proportion of these have been annotated according to the UD scheme (Nivre et al., 2016), a dependency-based format which has achie"
2020.lrec-1.645,D08-1110,0,0.0232123,"d apply to INTER CS (universaldependencies.org, 2019a). In the cases of INTRA CS that are compositional and the grammar of the switched text is known to annotators, the dependency labels should represent the syntactic role each switched token plays. • Markup symbols (e.g. < >) have the UPOS SYM similar to e.g., math operators in the UD guidelines, and they are attached to the root with dep. Code switching While capturing code-switching (CS) in tweets is also a motivation for a tweet-based unit of analysis (Çetino˘glu, 2016; Lynn and Scannell, 2019), it is an emerging topic of interest in NLP (Solorio and Liu, 2008; Solorio et al., 2014; Bhat et al., 2018) and thus should be captured in treebank data. CS (switching between languages) can occur on a number of levels. CS that occurs at the sentence or clause level is referred to as inter-sentential switching (INTER) as shown between English and Irish in Example 9: (9) “Má tá AON Gaeilge agat, úsáid í! It’s Irish Language Week.” If you have ANY Irish, use it! It’s Irish Language Week. INTER switching can also be used to describe bilingual tweets where the switched text represents a translation of the previous segment: “Happy St Patrick’s Day! La Fhéile Pád"
2020.lrec-1.645,W14-3907,0,0.0224716,"iversaldependencies.org, 2019a). In the cases of INTRA CS that are compositional and the grammar of the switched text is known to annotators, the dependency labels should represent the syntactic role each switched token plays. • Markup symbols (e.g. < >) have the UPOS SYM similar to e.g., math operators in the UD guidelines, and they are attached to the root with dep. Code switching While capturing code-switching (CS) in tweets is also a motivation for a tweet-based unit of analysis (Çetino˘glu, 2016; Lynn and Scannell, 2019), it is an emerging topic of interest in NLP (Solorio and Liu, 2008; Solorio et al., 2014; Bhat et al., 2018) and thus should be captured in treebank data. CS (switching between languages) can occur on a number of levels. CS that occurs at the sentence or clause level is referred to as inter-sentential switching (INTER) as shown between English and Irish in Example 9: (9) “Má tá AON Gaeilge agat, úsáid í! It’s Irish Language Week.” If you have ANY Irish, use it! It’s Irish Language Week. INTER switching can also be used to describe bilingual tweets where the switched text represents a translation of the previous segment: “Happy St Patrick’s Day! La Fhéile Pádraig sona daoibh!” Thi"
2020.lrec-1.645,L16-1250,0,0.0251562,"fach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding diff"
2020.lrec-1.645,W19-7723,1,0.892639,"Missing"
2020.lrec-1.645,L16-1102,0,0.0250592,"Missing"
2020.lrec-1.645,I11-1100,1,0.739484,"Missing"
2020.lrec-1.645,D15-1157,1,0.88306,"Missing"
2020.lrec-1.645,L16-1262,0,0.0724564,"Missing"
2020.lrec-1.645,W15-1610,0,0.0503228,"Missing"
2020.lrec-1.645,W19-7811,1,0.885346,"Missing"
2020.lrec-1.645,L18-1279,1,0.892617,"Missing"
2020.lrec-1.645,silveira-etal-2014-gold,0,0.0780259,"Missing"
2020.lrec-1.645,D14-1122,0,0.0259874,"Missing"
2020.lrec-1.645,P17-1159,0,0.0585556,"Missing"
2020.lrec-1.731,W14-0703,0,0.019091,"r a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. Other studies on German have also been focussed on discourse connectives. Stede et al. (1998; 2002) created a lexicon for German discourse markers, augmented with semantic relations (Scheffler and Stede, 2016). Gastel et al. (2011) present annotations for discourse connectives in the TüBa-D/Z (Telljohann et al., 2004), including a small number of causal connectives. A rule-based system for detecting a set of 8 causal German discourse connectives in spoken discourse has been presented by Bögel et al. (2014). Their system predicts whether or not a connective is causal and they also try to predict the causality type, i.e. Reason or Result. Automatic prediction of causal relations in text Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in the BeCause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more"
2020.lrec-1.731,W11-0906,0,0.0146999,"ext Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in the BeCause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more complex multi-word triggers with non-contiguous argument spans. Their new system is a transition-based parser, extending the transition system of the Propbank semantic parser of Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identify the causal triggers in the text, and then it labels the argument spans, i.e. cause, effect and means. The new system not only makes the time-consuming featureengineering of earlier work superfluous, it also outperforms the previous system by a large margin. Another neural approach for causal language detection is presented by Dasgupta et al. (2018)"
2020.lrec-1.731,W18-5035,0,0.0191701,"Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identify the causal triggers in the text, and then it labels the argument spans, i.e. cause, effect and means. The new system not only makes the time-consuming featureengineering of earlier work superfluous, it also outperforms the previous system by a large margin. Another neural approach for causal language detection is presented by Dasgupta et al. (2018) who extract causeeffect relations from text. They combine a bidirectional LSTM with linguistic features and use word and phrase embeddings to model the similarity between different causal arguments of the same type, e.g. the similarity between the two events ’engine failure’ and ’engine breakdown’. 3. Annotation Schema Our annotaiton scheme is adapted from Dunietz et al. (2015), but with an extended set of arguments. While Dunietz et al. (2015) annotate exactly two arguments, namely C AUSE and E FFECT, we also consider the ACTOR and the A FFECTED party of the causal event. The motivation behi"
2020.lrec-1.731,N19-1423,0,0.0161212,"in Table 5.6 train dev test Tokens 86,797 3,899 35,803 Sent. 2,915 151 1,336 Trigger 2,937 151 1,377 causal 1,787 78 873 non-causal 1,150 73 504 5.2. (8) Table 5: German causal annotation dataset split into training/development/test sets. Model: A BERT-based causal sequence tagger We model the task of causal language prediction as a sequence labelling problem, following related work on local semantic role labelling employing syntax-agnostic neural methods (Collobert et al., 2011). Our system is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Recently, transformers have pushed the state of the art for many NLP applications by learning context-sensitive embeddings with different optimisation strategies and then fine-tuning the pre-trained embeddings in a task-specific setup. BERT embeddings are usually trained on large amounts of data, incorporating word embeddings with positional information and self-attention. The representations are trained in two different task setups, i.e. by predicting masked words based on their left and right context and by classifying two sentences based on how probable it is that the second one immediate"
2020.lrec-1.731,W15-1622,0,0.290724,"es of causality such as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the paper is structured as follows. First, we review related work on annotating causal language (section 2.). In section 3., we present the anno"
2020.lrec-1.731,Q17-1009,0,0.161826,"s “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the paper is structured as follows. First, we review related work on annotating causal language (section 2.). In section 3., we present the annotation scheme we use i"
2020.lrec-1.731,W17-0812,0,0.208882,"s “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the paper is structured as follows. First, we review related work on annotating causal language (section 2.). In section 3., we present the annotation scheme we use i"
2020.lrec-1.731,D18-1196,0,0.0165722,"number of causal connectives. A rule-based system for detecting a set of 8 causal German discourse connectives in spoken discourse has been presented by Bögel et al. (2014). Their system predicts whether or not a connective is causal and they also try to predict the causality type, i.e. Reason or Result. Automatic prediction of causal relations in text Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in the BeCause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more complex multi-word triggers with non-contiguous argument spans. Their new system is a transition-based parser, extending the transition system of the Propbank semantic parser of Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identif"
2020.lrec-1.731,P15-1033,0,0.0204527,"ause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more complex multi-word triggers with non-contiguous argument spans. Their new system is a transition-based parser, extending the transition system of the Propbank semantic parser of Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identify the causal triggers in the text, and then it labels the argument spans, i.e. cause, effect and means. The new system not only makes the time-consuming featureengineering of earlier work superfluous, it also outperforms the previous system by a large margin. Another neural approach for causal language detection is presented by Dasgupta et al. (2018) who extract causeeffect relations from text. They combine a bidirectional LSTM with linguistic features and use word and"
2020.lrec-1.731,W03-1210,0,0.180402,"se of implicit discourse relations where the missing trigger is inserted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003) or on parallel or comparable corpora (Versley, 2010; Hidey and McKeown, 2016; Rehbein and Ruppenhofer, 2017). Girju (2003) has tried to detect instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN  causes bonynessN  . After identifying noun pairs that might express a causal relation, she uses the extracted pairs to search for verbs in a large corpus that might link the nouns and express the causal relation. She then collects these verbs and obtains a list of ambiguous verbs that might express causality. To disambiguate them, Girju extracts sentences from a lar"
2020.lrec-1.731,grivaz-2010-human,0,0.02253,"Dynamic Force Model which provides a framework that tries to distinguish weak and strong causal forces, and captures different types of causality such as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the p"
2020.lrec-1.731,P16-1135,0,0.13391,"serted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003) or on parallel or comparable corpora (Versley, 2010; Hidey and McKeown, 2016; Rehbein and Ruppenhofer, 2017). Girju (2003) has tried to detect instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN  causes bonynessN  . After identifying noun pairs that might express a causal relation, she uses the extracted pairs to search for verbs in a large corpus that might link the nouns and express the causal relation. She then collects these verbs and obtains a list of ambiguous verbs that might express causality. To disambiguate them, Girju extracts sentences from a large text corpus and manually annotates them, according to whether or not they"
2020.lrec-1.731,2005.mtsummit-papers.11,0,0.183731,"all disagreements were resolved by the two expert annotators. Table 2 shows inter-annotator agreement (IAA) scores for the different subsets of our data. We reAnnotating German Causal Language This section presents the data and annotation setup as well as inter-annotator agreement scores for the annotation of German causal language. 4.1. Confusion matrix for causal types (verbs) A NNOT 1 A NNOT 2 C ONSEQ . M OTIV. P URPOSE N ONE Data The data we annotate comes from two sources, (i) newspaper text from the TiGer corpus (Dipper et al., 2001) and (ii) political speeches from the Europarl corpus (Koehn, 2005). We chose those two sources as we wanted to include medially written and spoken data, and we selected corpora that allow us to make the annotated data available to the research community. 4.2. A NNOT 1 A NNOT 2 C ONSEQ . M OTIV. P URPOSE N ONE The annotation was done by three annotators, two expert annotators and one advanced student of Computation Linguistics. Each instance included only one causal trigger to be annotated. The triggers were marked and the annotators had been instructed to ignore other potentially causal expressions in the same sentence. For annotation, we used the POS verb n"
2020.lrec-1.731,C16-1007,0,0.0254721,"mporal relations, as a causal relation requires the temporal order of the two events involved, and many studies have looked at both phenomena together. Mirza et al. (2014) have annotated causal relations in the TempEval-3 corpus (UzZaman et al., 2013), with an annotation scheme inspired by TimeML (Pustejovsky et al., 2010). Based on Talmy (1988) and Wolff et al. (2005), they also distinguish whether the first event causes, enables or prevents the second event. Their annotations cover different parts of speech such as verbs, adpositions, adverbials and discourse connectives. In follow-up work (Mirza and Tonelli, 2016) they present a sieve-based system that jointly predicts temporal and causal relations in the TempEval-3 data and the TimeBank corpus (Pustejovsky et al., 2003). Their system makes use of a rich feature set, including morpho-syntactic information, syntactic dependencies, event order, WordNet similarity as well as the annotations that exist in the TimeBank corpus such as T IMEX 3 attribute types or temporal signals. Implicit vs. explicit causality It is well known that the description of causal events is not always expressed by 5968 means of an explicit causal trigger in the text, and humans ha"
2020.lrec-1.731,prasad-etal-2008-penn,0,0.481203,"own that the description of causal events is not always expressed by 5968 means of an explicit causal trigger in the text, and humans have no problem interpreting even implicit causal relations. This is exemplified in the Causality-by-default hypothesis (Sanders, 2005) that has shown that humans, when presented with two consecutive sentences expressing a relation that is ambiguous between a causal and an additive reading, tend to interpret the relation as causal, as in (1). (1) She went to the pub last night. This morning, she was late for work. The annotations in the Penn Discourse treebank (Prasad et al., 2008; Prasad et al., 2018) accomodate this phenomenon by the use of implicit discourse relations where the missing trigger is inserted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on"
2020.lrec-1.731,W18-4710,0,0.0385694,"ion of causal events is not always expressed by 5968 means of an explicit causal trigger in the text, and humans have no problem interpreting even implicit causal relations. This is exemplified in the Causality-by-default hypothesis (Sanders, 2005) that has shown that humans, when presented with two consecutive sentences expressing a relation that is ambiguous between a causal and an additive reading, tend to interpret the relation as causal, as in (1). (1) She went to the pub last night. This morning, she was late for work. The annotations in the Penn Discourse treebank (Prasad et al., 2008; Prasad et al., 2018) accomodate this phenomenon by the use of implicit discourse relations where the missing trigger is inserted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge ba"
2020.lrec-1.731,pustejovsky-etal-2010-iso,0,0.0445824,"n our new dataset in Section 5. and end with conclusions and suggestions for future work. 2. Related Work In this section, we give an overview over previous work on annotating causal relations. Temporal and causal relations It has often been noted that the concept of causality is closely linked to temporal relations, as a causal relation requires the temporal order of the two events involved, and many studies have looked at both phenomena together. Mirza et al. (2014) have annotated causal relations in the TempEval-3 corpus (UzZaman et al., 2013), with an annotation scheme inspired by TimeML (Pustejovsky et al., 2010). Based on Talmy (1988) and Wolff et al. (2005), they also distinguish whether the first event causes, enables or prevents the second event. Their annotations cover different parts of speech such as verbs, adpositions, adverbials and discourse connectives. In follow-up work (Mirza and Tonelli, 2016) they present a sieve-based system that jointly predicts temporal and causal relations in the TempEval-3 data and the TimeBank corpus (Pustejovsky et al., 2003). Their system makes use of a rich feature set, including morpho-syntactic information, syntactic dependencies, event order, WordNet similar"
2020.lrec-1.731,W17-0813,1,0.806488,"Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003) or on parallel or comparable corpora (Versley, 2010; Hidey and McKeown, 2016; Rehbein and Ruppenhofer, 2017). Girju (2003) has tried to detect instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN  causes bonynessN  . After identifying noun pairs that might express a causal relation, she uses the extracted pairs to search for verbs in a large corpus that might link the nouns and express the causal relation. She then collects these verbs and obtains a list of ambiguous verbs that might express causality. To disambiguate them, Girju extracts sentences from a large text corpus and manually annotates them, according to whether or not they have a causal meaning. The annot"
2020.lrec-1.731,W03-0419,0,0.177092,"dings with positional information and self-attention. The representations are trained in two different task setups, i.e. by predicting masked words based on their left and right context and by classifying two sentences based on how probable it is that the second one immediately succeeds the first one in a text document. As a result, the learned embeddings encode information about the left and right context for each word which makes them superior to most previous representations. Devlin et al. (2019) have proposed a BERT architecture for sequence tagging on the CoNLL-2003 NER shared task data (Sang and Meulder, 2003). The model uses the pretrained BERT embeddings for initialisation and then finetunes the representations by adding a simple classification layer on top of the pre-trained BERT model and jointly finetuning the model parameters on the downstream task. Each BERT model provides its own tokenisation which splits longer words into sub-tokens. The sequence tagger uses only the first sub-token as the input to the classifier, which then predicts a label for each token. 6 The mismatch between the number of sentences and triggers is caused by German particle verbs where the particle can be split from th"
2020.lrec-1.731,L16-1160,0,0.0149944,"it discourse relations, Versley (2010) presents a multilingual approach for data projection. He classifies German explicit discourse relations without German training data, solely based on the English annotations projected to German via word-aligned parallel text. He also presents a bootstrapping approach for a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. Other studies on German have also been focussed on discourse connectives. Stede et al. (1998; 2002) created a lexicon for German discourse markers, augmented with semantic relations (Scheffler and Stede, 2016). Gastel et al. (2011) present annotations for discourse connectives in the TüBa-D/Z (Telljohann et al., 2004), including a small number of causal connectives. A rule-based system for detecting a set of 8 causal German discourse connectives in spoken discourse has been presented by Bögel et al. (2014). Their system predicts whether or not a connective is causal and they also try to predict the causality type, i.e. Reason or Result. Automatic prediction of causal relations in text Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in t"
2020.lrec-1.731,P98-2202,0,0.0606705,"Missing"
2020.lrec-1.731,telljohann-etal-2004-tuba,0,0.212236,"Missing"
2020.lrec-1.731,S13-2001,0,0.0380343,"ng causal language. We present baseline results for a causal tagger on our new dataset in Section 5. and end with conclusions and suggestions for future work. 2. Related Work In this section, we give an overview over previous work on annotating causal relations. Temporal and causal relations It has often been noted that the concept of causality is closely linked to temporal relations, as a causal relation requires the temporal order of the two events involved, and many studies have looked at both phenomena together. Mirza et al. (2014) have annotated causal relations in the TempEval-3 corpus (UzZaman et al., 2013), with an annotation scheme inspired by TimeML (Pustejovsky et al., 2010). Based on Talmy (1988) and Wolff et al. (2005), they also distinguish whether the first event causes, enables or prevents the second event. Their annotations cover different parts of speech such as verbs, adpositions, adverbials and discourse connectives. In follow-up work (Mirza and Tonelli, 2016) they present a sieve-based system that jointly predicts temporal and causal relations in the TempEval-3 data and the TimeBank corpus (Pustejovsky et al., 2003). Their system makes use of a rich feature set, including morpho-sy"
2020.lrec-1.731,L16-1603,0,0.0315098,"Missing"
2020.lrec-1.731,P13-4001,0,0.0212306,"Support C ONSEQUENCE diesCause bedenkliche Folgen. 3 5970 by For the polar distinction, they report perfect agreement. For more details on the annotation scheme, we refer the reader to Dunietz et al. (2015; 2018). Figure 1: Causal annotations for the verb bereiten, visualised in the annotation tool Webanno. While the first version of the BeCause corpus included the label I NFERENCE for epistemic uses of causality, this label was given up in version 2.0 of the corpus (Dunietz et al. (2017a). We decided to follow their decision and only consider three types of causality. 4. online tool WebAnno (Yimam and Gurevych, 2013) (Figure 1). Each instance in the dataset was annotated by at least two annotators, and after the annotation process was completed, all disagreements were resolved by the two expert annotators. Table 2 shows inter-annotator agreement (IAA) scores for the different subsets of our data. We reAnnotating German Causal Language This section presents the data and annotation setup as well as inter-annotator agreement scores for the annotation of German causal language. 4.1. Confusion matrix for causal types (verbs) A NNOT 1 A NNOT 2 C ONSEQ . M OTIV. P URPOSE N ONE Data The data we annotate comes fro"
2020.lrec-1.878,C18-1139,0,0.256533,"utterances can predict the syntactic boundaries annotated in the SegCor corpus. They showed that while there is a correlation between gap length and surface syntax, gap length on its own is not sufficient for a reliable prediction of SLU boundaries. Our work builds on previous work on automatic boundary detection in German spoken language transcripts (Ruppenhofer and Rehbein, 2019) and tries to further improve the accuracy for SLU boundary detection. Ruppenhofer and Rehbein (2019) modelled the problem as a sequence tagging task and showed that neural models with contextual string embeddings (Akbik et al., 2018), based on the Flair library of Akbik et al. (2019), outperform a classical feature-based CRF classifier. This paper presents new experiments where we (i) test different neural architectures and task setups for SLU boundary detection, and (ii) investigate the potential benefits of additional training data from a different source of spoken language. This paper proceeds as follows. We discuss related work in Section 2. and present our dataset in Section 3. Our experiments and their results are described in sections 4. and 5. While we show that training data expansion for this task is not straigh"
2020.lrec-1.878,N19-4010,0,0.13254,"notated in the SegCor corpus. They showed that while there is a correlation between gap length and surface syntax, gap length on its own is not sufficient for a reliable prediction of SLU boundaries. Our work builds on previous work on automatic boundary detection in German spoken language transcripts (Ruppenhofer and Rehbein, 2019) and tries to further improve the accuracy for SLU boundary detection. Ruppenhofer and Rehbein (2019) modelled the problem as a sequence tagging task and showed that neural models with contextual string embeddings (Akbik et al., 2018), based on the Flair library of Akbik et al. (2019), outperform a classical feature-based CRF classifier. This paper presents new experiments where we (i) test different neural architectures and task setups for SLU boundary detection, and (ii) investigate the potential benefits of additional training data from a different source of spoken language. This paper proceeds as follows. We discuss related work in Section 2. and present our dataset in Section 3. Our experiments and their results are described in sections 4. and 5. While we show that training data expansion for this task is not straightforward even with data from the same domain (Secti"
2020.lrec-1.878,N19-1423,0,0.483044,"ion on speaker turns considerably improves results. We experimented with a feature-rich classification setup based on Conditional Random Fields (CRF) that allowed us to easily include additional information, such as PoS tags or lemmas. However, we also showed that the CRF classifier can be outperformed by a simpler neural model that incorporates contextualised string embeddings (Akbik et al., 2018; Akbik et al., 2019). Given the success of the neural model, we would like to test whether further improvements can be obtained with transfer learning based on BERT’s contextualised word embeddings (Devlin et al., 2019) (Section 5.). 3. Data This section presents our gold standard for the segmentation of German oral corpora, created in the SegCor (“Segmentation of Oral Corpora”) project,1 as well as the additional spoken language data we use in our training data expansion experiments. 3.1. SegCor The SegCor data has some features that distinguish it from most previous work. Our data represents conversational speech with two or more speakers that was recorded in nonlaboratory settings. Since tools based on the automatic processing of the audio signal do not work all that well on our data, we instead work with"
2020.lrec-1.878,L16-1147,0,0.0174895,"cture are shown in Table 4. When comparing our results to our previous work on the same dataset (Table 4, lower part), we see that the BERT model outperforms all previous models (CRF, Flair-FastText, Flair-FastText+Flair embeddings) except for the Flair model that was trained with our own customised embeddings (Flair-FastText+Flair+custom), in addition to the FastText and Flair embeddings provided by the Flair library (Akbik et al., 2019). These custom embeddings are character-based Flair embeddings that have been trained on ca. 11 million ‘sentences’ extracted from the open subtitles corpus (Lison and Tiedemann, 2016) and an in-house twitter dataset. All sentences with a length &gt; 60 characters have been removed, as have sentences that contained more than one comma and one period, question mark or exclamation mark. The punc8 7107 Taken from Devlin et al. (2019) and adapted to our setup. tuation marks were removed before training and the data was lowercased. The motivation for this setup was to train embeddings on text that is more similar to our spoken language transcripts. As shown above, our BERT sequence tagging model achieves results in the same range as the model stacked with the original FastText, Fla"
2020.lrec-1.878,P05-1056,0,0.0818523,"Missing"
2020.lrec-1.878,C12-2096,0,0.077911,"Missing"
2020.lrec-1.878,rehbein-etal-2014-kiezdeutsch,1,0.814425,"e-like units is not always one to one. Common deviations are as follows. First, a contribution may correspond to several SLUs as illustrated by (1). (1) 1 contribution : n SLUs a. &lt; c &gt;h ich weiß net ich glaub eher nich h h&lt; /c &gt; b. &lt; SLU &gt;h ich weiß net&lt; /SLU &gt; &lt; SLU &gt; ich glaub eher nich h h&lt; /SLU &gt; c. ‘I don’t know. I rather think not.’ Second, several contributions may jointly correspond to one SLU. (2) 3.2. KiDKo In addition to the rather small SegCor dataset we also have access to a much larger corpus of informal spoken youth language, the KiezDeutsch-Korpus (KiDKo) (Wiese et al., 2012; Rehbein et al., 2014). KiDKo contains spontaneous peergroup dialogues of adolescents from multiethnic Berlin-Kreuzberg (around 266,000 tokens) and a supplementary corpus with adolescent speakers from monoethnic Berlin-Hellersdorf (around 111,000 tokens, excluding punctuation). On the normalisation layer where punctuation is included, the token counts add up to around 359,000 tokens (main corpus) and 149,000 tokens (supplementary corpus). On the normalisation layer, the data includes punctuation and is segmented into sentence-like units. On top of the normalisation, the corpus comprises additional annotation layers"
2020.lrec-1.878,W11-3808,1,0.781091,"m as we have done before, but this time replacing the character-based contextual string embeddings of Akbik et al. (2018; Akbik et al. (2019) by the BERT embeddings. 5.1. SLU detection as sequence tagging In our experiments, we use the HuggingFace transformers library (Wolf et al., 2019) that provides pre-trained transformer models for different languages and tasks. As our input transcripts are lower-cased, we use the pre-trained German uncased BERT model (bert-base-german-dbmdzuncased).5 3 A similar method was used successfully in Søgaard (2011) for cross-lingual unsupervised parsing, and in Rehbein (2011) for self-training of monolingual parsers. 4 These transcripts, however, had only been annotated by one annotator so it was not clear whether the results might reflect a lower quality in the annotations. 5 The model has been trained by the MDZ Digital Library team (dbmdz) at the Bavarian State Library on 2,350,234,427 tokens of raw text, including Wikipedia, the EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. For details see https://github.com/dbmdz/german-bert. 7106 O G B E was what was bei in bei muttersprachlern untersucht native speakers investivated mutter ##spr"
2020.lrec-1.878,W15-5938,0,0.0693302,"Missing"
2020.lrec-1.878,schmidt-2014-research,1,0.804737,"“Segmentation of Oral Corpora”) project,1 as well as the additional spoken language data we use in our training data expansion experiments. 3.1. SegCor The SegCor data has some features that distinguish it from most previous work. Our data represents conversational speech with two or more speakers that was recorded in nonlaboratory settings. Since tools based on the automatic processing of the audio signal do not work all that well on our data, we instead work with the transcripts only. Our dataset consists of 33 documents with more than 54,000 lexical tokens originating from the FOLK corpus (Schmidt, 2014) that were divided into sentence-like units by the SegCor project. This data set was doubly annotated and disagreements were adjudicated (Westpfahl and Gorisch, 2018). Note that to avoid confusion, we reserve the term segment and related forms for the division of speech into chunks by the transcribers that was guided by silences in the speech signal. For the division of the material into sentence-like units we will use the term “SLU boundary detection”. 1 https://www1.ids-mannheim.de/prag/ muendlichekorpora/segcor.html 7103 The raw FOLK transcripts, which we take as our input and which lack SL"
2020.lrec-1.878,P11-2120,0,0.0292205,"ur task is to model SLU detection as a sequence tagging problem as we have done before, but this time replacing the character-based contextual string embeddings of Akbik et al. (2018; Akbik et al. (2019) by the BERT embeddings. 5.1. SLU detection as sequence tagging In our experiments, we use the HuggingFace transformers library (Wolf et al., 2019) that provides pre-trained transformer models for different languages and tasks. As our input transcripts are lower-cased, we use the pre-trained German uncased BERT model (bert-base-german-dbmdzuncased).5 3 A similar method was used successfully in Søgaard (2011) for cross-lingual unsupervised parsing, and in Rehbein (2011) for self-training of monolingual parsers. 4 These transcripts, however, had only been annotated by one annotator so it was not clear whether the results might reflect a lower quality in the annotations. 5 The model has been trained by the MDZ Digital Library team (dbmdz) at the Bavarian State Library on 2,350,234,427 tokens of raw text, including Wikipedia, the EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. For details see https://github.com/dbmdz/german-bert. 7106 O G B E was what was bei in bei mutters"
2020.lrec-1.878,A00-1012,0,0.475982,"Missing"
2020.lrec-1.878,W18-4913,0,0.0939406,"e application of Natural Language Processing tools (e.g. PoStaggers, syntactic parsers) to transcripts of spoken language. While various proposals have been made for how to divide spoken language in corpora into smaller units, typically these divisions were not guided by syntactic considerations. Instead, division into inter-pausal units is common (e.g. Hamaker et al. (1998) for the Switchboard corpus (John J. Godfrey, Edward Holliman, 1993)). For German, the SegCor project presented a proposal and guidelines for dividing transcribed speech into sentencelike units based on Topological Fields (Westpfahl and Gorisch, 2018; Westpfahl et al., 2019). The Topological Fields Model (Drach, 1937; H¨ohle, 1986) is a descriptive grammar formalism that captures regularities in German word order by dividing sentences in different verbal and non-verbal fields and describing their position with regard to the main verb. In a corpus-based study, Schmidt and Westpfahl (2018) then investigated how well the length of gaps between utterances can predict the syntactic boundaries annotated in the SegCor corpus. They showed that while there is a correlation between gap length and surface syntax, gap length on its own is not suffici"
2020.lrec-1.878,L16-1237,1,0.850272,"n (2019). Once we have extracted our input sequences from SegCor, we train the sequence tagger provided by the HuggingFace library with a batch size of 32 and a learning rate of 5e5 for three iterations on our data.7 The sequence tagging model was originally intended for NER and similar tasks. In our setup, however, we input an unsegmented sequence from our spoken transcripts and let the model predict for each token whether or not this token is followed by an SLU 6 The disambiguation between discourse markers and other adverbial forms was done based on automatically predicted finegrained PoS (Westpfahl and Schmidt, 2016). 7 We also experimented with other learning rates which gave inferior results on the development set. Macro Acc Macro F1 ID F1 B F1 O Embeddings/ Features 95.3 95.1 95.2 95.0 95.1 90.3 89.6 89.7 89.5 89.6 83.4 82.0 82.2 81.8 82.0 97.3 97.2 97.2 97.1 97.1 avg. 95.1 89.7 82.3 97.2 CRF BERT sequence tagging 1 2 3 4 5 94.7 92.3 95.1 94.8 95.4 88.3 83.4 89.6 89.3 90.2 79.7 71.3 82.0 81.6 83.1 96.9 95.5 97.1 97.0 97.4 Flair To accomodate BERT’s length restrictions, we extract sequences with a maximum length of 80 tokens as follows. We iterate over each token in the input data, looking for probable"
2020.udw-1.16,W19-8006,0,0.0202011,"Missing"
2020.udw-1.16,W17-0404,0,0.0650363,"Missing"
2020.udw-1.16,de-marneffe-etal-2014-universal,0,0.0772979,"Missing"
2020.udw-1.16,W10-0803,0,0.0406196,"ly in the basic annotation since these cases would involve an orphan relationship. Further, the apparent conjunction, where present, would need to be changed both in terms of POS and syntactic dependency. We might, for instance, treat und somewhat arbitrarily as an ADV related by advmod to the head of the right conjunct. 2.3 Presentational relative clause construction (PRC) The presentational relative construction (PRC) involves the combination of a semantically weak main clause and a relative clause which, rather than the main clause, contains the assertion of the utterance (Lambrecht, 1988; Duffield et al., 2010). The PRC is, therefore, normally more aptly paraphrased by a single sentence (29a) than by a sequence of sentences (29b). (29) You have some folks who deny she LOST when she suspended her campaign. a. Some folks deny she LOST when she suspended her campaign. b. #You have some folks. They deny she LOST when she suspended her campaign. 144 Figure 2: NFPK construction root root punct conj nsubj cc advmod punct Ich und lügen ? I and lie ? Ich und lügen ? I and lie ? nsubj (a) Subject and predicate, using enhanced UD (b) Subject and predicate, basic UD only Formally, PRCs mostly feature subject ga"
2020.udw-1.16,lacheret-etal-2014-rhapsodie,0,0.0299834,"s. Contrast the effect of the lie-test in response to a cleft (38) with that in response to the PRC (36) (38) A: It’s colleagues of mine who have done this. B: That’s a lie! 2 Lambrecht (1988) assumes PRCs are restricted to involve subject gaps in the relative clause but Duffield et al. (2010) report instances with object gaps in the relative clause. 145 a. It’s not them (but Pat who did it). b. #Nobody has done this. The Italian PoSTWITA treebank (Sanguinetti et al., 2018) contains an instance of what we call PRC that is simply annotated as a relative clause.3 The treebank for spoken French (Lacheret et al., 2014) handles PRC instances the same way. But given their special properties, presentational relative clauses should be treated differently from restrictive and appositive relative clauses. A simple option involving only basic UD would be to only subtype the acl-relation further and introduce acl:presrel. Figure 3: Presentational Relative Construction root advcl obj nsubj det acl:presrel nsubj ccomp mark nsubj nsubj obj det You have some folks who deny she LOST when she suspended her campaign . The annotation strategy for the English PRC can also be carried over to Italian, French and German. Note"
2020.udw-1.16,L18-1279,0,0.0238837,". a. He doesn’t want a car. b. # He can’t build it. Presentational relative clauses are also not the same as the relative clauses in cleft-sentences. Contrast the effect of the lie-test in response to a cleft (38) with that in response to the PRC (36) (38) A: It’s colleagues of mine who have done this. B: That’s a lie! 2 Lambrecht (1988) assumes PRCs are restricted to involve subject gaps in the relative clause but Duffield et al. (2010) report instances with object gaps in the relative clause. 145 a. It’s not them (but Pat who did it). b. #Nobody has done this. The Italian PoSTWITA treebank (Sanguinetti et al., 2018) contains an instance of what we call PRC that is simply annotated as a relative clause.3 The treebank for spoken French (Lacheret et al., 2014) handles PRC instances the same way. But given their special properties, presentational relative clauses should be treated differently from restrictive and appositive relative clauses. A simple option involving only basic UD would be to only subtype the acl-relation further and introduce acl:presrel. Figure 3: Presentational Relative Construction root advcl obj nsubj det acl:presrel nsubj ccomp mark nsubj nsubj obj det You have some folks who deny she"
2020.udw-1.16,L16-1376,0,0.0229223,"to support multilingual NLP applications on the one hand and to facilitate the linguistic study of similarities and differences from a typological perspective. While the UD framework also covers the annotation of parts of speech and of morphological features, the annotation of syntactic dependencies is at its core. UD currently uses 37 labels for broadly attested grammatical relations. In addition to these basic UD dependencies, UD allows for an enhanced representation that “aims to make implicit relations between content words more explicit by adding relations and augmenting relation names” (Schuster and Manning, 2016). The most prominent examples of added relations in enhanced representations are links that help propagate relations over conjunctions. While the basic dependency structure is assumed to form a (possibly non-projective) tree, enhanced UD representations often no longer are trees. The subtyping of relations serves to capture more fine-grained language-specific constructions (de Marneffe et al., 2014). For instance, in English, subjects of passive clauses bear the relation nsubj:pass. In this paper, we discuss several constructions that are found in spoken language and data from social media, wh"
2020.udw-1.16,silveira-etal-2014-gold,0,0.0429109,"Missing"
2020.udw-1.16,W16-1709,0,0.0123888,"nstructions with similar challenges will be encountered. To allow for consistent and expressive analyses, we think the UD community would benefit from discussing which mechanisms to use for which kinds of constructions. We have explored the use of enhanced UD annotations, keeping the relation inventory the same, and as an alternative introducing new dependency relations and relation subtypes, which may lead to sparsely attested relation types. One option we have not explored but which the GUM corpus uses is a kind of constructional annotation (specifically, of sentence types) in the metadata (Zeldes and Simonson, 2016). This would, however, not localize which words are part of the construction. Other ways to track constructions may be conceivable. Whatever the annotation mechanisms used, we think that adding linguistic analyses for such constructions to the UD guidelines might help to improve annotation consistency across languages, and thus the quality of the treebanks. Existing UD treebanks already feature some well-represented constructions that are treated inconsistently between treebanks and/or languages along the lines we discussed. For instance, the two clauses of the paratactic correlative construct"
2021.eacl-main.27,W19-3501,0,0.0450428,"Missing"
2021.eacl-main.27,N19-1423,0,0.0617662,"Missing"
2021.eacl-main.27,P16-1191,0,0.0178745,"istical significance testing (paired t-test at p < 0.05): ∗ : better than fastTextCommon Crawl ; † : better than BERT Table 5: Comparisons of different classifiers. word. A word may be associated with more than one of the 8 emotion categories. We represent each comparison by the set of emotion categories for the words also occurring in the NRC lexicon. WordNet Supersenses (SUPER). We also consider WordNet supersenses (Miller et al., 1990) in our experiments. They represent a set of 45 coarsegrained semantic categories and have been found effective in related tasks, such as sentiment analysis (Flekova and Gurevych, 2016). A comparison is represented by the set of semantic categories associated with the words contained in the comparison. 5 5.1 Experiments Classification Performance As a supervised classifier, we chose BERT-Large (Devlin et al., 2019). We initially experimented with two versions: one in which we fine-tune the model by adding a layer on top of the pre-trained model and a SVM (Joachims, 1999) that is trained on the BERT embeddings of the final layer. Since we did not measure any statistically significant difference between these models, we decided in favor of SVM due to its simplicity. We carry o"
2021.eacl-main.27,W19-3510,0,0.0120138,"ill be invented in this work is not directed towards specific invididuals or identity groups. Therefore, we believe that this procedure is justifiable. In principle, creating morally disputable content as part of research is not unusual. Both in plagiarism detection (Potthast 2 2 Related Work Datasets in abusive language detection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative comparisons (similes), have been examined with regard to sentiment. Qadir et al. (2015) investigate automatic polarity classification of comparisons while Other foci are"
2021.eacl-main.27,D14-1215,0,0.0312131,"Missing"
2021.eacl-main.27,P11-1032,0,0.155115,"Missing"
2021.eacl-main.27,E17-2068,0,0.0218414,"e that the boundary between implicit and explicit insults is not clear-cut and that there are ambiguous abusive words contained in our comparisons that still have a strong semantic similarity to abusive words from a lexicon of abusive words. We took the lexicon from Wiegand et al. (2018), computed a centroid embedding vector of its entries and ranked our comparisons according to the semantic similarity to the centroid. A comparison was represented by the embedding vector of the word in that comparison whose similarity was highest to the centroid. As embeddings we chose the fastText emeddings (Joulin et al., 2017) induced on Common Crawl.8 Emotions (EMO). In order to take into account the recently reported correlation between abusive language and emotions (Rajamanickam et al., 2020), we use the NRC lexicon (Mohammad and Turney, 2013) which lists the emotion categories associated to a particular frequent English 8 363 https://commoncrawl.org Classifier Prec majority 25.0 random 50.9 fastTextplain 60.6 fastTextCommon Crawl 68.0 BERTonly pattern 53.7 BERTonly vehicle 67.1 BERT 70.2 linguistic featuresonly auto 65.9 linguistic features 68.9 BERT+linguistic featuresonly auto 72.2 BERT+linguistic features 72"
2021.eacl-main.27,W18-4401,0,0.0382961,"Missing"
2021.eacl-main.27,W17-3008,0,0.0206378,"the type of abusive language that will be invented in this work is not directed towards specific invididuals or identity groups. Therefore, we believe that this procedure is justifiable. In principle, creating morally disputable content as part of research is not unusual. Both in plagiarism detection (Potthast 2 2 Related Work Datasets in abusive language detection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative comparisons (similes), have been examined with regard to sentiment. Qadir et al. (2015) investigate automatic polarity classification"
2021.eacl-main.27,C10-2115,0,0.127529,"Missing"
2021.eacl-main.27,D15-1019,0,0.165835,"previously examined tasks and that the established datasets for general language abuse are less suitable for this task. (9) Your face is as pale as a sheet. (10) You look like you haven’t slept in days. (11) Talking to you is like walking against a strong wind. Unlike many other datasets for abusive language detection, we create our new dataset with abusive comparisons by inventing instances (i.e. comparisons) rather than by annotating automatically extracted instances. This design choice is necessary since existing datasets contain either insufficient or biased comparisons: The dataset from Qadir et al. (2015) includes about 180 implicitly abusive comparisons, however, we found, next to many nearduplicates, a heavy bias towards very few recurring images (e.g. You behave like a child, You look like a monkey). We observed the same phenomenon when extracting comparisons from Twitter directly. Established datasets for abusive language detection (Founta et al., 2018; Zampieri et al., 2019) contain just about 30-40 abusive comparisons. Our dataset will be created via crowdsourcing. Of course, having abusive comparisons be invented this way will inevitably result in some articificial data. However, we thi"
2021.eacl-main.27,2020.acl-main.394,0,0.0263883,"trong semantic similarity to abusive words from a lexicon of abusive words. We took the lexicon from Wiegand et al. (2018), computed a centroid embedding vector of its entries and ranked our comparisons according to the semantic similarity to the centroid. A comparison was represented by the embedding vector of the word in that comparison whose similarity was highest to the centroid. As embeddings we chose the fastText emeddings (Joulin et al., 2017) induced on Common Crawl.8 Emotions (EMO). In order to take into account the recently reported correlation between abusive language and emotions (Rajamanickam et al., 2020), we use the NRC lexicon (Mohammad and Turney, 2013) which lists the emotion categories associated to a particular frequent English 8 363 https://commoncrawl.org Classifier Prec majority 25.0 random 50.9 fastTextplain 60.6 fastTextCommon Crawl 68.0 BERTonly pattern 53.7 BERTonly vehicle 67.1 BERT 70.2 linguistic featuresonly auto 65.9 linguistic features 68.9 BERT+linguistic featuresonly auto 72.2 BERT+linguistic features 72.9 BERT+linguistic feat. on biased dataset 77.4 human baseline (upper bound) 77.6 Rec 50.0 51.0 53.9 67.5 53.2 66.9 70.0 65.9 68.9 72.1 72.8 77.3 77.5 F1 33.3 51.0 57.1 67."
2021.eacl-main.27,W17-1101,1,0.877623,"etection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative comparisons (similes), have been examined with regard to sentiment. Qadir et al. (2015) investigate automatic polarity classification of comparisons while Other foci are fairly unlikely to be abusive. 359 3 The supplementary material, which consists of the supplementary notes and the new dataset, is available at: https://github.com/miwieg/implicitly_ abusive_comparisons Component Topic (T) Eventuality (E) Comparator (C) Property (P) Pattern (T+E+C+P) Vehicle Example Sentence [You] are as smart"
2021.eacl-main.27,W12-2103,0,0.0409073,"to do research on this novel research topic. Having crowdworkers invent instances of abusive language may raise ethical concerns. However, the type of abusive language that will be invented in this work is not directed towards specific invididuals or identity groups. Therefore, we believe that this procedure is justifiable. In principle, creating morally disputable content as part of research is not unusual. Both in plagiarism detection (Potthast 2 2 Related Work Datasets in abusive language detection mostly focus on different targets (e.g. Islamophobia (Waseem and Hovy, 2016), antisemitism (Warner and Hirschberg, 2012), misogyny (Anzovino et al., ´ 2018)), different languages (e.g. Spanish (AlvarezCarmona et al., 2018), Arabic (Mubarak et al., 2017), Portuguese (Fortuna et al., 2019)) or different domains (e.g. Twitter (Waseem and Hovy, 2016), Facebook (Kumar et al., 2018), Wikipedia (Wulczyn et al., 2017)). Despite some theoretical work outlining distinct subtypes of abusive language (Waseem et al., 2017), there has been little work on datasets that focus on particular subtypes. Schmidt and Wiegand (2017) present a more detailed overview on abusive language detection. Comparisons, particularly figurative c"
2021.eacl-main.27,W17-3012,0,0.423768,"A New Dataset and Linguistic Analysis Maja Geulig Michael Wiegand Digital Age Research Center (D!ARC) Institute of Computational Linguistics Heidelberg University Alpen-Adria-Universit¨at Klagenfurt D-69120 Heidelberg, Germany AT-9020 Klagenfurt, Austria geulig@cl.uni-heidelberg.de michael.wiegand@aau.at Josef Ruppenhofer Leibniz Institute for German Language D-68161 Mannheim, Germany ruppenhofer@ids-mannheim.de Abstract Though there has been much work on abusive language detection in general, there is has been comparatively little work focusing on implicit forms of abusive language (4)-(5) (Waseem et al., 2017). We examine the task of detecting implicitly abusive comparisons (e.g. Your hair looks like you have been electrocuted). Implicitly abusive comparisons are abusive comparisons in which abusive words (e.g. dumbass or scum) are absent. We detail the process of creating a novel dataset for this task via crowdsourcing that includes several measures to obtain a sufficiently representative and unbiased set of comparisons. We also present classification experiments that include a range of linguistic features that help us better understand the mechanisms underlying abusive comparisons. 1 (4) I haven’"
2021.eacl-main.27,N16-2013,0,0.534371,"erstand the mechanisms underlying abusive comparisons. 1 (4) I haven’t had an intelligent conversation with a woman in my whole life. (5) Why aren’t there any Mexicans on Star Trek? Because they don’t work in the future either. Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. The definition we follow in this work also restricts abusive language to those utterances that are made to deliberately insult the target. A second requirement of an utterance to be considered abusive is that the target itself has to perceive the utterance as abusive. Due to the rise of user-generated web content, the amount of abusive language is steadily growing. NLP methods are required to focus human review efforts towards the most relevant"
2021.eacl-main.27,N19-1060,1,0.879589,"lt the target. A second requirement of an utterance to be considered abusive is that the target itself has to perceive the utterance as abusive. Due to the rise of user-generated web content, the amount of abusive language is steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. 1 By implicit we understand abusive language that is not conveyed by (unambiguously) abusive words (e.g. dumbass, bimbo, scum). Detailed analysis on the output of existing classifiers has also revealed that currently only explicit abuse can be reliably detected (Wiegand et al., 2019). Given that implicit abuse is a challenging problem, we believe that the only reasonable approach to solve this problem is to address specific subtypes individually rather than consider all types of implicit abuse at once. In this paper, we examine implicitly abusive comparisons. A comparison is the act of evaluating two or more things by determining the relevant characteristics of each thing and to determine which characteristics of each are similar/different to the other (Bredin, 1998). By an abusive comparison, we understand a comparison that is perceived as abusive. In this work, we only"
2021.eacl-main.27,N18-1095,1,0.799713,"comparison is the act of evaluating two or more things by determining the relevant characteristics of each thing and to determine which characteristics of each are similar/different to the other (Bredin, 1998). By an abusive comparison, we understand a comparison that is perceived as abusive. In this work, we only consider those comparisons in which no (explicitly) abusive words are contained (6)-(8). Those comparisons are referred to as implicitly abusive comparisons. We exclude comparisons with abusive words since they can be easily detected with recent lexical resources for language abuse (Wiegand et al., 2018). http://thelawdictionary.org/ (6) You have the face of someone only a mother could love. (7) Your hair looks like you have been electrocuted. (8) You run like a headless chicken. We address abusive comparisons since they make up a large proportion of comparisons on 358 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 358–368 April 19 - 23, 2021. ©2021 Association for Computational Linguistics the most related dataset by Qadir et al. (2015). That dataset was created to automatically detect the sentiment of a comparison: one has"
2021.eacl-main.27,N16-1092,1,0.827138,"uthor describes the emotional frame of mind of the target (23). Since all our comparisons are negative, typical emotional frames are pain, sorrow, exhaustion or shock (as in (23)). The author of such comparisons does not necessarily evaluate the target. For example, if one states that some other person is in pain, this is not meant as some criticism, but rather some concern. Such comparisons are rarely perceived as abusive. (22) You look like an overfed cat. (ABUSE) (23) You look like a shocked cat. (OTHER) This distinction bears a resemblance to the distinction of sentiment views proposed by Wiegand et al. (2016). That work proposes a binary distinction into speaker views, which resembles evaluative Feature Cohen’s κ ABSURD CONTRAD DEHUM FIGUR TABOO VIEW 0.82 1.00 0.88 0.63 0.74 0.73 Table 4: Agreement on manual features. comparisons, and actor views, which resembles descriptions of the emotional frame of mind. Wiegand et al. (2016) also provide a list of verbs, nouns and adjectives classified into either of the two categories. Due to the fact that this lexicon seems inaccurate when it comes to ambiguous words7 , we annotated the binary distinction of evaluation vs. emotional frame of mind manually in"
2021.eacl-main.27,N19-1144,0,0.133299,"enting instances (i.e. comparisons) rather than by annotating automatically extracted instances. This design choice is necessary since existing datasets contain either insufficient or biased comparisons: The dataset from Qadir et al. (2015) includes about 180 implicitly abusive comparisons, however, we found, next to many nearduplicates, a heavy bias towards very few recurring images (e.g. You behave like a child, You look like a monkey). We observed the same phenomenon when extracting comparisons from Twitter directly. Established datasets for abusive language detection (Founta et al., 2018; Zampieri et al., 2019) contain just about 30-40 abusive comparisons. Our dataset will be created via crowdsourcing. Of course, having abusive comparisons be invented this way will inevitably result in some articificial data. However, we think only thus can we produce a dataset of reasonable size that has also a very low degree of bias which are two important requirements to be able to do research on this novel research topic. Having crowdworkers invent instances of abusive language may raise ethical concerns. However, the type of abusive language that will be invented in this work is not directed towards specific i"
2021.eacl-main.28,D17-1169,0,0.0319472,"lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step largely relies on resources that exist only for wellresourced languages, such as WordNet, sentiment intensity datasets or sentiment-view lexicons. Recently, there has been a general interest in exploiting extralinguistic information for natural language processing. Emoticons, such as :-), have been found useful for sentiment analysis, particularly emotion classification (Purver and Battersby, 2012). Emojis represent an even more fine-grained set of icons. Felbo et al. (2017) exploit them for pretraining neural models to produce a text representation of emotional content. Since this approach relies on a representative sample of tweets containing emojis, only the 64 most frequently occurring emojis are considered. This set, however, does not contain the very predictive emojis for abusive language detection (e.g. middle finger). Corazza et al. (2020) follow an approach similar to Felbo et al. (2017) in that they pretrain a language model with the help of emoji informarion. However, unlike Felbo et al. (2017), their emoji-based masked language model is evaluated for"
2021.eacl-main.28,L18-1686,0,0.0125943,"tweets containing the middle finger (Table 1). In order to obtain 10k Portuguese and German tweets more quickly, we included tweets with other predictive emojis. We extracted tweets containing one of the 4 most predictive emojis: face vomiting, pile of poo, angry face or middle finger. These 4 emojis are drawn from our English data (Table 1) in order to further demonstrate crosslingual validity. The distribution of emojis reflects their natural distribution on Twitter. For non-abusive text we sampled sentences from the Portuguese and German versions of the Web As Corpus (Baroni et al., 2009; Filho et al., 2018) from which we also induced word embeddings with word2vec (Mikolov et al., 2013). We decided against pre-trained Twitter embeddings since for many languages such resources are not available. We opted for a setting applicable to most languages. Evaluation. We evaluate our emoji-based lexicons on the Portuguese dataset from Fortuna et al. (2019) and the two German datasets from GermEval (Wiegand et al., 2018b; Struß et al., 2019). These are datasets for the classification of abusive microposts. As in our evaluation on English data (Table 6), we refrain from an in-domain evaluation since again we"
2021.eacl-main.28,W19-3510,0,0.0149415,") in order to further demonstrate crosslingual validity. The distribution of emojis reflects their natural distribution on Twitter. For non-abusive text we sampled sentences from the Portuguese and German versions of the Web As Corpus (Baroni et al., 2009; Filho et al., 2018) from which we also induced word embeddings with word2vec (Mikolov et al., 2013). We decided against pre-trained Twitter embeddings since for many languages such resources are not available. We opted for a setting applicable to most languages. Evaluation. We evaluate our emoji-based lexicons on the Portuguese dataset from Fortuna et al. (2019) and the two German datasets from GermEval (Wiegand et al., 2018b; Struß et al., 2019). These are datasets for the classification of abusive microposts. As in our evaluation on English data (Table 6), we refrain from an in-domain evaluation since again we want to avoid topic/author biases (§1). Instead, lexicon-based classifiers and a crosslingual approach are used as baselines. The former classifiers predict a micropost as abusive if one abusive word according to the lexicon has been found. In addition to the two variants of hurtlex (Bassignana et al., 2018), hl-conservative and hl-inclusive,"
2021.eacl-main.28,2020.acl-main.373,0,0.0110379,"nambiguously) abusive words; abusive words are identified with the help of the lexicon from Wiegand et al. (2018a) positive polar expressions rarely co-occur with abusive language, negative polar expressions, however, do; the polar expressions are obtained from the Subjectivity Lexicon (Wilson et al., 2005) 2nd person pronouns are typical of abusive usage: you are a bitch; 1st person pronouns are likely to indicate non-abusive usage: I am a bitch quotation signs indicate reported speech; a tweet may report an abusive remark, however, the reported remark itself may not be perceived as abusive (Chiril et al., 2020) a typical means of expressing high emotional intensity Table 8: Features for disambiguating a potentially abusive word (referred to as target word); context is defined as a window of 4 words neighbouring the target word. class abusive non abusive all bitch freq perc 248 24.8 752 75.2 1000 100.0 fuck freq perc 210 21.0 790 79.0 1000 100.0 bitch fuck approach classifier Acc F1 Acc F1 majority SVM 75.2 42.9 79.0 44.1 heuristic baseline SVM 74.3 58.3 77.5 57.6 text classif. (Kaggle) BERT 28.0 57.0 61.7 70.1 text classif. (Davidson) BERT 75.9 60.1 80.9 65.7 emoji SVM 77.3 66.3 82.9 71.7 word-speci"
2021.eacl-main.28,2020.findings-emnlp.84,0,0.0238975,"atural language processing. Emoticons, such as :-), have been found useful for sentiment analysis, particularly emotion classification (Purver and Battersby, 2012). Emojis represent an even more fine-grained set of icons. Felbo et al. (2017) exploit them for pretraining neural models to produce a text representation of emotional content. Since this approach relies on a representative sample of tweets containing emojis, only the 64 most frequently occurring emojis are considered. This set, however, does not contain the very predictive emojis for abusive language detection (e.g. middle finger). Corazza et al. (2020) follow an approach similar to Felbo et al. (2017) in that they pretrain a language model with the help of emoji informarion. However, unlike Felbo et al. (2017), their emoji-based masked language model is evaluated for zero-shot abusive language detection. The task is also considered in a multilingual setting: the target languages are English, German, Italian and Spanish. The improvements that Corazza et al. (2020) report over baseline language models that do not explicitly incorporate emoji information are only limited. Our work extends Felbo et al. (2017) and Corazza et al. (2020) in that w"
2021.eacl-main.28,N19-1423,0,0.0157172,"ibrary.com/icon/anger-icon-14.html about 7,000 English words. For our experiments on Portuguese and German data, we created similar word lists following Wiegand et al. (2018a). Tasks. In this work, there are two types of tasks: lexicon induction tasks in which we rank negative polar expressions where the high ranks should be abusive words, and classification of abusive microposts. The former is evaluated with precision at rank n (P@n), while the latter is evaluated with accuracy and macro-average F-score. Supervised Micropost Classification with BERT. In many experiments, we employ BERTLARGE (Devlin et al., 2019) as a baseline for stateof-the-art text classification for detecting abusive microposts. We always fine-tune the pretrained model by adding another layer on top of it. (The supplementary notes contain more details regarding all classifiers employed in this paper.) 4 4.1 Inducing a Lexicon of Abusive Words Methods for Lexicon Induction Pointwise Mutual Information (PMI). A standard method for inducing a lexicon from labeled documents is to rank the words according to the PMI with the target class (Turney, 2002). We use tweets in which either of the above emojis occur as abusive documents. In or"
2021.eacl-main.28,W17-5216,0,0.0557359,"Missing"
2021.eacl-main.28,D18-1471,0,0.0395923,"Missing"
2021.eacl-main.28,W18-5117,0,0.0238876,"or Computational Linguistics to detect abusive language primarily focuses on the detection of explicitly abusive language, i.e. abusive language that is conveyed by abusive words. Such an approach is currently the most effective clue known for cross-domain classification (Wiegand et al., 2018a). In general, other types of abusive language that are more implicit, such as sarcasm, jokes or stereotypes, require more contextual interpretation of words. Supervised classification is theoretically able to conduct such contextual interpretation. However, it has been reported to ˇ perform very poorly (Karan and Snajder, 2018; Arango et al., 2019) on this task because the biases these classifiers exploit are unlikely to be present across different datasets (Wiegand et al., 2019). Therefore, we focus on explicitly abusive language in this work, since there are no ways of reliably detecting implicitly abusive language. Despite the existence of lexicons for abusive words, induction methods are required, since new abusive words enter language constantly. Further, there are only few lexicons available in languages other than English. The aim of our work is not to detect completely new types of abusive language but to f"
2021.eacl-main.28,2020.alw-1.17,0,0.0181527,"the dataset from Holgate et al. (2018). That dataset was the only existing dataset with word-specific annotation that was available to us at the time we carried out our experiments so that we could use it as one baseline.8 For each of the two words, we extracted 1,000 tweets in which it occurs and had them annotated via crowdsourcing (ProlificAcademic9 ). Each tweet was annotated as abusive or profane based on the majority of 5 annotators (native speakers of English). (The supplementary notes contain the annotation guidelines.) 8 Meanwhile, two further datasets by Pamungkas et al. (2020) and Kurrek et al. (2020) have been made publicly available which might also be suitable for the kind of evaluation we present in our work. 9 www.prolific.co Baselines for Disambiguation Text Classification. We train a supervised text classifier (BERT) on each of the following two large datasets (containing several thousand microposts) manually annotated on the micropost level. The dataset from Davidson et al. (2017) distinguishes between the 3 classes: hate speech, offensive language and other. The first category matches our definition of abusive language whereas the second category resembles our category of profane"
2021.eacl-main.28,P09-1113,0,0.0612178,"by one person to another.1 In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. Due to the rise of user-generated web content, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. Building classifiers for abusive language detection requires expensive manually labeled data. In this paper we explore distant supervision (Mintz et al., 2009) for abusive language detection in which abusive emojis serve as a heuristic to identify abusive language (1)-(8). These texts are subsequently used as training data. The advantage 1 http://thelawdictionary.org Recently, there has been significant criticism of in-domain supervised classification in abusive language detection, whose evaluation has been shown to produce overly optimistic classification scores. They are the result of biases in the underlying datasets. Wiegand et al. (2019) show that on the most popular dataset for this task (Waseem and Hovy, 2016), classifiers learn co-incidental"
2021.eacl-main.28,2020.lrec-1.765,0,0.0241292,"ly ambiguous and frequent on the dataset from Holgate et al. (2018). That dataset was the only existing dataset with word-specific annotation that was available to us at the time we carried out our experiments so that we could use it as one baseline.8 For each of the two words, we extracted 1,000 tweets in which it occurs and had them annotated via crowdsourcing (ProlificAcademic9 ). Each tweet was annotated as abusive or profane based on the majority of 5 annotators (native speakers of English). (The supplementary notes contain the annotation guidelines.) 8 Meanwhile, two further datasets by Pamungkas et al. (2020) and Kurrek et al. (2020) have been made publicly available which might also be suitable for the kind of evaluation we present in our work. 9 www.prolific.co Baselines for Disambiguation Text Classification. We train a supervised text classifier (BERT) on each of the following two large datasets (containing several thousand microposts) manually annotated on the micropost level. The dataset from Davidson et al. (2017) distinguishes between the 3 classes: hate speech, offensive language and other. The first category matches our definition of abusive language whereas the second category resembles"
2021.eacl-main.28,D17-1117,0,0.0162929,"his paper. • We use emojis to disambiguate the context of potentially abusive words. We exemplify this on the two ambiguous and frequent words fuck and bitch. A by-product is a dataset of mentions of these words annotated in context. The supplementary material2 to this paper includes all resources newly created for our research and notes on implementation details. 2 https://github.com/miwieg/ emojis for abusive language detection 2 Related Work Abusive language detection is mostly framed as a supervised learning task (Schmidt and Wiegand, 2017). Feature-based (Nobata et al., 2016) and neural (Pavlopoulos et al., 2017) methods are applied. Lexicon induction for abusive language detection has received only little attention in previous work, the exceptions being Razavi et al. (2010) who present a lexicon generated using adaptive learning, Gitari et al. (2015) who bootstrap hate verbs and Wiegand et al. (2018a) who induce a lexicon of abusive words. This lexicon is currently the best performing lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step largely relies on resources that exist only for wellresourced languages, such as"
2021.eacl-main.28,D14-1162,0,0.0841241,"Missing"
2021.eacl-main.28,P19-1493,0,0.0185399,"d base lexicon). Moreover, we consider Wiegand-translated, which is the English lexicon from Wiegand et al. (2018a) translated to the target language via GoogleTranslate7 . Unlike Wiegand-replic, this lexicon is cheap to construct as it only requires the original English lexicon. Our crosslingual baseline exploits the abundance of labeled training data for abusive language detection on English and neural methods to close the language gap between English and the target language. We use multilingual BERT in which English, Portuguese and German share the same representation space. As proposed by Pires et al. (2019), we train a text classifier on an English dataset for abusive language detection and test the resulting multilingual model on the Portuguese or German microposts. The model that is learnt on English should be usable on the other languages as well, since the three languages share the same representation space. Our crosslingual approach is trained on the dataset from Zampieri et al. (2019), which like our non-English datasets originates from Twitter. Table 7 shows the results. We also added an upper bound for our emoji-based approach (emoji+manual) in which we also include abusive words manuall"
2021.eacl-main.28,E12-1049,0,0.0272403,") who induce a lexicon of abusive words. This lexicon is currently the best performing lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step largely relies on resources that exist only for wellresourced languages, such as WordNet, sentiment intensity datasets or sentiment-view lexicons. Recently, there has been a general interest in exploiting extralinguistic information for natural language processing. Emoticons, such as :-), have been found useful for sentiment analysis, particularly emotion classification (Purver and Battersby, 2012). Emojis represent an even more fine-grained set of icons. Felbo et al. (2017) exploit them for pretraining neural models to produce a text representation of emotional content. Since this approach relies on a representative sample of tweets containing emojis, only the 64 most frequently occurring emojis are considered. This set, however, does not contain the very predictive emojis for abusive language detection (e.g. middle finger). Corazza et al. (2020) follow an approach similar to Felbo et al. (2017) in that they pretrain a language model with the help of emoji informarion. However, unlike"
2021.eacl-main.28,W17-1101,1,0.814509,"that we make publicly available along with all other resources created in this paper. • We use emojis to disambiguate the context of potentially abusive words. We exemplify this on the two ambiguous and frequent words fuck and bitch. A by-product is a dataset of mentions of these words annotated in context. The supplementary material2 to this paper includes all resources newly created for our research and notes on implementation details. 2 https://github.com/miwieg/ emojis for abusive language detection 2 Related Work Abusive language detection is mostly framed as a supervised learning task (Schmidt and Wiegand, 2017). Feature-based (Nobata et al., 2016) and neural (Pavlopoulos et al., 2017) methods are applied. Lexicon induction for abusive language detection has received only little attention in previous work, the exceptions being Razavi et al. (2010) who present a lexicon generated using adaptive learning, Gitari et al. (2015) who bootstrap hate verbs and Wiegand et al. (2018a) who induce a lexicon of abusive words. This lexicon is currently the best performing lexicon for the task. It has been induced with the help of a (seed) base lexicon which had been manually annotated. The bootstrapping step large"
2021.eacl-main.28,D08-1061,0,0.037343,"n the basis of the projected embeddings we rank the negative polar expressions from our vocabulary (§3). Recall-based Expansion by Label Propagation (LP). While the very high ranks of an induction method typically coincide with the target class (in our case: abusive words), the lower a rank is, the more likely we are to encounter other words. Taking the high ranks as abusive seeds and then applying some form of label propagation on a wordsimilarity graph may increase the overall coverage of abusive words found. More specifically, we apply the Adsorption label propagation algorithm from junto (Talukdar et al., 2008) on a wordsimilarity graph where the words of our vocabulary are nodes and edges encode cosine-similarities of their embeddings. As negative (i.e. non-abusive) seeds, we take the most frequently occurring words from our vocabulary since they are unlikely to represent abusive words. In order to produce a meaningful comparison to PMI and projection-based induction, we need to convert the categorical output of label propagation to a ranking of our entire vocabulary. We achieve this by ranking the words pre5 We take the version with 200 dimensions which is a very frequently used configuration for"
2021.eacl-main.28,P02-1053,0,0.0456977,"Missing"
2021.eacl-main.28,W12-2103,0,0.1397,"Missing"
2021.eacl-main.28,N16-2013,0,0.161106,"sized ass kicking you little Twitt (3) @USER I challenge you to go on a diet you fat cunt (4) @USER You are so so stupid you monkey face (5) Send your location, I’ll send some killers (6) @USER @USER A vote for toddstone or any liberal. Id rather flush a toilet. (7) Fuck the 12 fuck the cops we aint forgot about you, kill em all kill em all (8) @USER She is such a disgusting despicable human being! Ugh! Introduction Abusive or offensive language is defined as hurtful, derogatory or obscene utterances made by one person to another.1 In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. Due to the rise of user-generated web content, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. Building classifiers for abusive language detection requires expensive manually labeled data. In this paper we explore distant supervision (Mintz et al., 2009) for abusive language detection in which abusive emojis serve as a heuristic to identify abus"
2021.eacl-main.28,N19-1060,1,0.926239,"ive language detection requires expensive manually labeled data. In this paper we explore distant supervision (Mintz et al., 2009) for abusive language detection in which abusive emojis serve as a heuristic to identify abusive language (1)-(8). These texts are subsequently used as training data. The advantage 1 http://thelawdictionary.org Recently, there has been significant criticism of in-domain supervised classification in abusive language detection, whose evaluation has been shown to produce overly optimistic classification scores. They are the result of biases in the underlying datasets. Wiegand et al. (2019) show that on the most popular dataset for this task (Waseem and Hovy, 2016), classifiers learn co-incidental correlations between specific words (e.g. football or sport) and the abusive class label. Such spurious correlations help classifiers to correctly classify difficult microposts on that particular dataset. Arango et al. (2019) show that since on the dataset from Waseem and Hovy (2016) the majority of abusive tweets originate from just 2 authors, classifiers learn the authors’ writing style rather than abusive language. In order to avoid an evaluation affected by such topic or author bia"
2021.eacl-main.28,N18-1095,1,0.162335,"language. In order to avoid an evaluation affected by such topic or author biases, we focus on learning a lexicon of abusive language. A lexicon-based approach 369 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 369–380 April 19 - 23, 2021. ©2021 Association for Computational Linguistics to detect abusive language primarily focuses on the detection of explicitly abusive language, i.e. abusive language that is conveyed by abusive words. Such an approach is currently the most effective clue known for cross-domain classification (Wiegand et al., 2018a). In general, other types of abusive language that are more implicit, such as sarcasm, jokes or stereotypes, require more contextual interpretation of words. Supervised classification is theoretically able to conduct such contextual interpretation. However, it has been reported to ˇ perform very poorly (Karan and Snajder, 2018; Arango et al., 2019) on this task because the biases these classifiers exploit are unlikely to be present across different datasets (Wiegand et al., 2019). Therefore, we focus on explicitly abusive language in this work, since there are no ways of reliably detecting i"
2021.eacl-main.28,H05-1044,0,0.136666,"context? which pronouns are in context? quotation signs in tweet? presence of exclamation sign? explanation may be helpful in order to learn phrases such as fuck off; larger context is avoided since we are likely to overfit to particular domains target word is likely to be abusive if it co-occurs with other (unambiguously) abusive words; abusive words are identified with the help of the lexicon from Wiegand et al. (2018a) positive polar expressions rarely co-occur with abusive language, negative polar expressions, however, do; the polar expressions are obtained from the Subjectivity Lexicon (Wilson et al., 2005) 2nd person pronouns are typical of abusive usage: you are a bitch; 1st person pronouns are likely to indicate non-abusive usage: I am a bitch quotation signs indicate reported speech; a tweet may report an abusive remark, however, the reported remark itself may not be perceived as abusive (Chiril et al., 2020) a typical means of expressing high emotional intensity Table 8: Features for disambiguating a potentially abusive word (referred to as target word); context is defined as a window of 4 words neighbouring the target word. class abusive non abusive all bitch freq perc 248 24.8 752 75.2 10"
2021.eacl-main.28,S19-2010,0,0.0476836,"ectionukwac on lower ranks. Cross-Domain Evaluation. Next, we test the best lexicon of our previous experiments (i.e. projectionukwac +LP(200 abusive seed words)) in cross-domain micropost classification. Posts are categorized into abusive and non-abusive posts. Through a cross-domain classification, in which we train on one dataset and test on another, we show that the chosen configuration is not overfit to a particular dataset. Table 5 provides some information on the datasets we consider. In addition to the datasets used in Wiegand et al. (2018a), we include the recent SemEval-dataset from Zampieri et al. (2019). Table 6 shows the results of cross-domain micropost classification. As baselines we use a majorityclass classifier, the feature-based approach from Nobata et al. (2016), BERT and the lexicon from Wiegand et al. (2018a). In order to demonstrate the intrinsic predictiveness of the words learned by our emoji-based approach, we do not train a classifier on the source domain (unlike Wiegand et al. (2018a) who use the rank of the lexicon entries as a feature) but simply classify a micropost as abusive if an abusive word from our emoji-based lexicon is found. As abusive words, we consider all 1,250"
2021.konvens-1.13,N13-1132,0,0.0292588,"ers or information from external knowledge bases. While these labelling functions are expected to have low coverage and might also introduce a certain amount of noise, Snorkel addresses this problem by learning an unsupervised generative model over the output of the labelling functions, based on the (dis)agreements between the predicted labels. This approach is similar in spirit to previous work on 7 See https://spacy.io/api/ dependencymatcher. To generate the trees, we use the German de_core_news_sm model also provided by spaCy. quality estimation for annotations obtained from crowdsourcing (Hovy et al., 2013). The output of Snorkel is a set of probabilistic labels that can be used as input to any supervised ML classifier. Table 5 shows the number of patterns used for each class and the number of hits, i.e., instances extracted by each pattern from the unlabelled training data. Please note that the number of patterns is not very informative on its own, as patterns can make use of regular expressions, lemma lists and syntactic patterns over dependency trees, thus allowing us to extract a larger variety of diverse training examples than could be obtained based on simple string matches. As an example,"
2021.konvens-1.13,C18-2002,0,0.0263387,"Missing"
2021.naacl-main.48,P15-2122,0,0.0218708,"defined as the activity of saying [...] the opposite of what you mean (Macmillan, 2007). The way in which is spoken is intended to make someone else feel stupid or show them that you are angry. This explains the strong connection towards abusive language as in (48): (48) It’s always fun watching sports with a woman in the room. Although the automatic detection of sarcasm has been investigated (Tsur et al., 2010; Riloff et al., 2013), the classification performance is still fairly limited. • Rhetorical questions. Rhetorical questions are asked not to elicit information but to make a statement (Bhattasali et al., 2015). They have been examined on social-media texts (Ranganath et al., 2016; Oraby et al., 2017). Future work needs to address what makes a rhetorical question abusive: (49) Did Stevie Wonder choose these ""models""? • Other implicit abuse. Our final category comprises all further forms of implicit abuse that require world knowledge and inferencing: (50) (51) (52) (53) She still thinks she matters. I live in Ethiopia. Happy new year 1219! These girls know skinny sausages are no fun. Welcome to the Hotel Islamfornia. You may check out any time but you can never leave. datasets subtype Kumar SBFrames"
2021.naacl-main.48,2020.figlang-1.20,0,0.033929,"r. (39) Liberals are not very smart. (40) I’m not excited about your existence. If we translate these euphemisms into their unequivocal counterparts (41)-(43), the abusive nature of these statements becomes more obvious. By dehumanization, we commonly understand the act of perceiving or treating people as less than human (Haslam and Loughnan, 2016). While Haslam (41) I want to kill you. and Loughnan (2016) propose a fairly comprehen(42) Liberals are retarded. sive set of different properties that characterize de(43) I hate you. humanization, we focus on the most commonly With the exception of Felt and Riloff (2020), accepted property of likening members of the target group to non-human entities (Haslam, 2006), euphemisms have not been addressed in natural language processing so far. such as machines, animals or diseases. We observed two different realizations of dehuAs a research question, one would need to anmanization. On the one hand, the target is explicitly swer how abusive euphemisms can be detected and equated with non-human entities (33). translated to their unequivocal counterpart. 579 4.6 Call for Action Calls for action represent another type of implicitly abusive language. By that we underst"
2021.naacl-main.48,D15-1294,0,0.0213675,"sonable starting point since not every negative sentence focusing on some identity group conveys some (abusive) stereotype (e.g. (8)-(10)). A first research question could be how to detect stereotypical statements among negative statements. (8) Gay people fight for the right to be accepted. (9) Muslims groan under the recession. (10) Jews mourn the loss of a member of their community. We believe that specific linguistic properties may be indicative for automatic classification. For example, stereotypes are more likely to co-occur with habitual aspect (11) rather than non-habitual aspect (12) (Friedrich and Pinkal, 2015). (11) Jews always support terror instability. (12) Jews currently fear displaying their faith in public. One should also examine whether generic phrases regarding identity groups (13) correlate with stereotypes (Reiter and Frank, 2010). Previous work already established that the definite article, which represents a subset of such generic phrases, is predictive for abusive language (Burnap and Williams, 2015; Palmer et al., 2017). (13) The jew does not care about the humankind. Further, the same stereotype can be expressed in different ways. For example, (14)-(17) convey the sexist stereotype"
2021.naacl-main.48,2020.acl-main.373,0,0.0794078,"Missing"
2021.naacl-main.48,2020.trac-1.1,0,0.0679416,"Missing"
2021.naacl-main.48,P19-1334,0,0.0607398,"Missing"
2021.naacl-main.48,P10-1005,0,0.0484349,"(8) Gay people fight for the right to be accepted. (9) Muslims groan under the recession. (10) Jews mourn the loss of a member of their community. We believe that specific linguistic properties may be indicative for automatic classification. For example, stereotypes are more likely to co-occur with habitual aspect (11) rather than non-habitual aspect (12) (Friedrich and Pinkal, 2015). (11) Jews always support terror instability. (12) Jews currently fear displaying their faith in public. One should also examine whether generic phrases regarding identity groups (13) correlate with stereotypes (Reiter and Frank, 2010). Previous work already established that the definite article, which represents a subset of such generic phrases, is predictive for abusive language (Burnap and Williams, 2015; Palmer et al., 2017). (13) The jew does not care about the humankind. Further, the same stereotype can be expressed in different ways. For example, (14)-(17) convey the sexist stereotype that women belong in the kitchen. (14) Men should drive and women should cook. (15) This is how America should be. 5 women slaving over a hot stove. (16) Get back in the kitchen. (17) Women should all stay at home in an apron, chained t"
2021.naacl-main.48,D13-1066,0,0.0955127,"Missing"
2021.naacl-main.48,2020.acl-main.486,0,0.0116408,", that, due to the sampling process, coincidentally only occur in abusive microposts. Although additional datasets containing larger amounts of implicit abuse have been released since Wiegand et al. (2019) published their findings, we found that these new datasets also suffer from bi5 What should(n’t) the datasets for ases. We outline these biases on the most recent implicit abuse look like? dataset that displays a high degree of implicit abuse Driven by the requirements of data-hungry deep- and that is also fairly large (Table 1): the dataset learning methods, the most common strategy for by Sap et al. (2020) (SBFrames). Of the recent abusive language detection is to create a single datasets, it is also the only dataset to cover a signifidataset and train a classifier on it. That dataset cant amount of abusive instances targeting common 581 identity groups (e.g. Jews, Muslims). In order to get a larger amount of microposts, existing datasets (e.g. Founta et al. (2018)) were merged into SBFrames. In addition, further raw data was added, such as posts from the whitesupremacist platform stormfront.org or subReddits on abusive jokes from reddit.com. While these additional data undoubtedly yield more a"
2021.naacl-main.48,W17-5537,0,0.01894,"in which is spoken is intended to make someone else feel stupid or show them that you are angry. This explains the strong connection towards abusive language as in (48): (48) It’s always fun watching sports with a woman in the room. Although the automatic detection of sarcasm has been investigated (Tsur et al., 2010; Riloff et al., 2013), the classification performance is still fairly limited. • Rhetorical questions. Rhetorical questions are asked not to elicit information but to make a statement (Bhattasali et al., 2015). They have been examined on social-media texts (Ranganath et al., 2016; Oraby et al., 2017). Future work needs to address what makes a rhetorical question abusive: (49) Did Stevie Wonder choose these ""models""? • Other implicit abuse. Our final category comprises all further forms of implicit abuse that require world knowledge and inferencing: (50) (51) (52) (53) She still thinks she matters. I live in Ethiopia. Happy new year 1219! These girls know skinny sausages are no fun. Welcome to the Hotel Islamfornia. You may check out any time but you can never leave. datasets subtype Kumar SBFrames Waseem Warner OffensEval average other implicit abuse 9.8 28.4 12.8 30.4 2.4 16.8 perpetrato"
2021.naacl-main.48,W17-1101,1,0.841904,"Computational Linguistics 2 The Story So Far By far the most prominent classification approaches applied to abusive language detection are supervised learning methods. Whereas initially, traditional learning algorithms, such as SVMs or logistic regression, were among the most popular methods for this task (Warner and Hirschberg, 2012; Burnap et al., 2015; Nobata et al., 2016), at present, best results are obtained by deep-learning methods, particularly transformers (Struß et al., 2019; Kumar et al., 2020; Zampieri et al., 2020). A more detailed summary of the methods explored can be found in Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). Unfortunately, so far there has been little error analysis of system output for abusive language detection. As a consequence, the community is fairly unaware of what types of errors are made and why. The most notable exception is van Aken et al. (2018) who carry out experiments on the dataset of Google’s Toxic Comment Classification Challenge3 and the dataset by Davidson et al. (2017). As prominent errors that a supervised classifier makes, van Aken et al. (2018) list toxicity without swearwords, rhetorical questions and comparisons/metaphorical language. All the"
2021.naacl-main.48,W17-3014,0,0.0414765,"Missing"
2021.naacl-main.48,D15-1019,0,0.0234523,") is compared to some offensive entity, action or state (idiot in (25)). Abusive comparisons need not be explicitly abusive (25) but can also be implicitly abusive (26)-(27). (25) You talk like an idiot. (26) You look like someone only a mother could love. (27) You sing like a dying bird. A research question that would need to be answered is whether detecting abusive comparisons is not (almost) identical to the detection of comparisons conveying a negative sentiment. Such classification of comparisons into positive (28), neutral (29) and negative comparisons (30) has already been addressed by Qadir et al. (2015). (28) You look like a princess. (29) You look like your brother. (30) You look like a crackhead. Another research question would be to examine whether abusive comparisons are not identical to (negative) comparisons using figurative language (i.e. similes as (31)). Intuitively, comparisons employing literal language should be less abusive (32). (31) You look like the back end of a bus. (32) You look like you have slept badly. 4.4 Dehumanization (33) Black people are monkeys. On the other hand, a more difficult form of dehumanization involves metaphorical language in which the target is not exp"
2021.naacl-main.48,W18-5105,0,0.0423634,"Missing"
2021.naacl-main.48,W12-2103,0,0.0447923,"e. They are taken from actual web data and in no way reflect the opinion of the authors. 2 576 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 576–587 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 The Story So Far By far the most prominent classification approaches applied to abusive language detection are supervised learning methods. Whereas initially, traditional learning algorithms, such as SVMs or logistic regression, were among the most popular methods for this task (Warner and Hirschberg, 2012; Burnap et al., 2015; Nobata et al., 2016), at present, best results are obtained by deep-learning methods, particularly transformers (Struß et al., 2019; Kumar et al., 2020; Zampieri et al., 2020). A more detailed summary of the methods explored can be found in Schmidt and Wiegand (2017) and Fortuna and Nunes (2018). Unfortunately, so far there has been little error analysis of system output for abusive language detection. As a consequence, the community is fairly unaware of what types of errors are made and why. The most notable exception is van Aken et al. (2018) who carry out experiments"
2021.naacl-main.48,W17-3012,0,0.0126222,"actually look like and why are we not getting there? Michael Wiegand Josef Ruppenhofer Digital Age Research Center (D!ARC) Leibniz Institute for German Language Alpen-Adria-Universität Klagenfurt D-68161 Mannheim, Germany AT-9020 Klagenfurt, Austria ruppenhofer@ids-mannheim.de michael.wiegand@aau.at Elisabeth Eder Institut für Germanistik Alpen-Adria-Universität Klagenfurt AT-9020 Klagenfurt, Austria elisabeth.eder@aau.at Abstract Though there has been much work on abusive language detection in general, comparatively little work has been focusing on implicit forms of abusive language (4)-(5) (Waseem et al., 2017). By implicit abuse we understand abusive language that is not conveyed by (unambiguously) abusive words (e.g. dumbass, bimbo, scum). Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs t"
2021.naacl-main.48,N16-2013,0,0.0295589,"tasets. Arguing for a divide-and-conquer strategy, we present a list of subtypes of implicitly abusive language and formulate research tasks and questions for future research. 1 (4) I haven’t had an intelligent conversation with a woman in my whole life. (5) Why aren’t there any Mexicans on Star Trek? Because they don’t work in the future either. Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person or group of persons.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyberbullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above.2 Detailed analyses of the output of existing classifiers have also revealed that currently only explicit abuse can be reliably detected (van Aken et al., 2018; Wiegand et al., 2019). In this position paper, we want to shed more light on the nature of implicitly abusive language. We identify subtypes of implicit abuse that can be found in existing datasets and the literature. We also outline shortcomings that prevent implicitly abusive language fr"
2021.naacl-main.48,2021.eacl-main.274,0,0.0290095,"uces fairly poor classifica- instances by merging datasets does not solve the tion performance on the cross-domain detection of problem. It may introduce further detrimental biabusive language, with lexicon-based approaches ases. Overall, our analysis supports the claim that performing much stronger (Wiegand et al., 2018). the currently available datasets are not really suitFurther, statistical debiasing methods for abusive able for effectively learning implicit abuse. We language detection have also been reported to yield strongly argue for new datasets that focus on parvery limited success (Zhou et al., 2021). The au- ticular subtypes of implicit abuse. This will also thors of that research argue that spending more facilitate thinking about appropriate negative data. efforts in ensuring a high quality of the datasets Larger datasets are not necessarily the best datasets during their creation is more worthwhile than ap- to train a classifier on, especially if they are domiplying sophisticated machine learning. nated by frequently observed words. Finally, it may We anticipate that there are also some subtasks in also make sense to learn on smaller units, such as the realm of implicit abuse that may"
2021.naacl-main.48,N19-1060,1,0.925322,"e future either. Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person or group of persons.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyberbullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above.2 Detailed analyses of the output of existing classifiers have also revealed that currently only explicit abuse can be reliably detected (van Aken et al., 2018; Wiegand et al., 2019). In this position paper, we want to shed more light on the nature of implicitly abusive language. We identify subtypes of implicit abuse that can be found in existing datasets and the literature. We also outline shortcomings that prevent implicitly abusive language from really being learned on its own terms. With this study, we hope to guide future research on implicitly abusive language. Our contributions in this paper are: • We present a list of subtypes of implicit abuse. This is accompanied by quantitative information from publicly available datasets. • We derive research tasks and questi"
2021.naacl-main.48,N18-1095,1,0.853625,"(explicitly) abusive types and perpetrators) are concerned, unsuitable language detection, machine learning has not pro- sampling causes biases that prevent classifiers from duced the anticipated results. For example, super- really learning these phenomena. Simply adding vised learning still produces fairly poor classifica- instances by merging datasets does not solve the tion performance on the cross-domain detection of problem. It may introduce further detrimental biabusive language, with lexicon-based approaches ases. Overall, our analysis supports the claim that performing much stronger (Wiegand et al., 2018). the currently available datasets are not really suitFurther, statistical debiasing methods for abusive able for effectively learning implicit abuse. We language detection have also been reported to yield strongly argue for new datasets that focus on parvery limited success (Zhou et al., 2021). The au- ticular subtypes of implicit abuse. This will also thors of that research argue that spending more facilitate thinking about appropriate negative data. efforts in ensuring a high quality of the datasets Larger datasets are not necessarily the best datasets during their creation is more worthwhi"
2021.naacl-main.48,W19-3502,0,0.0282718,"e content of a micropost is hidden in the nontextual components or results as an interplay of text and image/video. One could also regard many of these abusive posts as instances of implicit abuse since many of them do not contain mentions of abusive words. Therefore, a comprehensive classifier to detect implicitly abusive microposts should also consist of a multimodal component that analyses image or video content and fuses this information with text analysis. Indeed the community is aware of this form of abuse and there have been several attempts for multimodal analysis (Singh et al., 2017; Yang et al., 2019; Gomez et al., 2020). In our work, however, we do not address the aspect of multimodal abuse simply because many datasets only include the textual component of a micropost and the reconstruction of non-textual components of posts can only 580 • Jokes. Jokes as (47) can be severely abusive. (47) What’s better than winning gold in the paralympics? Walking. The computational modeling of humor remains a challenging task (Mihalcea and Strapparava, 2006). We are not aware of any research on the detection of abusive humor. • Sarcasm. Sarcasm is largely defined as the activity of saying [...] the opp"
2021.naacl-main.48,N19-1144,0,0.0443502,"Missing"
C08-1101,H05-1043,0,0.0189711,"iloff et al., 2003; Pang and Lee, 2004) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. An application of the idea of alternative targets can be seen in Kim and Hovy’s (2007) work on election prediction. They assume that if a speaker expresses support for one party, all mentions of the competing parties have negative polarity, thus creating automatically labeled training data. In the field of product review mining, sentiments and features (aspects) have been mined (Popescu and Etzioni, 2005), where the aspects correspond to our definition of targets. However, the aspects themselves are not related to each other in any fashion. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall deci"
C08-1101,W03-0404,1,0.298262,"e baseline which has comparable accuracy, namely Distribution, we see that our system improves in f-measure by 24 percentage points. Our results are encouraging - even using simple features to capture target relations achieves considerable improvement over the baselines. However, there is much room for improvement. Using more detailed target and discourse information promises to further improve system performance. These are avenues for future work. 6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. An application of the idea of alternative targets can be seen in Kim and Hovy’s (2007) work on election prediction. They assume that if a speaker expresses support for one party, all mentions of the competing parties have negative polarity, thus creating automatically labeled training data. In the field of product review mining, sentiments and features (aspects) have been mined (Popescu and Etzion"
C08-1101,N07-1038,0,0.0460476,"tomatically labeled training data. In the field of product review mining, sentiments and features (aspects) have been mined (Popescu and Etzioni, 2005), where the aspects correspond to our definition of targets. However, the aspects themselves are not related to each other in any fashion. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect model to make a more informed overall decision for sentiment classification. In our scheme, their aspects would be related as same and their high contrast relations would correspond to the non-reinforcing frames SPSNsame, SNSPsame. Additionally, our frame relations would link the sentiments across nonadjacent clauses, and make connections via alt target relations. With regard to meetings, the most closely related work includes the dialog-related annotation 807 schemes for various available corpora of conversatio"
C08-1101,2007.sigdial-1.5,1,0.931162,"on and polarity information. Our experiments thus focus on the new question: “Given two opinion sentences, determine if they participate in any frame relation.” Here, an opinion sentence is a sentence containing one or more sentiment or arguing expression. In this work, we consider frame detection only between sentence pairs belonging to the same speaker. 5.1 Annotation of Gold Standard Creating gold-standard opinion-frame data is accomplished by annotating frame components and then building the frames from those underlying annotations. We began with annotations created by Somasundaran et al. (2007), namely four meetings of the AMI meeting corpus annotated for sentiment and arguing opinions (text anchor and type). Following that annotation scheme, we annotated an additional meeting. This gave us a corpus of 4436 sentences or 2942 segments (utterances). We added attributes to the existing opinion annotations, namely polarity and target-id. The targetid attribute links the opinion to its local target span. Relations between targets were then annotated. When a newly annotated target is similar (or opposed) to a set of targets already participating in same relations, then the same (or alt) l"
C08-1101,W08-0122,1,0.75346,"n annotated. When a newly annotated target is similar (or opposed) to a set of targets already participating in same relations, then the same (or alt) link is made only to one of them - the one that seems most natural. This is often the one that is physically closest. Table 3: Features for Opinion Frame detection Link transitivity is then used to connect targets that are not explicitly linked by the annotators. All annotations were performed by two of the co-authors of this paper by consensus labeling. The details of our annotation scheme and interannotator agreement studies are presented in (Somasundaran et al., 2008). Once the individual frame components are annotated, conceptually, a frame exists for a pair of opinions if their polarities are either positive or negative and their targets are in a same or alt relation. For our experiments, if a path exists between two targets, then their opinions are considered to be participating in an opinion-frame relation. The experimental data consists of pairs of opinion sentences and the gold-standard information whether there exists a frame between them. We approximate continuous discourse by only pairing sentences that are not more than 10 sentences apart. We als"
C08-1101,W06-1639,0,0.0206321,"entage points. Our results are encouraging - even using simple features to capture target relations achieves considerable improvement over the baselines. However, there is much room for improvement. Using more detailed target and discourse information promises to further improve system performance. These are avenues for future work. 6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. An application of the idea of alternative targets can be seen in Kim and Hovy’s (2007) work on election prediction. They assume that if a speaker expresses support for one party, all mentions of the competing parties have negative polarity, thus creating automatically labeled training data. In the field of product review mining, sentiments and features (aspects) have been mined (Popescu and Etzioni, 2005), where the aspects correspond to our definition of targets. However, the aspects themselves are not related to"
C08-1101,J00-4003,0,0.0129342,"ish relations between targets, in the process relating their respective opinions. We address two types of relations, same and alternative. The same relation holds between targets that refer to the same entity, property, or proposition. Observing the relations marked by annotators, we found that same covers not only identity, but also part-whole, synonymy, generalization, specialization, entity-attribute, instantiation, cause-effect, epithets and implicit background topic, i.e., relations that have been studied by many researchers in the context of anaphora and co-reference (e.g. (Clark, 1975; Vieira and Poesio, 2000; Mueller and Strube, 2001)). Actually, same relations holding between entities often involve co-reference (where co-reference is broadly conceived to include relations such as part-whole listed above). However, there are no morphosyntactic constraints on what targets may be. Thus, same relations may also hold between adjective phrases, verb phrases, and clauses. An instance of this is Example 1, where the same target relation holds between the adjectives edgy and computery. 1 Polarity can also be neutral or both (Wilson and Wiebe, 2005), but these values are not significant for our opinion fr"
C08-1101,W05-0308,1,0.911842,"lt, ANSNalt ment (turn/utterance) information for each speaker. Each utterance consists of one or more sentences. We also use some of the accompanying manual annotations (like adjacency pairs) as features in our machine learning experiments. 3 Opinion Frames Table 1: Opinion Frames In this section, we lay out definitions relating to opinion frames, illustrate with examples how these are manifested in our data, and consider them in the context of discourse relations. 3.1 Definitions The components of opinion frames are individual opinions and the relationships between their targets. Following (Wilson and Wiebe, 2005; Somasundaran et al., 2007), we address two types of opinions, sentiment and arguing. Sentiment includes positive and negative evaluations, emotions, and judgments. Arguing includes arguing for or against something, and arguing that something should or should not be done. Opinions have a polarity that can be positive or negative. 1 The target of an opinion is the entity or proposition that the opinion is about. We establish relations between targets, in the process relating their respective opinions. We address two types of relations, same and alternative. The same relation holds between targ"
C08-1101,T75-2000,0,0.843069,"ut. We establish relations between targets, in the process relating their respective opinions. We address two types of relations, same and alternative. The same relation holds between targets that refer to the same entity, property, or proposition. Observing the relations marked by annotators, we found that same covers not only identity, but also part-whole, synonymy, generalization, specialization, entity-attribute, instantiation, cause-effect, epithets and implicit background topic, i.e., relations that have been studied by many researchers in the context of anaphora and co-reference (e.g. (Clark, 1975; Vieira and Poesio, 2000; Mueller and Strube, 2001)). Actually, same relations holding between entities often involve co-reference (where co-reference is broadly conceived to include relations such as part-whole listed above). However, there are no morphosyntactic constraints on what targets may be. Thus, same relations may also hold between adjective phrases, verb phrases, and clauses. An instance of this is Example 1, where the same target relation holds between the adjectives edgy and computery. 1 Polarity can also be neutral or both (Wilson and Wiebe, 2005), but these values are not signi"
C08-1101,D07-1113,0,0.0978778,"Missing"
C08-1101,W01-1612,0,0.018633,"gets, in the process relating their respective opinions. We address two types of relations, same and alternative. The same relation holds between targets that refer to the same entity, property, or proposition. Observing the relations marked by annotators, we found that same covers not only identity, but also part-whole, synonymy, generalization, specialization, entity-attribute, instantiation, cause-effect, epithets and implicit background topic, i.e., relations that have been studied by many researchers in the context of anaphora and co-reference (e.g. (Clark, 1975; Vieira and Poesio, 2000; Mueller and Strube, 2001)). Actually, same relations holding between entities often involve co-reference (where co-reference is broadly conceived to include relations such as part-whole listed above). However, there are no morphosyntactic constraints on what targets may be. Thus, same relations may also hold between adjective phrases, verb phrases, and clauses. An instance of this is Example 1, where the same target relation holds between the adjectives edgy and computery. 1 Polarity can also be neutral or both (Wilson and Wiebe, 2005), but these values are not significant for our opinion frames. The alternative relat"
C08-1101,P04-1035,0,0.0375306,"comparable accuracy, namely Distribution, we see that our system improves in f-measure by 24 percentage points. Our results are encouraging - even using simple features to capture target relations achieves considerable improvement over the baselines. However, there is much room for improvement. Using more detailed target and discourse information promises to further improve system performance. These are avenues for future work. 6 Related work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. An application of the idea of alternative targets can be seen in Kim and Hovy’s (2007) work on election prediction. They assume that if a speaker expresses support for one party, all mentions of the competing parties have negative polarity, thus creating automatically labeled training data. In the field of product review mining, sentiments and features (aspects) have been mined (Popescu and Etzioni, 2005), where the a"
C08-1101,H05-2017,0,\N,Missing
C10-1107,D09-1031,1,0.849439,"cisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where InterAnnotator Agreement (IAA) is low and thus the consistency of th"
C10-1107,burchardt-etal-2006-salsa,0,0.0832951,"Missing"
C10-1107,P07-1007,0,0.0721121,"instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be ap"
C10-1107,N06-1016,0,0.323805,"onment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign sem"
C10-1107,N04-1012,0,0.202379,"ier is re-trained on the new data set. The newly trained classifier now picks the next instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the"
C10-1107,rehbein-ruppenhofer-2010-theres,1,0.836151,"f sentences selected by the classifier. For our human annotators, however, due to different annotation decisions the resulting set of sentences is expected to differ. Sampling method Uncertainty sampling is a standard sampling method for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. During early stages of the learning process when the classifier is trained on a very small seed data set, it is not beneficial to add the instances with the lowest classifier confidence. Instead, we use a dynamic version of uncertainty sampling (Rehbein and Ruppenhofer, 2010), based on the confidence of a maximum entropy classifier2 , taking into account how much the classifier has learned so far. In each iteration one new instance is selected from the pool and presented to the oracle. After annotation the classifier is retrained on the new data set. The modified uncertainty sampling results in a more robust classifier performance during early stages of the learning process. 952 2 http://maxent.sourceforge.net A1 A2 A3 A4 A5 A6 ø sl Anlass R U 8.6 9.6 4.4 5.7 9.9 9.2 5.8 4.9 3.0 3.5 5.4 6.3 6.2 6.5 25.8 27.8 Motiv R U 5.9 6.6 4.8 5.9 6.8 6.7 3.6 3.6 3.0 2.6 5.3 4."
C10-1107,P08-1098,0,0.0208374,"ions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign semantic frames to predicates, following Erk (200"
C10-1107,W07-1516,0,0.17367,"signs the correct label. The newly-annotated instances are added to the seed data and the classifier is re-trained on the new data set. The newly trained classifier now picks the next instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL see"
C10-1107,D07-1051,0,0.0387629,"grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign semantic frames to predic"
C10-1107,D07-1082,0,0.198912,"its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grai"
C10-1107,C08-1143,0,0.124091,"ge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation sche"
C10-1107,J04-3001,0,0.391882,"istic environment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for lea"
C10-1107,C08-1059,0,0.314897,"Missing"
C10-1107,P08-1000,0,\N,Missing
C18-1010,W15-2210,0,0.06211,"Missing"
C18-1010,W11-2929,0,0.456228,"Missing"
C18-1010,N13-1132,0,0.21226,"automatically predicted parse trees. We evaluate our approach in two different scenarios, i) in an active learning (AL) setup where we try to detect and manually correct errors in existing treebanks, and ii) in a domain adaptation setting where we automatically improve parsing performance without manual correction. We make an implementation of our approach publically available.1 2 Model Our error detection model is an adaptation and substantial extension of M ACE -AL (Rehbein and Ruppenhofer, 2017) which combines a generative, unsupervised method for estimating annotator reliability (M ACE) (Hovy et al., 2013) with active learning for the task of error detection in automatically annotated data. M ACE -AL can be applied to any classification task and has been tested in two applications, POS tagging and Named Entity Recognition (NER) (Rehbein and Ruppenhofer, 2017). The model is, however, not applicable to tasks with structured output such as trees or graphs. In contrast to POS tagging or NER where we try to detect annotation errors in the predicted labels for individual tokens, when looking for errors in parse trees we have to deal with directed, labelled relations between nodes, and changing the re"
C18-1010,Q16-1023,0,0.0944823,"iational inference model (M ACE -AL-T REE) or on the output from the Chu-Liu-Edmonds algorithm (M ACE -AL-T REE -C LE). A key advantage of our model is that, while making use of the feedback from active learning, we do not have to retrain the parsers after each iteration, which would be infeasible due to the time requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive bias"
C18-1010,D12-1096,0,0.0276049,"This not only includes PPs but also adverbal and adjectival modification. The next frequent error type are incorrectly attached punctuations. This, however, is not an error type we are concerned with and it might make sense to exclude punctuation from the error correction.13 More interesting attachment errors include coordination (CJ, CD), parenthesis (PAR), apposition (APP) and again the core arguments (OA, AG, DA, SB, PD). This shows that our model is not biased toward a small set of specific error types but is able to detect frequent parsing errors that are well-known from the literature (Kummerfeld et al., 2012). 3.5 Domain adaptation with self-training In the final set of experiments we want to test whether we can use our method to automatically correct parser output for self-training, to improve parsing accuracy for out-of-domain data. As before, we use the English web treebank, split into different genres as described above. Again, we pretend that no annotated training data for the different genres are available. This time, however, we make use of the supplementary raw text data for the five web genres (Table 1) that were provided for the SANCL shared task (Petrov and McDonald, 2012). The data is"
C18-1010,P14-1130,0,0.130379,"output from the Chu-Liu-Edmonds algorithm (M ACE -AL-T REE -C LE). A key advantage of our model is that, while making use of the feedback from active learning, we do not have to retrain the parsers after each iteration, which would be infeasible due to the time requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can com"
C18-1010,D07-1013,0,0.0621733,"as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can complement each other (McDonald and Nivre, 2007). All parsers use default settings as reported in the literature. For the BISTGraph parser, we selected the model with the highest LAS on the development set after 10 training iterations.5 We run the Malt parser with the settings arc-eager, liblinear without any further optimisation. For all experiments, we remove the language-specific extended labels and only train the parsers on the universal dependency relations. 3 Other update strategies such as always updating the predictions of the annotator with the lowest performance, or updating the predictions of all annotators did not yield the same"
C18-1010,nilsson-nivre-2008-malteval,0,0.0676691,"Missing"
C18-1010,nivre-etal-2006-maltparser,0,0.0845134,"rning, we do not have to retrain the parsers after each iteration, which would be infeasible due to the time requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can complement each other (McDonald and Nivre, 2007). All parsers use default settings as reported in the literature. For the BISTGraph parser, we selected the mod"
C18-1010,D08-1093,0,0.247649,"Missing"
C18-1010,P17-1107,1,0.891838,"nd when dealing with low-resource languages. We present such a method, aimed at the detection of parser errors in manually and automatically predicted parse trees. We evaluate our approach in two different scenarios, i) in an active learning (AL) setup where we try to detect and manually correct errors in existing treebanks, and ii) in a domain adaptation setting where we automatically improve parsing performance without manual correction. We make an implementation of our approach publically available.1 2 Model Our error detection model is an adaptation and substantial extension of M ACE -AL (Rehbein and Ruppenhofer, 2017) which combines a generative, unsupervised method for estimating annotator reliability (M ACE) (Hovy et al., 2013) with active learning for the task of error detection in automatically annotated data. M ACE -AL can be applied to any classification task and has been tested in two applications, POS tagging and Named Entity Recognition (NER) (Rehbein and Ruppenhofer, 2017). The model is, however, not applicable to tasks with structured output such as trees or graphs. In contrast to POS tagging or NER where we try to detect annotation errors in the predicted labels for individual tokens, when look"
C18-1010,P07-1078,0,0.0345818,"etting, however, clearly outperforms our baselines and shows a significant increase in parsing accuracy. Nonetheless, when comparing the results for M ACE -AL-T REE with self-training to the results without self-training we see that our self-training approach fails to beat the results obtained on the original data on three web genres. We only observe improved results on the emails and on the reviews. These two genres are also the ones that have the largest amount of unknown words which might explain why self-training is more promising for emails and reviews than for the other three genres, as Reichart and Rappoport (2007) have shown that the number of unknown words can be a good indicator for the potential benefit from self-training. 4 Related work Most studies on error detection in treebanks have focussed on finding errors in manual annotations (Dickinson and Meurers, 2003; Ule and Simov, 2004; van Noord, 2004; Dickinson and Meurers, 2005; Agrawal et al., 2013). Dickinson and Meurers (2003; 2005) proposed the use of variation n-grams to detect inconsistencies in manually annotated constituency treebanks and Boyd et al. (2008) extend this line of work to dependency trees. This approach, however, only works for"
C18-1010,W14-6111,0,0.045865,"Missing"
C18-1010,K17-3001,0,0.241419,"ference on Computational Linguistics, pages 107–118 Santa Fe, New Mexico, USA, August 20-26, 2018. obl nsubj dobj nmod case She eats spaghetti with chopsticks nsubj dobj obl case She eats spaghetti with chopsticks nsubj dobj case She eats spaghetti with chopsticks Figure 1: Correct tree (left), tree with label error (center) and tree with attachment error (right). A high-precision method for error detection in automatically predicted trees, however, would be of tremendous use for many treebanking projects. Below, we first describe the approach of Hovy et al. (2013) and Rehbein and Ruppenhofer (2017) for unstructured data (§2.1) and then our extension of the model for the purpose of error detection in tree structures (§2.2). 2.1 MACE-AL: Error detection in unstructured data M ACE -AL combines variational inference (VI) with active learning (AL) to model the reliability of automatically predicted annotations. As input, it takes the output of a committee of classifiers (e.g. the output of N POS taggers) and uses Bayesian inference to learn which taggers’ predictions are more trustworthy than others. The method is unsupervised, meaning that the distribution of the true labels Y = {y1 , y2 ,"
C18-1010,N10-1091,0,0.0374489,"he settings arc-eager, liblinear without any further optimisation. For all experiments, we remove the language-specific extended labels and only train the parsers on the universal dependency relations. 3 Other update strategies such as always updating the predictions of the annotator with the lowest performance, or updating the predictions of all annotators did not yield the same increase in results. 4 It has been shown that the most important success criterion for ensemble parsing is the diversity of the base parsers rather than model complexity or a high accuracy for the individual parsers (Surdeanu and Manning, 2010). 5 We started with 30 iterations but noticed no further increase in LAS after the first 10 iterations. 110 genre answers email newsgroups rewiews weblogs # trees 3,488 4,900 2,391 3,813 2,030 dev 1,000 1,000 1,000 1,000 1,000 test 2,488 3,900 1,391 2,813 1,030 train 13,134 11,722 14,231 12,809 14,592 # unlabelled sent. 27,274 1,194,173 1,000,000 1,965,350 524,834 Table 1: Distribution in the UD-En web treebank (training set does not include data from the respective genre but contains all the remaining treebank sentences excluding the ones for this particular genre). 3.1 Error detection with a"
C18-1010,ule-simov-2004-unexpected,0,0.0924351,"Missing"
C18-1010,P04-1057,0,0.118926,"Missing"
C18-1010,P11-2060,0,0.0628969,"Missing"
C18-1010,E17-1063,0,0.103258,"requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can complement each other (McDonald and Nivre, 2007). All parsers use default settings as reported in the literature. For the BISTGraph parser, we selected the model with the highest LAS on the development set after 10 training iterations.5 We run the Malt parser with the"
C18-1213,E09-1005,0,0.0163956,"r. Heuristic using ‘jeglich’ (ANY): Negative polarity items (NPIs) are known to occur in the context of negation (Giannakidou, 2008). Schulder et al. (2017) showed that the English NPI any co-occurs with shifters, so its presence in a verb phrase can indicate the presence of a verbal shifter. We expect the same for the German NPI jeglich, as seen in (8). We collect all verbs with a polar direct object that is modified by the lemma jeglich. The resulting pattern matches are sorted by their frequency, normalized over their respective verb frequency and then reranked using Personalised PageRank (Agirre and Soroa, 2009). (8) Sie [verwehrtenshifter uns jegliche [Hilfedobj ]+ ]− . They [deniedshifter us any [helpdobj ]+ ]− . Anti-Shifter Feature (ANTI): This feature specifically targets anti-shifters, verbs that exhibit polar stability instead of causing polar shifting. These are commonly verbs indicating creation or continued existence, such as live, introduce, construct or prepare. Such verbs often co-occur with the adverbs ausschließlich, zuerst, neu and extra, as seen in (9)–(12). Accordingly, we can create a list of anti-shifters by selecting the verbs that most often co-occur with these adverbs. (9) Im W"
C18-1213,P17-1042,0,0.0528078,"Missing"
C18-1213,P98-1013,0,0.735741,"Missing"
C18-1213,burchardt-etal-2006-salsa,0,0.0445694,"ses, called paraphrases in GermaNet, GermaNet offers two variations: the paraphrases originally written for GermaNet, and a more extensive set of paraphrases harvested from Wiktionary (Henrich et al., 2014). To improve coverage we use this paraphrase extension in our experiments. Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features"
C18-1213,W14-2618,0,0.365453,"iginally written for GermaNet, and a more extensive set of paraphrases harvested from Wiktionary (Henrich et al., 2014). To improve coverage we use this paraphrase extension in our experiments. Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features that have not yet been used for the creation of a verbal shifter lexicon. 4.2.1 B"
C18-1213,N09-1016,0,0.028757,"fter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018). Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of Schulder et al. (2017), all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domain"
C18-1213,P13-2022,0,0.213574,"the paraphrases originally written for GermaNet, and a more extensive set of paraphrases harvested from Wiktionary (Henrich et al., 2014). To improve coverage we use this paraphrase extension in our experiments. Salsa FrameNet: Framenets provide semantic frames that group words with similar semantic behavior. Schulder et al. (2017) use the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features that have not yet been used for the creation of a verbal shif"
C18-1213,P16-1047,0,0.0270048,"opose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural"
C18-1213,D16-1057,0,0.0269314,"The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009), using the same hyperparameters as Schulder et al. (2017) and German translations of their negation seeds. Polarity Clash (CLASH"
C18-1213,W97-0802,0,0.346189,"ers (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013), fail to detect many instances of polarity shifting. Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. 3 Data We create a gold standard for German verbal shifters, following the approach Schulder et al. (2017) used for their English gold standard. An expert annotator, who is a native speaker of German, labeled 2000 verbs, randomly sampled from GermaNet (Hamp and Feldweg, 1997), a German wordnet resource. The remaining 7262 GermaNet verbs are used to bootstrap a larger lexicon in §5.3. Each verb is assigned a binary label of being a shifter or not. To qualify as a shifter, a verb must permit polar expressions as its dependents and cause the polarity of the expression that embeds both verb and polar expression to move towards the opposite of the polar expression. For example, in (6) verhindern shifts the negative polarity of its dependent ein Gemetzel, resulting in a positive expression. Annotation is performed at the lemma level, as word-sense disambiguation tends t"
C18-1213,I08-1039,0,0.0360223,"rge scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters"
C18-1213,C12-2056,0,0.0175194,"n and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity c"
C18-1213,W09-4635,0,0.0250517,"s (§4.1.1) and those that require complex semantic resources (§4.1.2). When working with languages with scarcer resources, it can be expected that the former will be more readily available than the latter. 4.1.1 Data-driven Features The main requirement of the following features is a reasonably sized text corpus to detect syntactic patterns and word frequencies. The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are a"
C18-1213,W09-1105,0,0.0233824,"ir bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioSc"
C18-1213,morante-2010-descriptive,0,0.0269621,"Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013), fail to detect many instances of polarity shifting. Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. 3 Data We create a gold standard for German verbal shifters, following the approach Schulder et al. (2017) used for their English gold standard. An expert annotator, who is a native speaker of German, labeled 2000 verbs, randomly sampled from GermaNet (Hamp and Feldweg, 1"
C18-1213,P10-1114,0,0.053544,"Missing"
C18-1213,D13-1066,0,0.0859941,"Missing"
C18-1213,W15-2910,1,0.856892,"the frame memberships of verbs as a feature, hypothesizing that verbal shifters will be found in the same frames. We reproduce this feature using frames from the German FrameNet project Salsa (Burchardt et al., 2006). 2520 EffektGermaNet: Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014) introduced the idea that events can have harmful or beneficial effects on their objects. These effects are related but not identical to polarity shifting. Choi et al. (2014) provide lexical information on effects in their English resource EffectWordNet. We use its German counterpart, EffektGermaNet (Ruppenhofer and Brandes, 2015), to model the effect feature in our data. 4.2 New Features In §4.1 we described how we reproduce features already used for English shifter classification. Next we introduce new features that have not yet been used for the creation of a verbal shifter lexicon. 4.2.1 Bilingual Dictionary The motivation behind the work of Schulder et al. (2017) was to introduce a large lexicon of verbal polarity shifters. Now that such a lexicon exists for English, it is an obvious resource to use when creating verbal shifter lexicons for other languages. We hypothesize that a verb with the same meaning as an En"
C18-1213,S16-1084,0,0.0141556,"To verify that expectation, we apply their approach to German, for which all resources required to reproduce their experiments are available. Keeping in mind that this is not the case for many other languages, we focus our evaluation on differentiating between features that rely on unstructured data and those requiring rare semantic resources. While polarity shifters are not restricted to a particular part of speech – shifter nouns (e.g. downfall), adjectives (devoid) and adverbs (barely) also exist – we limit ourselves to verbs. Verbs and nouns are the most important minimal semantic units (Schneider et al., 2016) and verbs are usually the main syntactic 2517 predicates of clauses, projecting far-reaching scopes. Focusing on verbs also allows us a closer comparison with Schulder et al. (2017) and to investigate cross-lingual similarities between verbal shifters. The contributions of this paper are: (i) we introduce a German lexicon of verbal polarity shifters; (ii) we reproduce and adapt the approach of Schulder et al. (2017) to German to extend our lexicon; (iii) we introduce additional methods that take advantage of the existence of the English verbal polarity shifter lexicon and improve upon the cur"
C18-1213,I17-1063,1,0.267253,"hifter ]− . She was [deniedshifter the [scholarship]+ ]− . (4) Die neue Behandlung hat ihre [[Schmerzen]− gelindertshifter ]+ . The new treatment has [alleviatedshifter her [pain]− ]+ . As can be seen for verhindern/prevent in (5) and (6), the same shifter can even affect both positive and negative expressions. (5) Seine Prinzipien [verhindertenshifter eine [Einigung]+ ]− . His principles [preventedshifter an [agreement]+ ]− . (6) Ihre Maßnahmen [verhindertenshifter ein [Gemetzel]− ]+ . Their measures [preventedshifter a [slaughter]− ]+ . We present a reproduction and extension to the work of Schulder et al. (2017), which introduced a lexicon of verbal polarity shifters, as well as methods to increase the size of this lexicon through bootstrapping. The lexicon lists verb lemmas and assigns a binary label (shifter or no shifter) to each. The original approach was developed on English. We apply it to German, validating the generality of the approach and creating a new resource, a German lexicon of 677 verbal polarity shifters. We also improve the bootstrapping process by adding features that leverage polarity shifter resources across languages. As is the case with negation, modeling polarity shifting is i"
C18-1213,L18-1222,1,0.733403,"ng German lexicon of 677 verbal polarity shifters is made publicly available.1 2 Related Work Existing work on negation modeling focuses almost exclusively on negation words (see the survey of Wiegand et al. (2010)). One reason for this is the lack of lexicons and corpora that cover other forms of polarity shifters. Even the most complex negation lexicon for English sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018). Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of Schulder et al. (2017), all further use of and comparison to an English shifter lexicon refers to their bootstrapped lexicon as well. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to"
C18-1213,D13-1170,0,0.0124462,"ues are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on"
C18-1213,W08-0606,0,0.0287512,"se of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for example, there are only 6 verbal shifters (Morante, 2010). Polarity classifiers trained on such corpora, such as the state-of-the-art Recursive Neural Tensor Network tagger (Socher et al., 2013), fail to detect many instances of polarity shifting. Schulder et al. (2017) show that the explicit knowledge provided by a shifter lexicon can improve polarity classification in such cases. 3 Data We create a gold standard for German verbal shifters, following the approach Schulder et a"
C18-1213,P02-1053,0,0.027394,"ntactic patterns and word frequencies. The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009), using the same hyperparameters as Schulder et al. (2017) and German translatio"
C18-1213,P16-1157,0,0.0271813,"Missing"
C18-1213,N10-1119,0,0.0378265,"ns and word frequencies. The text corpus was lemmatized using the TreeTagger (Schmid, 1994) and parsed for syntactic dependency structures with ParZu (Sennrich et al., 2009).2 For features requiring knowledge of polarities we use the PolArt Sentiment Lexicon (Klenner et al., 2009).3 2 Lacking an appropriate parser, a part-of-speech tagger may approximate required syntactic structures (Riloff et al., 2013). We chose to consider features that use a polarity lexicon to still be data-driven features as there exist robust methods to generate them automatically from unlabeled corpora (Turney, 2002; Velikovich et al., 2010; Hamilton et al., 2016). The lexicon we use was created using bootstrapping (Clematide and Klenner, 2010). 3 2519 Distributional Similarity (SIM): The distributional similarity feature assumes that words that are semantically similar to negation words are also likely to be polarity shifters. Semantic similarity is modeled as cosine similarity in a word embedding space. The word embeddings are created using Word2Vec (Mikolov et al., 2013) on the German web corpus DeWaC (Baroni et al., 2009), using the same hyperparameters as Schulder et al. (2017) and German translations of their negation seed"
C18-1213,W10-3111,1,0.958229,"(shifter or no shifter) to each. The original approach was developed on English. We apply it to German, validating the generality of the approach and creating a new resource, a German lexicon of 677 verbal polarity shifters. We also improve the bootstrapping process by adding features that leverage polarity shifter resources across languages. As is the case with negation, modeling polarity shifting is important for various tasks in NLP, such as relation extraction (Sanchez-Graillet and Poesio, 2007), recognition of textual entailment (Harabagiu et al., 2006) and especially sentiment analysis (Wiegand et al., 2010). However, while there has been significant research on negation in sentiment analysis (Wiegand et al., 2010), current classifiers fail to handle polarity shifters adequately (Schulder et al., 2017). This is in part due to the lack of lexical resources for polarity shifters. Unlike negation words (no, not, never, etc.), of which there are only a few dozen in a language, polarity shifters are far more numerous. Among verbs alone there are many hundreds (Schulder et al., 2017). Comprehensive shifter lexicons are, therefore, considerably more expensive to create. Once available, they can be used"
C18-1213,H05-2018,0,0.0976251,"f the existence of the English verbal polarity shifter lexicon and improve upon the current state of the art. The focus of our work is the binary classification of verbal polarity shifters in German. The resulting German lexicon of 677 verbal polarity shifters is made publicly available.1 2 Related Work Existing work on negation modeling focuses almost exclusively on negation words (see the survey of Wiegand et al. (2010)). One reason for this is the lack of lexicons and corpora that cover other forms of polarity shifters. Even the most complex negation lexicon for English sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. So far the only larger resources for polarity shifters are the English-language verbal shifter lexicons recently introduced by Schulder et al. (2017) and Schulder et al. (2018). Schulder et al. (2017) automatically bootstrap a lexicon which covers 980 verbal shifters at the lemma level, while Schulder et al. (2018) manually annotate word senses of verbs, creating a lexicon of 2131 shifter senses across 1220 verbs. As we reproduce and extend the work of Schulder et al. (2017), all further use of and comparison to an English shifter lexicon refers to their bo"
C18-1213,D13-1099,0,0.0196305,"ll. To create shifter lexicons at a large scale, automation and bootstrapping techniques are required. Danescu-Niculescu-Mizil et al. (2009) propose using negative polarity items (NPIs) to extract downwardentailing operators, which are closely related to polarity shifters. Schulder et al. (2017) also make use of NPIs in addition to a number of other features. Rather than using lexicons, another approach would be to learn polarity shifters from labelled corpora. In the case of negation, this has already been examined for the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013), the review domain (Ikeda et al., 2008; Kessler and Schütze, 2012; Socher et al., 2013; Yu et al., 2016) and across domains (Fancellu et al., 2016). Unfortunately, due to the considerably higher lexical diversity of polarity shifters, far larger corpora would be required for learning shifter than for learning negation. Available corpora that are suitable for negation learning, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small in size. Most verbs occur in them in very few instances or not at all. In the BioScope corpus, for exa"
C18-1325,J90-1003,0,0.532214,"et al., 2016) in its default setting to train word embeddings on the SdeWaC-corpus (Faaß and Eckart, 2013), which contains about 880 million tokens and is a cleaned up version of the deWaC corpus (Baroni et al., 2009). The choice of fastText was motivated by the fact that fastText computes vectors for words by adding up the vectors for n-grams found in the words, which allows us to produce vectors for words not seen in the training data. Since many of our complex forms are (near-)hapaxes, this is a crucial benefit of fastText. Pointwise mutual information. We use Pointwise mutual information (Church and Hanks, 1990) to capture the level of association between the two components of the complex word. The expectation is that the components of regular compounds exhibit higher PMI-scores than the components of a complex word involving an affixoid. This is motivated by Tellenbach (1985)’s observation that for complex forms containing an affixoid use, paraphrases containing the morpheme in question as a free word are unlikely. By contrast, compositional compounds are often paraphrased using their components. As an example, the compositional compound Perserk¨onig is readily paraphrased as K¨onig der Perser ‘king"
C18-1325,W14-5805,0,0.0133934,"icular set of suffixoid candidates differences between the second component and the complex word’s supersenses may be particularly important, we experiment with an alternative set of supersense features (supersenses diffs): we use a series of indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persu"
C18-1325,faass-etal-2010-design,0,0.066989,"Missing"
C18-1325,N09-1002,0,0.0355169,"Missing"
C18-1325,W97-0802,0,0.154792,"Missing"
C18-1325,W14-2302,0,0.0139202,"entiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 3858 our senses (integer, p"
C18-1325,W09-4635,0,0.0733123,"res (supersenses diffs): we use a series of indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words"
C18-1325,L16-1413,0,0.0555122,"Missing"
C18-1325,P08-2028,0,0.0918526,"Missing"
C18-1325,remus-etal-2010-sentiws,0,0.0213281,"hether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to"
C18-1325,ruppenhofer-etal-2017-evaluating,1,0.803388,"ining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 3858 our senses (integer, politics, . . . ) whereas concrete words refer to things we can perceive (sound, scent, . . . ). The second dimension concerns imageability (img). A large subset of concrete wo"
C18-1325,schmid-etal-2004-smor,0,0.0612312,"Missing"
C18-1325,S13-1038,0,0.0555683,"Missing"
C18-1325,N09-1001,1,0.79684,"Missing"
C18-1325,S14-2033,0,0.0245143,"unishment). The final dimension, arousal, represents the intensity of emotion caused by a stimulus (alert vs. calm).8 We obtain affective ratings from the resource of K¨oper and Schulte im Walde (2016). It provides information on 350k words and is far more comprehensive than the affective norm data of Kanske and Kotz (2010) or Lahl et al. (2009). It is also much larger than commonly used polarity lexicons for German such as PolArt (Klenner et al., 2009) or GermanPolarityClues (Waltinger, 2010). Emotion. Since emotion information is commonly used in sentiment-related classification tasks (e.g. Tang et al. (2014), Sulis et al. (2016)), we wanted to see to what extent emotion information could benefit our task. For this purpose, we use the NRC Word-Emotion Association Lexicon (EmoLex) for English which was created by Mohammad and Turner (2013) using a crowdsourcing approach. EmoLex contains binary associations of words with the eight basic emotions (joy, sadness, anger, fear, disgust, surprise, trust, anticipation) of Plutchik (1962) . Although the German version of the lexicon was produced using machine translation, we use it here because we do not have a similarly large natively produced resource ava"
C18-1325,D11-1063,0,0.0294224,"for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete (abstconc). Abstract words refer to things that we cannot perceive directly with 7 Note that for other suffixoids not covered here such as Papst (lit. ‘pope’, suffixoid ‘expert’) and Nest (lit. ‘nest’, affixoid ‘den/hideout’) there is no difference at all between the supersenses of the second component and the complex word. 385"
C18-1325,waltinger-2010-germanpolarityclues,0,0.213529,"indicator variables that code whether the second component and the complex word differ in their value for a given supersense.7 Polarity. Since affixoid uses are likely to have evaluative meanings, we explore whether this is reflected in the polarity of the two components and the complex form. We extract polarity information for all three from SentiMerge (Emerson and Declerck, 2014). With 96,918 entries, it is to date the largest available polarity lexicon for German. SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a Bayesian probabilistic model. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks, such as identifying metaphors (Turney et al., 2011; Klebanov et al., 2014); studying persuasion (Tan et al., 2016); sarcasm detection (Bamman and Smith, 2015); and, most similar to us, polarity prediction for complex words (Ruppenhofer et al., 2017). The first dimension places words on a scale from abstract to concrete ("
C18-1325,J04-3002,0,0.15836,"affixoids for the purposes of sentiment analysis. On the one hand, theoretical linguistic work that notes the expressive function of affixoids such as Meibauer This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3853 Proceedings of the 27th International Conference on Computational Linguistics, pages 3853–3865 Santa Fe, New Mexico, USA, August 20-26, 2018. (2013) suggests this. On the other hand, it is known from prior research on sentiment analysis that hapax words in general are often subjective (Wiebe et al., 2004). As we show in §3, affixoids tend to generate many (near-)hapax forms, which meshes with observations on their productivity in the morphological literature and the general expectation about the Zipfian distribution of word frequencies. Since hapaxes, by definition, cannot be readily analyzed based on their distribution in corpora, it would be very useful if we could make use of their intrinsic properties to classify such forms as affixoid uses (and therefore likely subjective) or not. The main task that we set ourselves in this paper is morpheme sense disambiguation: we want to classify compl"
C18-1325,N16-1094,1,0.855997,"e our complex forms are unlikely to be listed in lexical resources, we will usually lack information such as glosses, supersenses or example sentences for them. 1 https://github.com/josefkr/affixoids https://de.wiktionary.org 3 http://wortwarte.de/ 4 This assumption can be made plausible by the observations that longer words have more specific meanings than shorter ones, and that they tend to have fewer meanings than their head words, and fewer meanings than their components do on average (Altmann, 2002). 2 3854 In another strand of research involving German morphology and sentiment analysis, Wiegand et al. (2016) developed an approach to classify the first element of German compounds as expressing either the source or target of evaluation, or neither, relative to the second element, if in the first step of analysis the second element was determined to be subjective. As do we, those authors focused on noun-noun compounds and they did not address polarity classification. However, their approach targets higher frequency words as it relies on the availability of sufficient corpus data to enable the use of distributional similarity. For our dataset, we cannot directly model the distributional properties of"
clematide-etal-2012-mlsa,brants-hansen-2002-developments,0,\N,Missing
clematide-etal-2012-mlsa,H05-1116,0,\N,Missing
clematide-etal-2012-mlsa,H05-1044,0,\N,Missing
clematide-etal-2012-mlsa,P10-1059,0,\N,Missing
clematide-etal-2012-mlsa,P07-1056,0,\N,Missing
clematide-etal-2012-mlsa,P05-1015,0,\N,Missing
clematide-etal-2012-mlsa,W02-1011,0,\N,Missing
clematide-etal-2012-mlsa,R11-1028,0,\N,Missing
E14-4023,W10-0731,0,0.0309094,"Missing"
E14-4023,P98-1013,0,0.299999,"Missing"
E14-4023,Q13-1023,0,0.378287,"Missing"
E14-4023,P93-1023,0,0.706371,"fact that e.g. negation and adverbs such as very or slightly impact the perceived intensity of adjectives. We work with four scales of adjectives (cf. Table 1). Our polar adjectives include 29 adjectives referring to quality and 18 adjectives relating to intelligence. Our non-polar adjectives include 8 dimensional adjectives denoting size and 22 denoting duration. The adjectives are taken, in part, from FrameNet’s (Baker et al., 1998) frames for Table 1: Adjectives used grouped by human gold standard intensity classes 1 2 As there has been previous work on how to group adjectives into scales (Hatzivassiloglou and McKeown, 1993), we consider this grouping as given. The ratings we collected and our scripts are available at www.uni-hildesheim.de/ruppenhofer/ data/DISA_data.zip. 117 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 117–122, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Data and resources 4.1 Corpus-based methods Our first method, distinctive-collexeme analysis (Collex) (Gries and Stefanowitsch, 2004) assumes that adjectives with different types of intensities co-occur with different types of adver"
E14-4023,P97-1023,0,0.920184,"Missing"
E14-4023,kamps-etal-2004-using,0,0.1266,"Missing"
E14-4023,J11-2001,0,0.174941,"ves. One resource we consider are the affective ratings (elicited with AMT) for almost 14,000 English words collected by Warriner et al. (2013). They include scores of valence (unhappy to happy), arousal (calm to aroused) and dominance (in control to controlled) for each word in the list. This scoring system follows the dimensional theory of emotion by Osgood et al. (1957). We will interpret each of these dimensions as a separate intensity score, i.e. WarV al , WarAro and WarDom . Beyond Warriner’s ratings, we consider the two polarity lexicons SentiStrength (Thelwall et al., 2010) and SoCAL (Taboada et al., 2011) which also assign intensity scores to polar expressions. 5.2 Results The results of the pairwise correlations between the human-elicited gold standard and the rankings derived from various methods and resources are shown in Table 5. For polar adjectives, most rankings correlate fairly well with human judgments. Warriner’s arousal list, however, performs poorly on quality adjectives, whereas MeanStar and Warriner’s dominance and valence lists perform better on quality than on intelligence adjectives. For MeanStar, this does not come as a surprise as quality adjectives are much more frequent in"
E14-4023,N13-1059,1,0.887048,"Missing"
E14-4023,C98-1013,0,\N,Missing
I17-1063,D08-1083,0,0.115687,"and its dependent noun have the same polarity or not, the polarity is considered to have shifted or not shifted, as detailed in Table 8. These are the class labels onto which the output of the systems (and the annotation) will be mapped. The quantitative evaluation happens on these labels. There is currently no consensus as to how shifting is to be modeled in terms of resulting polarities. For example, the shifting of excellent in (16) could either be interpreted as the resulting phrase wasn’t excellent carrying negative or neutral polarity. The first interpretation simply flips the polarity (Choi and Cardie, 2008), while the second interpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 200"
I17-1063,W14-2618,0,0.268664,"ures, we consider all verbs ranked by their frequency in our text corpus (FREQ), as well as all negative polar expressions3 ranked by frequency (NEGATIVE). 1 We assume that the scope of a verbal shifter is the set of its dependents (typically its subject or objects). 2 https://github.com/marcschulder/ ijcnlp2017 3 We consider negative polar expressions since the proportion of shifters is greatest among these expressions (Table 2). 625 EffectWordNet (EFFECT): This feature uses the idea that events may have beneficial or harmful effects on their objects. Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014; Choi and Wiebe, 2014) introduced this idea in the context of annotation and lexical acquisition work for opinion inference.4 For example, in (5) the combined facts that fall has a negative effect (henceforth referred to as –effect) on its theme (i.e. Chavez) and that people are happy about Chavez’ fall suggest that people have a negative attitude towards Chavez. As (5) shows, verbs with a –effect, such as fall, often coincide with verbal shifters. However, –effect words do not necessarily shift polarity. For instance, while abuse has a –effect, it does not shift, as shown by the fact that th"
I17-1063,D14-1125,0,0.47024,"ll verbs ranked by their frequency in our text corpus (FREQ), as well as all negative polar expressions3 ranked by frequency (NEGATIVE). 1 We assume that the scope of a verbal shifter is the set of its dependents (typically its subject or objects). 2 https://github.com/marcschulder/ ijcnlp2017 3 We consider negative polar expressions since the proportion of shifters is greatest among these expressions (Table 2). 625 EffectWordNet (EFFECT): This feature uses the idea that events may have beneficial or harmful effects on their objects. Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014; Choi and Wiebe, 2014) introduced this idea in the context of annotation and lexical acquisition work for opinion inference.4 For example, in (5) the combined facts that fall has a negative effect (henceforth referred to as –effect) on its theme (i.e. Chavez) and that people are happy about Chavez’ fall suggest that people have a negative attitude towards Chavez. As (5) shows, verbs with a –effect, such as fall, often coincide with verbal shifters. However, –effect words do not necessarily shift polarity. For instance, while abuse has a –effect, it does not shift, as shown by the fact that the verb phrase remains n"
I17-1063,N09-1016,0,0.244879,"formation on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous work. We show how the usage of NPIs can be further refined to improve the recognition of shifters (i.e. require the direct object to be a polar noun and subsequently apply PageRank). Moreover, we successfully combine this information with other features. Unlike Danescu-Niculescu-Mizil et al. (2009), we also carry out a recall-oriented evaluation and examine the impact of explicit knowledge of verbal shifters on contextual sentiment analysis. 10 631 Shifters are referred to as downward entail"
I17-1063,N10-1138,0,0.0412321,"Missing"
I17-1063,P11-1144,0,0.190173,"Missing"
I17-1063,P13-2022,0,0.118488,"on As baseline features, we consider all verbs ranked by their frequency in our text corpus (FREQ), as well as all negative polar expressions3 ranked by frequency (NEGATIVE). 1 We assume that the scope of a verbal shifter is the set of its dependents (typically its subject or objects). 2 https://github.com/marcschulder/ ijcnlp2017 3 We consider negative polar expressions since the proportion of shifters is greatest among these expressions (Table 2). 625 EffectWordNet (EFFECT): This feature uses the idea that events may have beneficial or harmful effects on their objects. Wiebe and colleagues (Deng et al., 2013; Choi et al., 2014; Choi and Wiebe, 2014) introduced this idea in the context of annotation and lexical acquisition work for opinion inference.4 For example, in (5) the combined facts that fall has a negative effect (henceforth referred to as –effect) on its theme (i.e. Chavez) and that people are happy about Chavez’ fall suggest that people have a negative attitude towards Chavez. As (5) shows, verbs with a –effect, such as fall, often coincide with verbal shifters. However, –effect words do not necessarily shift polarity. For instance, while abuse has a –effect, it does not shift, as shown"
I17-1063,P16-1047,0,0.0497277,"− − (18) She [[brought down]V a curseN ]VP on the village. 6 Related Work Negation modeling is a central research issue in sentiment analysis, but only few works consider more than typical negation words. We refer the reader to the survey of Wiegand et al. (2010) for more information on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous work. We show how the usage of NPIs can be further refined to improve the recognition of shifters (i.e. require the direct object to be a polar noun and subsequently apply PageRank). Moreover, we successful"
I17-1063,P16-1191,0,0.0822976,"Missing"
I17-1063,E09-1005,0,0.0331185,"Missing"
I17-1063,P98-1013,0,0.798538,"Missing"
I17-1063,N09-1002,0,0.487693,"Missing"
I17-1063,W13-3514,0,0.234365,"Missing"
I17-1063,morante-2010-descriptive,0,0.476051,"ources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are content words that have semantic properties similar to negation. For example, the negated statement in (1) involving the negation word not can also be expressed by the verbal shifter fail in (2). (1) Peter did not pass the exam. (2) Peter failedshifter to pass the exam. Similarly, shifting is also caused by nouns (e.g. downfall) and adjectives (e.g. devoid). Polarity shifters are important for various tasks in NLP, such as relation extraction (SanchezGraillet and Poesio, 2007), rec"
I17-1063,W09-1105,0,0.468525,"Similarity (SIM): As our aim is to identify verbs whose semantics resembles that of negation words, a straightforward method is to extract verbs that are distributionally similar to negation words. Using Word2Vec (Mikolov et al., 2013), we compute word embeddings for our text corpus.5 All verbs are ranked by their cosine similarity to a given negation word. The highest ranking verbs are considered verbal shifters. As negation words we consider the intersection of two negation word lists: the negation category in the valence shifter lexicon by Wilson et al. (2005) and the negation signals from Morante and Daelemans (2009). The negation words are neither, never, no, none, nor, not and without. (7) This [tore downshifter our great [dream]+ ]− . (8) Please [lay asideshifter all your [worries]− ]+ . We only consider particles which typically indicate a complete transition to a negative end state: aside, away, back, down, off and out. To produce rankings, we sort the particle verbs by their absolute frequency in our text corpus. Heuristic using ‘any’ (ANY): Our final shifter feature rests on the linguistic insight that negative polarity items (NPIs) (Giannakidou, 2008), such as English any, typically appear in the"
I17-1063,I08-1039,0,0.401329,"evaluate the shifter lexicons generated by our best graph-based classifier (LEXLP ) and best supervised classifier (LEXSVM ) from §5.2. Our + (17) The revolution [[brought down]V the tyrant− N ]VP . − − (18) She [[brought down]V a curseN ]VP on the village. 6 Related Work Negation modeling is a central research issue in sentiment analysis, but only few works consider more than typical negation words. We refer the reader to the survey of Wiegand et al. (2010) for more information on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous wor"
I17-1063,D13-1170,0,0.128113,"y on verbs. As the main predicates of phrases they tend to have larger scopes than nouns and adjectives, increasing the impact of their polarity shifting. Their vocabulary size is also smaller, allowing us to cover a reasonable share of it in our evaluation. Existing resources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are content words that have semantic properties similar to negation. For example, the negated statement in (1) involving the negation word not can also be expressed by the verbal shifter fail in (2). (1) Peter did not pass th"
I17-1063,R11-1028,0,0.0264406,"erpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 2009) and summarization (Stoyanov and Cardie, 2011). For that reason and because we specifically study compositionality between verbs and their object, we exclusively consider polarity classification for verb phrases. The experiment is treated as a binary classification task, where the polarity of a noun has either shifted in the context of a verb phrase (VP) or not. For example in (15), the VP lack her usual passion contains the positive polarity noun passion which is shifted by lack. − (15) The book seemed to [lackV [her usual passion+ N ]NP ]VP . We compiled sentences from our text corpus (Amazon Product Review Data, §2) that contain 9 630"
I17-1063,P14-1145,0,0.0825032,"the respective verb 4 Initially, events with positive/negative effects were referred to as good-for/bad-for events. We use the terminology Choi and Wiebe (2014) introduced for EffectWordNet. 5 Following the work of Wiegand and Ruppenhofer (2015) in verb category induction for sentiment roles, a task similar to ours, we use continuous bag-of-words with 500 dimensions. 626 (ANYnorm ). As a further constraint we demand that the direct object represents a polar expression (ANYnorm+polar ). This constraint is fulfilled in (10) since help is a positive polar expression. 2009; Choi and Wiebe, 2014; Kang et al., 2014). We assume that the explanatory texts of glosses are similar among shifters. We treat glosses as a bag-of-words feature. 3.2 We also use WordNet to assign semantic types. Our intuition is that verbal shifters share the same semantic types. We consider two types of information that have been previously found effective for sentiment analysis in general, namely the hypernyms of verbs (Breck et al., 2007) and their supersenses (Flekova and Gurevych, 2016). Anti-Shifter Feature (ANTI) We also introduce a feature for automatically retrieving verbs that – semantically speaking – are the exact opposi"
I17-1063,W08-0606,0,0.154026,"they tend to have larger scopes than nouns and adjectives, increasing the impact of their polarity shifting. Their vocabulary size is also smaller, allowing us to cover a reasonable share of it in our evaluation. Existing resources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are content words that have semantic properties similar to negation. For example, the negated statement in (1) involving the negation word not can also be expressed by the verbal shifter fail in (2). (1) Peter did not pass the exam. (2) Peter failedshifter to pass the ex"
I17-1063,C12-2056,0,0.382928,"Missing"
I17-1063,J11-2001,0,0.218878,"s labels onto which the output of the systems (and the annotation) will be mapped. The quantitative evaluation happens on these labels. There is currently no consensus as to how shifting is to be modeled in terms of resulting polarities. For example, the shifting of excellent in (16) could either be interpreted as the resulting phrase wasn’t excellent carrying negative or neutral polarity. The first interpretation simply flips the polarity (Choi and Cardie, 2008), while the second interpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 2009) and summarization (Stoyanov and Cardie, 2011). For that reason and because we specifically study compositionality between verbs and their object, we e"
I17-1063,D08-1061,0,0.0103467,"eds for anti-shifters8 (using ANTI from §3.2) to reflect the general bias towards non-shifter verbs (Table 1). In order to examine whether anti-shifters are actually necessary to get negative seeds of sufficient quality, we also run an alternative setting (noAntiShifter) in which the same number of negative seeds is simply extracted from the ranking of frequent verbs. The reasoning behind this is that the proportion of frequent verbs not being shifters is already fairly high, as shown by FREQ in Table 4. For LP, we considered the Adsorption label propagation algorithm as implemented in junto (Talukdar et al., 2008). For kNN, we set k = 10. Apart from the graph-based classifiers, we also consider a supervised classifier, namely Support Vector Machines (SVM) as implemented in SVMlight (Joachims, 1999). This classifier uses manually labeled training data, but, unlike LP and kNN, we may combine arbitrary feature sets. We perform 10-fold cross validation and report on accuracy and macro-average precision, recall and Fscore. For the task-specific features (§3) we use their most complex configurations from Table 3 (e.g. SIMcentroid rather than SIMnor or SIMwithout ). Table 5 shows that among the graph-based cl"
I17-1063,W06-0301,0,0.291745,"Missing"
I17-1063,W10-3111,1,0.949649,"r sheer number. On a sample of manually annotated verbs we examine a variety of linguistic features for this task. Then we build a supervised classifier to increase coverage. We show that this approach drastically reduces the annotation effort while ensuring a high-precision lexicon. We also show that our acquired knowledge of verbal polarity shifters improves phrase-level sentiment analysis. 1 (3) She was [deniedshifter the [scholarship]+ ]− . (4) The new treatment has [alleviatedshifter her [pain]− ]+ . Although there has been significant research on polarity shifting in sentiment analysis (Wiegand et al., 2010), this work has focused on the presence of negation words. Negation words (no, not, never, etc.) are typically function words, so only a few exist. Polarity shifters are content words, of which there are a lot more. For instance, WordNet (Miller et al., 1990) contains over 10k verbal, 20K adjectival and 110K nominal lemmas. An exhaustive manual annotation would be far too costly. To reduce cost, we introduce a bootstrapping approach for the acquisition of polarity shifters. In this work we focus exclusively on verbs. As the main predicates of phrases they tend to have larger scopes than nouns"
I17-1063,W16-0410,0,0.0492823,"e output of the systems (and the annotation) will be mapped. The quantitative evaluation happens on these labels. There is currently no consensus as to how shifting is to be modeled in terms of resulting polarities. For example, the shifting of excellent in (16) could either be interpreted as the resulting phrase wasn’t excellent carrying negative or neutral polarity. The first interpretation simply flips the polarity (Choi and Cardie, 2008), while the second interpretation is driven by the fact that the negation of excellent is not synonymous with its antonym atrocious (Taboada et al., 2011; Kiritchenko and Mohammad, 2016). The polar inImpact on Sentiment Analysis We now investigate whether knowledge of verbal shifters can be useful for the identification of contextual phrase-level sentiment. Apart from being an intermediate step in compositional sentencelevel classification, phrase-level classification is also independently needed for applications such as knowledge base population (Mitchell, 2013), question answering (Dang, 2009) and summarization (Stoyanov and Cardie, 2011). For that reason and because we specifically study compositionality between verbs and their object, we exclusively consider polarity clas"
I17-1063,K15-1022,1,0.845674,"ages (Krifka, 1991). (9) They did [not give us any [helpdobj ]+ ]− . (10) They [deniedshifter us any [helpdobj ]+ ]− . The feature we design collects all verbs that take a direct object that is modified by the NPI any, as in (10). We sort the verbs by their frequency of co-occurrence with this particular textual pattern (ANY). We normalize that pattern frequency by the frequency of the respective verb 4 Initially, events with positive/negative effects were referred to as good-for/bad-for events. We use the terminology Choi and Wiebe (2014) introduced for EffectWordNet. 5 Following the work of Wiegand and Ruppenhofer (2015) in verb category induction for sentiment roles, a task similar to ours, we use continuous bag-of-words with 500 dimensions. 626 (ANYnorm ). As a further constraint we demand that the direct object represents a polar expression (ANYnorm+polar ). This constraint is fulfilled in (10) since help is a positive polar expression. 2009; Choi and Wiebe, 2014; Kang et al., 2014). We assume that the explanatory texts of glosses are similar among shifters. We treat glosses as a bag-of-words feature. 3.2 We also use WordNet to assign semantic types. Our intuition is that verbal shifters share the same sem"
I17-1063,H05-1044,0,0.731135,"ectival and 110K nominal lemmas. An exhaustive manual annotation would be far too costly. To reduce cost, we introduce a bootstrapping approach for the acquisition of polarity shifters. In this work we focus exclusively on verbs. As the main predicates of phrases they tend to have larger scopes than nouns and adjectives, increasing the impact of their polarity shifting. Their vocabulary size is also smaller, allowing us to cover a reasonable share of it in our evaluation. Existing resources barely cover any verbal shifters at all. Even the most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our initial random sample of 2000 verb lemmas contained 300 shifters. The corpora on which negation can be learned, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), only comprise contiguous sentences of fairly small datasets, so only the most frequently occurring negation words are considered. For example, only 6 verbal shifters are observed on the BioScope corpus (Morante, 2010). Introduction We present an approach towards bootstrapping a lexicon of polarity shifters. Polarity shifters are con"
I17-1063,D13-1099,0,0.0763162,"[[brought down]V the tyrant− N ]VP . − − (18) She [[brought down]V a curseN ]VP on the village. 6 Related Work Negation modeling is a central research issue in sentiment analysis, but only few works consider more than typical negation words. We refer the reader to the survey of Wiegand et al. (2010) for more information on negation modeling. Approaches to learning negation from labeled corpora have been examined in the review domain (Ikeda et al., 2008; Kessler and Sch¨utze, 2012; Socher et al., 2013; Yu et al., 2016), the biomedical domain (Huang and Lowe, 2007; Morante and Daelemans, 2009; Zou et al., 2013) and across domains (Fancellu et al., 2016). However, as outlined in §1, due to their small size the labeled datasets include few different verbal shifters. Moreover, these works mostly focus on scope detection rather than the identification of shifters. The work most closely related to ours is Danescu-Niculescu-Mizil et al. (2009) who propose using NPIs for shifter extraction.10 However, our work substantially extends that previous work. We show how the usage of NPIs can be further refined to improve the recognition of shifters (i.e. require the direct object to be a polar noun and subsequent"
I17-1063,C98-1013,0,\N,Missing
I17-1063,D14-1082,0,\N,Missing
K15-1022,P97-1023,0,0.163194,"arity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two wo"
K15-1022,P98-1013,0,0.166676,"cs. We evaluate our approach against a new manuallycompiled opinion role lexicon and perform in-context classification. 1 Introduction While there has been much research in sentiment analysis on subjectivity detection and polarity classification, there has been less work on the extraction of opinion roles, i.e. entities that express an opinion (opinion holders), and entities or propositions at which sentiment is directed (opinion targets). Previous research relies on large amounts of labeled training data or leverages general semantic resources which are expensive to construct, e.g. FrameNet (Baker et al., 1998). In this paper, we present an approach to induce opinion roles of verbal predicates. The input is a set of opinion verbs that can be found in a common sentiment lexicon. Our model rests on the assumption that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Associati"
K15-1022,P14-1023,0,0.0128308,"sts for each of the verb types the 12 seeds most frequently occurring with the respective patterns. We observed that the SP-verb seeds are exclusively negative polar expressions. That is why we also extracted seeds from an additional pattern help to XVB producing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the fin"
K15-1022,W09-1206,0,0.0329091,"Missing"
K15-1022,H05-1045,0,0.345073,"Missing"
K15-1022,P11-1144,0,0.0598548,"graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced Lexicon Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′ ) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: P 1 ,r,w)+I(w2 ,r,w)) (r,w)∈T (w1 )∩T (w2 ) (I(w P (r,w)∈T (w1 ) I(w1 ,r,w)+ (r,w)∈T (w2 ) I(w2 ,r,w) kw,r,w′ k×k∗,r,∗k I(w, r, w′ ) = log kw,r,∗k×k∗,r,w′ k sim(w1 , w2 ) = P where and T (w) is defined as the set of pairs (r, w′ ) such that kw,r,w′ k×k∗,r,∗k &gt; 0. log kw,r,∗k×k∗,r,w′k 4.3 Prec Majority Class Only Seeds Table 5: Eval. of similarity metrics and classifiers. (15) They criticize and hate him. (16) conj(criticiz"
K15-1022,D10-1101,0,0.139958,"Missing"
K15-1022,P10-1060,0,0.0608076,"Missing"
K15-1022,J13-3002,0,0.457996,"&lt; 0.05) ∗ : better than CK; ◦ : better than CK + inducWiegand 2012 ; † : better than CK + inducgraph ; ‡ : better than CK + coarse-grain lex Distribution of Verb Types Corpus MPQA CK CK + inducWiegand CK + inducgraph Table 12: Lexical resources and the impact of other (not lexicon-based) features (evaluation measure: macro-average F-score). out of domain Config Comparison to Previous Cross-Domain Opinion Holder Extraction We now compare our proposed induction approach with previous work on opinion holder ex7 The split-up of training and test set on the MPQA corpus follows the specification of Johansson and Moschitti (2013). 222 For classifiers, we consider convolution kernels CK from Wiegand and Klakow (2012) and the sequence labeler from Johansson and Moschitti (2013) MultiRel that incorporates relational features taking into account interactions between multiple opinion cues. It is currently the most sophisticated opinion holder extractor. CK can be combined with additional knowledge. We compare inducgraph with inducWiegand 2012 , which employs the word lists induced for AG- and PTverbs in the fashion of Wiegand and Klakow (2012), i.e. without graph clustering. As an upper bound for the induction methods, coa"
K15-1022,P14-1145,0,0.171151,"antic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced L"
K15-1022,W06-0301,0,0.521525,"in this paper also notably outperforms the induction from Wiegand and Klakow (2012). Although the fine-grained lexicon is among the top performing systems, we only note large improvements on VERB. VERB has the highest proportion of PT- and SP-verbs (Table 13). Knowledge about role-assignment is most critical here. 6 Related Work Most approaches for opinion role extraction employ supervised learning. The feature design is mainly inspired by semantic role labeling (Bethard et al., 2004; Li et al., 2012). Some work also employs information from existing semantic role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of sema"
K15-1022,kingsbury-palmer-2002-treebank,0,0.485079,"ion that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s κ = 60.8 for opinion holders, κ = 62.3 for opinion targets and κ = 59.9 for speaker views. This agreement is mostly substantial (Landis and Koch, 1977). s"
K15-1022,N10-1138,0,0.142252,"Missing"
K15-1022,P03-1054,0,0.005052,"ly applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced Lexicon Dependency-based Similarity The metric proposed by Lin (1998) exploits t"
K15-1022,S12-1029,0,0.0532905,"Missing"
K15-1022,W09-4635,0,0.170875,"hose verbs by frequency. Size and verb type distribution are preserved. We also examine what impact doubling the size of seeds (Gold|Patterndouble ) and halving them (Gold|Patternhalf ) has on classification. Dependency-based similarity and graph clustering is used for all configurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6 . As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our pr"
K15-1022,E14-1040,0,0.0742685,"orizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s κ = 60.8 for opinion holders, κ = 62.3 for opinion targets and κ = 59.9 for speaker views. This agreement is mostly substantial (Landis and Koch, 1977). semantic roles for holders and targets. 2 Lexicon-based Opinion Role Extraction Opinion holder and target extraction is a hard task (Ruppenhofer et al., 2008). Conventional syntactic or semantic levels of representation do not capt"
K15-1022,esuli-sebastiani-2006-sentiwordnet,0,0.272652,"e word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen as a proxy. Verbs co-occurring with prototypical opinion holders do not represent the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coo"
K15-1022,P05-1045,0,0.0246464,"Missing"
K15-1022,D10-1008,0,0.0168574,"ntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(con"
K15-1022,W97-0802,0,0.282723,"gurations. Only if we double the amount of seeds are the gold seeds notably better than the automatically generated seeds. Since our induction approach just requires a sentiment lexicon and aims at low-resource languages, we replicated the experiments for German, as shown in Table 9. We use the PolArtsentiment lexicon (Klenner et al., 2009) (1416 entries). (As a gold standard, we manually annotated that lexicon according to our three verb types.) As an unlabeled corpus, we chose the Huge German Corpus6 . As a parser, we used ParZu (Sennrich et al., 2009). Instead of WordNet, we used GermaNet (Hamp and Feldweg, 1997). The automatically generated seeds were manually translated from English to German. Table 9 shows that as on English data, dependency-based similarity combined with graph clustering performs best. The fact that we can successfully replicate our approach in another language supports the general applicability of our proposed categorization of verbs into three types for opinion role extraction. SP-verbs graph Embedd. Table 9: Comparison of English and German data (evaluation measure: macro-average F-score). Table 6: The 12 most similar verbs to outrage (PT-verb) according to the different metric"
K15-1022,P98-2127,0,0.0844581,"in and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8 72.0 65.4 68.0 64.5 70.6 5 Experiments As a similarity function, we simply take the absolute frequency of observing two words w1 and w2 in a conjunction, i.e. sim(w1 , w2 ) = f req(conj(w1 , w2 )). 5.1 Evaluation of the Induced Lexicon Dependency-based Similarity The metric proposed by Lin (1998) exploits the rich set of dependency-relation labels in the context of distributional similarity. Moreover, it has been effectively used for the related task of extending frames of unknown predicates in semantic parsing (Das and Smith, 2011). The metric is based on dependency triples (w, r, w′ ) where w and w′ are words and r is a dependency relation (e.g. (argue-V, nsubj, critics-N)). The metric is defined as: P 1 ,r,w)+I(w2 ,r,w)) (r,w)∈T (w1 )∩T (w2 ) (I(w P (r,w)∈T (w1 ) I(w1 ,r,w)+ (r,w)∈T (w2 ) I(w2 ,r,w) kw,r,w′ k×k∗,r,∗k I(w, r, w′ ) = log kw,r,∗k×k∗,r,w′ k sim(w1 , w2 ) = P where and"
K15-1022,D13-1171,0,0.0264352,"Missing"
K15-1022,N04-3012,0,0.0630317,"ducing prototypical positive SP-verbs, such as stabilize, allay or heal. 4.2 Similarity Metrics 4.2.1 Word Embeddings Recent research in machine learning has focused on inducing vector representations of words. As an example of a competitive word embedding method, we induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen"
K15-1022,J11-1002,0,0.0429458,"role labelers based on FrameNet (Kim and Hovy, 2006) or PropBank (Johansson and Moschitti, 2013; Wiegand and Klakow, 2012). Although those resources give extra information for opinion role extraction in comparison to syntactic or other surface features, we showed in this work that further taskspecific knowledge, i.e. either opinion verb types or a manually-built opinion role lexicon, provide even more accurate information. There has been a substantial amount of research on opinion target extraction. It focuses, however, on the extraction of topic-specific opinion terms (Jijkoun et al., 2010; Qiu et al., 2011) rather than the variability of semantic roles for opinion holders and targets. Mitchell et al. (2013) present a low-resource approach for target extraction but their aim is to process Twitter messages without using general syntax tools. In this work, we use such tools. Our notion of low resources is different in that we mean the absence of semantic resources helpful for our task (e.g. FrameNet). 7 Conclusion We presented an approach for opinion role induction for verbal predicates. We assume that those predicates can be divided into three different verb types where each type is associated wit"
K15-1022,E09-1077,0,0.181561,"induce vectors for our opinion verbs with Word2Vec (Mikolov et al., 2013). Baroni et al. (2014) showed that this method outperforms count vector representations on a variety of tasks. For the similarity between two verbs, we compute the cosine-similarity between their vectors. 4.2.2 WordNet::Similarity We use WordNet::Similarity (Pedersen et al., 2004) as an alternative source for similarity metrics. The metrics are based on WordNet’s graph structure (Miller et al., 1990). Various relations within WordNet have been shown to be effective for polarity classification (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009). Pattern-based Seed Initialization For AG-verbs, we rely on the findings of Wiegand and Klakow (2012) who suggest that verbs predictive for opinion holders can be induced with the help of prototypical opinion holders. These common nouns, e.g. opponents (9) or critics (10), act like opinion holders and, therefore, can be seen as a proxy. Verbs co-occurring with prototypical opinion holders do not represent the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a sy"
K15-1022,W97-0313,0,0.144381,"n holders do not represent the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependenc"
K15-1022,P98-2182,0,0.118416,"the entire range of opinion verbs but coincide with AG-verbs. 4.2.3 Coordination Another method to measure similarity is obtained by leveraging coordination. Coordination is known to be a syntactic relation that also preserves great semantic coherence (Ziering et al., 2013), e.g. (15). It has been successfully applied (9) Opponents claim these arguments miss the point. (10) Critics argued that the proposed limits were unconstitutional. For PT-verbs, we make use of the adjective heuristic proposed by Wiegand and Klakow (2012). The 218 not only to noun categorization (Riloff and Shepherd, 1997; Roark and Charniak, 1998) but also to different tasks in sentiment analysis, including polarity classification (Hatzivassiloglou and McKeown, 1997), the induction of patient polarity verbs (Goyal et al., 2010) and connotation learning (Kang et al., 2014). We use the dependency relation from Stanford parser (Klein and Manning, 2003) to detect coordination (16). Rec F1 45.7 8.9 14.2 87.0 33.3 9.8 20.9 17.6 Coordination kNN graph 45.2 42.7 61.5 68.7 47.3 39.7 53.4 50.4 WordNet kNN graph 52.8 51.1 51.5 51.9 50.7 51.5 51.1 51.5 Embedding kNN graph 59.3 64.0 58.4 70.5 61.0 59.4 59.7 64.5 Dependency kNN graph 65.7 70.3 63.8"
K15-1022,W12-3716,1,0.849551,"that we can compensate some lexical knowledge missing in induction by standard features. Since we could substantially outperform the features relying on FrameNet with our new lexical resources, we looked closer at the predicted frame structures. Beside obvious errors in automatic frame assignment, we also found that there are problems inherent in the frame design. Particularly, the notion of SP-verbs (§3.3) is not properly reflected. Many frames, such as S CRUTINY, typically devised for AG-verbs, such as investigate or analyse, also contain SP-verbs like pry. This observation is in line with Ruppenhofer and Rehbein (2012) who claim that extensions to FrameNet are necessary to properly represent opinions evoked by verbal predicates. 5.3 FICTION CK + coarse-grain lex 66.82∗ 64.13∗◦ 63.72∗◦† CK + fine-grain lex 66.16∗ 64.98∗◦ 70.85∗◦†‡ statistical significance testing (permutation test, significance level p &lt; 0.05) ∗ : better than CK; ◦ : better than CK + inducWiegand 2012 ; † : better than CK + inducgraph ; ‡ : better than CK + coarse-grain lex Distribution of Verb Types Corpus MPQA CK CK + inducWiegand CK + inducgraph Table 12: Lexical resources and the impact of other (not lexicon-based) features (evaluation m"
K15-1022,D08-1061,0,0.444093,"st neighbour classifier (kNN) (Cover and Hart, 1967) as a simple method for propagating labels from seeds to other instances. Alternatively, we consider verb categorization as a clustering task on a graph G = (V, E, W ) where V is the set of nodes (i.e. our opinion verbs), E is the set of edges connecting them with weights W : E → R+ . W can be directly derived from any of the similarity metrics (§4.2.1-§4.2.4). The aim is that all nodes v ∈ V are assigned a label l ∈ {AG, P T, SP }. Initially, only the verb seeds are labeled. We then use the Adsorption label propagation algorithm from junto (Talukdar et al., 2008) in order propagate the labels from the seeds to the remaining verbs. 219 Table 5 compares the performance of the different similarity metrics when incorporated in either kNN or graph clustering. The resulting categorizations are compared against the gold standard coarse-grained lexicon (§3.4). For kNN, we set k = 3 for which we obtained best performance in all our experiments. As seeds, we took the top 40 AG-verbs, 30 PTverbs and 50 SP-verbs produced by the respective initialization methods (§4.1). The seed proportions should vaguely correspond to the actual class distribution (Table 3). Larg"
K15-1022,P06-1134,0,0.0163662,"AG-view verbs with AG- and SP-view verbs with AG- and PT-view Relation to Fine-Grained Lexicon Table 2 provides statistics as to how clear-cut the three prototypical verb types are in the manuallycompiled fine-grained lexicon. These numbers suggest that many verbs evoke several opinion views (e.g. a verb with an AG-view may also evoke a PT-view). While the fine-grained lexicon is fairly exhaustive in listing semantic roles for opinion holders and targets, it may also occasionally overgenerate. One major reason for this is that we do not annotate on the sense-level (word-sense disambiguation (Wiebe and Mihalcea, 2006) is still in its infancy) but on the lemmalevel. Accordingly, we attribute all views to all senses, whereas actually certain views pertain only to specific senses. However, we found that usually one view is conveyed by most (if not all) senses of a word. For example, the lexicon lists both an AGview and a PT-view for appease. This is correct (5) (6) (7) (8) [Chamberlain]agent appeased [Hitler]patient . [The orange juice]agent appeased [him]patient for a while. [Mary]agent degrades [Henrietta]patient . [This technique]agent degrades [the local water supply]patient . 4 Induction of Verb Categori"
K15-1022,E12-1033,1,0.115441,", conspire authors make use of the observation that morphologically related adjectives exist for PT-verbs, unlike for AG- and SP-verbs. Therefore, in order to extract PT-verbs, one needs to check whether a verb in its past participle form, such as upset in (11), is identical to some predicate adjective (12). Table 4: The top 12 extracted verb seeds. ond step, a similarity metric (§4.2) is employed in order to propagate the verb type labels from the seeds to the remaining opinion verbs (§4.3). The North American News Text Corpus is used for seed extraction and computation of verb similarities. Wiegand and Klakow (2012) proposed methods for extracting AG- and PT-verbs. We will re-use these methods for generating seeds. A major contribution of this paper is the introduction of the third dimension, i.e. SP-verbs, in the context of induction. We show that in combination with this third dimension, one can categorize all opinion verbs contained in a sentiment lexicon. Furthermore, given this three-way classification, we also obtain better results on the detection of AG-verbs and PT-verbs than by just detecting those verbs in isolation without graph clustering (this will be shown in Table 7 and discussed in §5.1)."
K15-1022,H05-1044,0,0.0270769,"ion verbs that can be found in a common sentiment lexicon. Our model rests on the assumption that those verbs can be divided into three different types. Each type has a characteristic mapping between semantic roles and opinion holders and targets. Thus, the problem of opinion role induction is reduced to automatically categorizing opinion verbs. 215 Proceedings of the 19th Conference on Computational Language Learning, pages 215–225, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Our lexicon covers the 1175 verb lemmas contained in the Subjectivity Lexicon (Wilson et al., 2005). We annotated the semantic roles similar to the format of PropBank (Kingsbury and Palmer, 2002). The basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a verb definition and example sentences. We do not annotate implicature-related information about effects (Deng and Wiebe, 2014) but inherent sentiment (the data release2 includes more details regarding the annotation process and our notion of holders and targets). On a sample of 400 verbs, we measured an interannotation agreement of Cohen’s κ = 60.8 for opinion holders, κ = 62.3 for opinion targets"
K15-1022,P94-1019,0,0.527485,"increases of the seed sets do not improve the quality (as shown below). 10 of the 50 SP-verbs are extracted from the positive SP-patterns, while the remaining verbs are extracted from the negative SP-patterns (§4.1). As baselines, we include a classifier only employing the seeds and a majority class classifier always predicting an SP-verb. For word embeddings (§4.2.1) and WordNet::Similarity (§4.2.2), we only report the performance of the best metric/configuration, i.e. for embeddings, the continuous bag-of-words model with 500 dimensions and for WordNet::Similarity, the Wu & Palmer measure (Wu and Palmer, 1994). Table 5 shows that the baselines can be outperformed by large margins. The performance of the different similarity metrics varies. The dependency-based metric performs notably better than the other metrics. Together with word embeddings, it is the only metric for which graph clustering produces a notable improvement over kNN. Table 6 illustrates the quality of the similarity metrics for the present task. The table shows that the dependency-based similarity metric provides the most suitable output. The poor quality of coordination may come as a surprise. That Coordin. appear, believe, refuse,"
K15-1022,P13-1161,0,0.0805788,"Missing"
K15-1022,I13-1188,0,0.0340422,"Missing"
K15-1022,C98-1013,0,\N,Missing
K15-1022,C98-2177,0,\N,Missing
K15-1022,C98-2122,0,\N,Missing
L16-1460,E14-1040,0,0.0244847,"evaluation to participant evaluation. Keywords: Opinion Inference, Effects, Subjectivity 1. Introduction So far, sentiment analysis has mainly focused on the detection of explicit opinions. However, recently the relevance of implicit opinions has received broader attention within the field. Here, the work of Klenner and colleagues on verb polarity frames (Klenner et al., 2014; Klenner, 2015), the functor-based framework of event evaluation by Anand and Reschke (Anand and Reschke, 2010; Reschke and Anand, 2011) and the work on opinion implicature by Wiebe and her colleagues (Deng et al., 2013; Deng and Wiebe, 2014; Wiebe and Deng, 2014), based on the effects that events have on object entities, have been the most important contributions to the theory of implicit opinions. The key observation of this line of work is that, while some predicates do not denote sentiment, in combination with expressions of explicit sentiment and knowledge about attitudes towards entities, one can infer implicit opinions being conveyed that hinge on these inference-relevant predicates. Consider (1). From the explicit positive sentiment (in boldface) by Peter towards the fact that the Colts lost, an event which affects them n"
L16-1460,P13-2022,0,0.0208763,"reason from event evaluation to participant evaluation. Keywords: Opinion Inference, Effects, Subjectivity 1. Introduction So far, sentiment analysis has mainly focused on the detection of explicit opinions. However, recently the relevance of implicit opinions has received broader attention within the field. Here, the work of Klenner and colleagues on verb polarity frames (Klenner et al., 2014; Klenner, 2015), the functor-based framework of event evaluation by Anand and Reschke (Anand and Reschke, 2010; Reschke and Anand, 2011) and the work on opinion implicature by Wiebe and her colleagues (Deng et al., 2013; Deng and Wiebe, 2014; Wiebe and Deng, 2014), based on the effects that events have on object entities, have been the most important contributions to the theory of implicit opinions. The key observation of this line of work is that, while some predicates do not denote sentiment, in combination with expressions of explicit sentiment and knowledge about attitudes towards entities, one can infer implicit opinions being conveyed that hinge on these inference-relevant predicates. Consider (1). From the explicit positive sentiment (in boldface) by Peter towards the fact that the Colts lost, an even"
L16-1460,W97-0802,0,0.417121,"effect information in +/-EffectWordNet is intended to feed into a rule-based inference machinery along the lines of the ideas laid out in Wiebe and Deng (2014), which computes implicit opinions from the presence of effect information and explicit sentiment within a sentence, similar to the reasoning used in our discussion of (1) above. Note that 2879 +/-EffectWordNet in its current form does not include (morphosyntactic or semantic) information with regard to the entity affected. Inspired by +/-EffectWordNet, Ruppenhofer and Brandes (2015) began work on a German resource, annotating GermaNet (Hamp and Feldweg, 1997) synsets with information relevant to opinion inference. 3. Data We use the same GermaNet V9.0 (Hamp and Feldweg, 1997) synsets as Ruppenhofer and Brandes (2015). These authors extracted 1667 synsets for annotation in order to have gold standard data for automatic methods.1 Note that the relevant synsets include verbs and adjectives, but no nouns. Figure 1 displays an instance of a GermaNet synset, along with its annotations. We discuss the meaning of the annotations in the next section. 4. a the initial pair of square brackets containing our effect annotations (bold-face); b the orth forms se"
L16-1460,W09-4635,0,0.0137069,"We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8. 2. Related Work Resources that contain information relevant to opinion inference are rather sparse, compared to the abundance of sentiment lexica for English (Wilson et al., 2005; Taboada et al., 2011) and German (Klenner et al., 2009; Remus et al., 2010; Waltinger, 2010). In fact, even for English there seems to exist only one resource for this task, namely +/-EffectWordNet (Choi et al., 2014). This resource is structured on the sense level, building on WordNet (Miller et al., 1990) as its underlying structure. The effect information in +/-EffectWordNet is intended to feed into a rule-based inference machinery along the lines of the ideas laid out in Wiebe and Deng (2014), which computes implicit opinions from the presence of effect information and explicit sentiment within a sentence, similar to the reasoning used in our"
L16-1460,W15-2919,0,0.0179876,"greement study. We also present results of a crowdsourcing experiment to test the utility of some known and some new functors for opinion inference where, unlike in previous work, subjects are asked to reason from event evaluation to participant evaluation. Keywords: Opinion Inference, Effects, Subjectivity 1. Introduction So far, sentiment analysis has mainly focused on the detection of explicit opinions. However, recently the relevance of implicit opinions has received broader attention within the field. Here, the work of Klenner and colleagues on verb polarity frames (Klenner et al., 2014; Klenner, 2015), the functor-based framework of event evaluation by Anand and Reschke (Anand and Reschke, 2010; Reschke and Anand, 2011) and the work on opinion implicature by Wiebe and her colleagues (Deng et al., 2013; Deng and Wiebe, 2014; Wiebe and Deng, 2014), based on the effects that events have on object entities, have been the most important contributions to the theory of implicit opinions. The key observation of this line of work is that, while some predicates do not denote sentiment, in combination with expressions of explicit sentiment and knowledge about attitudes towards entities, one can infer"
L16-1460,remus-etal-2010-sentiws,0,0.0249773,"lity and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8. 2. Related Work Resources that contain information relevant to opinion inference are rather sparse, compared to the abundance of sentiment lexica for English (Wilson et al., 2005; Taboada et al., 2011) and German (Klenner et al., 2009; Remus et al., 2010; Waltinger, 2010). In fact, even for English there seems to exist only one resource for this task, namely +/-EffectWordNet (Choi et al., 2014). This resource is structured on the sense level, building on WordNet (Miller et al., 1990) as its underlying structure. The effect information in +/-EffectWordNet is intended to feed into a rule-based inference machinery along the lines of the ideas laid out in Wiebe and Deng (2014), which computes implicit opinions from the presence of effect information and explicit sentiment within a sentence, similar to the reasoning used in our discussion of (1) a"
L16-1460,W11-0145,0,0.369329,"e new functors for opinion inference where, unlike in previous work, subjects are asked to reason from event evaluation to participant evaluation. Keywords: Opinion Inference, Effects, Subjectivity 1. Introduction So far, sentiment analysis has mainly focused on the detection of explicit opinions. However, recently the relevance of implicit opinions has received broader attention within the field. Here, the work of Klenner and colleagues on verb polarity frames (Klenner et al., 2014; Klenner, 2015), the functor-based framework of event evaluation by Anand and Reschke (Anand and Reschke, 2010; Reschke and Anand, 2011) and the work on opinion implicature by Wiebe and her colleagues (Deng et al., 2013; Deng and Wiebe, 2014; Wiebe and Deng, 2014), based on the effects that events have on object entities, have been the most important contributions to the theory of implicit opinions. The key observation of this line of work is that, while some predicates do not denote sentiment, in combination with expressions of explicit sentiment and knowledge about attitudes towards entities, one can infer implicit opinions being conveyed that hinge on these inference-relevant predicates. Consider (1). From the explicit posi"
L16-1460,W15-2910,1,0.536853,"ent towards the Colts. Furthermore, depending on the context, one may infer that the speaker/writer producing (1) may have a positive or negative attitude towards Peter, depending on whether the speaker disapproves or approves of the Colts. (1) Peter is happy because the Colts LOST. While we expect a sentiment lexicon to provide us with information that e.g. happy expresses positive sentiment, the information that the effect-bearing word lose entails negative affect on the subject can so far not be retrieved from any lexical-semantic resource. In this contribution, we push the effort begun by Ruppenhofer and Brandes (2015) toward a synthesis between the effect-based approach and the functor approach further in several respects. Firstly, we identify and propose additional functors for verbs embedding states of possibility, predicates expressing location, and predicates expressing similarity. These functors allow us to cover predicates whose entailments have so far not been allowed for by Anand and Reschke’s functors (Anand and Reschke, 2010; Reschke and Anand, 2011). We added these new functors because they seem to us to be relevant to opinion inference, which we empirically confirmed for some of them (cf. Secti"
L16-1460,J11-2001,0,0.0282176,"r interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8. 2. Related Work Resources that contain information relevant to opinion inference are rather sparse, compared to the abundance of sentiment lexica for English (Wilson et al., 2005; Taboada et al., 2011) and German (Klenner et al., 2009; Remus et al., 2010; Waltinger, 2010). In fact, even for English there seems to exist only one resource for this task, namely +/-EffectWordNet (Choi et al., 2014). This resource is structured on the sense level, building on WordNet (Miller et al., 1990) as its underlying structure. The effect information in +/-EffectWordNet is intended to feed into a rule-based inference machinery along the lines of the ideas laid out in Wiebe and Deng (2014), which computes implicit opinions from the presence of effect information and explicit sentiment within a sentence, sim"
L16-1460,waltinger-2010-germanpolarityclues,0,0.01557,"this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8. 2. Related Work Resources that contain information relevant to opinion inference are rather sparse, compared to the abundance of sentiment lexica for English (Wilson et al., 2005; Taboada et al., 2011) and German (Klenner et al., 2009; Remus et al., 2010; Waltinger, 2010). In fact, even for English there seems to exist only one resource for this task, namely +/-EffectWordNet (Choi et al., 2014). This resource is structured on the sense level, building on WordNet (Miller et al., 1990) as its underlying structure. The effect information in +/-EffectWordNet is intended to feed into a rule-based inference machinery along the lines of the ideas laid out in Wiebe and Deng (2014), which computes implicit opinions from the presence of effect information and explicit sentiment within a sentence, similar to the reasoning used in our discussion of (1) above. Note that 28"
L16-1460,H05-1044,0,0.0107287,"unctor types and their interrelationships in Section 5. We assess the reliability and validity of this extended annotation scheme via an interannotator agreement experiment in Section 6. We next report on a crowdsourcing experiment that we performed to assess the utility of some of the well-known and some of our new functors for opinion inference (cf. Section 7). We conclude the paper and discuss directions for future work in Section 8. 2. Related Work Resources that contain information relevant to opinion inference are rather sparse, compared to the abundance of sentiment lexica for English (Wilson et al., 2005; Taboada et al., 2011) and German (Klenner et al., 2009; Remus et al., 2010; Waltinger, 2010). In fact, even for English there seems to exist only one resource for this task, namely +/-EffectWordNet (Choi et al., 2014). This resource is structured on the sense level, building on WordNet (Miller et al., 1990) as its underlying structure. The effect information in +/-EffectWordNet is intended to feed into a rule-based inference machinery along the lines of the ideas laid out in Wiebe and Deng (2014), which computes implicit opinions from the presence of effect information and explicit sentiment"
L16-1460,W14-2618,0,\N,Missing
L18-1097,D09-1020,0,0.031681,"analysis (Flekova and Gurevych, 2016). 3.3. Polarity Information We assume that many ambiguous shifters convey a negation if they co-occur with polar expressions. This is illustrated in Table 4. Therefore, we count the number of polar expressions in a sentence to be classified. We identify such expressions with the help of the Subjectivity Lexicon (Wilson et al., 2005). 3.4. Focused Features Our task can be considered as a word-sense disambiguation (WSD) task. Therefore, we should also consider a feature set established in previous work on WSD. Table 5 lists those features mainly inspired by Akkaya et al. (2009). 2 Since we are not aware of any robust open-domain wordsense disambiguation software, we always consider the union of all synsets associated with a particular lemma. Feature bag of words embeddings bag of words + embed. unknown verbs Acc F1 63.8 59.6 64.4 60.8 64.3 60.7 known verbs Acc F1 70.7 67.9 70.8 68.1 70.9 68.2 Table 6: Bag of words vs. word embeddings. They have in common that they all only consider a very local context of the mention of the verbal shifter (i.e. typically its dependents). Some of these features incorporate syntactic information. We use the Stanford Parser (Chen and M"
L18-1097,J92-4003,0,0.572721,"se unknown words and words observed in the training data. Table 3 illustrates for the ambiguous verb spoil that the objects for the sense negation and no negation are of a specific semantic type. For the sense negation, the objects represent some form of activity, while for the sense no negation, they typically represent some human being (or at least some animate entity). These observed selectional preferences are an indication that some form of generalization of the context words might help for this task. Brown Clustering. A popular data-driven word generalization method is Brown Clustering (Brown et al., 1992). This is an unsupervised clustering method in which words with the similar distributional contexts are automatically assigned the same clusters. Clusters therefore represent a set Sense Contexts Type negation spoil a(n) {effort|idea|fun} activity no negation spoil one’s {partner|girlfriend|spouse} human Table 3: Illustration of word generalization. 1 https://github.com/miwieg/lrec2018 609 [Negation Sense [clear someone of murder− ]+ [spoil someone’s efforts+ ]− [peace+ crumples]− [cost someone their inner peace+ ]− No Negation Sense sky cleared spoil one’s spouse crumple a shirt cost some amo"
L18-1097,D14-1082,0,0.032304,"al. (2009). 2 Since we are not aware of any robust open-domain wordsense disambiguation software, we always consider the union of all synsets associated with a particular lemma. Feature bag of words embeddings bag of words + embed. unknown verbs Acc F1 63.8 59.6 64.4 60.8 64.3 60.7 known verbs Acc F1 70.7 67.9 70.8 68.1 70.9 68.2 Table 6: Bag of words vs. word embeddings. They have in common that they all only consider a very local context of the mention of the verbal shifter (i.e. typically its dependents). Some of these features incorporate syntactic information. We use the Stanford Parser (Chen and Manning, 2014) to obtain that information. 4. Experiments We evaluate our features in a 10-fold crossvalidation. The classifier, we consider is a Support Vector Machine. As a tool we use SVMlight (Joachims, 1999). We consider two different evaluation settings. On the one hand, we arrange the verbs in such a way that the test data contain contexts of verbs which have not been observed in the training data. On the other hand, we ensure that all test data contain contexts of verbs that have been observed in the training data. As a baseline, we also list the performance of a majority-class classifier. For the b"
L18-1097,P16-1191,0,0.0274857,"ypernyms and Supersenses. We use WordNet (Miller et al., 1990) as a resource-based method for word generalization. WordNet is the largest lexical ontology for the English language. It is organized in word senses called synsets.2 By considering hypernyms of a synset representing some set of lemmas as a feature, we enable similar words, i.e. synonyms or near-synonyms, to have a feature in common. While hypernyms are a fairly fine-grained form of generalization, we also consider supersenses, a set of coarse-grained classes, which have previously been found to be effective for sentiment analysis (Flekova and Gurevych, 2016). 3.3. Polarity Information We assume that many ambiguous shifters convey a negation if they co-occur with polar expressions. This is illustrated in Table 4. Therefore, we count the number of polar expressions in a sentence to be classified. We identify such expressions with the help of the Subjectivity Lexicon (Wilson et al., 2005). 3.4. Focused Features Our task can be considered as a word-sense disambiguation (WSD) task. Therefore, we should also consider a feature set established in previous work on WSD. Table 5 lists those features mainly inspired by Akkaya et al. (2009). 2 Since we are n"
L18-1097,N06-2015,0,0.180691,"Missing"
L18-1097,P06-1014,0,0.0833171,"Missing"
L18-1097,W04-2807,0,0.137902,"Missing"
L18-1097,I17-1063,1,0.839989,"her word classes, particularly the content words, such as verbs, nouns and adjectives, there also exist words expressing negation. For example, in (2) the verb failed has a similar function as the negation word not in (1). These negation content words, which are also called shifters, are often excluded from discussion since there does not (yet) exist a commonly accepted resource with these expressions. Even though the frequency of a single negation function word is much higher than that of a shifter, the overall number of shifters is significantly larger than those of negation function words. Schulder et al. (2017) identified 980 verbal shifters while the popular negation word lexicon proposed by Wilson et al. (2005) only includes 15 negation function words. Moreover, since content words are more ambiguous than function words, we also envisage shifters to be more ambiguous than negation function words. In this paper, we address the ambiguity of shifters. We select a set of 20 ambiguous verbal shifters and try to disambiguate them automatically. We focus on verbal shifters since it has been recently shown that a large amount of such verbs exist and they are important to polarity classification (Schulder"
L18-1097,D07-1107,0,0.0952475,"Missing"
L18-1097,P10-1040,0,0.0491317,"cost some amount of money Feature subcategorization frame of verbal shifter hypernym(s) of dependents of verbal shifter supersense(s) of dependents of verbal shifter is a polar expression among dependents of verbal shifter? is verbal shifter coordinated with another verbal shifter? words representing dependents of verbal shifter Table 4: Polar expressions. to indicate negation sense. Table 5: Focused features of words rather than individual ones. In our experiments, we induce 1000 clusters from the North American News Text Corpus. This is a standard configuration proven to yield good results (Turian et al., 2010). We induce the clusters using the SRILM-toolkit (Stolcke, 2002). Word Embeddings. A more recent alternative to Brown clustering is the usage of word embeddings. Word embeddings are (dense) vector representations of words that are automatically induced from corpora. They are devised as a more robust alternative to bag of words. Unlike a one-hot bag-of-words vector representation where different words (no matter how similar they are in meaning) are always orthogonal to each other, embeddings allow different words which are distributionally similar, such as partner and spouse, also to have simil"
L18-1097,W10-3111,1,0.790519,"egation takes place. We present a supervised learning approach to disambiguating verbal shifters. Our approach takes into consideration various features, particularly generalization features. Keywords: sentiment analysis, negation modeling, word-sense disambiguation 1. Introduction Negation is one of the most central linguistic phenomena. Therefore, negation modeling is essential to various common tasks in natural language processing, such as relation extraction (Sanchez-Graillet and Poesio, 2007), recognition of textual entailment (Harabagiu et al., 2006) and particularly sentiment analysis (Wiegand et al., 2010). In the latter task, a negation typically reverses the polarity of polar expressions. For example, in (1), the negated positive polar expression pass conveys negative polarity. (1) Peter did [not [pass]+ ]− the exam. (2) Peter [failed to [pass]+ ]− the exam. So far, most research in negation modeling for sentiment analysis has focused on negation (function) words, such as the particle not or the adverbs no or never. However, among other word classes, particularly the content words, such as verbs, nouns and adjectives, there also exist words expressing negation. For example, in (2) the verb fa"
L18-1097,H05-1044,0,0.163968,"rds expressing negation. For example, in (2) the verb failed has a similar function as the negation word not in (1). These negation content words, which are also called shifters, are often excluded from discussion since there does not (yet) exist a commonly accepted resource with these expressions. Even though the frequency of a single negation function word is much higher than that of a shifter, the overall number of shifters is significantly larger than those of negation function words. Schulder et al. (2017) identified 980 verbal shifters while the popular negation word lexicon proposed by Wilson et al. (2005) only includes 15 negation function words. Moreover, since content words are more ambiguous than function words, we also envisage shifters to be more ambiguous than negation function words. In this paper, we address the ambiguity of shifters. We select a set of 20 ambiguous verbal shifters and try to disambiguate them automatically. We focus on verbal shifters since it has been recently shown that a large amount of such verbs exist and they are important to polarity classification (Schulder et al., 2017). Examples for ambiguous verbal shifters are clear, spoil, cloud or slump which in (3), (5)"
L18-1222,W16-0410,0,0.106925,"erbs, together with nouns, are the most important minimal semantic units in text and thus are prime candidates for being tackled first. Verbs are usually the main syntactic predicates of clauses and sentences and thus verbal shifters can be expected to project far-reaching 2 Note that the example also illustrates how distinguishing between items that induce a switch between polarities and others that affect intensity without changing overall polarity is an idealization. Simple syntactic negation of a polar adjective may influence intensity as well as polarity (e.g. not terrible 6= excellent) (Kiritchenko and Mohammad, 2016). scopes. Most nominal shifters (e.g. failure, loss), on the other hand, have morphologically related verbs (e.g. fail, lose) and we expect that this connection can be exploited to spread shifter classification from verbs to nouns in the future. Related to this, the grammar of verbs, for instance with respect to the diversity of scope types, is more complex than that of nouns and so we expect it to be easier to project from verbs to nouns rather than in the opposite direction. 2.3. Related Work Existing lexicons and corpora that cover polarity shifting focus almost exclusively on negation word"
L18-1222,morante-2010-descriptive,0,0.349171,"Missing"
L18-1222,pak-paroubek-2010-twitter,0,0.0150347,"this work focusses on verbs, due to their importance as minimal semantic units, far-reaching scopes and potential basis for nominal shifter lexicons (see §2.2.). Knowledge of polarity shifting is important for a variety of tasks, especially sentiment analysis (Wiegand et al., 2010; Liu, 2012; Wilson et al., 2005), as well as relation extraction (Sanchez-Graillet and Poesio, 2007) and textual entailment recognition (Harabagiu et al., 2006). The majority of research into polarity shifting for sentiment analysis has focussed on negation words (Wiegand et al., 2010; Schouten and Frasincar, 2016; Pak and Paroubek, 2010). Negation words (e.g. not, no, never) are mostly function words, of which only a small number exists, so exhaustive coverage is comparatively simple. Content word classes, such as verbs, are considerably more difficult to cover comprehensively due to their sheer number. For example, WordNet (Miller et al., 1990) contains over 10k verbal lemmas. Most verbs are also far less frequent than 2. Background In this section we will provide a formal definition of polarity shifters (§2.1.), motivate our focus on verbal shifters (§2.2.) and discuss related work (§2.3.). 2.1. Polarity Shifters The notion"
L18-1222,W15-2910,1,0.700122,"rsed himdobj [for his + expenses]− pobj ] .” as pobj for. Clausal complement(comp): The verbal shifter affects a clausal complement, such as infinitive clauses like“He − [failed [to pass the exam]+ comp ] .” or gerunds like + “She [stopped [using drugs]− comp ] .” The given scopes assume that verb phrases are in their active form. In passive phrases, subject and object roles are inverted. To avoid this issue, sentence structure normalization should be performed before computing shifter scope. Synsets in WordNet only capture the semantic similarity of words, but almost no syntactic properties (Ruppenhofer and Brandes, 2015). The shifter scope of a verb depends on its syntactic arguments, which can differ between verbs of the same synset. For example, discard and dispose share the sense “throw or cast away”, but while discard shifts its direct object (10), dispose requires a prepositional object (11). For this reason we annotate lemma-synset pairs individually, instead of assigning scope labels to an entire synset. (10) He [discarded [the evidence]+ ]− . dobj (11) He [disposed [of the evidence]+ ]− . pobj We also consider cases where a verbal shifter has more than one potential scope for the same lemma-sense pair"
L18-1222,S16-1084,0,0.0756036,"excellent.”), something being somewhat good bounds its positiveness and opens up more negative meanings (“The performance was somewhat good, but overall rather disappointing”). Considering these properties of scales, one can see shifting at work even in the case of downtoning. 2.2. Verbal Shifters While the inclusion of shifting and scalar semantics in semantic representations is not limited to lexical items of particular parts-of-speech – we also find shifter adjectives (e.g. devoid) and adverbs (e.g. barely) – we limit our work to verbal shifters for several reasons. As shown by the work of Schneider et al. (2016), verbs, together with nouns, are the most important minimal semantic units in text and thus are prime candidates for being tackled first. Verbs are usually the main syntactic predicates of clauses and sentences and thus verbal shifters can be expected to project far-reaching 2 Note that the example also illustrates how distinguishing between items that induce a switch between polarities and others that affect intensity without changing overall polarity is an idealization. Simple syntactic negation of a polar adjective may influence intensity as well as polarity (e.g. not terrible 6= excellent"
L18-1222,I17-1063,1,0.779913,"k Existing lexicons and corpora that cover polarity shifting focus almost exclusively on negation words. The most complex negation lexicon for sentiment analysis (Wilson et al., 2005) includes a mere 12 verbal shifters. In contrast, our resource covers over 1200 verbal shifter lemmas. Corpora used as training data for negation processing, such as the Sentiment Treebank (Socher et al., 2013) or the BioScope corpus (Szarvas et al., 2008), are fairly small datasets, so only the most frequent negation words appear. The BioScope corpus, for example, contains only 6 verbal shifters (Morante, 2010). Schulder et al. (2017) show that state-of-the-art systems trained on such data do not reliably detect polarity shifting and should profit from explicit knowledge of verbal shifters. The only work to date that covers a larger number of verbal shifters is Schulder et al. (2017), who annotate a sample of the English verbs found in WordNet for whether they exhibit polarity shifting. They start by manually annotating an initial 2000 verbs. These verbs are used to train an SVM classifier using linguistic features and common language resources. The classifier is then run on the remaining WordNet verbs to bootstrap a list"
L18-1222,D13-1170,0,0.0168397,"Missing"
L18-1222,W08-0606,0,0.0586375,"Missing"
L18-1222,W10-3111,1,0.881473,"i) Annotations for shifter scope, indicating which parts of a sentence are affected by the shifting. The entire dataset is publicly available.1 (4) The new treatment has [alleviatedshifter her [pain]− ]+ . Polarity shifting is also caused by other content word classes, such as nouns (e.g. downfall) and adjectives (e.g. devoid). However, this work focusses on verbs, due to their importance as minimal semantic units, far-reaching scopes and potential basis for nominal shifter lexicons (see §2.2.). Knowledge of polarity shifting is important for a variety of tasks, especially sentiment analysis (Wiegand et al., 2010; Liu, 2012; Wilson et al., 2005), as well as relation extraction (Sanchez-Graillet and Poesio, 2007) and textual entailment recognition (Harabagiu et al., 2006). The majority of research into polarity shifting for sentiment analysis has focussed on negation words (Wiegand et al., 2010; Schouten and Frasincar, 2016; Pak and Paroubek, 2010). Negation words (e.g. not, no, never) are mostly function words, of which only a small number exists, so exhaustive coverage is comparatively simple. Content word classes, such as verbs, are considerably more difficult to cover comprehensively due to their s"
L18-1222,H05-1044,0,0.593442,"indicating which parts of a sentence are affected by the shifting. The entire dataset is publicly available.1 (4) The new treatment has [alleviatedshifter her [pain]− ]+ . Polarity shifting is also caused by other content word classes, such as nouns (e.g. downfall) and adjectives (e.g. devoid). However, this work focusses on verbs, due to their importance as minimal semantic units, far-reaching scopes and potential basis for nominal shifter lexicons (see §2.2.). Knowledge of polarity shifting is important for a variety of tasks, especially sentiment analysis (Wiegand et al., 2010; Liu, 2012; Wilson et al., 2005), as well as relation extraction (Sanchez-Graillet and Poesio, 2007) and textual entailment recognition (Harabagiu et al., 2006). The majority of research into polarity shifting for sentiment analysis has focussed on negation words (Wiegand et al., 2010; Schouten and Frasincar, 2016; Pak and Paroubek, 2010). Negation words (e.g. not, no, never) are mostly function words, of which only a small number exists, so exhaustive coverage is comparatively simple. Content word classes, such as verbs, are considerably more difficult to cover comprehensively due to their sheer number. For example, WordNet"
L18-1613,W15-5703,0,0.270377,"Zugriff Morphological splitters for German such as Gertwol (Haapalainen and Majorin, 1995), MORPH (Hanrieder, 1996), SMOR (Schmid et al., 2004), or TAGH (Geyken and Hanneforth, 2006) generate many ambiguous analyses. Usually, this problem is approached by filtering procedures on the output analyses. For the ranking of the different morphological analyses, the geometric mean is a common score (Cap, 2014; Koehn and Knight, 2003; Steiner and Ruppenhofer, 2015). Another method is pattern matching with tokens (Henrich and Hinrichs, 2011) or comparisons of lemmas (Weller-Di Marco, 2017) or strings (Daiber et al., 2015) with corpus data. Ziering and van der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner and Hanneforth (2013) apply a probabilistic context free grammar for parsing adjectives. Ma et al. (2016) apply Conditional Random Fields modeling for letter sequences. More recent approaches exploit semantic information for the ranking (Riedl and Biemann, 2016; Ziering et al., 2016). Besides W¨urzner and Hanneforth (2013), none of the above-mentioned authors tackled the challenge of generating deep-level analyses and with the exception of Henrich and Hinrichs (2011) who a"
L18-1613,W97-0802,0,0.818734,"der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner and Hanneforth (2013) apply a probabilistic context free grammar for parsing adjectives. Ma et al. (2016) apply Conditional Random Fields modeling for letter sequences. More recent approaches exploit semantic information for the ranking (Riedl and Biemann, 2016; Ziering et al., 2016). Besides W¨urzner and Hanneforth (2013), none of the above-mentioned authors tackled the challenge of generating deep-level analyses and with the exception of Henrich and Hinrichs (2011) who are using compounds from GermaNet (Hamp and Feldweg, 1997), all of them rely merely on corpus data as input for the heuristics and scores. However, carefully produced lexical data with morphological information would be a valuable asset for many applications in natural language processing. High quality morphologically deep-level analyses could especially be used as 1. input for statistical approaches for full morphologically parsing of German words zugreifen ‘to grab/grasp’ zu ‘at’ greifen ‘to grip’ 2. base of frequency counts for the testing of statistical hypotheses about morphological tendencies and laws Zug ‘train’ 3. gold standards and test kits"
L18-1613,L16-1173,0,0.0450404,"Missing"
L18-1613,R11-1058,0,0.521703,"rms which could be erroneously recognized by a morphological analysis program. Zugriff ] Zugriff Morphological splitters for German such as Gertwol (Haapalainen and Majorin, 1995), MORPH (Hanrieder, 1996), SMOR (Schmid et al., 2004), or TAGH (Geyken and Hanneforth, 2006) generate many ambiguous analyses. Usually, this problem is approached by filtering procedures on the output analyses. For the ranking of the different morphological analyses, the geometric mean is a common score (Cap, 2014; Koehn and Knight, 2003; Steiner and Ruppenhofer, 2015). Another method is pattern matching with tokens (Henrich and Hinrichs, 2011) or comparisons of lemmas (Weller-Di Marco, 2017) or strings (Daiber et al., 2015) with corpus data. Ziering and van der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner and Hanneforth (2013) apply a probabilistic context free grammar for parsing adjectives. Ma et al. (2016) apply Conditional Random Fields modeling for letter sequences. More recent approaches exploit semantic information for the ranking (Riedl and Biemann, 2016; Ziering et al., 2016). Besides W¨urzner and Hanneforth (2013), none of the above-mentioned authors tackled the challenge of generati"
L18-1613,E03-1076,0,0.500527,"is the product of a conversion process from zugreifen ‘to grab/to grasp’ and not a compound of other forms which could be erroneously recognized by a morphological analysis program. Zugriff ] Zugriff Morphological splitters for German such as Gertwol (Haapalainen and Majorin, 1995), MORPH (Hanrieder, 1996), SMOR (Schmid et al., 2004), or TAGH (Geyken and Hanneforth, 2006) generate many ambiguous analyses. Usually, this problem is approached by filtering procedures on the output analyses. For the ranking of the different morphological analyses, the geometric mean is a common score (Cap, 2014; Koehn and Knight, 2003; Steiner and Ruppenhofer, 2015). Another method is pattern matching with tokens (Henrich and Hinrichs, 2011) or comparisons of lemmas (Weller-Di Marco, 2017) or strings (Daiber et al., 2015) with corpus data. Ziering and van der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner and Hanneforth (2013) apply a probabilistic context free grammar for parsing adjectives. Ma et al. (2016) apply Conditional Random Fields modeling for letter sequences. More recent approaches exploit semantic information for the ranking (Riedl and Biemann, 2016; Ziering et al., 2016)."
L18-1613,kupietz-etal-2010-german,0,0.66636,"Missing"
L18-1613,W16-2012,0,0.169611,"roblem is approached by filtering procedures on the output analyses. For the ranking of the different morphological analyses, the geometric mean is a common score (Cap, 2014; Koehn and Knight, 2003; Steiner and Ruppenhofer, 2015). Another method is pattern matching with tokens (Henrich and Hinrichs, 2011) or comparisons of lemmas (Weller-Di Marco, 2017) or strings (Daiber et al., 2015) with corpus data. Ziering and van der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner and Hanneforth (2013) apply a probabilistic context free grammar for parsing adjectives. Ma et al. (2016) apply Conditional Random Fields modeling for letter sequences. More recent approaches exploit semantic information for the ranking (Riedl and Biemann, 2016; Ziering et al., 2016). Besides W¨urzner and Hanneforth (2013), none of the above-mentioned authors tackled the challenge of generating deep-level analyses and with the exception of Henrich and Hinrichs (2011) who are using compounds from GermaNet (Hamp and Feldweg, 1997), all of them rely merely on corpus data as input for the heuristics and scores. However, carefully produced lexical data with morphological information would be a valuabl"
L18-1613,N16-1075,0,0.420989,"common score (Cap, 2014; Koehn and Knight, 2003; Steiner and Ruppenhofer, 2015). Another method is pattern matching with tokens (Henrich and Hinrichs, 2011) or comparisons of lemmas (Weller-Di Marco, 2017) or strings (Daiber et al., 2015) with corpus data. Ziering and van der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner and Hanneforth (2013) apply a probabilistic context free grammar for parsing adjectives. Ma et al. (2016) apply Conditional Random Fields modeling for letter sequences. More recent approaches exploit semantic information for the ranking (Riedl and Biemann, 2016; Ziering et al., 2016). Besides W¨urzner and Hanneforth (2013), none of the above-mentioned authors tackled the challenge of generating deep-level analyses and with the exception of Henrich and Hinrichs (2011) who are using compounds from GermaNet (Hamp and Feldweg, 1997), all of them rely merely on corpus data as input for the heuristics and scores. However, carefully produced lexical data with morphological information would be a valuable asset for many applications in natural language processing. High quality morphologically deep-level analyses could especially be used as 1. input for stat"
L18-1613,schmid-etal-2004-smor,0,0.69181,"tructure. Hauptbahnhof Haupt ‘main/central’ Bahnhof ‘station’ ] Hauptbahnhof Hauptbahn ‘*main rail’ Hof ‘yard’ Figure 1: Ambiguous analysis of Hauptbahnhof ‘central station’ Other word forms have ambiguous boundaries of morphs as in Figure 2 where the word form Zugriff ‘grasp/access’ is the product of a conversion process from zugreifen ‘to grab/to grasp’ and not a compound of other forms which could be erroneously recognized by a morphological analysis program. Zugriff ] Zugriff Morphological splitters for German such as Gertwol (Haapalainen and Majorin, 1995), MORPH (Hanrieder, 1996), SMOR (Schmid et al., 2004), or TAGH (Geyken and Hanneforth, 2006) generate many ambiguous analyses. Usually, this problem is approached by filtering procedures on the output analyses. For the ranking of the different morphological analyses, the geometric mean is a common score (Cap, 2014; Koehn and Knight, 2003; Steiner and Ruppenhofer, 2015). Another method is pattern matching with tokens (Henrich and Hinrichs, 2011) or comparisons of lemmas (Weller-Di Marco, 2017) or strings (Daiber et al., 2015) with corpus data. Ziering and van der Plas (2016) use normalization combined with ranking by the geometric mean. W¨urzner"
L18-1613,L16-1176,1,0.765726,"rt of the CELEX database (Baayen et al., 1995) comprises word tree information for a lexicon containing words of all parts of speech and is therefore an important source for deep-level morphological analyses of German, which are not available elsewhere. The linguistic information is combined with frequency information based on corpora (Burnage, 1995) which makes it useful for automated morphological analysis of unknown words. The original drawbacks of the German part of the database were an outdated format and use of obsolete orthographical conventions. However, these problems were tackled by Steiner (2016), so that the refurbished database yields a foundation for further exploitation. The lexicon with its 51,728 entries is relatively small but it covers a core vocabulary, similar to the small dictionary Der kleine Wahrig (Wahrig-Burfeind and Bertelsmann, 2007). Shafaei et al. (2017) use the German data of CELEX for inferring derivational families which are more precise than DErivBase. The produced database DErivCELEX is drawn from the original CELEX version with its old orthographical standard4 and therefore contains some inconsistencies and mistakes from string transformations such as (4). As"
L18-1613,W17-7619,1,0.529853,"is: Abschreckung|Ara (1) This is connected with some typical features of the much used morphological tool SMOR (Schmid et al., 2004). However, these interfixes are frequently marking boundaries between morphological constituents of higher levels. This is a reason why Steiner and Ruppenhofer (2015) modified the output of this tool to splits as (2). ¨ Abschreckung|s-|Ara (2) Henrich and Hinrichs (2011) augmented the GermaNet database with information on compound splits. This is restricted to nouns and does not provide interfixes or deeplevel structures. However, in connection with this project, Steiner (2017)2 derives deep-level structures with information on interfixes and grammatical properties from the GermaNet compounds which can be combined with analyses from CELEX. DErivBase3 (Zeller et al., 2013) comprises derivational families (word nests), however, the unsupervised generation of this derivational lexicon is based on heuristics of rules and string transformations. These rules do not always produce word families whose members are actually morphologically connected and the process of generation does not comply with linguistic evidence. However, the sets are produced as data for semantic (sim"
L18-1613,W17-1722,0,0.749436,"Missing"
L18-1613,W13-1807,0,0.205696,"Missing"
L18-1613,L16-1208,0,0.0549276,"Missing"
L18-1613,P13-1118,0,0.0247205,"Missing"
L18-1613,C14-1163,0,0.0536843,"Missing"
L18-1613,N16-1078,0,0.481847,"Missing"
L18-1613,W16-1807,0,0.49991,"Missing"
N13-1059,P05-1045,0,0.00752314,"ive) (10) [Evil witches are stereotypically dressed in black] and [good fairies in white]. We also experimented with other related weaklysupervised extraction methods, such as mutual information of two adjectives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards"
N13-1059,P97-1023,0,0.533226,"ves from the nonsubjective adjectives. In this work, we are interested in an out-ofcontext assessment of adjectives and therefore evaluate them with the help of sentiment lexicons. We examine the property of being a predicative adjective as an extraction criterion. Predicative adjectives are adjectives that do not modify the head of a noun (3) (4) (5) (6) Her idea was brilliant. This is a financial problem. She came up with a brilliant idea. ?The problem is financial. 2 Related Work The extraction of subjective adjectives has already attracted some considerable attention in previous research. Hatzivassiloglou and McKeown (1997) extract polar adjectives by a weakly supervised method in which subjective adjectives are found by searching for adjectives that are conjuncts of a pre-defined set of polar seed adjectives. Wiebe (2000) induces subjective adjectives with the help of distributional similarity. Hatzivassiloglou and Wiebe (2000) examine the properties of dynamic, gradable and polar adjectives as a means to detect subjectivity. Vegnaduzzo (2004) presents another bootstrapping method of extracting subjective adjectives with the help of head nouns of the subjective candidates and distributional similarity. Baroni a"
N13-1059,C00-1044,0,0.53182,"ot modify the head of a noun (3) (4) (5) (6) Her idea was brilliant. This is a financial problem. She came up with a brilliant idea. ?The problem is financial. 2 Related Work The extraction of subjective adjectives has already attracted some considerable attention in previous research. Hatzivassiloglou and McKeown (1997) extract polar adjectives by a weakly supervised method in which subjective adjectives are found by searching for adjectives that are conjuncts of a pre-defined set of polar seed adjectives. Wiebe (2000) induces subjective adjectives with the help of distributional similarity. Hatzivassiloglou and Wiebe (2000) examine the properties of dynamic, gradable and polar adjectives as a means to detect subjectivity. Vegnaduzzo (2004) presents another bootstrapping method of extracting subjective adjectives with the help of head nouns of the subjective candidates and distributional similarity. Baroni and Vegnaduzzo 534 Proceedings of NAACL-HLT 2013, pages 534–539, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics (2004) employ Web-based Mutual information for this task and largely outperform the results produced by Vegnaduzzo (2004). 3 Method In the following, we present dif"
N13-1059,J11-2001,0,0.0323326,"ctives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards a particular gold standard (which would challenge the general validity of the proposed method). ALL PRD other new last many first such next political federal own several few good∗ former same economic public"
N13-1059,J04-3002,0,0.0545981,"xtract subjective adjectives. We do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives, i.e. another highly subjective subset of adjectives that can be extracted in an unsupervised fashion. In order to prove the robustness of this extraction method, we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons (as a gold standard). 1 Introduction Since the early work on sentiment analysis, it has been established that the part of speech with the highest proportion of subjective words are adjectives (Wiebe et al., 2004) (see Sentence (1)). However, not all adjectives are subjective (2). (1) A grumpy guest made some impolite remarks to the insecure and inexperienced waitress. (2) The old man wearing a yellow pullover sat on a plastic chair. This justifies the exploration of criteria to automatically separate the subjective adjectives from the nonsubjective adjectives. In this work, we are interested in an out-ofcontext assessment of adjectives and therefore evaluate them with the help of sentiment lexicons. We examine the property of being a predicative adjective as an extraction criterion. Predicative adject"
N13-1059,H05-1044,0,0.0764966,"such as mutual information of two adjectives at the sentence level (or even smaller window sizes). However, using conjunctions largely outperformed these alternative approaches so we only pursue conjunctions here. 4 Experiments As a large unlabeled (training) corpus, we chose the North American News Text Corpus (LDC95T21) comprising approximately 350 million words of news text. For syntactic analysis we use the Stanford Parser (Finkel et al., 2005). In order to decide whether an extracted adjective is subjective or not, we employ two sentiment lexicons, namely the Subjectivity Lexicon (SUB) (Wilson et al., 2005) and SO-CAL (SOC) (Taboada et al., 2011). According to the recent in-depth evaluation presented in Taboada et al. (2011), these two sentiment lexicons are the most effective resources for English sentiment analysis. By taking into account two different lexicons, which have also been built independently of each other, we want to provide evidence that our proposed criterion to extract subjective adjectives is not sensitive towards a particular gold standard (which would challenge the general validity of the proposed method). ALL PRD other new last many first such next political federal own sever"
N16-1092,P98-1013,0,0.0507685,"005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment analysis related to ours, i.e. opinion holder/target extraction (Bethard et al., 2004; Kim and Hovy, 2006). It includes a large set of more than 1, 200 semantic frames that comprise words with similar semantic behaviour. As a feature we use the framemembership of the opinion words, assuming that different frames are associated with different sentiment views. We use FrameNet version 1.5. 4.3 Subcategorization Frames (SUB) Subcategorization frames could also be predictive. For example, actor views demand the presence of"
N16-1092,D14-1125,0,0.125435,"Missing"
N16-1092,C94-1042,0,0.0450304,"general for our task. Instead we use the lexicographer files of all nouns and verbs occurring in the glosses of those adjectives. Type Sentiment Neutral Affixes Used -able, dis-, mis-, over-, under-, -(i)sm adj → noun: -cy, -ity, -ness; adj/noun → verb: -ize; verb → adj: -ed, -ing; verb → noun: -ion, -ing Table 2: Affixes used as features. explicit entity that utters some opinion, i.e. the opinion holder. For a speaker view, this entity remains implicit. This should be reflected in the argument valence of the respective opinion words. We employ the subcategorization frames encoded in COMLEX (Grishman et al., 1994) for verbs and adjectives, and NOMLEX (Macleod et al., 1998) for nouns. 4.4 Morphological Information (MORPH) (11) The UN was blamed for misinterpretingverb climate data. (12) The UN was blamed for the misinterpretationnoun of climate data. 4.6 As morphological information, we consider derivational affixes. Table 2 lists our choice of prefixes and suffixes. We only included affixes that occurred at least 10 times in our dataset. We distinguish between sentiment and neutral affixes. The sentiment affixes are affixes which, due to their meaning, suggest a sentiment view. For example, mis- as in"
N16-1092,N09-1002,0,0.0138176,"words conveying the same sentiment view also contain similar glosses. Glosses are a special type of feature. It is basically a bag-of-words feature set, i.e. a low-level feature set, which is known to be sparse yet effective when sufficient training data are used. All the other features presented in this paper are high-level features, i.e. more frequently occurring features already being effective if only few labeled data are used. Glosses are one of the most frequently used features for lexicon induction tasks in sentiment analysis (Esuli and Sebastiani, 2005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been"
N16-1092,N13-1111,0,0.0169224,"set of constants C. The probability distribution that is estimated is a log-linear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorpor"
N16-1092,J13-3002,0,0.0805017,"morphologically related instances. Finally, we also examine the relationship between prior lexical information (i.e. our approach) and contextual annotation in the MPQA corpus. 2 Related Work The annotation scheme of the MPQA corpus (Wiebe et al., 2005) was the first work to address the distinction between different sentiment views. The two sentiment views are referred to as direct subjectivity (=actor view) and expressive subjectivity (=speaker view). In subsequent research, some approaches have been proposed to distinguish these two categories in the MPQA corpus. The most extensive work is Johansson and Moschitti (2013). Since MPQA provides annotation regarding sentiment in context, sentiment views are exclusively considered in contextual classification. The fact that it is the opinion words that convey those views, as we do in this paper, is not addressed. Unlike in this paper, the focus of Johansson and Moschitti (2013) is also on optimizing a machine-learning classifier, in particular to model the interaction between different subjective phrases within the same sentence. 779 Part of Speech adjective noun verb Actor View Freq Perc 223 8.9 487 29.1 618 52.6 Speaker View Freq Perc 2279 91.1 1189 70.9 557 47."
N16-1092,P14-1145,0,0.0931427,"o contain similar glosses. Glosses are a special type of feature. It is basically a bag-of-words feature set, i.e. a low-level feature set, which is known to be sparse yet effective when sufficient training data are used. All the other features presented in this paper are high-level features, i.e. more frequently occurring features already being effective if only few labeled data are used. Glosses are one of the most frequently used features for lexicon induction tasks in sentiment analysis (Esuli and Sebastiani, 2005; Andreevskaia and Bergler, 2006; Gyamfi et al., 2009; Choi and Wiebe, 2014; Kang et al., 2014). We will consider them as a baseline, showing that our proposed high-level features are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment an"
N16-1092,D15-1080,0,0.0144329,"stribution that is estimated is a log-linear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorporated in MLNs. based on the two most effe"
N16-1092,W06-0301,0,0.181649,"are more suitable for our task. 4.1.2 Lexicographer Files (LEX) Lexicographer files organize the synset inventory of WordNet into a coarse-grained set of semantic categories. In total, there are 45 categories for the three parts of speech we consider.2 The advantage of such a coarse-grained inventory is that it should require only few labeled training data in supervised classification. 4.2 FrameNet (FN) FrameNet (Baker et al., 1998) is a semantic resource that has been found useful for subtasks of sentiment analysis related to ours, i.e. opinion holder/target extraction (Bethard et al., 2004; Kim and Hovy, 2006). It includes a large set of more than 1, 200 semantic frames that comprise words with similar semantic behaviour. As a feature we use the framemembership of the opinion words, assuming that different frames are associated with different sentiment views. We use FrameNet version 1.5. 4.3 Subcategorization Frames (SUB) Subcategorization frames could also be predictive. For example, actor views demand the presence of an 2 For adjectives there exist only two categories. These are too general for our task. Instead we use the lexicographer files of all nouns and verbs occurring in the glosses of tho"
N16-1092,P98-2127,0,0.0617041,"∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorporated in MLNs. based on the two most effective types of word similarities from Wiegand and Ruppenhofer (2015). The first word similarity measures the cosine of word vectors representing opinion words produced by Word2Vec-embeddings (Mikolov et al., 2013). The second word similarity is represented by the metric of Lin (1998), which exploits the rich set of dependency-relation labels in the context of distributional similarity.3 The third type of consistency considers morphological relatedness by which we understand two words deriving from two different parts of speech but belonging to the same lexical root and therefore carrying similar meaning (e.g. happiness.noun and happy.adj). We obtain that type of relatedness from WordNet (Miller et al., 1990). Table 3 lists our constraints. They state that if for two opinion words some similarity or morphological relatedness holds, then these words should convey the same s"
N16-1092,maks-vossen-2012-building,0,0.0180962,"eraction between different subjective phrases within the same sentence. 779 Part of Speech adjective noun verb Actor View Freq Perc 223 8.9 487 29.1 618 52.6 Speaker View Freq Perc 2279 91.1 1189 70.9 557 47.4 Table 1: Distribution of the different sentiment views. Some of the lexical resources we examine, i.e. WordNet (§4.1) and FrameNet (§4.2), have also been employed in Breck et al. (2007) who, like Johansson and Moschitti (2013), also deal with contextual (sentiment) classification. However, the authors do not examine in how far these individual resources separate speaker and actor views. Maks and Vossen (2012b) link sentiment views to opinion words as part of a lexicon model for sentiment analysis. Maks and Vossen (2012a) also examine a corpus-driven method to induce opinion words for the different sentiment views. The authors, however, conclude that their approach, which sees news articles as a source for actor views and news comments as a source for speaker views, is not sufficiently effective. The work most closely related to our research is Wiegand and Ruppenhofer (2015). Opinion words are categorized according to their sentiment view. Our work substantially goes beyond that previous research:"
N16-1092,N09-1018,0,0.0225131,"template for constructing a Markov network given a set of constants C. The probability distribution that is estimated is a log-linear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcin"
N16-1092,D08-1061,0,0.0667403,"Missing"
N16-1092,R11-1039,1,0.844283,"adjectives. For speaker views (PATT speaker), Wiegand and Ruppenhofer introduced reproach patterns, e.g. blamed for X as in (11). These patterns can also be applied to nouns (12) but not to adjectives. For the latter, we did not find any pattern. The patterns were applied to the North American News Text Corpus (LDC95T21). Context Patterns (PATT) Wiegand and Ruppenhofer (2015) proposed patterns for actor-view and speaker-view verbs. For actor views (PATT actor), they rely on prototypical opinion holders (protoOHs), i.e. common nouns, such as opponents or critics, that act like opinion holders (Wiegand and Klakow, 2011). If a verb often co-occurs with an opinion holder – Wiegand and Ruppenhofer (2015) take protoOHs as a proxy – then this is a good indicator of being an actor view 781 Polarity Information (POLAR) We also investigate in how far polarity information correlates with sentiment views. This information is obtained from the Subjectivity Lexicon (Wilson et al., 2005). Each opinion word is assigned a polarity type, i.e. positive, negative or neutral. 5 Markov Logic Networks and Global Constraints Markov Logic Networks (MLNs) are a supervised classifier combining first-order logic with probabilities. M"
N16-1092,K15-1022,1,0.0992461,"adjectives. We consider many high-level features requiring only few labeled training data. A detailed feature analysis produces linguistic insights into the nature of sentiment views. We also examine how far global constraints between different opinion words help to increase classification performance. Finally, we show that our (prior) word-level annotation correlates with contextual sentiment views. The distinction between those categories is relevant for related tasks in sentiment analysis, most importantly, opinion holder and target extraction. This has already been demonstrated for verbs (Wiegand and Ruppenhofer, 2015). For example, even though the noun Peter has the same grammatical relation to the opinion verb in (5) & (6), in the former sentence it is a holder but in the latter it is a target. Similar cases can be observed for opinion nouns (7) & (8) and opinion adjectives (9) & (10). Only the knowledge of sentiment views helps us to assign opinion roles correctly. 1 Introduction While there has been much research in sentiment analysis on the tasks of subjectivity detection and polarity classification, there has been less work on other types of categorizations that can be imposed upon subjective expressi"
N16-1092,H05-1044,0,0.345613,"gand and Ruppenhofer (2015) distinguish between two types of actor views, agent views and patient views. The former take their opinion holder as an agent and their target as a patient (typical verbs are criticize, love, believe), while the latter align their roles inversely (typical verbs are disappoint, please, interest). Since this distinction between actor views does not exist among nouns or adjectives, we consider one merged (actor-view) category for all three parts of speech in this paper. 3 Data We manually annotated all verbs, nouns and adjectives contained in the Subjectivity Lexicon (Wilson et al., 2005) for view type. The dataset comprises 2502 adjectives, 1676 nouns and 1175 verbs. Since our new dataset1 is an extension of the dataset from Wiegand and Ruppenhofer (2015), we adhere to the annotation process proposed in that paper. That is, the basis of the annotation were online dictionaries (e.g. Macmillan Dictionary) which provide both a word definition and example sentences. Each word is either labeled as primarily conveying an actor or a speaker view. (Our categorization is binary.) On a subset of 250 words for each part of speech, we computed an interannotation agreement (Cohen’s κ) of"
N16-1092,I11-1038,0,0.0281247,"x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x and Z is a normalization constant. As an implementation, we use thebeast (Riedel, 2008). We employ MLNs since they allow us (in addition to including ordinary features, i.e. §4.1-§4.6) to formulate constraints holding between individual instances. Such global constraints have been effectively exploited with MLNs in related tasks, such as semantic-role labeling (Meza-Ruiz and Riedel, 2009), anaphora resolution (Hou et al., 2013), question answering (Khot et al., 2015) and discoursebased sentiment analysis (Zirn et al., 2011). We formulate three such constraints. Two of them are Abbrev. w2v lin morph Constraint as Logic Formula ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧Word2Vec-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧DekangLin-Similar (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] ∀x[∀y[∀z[∀u[[Opin.Word(x)∧Opin.Word (y)∧MorphRelated (x, y)∧ViewOf (z, x)∧ViewOf (u, y)] → (z == u)]]]] Table 3: Global constraints enforcing sentiment view consistency as incorporated in MLNs. based on the two most effective types of word similarities from Wiegand and Ruppenho"
N16-1092,E06-1027,0,\N,Missing
N16-1092,C98-1013,0,\N,Missing
N16-1094,D09-1020,0,0.0444841,"Missing"
N16-1094,C08-1011,0,0.0363909,"Missing"
N16-1094,H05-1045,0,0.135799,"nd only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, there is no context between opinion role and opinion word. In the area of noun compound analysis, there are two predominant approaches. On the one hand, lexical resources, such as WordNet (Miller et al., 1990), are employed in order to assign semantic categories to head and modifier and infer fr"
N16-1094,E14-1040,0,0.0141671,"that encodes for each compound the category of its modifier. (4) (5) (6) (7) (8) (9) [user OH ] rating (i.e. user rates something) [consumer OH ] uncertainty (i.e. consumers are uncertain) [victim OT ] support (i.e. support for victims) [test OT ] anxiety (i.e. having anxiety towards test taking) spring upswing (i.e. economic upswing in spring) phone harassment (i.e. harassment inflicted via phone) Notice that we focus exclusively on opinion role extraction. We do not try to detect the polarity associated with the compound. Neither do we consider implicature-related information about effects (Deng and Wiebe, 2014), but only inherent sentiment. We study opinion role extraction on opinion compounds in German. German is known for its frequent 800 Proceedings of NAACL-HLT 2016, pages 800–810, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics compounds user rating; victim support; spring upswing immediate constituents user; victim; spring rating; support; upswing grammatical function modifier head Table 1: Internal structure of opinion compounds. use of noun compounds. In the STEPS-corpus, the benchmark dataset for German opinion role extraction (Ruppenhofer et al., 2"
N16-1094,D15-1018,0,0.028744,"Missing"
N16-1094,dima-etal-2014-tell,0,0.0304548,"uages. Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds. We also created a new gold standard for this task (see also §3). The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, the"
N16-1094,P07-1072,0,0.0294478,"Missing"
N16-1094,W97-0802,0,0.649858,"Missing"
N16-1094,N13-1111,0,0.0179909,"haring the same head.6 Given this selectional preference, we formulate a global head constraint (Table 7) that if two compounds have the same head, their modifiers should convey the same opinion role. In order to implement this constraint in a supervised classifier we employ Markov Logic Networks (MLNs), which combine first-order logic with probabilities. As a tool, we use thebeast (Riedel, 2008). MLNs have been effectively used in various related NLP tasks, such as discourse-based sentiment analysis (Zirn et al., 2011), semantic-role labeling (MezaRuiz and Riedel, 2009), anaphora resolution (Hou et al., 2013) or question answering (Khot et al., 2015). 6 On average, a head occurs in 5 different compounds on Dataset I, and in 4 different compounds on Dataset II. 805 88.86 Dataset II 91.36 Table 6: Role-purity of compounds with the same head. MLNs are a set of pairs (Fi , wi ) where Fi is a first-order logic formula and wi an associated realvalued weight. They build a template for constructing a Markov network given a set of constants C. The probability distribution that is estimated is a loglinear model P (X = x) = 1 exp Z k X i=1 ! wi ni (x) (1) where ni (x) is the number of groundings in Fi in x a"
N16-1094,D10-1101,0,0.0309836,"of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, there is no context between opinion role and opinion word. In the area of noun compound analysis, there are two predominant approaches. On the one hand, lexical resources, such as WordNet (Miller et al., 1990), are employed in order to assign semantic categories to head and modifier and infer from those labels the 801 Dataset I 2000 compounds 389 (unique) heads category of modifier role no role frequency 937 1063 proportion"
N16-1094,J13-3002,0,0.130096,"Missing"
N16-1094,I05-1082,0,0.0285967,"our approach can be replicated on other languages. Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds. We also created a new gold standard for this task (see also §3). The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to"
N16-1094,P06-2064,0,0.0349529,"): a) Beurteilungnoun [von Sch¨ulern objpvon ] (assessmentnoun [of students objpof ]) b) Lehrer beurteilenverb Sch¨ulerobj . (Teachers assessverb studentsobj .) Even though the disambiguation of deverbal noun compounds with the help of verb relations has been examined before (Lapata, 2002), it has not been exploited for an actual application, such as opinion role extraction. Neither has it been compared against plain paraphrases, which use the head noun of the compound directly (§5.1). Our use of verb semantics for compound analysis is also different from its predominant use in previous work (Kim and Baldwin, 2006; Nakov and Hearst, 2013) where noun compounds are considered whose parts represent arguments of an abstract verbal relation (e.g. malaria mosquito are arguments of relation ‘mosquito causes malaria’). Thus, the aim has been to predict verbs for those compounds that match those abstract relations (e.g. to cause). We are looking for different verbs, namely those that are the morphological basis for the head noun. For this verb detour, we produce a mapping from nouns (i.e. the heads of our opinion compounds) to verbs by combining distributional and string similarity. We extracted the verbs most"
N16-1094,D11-1060,0,0.0289257,"Missing"
N16-1094,W09-4635,0,0.474796,"ion We created a new dataset1 by retrieving opinion compounds from the deWaC-corpus (Baroni et al., 2009) comprising 1.7 billion words. (Word embeddings (§5.2 & §5.6) and word similarity graphs (§5.7 & §6.4) were also created from this corpus.) In German, noun compounds are typically realized as single tokens. In order to obtain a set of opinion compounds, we extracted all noun compounds from deWaC whose second morpheme is an opinion noun. Morphological analysis was carried out using morphisto (Zielinski and Simon, 2009).2 As opinion nouns, we used the nouns from the PolArt sentiment lexicon (Klenner et al., 2009). Unfortunately, this lexicon is lacking in neutral opinion nouns, such as Meinung (opinion) or Erwartung (expectation) which frequently occur in compounds, e.g. Expertenmeinung (expert opinion) or Kundenerwartungen (customer expectations). Therefore, we translated the 235 neutral opinion nouns from the (English) Subjectivity Lexicon (Wilson et al., 2005) into German. From the opinion compounds extracted from deWaC, we created two manually annotated datasets (Table 2). We use more than one dataset as we consider our task as a multi-stage task as shown in Figure 1. We believe that this is neces"
N16-1094,J02-3004,0,0.0609779,"elation subj and patients with the relation obj.) (12) paraphrases for Leserkommentar (reader comments): a) Kommentarnoun [von Lesern objpvon ] (commentnoun [of readers objpof ]). b) Lesersubj kommentierenverb ein Ereignis. (Readerssubj commentverb on an event.) (13) paraphrases for Sch¨ulerbeurteilung (student assessment): a) Beurteilungnoun [von Sch¨ulern objpvon ] (assessmentnoun [of students objpof ]) b) Lehrer beurteilenverb Sch¨ulerobj . (Teachers assessverb studentsobj .) Even though the disambiguation of deverbal noun compounds with the help of verb relations has been examined before (Lapata, 2002), it has not been exploited for an actual application, such as opinion role extraction. Neither has it been compared against plain paraphrases, which use the head noun of the compound directly (§5.1). Our use of verb semantics for compound analysis is also different from its predominant use in previous work (Kim and Baldwin, 2006; Nakov and Hearst, 2013) where noun compounds are considered whose parts represent arguments of an abstract verbal relation (e.g. malaria mosquito are arguments of relation ‘mosquito causes malaria’). Thus, the aim has been to predict verbs for those compounds that ma"
N16-1094,P98-2127,0,0.0250485,"Missing"
N16-1094,N09-1018,0,0.0422085,"Missing"
N16-1094,P09-1113,0,0.00929895,"fier mentions part-of-speech tags before/after head mentions dependency paths between head and modifier mentions proportion of opinion words in the sentences each training/test instance represents the set of all sentences in which head and modifier of a specific compound co-occur Table 12: Features for distant supervision (baseline) classifier. 6.4 Comparison against Baselines Table 13 compares the best result from our previous experiments against 3 baselines. The first is a majority classifier predicting the majority class. The second baseline is a classifier inspired by distant supervision (Mintz et al., 2009). As in our paraphrase features, this classifier considers the context in which modifier and head of a compound occur as separate constituents. The difference is, however, that we consider every such co-occurrence (within the same sentence) as a context that conveys the same relation as the one that is (implicitly) conveyed by the compound. Even though such an assumption is naive, it has been shown to produce quite reasonable performance in relation extraction (Mintz et al., 2009). The advantage of such an approach is that a generic relation extraction/opinion role extraction classifier can be"
N16-1094,W01-0511,0,0.116221,"Missing"
N16-1094,D08-1061,0,0.0906089,"Missing"
N16-1094,P10-1070,0,0.0590467,"plicated on other languages. Apart from examining traditional features from noun compound analysis, in this paper, we also introduce novel features specially designed for the analysis of opinion compounds. We also created a new gold standard for this task (see also §3). The STEPS-corpus, as such, is fairly small and only contains about 200 unique compounds. We considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for op"
N16-1094,N10-1121,1,0.739881,"considered this amount insufficient for producing a gold standard. Also, none of the existing datasets on noun compounds (Lauer, 1995; Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2009; Kim and Baldwin, 2005; Tratz and Hovy, 2010; Dima et al., 2014) contain any information regarding opinion roles. 2 Related Work With regard to opinion role extraction, many features for supervised learning have been explored. They typically address the relationship between opinion word and opinion role on the basis of surface patterns (Choi et al., 2005), part-of-speech information (Wiegand and Klakow, 2010), syntactic information (Kessler and Nicolov, 2009; Jakob and Gurevych, 2010) or semantic role labeling (Johansson and Moschitti, 2013; Deng and Wiebe, 2015). The majority of those features cannot be applied to our task since for opinion compounds, there is no context between opinion role and opinion word. In the area of noun compound analysis, there are two predominant approaches. On the one hand, lexical resources, such as WordNet (Miller et al., 1990), are employed in order to assign semantic categories to head and modifier and infer from those labels the 801 Dataset I 2000 compounds 389 (u"
N16-1094,R11-1039,1,0.902698,"Missing"
N16-1094,E12-1033,1,0.903414,"Missing"
N16-1094,K15-1022,1,0.920972,"s on a lexical level has only been examined for opinion verbs. Wiegand and Ruppenhofer (2015) propose a boot806 strapping approach in which seed verbs for the different sentiment views are automatically extracted.7 Then, a label propagation algorithm (Talukdar et al., 2008) is run on a word-similarity graph generated from the opinion verbs. Thus labels from the seeds can be expanded to the remaining opinion verbs. The nodes in the graph correspond to the opinion verbs. The best performing graph is based on the similarity metric introduced in Lin (1998). A critical step is the seed generation. Wiegand and Ruppenhofer (2015) extract seeds representing actor views by looking for opinion words frequently co-occurring with prototypical opinion holders (protoOHs). These are common nouns, such as opponents or critics, that typically act as opinion holders (Wiegand and Klakow, 2011). By definition, such explicit opinion holders indicate an actor view. Seeds for speaker-view verbs are obtained by extracting verbs co-occurring with reproach-patterns, such as obji(beschuldigt/blamed for, &lt;verb&gt;) (14) that matches in (15). (14) Pattern: obji(beschuldigt/blamed for, &lt;speaker-view verb&gt;) (15) Die UNO wurde beschuldigt, [die"
N16-1094,H05-1044,0,0.0155272,"we extracted all noun compounds from deWaC whose second morpheme is an opinion noun. Morphological analysis was carried out using morphisto (Zielinski and Simon, 2009).2 As opinion nouns, we used the nouns from the PolArt sentiment lexicon (Klenner et al., 2009). Unfortunately, this lexicon is lacking in neutral opinion nouns, such as Meinung (opinion) or Erwartung (expectation) which frequently occur in compounds, e.g. Expertenmeinung (expert opinion) or Kundenerwartungen (customer expectations). Therefore, we translated the 235 neutral opinion nouns from the (English) Subjectivity Lexicon (Wilson et al., 2005) into German. From the opinion compounds extracted from deWaC, we created two manually annotated datasets (Table 2). We use more than one dataset as we consider our task as a multi-stage task as shown in Figure 1. We believe that this is necessary as differ1 available at: www.coli.uni-saarland.de/ miwieg/naacl_2016_op_compounds_data.tgz ˜ 2 The data release provides more details regarding the gold standard, e.g. how compound instances were sampled. Each question (indicated by a rhombus) can be modeled with one binary supervised classifier. We build 3 classifiers, thus excluding the second ques"
N16-1094,I11-1038,0,0.0129991,"asured by the proportion of the most frequent role occurring within each group of compounds sharing the same head.6 Given this selectional preference, we formulate a global head constraint (Table 7) that if two compounds have the same head, their modifiers should convey the same opinion role. In order to implement this constraint in a supervised classifier we employ Markov Logic Networks (MLNs), which combine first-order logic with probabilities. As a tool, we use thebeast (Riedel, 2008). MLNs have been effectively used in various related NLP tasks, such as discourse-based sentiment analysis (Zirn et al., 2011), semantic-role labeling (MezaRuiz and Riedel, 2009), anaphora resolution (Hou et al., 2013) or question answering (Khot et al., 2015). 6 On average, a head occurs in 5 different compounds on Dataset I, and in 4 different compounds on Dataset II. 805 88.86 Dataset II 91.36 Table 6: Role-purity of compounds with the same head. MLNs are a set of pairs (Fi , wi ) where Fi is a first-order logic formula and wi an associated realvalued weight. They build a template for constructing a Markov network given a set of constants C. The probability distribution that is estimated is a loglinear model P (X"
N16-1094,C98-2122,0,\N,Missing
N16-1094,D15-1080,0,\N,Missing
N18-1095,P98-1013,0,0.678385,"Missing"
N18-1095,D14-1125,0,0.0367028,"Missing"
N18-1095,P16-1191,0,0.0651578,"Missing"
N18-1095,P97-1023,0,0.647125,"ings from the two largest corpora, i.e. AMZ and WAC (Table 2) using Word2Vec (Mikolov et al., 2013) in default configuration (i.e. 200 dimensions; cbow). The best performance was obtained by concatenating for each word the vectors induced from the two corpora.9 4.3 Baselines to Feature-based Approach In addition to a majority-class classifier we consider the following baselines: Weak Supervision (WSUP). With this baseline we want to build a lightweight classifier that does not require proper labeled training data. It is inspired by previous induction approaches for sentiment lexicons, such as Hatzivassiloglou and McKeown (1997) or Velikovich et al. (2010) which heuristically label some seed instances and then apply graph-based propagation to label the remaining words of a dataset. On the basis of word embeddings (§4.2), we build a word-similarity graph, where the nodes represent our negative polar expressions and each edge denotes the seman9 We also ran experiments with pretrained embeddings from GoogleNews but they did not improve classification. tic similarity between two arbitrary words. We compute it by the cosine of their word-embedding vectors. The output of PAT from Twitter (§4.1.4) is considered as positive"
N18-1095,E17-2068,0,0.0383637,"e13 , which has been used in Nobata et al. (2016) and Davidson et al. (2017), and the derogatory words from Wiktionary (Derogatory)14 .15 Finally, we also include our base lexicon (Table 1) in order to evaluate the expansion process of our two expanded lexicons (§5). For all lists, we train on a single feature indicating the frequency of abusive words in a micropost to be classified. Ottawa also contains weights assigned to abusive words. We weight the observed frequency with these weights. We further evaluate 3 classifiers representing the state of the art of in-domain evaluations: FastText (Joulin et al., 2017), Gated Recurrent Units Recurrent Neural Networks RNN, which have been reported to work best on English microposts (Pavlopoulos et al., 2017), and Yahoo, an SVM 13 www.hatebase.org https://en.wiktionary.org/wiki/ Category:English_derogatory_terms 15 There are also similar but smaller lists in Wiktionary, e.g. offensive terms. They produced no better results. 14 test Warner Waseem train Razavi Waseem Wulczyn Average Warner Razavi Wulczyn Average all 55.4 58.1 60.2 57.9 58.5 61.1 51.2 56.9 Yahoo explicit 65.2 55.9 72.8 64.6 61.2 63.1 68.2 64.2 feature-b. lex. all explicit 65.0 80.6 64.6 79.0 63."
N18-1095,P14-1145,0,0.0757256,"Missing"
N18-1095,N12-1071,0,0.0301779,"k of cross-domain classification of abusive documents (§6) where we use it as a highlevel feature. In this work, we consider microposts as documents. While for in-domain classification, supervised classifiers trained on generic features, such as bag of words or word embeddings, usually score very well, on cross-domain classification they perform poorly since they latch on to domain-specific information. In subjectivity, polarity and emotion classification, high-level features based on predictive domain-independent word lists have been proposed to bridge the domain mismatch (Dias et al., 2009; Mohammad, 2012; Wiegand et al., 2013). New abusive words constantly enter natural language. For example, according to Wiktionary4 the word gimboid, which refers to an incompetent person, was coined in the British television series Red Dwarf, possibly from the word gimp and the suffix -oid. According to Urban Dictionary5 , the word twunt, which is a portmanteau of the swearwords twat and cunt, has been invented 4 5 https://en.wiktionary.org www.urbandictionary.com 1046 Proceedings of NAACL-HLT 2018, pages 1046–1056 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics by"
N18-1095,D16-1057,0,0.0208676,"ctivity Lexicon with their respective polarities. As features, we use word embeddings (§4.2). In order to produce the feature-based lexicon of abusive words another SVM is trained on our base lexicon (Table 1) using the best feature set from Table 6. With 2989 abusive words, our expanded lexicon is 5 times as large as the base lexicon. In order to measure the impact of our proposed features on the quality of the resulting lexicon, we devised an alternative expansion which just employs word embeddings. For this, we used SentProp, the most effective induction method from the SocialSent package (Hamilton et al., 2016).11 6 6.1 : total number of microposts in the dataset Cross-domain Classification Motivation and Set Up We now apply our expanded lexicon (§5) to the classification of abusive microposts, i.e. we classify entire comments rather than words out of context. Table 8 shows the datasets of labeled microposts that we use. The difference between these datasets is the source from which they originate. Consequently, different topics are represented in the different datasets. Still, we find similar types 11 Since SentProp produces a ranking rather than a classification, we consider 2989 as a cut-off valu"
N18-1095,D17-1117,0,0.0422101,"5 Finally, we also include our base lexicon (Table 1) in order to evaluate the expansion process of our two expanded lexicons (§5). For all lists, we train on a single feature indicating the frequency of abusive words in a micropost to be classified. Ottawa also contains weights assigned to abusive words. We weight the observed frequency with these weights. We further evaluate 3 classifiers representing the state of the art of in-domain evaluations: FastText (Joulin et al., 2017), Gated Recurrent Units Recurrent Neural Networks RNN, which have been reported to work best on English microposts (Pavlopoulos et al., 2017), and Yahoo, an SVM 13 www.hatebase.org https://en.wiktionary.org/wiki/ Category:English_derogatory_terms 15 There are also similar but smaller lists in Wiktionary, e.g. offensive terms. They produced no better results. 14 test Warner Waseem train Razavi Waseem Wulczyn Average Warner Razavi Wulczyn Average all 55.4 58.1 60.2 57.9 58.5 61.1 51.2 56.9 Yahoo explicit 65.2 55.9 72.8 64.6 61.2 63.1 68.2 64.2 feature-b. lex. all explicit 65.0 80.6 64.6 79.0 63.4 80.7 64.3 80.1 63.3 62.0 58.7 78.8 62.9 78.5 61.6 73.1 Table 12: Cross-domain classification of microposts: all test data vs. explicit subs"
N18-1095,E14-4023,1,0.862613,"tor speaker 9.7 32.8 90.3 67.2 all numbers only refer to the subset of the base lexicon (Table 1) taken from the Subjectivity Lexicon (i.e. 1500 entries) Table 3: Percentage of abusive/not abusive instances among (binary) intensity and views. corpus. A review is awarded between 1 and 5 stars where 1 is the most negative score. We infer the polar intensity of a word by the distribution of starratings associated with the reviews in which it occurs. We assume negative polar expressions with a very high polar intensity to occur significantly more often in reviews assigned few stars (i.e. 1 or 2). Ruppenhofer et al. (2014) established that the most effective method to derive such polar intensity is by ranking words by their weighted mean of star ratings (Rill et al., 2012). All words of our base lexicon are ranked according to that score. As a feature we use the rank of a word. Intensity Directed towards Persons (INTperson ). Not all negative polar expressions with a high intensity are equally likely to be abusive. The high intensity expressions should also be words typically directed towards persons. Most polar statements in AMZ, however, are directed towards a movie, book or some electronic product. In order"
N18-1095,W17-3010,0,0.0611078,"ompiled lexicon from Razavi et al. (2010) and the lexicon of hate verbs from Gitari et al. (2015) have been compiled for this specific task. Since the latter lexicon is not publicly available we can only consider the former in our evaluation. In both publications, very little is said on the creation of these resources. Previous work focused on in-domain classification, a setting where generic features (e.g. bag of 6 https://github.com/miwieg/naacl2018 words) work well and word lists are less important. There have been investigations examining features on various datasets (Nobata et al., 2016; Samghabadi et al., 2017), however, these studies always trained and tested on the same domain. We show that a lexicon-based approach is effective in cross-domain classification. For a more detailed overview on previous work on the detection of abusive language in general, we refer the reader to Schmidt and Wiegand (2017). 3 Data Base Lexicon. Our base lexicon exclusively comprises negative polar expressions. It is a small set which we have annotated via crowdsourcing. We consider abusive words to be a proper subset of negative polar expressions. By just focusing on these types of words, we are more likely to obtain a"
N18-1095,W17-1101,1,0.937306,"tion of these resources. Previous work focused on in-domain classification, a setting where generic features (e.g. bag of 6 https://github.com/miwieg/naacl2018 words) work well and word lists are less important. There have been investigations examining features on various datasets (Nobata et al., 2016; Samghabadi et al., 2017), however, these studies always trained and tested on the same domain. We show that a lexicon-based approach is effective in cross-domain classification. For a more detailed overview on previous work on the detection of abusive language in general, we refer the reader to Schmidt and Wiegand (2017). 3 Data Base Lexicon. Our base lexicon exclusively comprises negative polar expressions. It is a small set which we have annotated via crowdsourcing. We consider abusive words to be a proper subset of negative polar expressions. By just focusing on these types of words, we are more likely to obtain a significant amount of abusive words than just considering a sample of arbitrary words. This lexicon will be used as a gold standard for calibrating features of a classifier. That classifier will be run on a large set of unlabeled negative polar expressions to produce our expanded lexicon (§5). We"
N18-1095,D08-1061,0,0.0389581,"t of PAT from Twitter (§4.1.4) is considered as positive class seed instances. We chose PAT since it is an effective feature that does not depend on a lexical resource. As negative class seeds, we use the most frequent words in the WAC corpus (Table 2). Our rationale is that highfrequency words are unlikely to be abusive. We chose WAC instead of Twitter since the evidence of PAT (Table 5) suggested less abusive language in that corpus. This word-similarity graph is illustrated in Figure 1. In order to propagate the labels to the unlabeled words from the seeds, we use the Adsorption algorithm (Talukdar et al., 2008). Using Labeled Microposts (MICR). With our last baseline we examine in how far we can detect abusive words by only using information from labeled microposts rather than labeled words. These experiments are driven by the fact that labeled microposts already exist. We consider two methods using the largest dataset comprising manually labeled microposts, Wulczyn (Table 8). The class labels of the microposts and our base lexicon (§3) are the same. Our aim is to produce a ranking of words where the high ranks represent words more likely to be abusive. Since we want to produce a strong baseline, we"
N18-1095,N10-1119,0,0.012075,"AMZ and WAC (Table 2) using Word2Vec (Mikolov et al., 2013) in default configuration (i.e. 200 dimensions; cbow). The best performance was obtained by concatenating for each word the vectors induced from the two corpora.9 4.3 Baselines to Feature-based Approach In addition to a majority-class classifier we consider the following baselines: Weak Supervision (WSUP). With this baseline we want to build a lightweight classifier that does not require proper labeled training data. It is inspired by previous induction approaches for sentiment lexicons, such as Hatzivassiloglou and McKeown (1997) or Velikovich et al. (2010) which heuristically label some seed instances and then apply graph-based propagation to label the remaining words of a dataset. On the basis of word embeddings (§4.2), we build a word-similarity graph, where the nodes represent our negative polar expressions and each edge denotes the seman9 We also ran experiments with pretrained embeddings from GoogleNews but they did not improve classification. tic similarity between two arbitrary words. We compute it by the cosine of their word-embedding vectors. The output of PAT from Twitter (§4.1.4) is considered as positive class seed instances. We cho"
N18-1095,W12-2103,0,0.698211,"Missing"
N18-1095,N16-2013,0,0.256071,"resources. These features are calibrated on a small manually annotated base lexicon which we use to produce a large lexicon. We show that the word-level information we learn cannot be equally derived from a large dataset of annotated microposts. We demonstrate the effectiveness of our (domain-independent) lexicon in the crossdomain detection of abusive microposts. 1 Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning2 , they are all compatible with the general definition above for abusive language.3 (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. In this paper, we address the task of detecting"
N18-1095,N16-1094,1,0.876145,"ntensity directed towards persons, we replace the AMZ corpus with the RIA corpus (Table 2). RIA contains reviews on arbitrary entities rather than just commercial products as in the case of AMZ. Each review has a category label (e.g. computer, person, travel) that very easily allows us to extract from RIA just those reviews that concern persons. Table 4 compares a typical 1-star review from AMZ with one from RIA. We consider the RIAreview an abusive comment. It contains many words predictive of abusive language (e.g. selfabsorbed, loser, arrogant or loud-mouthed). 4.1.2 Sentiment Views (VIEW) Wiegand et al. (2016b) define sentiment views as the perspective of the opinion holder of polar expressions. They distinguish between expressions conveying the view of the implicit speaker of the utterance typically referred to as speaker views (e.g. cheating in (4); ugly and stinks in (5)), and expressions conveying the view of event participants typically referred to as actor views (e.g. disappointed and horrified in (6); protested in (7)). 1048 corpus RateItAll (RIA) Amazon (AMZ) Web as Corpus (WAC) size 4.7M 1.2B 2.3B properties review corpus focused on persons product review corpus large general web corpus p"
N18-1095,N16-1092,1,0.94168,"ntensity directed towards persons, we replace the AMZ corpus with the RIA corpus (Table 2). RIA contains reviews on arbitrary entities rather than just commercial products as in the case of AMZ. Each review has a category label (e.g. computer, person, travel) that very easily allows us to extract from RIA just those reviews that concern persons. Table 4 compares a typical 1-star review from AMZ with one from RIA. We consider the RIAreview an abusive comment. It contains many words predictive of abusive language (e.g. selfabsorbed, loser, arrogant or loud-mouthed). 4.1.2 Sentiment Views (VIEW) Wiegand et al. (2016b) define sentiment views as the perspective of the opinion holder of polar expressions. They distinguish between expressions conveying the view of the implicit speaker of the utterance typically referred to as speaker views (e.g. cheating in (4); ugly and stinks in (5)), and expressions conveying the view of event participants typically referred to as actor views (e.g. disappointed and horrified in (6); protested in (7)). 1048 corpus RateItAll (RIA) Amazon (AMZ) Web as Corpus (WAC) size 4.7M 1.2B 2.3B properties review corpus focused on persons product review corpus large general web corpus p"
N18-1095,H05-1044,0,0.205019,"Missing"
N18-1095,W17-3012,0,0.247349,"Missing"
N19-1060,D18-1302,0,0.157898,"64.7 Warner (Warner and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (datas"
N19-1060,N18-2019,0,0.0243528,"r and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (dataset produced by bias"
N19-1060,W18-5115,0,0.0270705,"012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (dataset produced by biased sampling). Topic Bia"
N19-1060,E17-2068,0,0.10408,"Missing"
N19-1060,W18-5114,0,0.0344333,"biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sample) and Waseem (dataset produced by biased sampling). Topic Bias The Waseem-dataset has"
N19-1060,W18-5117,0,0.140307,"Missing"
N19-1060,W18-5110,0,0.0943693,"Missing"
N19-1060,W18-5104,0,0.0424433,"Missing"
N19-1060,W12-2103,0,0.642114,"Missing"
N19-1060,W18-4401,0,0.101243,"Missing"
N19-1060,W17-3012,0,0.384825,"nces made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. 2 (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Explicit and Implicit Abuse One major distinction that has been proposed in the literature is the division into explicitly and implicitly abusive language (Waseem et al., 2017). The former are microposts that employ some abusive words (1)-(3) (e.g. dumbass or scum), while the latter represents the more difficult case in which the abusive nature is conveyed by other means, such as sarcasm, jokes, and particularly the usage of negative stereotypes etc. (4)-(5). Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. In this paper, we examine the issue of data bias. For the creation of manua"
N19-1060,C18-1093,0,0.0325417,"boosted random sampling 64.7 Warner (Warner and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sam"
N19-1060,N16-2013,0,0.677141,"hat classification scores on popular datasets reported in previous work are much lower under realistic settings in which this bias is reduced. Such biases are most notably observed on datasets that are created by focused sampling instead of random sampling. Datasets with a higher proportion of implicit abuse are more affected than datasets with a lower proportion. 1 Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. 2 (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Explicit and Implicit Abuse One major distinction that has been proposed in the literature is the division into explicitly and implicitly abusive language (Waseem et al., 2017). The former are microposts that employ some abusive words (1)-(3) (e.g. dumbass or scum), while the latter represents the more di"
N19-1060,W18-5101,0,0.0345282,"boosted random sampling 64.7 Warner (Warner and Hirschberg, 2012) diverse 3,438 14.3 biased sampling 51.3 Waseem (Waseem and Hovy, 2016) Twitter 16,165 35.3 biased sampling 44.4 Kumar (Kumar et al., 2018) Facebook 15,000 58.1 biased sampling 32.7 F1 88.2 87.3 83.3 71.8 80.5 70.4 Table 1: Properties of the different datasets. (∗ : proportion of explicitly abusive microposts among abusive microposts. † : This is an extension of the dataset presented in Wulczyn et al. (2017). Details on the corpus creation about Kaggle can therefore be found in that publication.) rank 1 2 3 4 5 6 7 8 9 10 2018; Mishra et al., 2018a,b; Park et al., 2018; Qian et al., 2018; Sahlgren et al., 2018; Sharifirad et al., 2018; Unsv˚ag and Gamb¨ack, 2018; Wiegand et al., 2018a). This investigation is only possible since, fortunately, Waseem is one of the datasets whose creation process has been meticulously documented. 4 Founta bitch niggas motherfucker fucking nigga idiot asshole fuck fuckin pussy Waseem commentator comedian football announcer pedophile mankind sexist sport outlaw driver Table 2: Top 10 words having strongest correlation with abusive microposts according to PMI on Founta (dataset representing almost random sam"
N19-1060,N18-1095,1,0.890618,"marry children. Muslims do. All the time. To determine which of the datasets that we consider in this work contain which type of abusive language, we proceeded as follows. On the set of abusive microposts of each dataset, we computed the proportion of microposts that include at least 0 Present affiliation: Leibniz ScienceCampus, Heidelberg/Mannheim, Germany 1 http://thelawdictionary.org/ 602 Proceedings of NAACL-HLT 2019, pages 602–608 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics one abusive word according to the lexicon of abusive words from Wiegand et al. (2018a). Datasets with a high proportion of abusive words typically contain a high amount of explicitly abusive microposts, whereas datasets with a low proportion contain a higher amount of implicitly abusive language. The resulting figures, of course, are only a lower bound estimate for explicit language abuse. There will also be microposts containing abusive words that are missing from the lexicon. However, after manual inspection of a sample of microposts, we are fairly confident that this does not significantly change the relative order of datasets when ranked according to their degree of expli"
N19-1211,E09-1005,0,0.0160256,"of NEG. From that ranking we remove all those compounds which have not co-occurred at the high ranks of at least one of the other diagnostics (COMCON, OUT, PERSON).9 Compounds that are highly ranked by several diagnostics should more likely represent derogatory compounds. Re-Ranking by PageRank (PRANK). We observed that among the top ranks of COMB, the derogatory compounds are semantically similar (e.g. dwarf tosser, mischief maker, slimeball) while the non-derogatory compounds are semantically different from each other (e.g. biker club, spirit bear). Therefore, we run personalized PageRank (Agirre and Soroa, 2009) to further improve the ranking by enforcing the compounds on the high ranks to be distributionally similar. We build a word-similarity graph where our compounds are nodes and edges encode cosine-similarities of their embeddings. PageRank then produces a ranking of nodes where the highest ranked nodes are the ones most highly connected. In personalized PageRank prior information is added. A biased graph is constructed in which attention is drawn towards particular regions of interest. This is achieved by assigning re-entrance weights to the individual nodes. As prior information, we set the no"
N19-1211,D15-1188,0,0.031153,"(of which embeddings are the main contributor) on the constituents of the compounds. Moreover, we train an LSTM on the sequence of characters of the compound. Table 4 shows that information drawn from units other than the compound itself is less effective. The feature combination of head, modifier and compound is not effective either. Instead of applying embeddings on constituents and concatenating them, we also examine a sophisticated compositional model (Wmask) based on a masking process that takes into account the variation of a constituent depending on whether it is a head or a modifier (Dima, 2015). Table 5 shows the performance of the two best previous classifiers where compounds lacking an embedding are represented by an embedding approximated by Wmask (rather than a dummy vector). The table shows that the two classifiers can be improved by adding the approximated embeddings. 6 Conclusion We examined the new task of detecting derogatory compounds and proposed an unsupervised approach incorporating linguistic properties of compounds that mostly depend on a distributional representation. Our method outperforms linguistic features previously shown to be effective for the detection of der"
N19-1211,W09-4635,0,0.0196144,"d compounds, e.g. Milchbube (milk sop) or Schnapsdrossel (booze hound), we can employ standard tokenization7 for inducing embeddings for our compounds. 4.1 Individual High-Precision Diagnostics Negative Polarity (NEG). Derogatory words form a subset of negative polar expressions. Due to their sparsity, however, derogatory compounds are rarely part of any sentiment lexicon (containing polar expressions). We, therefore, rank all our compounds according to their cosine similarity to a centroid embedding-vector computed from all negative polar expressions from the German PolArt sentiment lexicon (Klenner et al., 2009). Compound Occurrence vs. Constituent Occurrence (COMCON). Derogatory compounds can be creative word constructions (e.g. booze hound, oxygen thief, keyboard warrior). Consequently, their constituents are often not semantically related. For instance, in booze hound, booze bears no common semantic relation to hound. Therefore, the corpus frequency of a derogatory compound should be much higher than its constituents co-occurring in a sentence (i.e. with other words occurring in between). Such cooccurrences should be coincidental. We capture this by the following formula (frequencies are computed"
N19-1211,D08-1061,0,0.0492548,"awn towards particular regions of interest. This is achieved by assigning re-entrance weights to the individual nodes. As prior information, we set the nodes representing the compounds returned by COMB with a uniform re-entrance weight (α)10 while all other nodes receive a weight of 0. Label Propagation (LP). While previous diagnostics were designed to isolate a few derogatory compounds with a high precision, LP aims for increasing recall. We define some high-precision seeds for the two categories of our task and then propagate the labels to the unlabeled compounds by using label propagation (Talukdar et al., 2008). The algorithm operates on the same wordsimilarity graph that we used for PRANK. We define highly ranked compounds from PRANK as 9 We took top 350 from all these rankings which resembles the number of derogatory compounds on our dataset. 10 Following Manning et al. (2008), we set α = 0.1. derogatory seeds and lowly ranked compounds as non-derogatory seeds. Unlike the previous diagnostics, the output of LP is a binary categorization rather than a ranking. In order to make this output comparable to the other diagnostics, we converted the output of LP to a ranking. This is achieved by ranking th"
N19-1211,N16-2013,0,0.0713261,"represented in lexical resources previously found effective for this task (e.g. Wiktionary). We propose an unsupervised classification approach that incorporates linguistic properties of compounds. It mostly depends on a simple distributional representation. We compare our approach against previously established methods proposed for extracting derogatory unigrams. 1 Introduction Abusive or offensive language is commonly defined as hurtful, derogatory or obscene utterances made by one person to another person.1 Examples are (1)-(3). In the literature, closely related terms include hate speech (Waseem and Hovy, 2016) or cyber bullying (Zhong et al., 2016). While there may be nuanced differences in meaning, they are all compatible with the general definition above. (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. A substantial amount of abusive utterances comprises derogatory words ("
N19-1211,N18-1095,1,0.930788,"l definition above. (1) stop editing this, you dumbass. (2) Just want to slap the stupid out of these bimbos!!! (3) Go lick a pig you arab muslim piece of scum. Due to the rise of user-generated web content, in particular on social media networks, the amount of abusive language is also steadily growing. NLP methods are required to focus human review efforts towards the most relevant microposts. A substantial amount of abusive utterances comprises derogatory words (e.g. bimbo or scum). Automatic extraction methods of such words are required since new derogatory words constantly enter language. Wiegand et al. (2018a) extracted a 0 Present affiliation: Leibniz ScienceCampus, Heidelberg/Mannheim, Germany 1 http://thelawdictionary.org/ large list of such expressions and demonstrated its importance for text classification. In this work, we focus on a subtype of derogatory terms, namely derogatory compounds (e.g. booze hound, curry muncher, fault finder). Distinguishing such multi-word expressions from nonderogatory ones (e.g. fox hound, mile muncher, branch finder) is more difficult than classifying unigrams since they are only sparsely represented in general-purpose lexical resources which have previously"
N19-1211,W17-1101,1,0.839554,"rhero.com/de/insults.htm www.schimpfwoerter.de www.seechat.de/warmduscher.htm 6 These lists contain many compounds commonly used in a non-offensive manner, e.g. Colatrinker (coke drinker). 4 0.01 0 0 200 400 600 800 1000 Rank Figure 1: Head distribution of derogatory compounds. Property Freq total compounds 3500 derogatory compounds 382 head groups (each group contains 20 compounds) 175 average no. of derogatory compounds in head group 2.2 Table 1: Some statistics of the gold standard. Lexical knowledge for the detection of abusive language has only received little attention in previous work (Schmidt and Wiegand, 2017), the notable exceptions are Razavi et al. (2010) who present a manually-compiled lexicon, Gitari et al. (2015) who bootstrap hate verbs and Wiegand et al. (2018a) who induce a lexicon of derogatory words. In all these researches, however, derogatory compounds are not explicitly addressed. 3 0.015 0.005 Related Work 3 0.02 stag hound). Since among those putative nonderogatory instances, there could well be further derogatory compounds, we manually annotated them as well. We limited the set of compounds sharing the same head, which we henceforth call head group, to 20 compounds. Thus, we hope t"
P11-1005,J09-4005,0,0.0942503,"of annotation noise in the context of supervised classification. Section 3 describes the experimental setup of our simulation study and presents results. In Section 4 we present our filtering approach and show its impact on AL performance. Section 5 concludes and outlines future work. ing, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The authors argue that even for those data points where the annotators agreed on one particular class, a proportion of the agreement might be merely due to chance. Following this assumption, the authors propose a measure to estimate the amount of annotation noise in the data after removing all hard cases. Klebanov et al. (2008; 2009) show that, according to their model, high inter-annotator agreement (κ) ac"
P11-1005,W08-1202,0,0.0312749,"n recent research on the impact of annotation noise in the context of supervised classification. Section 3 describes the experimental setup of our simulation study and presents results. In Section 4 we present our filtering approach and show its impact on AL performance. Section 5 concludes and outlines future work. ing, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The authors argue that even for those data points where the annotators agreed on one particular class, a proportion of the agreement might be merely due to chance. Following this assumption, the authors propose a measure to estimate the amount of annotation noise in the data after removing all hard cases. Klebanov et al. (2008; 2009) show that, according to their model,"
P11-1005,P07-1007,0,0.0225273,"(e.g. sentiment analysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learn2.2 44 Annotation Noise and Active Learning For AL to be succesful, we need to remove"
P11-1005,N06-1016,0,0.0259507,"for many NLP tasks (e.g. sentiment analysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learn2.2 44 Annotation Noise and Active Learning For AL to be succesful,"
P11-1005,P09-1021,0,0.0207875,"ics Saarland University josefr@coli.uni-sb.de Ines Rehbein Computational Linguistics Saarland University rehbein@coli.uni-sb.de Abstract Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense"
P11-1005,J04-3001,0,0.074961,"Missing"
P11-1005,C08-1059,0,0.0426874,"Missing"
P11-1005,P00-1016,0,0.0724716,"Missing"
P11-1005,N04-1012,0,0.0727825,"Missing"
P11-1005,C10-1107,1,0.797444,"metimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense Disambiguation (WSD) with fine-grained sense distinctions, AL can actually harm the learning process (Dang, 2004; Rehbein et al., 2010). Dang suggests that the lack of a positive effect of AL might be due to inconsistencies in the human annotations and that AL cannot efficiently be applied to tasks which need double blind annotation with adjudication to insure a sufficient data quality. Even if we take a more optimistic view and assume that AL might still be useful even for tasks featuring a high degree of ambiguity, it remains crucial to address the problem of annotation noise and its impact on AL. Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised"
P11-1005,J08-3001,0,0.36679,"ssumes that a) all instances where annotators disagreed are in fact hard cases, and b) that for the hard cases the annotators decisions are obtained by coin-flips. In our experience, some amount of disagreement can also be observed for easy cases, caused by attention slips or by a deviant interpretation of some class(es) by one of the annotators, and the annotation decision of an individual annotator cannot so much be described as random choice (coin-flip) but as systematically biased selection, causing the types of errors which have been shown to be problematic for supervised classification (Reidsma and Carletta, 2008). Further problems arise in the AL scenario where the instances to be annotated are selected as a function of the sampling method and the annotation judgements made before. Therefore, Beigman and Klebanov Beigman (2009)’s approach of identifying unreliably annotated instances by disagreement is not applicable to AL, as most instances are annotated only once. 2 Related Work We are interested in the question whether or not AL can be successfully applied to a supervised classification task where we have to deal with a considerable amount of inconsistencies and noise in the data, which is the case"
P11-1005,W07-1516,0,0.0526379,"Missing"
P11-1005,P04-1075,0,0.0690551,"Missing"
P11-1005,D07-1082,0,0.028536,"alysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learn2.2 44 Annotation Noise and Active Learning For AL to be succesful, we need to remove systematic noise in t"
P11-1005,C08-1143,0,0.0190414,"s noise-free. For classification we use a maximum entropy classifier.3 Our sampling method is uncertainty sampling (Lewis and Gale, 1994), a standard sampling heuristic for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. As a measure of uncertainty we use Shannon entropy (1) (Zhang and Chen, 2002) and the margin metric (2) (Schein and Ungar, 2007). The first measure considers the model’s predictions q for each class c and selects those instances from the pool where the Shannon entropy is highest. assigned by the human coders. Zhu et al. (2008) present a method for detecting outliers in the pool of unannotated data to prevent these instances from becoming part of the training data. This approach is different from ours, where we focus on detecting annotation noise in the manually labelled training data produced by the human coders. Schein and Ungar (2007) provide a systematic investigation of 8 different sampling methods for AL and their ability to handle different types of noise in the data. The types of noise investigated are a) prediction residual error (the portion of squared error that is independent of training set size), and b"
P17-1107,E03-1068,0,0.127447,"entify errors in automatically labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have proposed selective relabelling"
P17-1107,W11-3405,0,0.0226244,"with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have proposed selective relabelling strategies when working with non-exper"
P17-1107,D09-1015,0,0.0113734,"a development set during training, we also extract the dev data from sections 00-18 of the PTB. 1163 hours. This is a problem for the typical AL setting where it is crucial not to keep the human annotators waiting for the next instance while the system retrains. A major advantage of our setup is that we do not need to retrain the baseline classifiers as we only use them once, for preprocessing, before the actual error detection starts. For the NER experiment, we use tools for which pretrained models for German are available, namely GermaNER (Benikova et al., 2015), and the StanfordNER system (Finkel and Manning, 2009) with models trained on the HGC and the DeWaC corpus (Baroni et al., 2009; Faruqui and Pad´o, 2010).6 4.2 EVAL: tagger acc. Preprocessing Annotation matrix: ... cn ... DT N NE ... N V V ... V ... ... ... ... AL for N iterations Select instances Evaluation measures QBC entropy To increase the number of annotators we use an older version of the StanfordNER (2009-01-16) and a newer version (2015-12-09), with both the DeWaC and HGC models, resulting in a total of 5 annotators for the NER task. 7 Please note that the success of our method relies on the variation in the ensemble predictions, and thu"
P17-1107,P09-1032,0,0.053732,"Missing"
P17-1107,gimenez-marquez-2004-svmtool,0,0.0309355,"Missing"
P17-1107,P07-2053,0,0.0441676,"Missing"
P17-1107,N13-1132,0,0.574409,"rrection of automatically prelabelled text is not feasible. Given the importance of identifying noisy annotations in automatically annotated data, it is all the more surprising that up to now this area of research has been severely understudied. This paper addresses this gap and presents a method for error detection in automatically labelled text. As test cases, we use POS tagging and Named Entity Recognition, both standard preprocessing steps for many NLP applications. However, our approach is general and can also be applied to other classification tasks. Our approach is based on the work of Hovy et al. (2013) who develop a generative model for estimating the reliability of multiple annotators in a crowdsourcing setting. We adapt the generative model to the task of finding errors in automatically labelled data by integrating it in an active learning (AL) framework. We first show that the approach of Hovy et al. (2013) on its own is not able to beat a strong baseline. We then present our integrated model, in which we impose human supervision on the generative model through AL, and show that we are able to achieve substantial improvements in two different tasks and for two languages. Our contribution"
P17-1107,D07-1031,0,0.0204342,"al behaviour of annotator j in the case that the annotator is not trying to predict the correct label. The model parameters are learned by maximizing the marginal likelihood of the observed data, using Expectation Maximization (EM) (Dempster et al., 1977) and Bayesian variational inference. Bayesian inference is used to provide the model with priors on the annotators’ behaviour and yields improved correlations over EM between the model estimates and the annotators’ proficiency while keeping accuracy high. For details on the implementation and parameter settings refer to Hovy et al. (2013) and Johnson (2007). We adapt the model of Hovy et al. (2013) and apply it to the task of error detection in automatically labelled text. To that end, we integrate the variational model in an active learning (AL) setting, with the goal of identifying as many errors as possible while keeping the number of instances to be checked as small as possible. The tasks we chose in our experiments are POS tagging and NER, but our approach is general and can easily be applied to other classification tasks. 3.2 Active learning Active learning (Cohn et al., 1996) is a semisupervised framework where a machine learner is traine"
P17-1107,C02-1021,0,0.0703545,"ction that is able to identify errors in automatically labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have pr"
P17-1107,P10-1052,0,0.0404155,"Missing"
P17-1107,E09-1060,0,0.244572,"y labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have proposed selective relabelling strategies when"
P17-1107,P14-1014,0,0.0701229,"Missing"
P17-1107,P16-2067,0,0.0416643,"Missing"
P17-1107,W14-4903,1,0.896919,"Missing"
P17-1107,J08-3001,0,0.0222801,"Missing"
P17-1107,N03-1033,0,0.0311353,"Missing"
P17-1107,W00-1907,0,0.529233,"Missing"
P17-1107,D08-1027,0,0.296059,"Missing"
R11-1046,S10-1059,0,0.42829,"Missing"
R11-1046,N10-1138,0,0.0632222,"Missing"
R11-1046,P10-1160,0,0.157854,"rpus provides annotations for running texts not for individual occurrences of selected target predicates. It thus treats many different generallanguage predicates of all parts of speech. While the overall size of the corpus in terms of sentences is comparable to Gerber and Chai’s corpus, the SemEval corpus contains many more target predicates and fewer instances for each.3 These properties make it much harder to obtain good results on the SemEval corpus, which is supported by the fact that the NI resolution results obtained by the Task-10 participants are significantly below those reported by Gerber and Chai (2010). While the SemEval-10 Task-10 is harder than the problem tackled by Gerber and Chai (2010), we also believe it is more realistic. Given the complexity of annotating semantic argument structures in general and null instantiations in particular, it seems infeasible to annotate large amounts of text with the required information. Hence, automated systems will always have to make do with scarce resources. We investigate different strategies of incorporating linguistic background knowledge to overcome this data sparseness problem, e.g., by explicitly modeling the DNI v. INI distinction, which is i"
R11-1046,J01-4005,0,0.065965,"Missing"
R11-1046,P10-1005,0,0.0832063,"Missing"
R11-1046,S10-1008,1,0.600884,"ate. The other arguments are so-called null instantiations (NIs). Even core arguments of a predicate, i.e., those that express participants which are necessarily present in the situation which the predicate evokes (see Section 2 for a more detailed explanation of core vs. peripheral arguments), are frequently not instantiated in the local context. While null instantiated arguments are not locally realized, they can often be inferred from the context. Consider examples (1) and (2) below (taken from Arthur Conan Doyle’s “The Adventure of Wisteria Lodge” and part of the SemEval-10 Task10 corpus (Ruppenhofer et al., 2010)). We use A and B in the examples to indicate speakers.1 In a frame-semantic analysis of (1) interesting evokes the Mental stimulus stimulus focus (1) A. [“A white cock,”]Stim said [he]Exp . “[Most]Deg interestingMssf !” (2) A. [“Your powers seem superior to your opportunities.”]Inf “[You]Src ’re rightCorr , Mr. Holmes.” B. While humans have no problem inferring uninstantiated roles that can be filled from the linguistic context, this is beyond the capacity of state-ofthe-art semantic role labeling systems, which tacitly ignore all roles that are not instantiated locally. SRL systems thus disr"
R11-1046,S10-1065,0,0.258966,"Missing"
R11-1064,P08-2012,0,0.119345,"p the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), which share roles (frame ele"
R11-1064,P04-1051,1,0.796122,"dered temporally. This guided way of learning script data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scrip"
R11-1064,H92-1045,0,0.0355937,"Missing"
R11-1064,P98-1013,0,0.12353,"variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), which share roles (frame elements) specifying people and objects involved in the events. The supervised approach of Mani et al. (2006) learns temporal event relations from TimeBank (Pustejovsky et al., 2006). All of these approaches rely on elaborate manual annotation efforts, and so it is unclear how they would scale to wide-coverage resources. Chambers and Jurafsky (2008; 2009) exploit coreference chains and co-occurrence frequency of verbs in te"
R11-1064,P03-1054,0,0.00700172,"Missing"
R11-1064,N06-1046,0,0.0281613,"guided way of learning script data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants."
R11-1064,P06-1095,0,0.085404,"Missing"
R11-1064,P10-1124,0,0.0200376,"ipt data produces representations associated with known scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a st"
R11-1064,J93-2004,0,0.0366212,"Missing"
R11-1064,W10-4305,0,0.0136123,"ral information by itself is worthless: high precision loss makes align even worse than the na¨ıve baseline. (6) (7) Overall precision and recall is averaged over all tokens in the annotation. Overall F1 score is then computed as follows: F1 = 2 ∗ precision ∗ recall precision + recall (8) Unlike in coreference resolution, we have the problem that we compare gold-standard annotations against tokens extracted from automatic parses. However, the b3 -metric is only applicable if the gold standard and the test data contain the same set of tokens. Thus we apply b3sys , a variant of b3 introduced by Cai and Strube (2010). b3sys extends the gold standard and the test set such that both contain the same set of tokens. Roughly speaking, every token that appears in the gold standard but not in the test set is copied to the latter and treated as singleton set, and vice versa. See Cai and Strube for details. With the inaccurate parser, noun phrases are often parsed incompletely, missing modifiers or relative clauses. We therefore consider a participant description as equivalent with a gold standard phrase if they have the same head. This relaxed scoring metric evaluates the system realistically by punishing parsing"
R11-1064,P08-1090,0,0.602925,"and computational linguistics, including commonsense reasoning for text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989) and automated storytelling (Swanson and Gordon, 2008). But there is hardly an area where the discrepancy between the felt importance of a type of knowledge and the inability to provide any substantial amount of this knowledge for serious applications is greater. Recently, several groups have tackled the problem using unsupervised methods for learning script-like knowledge from text corpora or data obtained through web experiments (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010). For the first time, they open up a perspective to wide-coverage resources of script knowledge. However, each of these approaches handles only specific aspects of script 463 Proceedings of Recent Advances in Natural Language Processing, pages 463–470, Hissar, Bulgaria, 12-14 September 2011. 1 2 3 4 5 6 7 ESD 1 ESD 2 ESD3 put food on plate open microwave put plate in close microwave press start put food in bowl open door put food inside close door enter time push button ... put food on dish open oven place dish in oven close select desired le"
R11-1064,P09-1068,0,0.111637,"s, including commonsense reasoning for text understanding (Cullingford, 1977; Mueller, 2004), information extraction (Rau et al., 1989) and automated storytelling (Swanson and Gordon, 2008). But there is hardly an area where the discrepancy between the felt importance of a type of knowledge and the inability to provide any substantial amount of this knowledge for serious applications is greater. Recently, several groups have tackled the problem using unsupervised methods for learning script-like knowledge from text corpora or data obtained through web experiments (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Regneri et al., 2010). For the first time, they open up a perspective to wide-coverage resources of script knowledge. However, each of these approaches handles only specific aspects of script 463 Proceedings of Recent Advances in Natural Language Processing, pages 463–470, Hissar, Bulgaria, 12-14 September 2011. 1 2 3 4 5 6 7 ESD 1 ESD 2 ESD3 put food on plate open microwave put plate in close microwave press start put food in bowl open door put food inside close door enter time push button ... put food on dish open oven place dish in oven close select desired length strengths and weaknesses"
R11-1064,P10-1100,1,0.934831,"Department of Computational Linguistics, Saarland University {regneri,pinkal}@coli.uni-saarland.de † Department of Linguistics, University of Potsdam koller@ling.uni-potsdam.de ‡ Department of Information Science and Language Technology, University of Hildesheim Josef.Ruppenhofer@uni-hildesheim.de Abstract information: Chambers and Jurafsky (2009) learn narrative schemas and their participants; they group verbs into schemas by virtue of shared participants assuming that this is an indicator for being part of the same stereotypical activity, without knowing the actual scenarios. The system of Regneri et al. (2010) learns the temporal order of events occurring in specific stereotypical scenarios, but does not determine participants. In this paper, we present a system that automatically learns sets of participants associated with specific scenarios. We take the approach of Regneri et al. as our starting point. In this earlier work, several experimental subjects described what happens in a given scenario in a web experiment; the system then learns what event descriptions from different subjects refer to the same event, and how they are temporally ordered, using Multiple Sequence Alignment (Durbin et al.,"
R11-1064,H94-1010,0,0.343914,"Missing"
R11-1064,N07-1030,0,0.0286859,"scenarios, and also opens up the possibility of learning about activities that are too stereotypical to be elaborated much in text corpora (and which thus can’t be induced from there). However, the approach is limited by its reliance on scenarios that have to be determined beforehand. Tying in with this previous work, we compute participants using Integer Linear Programming to globally combine information from diverse sources. ILP has been applied to a variety of different problems in NLP (Althaus et al., 2004; Barzilay and Lapata, 2006; Berant et al., 2010), including coreference resolution (Denis and Baldridge, 2007; Finkel and Manning, 2008). Figure 1: Alignment for the MICROWAVE scenario. the evaluation before we finally conclude. 2 Related Work Many papers on scripts and their application perspectives have been published in the seventies (Schank and Abelson, 1977; Barr and Feigenbaum, 1981). Script knowledge was manually modeled, and never exceeded a handful of domains and implementations operating on them. Scenario frames in FrameNet (Baker et al., 1998) are another approach to modeling scripts and their participants. They describe how a stereotypical activity is made up of smaller events (frames), w"
R11-1064,C98-1013,0,\N,Missing
R15-1071,P10-1018,0,0.443956,"Missing"
R15-1071,Q13-1023,0,0.438187,"Missing"
R15-1071,C12-2036,0,0.0143082,". In subsequent work, Rill et al. (2012b) mention ways to infer the scores of unobserved adverb-adjective combinations based on observed combinations involving other, similar adjectives. However, the authors do not implement and evaluate these ideas. Finally, a great deal of research on intensity has focused on acquiring prior polarity scores for individual words, and specifically adjectives. Various methods have been explored, including phrasal patterns (Sheinman et al., 2013; de Melo and Bansal, 2013); the use of star ratings (Rill et al., 2012b); extracting knowledge from lexical resources Gatti and Guerini (2012); and collostructional analysis (Ruppenhofer et al., 2014). 7 Conclusion We examined various methods for ranking degree adverbs by their effect on the intensity of adjectives. We evaluated the methods against a new carefully-built gold standard that we collected experimentally as well as against a larger expertconstructed gold standard that we found to correlate well with ours for the overlapping members. While we found one method, Horn surface patterns, to currently not be workable at all due to the lack of suitable n-gram resources, we developed a MeanStar-based method that produces very goo"
R15-1071,E14-4023,1,0.799069,"lment. For instance, as illustrated by de Marneffe et al. (2010), when interpreting dialogue (A: Was it good? B: It was ok / great / excellent.), a yes/no question involving a gradable predicate may require understanding the entailment relations between that predicate and another contained in the answer. Another application is within sentiment analysis, where assessing the strength of subjective expressions (e.g. good &lt; great &lt; excellent) is one of the central tasks besides subjectivity detection and polarity classification (Rill et al., 2012b; Sheinman et al., 2013; de Melo and Bansal, 2013; Ruppenhofer et al., 2014, inter alia). It is also well known that subjective adjectives are frequently modified by adverbs that increase (very expensive) or decrease (fairly expensive) their intensity. As Benamara et al. (2007) have shown, it is useful to take such adverbial intensification into account when predicting document-level sentiment scores. However, Benamara et al. (2007) used human-assigned scores to model adverbs’ effect on adjectives. As far as we know, there is no well-established automatic method that can determine for degree adverbs what their effect will be on the intensity of various adjectives. In"
R15-1071,J11-2001,0,0.52357,"produces essentially the same result: the Spearman rank correlation with the absolute ranking in Table 5 is ρ=0.993. Due to space limitations, we only report results relative to the absolute gold standard in the remainder of the paper. In order to be able to experiment with more than the 14 prototypical and frequent adverbs that we could collect ratings for, we make use of the intensity ratings for 93 adverbs provided by Taboada et al.’s (2011) SoCaL resource. While various lexical resources provide polarity scores for nouns, verbs, and adjectives (Wilson et al., 2005; Thelwall et al., 2010; Taboada et al., 2011, inter alia), few resources cover and assign scores to degree adverbs. The adverb ranking obtained from the SoCaL resource for our 14 adverbs correlates strongly with our two gold standards, with coefficients of 0.969 against the absolute gold standard and 0.976 against the relative one. This gives us confidence that we can use the SoCaL ratings as an extended gold standard. Note that the set of 93 adverbs from SoCaL contains many adverbs that are less frequent and less grammaticized than the 14 adverbs from the smaller set. 4 Methods Our methods to determine the intensifying effect of adverb"
R15-1071,H05-1044,0,0.0227395,"combination involving adverb B. That method produces essentially the same result: the Spearman rank correlation with the absolute ranking in Table 5 is ρ=0.993. Due to space limitations, we only report results relative to the absolute gold standard in the remainder of the paper. In order to be able to experiment with more than the 14 prototypical and frequent adverbs that we could collect ratings for, we make use of the intensity ratings for 93 adverbs provided by Taboada et al.’s (2011) SoCaL resource. While various lexical resources provide polarity scores for nouns, verbs, and adjectives (Wilson et al., 2005; Thelwall et al., 2010; Taboada et al., 2011, inter alia), few resources cover and assign scores to degree adverbs. The adverb ranking obtained from the SoCaL resource for our 14 adverbs correlates strongly with our two gold standards, with coefficients of 0.969 against the absolute gold standard and 0.976 against the relative one. This gives us confidence that we can use the SoCaL ratings as an extended gold standard. Note that the set of 93 adverbs from SoCaL contains many adverbs that are less frequent and less grammaticized than the 14 adverbs from the smaller set. 4 Methods Our methods t"
rehbein-ruppenhofer-2010-theres,N07-1051,0,\N,Missing
rehbein-ruppenhofer-2010-theres,nivre-etal-2006-maltparser,0,\N,Missing
rehbein-ruppenhofer-2010-theres,burchardt-etal-2006-salsa,0,\N,Missing
rehbein-ruppenhofer-2010-theres,N06-1016,0,\N,Missing
rehbein-ruppenhofer-2010-theres,D07-1082,0,\N,Missing
rehbein-ruppenhofer-2010-theres,N03-1033,0,\N,Missing
rehbein-ruppenhofer-2010-theres,W08-1008,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P03-1054,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P95-1026,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P98-1013,0,\N,Missing
rehbein-ruppenhofer-2010-theres,C98-1013,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P07-1007,0,\N,Missing
rehbein-ruppenhofer-2010-theres,I05-1081,0,\N,Missing
rehbein-ruppenhofer-2010-theres,kunze-lemnitzer-2002-germanet,0,\N,Missing
rehbein-ruppenhofer-2010-theres,W09-1107,0,\N,Missing
ruppenhofer-etal-2008-finding,E06-1025,0,\N,Missing
ruppenhofer-etal-2008-finding,N07-1037,0,\N,Missing
ruppenhofer-etal-2008-finding,W05-0308,1,\N,Missing
ruppenhofer-etal-2008-finding,W06-0305,0,\N,Missing
ruppenhofer-etal-2008-finding,W06-0301,0,\N,Missing
ruppenhofer-etal-2008-finding,H05-2017,0,\N,Missing
ruppenhofer-etal-2008-finding,H05-1043,0,\N,Missing
ruppenhofer-etal-2008-finding,W03-1014,1,\N,Missing
ruppenhofer-etal-2008-finding,W06-1651,0,\N,Missing
ruppenhofer-etal-2008-finding,P04-1035,0,\N,Missing
ruppenhofer-etal-2008-finding,P98-1013,0,\N,Missing
ruppenhofer-etal-2008-finding,C98-1013,0,\N,Missing
ruppenhofer-etal-2008-finding,N07-1039,0,\N,Missing
ruppenhofer-etal-2008-finding,P02-1053,0,\N,Missing
ruppenhofer-etal-2008-finding,J05-1004,0,\N,Missing
ruppenhofer-etal-2008-finding,P97-1023,0,\N,Missing
ruppenhofer-etal-2008-finding,andreevskaia-bergler-2006-semantic,0,\N,Missing
ruppenhofer-etal-2010-generating,D07-1002,0,\N,Missing
ruppenhofer-etal-2010-generating,burchardt-pennacchiotti-2008-fate,0,\N,Missing
ruppenhofer-etal-2010-generating,W07-1206,0,\N,Missing
ruppenhofer-etal-2010-generating,E09-1026,0,\N,Missing
ruppenhofer-etal-2010-generating,D09-1002,0,\N,Missing
ruppenhofer-etal-2010-generating,P98-1013,0,\N,Missing
ruppenhofer-etal-2010-generating,C98-1013,0,\N,Missing
ruppenhofer-etal-2010-generating,P09-1003,0,\N,Missing
ruppenhofer-etal-2010-generating,J05-1004,0,\N,Missing
ruppenhofer-etal-2010-generating,W09-3003,1,\N,Missing
ruppenhofer-etal-2010-speaker,krestel-etal-2008-minding,0,\N,Missing
ruppenhofer-etal-2010-speaker,W06-0301,0,\N,Missing
ruppenhofer-etal-2010-speaker,H05-1045,0,\N,Missing
ruppenhofer-etal-2017-evaluating,waltinger-2010-germanpolarityclues,0,\N,Missing
ruppenhofer-etal-2017-evaluating,J04-3002,0,\N,Missing
ruppenhofer-etal-2017-evaluating,P08-2028,0,\N,Missing
ruppenhofer-etal-2017-evaluating,W14-5805,0,\N,Missing
ruppenhofer-etal-2017-evaluating,E14-4023,1,\N,Missing
ruppenhofer-etal-2017-evaluating,D13-1170,0,\N,Missing
ruppenhofer-etal-2017-evaluating,W14-4721,1,\N,Missing
ruppenhofer-etal-2017-evaluating,N15-1071,0,\N,Missing
ruppenhofer-etal-2017-evaluating,N16-1094,1,\N,Missing
ruppenhofer-etal-2017-evaluating,W09-4635,0,\N,Missing
S10-1008,S07-1018,1,0.754355,"rd word senses (i.e., frames) for the target words and the participants had to perform role recognition/labeling and null instantiation linking, and a NI only task, in which the test set was already annotated with gold standard semantic argument structures and the participants only had to recognize definite null instantiations and find links to antecedents in the wider context (NI linking). However, it turned out that the basic semantic role labeling task was already quite challenging for our data set. Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al., 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available. In our case the difficulty was increased because our data came from a new genre and domain (i.e., crime fiction, see Section 3.2). Hence, we decided to add standard SRL, i.e., role recognition and labeling, as a third task (SRL only). This task did not involve NI linking. The theory of null complementation used here is the one adopted by FrameNet, which derives from the work of Fillmore (1986).3 Briefly, omissions of core arguments of predicates are categoriz"
S10-1008,erk-pado-2004-powerful,0,0.102782,"Missing"
S10-1008,J02-3001,0,0.0764846,"scourse entities or events. In the shared task we looked at one particular aspect of cross-sentence links between argument structures, namely linking locally uninstantiated roles to their co-referents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications, such as information extraction, question answering or text summarization. 1 Introduction Semantic role labeling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. However, semantic role labeling as it is currently defined misses a lot of information due to the fact that it is viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded. This view of SRL as a sentence-internal task is partly due to the fact that large-scale manual annotation 1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalm"
S10-1008,P86-1004,1,0.741997,"fer and Caroline Sporleder Roser Morante Computational Linguistics CNTS Saarland University University of Antwerp {josefr,csporled}@coli.uni-sb.de Roser.Morante@ua.ac.be Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu Collin Baker ICSI Berkeley, CA 94704 collin@icsi.berkeley.edu projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and tendencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semantic relations between the predicates involved. Two notable exceptions are Fillmore and Baker (2001) and Burchardt et al. (2005). Fillmore and Baker (2001) analyse a short newspaper article and discuss how frame semantics could benefit discourse processing but without making concrete suggestions"
S10-1008,W09-2417,1,0.499792,"Missing"
S10-1008,H86-1011,1,\N,Missing
W08-0122,P04-1035,0,0.0146039,"a target respects the co-reference but it also results in incorrect conclusions: the speech recognition is an alternative to having both speech recognition and buttons. (7) A:: One thing is interesting is talking about speech recognition in a remote control... D:: ... So that we don’t need any button on the remote control it would be all based on speech. A:: ... I think that would not work so well. You wanna have both options. 5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004)) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. Additionally, in our scheme opinions that are not in the immediate context may be allowed to influence the interpretation of a given opinion via target chains. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can"
W08-0122,passonneau-2004-computing,0,0.0444751,"ging is very high. This confirms our hypothesis that Sentiment and Arguing can be reliably distinguished once the opinion spans are known. Our polarity detection task shows an improvement in κ over a similar polarity assignment task by Wilson et al. (2005) for the news corpus (κ of 0.72). We believe this improvement can partly be attributed to the target information available to our annotators. 4.4 Target Linking As an intuitive first step in evaluating target linking, we treat target links in the discourse similarly to anaphoric chains and apply methods developed for co-reference resolution (Passonneau, 2004) for our evaluation. Passonneau’s method is based on Krippendorf’s α metric (Krippendorff, 2004) and allows for partial matches between anaphoric chains. In addition to this, we evaluate links identified by both annotators for the type (same / alternative) labeling task with the help of the κ metric. Passonneau (2004) reports that in her co-reference task on spoken monologs, α varies with the difficulty of the corpus (from 0.46 to 0.74). This is true in our case too. Table 6 shows our agreement for the four types of meetings in the AMI corpus: the kickoff meeting (a), the functional design (b)"
W08-0122,W06-0305,0,0.0127562,"nce. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification. The contrastive cue indicates a change in the sentiment polarity. In our scheme, their aspects would be related as same and their high contrast relations would result in frames such as SPSNsame, SNSPsame. Additionally, our frame relations would link sentiments across non-adjacent clauses, and make connections via alt target relations. Considering the discourse relation annotations in the PDTB (Prasad et al., 2006), there can be alignment between discourse relations (like contrast) and our opinion frames when the frames represent dominant relations between two clauses. However, when the relation between opinions is not the most prominent one between two clauses, the discourse relation may not align with the opinion frames. And when an opinion frame is between two opinions in the same clause, there would be no discourse relation counterpart at all. Further, opinion frames assume particular intentions that are not necessary for the establishment of ostensibly similar discourse relations. For example, we m"
W08-0122,W03-0404,1,0.716835,"speech recognition as a target respects the co-reference but it also results in incorrect conclusions: the speech recognition is an alternative to having both speech recognition and buttons. (7) A:: One thing is interesting is talking about speech recognition in a remote control... D:: ... So that we don’t need any button on the remote control it would be all based on speech. A:: ... I think that would not work so well. You wanna have both options. 5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004)) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. Additionally, in our scheme opinions that are not in the immediate context may be allowed to influence the interpretation of a given opinion via target chains. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that di"
W08-0122,N07-1038,0,0.0392226,"predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. Additionally, in our scheme opinions that are not in the immediate context may be allowed to influence the interpretation of a given opinion via target chains. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification. The contrastive cue indicates a change in the sentiment polarity. In our scheme, their aspects would be related as same and their high contrast relations would result in frames such as SPSNsame, SNSPsame. Additionally, our frame relations would link sentiments across non-adjacent clauses, and make connections via alt target relations. Considering the discourse relation annotations in the PDTB (Prasad et al., 2006), there can"
W08-0122,2007.sigdial-1.5,1,0.928388,"l votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. Additionally, in our scheme opinions that are not in the immediate context may be allowed to influence the interpretation of a given opinion via target chains. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification. The contrastive cue indicates a change in the sentiment polarity. In our scheme, their aspects would be related as same and their high contrast relations would result in frames such as SPSNsame, SNSPsame. Additionally, our frame relations would link sentiments across non-adjacent clauses, and make connections via alt target relations. Considering the discourse relation annotations in the PDTB (Prasad et al., 2006), there can"
W08-0122,W06-1639,0,0.0256134,"nition is an alternative to having both speech recognition and buttons. (7) A:: One thing is interesting is talking about speech recognition in a remote control... D:: ... So that we don’t need any button on the remote control it would be all based on speech. A:: ... I think that would not work so well. You wanna have both options. 5 Related Work Evidence from the surrounding context has been used previously to determine if the current sentence should be subjective/objective (Riloff et al., 2003; Pang and Lee, 2004)) and adjacency pair information has been used to predict congressional votes (Thomas et al., 2006). However, these methods do not explicitly model the relations between opinions. Additionally, in our scheme opinions that are not in the immediate context may be allowed to influence the interpretation of a given opinion via target chains. Polanyi and Zaenen (2006), in their discussion on contextual valence shifters, have also observed the phenomena described in this work - namely that a central topic may be divided into subtopics in order to perform evaluations, and that discourse structure can influence the overall interpretation of valence. Snyder and Barzilay (2007) combine an agreement m"
W08-0122,W05-0308,1,0.928307,"SNSPsame, APANsame, ANAPsame, SPANsame, APSNsame, SNAPsame, ANSPsame, SPSPalt, SNSNalt, APAPalt, ANANalt, SPAPalt, SNANalt, APSPalt, ANSNalt Opinion frames are presented in Section 2, our annotation scheme is described in Section 3, the interannotator agreement studies are presented in Section 4, related work is discussed in Section 5, and conclusions are in Section 6. 2 Opinion Frames 2.1 Table 1: Opinion Frames Introduction The components of opinion frames are individual opinions and the relationships between their targets. We address two types of opinions, sentiment and arguing. Following (Wilson and Wiebe, 2005; Somasundaran et al., 2007), sentiment includes positive and negative evaluations, emotions, and judgments, while arguing includes arguing for or against something, and arguing that something should or should not be done. In our examples, the lexical anchors revealing the opinion type (as the words are interpreted in context) are indicated in bold face. In addition, the text span capturing the target of the opinion (again, as interpreted in context) is indicated in italics. (2) D:: . . . this kind of rubbery material, it’s a bit more bouncy, like you said they get chucked around a lot. A bit"
W08-0122,H05-1044,1,0.0655995,"Missing"
W09-2417,S07-1018,1,0.851873,"nce resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank,"
W09-2417,W04-2412,0,0.0812467,"Missing"
W09-2417,W05-0620,0,0.313707,"Missing"
W09-2417,J02-3001,0,0.0966796,"s sentence boundaries. Specifically, the task aims at linking locally uninstantiated roles to their coreferents in the wider discourse context (if such co-referents exist). This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inf"
W09-2417,W04-0803,0,0.0306698,"so from co-reference resolution and information extraction. 1 Introduction Semantic role labelling (SRL) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate (Gildea and Jurafsky, 2002). Semantic roles describe the function of the participants in an event. Identifying the semantic roles of the predicates in a text allows knowing who did what to whom when where how, etc. SRL has attracted much attention in recent years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (B"
W09-2417,S07-1008,0,0.0681195,"Missing"
W09-2417,C08-1084,1,0.879566,"Missing"
W09-2417,P86-1004,1,0.770233,"he fact that large-scale manual annotation projects such as FrameNet1 and PropBank2 typically present their annotations lexicographically by lemma rather than by source text. Furthermore, in the case of FrameNet, the annotation effort did not start out with the goal of exhaustive corpus annotation but instead focused on isolated instances of the target words sampled from a very large corpus, which did not allow for a view of the data as ‘full-text annotation’. It is clear that there is an interplay between local argument structure and the surrounding discourse (Fillmore, 1977). In early work, Palmer et al. (1986) discussed filling null complements from context by using knowledge about individual predicates and ten1 http://framenet.icsi.berkeley.edu/ http://verbs.colorado.edu/˜mpalmer/ projects/ace.html 2 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 106–111, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics dencies of referential chaining across sentences. But so far there have been few attempts to find links between argument structures across clause and sentence boundaries explicitly on the basis of semanti"
W09-2417,D07-1002,0,0.0290019,"in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic information. This view of SRL as a sentence-"
W09-2417,P03-1002,0,0.0620561,"years, as witnessed by several shared tasks in Senseval/SemEval (M`arquez et al., 2007; Litkowski, 2004; Baker et al., 2007; Diab et al., 2007), and CoNLL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005; Surdeanu et al., 2008). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures 106 Martha Palmer Department of Linguistics University of Colorado at Boulder martha.palmer@colorado.edu can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). However, semantic role labelling as it is currently defined also misses a lot of information that would be beneficial for NLP applications that deal with text understanding (in the broadest sense), such as information extraction, summarisation, or question answering. The reason for this is that SRL has traditionally been viewed as a sentence-internal task. Hence, relations between different local semantic argument structures are disregarded and this leads to a loss of important semantic"
W09-2417,W08-2121,0,0.0907887,"Missing"
W09-2417,S07-1017,1,\N,Missing
W09-2417,H86-1011,1,\N,Missing
W09-2417,erk-pado-2004-powerful,0,\N,Missing
W09-3003,W04-3202,0,0.070424,"Missing"
W09-3003,brants-plaehn-2000-interactive,0,0.0682926,"Missing"
W09-3003,burchardt-etal-2006-salto,0,0.0276165,"Missing"
W09-3003,H01-1026,0,0.0727584,"Missing"
W09-3003,W06-0602,0,0.0520171,"Missing"
W09-3003,W07-1509,0,0.0260804,"Missing"
W09-3003,J93-2004,0,0.0329044,"ion and for Shalmaneser error types are made by human annotators throughout all three annotation trials, and that these errors are different from the ones made by the ASRL. Indicated by f-score, the most difficult frames in our data set are Scrutiny, Fluidic motion, Seeking, Make noise and Communication noise. This shows that automatic pre-annotation, even if noisy and of low quality, does not corrupt human annotators on a grand scale. Furthermore, if the preannotation is good it can even improve the overall annotation quality. This is in line with previous studies for other annotation tasks (Marcus et al., 1993). 4.3 pre-annotation decreases annotation quality. Most interestingly, the two annotators who showed a decrease in f-score on the text segments pre-annotated by Shalmaneser (compared to the text segments with no pre-annotation provided) had been assigned to the same group (Group I). Both had first annotated the enhanced, high-quality pre-annotation, in the second trial the sentences pre-annotated by Shalmaneser, and finally the texts with no pre-annotation. It might be possible that they benefitted from the ongoing training, resulting in a higher f-score for the third text segment (no pre-anno"
W09-3003,P02-1045,0,0.0339238,"cquisition bottleneck is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly 19 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP frame typically also requires fewer physical operations from the annotator than correcting a number of wrongly assigned frame elements. We aim to answer three research questions in our study: First, we explore whether pre-annotation of frame labels can indeed speed up the annotation process. This question is important because frame as"
W09-3003,W03-1015,0,0.0167118,"is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly 19 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP frame typically also requires fewer physical operations from the annotator than correcting a number of wrongly assigned frame elements. We aim to answer three research questions in our study: First, we explore whether pre-annotation of frame labels can indeed speed up the annotation process. This question is important because frame assignment, in terms of"
W09-3003,C02-1145,0,0.0741674,"Missing"
W09-3003,P98-1013,0,0.00732122,"number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem from the data creation side. Specifically we explore whether a semi-automatic annotation set-up in which a human expert corrects the output of an automatic system can help to speed up the annotation process without sacrificing annotation quality. For our study, we explore the task of framesemantic argument structure annotation (Baker et al., 1998). We chose this particular task because it is a rather complex – and therefore time-consuming – undertaking, and it involves making a number of different but interdependent annotation decisions for each instance to be labeled (e.g. frame assignment and labeling of frame elements, see Section 3.1). Semi-automatic support would thus be of real benefit. More specifically, we explore the usefulness of automatic pre-annotation for the first step in the annotation process, namely frame assignment (word sense disambiguation). Since the available inventory of frame elements is dependent on the chosen"
W09-3003,C98-1013,0,\N,Missing
W12-3716,P98-1013,0,0.396438,"d Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1998) offering a novel representation for sentiment analysis based on frame semantics. 2 Shallow and pragmatic approaches Current approaches to sentiment analysis are mainly pragmatically oriented, without giving equal weight to semantics. One aspect concerns the identification of sentiment-bearing expressions. The annotations in the MPQA corpus (Wiebe et al., 2005), for instance, were created without limiting what annotators can annotate in terms of syntax or lexicon. While this serves the spirit of discovering the variety of opinion expressions in actual contexts, it makes it difficult to match o"
W12-3716,W11-1707,0,0.0253401,"ion for sentiment analysis that is tailored to these aims. 1 Introduction Sentiment analysis has made a lot of progress on more coarse-grained analysis levels using shallow techniques. However, recent years have seen a trend towards more fine-grained and ambitious analyses requiring more linguistic knowledge and more complex statistical models. Recent work has tried to produce relatively detailed summaries of opinions expressed in news texts (Stoyanov and Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1998) offering a novel representation for sentiment analysis based on frame semantics. 2 Shallow and pragmatic approaches Current approaches to senti"
W12-3716,R11-1060,0,0.0156682,"t, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims. 1 Introduction Sentiment analysis has made a lot of progress on more coarse-grained analysis levels using shallow techniques. However, recent years have seen a trend towards more fine-grained and ambitious analyses requiring more linguistic knowledge and more complex statistical models. Recent work has tried to produce relatively detailed summaries of opinions expressed in news texts (Stoyanov and Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1998) offering a novel representation for sentiment analysis based on frame semantics. 2 Shallow"
W12-3716,W06-0301,0,0.105245,"ons are tied to one expression and where presuppositions and temporal structure come into play. An example is the verb despoil: there is a positive opinion by the reporter about the despoiled entity in its former state, a negative opinion about its present state, and (inferrable) negative sentiment towards the despoiler. In most resources, the positive opinion will not be represented. The most common approach to the task is an information extraction-like pipeline. Expressions of opinion, sources and targets are often dealt with separately, possibly using separate resources. Some work such as (Kim and Hovy, 2006) has explored 105 the connection to role labeling. One reason not to pursue this is that “in many practical situations, the annotation beyond opinion holder labeling is too expensive” (Wiegand, 2010, p.121). (Shaikh et al., 2007) use semantic dependencies and composition rules for sentence-level sentiment scoring but do not deal with source and target extraction. The focus on robust partial solutions, however, prevents the creation of an integrated high-quality resource. 3 The extended frame-semantic approach We now sketch a view of sentiment analysis on the basis of an appropriately extended"
W12-3716,W11-1702,0,0.0123826,"as described above enables us to readily represent multiple opinions. For instance, the verb brag in the modified Bragging frame has two opinion frames. The first one has positive polarity and represents the frame-internal point of view. The S PEAKER is the Source relative to the T OPIC as the Target. The second opinion frame has negative polarity, representing the reporter’s point of view. The S PEAKER is the Target but the Source is unspecified, indicating that it needs to be resolved to an embedded source. For a similar representation of multiple opinions in a Dutch lexical resource, see (Maks and Vossen, 2011). Event structure and presuppositions A complete representation of subjectivity needs to include event and presuppositional structure. This is necessary, for instance, for predicates like come around (on) in (1), which involve changes of opinion relative to the same target by the same source. Without the possibility of distinguishing between attitudes held at different times, the sentiment associated with these predicates cannot be modeled adequately. (1) Newsom is still against extending weekday metering to evenings, but has COME AROUND on Sunday enforcement. For come around (on), we want to"
W12-3716,ruppenhofer-etal-2008-finding,1,0.879942,"igh-quality resource. 3 The extended frame-semantic approach We now sketch a view of sentiment analysis on the basis of an appropriately extended model of frame semantic representation.1 Link to semantic frames and roles Since the possible sources and targets of opinion are usually identical to a predicate’s semantic roles, we add opinion frames with slots for Source, Target, Polarity and Intensity to the FrameNet database. We map the Source and Target opinion roles to semantic roles as appropriate, which enables us to use semantic role labeling systems in the identification of opinion roles (Ruppenhofer et al., 2008). In SentiFrameNet all lexical units (LUs) that are inherently evaluative are associated with opinion frames. The language of polar facts is not associated with opinion frames. However, we show in the longer version of this paper (cf. footnote 1) how we support certain types of inferred sentiment. With regard to targets, our representation selects as targets of opinion the target spans of (Stoyanov and Cardie, 2008) rather than their opinion topics (see Section 2). For us, opinion topics that do not coincide with target spans are inferential opinion targets. Formal diversity of opinion express"
W12-3716,C08-1103,0,0.119217,"the subjectivity clues extracted are inherently evaluative or Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 104–109, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics merely associated with statements of polar fact. Pragmatic considerations also lead to certain expressions of sentiment or opinion being excluded from analysis. (Seki, 2007), for instance, annotated sentences as “not opinionated” if they contain indirect hearsay evidence or widely held opinions. In the case of targets, the work by (Stoyanov and Cardie, 2008) exhibits a pragmatic focus as well. These authors distinguish between (a) the topic of a fine-grained opinion, defined as the real-world object, event or abstract entity that is the subject of the opinion as intended by the opinion holder; (b) the topic span associated with an opinion expression is the closest, minimal span of text that mentions the topic; and (c) the target span defined as the span of text that covers the syntactic surface form comprising the contents of the opinion. As the definitions show, (Stoyanov and Cardie, 2008) focus on text-level, pragmatic relevance by paying atten"
W12-3716,R11-1028,0,0.0607944,"mbining ressources to create synergies with related work in NLP. In the paper, we propose SentiFrameNet, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims. 1 Introduction Sentiment analysis has made a lot of progress on more coarse-grained analysis levels using shallow techniques. However, recent years have seen a trend towards more fine-grained and ambitious analyses requiring more linguistic knowledge and more complex statistical models. Recent work has tried to produce relatively detailed summaries of opinions expressed in news texts (Stoyanov and Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1"
W12-3716,P10-1059,0,0.0732426,"treated differently. A similar challenge lies in distinguishing so-called polar facts from inherently sentiment-bearing expressions. For example, out of context, one would not associate any of the words in the sentence Wages are high in Switzerland with a particular evaluative meaning. In specific contexts, however, we may take the sentence as reason to either think positively or negatively of Switzerland: employees receiving wages may be drawn to Switzerland, while employers paying wages may view this state of affairs negatively. As shown by the inter-annotator agreement results reported by (Toprak et al., 2010), agreement on distinguishing polar facts from inherently evaluative language is low. Unsurprisingly, many efforts at automatically building up sentiment lexica simply harvest expressions that frequently occur as part of polar facts without resolving whether the subjectivity clues extracted are inherently evaluative or Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 104–109, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics merely associated with statements of polar fact. Pragmatic considerations al"
W12-3716,P06-1134,0,0.0607214,"rdie, 2008) focus on text-level, pragmatic relevance by paying attention to what the author intends, rather than concentrating on the explicit syntactic dependent (their target span) as the topic. This pragmatic focus is also in evidence in (Wilson, 2008)’s work on contextual polarity classification, which uses features in the classification that are syntactically independent of the opinion expression such as the number of subjectivity clues in adjoining sentences. Among lexicon-driven approaches, we find that despite arguments that word sense distinctions are important to sentiment analysis (Wiebe and Mihalcea, 2006), often-used resources do not take them into account and new resources are still being created which operate on the more shallow lemma-level (e.g. (Neviarouskaya et al., 2009)). Further, most lexical resources do not adequately represent cases where multiple opinions are tied to one expression and where presuppositions and temporal structure come into play. An example is the verb despoil: there is a positive opinion by the reporter about the despoiled entity in its former state, a negative opinion about its present state, and (inferrable) negative sentiment towards the despoiler. In most resou"
W12-3716,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W12-3716,C98-1013,0,\N,Missing
W13-0111,S10-1059,0,0.523964,"“[You]Src ’re rightCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences for just a few predi"
W13-0111,P10-1160,0,0.511495,"oken by a different speaker, namely Holmes), which provides details of the fact about which Holmes was right. (1) [“A white cock,”]Stim said [he]Exp . “[Most]Deg interestingMssf !” (2) A. [“Your powers seem superior to your opportunities.”]Inf B. “[You]Src ’re rightCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven Prop"
W13-0111,J12-4003,0,0.151593,"nal results we report are for the unseen Hound data (the test set in SemEval). 5 Modeling NI Resolution While the complete NI resolution task consists of three steps, detecting NIs, classifying NIs as DNIs or INIs, and resolving DNIs, in this paper, we focus exclusively on the third task as this is by far the most difficult one. We model the problem as a weakly supervised task, where the only type of supervision is the use of a corpus annotated with overtly realised semantic roles. We do not make use of the NI annotations in the training set. This distinguishes our work from the approaches by Gerber and Chai (2012; 2010) and Silberer and Frank (2012). However, like these two we employ an entity mention model, that is, we take into account the whole coreference chain for a discourse entity when assessing its likelihood of filling a null instantiated role. For this, we make use of the gold standard coreference chains in the SemEval data. So as not to have an unfair advantage, we also create singleton chains for all noun phrases without an overt co-referent, since such cases could, in theory, be antecedents for omitted arguments. Finally, since NIs can also refer to complete sentences, we augment the enti"
W13-0111,S10-1008,1,0.624077,"our opportunities.”]Inf B. “[You]Src ’re rightCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences f"
W13-0111,S12-1001,0,0.460718,"labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences for just a few predicates. This makes the latter data set easier as there are few"
W13-0111,S10-1065,0,0.270962,"tCorr , Mr. Holmes.” Semantic role labeling (SRL) systems typically only label arguments that are locally realised (e.g., within the maximal projection of the target predicate); they tacitly ignore all roles that are not instantiated locally. Previous attempts to resolve null instantiated arguments have obtained mixed results. While Gerber and Chai (2010, 2012) obtain reasonable results for NI resolution within a restricted PropBankbased scenario, the accuracies obtained on the FrameNet-based data set provided for the SemEval 2010 1 Shared Task 10 (Ruppenhofer et al., 2010; Chen et al., 2010; Tonelli and Delmonte, 2010, 2011; Silberer and Frank, 2012) are much lower. This has two reasons: Semantic role labelling in the FrameNet framework is generally harder than in the PropBank framework, even for overt arguments, due to the fact that FrameNet roles are much more grounded in semantics as opposed to the shallower, more syntacticallydriven PropBank roles. Second, the SemEval 2010 data set consists of running text in which null instantiations are marked and resolved, while the data set used by Gerber and Chai (2010, 2012) consists of annotated examples sentences for just a few predicates. This makes the latter"
W13-0111,W11-0908,0,0.786859,"Missing"
W14-4721,W08-1301,0,0.032932,"Missing"
W14-4721,W13-0906,0,0.284313,"Introduction Figurative language plays an important role in “grounding” our communication in the world around us. Being able to talk metaphorically about “the journey of life”, “getting into a relationship”, whether there are “strings attached” to a contract, or even just “surfing the internet”, are important and useful aspects of everyday discourse. Recent work on such phenomena has pursued this kind of grounding in interesting directions, in particular, treating it as a way of injecting meanings that are somehow more “concrete” into daily discourse (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2013), or else as a way of expressing abstract ideas in terms of concepts that are more “imageable”, where imageability can be defined as how easily a word can evoke mental imagery, (Cacciari and Glucksberg, 1995; Gibbs, 2006; Urena and Faber, 2010). It should be noted that while it is generally accepted that imageability and concreteness are highly correlated, recent work has shown they are contrastive, in particular, in their interaction with additional cognitive dimensions such as affective states, so that they “can no longer be considered interchangeable constructs” (Dellantonio et al., 2014)."
W14-4721,D11-1063,0,0.210801,"taphorical or not. 1 Introduction Figurative language plays an important role in “grounding” our communication in the world around us. Being able to talk metaphorically about “the journey of life”, “getting into a relationship”, whether there are “strings attached” to a contract, or even just “surfing the internet”, are important and useful aspects of everyday discourse. Recent work on such phenomena has pursued this kind of grounding in interesting directions, in particular, treating it as a way of injecting meanings that are somehow more “concrete” into daily discourse (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2013), or else as a way of expressing abstract ideas in terms of concepts that are more “imageable”, where imageability can be defined as how easily a word can evoke mental imagery, (Cacciari and Glucksberg, 1995; Gibbs, 2006; Urena and Faber, 2010). It should be noted that while it is generally accepted that imageability and concreteness are highly correlated, recent work has shown they are contrastive, in particular, in their interaction with additional cognitive dimensions such as affective states, so that they “can no longer be considered interchangeable constructs” (Del"
W15-2910,E12-1039,0,0.01496,"re AN represents the accusative object and BM (adverbial of manner) covers the adjectival secondary predicate. This information can be used to, for instance, compose the polarity of the sentence in (4) as follows. First, we compute if ‘me/I fear for my Underlying linguistic model of effects Let us consider the prototypical case of predicates relevant for opinion inference, namely ones that involve a causal event (Cause) that brings about a resulting event, the (Effect). A clear example of such a predicate is produce in (3). 6 Further example sentences are available through the WebCage corpus (Henrich et al., 2012) which, however, lack explicit syntactic frame information. We only use these additional sentences to ascertain the relevant meaning. 70 phrase (AN). Note that the order matters: the (animate) Recipient’s state changes more saliently by coming into possession of the (inanimate) Theme than the other way around. For (7), we reason with Reschke and Anand (2011) that it’s good if a person we like has something good. Assuming we like our mothers, the possession of the valuable painting is good. Since agents and causes get credit and blame for the good and bad things they bring about, Bill is evalua"
W15-2910,P98-1013,0,0.106204,"on classes of verbs assuming that verb classes such as verbs of creation behave consistently due to lexical entailments about their arguments. They focus on three prominent kinds of entailments: ones related to possession, existence and affectedness. Reschke and Anand (2011) test the correctness of the predictions generated by their theory by annotating actual corpus instances. They do not evaluate the agreement on the presence of the lexical entailments for predicates themselves. Instead, they simply identified the verbs having particular lexical entailments by inspecting FrameNet’s frames (Baker et al., 1998; Ruppenhofer et al., 2010) and its hierarchy. While acquisition or validation of a large lexicon was not the aim of Reschke and Anand (2011), it is the focus of later work by Choi and Wiebe (2014) who seek to generate EffectWordNet, a large lexical resource based on WordNet, in which information relevant for opinion inferences is recorded. The notion to which the annotation of the WordNet word senses appeals is that of effect rather than the more specific entailments used by Reschke and Anand (2011). The idea is that in order to determine a nested source’s view on an event, one first needs to"
W15-2910,P10-1023,0,0.0108824,"d senses in GermaNet V9.0 (Hamp and Feldweg, 1997), the sister resource to WordNet for German, that can serve both as training / seed data for automatic methods for graph-based label propagation, and also as a gold standard for evaluating automatic methods. In picking German synsets to annotate, we made use of the work done by the EffectWordNet group. We extracted all 745 synsets from the EffectWordNet gold standard that were annotated as either +effect or -effect. We omitted all synsets annotated as Null in the source synsets. We then retrieved 273 corresponding German synsets from BabelNet (Navigli and Ponzetto, 2010) on the basis of the WordNet synset IDs. Using the German lemma information and the POS information in BabelNet, we next extracted 998 unique synsets from GermaNet that contained any word senses for the lemmas found in the 273 BabelNet synsets. After expanding the set again based on lemmas found in GermaNet but not in BabelNet, we obtained 1492 GermaNet synsets. As we will show in §4.2, our annotation scheme c the paraphrases, which help us understand the intended senses of the lemmas (italics); d and the example sentences (lines beginning with “# GermaNet”). Unlike in (Effect)WordNet, each ex"
W15-2910,burchardt-etal-2006-salsa,0,0.188904,"Missing"
W15-2910,W11-0145,0,0.101434,"rmation has previously been referred to as effect or couched in terms of eventevaluation functors. We extend the theory and present an extensive scheme that combines both approaches and thus extends the set of inference-relevant predicates. Using these guidelines to annotate 726 German synsets, we achieve good inter-annotator agreement. 1 Introduction In recent years, there has been increasing interest in inferring implicit opinions in addition to capturing explicit expressions of opinion. A series of papers by Reschke and Anand as well as Wiebe and her collaborators (Anand and Reschke, 2010; Reschke and Anand, 2011; Deng et al., 2013; Wiebe and Deng, 2014) has shown the great potential of opinion inference: speakers and authors leave many implicit opinions for hearers to infer. While these additional inferred opinions involve sentences or clauses that bear no explicit sentiment at all, they very often interact with sentences or clauses that do bear explicit sentiment, as in example (1). (1) • We extend the range of predicates covered, relative to Choi and Wiebe (2014). • We provide a typology of effect-relevant predicates in terms of lexical decomposition. She is disappointed that Peter is happy because"
W15-2910,D14-1125,0,0.0670511,"iment expressions, the acquisition of information that is relevant for opinion inference is in its infancy, by comparison. In this paper, we report on an effort to manually annotate effect-relevant predicates in GermaNet (Hamp and Feldweg, 1997), a WordNet-like (Fellbaum, 1998) electronic lexical resource for German. The purpose of annotating the word senses of lemmas that have at least some effect-entailing senses is to construct a gold standard for evaluating automatic systems that provide a complete automatic annotation of the senses in the resource via label propagation along the lines of Choi and Wiebe (2014). Here we focus on the following contributions of our work: In this contribution, we report on an effort to annotate German data with information relevant to opinion inference. Such information has previously been referred to as effect or couched in terms of eventevaluation functors. We extend the theory and present an extensive scheme that combines both approaches and thus extends the set of inference-relevant predicates. Using these guidelines to annotate 726 German synsets, we achieve good inter-annotator agreement. 1 Introduction In recent years, there has been increasing interest in infer"
W15-2910,W14-2618,0,0.220045,"Missing"
W15-2910,P13-2022,0,0.304993,"en referred to as effect or couched in terms of eventevaluation functors. We extend the theory and present an extensive scheme that combines both approaches and thus extends the set of inference-relevant predicates. Using these guidelines to annotate 726 German synsets, we achieve good inter-annotator agreement. 1 Introduction In recent years, there has been increasing interest in inferring implicit opinions in addition to capturing explicit expressions of opinion. A series of papers by Reschke and Anand as well as Wiebe and her collaborators (Anand and Reschke, 2010; Reschke and Anand, 2011; Deng et al., 2013; Wiebe and Deng, 2014) has shown the great potential of opinion inference: speakers and authors leave many implicit opinions for hearers to infer. While these additional inferred opinions involve sentences or clauses that bear no explicit sentiment at all, they very often interact with sentences or clauses that do bear explicit sentiment, as in example (1). (1) • We extend the range of predicates covered, relative to Choi and Wiebe (2014). • We provide a typology of effect-relevant predicates in terms of lexical decomposition. She is disappointed that Peter is happy because the Colts LOST. •"
W15-2910,D11-1101,0,0.0130444,"nt that is affected. In (1), we can infer, for instance, that Peter, the (nested) source of the explicit sentiment (in boldface) towards the event as a whole, is also negative towards the Colts football team given that the event affected them negatively (in small caps). As laid out in great detail by Wiebe and Deng (2014), given partial knowledge about explicit subjective expressions, sources’ attitudes about participants and knowledge about the effects of events on their • We report inter-annotator agreement results for labeling lexical entries with argument1 Work on connotation detection by Feng et al. (2011) can be seen as addressing the issue of determining sources’ attitudes in those cases when they are aligned with their stereotypical attitudes within particular (discourse) communities or cultures. 67 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 67–76, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. specific effect information, whereas so far only agreement on annotations of corpus instances has been studied. Reschke and Anand (2011) focus on classes of verbs assuming"
W15-2910,W97-0802,0,0.629716,"r that ‘she’ potentially holds the opposite opinion towards the Colts from Peter, given that she is disappointed at his attitude. Although opinion inference is a pragmatic process, it relies on rich lexical knowledge of subjective expressions and of predicates, which entail some kind of effect.1 While a great deal of effort has been devoted to the acquisition of explicit sentiment expressions, the acquisition of information that is relevant for opinion inference is in its infancy, by comparison. In this paper, we report on an effort to manually annotate effect-relevant predicates in GermaNet (Hamp and Feldweg, 1997), a WordNet-like (Fellbaum, 1998) electronic lexical resource for German. The purpose of annotating the word senses of lemmas that have at least some effect-entailing senses is to construct a gold standard for evaluating automatic systems that provide a complete automatic annotation of the senses in the resource via label propagation along the lines of Choi and Wiebe (2014). Here we focus on the following contributions of our work: In this contribution, we report on an effort to annotate German data with information relevant to opinion inference. Such information has previously been referred t"
W15-2910,C98-1013,0,\N,Missing
W15-2921,baccianella-etal-2010-sentiwordnet,0,0.00662151,"show that, even with the PropBank-like semantic roles (i.e. agent, patient1 ) assigned to the entities, one may not be able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applaud, belongs to a group with"
W15-2921,P98-1013,0,0.313186,"Missing"
W15-2921,D14-1125,0,0.0323956,"Missing"
W15-2921,N12-1086,0,0.023781,"Missing"
W15-2921,N10-1138,0,0.0264134,"on verb is presented in (4) and (5) where two viewpoints are evoked by the same verb in the same sentence. (4) denotes the sentiment view of Peter towards Mary while (5) represents the sentiment view of Mary towards Peter (i.e. Peter made Mary feel better). One resource that has previously been examined for this task is FrameNet (Baker et al., 1998). The idea is to identify in frames (which predominantly contain opinion expressions) those frame elements that typically contain either opinion holders or opinion targets. Once this mapping has been established, a FrameNet-parser, such as Semafor (Das et al., 2010), could be used to automatically recognize frame structures in natural language text. By consulting the mapping from frame elements to opinion roles, specific opinion roles could be extracted. Kim and Hovy (2006) followed this approach for a set of opinion verbs and adjectives. Thus, they were able to correctly resolve some problems which cannot be solved with the help of syntactic parsing or PropBank-like semantic roles, such as the role distinctions in (1) and (2). For instance, while the opinion holders in (6) and (7) map to the same frame element EXPERIENCER, the PropBank-like semantic rol"
W15-2921,N15-1146,0,0.0279268,"y focus on the relationship between opinion roles and their syntactic argument realization. Previous work hardly addressed this issue since either little variation between opinion roles and their syntactic arguments was perceived on the corpora on which this task was examined, or there were other domain-specific properties that could be used in order to extract opinion roles correctly without the knowledge about opinion role realization. Currently, there exists only one commonly accepted corpus for English containing manual annotation of both opinion holders and targets, i.e. the MPQA corpus (Deng and Wiebe, 2015). Apart from that, not a single lexical resource for that specific task is available. Moreover, there does not exist any publicly available tool that supports both opinion holder and target extraction. Typical applications, such as opinion summarization, however, require both components simultaneously (Stoyanov and Cardie, 2011). These facts indicate that there definitely needs to be more research on the task of opinion role extraction. In order to stimulate more research in this direction, we present a verb-based corpus for opinion role extraction. The difference to previous datasets is that"
W15-2921,erk-pado-2004-powerful,0,0.102974,"Missing"
W15-2921,esuli-sebastiani-2006-sentiwordnet,0,0.0165852,"ders and targets with the help of the WordNet ontology graph. One common way of doing so would be the application of some bootstrapping method in which one defines seed opinion verbs with distinct selectional preferences (for instance, one defines as one group opinion verbs that take agents as opinion holders, such as dislike, as another group verbs that take patients as opinion holders, such as disappoint, and so on) and propagate their labels to the remaining opinion verbs via the WordNet graph. Such bootstrapping on WordNet has been effectively used for the induction of sentiment lexicons (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009) or effect predicates (Choi and Wiebe, 2014). It relies on a good similarity metric in order to propagate the labels from labeled seed words to unlabeled words. We experimented with the metrics in WordNet::Similarity (Pedersen et al., 2004) and found that the opinion verbs most similar to a specified opinion verb do not necessarily share the same syntactic properties. For example, Table 2 lists the 12 opinion verbs most similar to outrage and please, which are typical opinion verbs that take an opinion holder in patient position and an opinion target in agent posit"
W15-2921,D10-1101,0,0.0326016,"at the problem of opinion role extraction can be appropriately evaluated on them. We start by looking at the review domain. 4.1 Why the review domain is not suitable for studying opinion role extraction for verbs There has been a lot of research on the review domain, which also means that there are several datasets from different domains allowing crossdomain sentiment analysis. However, for more indepth opinion role extraction evoked by verb predicates, these types of texts seem to be less suitable – despite the plethora of previous publications on opinion target extraction (Hu and Liu, 2004; Jakob and Gurevych, 2010; Liu et al., 2013b; Liu et al., 2013a; Liu et al., 2014). We identified the following reasons for that: Firstly, the subtask of opinion holder extraction is not really relevant on this text type. Product reviews typically reflect the author’s views on a particular product. Therefore, the overwhelming majority of explicitly mentioned opinion holders agent of verb patient of verb no (direct) relationship 21.8 44.5 33.8 Table 4: Proportion of relationships between opinion targets and opinion verbs in the Darmstadt Service Review Corpus (DSRC). 4 We chose this corpus as a typical representative c"
W15-2921,J13-3002,0,0.0495278,"Missing"
W15-2921,P14-1145,0,0.0149165,"able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applaud, belongs to a group with different linguistic properties than verbs, such as disappoint, interest or frighten. However, the realizations"
W15-2921,W06-0301,0,0.0555054,"wards Peter (i.e. Peter made Mary feel better). One resource that has previously been examined for this task is FrameNet (Baker et al., 1998). The idea is to identify in frames (which predominantly contain opinion expressions) those frame elements that typically contain either opinion holders or opinion targets. Once this mapping has been established, a FrameNet-parser, such as Semafor (Das et al., 2010), could be used to automatically recognize frame structures in natural language text. By consulting the mapping from frame elements to opinion roles, specific opinion roles could be extracted. Kim and Hovy (2006) followed this approach for a set of opinion verbs and adjectives. Thus, they were able to correctly resolve some problems which cannot be solved with the help of syntactic parsing or PropBank-like semantic roles, such as the role distinctions in (1) and (2). For instance, while the opinion holders in (6) and (7) map to the same frame element EXPERIENCER, the PropBank-like semantic roles differ. Unfortunately, the resulting mapping lists from that work are not publicly available. OT (4) [Peter]OH agent consoles [Mary]patient . OH (5) [Peter]OT agent consoles [Mary]patient . These types of sele"
W15-2921,kingsbury-palmer-2002-treebank,0,0.117256,"(Wilson et al., 2005). Our main assumption is that the opinion verbs from that lexicon can be considered a representative choice of all kinds of opinion expressions that exists in the English language. OT (6) [Peter EXPERIENCER ]OH agent dislikes [Mary]patient . OH (7) [Peter]OT agent disappoints [Mary EXPERIENCER ]patient . Table 1 shows some statistics of our opinion verbs with regard to matched frames and frame elements. Considering that there are 615 different frame elements associated to the different frames2 1 By agent and patient, we mean constituents labeled as A0 and A1 in PropBank (Kingsbury and Palmer, 2002). 2 This count conflates frame elements of the same name that occur in different frames. 149 # opinion verbs (from the Subjectivity Lexicon) # opinion verbs with at least one frame # different frames associated with opinion verbs # different frame elements associated with opinion verbs 1175 691 306 615 con are all contained in WordNet). A straightforward solution for using that resource in the current task would be to group opinion verbs that share the same selectional preferences for opinion holders and targets with the help of the WordNet ontology graph. One common way of doing so would be t"
W15-2921,P13-1172,0,0.126589,"at adjectives are much more frequent than verbs. Thirdly, the review domain is typically focused on products, e.g. movies, books, electronic devices etc. This also means that only specific semantic types are eligible for opinion holders and targets, e.g. persons are less likely to be opinion targets. Therefore, much of the research in opinion target extraction relies on entity priors. By that we mean that (supervised) classifiers learn weights for specific entities (typically nouns or noun phrases) of how likely they represent a priori an opinion target (Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2013b; Liu et al., 2014). For example, in the movie domain Psycho is very likely to be an opinion target as will be iPhone in the electronics domain. However, as such features do not transfer to other domains, they distract research efforts from the universally applicable feature of selectional preferences. Table 4, for example, shows the proportion of different relationships between opinion targets and opinion verbs on DSRC. It shows that there is a considerable number of targets in both agent position (14) and patient position (13) & (15). So, it is not trivial to detect opinion targets here. Ho"
W15-2921,P14-1030,0,0.0657429,"uch more frequent than verbs. Thirdly, the review domain is typically focused on products, e.g. movies, books, electronic devices etc. This also means that only specific semantic types are eligible for opinion holders and targets, e.g. persons are less likely to be opinion targets. Therefore, much of the research in opinion target extraction relies on entity priors. By that we mean that (supervised) classifiers learn weights for specific entities (typically nouns or noun phrases) of how likely they represent a priori an opinion target (Zhuang et al., 2006; Qiu et al., 2011; Liu et al., 2013b; Liu et al., 2014). For example, in the movie domain Psycho is very likely to be an opinion target as will be iPhone in the electronics domain. However, as such features do not transfer to other domains, they distract research efforts from the universally applicable feature of selectional preferences. Table 4, for example, shows the proportion of different relationships between opinion targets and opinion verbs on DSRC. It shows that there is a considerable number of targets in both agent position (14) and patient position (13) & (15). So, it is not trivial to detect opinion targets here. However, if one looks"
W15-2921,N04-3012,0,0.0525926,"s that take agents as opinion holders, such as dislike, as another group verbs that take patients as opinion holders, such as disappoint, and so on) and propagate their labels to the remaining opinion verbs via the WordNet graph. Such bootstrapping on WordNet has been effectively used for the induction of sentiment lexicons (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009) or effect predicates (Choi and Wiebe, 2014). It relies on a good similarity metric in order to propagate the labels from labeled seed words to unlabeled words. We experimented with the metrics in WordNet::Similarity (Pedersen et al., 2004) and found that the opinion verbs most similar to a specified opinion verb do not necessarily share the same syntactic properties. For example, Table 2 lists the 12 opinion verbs most similar to outrage and please, which are typical opinion verbs that take an opinion holder in patient position and an opinion target in agent position.3 (They would be plausible candidates for verb seeds for that verb category.) Unfortunately, among the list of similar verbs, we find many opinion verbs which have opinion holder and target in a different argument position, such as hate on the list for outrage: Tab"
W15-2921,J11-1002,0,0.123132,"Missing"
W15-2921,E09-1077,0,0.0350976,"lp of the WordNet ontology graph. One common way of doing so would be the application of some bootstrapping method in which one defines seed opinion verbs with distinct selectional preferences (for instance, one defines as one group opinion verbs that take agents as opinion holders, such as dislike, as another group verbs that take patients as opinion holders, such as disappoint, and so on) and propagate their labels to the remaining opinion verbs via the WordNet graph. Such bootstrapping on WordNet has been effectively used for the induction of sentiment lexicons (Esuli and Sebastiani, 2006; Rao and Ravichandran, 2009) or effect predicates (Choi and Wiebe, 2014). It relies on a good similarity metric in order to propagate the labels from labeled seed words to unlabeled words. We experimented with the metrics in WordNet::Similarity (Pedersen et al., 2004) and found that the opinion verbs most similar to a specified opinion verb do not necessarily share the same syntactic properties. For example, Table 2 lists the 12 opinion verbs most similar to outrage and please, which are typical opinion verbs that take an opinion holder in patient position and an opinion target in agent position.3 (They would be plausibl"
W15-2921,P94-1019,0,0.631708,"Missing"
W15-2921,P13-1161,0,0.0207982,"s unattested. In order to demonstrate that our new corpus is a more suitable resource in order to study selectional preferences (Goal 2) and multiple viewpoint evocation (Goal 3), we prepared some statistics regarding mentions of opinion verbs and their properties in the MPQA corpus and our corpus (denoted by VERB). Due to the unavailability of MPQA 3.0, we had to use MPQA 2.0, whose annotation with regard to opinion targets is incomplete. We therefore compare opinion verbs only with regard to their opinion holders. However, given the strong interrelations between opinion holders and targets (Yang and Cardie, 2013), we think that if it is shown that our corpus better represents the versatility of opinion holders, this should (almost) equally also apply for opinion targets. Table 5 examines the types of argument positions in which an opinion holder is realized. We distinguish between three different roles (already informally introduced in §2): the holder is in agent position (example: dislike), the holder is in patient position (example: disappoint) or the holder is not an argument at all (example: gossip). The latter are cases in which the speaker (or some nested source) is the opinion holder. Table 5 a"
W15-2921,W12-3716,1,0.878384,"Missing"
W15-2921,R11-1028,0,0.0274209,"that could be used in order to extract opinion roles correctly without the knowledge about opinion role realization. Currently, there exists only one commonly accepted corpus for English containing manual annotation of both opinion holders and targets, i.e. the MPQA corpus (Deng and Wiebe, 2015). Apart from that, not a single lexical resource for that specific task is available. Moreover, there does not exist any publicly available tool that supports both opinion holder and target extraction. Typical applications, such as opinion summarization, however, require both components simultaneously (Stoyanov and Cardie, 2011). These facts indicate that there definitely needs to be more research on the task of opinion role extraction. In order to stimulate more research in this direction, we present a verb-based corpus for opinion role extraction. The difference to previous datasets is that it has been sampled in such a way that all opinion verbs of a common sentiment lexicon are widely represented. Previous corpora have a bias towards those opinion expressions that are frequent in a particular domain. We demonstrate on two opinion holder extraction systems that performance on the new corpus massively drops compare"
W15-2921,J11-2001,0,0.0603571,"PropBank-like semantic roles (i.e. agent, patient1 ) assigned to the entities, one may not be able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applaud, belongs to a group with different linguistic p"
W15-2921,P10-1059,0,0.0456984,"Missing"
W15-2921,E12-1033,1,0.929205,"ance, while the opinion holders in (6) and (7) map to the same frame element EXPERIENCER, the PropBank-like semantic roles differ. Unfortunately, the resulting mapping lists from that work are not publicly available. OT (4) [Peter]OH agent consoles [Mary]patient . OH (5) [Peter]OT agent consoles [Mary]patient . These types of selectional preferences (1)-(5) have been observed before including the case of multiple viewpoint evocation (4)-(5), most prominently by Ruppenhofer et al. (2008). Yet little research on opinion role extraction has actually paid attention to this issue. One exception is Wiegand and Klakow (2012) who experiment with an induction approach to distinguish cases like (1) and (2). Nonetheless, datasets and lists of types of opinion verbs have not been publicly released. The above analysis suggests more research on lexical resources is required. In the following, we show that existing resources are not suitable to provide the type of information we are looking for. As a reference of opinion verbs, we use the set of 1175 verbs contained in the Subjectivity Lexicon (Wilson et al., 2005). Our main assumption is that the opinion verbs from that lexicon can be considered a representative choice"
W15-2921,H05-1044,0,0.0745814,"by (1) and (2) which show that, even with the PropBank-like semantic roles (i.e. agent, patient1 ) assigned to the entities, one may not be able to discriminate between the opinion roles. In §2, we demonstrated the need for acquiring more lexical knowledge about opinion verbs for open-domain opinion role extraction. This raises the question whether existing general-purpose resources could be exploited for this purpose. If one considers the plethora of different lexical resources developed for sentiment analysis, i.e. sentiment lexicons listing subjective expressions and their prior polarity (Wilson et al., 2005; Baccianella et al., 2010; Taboada et al., 2011), emotion lexicons (Mohammad and Turney, 2013) or connotation lexicons (Kang et al., 2014), one finds, however, that with respect to opinion role extraction there is a gap. What is missing is a lexicon that states for each opinion verb in which argument position an opinion role can be found. OT (1) [Peter]OH agent dislikes [Mary]patient . OH (2) [Peter]OT agent disappoints [Mary]patient . We assume that it is lexical information that decides in what argument position opinion roles are realized. That is, a verb, such as dislike, believe or applau"
W15-2921,C98-1013,0,\N,Missing
W17-0813,W14-0703,0,0.291055,"Missing"
W17-0813,P07-1003,0,0.0095194,"e world. We, instead, are also interested in relations that are interpreted as causal by humans, even if they are not strictly expressed as causal by a lexical marker, such as temporal relations or speech-act causality. (7) nsubj Gentrification N1: NOUN Gentrifizierung SB social problems VERB ADP ADJ N2: NOUN ¨ fuhrt zu sozialen Problemen MO NK Figure 1: Parallel tree with English cause and aligned German noun pair And if you want to say no, say noEffect ’Cause there’s a million ways to goCause M OTIVATION and the RBG parser (Lei et al., 2014) for German. We then applied the Berkeley Aligner (DeNero and Klein, 2007) to obtain word alignments for all aligned sentences. This allows us to map the dependency trees onto each other and to project (most of) the tokens from English to German and vice versa.3 4.2 Method Step 1 First, we select all sentences in the corpus that contain a form of the English verb cause. We then restrict our set of candidates to instances of cause where both the subject and the direct object are realised as nouns, as illustrated in example (8). Knowledge-lean extraction of causal relations and their participants (8) Alcoholnsubj causes 17 000 needless deathsdobj on the road a year. S"
W17-0813,W15-1622,0,0.271817,"h as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. The problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011). Some annotation efforts abstain altogether from reporting inter-annotator agreement at all. A notable exception is Dunietz et al. (2015) who take a lexical approach and aim at building a constructicon for English causal language. By constructicon they mean “a list of English constructions that conventionally express causality” (Dunietz et al., 2015). They show that their approach dramatically increases agreement between the annotators and thus the quality of the annotations (for details see section 2). We adapt their approach of framing the annotation task as a lexicon creation process and present first steps towards buildIn this paper, we present a simple, yet effective method for the automatic identification and extraction o"
W17-0813,W03-1210,0,0.258305,", and b) annotation studies that discuss the description and disambiguation of causal phenomena in natural language. As we are still in the process of building our resource and collecting training data, we will for now set aside work on automatic classification of causality such as (Mirza and Tonelli, 2014; Dunietz et al., In press) as well as the rich literature on shallow discourse parsing, and focus on annotation and identification of causal phenomena. Early work on identification and extraction of causal relations from text heavily relied on knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003). Girju (2003) identifies instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN1 causes bonynessN2 . Like Versley (2010), most work on identifying causal language for German has been focusing on discourse connectives. Stede et al. (1998; 2002) have developed a lexicon of German discourse markers that has been augmented with semantic relations (Scheffler and Stede, 2016). Another resource for German is the T¨uBa-D/Z that includes annotations for selected discourse connectives, with a small number of causal connectives (Gastel et al., 2011). B¨ogel et al. (2014) pr"
W17-0813,grivaz-2010-human,0,0.223503,"s Dynamic Force Model which provides a framework that tries to distinguish weak and strong causal forces, and captures different types of causality such as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. The problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011). Some annotation efforts abstain altogether from reporting inter-annotator agreement at all. A notable exception is Dunietz et al. (2015) who take a lexical approach and aim at building a constructicon for English causal language. By constructicon they mean “a list of English constructions that conventionally express causality” (Dunietz et al., 2015). They show that their approach dramatically increases agreement between the annotators and thus the quality of the annotations (for details see section 2). We adapt their approach of framing the annotation task as a lexicon"
W17-0813,P16-1135,0,0.112993,"English verb cause as a seed to identify transitive causal verbs. In contrast to Girju’s WordNet-based approach, we use parallel data and project the English tokens to their German counterparts. The contributions of this paper are as follows. 1. We present a bootstrapping method to identify and extract causal relations and their participants from text, based on parallel corpora. 2. We present the first version of a German causal constructicon, containing 100 entries for causal verbal expressions. Ours is not the first work that exploits parallel or comparable corpora for causality detection. Hidey and McKeown (2016) work with monolingual comparable corpora, English Wikipedia and simple Wikipedia. They use explicit discourse connectives from the PDTB (Prasad et al., 2008) as seed data and identify alternative lexicalizations for causal discourse relations. Versley (2010) classifies German explicit discourse relations without German training data, solely based on the English annotations projected to German via word-aligned parallel text. He also presents a bootstrapping approach for a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. 3. We provide over"
W17-0813,L16-1160,0,0.0617345,"rsing, and focus on annotation and identification of causal phenomena. Early work on identification and extraction of causal relations from text heavily relied on knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003). Girju (2003) identifies instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN1 causes bonynessN2 . Like Versley (2010), most work on identifying causal language for German has been focusing on discourse connectives. Stede et al. (1998; 2002) have developed a lexicon of German discourse markers that has been augmented with semantic relations (Scheffler and Stede, 2016). Another resource for German is the T¨uBa-D/Z that includes annotations for selected discourse connectives, with a small number of causal connectives (Gastel et al., 2011). B¨ogel et al. (2014) present a rule-based system for identifying eight causal German connectors in spoken multilogs, and the causal relations R EASON , R ESULT expressed by them. To the best of our knowledge, ours is the first effort to describe causality in German on a broader scale, not limited to discourse connectives. 106 3 Annotation Scheme (1) Elektromagnetische FelderCause Electromagnetic fields k¨onnen KrebsEffect"
W17-0813,P98-2202,0,0.415512,"Missing"
W17-0813,2005.mtsummit-papers.11,0,0.114294,"t incorporate the result (e.g. death) or means (e.g. talk) of causation as part of their meaning. Again, we follow Dunietz et al. and also exclude such cases from our lexicon. In this work, we focus on verbal triggers of causality. Due to our extraction method (section 4), we are mostly dealing with verbal triggers that are instances of the type C ONSEQUENCE. Therefore we cannot say much about the applicability of the different annotation types at this point but will leave this to future work. 4 amod Data The data we use in our experiments come from the English-German part of Europarl corpus (Koehn, 2005). The corpus is aligned on the sentence-level and contains more than 1,9 mio. English-German parallel sentences. We tokenised and parsed the text to obtain dependency trees, using the Stanford parser (Chen and Manning, 2014) for English 3 Some tokens did not receive an alignment and are thus ignored in our experiments. 108 Data: Europarl (En-Ge) Input: seed word: cause (En) Output: list of causal triggers (Ge) S TEP 1: if seed in sentence then if cause linked to subj, dobj (En) then if subj, dobj == noun then if subj, dobj aligned with nouns (Ge) then extract noun pair (Ge); end end end end S"
W17-0813,P14-1130,0,0.0195583,"(2015) who only deal with causal language, not with causality in the world. We, instead, are also interested in relations that are interpreted as causal by humans, even if they are not strictly expressed as causal by a lexical marker, such as temporal relations or speech-act causality. (7) nsubj Gentrification N1: NOUN Gentrifizierung SB social problems VERB ADP ADJ N2: NOUN ¨ fuhrt zu sozialen Problemen MO NK Figure 1: Parallel tree with English cause and aligned German noun pair And if you want to say no, say noEffect ’Cause there’s a million ways to goCause M OTIVATION and the RBG parser (Lei et al., 2014) for German. We then applied the Berkeley Aligner (DeNero and Klein, 2007) to obtain word alignments for all aligned sentences. This allows us to map the dependency trees onto each other and to project (most of) the tokens from English to German and vice versa.3 4.2 Method Step 1 First, we select all sentences in the corpus that contain a form of the English verb cause. We then restrict our set of candidates to instances of cause where both the subject and the direct object are realised as nouns, as illustrated in example (8). Knowledge-lean extraction of causal relations and their participant"
W17-0813,C14-1198,0,0.0225917,"ion 3, we describe our annotation scheme and the data we use in our experiments. Sections 4, 5 and 6 present our approach and the results, and we conclude and outline future work in section 7. 2 Related Work Two strands of research are relevant to our work, a) work on automatic detection of causal relations in text, and b) annotation studies that discuss the description and disambiguation of causal phenomena in natural language. As we are still in the process of building our resource and collecting training data, we will for now set aside work on automatic classification of causality such as (Mirza and Tonelli, 2014; Dunietz et al., In press) as well as the rich literature on shallow discourse parsing, and focus on annotation and identification of causal phenomena. Early work on identification and extraction of causal relations from text heavily relied on knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003). Girju (2003) identifies instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN1 causes bonynessN2 . Like Versley (2010), most work on identifying causal language for German has been focusing on discourse connectives. Stede et al. (1998; 2002) have developed a lex"
W17-0813,prasad-etal-2008-penn,0,0.118477,"ens to their German counterparts. The contributions of this paper are as follows. 1. We present a bootstrapping method to identify and extract causal relations and their participants from text, based on parallel corpora. 2. We present the first version of a German causal constructicon, containing 100 entries for causal verbal expressions. Ours is not the first work that exploits parallel or comparable corpora for causality detection. Hidey and McKeown (2016) work with monolingual comparable corpora, English Wikipedia and simple Wikipedia. They use explicit discourse connectives from the PDTB (Prasad et al., 2008) as seed data and identify alternative lexicalizations for causal discourse relations. Versley (2010) classifies German explicit discourse relations without German training data, solely based on the English annotations projected to German via word-aligned parallel text. He also presents a bootstrapping approach for a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. 3. We provide over 1,000 annotated causal instances (and growing) for the lexical triggers, augmented by a set of negative instances to be used as training data. The remainder o"
W17-0813,L16-1603,0,0.103989,"Missing"
W17-0813,C98-2197,0,\N,Missing
W17-0813,D14-1082,0,\N,Missing
W19-2101,P15-2072,0,0.509626,"of six G20 countries, which we make publicly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more of"
W19-2101,D16-1148,0,0.135425,"in Section 5, we do not make this distinction. 1 Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 1–6 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics Subcorpus France Indonesia Russia South Africa Turkey United States interviews, whereas our work examines naming quantitatively and on social media data. The concept of framing has been applied to a variety of issues and events (Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The data"
W19-2101,S16-1003,0,0.260767,"Missing"
W19-2101,S17-2088,0,0.0471267,"ata. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not unanimous. Subcorpus France Indonesia Russia South-Africa Turkey United States Adj. tweets 281 290 121 227 128 192 Diff. w/ expert 1 0.0"
W19-2101,W10-0214,0,0.0509992,"oni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not un"
W19-2101,D18-1393,0,0.228381,"cly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more often when their content is emotionally charg"
W19-2101,L16-1591,0,0.531323,"s, which we make publicly available.1 Framing is a field of research in communication theory and political science investigating how information is presented to audiences, especially in news media. According to a common definition, to frame is to “to select some aspects of a perceived reality and make them more salient in a communication text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation” (Entman, 1993, p. 52). Most work on framing has focused on issues and events, rather than entities (Card et al., 2015; Fulgoni et al., 2016; Field et al., 2018). We therefore introduce entity framing, which we define as a presentation of an entity which intentionally or unintentionally promotes a particular viewpoint towards that entity. We focus on the framing of political figures on social media, in order to better understand computer-mediated civil political discourse. Online political discussion has been said to have an increasing influence on the democratic process, including on the tone and civility of political debates (Persily, 2017; Ott, 2017). Tweets on political themes are indeed retweeted more often when their content"
W19-2101,P15-1157,0,0.0269095,"fer to the conversation partner or as a form of reference to refer to a third party. For reasons described in Section 5, we do not make this distinction. 1 Proceedings of the Third Workshop on Natural Language Processing and Computational Social Science, pages 1–6 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics Subcorpus France Indonesia Russia South Africa Turkey United States interviews, whereas our work examines naming quantitatively and on social media data. The concept of framing has been applied to a variety of issues and events (Card et al., 2015; Tsur et al., 2015; Fulgoni et al., 2016; Field et al., 2018), and in one case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than per"
W19-2101,N13-1132,0,0.0262985,"7). We expect it to better capture differences between tweets which are neutral in tone but reflect differently on the president, such as ‘Trump trailing in primaries’ vs ‘Jobs market improving under Trump’. Crucially, the prompt also allows annotators to give different ratings to ‘President Trump visits France’ and ‘Trump visits France’. As in Card et al. (2015), the perspective is anchored to that of a proponent of the target in order to combat the lower reliability of reader-perspective prompts (Buechel and Hahn, 2017). After annotation we used Multi-Annotator Competence Estimation (MACE) (Hovy et al., 2013) to identify and remove the least reliable annotators. We collected an additional two judgH0 Variation in naming and stance are not related. H1 Naming primarily downplays or emphasises the president’s status. Therefore, formality of naming is positively related to stance. H2 Naming primarily conveys the degree of solidarity with the president. Therefore, formality of naming is negatively related to stance. Table 4 gives examples of tweets which can be interpreted to support either H1 or H2, or to support the existence of alternative, context-specific functions of naming, such as sarcasm. We gr"
W19-2101,P11-1016,0,0.0645378,"case to the framing of entities (Card et al., 2016), but not previously on social media data. Use of social media to express political opinions has instead been studied to forecast elections (Burnap et al., 2016), political mobilisation (Weeks et al., 2017), and assess political polarization (Bail et al., 2018). A prominent area of NLP that focuses on expressions of favour is stance detection, the detection of sentiment towards a specified target. Most systems focus on stance towards products, companies and abstract topics rather than persons (Somasundaran and Wiebe, 2010; Meng et al., 2012; Jiang et al., 2011; Mohammad et al., 2016). The datasets for SemEval 2017 (Task A and B) (Rosenthal et al., 2017) and RepLab (Amig´o et al., 2012, 2013, 2014) as well as the dataset created by (Taddy, 2013) do include a variety of person entities, but no stance detection work has investigated the influence of naming on stance. 3 Expert agr. 0.78 0.91 0.72 0.87 0.65 0.78 Table 1: Inter-annotator agreement for the on-target/offtarget task (Krippendorff alpha): agreement among FE workers and agreement between two experts adjudicating tweets where FE worker judgment was not unanimous. Subcorpus France Indonesia Rus"
W19-7811,W17-1312,0,0.0352609,"Missing"
W19-7811,P18-1131,0,0.243557,"Missing"
W19-7811,W17-0404,0,0.0202856,"ower: OOV for lower-cased word forms). Inter-Annotator Agreement We computed IAA on a subset of the data with 1,630 tokens. For labelled attachments, the agreement between the two annotators was 0.83 κ, for unlabelled attachments the score increased to 0.89 κ. 3 Annotation decisions Below we discuss decisions we made during the annotation process that deviate from other existing German UD treebanks, i.e. the UD-GSD and the UD-TüBa-D/Z. UD-GSD has been converted from an earlier version of Stanford-style dependencies (McDonald et al., 2013) and contains mostly web reviews while the UD-TüBa-D/Z (Çöltekin et al., 2017) is a conversion of the TüBa-D/Z (Telljohann et al., 2004) and includes articles from a German daily newspaper. Placeholder sentences In the UD-GSD treebank, finite subordinate placeholder sentences with dass or ob (that, whether) are mostly analysed as ccomp while infinite correlates are annotated as acl and attached to the placeholder, usually a pronominal adverb. In contrast, the TüBa-D/Z attaches both finite and infinite placeholder clauses as adverbial clause to the verb of the matrix clause. We decided to annotate finite and infinite placeholder sentences as acl and attach both to their"
W19-7811,E03-1068,0,0.0901857,"Missing"
W19-7811,K17-3002,0,0.0171392,"al. 2014) EN (Liu et al. 2018) EN-AAE (Blodgett et al. 2018) EN-MS (Blodgett et al. 2018) IT (Sanguinetti et al. 2018) # token n.a. 12,149 55,607 3,072 3,524 124,410 # tweets 519* 840 3,550 250 250 6,712 LAS 67.3 – 77.7 56.1 67.7 81.5 (parser) Malt2006 D&M2017 D&M2017 D&M2017 D&M2017 Table 3: Statistics for manually annotated treebanks (*Foster et al. only report # sentences, not # tweets. We expect the no. of tweets to be slightly lower than 500). The data of Blodgett et al. includes AAE and main-stream (MS) English tweets. The last two columns report results for the Dozat & Manning parser (Dozat et al., 2017) (w/o domain adaptation) or the Malt parser from the literature. 6 Related work Twitter treebanks exist not only for English (Kong et al., 2014; Liu et al., 2018; Blodgett et al., 2018) but also for Italian (Sanguinetti et al., 2018) and Arabic (Albogamy et al., 2017). Foster et al. (2011) were among the first to provide syntactic analyses for Twitter microtext. They created a testset with over 500 sentences extracted from tweets. The data was automatically parsed with a constituency parser and the trees were manually corrected by one annotator. Inter-annotator agreement (IAA) for labelled bra"
W19-7811,foth-etal-2014-size,0,0.0547846,"Missing"
W19-7811,W17-0407,0,0.0129448,"istent with the one for conditional clauses that are similar in meaning (e.g.: If I scroll down further, I can see more), where the subordinate if-clause is also an adverbial clausal modifier of the matrix clause. the more_constant the market_shares declined , the more_regular became reformed “The more consistently market shares declined, the more regularly reforms were carried out.” Figure 6: Comparative clause with je-desto in the TüBa-D/Z-UD. 4 Comparison to other German UD treebanks We now compare tweeDe to three other German treebanks, i) UD-GSD, ii) TüBa-D/Z and iii) UD-HDT. The UD-HDT (Hennig and Köhn, 2017) is a conversion of the Hamburg Dependency Treebank (Foth et 3 While this is the canonical order, it is also possible to switch the order of the matrix and subordinate clauses. Constructions without verbal predicates are also possible: Je mehr, desto lustiger. (The more, the merrier). 4 While these are less frequent than the canonical form with je-desto/umso, it is easy to find instances in a large corpus such as the DeWac (Baroni et al., 2009), as well as instances that include only the je without a second particle where the matrix clause then needs to be in V1 word order. Figure 7: Distribut"
W19-7811,D14-1108,0,0.0239314,"Missing"
W19-7811,N18-1088,0,0.0967366,"Missing"
W19-7811,P13-2017,0,0.07675,"ite (OOV: number of out-of-vocabulary words with regard to the training set; lower: OOV for lower-cased word forms). Inter-Annotator Agreement We computed IAA on a subset of the data with 1,630 tokens. For labelled attachments, the agreement between the two annotators was 0.83 κ, for unlabelled attachments the score increased to 0.89 κ. 3 Annotation decisions Below we discuss decisions we made during the annotation process that deviate from other existing German UD treebanks, i.e. the UD-GSD and the UD-TüBa-D/Z. UD-GSD has been converted from an earlier version of Stanford-style dependencies (McDonald et al., 2013) and contains mostly web reviews while the UD-TüBa-D/Z (Çöltekin et al., 2017) is a conversion of the TüBa-D/Z (Telljohann et al., 2004) and includes articles from a German daily newspaper. Placeholder sentences In the UD-GSD treebank, finite subordinate placeholder sentences with dass or ob (that, whether) are mostly analysed as ccomp while infinite correlates are annotated as acl and attached to the placeholder, usually a pronominal adverb. In contrast, the TüBa-D/Z attaches both finite and infinite placeholder clauses as adverbial clause to the verb of the matrix clause. We decided to annot"
W19-7811,nivre-etal-2006-maltparser,0,0.229204,"Missing"
W19-7811,petrov-etal-2012-universal,0,0.0416535,"Missing"
W19-7811,L18-1279,0,0.0752658,"Missing"
W19-7811,K17-3001,0,0.0557305,"Missing"
W19-7811,telljohann-etal-2004-tuba,0,0.0162673,"greement We computed IAA on a subset of the data with 1,630 tokens. For labelled attachments, the agreement between the two annotators was 0.83 κ, for unlabelled attachments the score increased to 0.89 κ. 3 Annotation decisions Below we discuss decisions we made during the annotation process that deviate from other existing German UD treebanks, i.e. the UD-GSD and the UD-TüBa-D/Z. UD-GSD has been converted from an earlier version of Stanford-style dependencies (McDonald et al., 2013) and contains mostly web reviews while the UD-TüBa-D/Z (Çöltekin et al., 2017) is a conversion of the TüBa-D/Z (Telljohann et al., 2004) and includes articles from a German daily newspaper. Placeholder sentences In the UD-GSD treebank, finite subordinate placeholder sentences with dass or ob (that, whether) are mostly analysed as ccomp while infinite correlates are annotated as acl and attached to the placeholder, usually a pronominal adverb. In contrast, the TüBa-D/Z attaches both finite and infinite placeholder clauses as adverbial clause to the verb of the matrix clause. We decided to annotate finite and infinite placeholder sentences as acl and attach both to their respective placeholder (figure 3). there belongs also a re"
