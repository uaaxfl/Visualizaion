2011.mtsummit-wpt.7,P07-2045,0,0.0412064,"tistical translations the parts not covered by GF. However, the grammar must be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47"
2011.mtsummit-wpt.7,P03-1021,0,0.0589389,"st be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47 0.29 6.72 0.31 0.24 0.45 0.03 0.52 0.56 0.43 8.21 0.38 0.30 0.56 0.22"
2011.mtsummit-wpt.7,J03-1002,0,0.00441486,"h ambiguities, i.e., multiple translation options, and can complete with statistical translations the parts not covered by GF. However, the grammar must be expanded so that the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST RO"
2011.mtsummit-wpt.7,P02-1040,0,0.0882648,"t the two systems can collaborate on equal terms. 3.2 Statistical translation, SMT The statistical system is a state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 2.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM [9]. Word alignment is done with GIZA++ [5] and both phrase extraction and decoding are done with the Moses package [3, 2]. The optimisation of the weights of the model is trained with MERT [4] against the BLEU [6] evaluation metric. Table 2 shows a ﬁrst evaluation of this system (Domain) using a variety of lexical metrics. This set of metrics is a subset of the metrics available in the Asiya evaluation package [1]. We speciﬁcally select this set of metrics because all of them are available for the three languages: English, German and French. Together with our in-domain system we show the same 5 74 DE2FR METRIC 1−WER 1−TER BLEU NIST ROUGE-W GTM-2 METEOR-pa ULC FR2DE Bing Google Domain Bing Google Domain 0.42 0.47 0.29 6.72 0.31 0.24 0.45 0.03 0.52 0.56 0.43 8.21 0.38 0.30 0.56 0.22 0.76 0.68 0.56 9.10 0"
2012.eamt-1.61,2010.eamt-1.33,0,0.0341778,"at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu/ mainly used for multil"
2012.eamt-1.61,2007.mtsummit-wpt.4,0,0.085674,"tion 5 summarises the work and outlines possible lines to follow. 2 Related work This work tackles two topics which are lately attracting the attention of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a"
2012.eamt-1.61,W08-0328,0,0.0588025,"OLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu"
2012.eamt-1.61,2011.mtsummit-wpt.7,1,0.890558,"Missing"
2012.eamt-1.61,P02-1040,0,0.0863732,"e using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜na-Bonet et al. (2011a). 3.3 GF system As explained in Section 2, the extension of GF to a new domain implies the construction of a specialised grammar that expands the general resource grammar. Since in our case of applica4 Figure 1: Architecture of the GF translation system. http://www.epo.org/ 271 tion we are far from a close and limited domain, some probabilistic components are also necessary. The general arch"
2012.eamt-1.61,2011.mtsummit-papers.63,1,0.780941,"Missing"
2012.eamt-1.61,W10-1708,0,0.0126031,"s not only useful in this context but is being applied in different domains and language pairs. Besides system combination strategies, hybrid models are designed so that there is one leading translation system assisted or complemented by other kinds of engines. This way the final translator benefits from the features of all the approaches. A family of models are based on SMT systems enriched with lexical information from RBMT (Eisele et al., 2008; Chen and Eisele, 2010). On the other side there are the models that start from the RBMT analysis and use SMT to complement it (Habash et al., 2009; Federmann et al., 2010; Espa˜na-Bonet et al., 2011b). Our work can be classified in the two families. On the one hand, SMT helps on the construction of the RBMT translator but, on the other hand, there is the final decoding step to integrate translations and complete those phrases untranslated by RBMT. We use GF as rule-based system. GF is a type-theoretical grammar formalism, 1 2 http://www.pluto-patenttranslation.eu/ http://www.molto-project.eu/ mainly used for multilingual natural language applications. Grammars in GF are represented as a pair of an abstract syntax –an interlingua that captures the semantics of"
2012.eamt-1.61,P07-2045,0,0.0204122,"patents. The language of patents follows a formal style adequate to be analysed with a grammar, but at the same time uses a rich and particular vocabulary adequate to be gathered statistically. We focus on the English-French language pair so that the effects of translating into a morphologically rich language can be studied. With respect to the engine, a grammar-based translator is developed to assure grammatically correct translations. We extend GF (Grammatical Framework, Ranta (2011)) and write a new grammar for patent translation. The SMT system that complements the RBMT is based on Moses (Koehn et al., 2007). This system works on two different levels. First, it is used to build the parallel lexicon of the GF translator on the fly. Second, it is the top level decoder that takes the final decision about which phrases should be used. In the following Section 2 describes recent work both in patent translation and hybrid systems. Section 3 explains our hybrid system and Section 4 evaluates its performance. Finally, Section 5 summarises the work and outlines possible lines to follow. 2 Related work This work tackles two topics which are lately attracting the attention of researchers, patent translation"
2012.eamt-1.61,J03-1002,0,0.00340697,"rk-up within the patent such as paragraph tags for example. Two small sets for development and test purposes have also been selected with the same restrictions: 993 fragments for development and 1008 for test. 3.2 In-domain SMT system The first component is a standard state-of-the-art phrase-based SMT system trained on the biomedical domain with the corpus described in Section 3.1. Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜"
2012.eamt-1.61,P03-1021,0,0.0115529,"Its development has been done using standard freely available software. A 5-gram language model is estimated using interpolated Kneser-Ney discounting with SRILM (Stolcke, 2002). Word alignment is done with GIZA++ (Och and Ney, 2003) and both phrase extraction and decoding are done with the Moses package (Koehn et al., 2006; Koehn et al., 2007). Our model considers the language model, direct and inverse phrase probabilities, direct and inverse lexical probabilities, phrase and word penalties, and a non-lexicalised reordering. The optimisation of the weights of the model is trained with MERT (Och, 2003) against the BLEU (Papineni et al., 2002) evaluation metric. A wider explanation of this system, the preprocess applied to the corpus before training the system and a deep evaluation of the translations can be found in Espa˜na-Bonet et al. (2011a). 3.3 GF system As explained in Section 2, the extension of GF to a new domain implies the construction of a specialised grammar that expands the general resource grammar. Since in our case of applica4 Figure 1: Architecture of the GF translation system. http://www.epo.org/ 271 tion we are far from a close and limited domain, some probabilistic compon"
2012.eamt-1.61,W04-2104,0,0.0234676,"Missing"
2012.eamt-1.61,W03-2008,0,0.0281322,"acting the attention of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not"
2012.eamt-1.61,2005.mtsummit-wpt.6,0,0.0495801,"of researchers, patent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this"
2012.eamt-1.61,2009.eamt-1.28,0,0.0457617,"ent translation and hybrid translators. The high number of patents being registered and the necessity for these patents to be translated into several languages are the reason so that important efforts are being made in the last years to automate its translation between various language pairs. Different methods have been used for this task, ranging from SMT (Ceausu et al., 2011; Espa˜na-Bonet et al., 2011a) to hybrid systems (Ehara, 2007; Ehara, 2010). Besides full systems, various components associated to patent translation are being studied separately (Sheremetyeva, 2003; Sheremetyeva, 2005; Sheremetyeva, 2009). Part of this work is being done within the framework of two European projects, PLuTO (Patent Language Translations Online1 ) and MOLTO (Multilingual Online Translation2 ). PLuTO aims at making a substantial contribution to patent translation by using a number of techniques that include hybrid systems combining example-based and hierarchical techniques. On the other hand, one of MOLTO’s use cases aims at extending a grammar-based translator with an SMT to gain robustness in the translation of patents. This paper is carried out within MOLTO. HMT is not only useful in this context but is being"
2012.eamt-1.61,2009.mtsummit-posters.21,0,0.0427205,"sted, the technology evolved towards rule-based systems (RBMT). Later in the 90s the everyday more powerful computers allowed to develop empirical translation systems. Recently a type of empirical system, the statistical one (SMT), has become a widely used standard for translation. At this point the two main paradigms, RBMT and SMT, coexist with their strengths and weaknesses. Luckily these strengths and weaknesses are complementary and current efforts are being made to hybridise both of them and develop new technologies. A classification and description of hybrid translation can be found in (Thurmair, 2009). In general RBMT provides high precision, due to an analysis of the text, but has limited coverage c 2012 European Association for Machine Translation. 269 and a considerable amount of effort and linguistic knowledge is required in order to build such a system. On the other hand, SMT can achieve a huge coverage and is good at lexical selection and fluency but has problems in building structurally and grammatically correct translations. Hybrid MT (HMT) is an emerging and challenging area of machine translation, which aims at combining the known techniques into systems that retain the best feat"
2012.eamt-1.61,2011.eamt-1.5,0,\N,Missing
2012.eamt-1.61,N09-2055,0,\N,Missing
2013.mtsummit-posters.10,W13-2715,1,0.877354,"Missing"
2013.mtsummit-posters.10,2011.mtsummit-wpt.7,1,0.89823,"Missing"
2013.mtsummit-posters.10,2010.amta-commercial.14,0,0.114865,"Missing"
2020.cl-2.6,N09-1003,0,0.0956804,"Missing"
2020.cl-2.6,E14-2011,1,0.894929,"Missing"
2020.cl-2.6,E14-1039,1,0.826721,"Missing"
2020.cl-2.6,E09-1009,1,0.59036,"enough data for statistical methods. Writing linguistic rules can then be a way out. Another aspect is the possibility to use additional knowledge: Statistical methods typically learn translation from parallel texts, whereas correct translation can also require knowledge that is impossible to extract from the texts alone (Kay 2017). It is natural to ask whether GF could be used for wide-coverage tasks by sacrificing some precision but maintaining explainability, programmability, and data austerity. This question has been the focus of much of GF research for the last ten years. It was ¨ 2004; Angelov 2009; first enabled by efficient and scalable parsing techniques (Ljunglof ¨ 2014). The first experiments addressed full-scale morphology Angelov and Ljunglof implementations (Forsberg and Ranta 2004; D´etrez and Ranta 2012), hybrid GF-SMT translation (Enache et al. 2012), and multilingual lexicon extraction (Virk et al. 2014; Angelov 2014). Much of the focus has been on machine translation, with an early mobile demo (Angelov, Bringert, and Ranta 2014) and an emphasis on explainability rather than optimized BLEU scores. The prospects for Explainable Machine Translation (XMT) are studied in Ranta ("
2020.cl-2.6,angelov-2014-bootstrapping,1,0.912621,"t is natural to ask whether GF could be used for wide-coverage tasks by sacrificing some precision but maintaining explainability, programmability, and data austerity. This question has been the focus of much of GF research for the last ten years. It was ¨ 2004; Angelov 2009; first enabled by efficient and scalable parsing techniques (Ljunglof ¨ 2014). The first experiments addressed full-scale morphology Angelov and Ljunglof implementations (Forsberg and Ranta 2004; D´etrez and Ranta 2012), hybrid GF-SMT translation (Enache et al. 2012), and multilingual lexicon extraction (Virk et al. 2014; Angelov 2014). Much of the focus has been on machine translation, with an early mobile demo (Angelov, Bringert, and Ranta 2014) and an emphasis on explainability rather than optimized BLEU scores. The prospects for Explainable Machine Translation (XMT) are studied in Ranta (2017), and the general name for all the approaches described in this paper could be Explainable NLP. Figure 5 shows the architecture of a GF-based explainable MT system as an instance of Explainable Artificial Intelligence: an AI system which, in addition to the output, also gives an explanation of why this output is produced: For insta"
2020.cl-2.6,W15-3305,1,0.821448,"ection tables and add suffixes at runtime. For example, the equivalent of “did we walk” is k¨avelimmek¨o, which is produced from k¨aveli+mme+k¨o (“walk(past)+1personPlural+question”). 10 These stems were identified by analyzing the full inflection tables of a comprehensive set of verbs. For a large majority of verbs, much fewer stems would be enough. 439 Computational Linguistics Volume 46, Number 2 In running Finnish text, the morpheme boundaries are not visible. The parser hence has to restore the boundaries in order to match the original grammar rules. An algorithm for this is presented in Angelov (2015). It is an essential ingredient in enabling GF to parse morphologically rich languages with large lexica. Segment analysis is also used when forming compound words in languages like Finnish, German, and Swedish. Thus English summer time translates compositionally to Swedish sommar+tid, which is rendered as sommartid. The algorithm of Angelov (2015) separates these tokens when analyzing the Swedish word. In addition, the Swedish lexicon (as well as Finnish and German) must keep track of the special forms that are used in compounding, so that for instance response time becomes svars+tid and not"
2020.cl-2.6,W16-4504,1,0.65833,"chment scores (LAS). Data transfer is related to augmentation, because it uses in a similar way both ud2gf and gf2ud. But the linearization involved in the gf2ud phase targets a language different 30 A naive approach to augmentation increases the size of the treebank by a factor of 1,000. 464 Ranta et al. Abstract Syntax as Interlingua from the source. The augmentation phase can adapt the data to the target language, for instance, by adding examples of morphological variation not found in the source language. This method is an instance of crosslingual treebanking, which in Tiedemann and Agi´c (2016) is approached by statistical machine translation methods. Just like in data augmentation, one could expect better precision if grammatical knowledge is used in the generation phase. Kolachina and Ranta (2016) give some examples of this, but a large-scale evaluation remains to be done. Data bootstrapping is the most radical method, of particular interest for languages that have grammars but no treebanks. The method produces a set of trees from a GF grammar and converts them to UD. No ud2gf is needed, but only gf2ud. Figure 15 shows, following Kolachina and Ranta (2019), that 20,000 synthetic t"
2020.cl-2.6,W13-2322,0,0.0407424,"or instance, which currently specifies nearly 700 constructions, is being built in parallel with SweCcn (Janda et al. 2018), reusing the technical infrastructure and the data format of SweCcn. Thus, the SweCcn-in-GF approach can be applied to the Russian Constructicon in a relatively straightforward manner. Moreover, this would open a further research direction on developing a common GF abstract syntax for a multilingual computational constructicon, building on the recent linguistic research in multilingual constructicography by Lyngfelt et al. (2018). 7.4 Abstract Meaning Representation AMR (Banarescu et al. 2013) has been proposed as a robust whole-sentence semantic representation, and it has gained considerable attention in the research community. Although the original work on both AMR as a representation and AMR as a semantically annotated text corpus (sembank) has been focused on English, there have been promising efforts in pushing it toward a semantic interlingua (Xue et al. 2014; Damonte and Cohen 2018). 475 Computational Linguistics Volume 46, Number 2 Following a similar task in text-to-AMR parsing (May 2016), a recent shared task at SemEval 2017 unveiled the state-of-the-art in AMR-to-text ge"
2020.cl-2.6,I05-2035,0,0.10135,"ever, the linguistic mechanisms needed in the concrete syntax—morphology, agreement, word order—are largely the same in all areas of discourse. It would be tedious to implement these mechanisms over and over again in different applications. The solution to this problem was to develop a library of basic linguistic structures. The library, called the Resource Grammar Library (RGL), started to develop in 2001. It was inspired primarily by the Core Language Engine (Alshawi 1992), where a similar need had been identified. It is also related to Xerox ParGram (Butt et al. 2002) and the LiNGO Matrix (Bender and Flickinger 2005), which are collections of grammars for different languages.3 The RGL implements morphology and syntax rather than semantics. It was not meant to be a translation interlingua in itself, but to boost the development of domainspecific semantic CNLs by taking care of linguistic details. Not only does it save the work of implementing morphology and syntax again and again. It also has its own common abstract syntax, which provides a common Application Programming Interface (API) enabling a CNL implementation for many languages simultaneously, sharing almost all of the source code except content wor"
2020.cl-2.6,P10-4001,0,0.0380262,"align subtrees of abstract syntax trees obtained by wide coverage parsing, or—more robustly—parallel UD trees, which are then converted to GF. Preliminary results have been obtained from parallel GDPR data (Section 5.4; Ranta 2018), but they still need a lot of manual postprocessing. The potential for typological studies comes from the very idea of GF, which is to maintain an explicit distinction between shared and differing parts of grammars. One idea would be to elaborate the “questionnaire” technique in the LinGO Matrix: to implement a new language proceeds by answering a set of questions (Bender et al. 2010). The questions are posed by the abstract syntax, and the answers end up in the concrete syntax. The Community. For any approach based on linguistic knowledge, an open culture for creating and sharing knowledge is essential. An important factor in building and 479 Computational Linguistics Volume 46, Number 2 expanding the RGL has been the series of summer schools training new contributors. Except for the commercial applications, all of the projects mentioned in this article are open-source and available via public repositories. The grammar compiler is distributed under GPL (GNU General Public"
2020.cl-2.6,W17-6801,0,0.0256458,"the subtrees themselves. The operations on the right-hand sides of these rules are restricted to a few operations on strings, tables, and records. The important thing is that every subtree is a linguistically meaningful unit, that is, has an independent “linguistic meaning” (built from strings, records, and tables). This implies that the claim of a common abstract syntax is not vacuous: to share an abstract syntax function, a language must have a compositional linearization function for it. The abstract syntax trees of GF also support Montague-style semantics (Ranta 2004a, 2011b), extended by Bernardy and Chatzikyriakidis (2017) to cover the entire abstract syntax of the GF RGL. It has thereby become usable in pipelines that convert open-domain text to logical formulas (of course partially, since RGL parsing is not total). Because of the common abstract syntax, this semantics is in principle defined for all RGL languages. GF abstract syntax inherits from LF the full power of dependent types and variable bindings, which enables a compositional model of anaphora, even between sentences in a text (Ranta 1994). This possibility was in fact the main initial reason to develop a typetheoretical version of Montague grammar i"
2020.cl-2.6,W02-1503,0,0.22,"mathematics to tourist phrasebooks. However, the linguistic mechanisms needed in the concrete syntax—morphology, agreement, word order—are largely the same in all areas of discourse. It would be tedious to implement these mechanisms over and over again in different applications. The solution to this problem was to develop a library of basic linguistic structures. The library, called the Resource Grammar Library (RGL), started to develop in 2001. It was inspired primarily by the Core Language Engine (Alshawi 1992), where a similar need had been identified. It is also related to Xerox ParGram (Butt et al. 2002) and the LiNGO Matrix (Bender and Flickinger 2005), which are collections of grammars for different languages.3 The RGL implements morphology and syntax rather than semantics. It was not meant to be a translation interlingua in itself, but to boost the development of domainspecific semantic CNLs by taking care of linguistic details. Not only does it save the work of implementing morphology and syntax again and again. It also has its own common abstract syntax, which provides a common Application Programming Interface (API) enabling a CNL implementation for many languages simultaneously, sharin"
2020.cl-2.6,N18-1104,0,0.128826,"Missing"
2020.cl-2.6,dannells-gruzitis-2014-extracting,1,0.856791,"Missing"
2020.cl-2.6,E12-1066,1,0.707665,"Missing"
2020.cl-2.6,J94-4004,0,0.621334,". The translator would first map source language words to these senses, and then map the senses as target language words in the same order. The words would often appear in wrong senses, in wrong order, or in wrong forms, but the translation could still be helpful. A bit less shallow interlingua could add syntactic structure to the words, so that it could reorder the words or even enforce agreement in accordance with the rules of the target language. Such a system would still not always select correct word senses, identify multiword constructions, or change the syntactic structure when needed (Dorr 1994). But it could guarantee the grammatical correctness of the output. Figure 3 shows the architecture of a layered interlingua system, where the transferbased shortcuts of the original Vauquois triangle are replaced by interlinguas of varying depth. Such a system enjoys some of the advantages of the original interlingua idea—in particular, the linear scale-up as languages are added. The main topic of this article is to show how to build and combine interlinguas of different levels, by using Grammatical Framework (GF; Ranta 2004b, 2011a), a tool designed for this very purpose. More precisely, GF"
2020.cl-2.6,C00-1036,1,0.649826,"Missing"
2020.cl-2.6,2012.eamt-1.61,1,0.864346,"Missing"
2020.cl-2.6,N16-1087,0,0.0652151,"Missing"
2020.cl-2.6,W15-3307,1,0.902304,"Missing"
2020.cl-2.6,S17-2159,1,0.911539,"Missing"
2020.cl-2.6,J13-1006,0,0.0120438,"order to match a given abstract syntax. For example, VP in German and Scandinavian can have fields for all of the positions in the topological structure (Diderichsen 1962), which permits the reordering of constituents in different syntactic contexts (main clause, question, subordinate clause, topicalizations). 3.4.6 Linguistic Generalizations. Plain PMCFG is a low-level language, where grammar rules are encoded in ways that are unintuitive and repetitive for humans. At the same time, it is well-suited for data-driven parsing, and the models can be directly used in GF’s run time. An example is Kallmeyer and Maier (2013), whose model was used in ¨ (2014), showing that the run-time performance of a GF parser is Angelov and Ljunglof competitive even in large-scale statistical parsing.12 The source language of GF has a very different look and feel: It follows the design of modern functional programming language with abstraction mechanisms that enable the grammarian to express many kinds of linguistic generalizations. The goal has been to give the linguist a “freedom of speech” and help avoid repetitive coding both inside and between languages. Here is a summary of some of these mechanisms: • The abstract syntax"
2020.cl-2.6,kamholz-etal-2014-panlex,0,0.053346,"Missing"
2020.cl-2.6,J94-3001,0,0.477106,"ject. A grammar using the RGL as a library resembles an NLG pipeline that goes via different levels of representation: semantic, syntactic, and lexical. However, in typical GF systems, the intermediate representations are not visible at run time: All that remains are the semantic trees and the concrete strings. Making this possible in the grammar compiler was a major challenge in the development of GF, using compilation techniques such as partial evaluation (Ranta 2004b; Angelov, Bringert, and Ranta 2009). GF grammar composition is analogous to transducer composition in the finitestate world (Kaplan and Kay 1994), where grammar writers can structure their source code into smaller components, but this structure is optimized away from the run-time system, which works as a single end-to-end transducer. Borrowing the notation of XFST, we could express the implementation of a semantic grammar for a language, say Hindi, as SemanticAbstract .o. SemanticHin .o. SyntacticHin While grammar composition is an appealing idea linguistically, it also has the practical advantage that it enables a division of labor between linguists, who write syntactic grammars, and domain experts, who write semantic grammars. The li"
2020.cl-2.6,P06-2062,0,0.0772547,"e sense of a software library, as further explained in Section 5 (see also Ranta 2007, 2009a). But it turned out later that the RGL can also be used on its own for wide-coverage translation and parsing. This was done first in a pure GF system (Angelov, Bringert, and Ranta 2014). Later, as the shared dependency structure that was built in UD turned out to be close to the abstract syntax of the RGL, it became possible to combine UD and GF into a robust pipeline (Section 6). 4.1 The RGL Languages The development of the RGL started in 2001, with grammars for English, Swedish, French, and Russian (Khegai 2006). Finnish was soon added, showing the adequacy of the abstract syntax outside the Indo-European family. By the end of the European MOLTO project (2010–2013),15 a majority of the 27 official languages of the European Union had been covered. As of 2019, the RGL has “complete” implementations of 35 languages—complete in the sense of covering the common abstract syntax and a set of inflectional morphology paradigms able to produce all word forms. The nonIndo-European languages are Arabic (Dada and Ranta 2006), Basque, Chinese (Ranta, Haiyan, and Tian 2015), Estonian (Listenmaa and Kaljurand 2014),"
2020.cl-2.6,J10-4005,0,0.0272402,"c interpretation can be labeled as highly uncertain. Finally, the semantic interlingua itself is problematic, and all known attempts to build one have remained incomplete. Target generation is in principle a solved problem if it starts from a formalized interlingua, since it can be performed by a deterministic program. But the transfer from source to target is less certain on the lower levels than on the higher levels; for example, on the word-to-word level, the order of words in the target language is just a guess. Most traditional MT methods operate on the lower levels: Statistical MT (SMT; Koehn 2010) is usually either word-to-word or phrase-to-phrase transfer,1 and so are 1 A “phrase” in SMT can be any sequence of words, not necessarily a grammatical phrase. 426 Ranta et al. Abstract Syntax as Interlingua incomplete semantic interlingua v @ @ highly uncertain semantic interpretation uncertain syntactic parsing v well-understood lexical analysis source uncertain @ @ syntactic transfer correct by construction v @v syntactic structure generation @ @ highly uncertain @ lexical transfer @v correct by construction @ lexical form generation @ ?? R @ character transfer - target Figure 1 The Vauq"
2020.cl-2.6,2016.lilt-13.4,1,0.935124,"sleeps advmod head amod head head advmod head obj head xcomp det head head nummod head nsubj head head aux advmod head head head head The main proof for the abstract syntax is that “it works”—at least, has been found to work so far. No claims are made that it should continue to work for all languages of the world and be “universal” in the most pregnant sense of the word. However, the recent advances of UD (Nivre et al. 2016) have not only shown that it is possible to assign the same syntactic structures to vastly different languages, but also ended up with structures very similar to the RGL (Kolachina and Ranta 2016). We will return to the GF-UD correspondence in Section 6. 4.4 Dealing with Structural Differences The abstract syntax of the GF Resource Grammar is, in the technical sense, an interlingua, which implements a certain linguistic theory of crosslingual syntax. However, it is not good enough to serve as a semantic interlingua that guarantees meaning-preserving translation. The reason is that translation often has to change the syntactic structure to render the original meaning. Here are some simple and well-known examples:19 • English I am a teacher, French je suis professeur: no indefinite artic"
2020.cl-2.6,W19-6102,1,0.719952,"al treebanking, which in Tiedemann and Agi´c (2016) is approached by statistical machine translation methods. Just like in data augmentation, one could expect better precision if grammatical knowledge is used in the generation phase. Kolachina and Ranta (2016) give some examples of this, but a large-scale evaluation remains to be done. Data bootstrapping is the most radical method, of particular interest for languages that have grammars but no treebanks. The method produces a set of trees from a GF grammar and converts them to UD. No ud2gf is needed, but only gf2ud. Figure 15 shows, following Kolachina and Ranta (2019), that 20,000 synthetic trees can be as good as 10,000 manually annotated trees, both in English and Finnish. As the process is fully automatic, no extra work is required to produce any number of trees—millions or billions. However, as noticed in Kolachina and Ranta (2019), the learning curves flatten on a lower accuracy level than manual treebanks, as soon as the variations covered by the grammar are exhausted. An obvious next question is what combinations of bootstrapping, manual annotation, and augmentation are the best ways to build parsers with the minimum of manual work. All three method"
2020.cl-2.6,J14-1005,0,0.306504,"Missing"
2020.cl-2.6,P16-1160,0,0.0196127,"Missing"
2020.cl-2.6,W18-6309,0,0.0247587,"fer, which Vauquois did not consider at all (Lee, Cho, and Hofmann 2016) . But in a system where all steps are uncertain, ranking the alternatives globally rather than separately in a pipeline actually does make sense, even though it comes with the price of not being able to use linguistic knowledge and to explain step by step how the translation is achieved. At the same time as going down to the lowest possible level of transfer, NMT has made claims of using an interlingua, just not of the kind that humans construct but something that arises as an internal representation in a neural network (Lu et al. 2018). The main advantage is the same as in traditional interlinguas: one does not need to build n(n − 1) translation functions to cover all pairs of n languages, but it is enough to have 2n (Figure 2). In the NMT world, this method has been given the name zeroshot translation—translation between language pairs for which the system has not been explicitly trained. Similar advantages can be shown in other NLP tasks: For instance, the answer function in question answering can be defined uniformly in terms of the interlingua, rather than for all languages separately (Zimina et al. 2018). A universal i"
2020.cl-2.6,J93-2004,0,0.0693431,"required. In our experience, writing a resource grammar indeed requires more programming skills than corpus annotation, and the human labor is therefore “more expensive.” But grammarians usually see it as an enjoyable intellectual challenge, which they do voluntarily. It has attracted people who would probably not volunteer for largescale corpus annotation—for instance, professors of mathematics and computer science. Grammar writing vs. corpus annotation is not an exclusive choice. Using a grammarbased parser as the first step for building a treebank is a proven method (Marcus, Santorini, and Marcinkiewicz 1993; Oepen and Manning 2004). Because natural language grammars typically yield ambiguous results and have limited coverage, a human is needed to disambiguate and complete the parser output. But this method can still save work compared with fully manual annotation. It can also improve the consistency of the resulting treebanks, by mechanically analyzing the same structures always in the same way. What is more, a computational grammar is in many ways a richer resource than an annotated corpus. A GF grammar, in particular, gives • an accurate method to generate language and not only to analyze it,"
2020.cl-2.6,S16-1166,0,0.038746,"Missing"
2020.cl-2.6,S17-2090,0,0.0257321,"Missing"
2020.cl-2.6,R13-2019,0,0.0206721,"e definition of crosslingual relations via the shared abstract syntax. We will see in Section 6.3 that a resource grammar can even replace much of the work of hand-annotating corpora, because it enables the generation of synthetic corpora that come a long way in providing data for machine learning without any additional cost. So what does it cost to build a GF grammar? For a complete resource grammar (in the sense of Table 4), a typical development time is 2 to 6 months of one person’s work (?). This is an appropriate size for a master’s thesis; examples include Japanese (Zimina 2012), Greek (Papadopoulou 2013), Maltese (Camilleri 2013), and Icelandic (Traustason 2016). To extend this to a shallow wide-coverage grammar, such as used in Angelov, Bringert, and Ranta (2014), just a few weeks more is needed, if the lexicon can be extracted from available resources (cf. Sections 5.2 and 7.1). However, there is no GF experience yet from a wide-coverage deep and precise grammar comparable 446 Ranta et al. Abstract Syntax as Interlingua Text Tense Pron Art Quant PN CN Ord Card Numeral,Digits PConj Utt Imp S Pol Cl VP Det Num Phr Ant NP Predet Punct ListNP N,N2,N3 AdV V,V2,V3,V*,V2* RS AdA Voc QS ListS Conj"
2020.cl-2.6,P02-1040,0,0.106716,"Missing"
2020.cl-2.6,W17-0414,1,0.839728,"rents of the English pronouns (via bound variables in the logical formula) can replace them by the unambiguous translations (4) and (5), as described in Ranta (1994): 1. if a man owns a donkey he beats it 2. if a man owns a donkey it beats him 3. si un homme poss`ede un aˆ ne il le bat 461 Computational Linguistics 4. si un homme poss`ede un aˆ ne l’homme le bat 5. si un homme poss`ede un aˆ ne l’ˆane le bat Volume 46, Number 2 In order to build pipelines like those in Figure 12, we need a ud2gf component to retrieve abstract syntax from dependency trees. An algorithm for this is presented in Ranta and Kolachina (2017). Its core is the same kind of dependency configurations as in gf2ud, which are now used in a search mode: Given a UD tree, find all GF trees that can generate it. The ud2gf conversion is not deterministic, since gf2ud is a lossy operation. For instance, the order of adjectival modifiers is not preserved in UD: The French phrase (6) has just one UD tree, whereas the RGL gives two trees corresponding to two different English translations: 6. petit animal carnivore 7. (petit animal) carnivore—“carnivorous small animal” 8. petit (animal carnivore)—“small carnivorous animal” Another problem is eve"
2020.cl-2.6,W15-3117,1,0.904708,"Missing"
2020.cl-2.6,W15-3301,1,0.905584,"Missing"
2020.cl-2.6,P02-1035,0,0.0369744,"Missing"
2020.cl-2.6,P09-4010,0,0.100009,"Missing"
2020.cl-2.6,D18-1545,0,0.135533,"Missing"
2020.cl-2.6,C90-3045,0,0.439734,". Add Const i_13 Mul Const i_2 Var bipush 13 13 iconst_2 + iload 8 2 imul * iadd x x Figure 6 An abstract syntax tree and word alignment for infix to postfix (Java to Java Virtual Machine) translation. 434 Ranta et al. Abstract Syntax as Interlingua In the early days of compilers, back-end operations were often performed directly in the parser, without explicitly building abstract syntax trees. The purpose was to minimize the usage of the scarce computational resources. Synchronous grammars are a method designed for such direct translation, and have also inspired natural language translation (Shieber and Schabes 1990). In modern compilers, building an AST and storing it separately is considered best practice, as it enables more powerful semantic analysis and optimizations (Appel 1998). It also enables sharing compiler components between different source and target languages, because much of the semantic analysis and optimizations can be performed on the AST level. A prime example is the GNU Compiler Collection, which is a “multi-source multi-target compiler” (Stallman 2001). One reason to use abstract syntax in NLP is similar to compilers: the possibility to share processing components between languages. T"
2020.cl-2.6,simov-osenova-2010-constructing,0,0.0138309,"ers are composed of an English lemma plus a GF category. When that is not enough, we add a consecutive number to make the identifier unique. 468 Ranta et al. nom gen sghouse house&apos;s pl houses houses&apos; obest best sghus huset nom pl hus husen gen sghus husets pl hus husens Abstract Syntax as Interlingua . . . . yksikkö monikko nominatiivi talo talot genetiivi talon talojen partitiivi taloa taloja translatiivi taloksi taloksi ... ... ... Figure 17 Snapshots of morphological tables generated from GF WordNet. ¨ (Borin, Forsberg, and Lonngren 2013) for Swedish, and BulTreeBank WordNet for Bulgarian (Simov and Osenova 2010). The BalkaNet WordNet for Bulgarian (Koeva, S., and Genov 2004) was not used because of its restrictive license. Both the Swedish and the Bulgarian resources are incomplete (28,046 senses in Swedish and 8,936 in Bulgarian), so additional translations are added from the Folkets Lexikon for Swedish (Kann and Hollman 2011) and KBEDict for Bulgarian (Angelov 2014). The initial bootstrapped resources are then subjected to an ongoing validation effort. The validation ensures that the best translation pairs are chosen according to the criteria: • the two words should share as many senses as possible"
2020.cl-2.6,K17-3009,0,0.0400066,"Missing"
2020.cl-2.6,P87-1015,0,0.753881,"Missing"
2020.cl-2.6,W14-5508,1,0.88614,"Missing"
2020.cl-2.6,xue-etal-2014-interlingua,0,0.0239948,"GF abstract syntax for a multilingual computational constructicon, building on the recent linguistic research in multilingual constructicography by Lyngfelt et al. (2018). 7.4 Abstract Meaning Representation AMR (Banarescu et al. 2013) has been proposed as a robust whole-sentence semantic representation, and it has gained considerable attention in the research community. Although the original work on both AMR as a representation and AMR as a semantically annotated text corpus (sembank) has been focused on English, there have been promising efforts in pushing it toward a semantic interlingua (Xue et al. 2014; Damonte and Cohen 2018). 475 Computational Linguistics Volume 46, Number 2 Following a similar task in text-to-AMR parsing (May 2016), a recent shared task at SemEval 2017 unveiled the state-of-the-art in AMR-to-text generation (May and Priyadarshi 2017). According to the SemEval 2017 Task 9 evaluation, the convincingly best-performing AMR-to-text generation system (Gruzitis, Gosko, and Barzdins 2017), among the contestants, combines a GF-based generator with the JAMR generator (Flanigan et al. 2016), achieving the Trueskill score (human evaluation) of 1.03–1.07 and the BLEU score (automatic"
2020.cl-2.6,C10-1061,0,\N,Missing
2021.cnl-1.2,J93-2003,0,0.200021,"strong guarantees of grammatical correctness. However, lexical exactness is, in this context, just as important as grammaticality. An important part of the design of a Controlled Natural Language (CNL) is the creation of a high-quality translation lexicon, preserving both semantics and grammatical correctness. A translation lexicon is often built manually, which is a time consuming task requiring significant linguistic knowledge. When the task is based on a corpus of parallel example sentences, part of this process can be automated by means of statistical word and phrase alignment techniques (Brown et al., 1993; Och and Ney, 2000; Dyer et al., 2013). None of them is, however, suitable for the common case in which only a small amount of example data is available — typically, with just one occurrence of each relevant lexical item. Aarne Ranta University of Gothenburg, Department of Computer Science and Engineering; Digital Grammars aarne.ranta@cse.gu.se In this paper, we propose an alternative approach to the automation of this task. While still being data-driven, our method is also grammar-based and, as such, capable of extracting meaningful correspondences even from individual sentence pairs. A furt"
2021.cnl-1.2,D14-1082,0,0.0192865,"the one it depends on, called its head, via a directed labelled link specifying the syntactic relation between them. Importantly for our application, the standard format for UD trees, CoNNL-U, gives information not only on the syntactic role of each word, but also on its Part-Of-Speech (POS) tag, lemma, and morphological features. While both formalisms independently solve the issues related to having to work with grammars that are inconsistent with each other, UD is especially appealing since, being dependency trees an easier target, several robust parsers, such as (Straka et al., 2016) and (Chen and Manning, 2014) are available. Alone, UD trees are sufficient to extract (or propagate) tree-to-tree alignments, but not to automate the generation of a morphologically-aware translation lexicon for a generative grammar. This is where GF comes into play: after correspondences are inferred from a parallel text, our system is able to convert them to GF grammar rules, easy to embed in a domain-specific grammar but also making it immediate to carry out small-scale translation experiments using pre-existing grammatical constructions implemented in GF’s Resource Grammar Library (RGL), which covers the morphology a"
2021.cnl-1.2,J94-4004,0,0.746546,"tudies, studiai (head alignment), hshe, leii (matching nsubj label) and hconsistently, con costanzai (translation divergence; amod and advmod treated as equivalent). grammatical distinctions. When this is the case, it is often straightforward to define alignment criteria based on recognizing the corresponding patterns. While many distinctions of this kind are specific to particular language pairs or even stylistical, some of them occur independently of what languages are involved and do not depend on idiomatic usage nor aspectual, discourse, domain or word knowledge. Drawing inspiration from (Dorr, 1994), we refer to them as translation divergences and handle some of the most common ones explicitly. For instance, categorial divergences occur when the POS tag of a word in the source language changes in its translation. An ubiquitous example of this is that of adverbial modifiers (with the UD label advmod) translated as prepositional phrases (with the UD label obl, for oblique), such as in the English-Italian pair hShe studies consistently, Lei studia con costanzai (see Figure 1). Structural divergences, where a direct object in one language is rendered as an oblique in the other, as in the Eng"
2021.cnl-1.2,N13-1073,0,0.0273519,"tness. However, lexical exactness is, in this context, just as important as grammaticality. An important part of the design of a Controlled Natural Language (CNL) is the creation of a high-quality translation lexicon, preserving both semantics and grammatical correctness. A translation lexicon is often built manually, which is a time consuming task requiring significant linguistic knowledge. When the task is based on a corpus of parallel example sentences, part of this process can be automated by means of statistical word and phrase alignment techniques (Brown et al., 1993; Och and Ney, 2000; Dyer et al., 2013). None of them is, however, suitable for the common case in which only a small amount of example data is available — typically, with just one occurrence of each relevant lexical item. Aarne Ranta University of Gothenburg, Department of Computer Science and Engineering; Digital Grammars aarne.ranta@cse.gu.se In this paper, we propose an alternative approach to the automation of this task. While still being data-driven, our method is also grammar-based and, as such, capable of extracting meaningful correspondences even from individual sentence pairs. A further advantage of performing syntactic a"
2021.cnl-1.2,2016.lilt-13.4,0,0.0142675,", but not to automate the generation of a morphologically-aware translation lexicon for a generative grammar. This is where GF comes into play: after correspondences are inferred from a parallel text, our system is able to convert them to GF grammar rules, easy to embed in a domain-specific grammar but also making it immediate to carry out small-scale translation experiments using pre-existing grammatical constructions implemented in GF’s Resource Grammar Library (RGL), which covers the morphology and basic syntax of over 30 languages. This is enabled by gf-ud, a conversion tool described in (Kolachina and Ranta, 2016) and (Ranta and Kolachina, 2017). Concretely, then, the system we propose consists of a UD parser, an alignment module based on UD tree comparison and a program, based on gf-ud, that converts them into the rules of a GF translation lexicon. 2.1 Extracting concepts The core part of the system outlined above is the alignment module. Its function is to extract alignments from parallel bilingual UD treebanks. The outline of the algorithm is given in the following pseudocode: procedure EXTRACT(criteria,(t, u)) alignments = ∅ if (t, u) matches any alignment criteria then alignments += (t, u) for (t0"
2021.cnl-1.2,P00-1056,0,0.615255,"grammatical correctness. However, lexical exactness is, in this context, just as important as grammaticality. An important part of the design of a Controlled Natural Language (CNL) is the creation of a high-quality translation lexicon, preserving both semantics and grammatical correctness. A translation lexicon is often built manually, which is a time consuming task requiring significant linguistic knowledge. When the task is based on a corpus of parallel example sentences, part of this process can be automated by means of statistical word and phrase alignment techniques (Brown et al., 1993; Och and Ney, 2000; Dyer et al., 2013). None of them is, however, suitable for the common case in which only a small amount of example data is available — typically, with just one occurrence of each relevant lexical item. Aarne Ranta University of Gothenburg, Department of Computer Science and Engineering; Digital Grammars aarne.ranta@cse.gu.se In this paper, we propose an alternative approach to the automation of this task. While still being data-driven, our method is also grammar-based and, as such, capable of extracting meaningful correspondences even from individual sentence pairs. A further advantage of pe"
2021.cnl-1.2,petrov-etal-2012-universal,0,0.0612735,"Missing"
2021.cnl-1.2,2020.cl-2.6,1,0.83814,"nces available, statistical alignment tools can help automate part of the process, but they are not suitable for small datasets and do not always perform well with complex multiword expressions. In addition, the correspondences between word forms obtained in this way cannot be used directly. Addressing these problems, we propose a grammar-based approach to this task and put it to test in a simple translation pipeline. 1 Introduction Grammar-based translation pipelines such as those based on Grammatical Framework (GF) have been successfully employed in domain-specific Machine Translation (MT) (Ranta et al., 2020). What makes these systems well suited to the task is the fact that, when we constrain ourselves to a specific domain, where precision is often more important than coverage, they can provide strong guarantees of grammatical correctness. However, lexical exactness is, in this context, just as important as grammaticality. An important part of the design of a Controlled Natural Language (CNL) is the creation of a high-quality translation lexicon, preserving both semantics and grammatical correctness. A translation lexicon is often built manually, which is a time consuming task requiring significa"
2021.cnl-1.2,W17-0414,1,0.855879,"tion of a morphologically-aware translation lexicon for a generative grammar. This is where GF comes into play: after correspondences are inferred from a parallel text, our system is able to convert them to GF grammar rules, easy to embed in a domain-specific grammar but also making it immediate to carry out small-scale translation experiments using pre-existing grammatical constructions implemented in GF’s Resource Grammar Library (RGL), which covers the morphology and basic syntax of over 30 languages. This is enabled by gf-ud, a conversion tool described in (Kolachina and Ranta, 2016) and (Ranta and Kolachina, 2017). Concretely, then, the system we propose consists of a UD parser, an alignment module based on UD tree comparison and a program, based on gf-ud, that converts them into the rules of a GF translation lexicon. 2.1 Extracting concepts The core part of the system outlined above is the alignment module. Its function is to extract alignments from parallel bilingual UD treebanks. The outline of the algorithm is given in the following pseudocode: procedure EXTRACT(criteria,(t, u)) alignments = ∅ if (t, u) matches any alignment criteria then alignments += (t, u) for (t0 , u0 ) in SORT(SUBTS(t)) × SORT"
2021.cnl-1.2,L16-1680,0,0.0231836,"Missing"
C00-1036,W00-1404,1,0.361901,"mber x0 such that x is smaller than x0, where x is an arbitrary number given in the context (for the sake of Universal Introduction). 3 IG : Interaction Grammars We have just described an approach to solving the limitations of usual XML tools for multilingual document authoring which originates in the tradition of constructive type-theory and mathematical proof editors. We will now sketch an approach strongly inspired by GF but which formally is more in the tradition of logic-programming based unification grammars, and which is currently under development at Xerox Research Centre Europe (see (Brun et al., 2000) for a more extended description of this project). Definite Clause Grammars, or DCG’s, (Pereira and Warren, 1980), are possibly the simplest unificationbased extension of context-free grammars, and have good reversibility properties which make them adapted both to parsing and to generation. A typical view of what a DCG rule looks like is the following:5 a(a1(B,C,...)) --> <text1>, b(B), <text2>, c(C), <text3>, ... {constraints(B,C,...)}. This rule expresses the fact that (1) some abstract structure a1(B,C,...) is in category a if the structure B is in category b, the structure C in category c,"
C00-1036,C96-1043,0,0.0364204,"thoring in which the author is guided in the specification of the document content, and where the system is responsible 4 There are authoring situations in which it may be necessary for the user to introduce new semantic labels corresponding to expressive needs not foreseen by the creator of the original DTD. To handle such situations, it is useful to view the DTD’s as open-ended objects to which new semantic labels and types can be added at authoring time. for generating from this content textual output in several languages simultaneously (see (Power and Scott, 1998; Hartley and Paris, 1997; Coch, 1996)). Now there are some obvious problems with this view, due to the current limitations of XML tools. Limitations of XML for multilingual document authoring. The first, possibly most serious, limitation originates in the fact that a standard DTD is severely restricted in the semantic dependencies it can express between two subtrees in the document structure. Thus, if in the description of a contact, a city of residence is included, one may want to constrain such an information depending on the country of residence; or, in the aircraft maintenance manual example, one might want to automatically i"
C00-1036,P98-2173,0,0.170397,"on to the enterprise of Multilingual Document Authoring in which the author is guided in the specification of the document content, and where the system is responsible 4 There are authoring situations in which it may be necessary for the user to introduce new semantic labels corresponding to expressive needs not foreseen by the creator of the original DTD. To handle such situations, it is useful to view the DTD’s as open-ended objects to which new semantic labels and types can be added at authoring time. for generating from this content textual output in several languages simultaneously (see (Power and Scott, 1998; Hartley and Paris, 1997; Coch, 1996)). Now there are some obvious problems with this view, due to the current limitations of XML tools. Limitations of XML for multilingual document authoring. The first, possibly most serious, limitation originates in the fact that a standard DTD is severely restricted in the semantic dependencies it can express between two subtrees in the document structure. Thus, if in the description of a contact, a city of residence is included, one may want to constrain such an information depending on the country of residence; or, in the aircraft maintenance manual exam"
C00-1036,C98-2168,0,\N,Missing
detrez-etal-2014-sharing,E12-1066,1,\N,Missing
detrez-etal-2014-sharing,P02-1040,0,\N,Missing
detrez-etal-2014-sharing,W05-0909,0,\N,Missing
detrez-etal-2014-sharing,W04-3250,0,\N,Missing
detrez-etal-2014-sharing,2011.mtsummit-papers.64,1,\N,Missing
detrez-etal-2014-sharing,2012.eamt-1.61,1,\N,Missing
E09-2003,E09-1009,1,0.661867,"based solely on HTTP GET requests. It returns responses in JavaScript Object Notation (JSON, Crockford, 2006). The serverside program is distributed as part of the GF software distribution, under the GNU General Public License (GPL). The program is generic, in the sense that it can be used with any PGF grammar without any modification of the program. 2 2.2 Parsing and Word Prediction For each concrete syntax in a PGF file, there is a parsing grammar, which is a Parallel Multiple Context Free Grammar (PMCFG, Seki et al., 1991). The PGF interpreter uses an efficient parsing algorithm for PMCFG (Angelov, 2009) which is similar to the Earley algorithm for CFG. The algorithm is top-down and incremental which makes it possible to use it for word completion. When the whole sentence is known, the parser just takes the tokens one by one and computes the chart of all possible parse trees. If the sentence is not yet complete, then the known tokens can be used to compute a partial parse chart. Since the algorithm is topdown it is possible to predict the set of valid next tokens by using just the partial chart. The prediction can be used in applications to guide the user to stay within the coverage of the gr"
E09-2015,I05-2035,0,0.0496909,"Missing"
E09-2015,W07-1801,1,0.902998,"Missing"
E09-2015,2007.sigdial-1.39,1,0.898256,"Missing"
E09-2015,W02-1503,0,0.0605254,"Missing"
E09-2015,C00-1036,1,0.815699,"Missing"
E09-2015,W07-1803,1,0.904887,"Missing"
E09-2015,P98-2173,0,0.0551861,"Missing"
E09-2015,C98-2168,0,\N,Missing
E12-1066,W06-3209,0,0.127821,"Missing"
E12-1066,clement-etal-2004-morphology,0,0.619294,"Missing"
E12-1066,D11-1057,0,0.185128,"Missing"
E12-1066,J94-3001,0,0.358586,"Missing"
E12-1066,W04-2104,0,0.0824068,"Missing"
E14-2011,E14-1039,1,0.801419,"Missing"
E14-2011,W08-1502,1,0.811375,"arne Ranta University of Gothenburg aarne@chalmers.se Query (What Age (Name ”Madonna”)) English: How old is Madonna? Finnish: Kuinka vanha Madonna on? French: Quel aˆ ge a Madonna? Italian: Quanti anni ha Madonna? In recent years much focus in GF has been put on cloud applications (Ranta et al., 2010) and on mobile apps, for both Android (D´etrez and Enache, 2010) and iOS (Djupfeldt, 2013). They all implement text-based phrasebooks, whereas Alum¨ae and Kaljurand (2012) have built a speechenabled question-answering system for Estonian. An earlier speech translation system in GF is presented in Bringert (2008). All embedded GF systems are based on a standardized run-time format of GF, called PGF (Portable Grammar Format; Angelov et al. 2009, Angelov 2011). PGF is a simple “machine language”, to which the much richer GF source language is compiled by the GF grammar compiler. PGF being simple, it is relatively straightforward to write interpreters that perform parsing and linearizations with PGF grammars. The first mobile implementations were explicitly designed to work on small devices with limited resources. Thus they work fine for small grammars (with up to hundreds of rules and lexical entries pe"
E14-2011,P10-4012,1,0.824127,"the system to new languages and for special purposes. Thus the architecture can be used for controlled-language-like translators that deliver very high quality, which is the traditional strength of GF. However, this paper focuses on a general-purpose system that allows arbitrary input. It covers eight languages. 1 Aarne Ranta University of Gothenburg aarne@chalmers.se Query (What Age (Name ”Madonna”)) English: How old is Madonna? Finnish: Kuinka vanha Madonna on? French: Quel aˆ ge a Madonna? Italian: Quanti anni ha Madonna? In recent years much focus in GF has been put on cloud applications (Ranta et al., 2010) and on mobile apps, for both Android (D´etrez and Enache, 2010) and iOS (Djupfeldt, 2013). They all implement text-based phrasebooks, whereas Alum¨ae and Kaljurand (2012) have built a speechenabled question-answering system for Estonian. An earlier speech translation system in GF is presented in Bringert (2008). All embedded GF systems are based on a standardized run-time format of GF, called PGF (Portable Grammar Format; Angelov et al. 2009, Angelov 2011). PGF is a simple “machine language”, to which the much richer GF source language is compiled by the GF grammar compiler. PGF being simple,"
P10-4012,E09-2003,1,0.830522,"Missing"
P10-4012,P06-2062,0,0.0207824,"the complexity of natural languages. This task is where GF claims perhaps the highest advantage over other approaches to special-purpose grammars. The two main assets are: • Programming language support: GF is a modern functional programming language, with a powerful type system and module system supporting modular and collaborative programming and reuse of code. • RGL, the GF Resource Grammar Library, implementing the basic linguistic details of languages: inflectional morphology and syntactic combination functions. The RGL covers fifteen languages at the moment, shown in Figure 1; see also Khegai 2006, El Dada and Ranta 2007, Angelov 2008, Ranta 2009a,b, and Enache et al. 2010. To give an example of what the library provides, let us first consider the inflectional morphology. It is presented as a set of lexicon-building functions such as, in English, mkV : Str -> V i.e. function mkV, which takes a string (Str) as its argument and returns a verb (V) as its value. The verb is, internally, an inflection table containing all forms of a verb. The function mkV derives all these forms from its argument string, which is the infinitive form. It predicts all regular variations: (mkV ""walk"") yields t"
P10-4012,W02-1503,0,0.0737561,"Missing"
P10-4012,W07-1803,1,0.695118,"her than one universal interlingua. This makes the interlingual approach more light-weight and feasible than in systems assuming one universal interlingua, such as Rosetta and UNL, Universal Networking Language5 . It also gives more precision to special-purpose translation: the interlingua of a GF translation system (i.e. the abstract syntax of a multilingual grammar) can encode precisely those structures and distinctions that are relevant for the task at hand. Thus an interlingua for mathematical proofs (Hallgren and Ranta 2000) is different from one for commands for operating an MP3 player (Perera and Ranta 2007). The expressive power of the logical framework is sufficient for both kinds of tasks. One important source of inspiration for GF was the WYSIWYM system (Power and Scott 1998), which used domain-specific interlinguas and produced excellent quality in multilingual generation. But the generation components were hard-coded in the program, instead of being defined declaratively as in GF, and they were not usable in the direction of parsing. Figure 1: A multilingual GF grammar with reversible mappings from a common abstract syntax to the 15 languages currently available in the GF Resource Grammar L"
P10-4012,C00-1036,1,0.940954,"Missing"
P10-4012,P98-2173,0,0.0150413,"UNL, Universal Networking Language5 . It also gives more precision to special-purpose translation: the interlingua of a GF translation system (i.e. the abstract syntax of a multilingual grammar) can encode precisely those structures and distinctions that are relevant for the task at hand. Thus an interlingua for mathematical proofs (Hallgren and Ranta 2000) is different from one for commands for operating an MP3 player (Perera and Ranta 2007). The expressive power of the logical framework is sufficient for both kinds of tasks. One important source of inspiration for GF was the WYSIWYM system (Power and Scott 1998), which used domain-specific interlinguas and produced excellent quality in multilingual generation. But the generation components were hard-coded in the program, instead of being defined declaratively as in GF, and they were not usable in the direction of parsing. Figure 1: A multilingual GF grammar with reversible mappings from a common abstract syntax to the 15 languages currently available in the GF Resource Grammar Library. smaller scale as regards languages and application domains. A running demo system is available at http: //grammaticalframework.org:41296. 2 Multilingual Grammars The t"
P10-4012,C90-3045,0,0.274277,"rs. Thus GF is comparable with formalism such as HPSG (Pollard and Sag 1994), LFG (Bresnan 1982) or TAG (Joshi 1985). The novel feature of GF is the notion of multilingual grammars, which describe several languages simultaneously by using a common representation called abstract syntax; see Figure 1. In a multilingual GF grammar, meaning-preserving translation is provided as a composition of parsing and generation via the abstract syntax, which works as an interlingua. This model of translation is different from approaches based on other comparable grammar formalisms, such as synchronous TAGs (Shieber and Schabes 1990), Pargram (Butt & al. 2002, based on LFG), LINGO Matrix (Bender and Flickinger 2005, based on HPSG), and CLE (Core Language Engine, Alshawi 1992). These approaches use transfer rules between individual languages, separate for each pair of languages. Being interlingua-based, GF translation scales up linearly to new languages without the quadratic blowup of transfer-based systems. In transfer-based sys4 3 Grammars and Ontologies Parallel to the first development efforts of GF in the late 1990’s, another framework idea was emerging in web technology: XML, Extensible Mark-up Language, which unlike"
P10-4012,E09-1009,1,\N,Missing
P10-4012,C98-2168,0,\N,Missing
P10-4012,W90-0102,0,\N,Missing
P10-4012,I05-2035,0,\N,Missing
R11-1010,W02-1503,0,0.0577202,"Missing"
R11-1010,Y10-1020,1,0.935436,"the work of a noun (Verma, 1974). Now we show the structure of noun phrase in our implementation, followed by the description of its different parts. Morphology Every grammar in GF resource grammar library has a test lexicon, which is built through the lexical functions called the lexical paradigms; see (Bringert et el, 2011) for synopsis. These paradigms take lemma of a word and make finite inflection tables, containing different forms of the word, according to the lexical rules of that particular language. A suite of Punjabi resources including morphology and a big lexicon are reported by (Humayoun and Ranta, 2010). With minor required adjustments, we have reused morphology and a subset of that lexicon, as a test lexicon of about 450 words for our grammar implementation. However, the morphological details are beyond the scope of this paper and we refer to (Humayoun and Ranta, 2010) for more details on Punjabi morphology. 3. Structure: In GF, we represent the NP as a record with three fields, labeled as: ‘s’ , ‘a’ and ‘isPron’: NP: Type={s a isPron : NPCase => Str ; : Agr ; : Bool } ; The label ‘s’ is an inflection table from NPCase to string (NPCase => Str). NPCase has two constructs (NPC Case, and NPEr"
W07-1803,W07-1801,0,0.102935,"Missing"
W07-1803,rayner-etal-2006-regulus,0,0.0637896,"Missing"
W10-3220,W04-1614,0,0.0292192,"Pargram project (Butt et el., 2007) aims at building a set of parallel grammars including Urdu. The grammars in Pargram are connected with each other by transfer functions, rather than a common representation. Further, the Urdu grammar is still one of the least implemented grammars in Pargram at the moment. This project is based on the theoretical framework of lexical functional grammar (LFG). Other than Pargram, most work is based on LFG and translation is unidirectional i.e. from English to Urdu only. For instance, English to Urdu MT System is developed under the Urdu Localization Project (Hussain, 2004), (Sarfraz and Naseem, 2007) and (Khalid et el., 2009). Similarly, (Zafer and Masood, 2009) reports another English-Urdu MT system developed with example based approach. On the other hand, (Sinha and Mahesh, 2009) presents a strategy for deriving Urdu sentences from English-Hindi MT system. However, it seems to be a partial solution to the problem. 8. The common resource grammar API does not cover all the aspects of Urdu language, and nongeneralizable language-specific features are supposed to be handled in language-specific modules. In our current implementation of Urdu resource grammar we ha"
W10-3220,W02-1503,0,\N,Missing
W13-2715,W05-1617,0,0.0314627,"to form queries using the standard query language SPARQL. The generation and retrieval system builds on W3C standards and is available for further research. 1 Introduction As the amount of cultural data available on the Semantic Web is expanding (Dekkers et al., 2009; Brugman et al., 2008), the demand of accessing this data in multiple languages is increasing (Stiller and Olensky, 2012). There have been several applications that applied Natural Language Generation (NLG) technologies to allow multilingual access to Semantic Web ontologies (Androutsopoulos et al., 2001; O’Donnell et al., 2001; Androutsopoulos and Karkaletsis, 2005; Androutsopoulos and Karkaletsis, 2007; Davies, 2009; Bouayad-Agha et al., 2012). The above authors have shown it is necessary to have an extensive lexical and syntactic knowledge when generating multilingual natural language from Semantic Web ontologies. However, because previous applications are mainly concerned with two or three languages, it is still not clear how to minimize the efforts in assigning lexical and syntactic knowledge for the purpose of enhancing automatic generation of adequate descriptions in multiple languages. This paper presents our work on making Cultural Heritage (CH)"
W13-2715,brugman-etal-2008-common,0,0.380594,"Missing"
W13-2715,W11-4102,1,0.805202,"Missing"
W14-1401,E14-1039,0,0.0277749,"Missing"
W14-1401,J93-2004,0,0.0605708,"in this case (but did not need to do so elsewhere). cat NP (d : Dom) cat VP (d : Dom)(x : Args) fun PredVP : (d : Dom) -> (x : Args) -> NP d -> VP d x -> Cl x The predication rule checks that the NP and the VP have the same domain. 6 Evaluation Coverage. The dependent type system for verbs, verb phrases, and clauses is a generalization of the old Resource Grammar Library (Ranta, 2009), which has a set of hard-wired verb subcategories and a handful of slash categories. While it covers “all usual cases”, many logically possible ones are missing. Some such cases even appear in the Penn treebank (Marcus et al., 1993), requiring extra rules in the GF interpretation of the treebank (Angelov, 2011). An example is a function of type V (c np (c vp O)) -> VPC (c np O) -> VP (c np O) which is used 12 times, for example in This is designed to get the wagons in a circle and defend the smoking franchise. It has been easy to write conversion rules showing that the old coverage is preserved. But it remains future work to see what new cases are covered by the increased generality. 7 1. Extraction. cat cat fun fun fun QCl (x : Args) IP QuestCl : (x : Args) -> Cl x -> QCl x QuestVP : (x : Args) -> IP -> VP x -> QCl x Qu"
W14-1401,P01-1033,0,0.0842884,"Missing"
W14-1401,C82-1028,0,0.239627,"born via two additions to the natural language interface idea. The first one was multilinguality: one and the same tectogrammar can be given multiple phenogrammars. The second addition was parsing: the phenogrammar, which was initially just linearization (generating strings from type theoretical formulas), was reversed to rules that parse natural language into type theory. The result was a method for translation, which combines parsing the source language with linearization into the target language. This idea was indeed suggested in (Curry, 1961), and applied before GF in the Rosetta project (Landsbergen, 1982), which used Montague’s analysis trees as tectogrammar. GF can be seen as a formalization and generalization of Montague grammar. Formalization, because it introduces a formal notation for the linearization rules that in Montague’s work were expressed informally. Generalization, because of multilinguality and also because the type system for analysis trees has dependent types. Following the terminology of programming language theory, the tectogrammar is in GF called the abstract syntax whereas the phenogrammar is called the concrete syntax. As in compilers and 3 Example: subject-verb-object se"
W14-5508,C12-1046,0,0.0156216,"on has some script-related issues which should be ﬁxed in future. When it comes to interlingua-based arbitrary machine translation, an important concern is the size of lexicons. We are aware of the fact that the size of our lexicons is not comparable to some of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is in millions. We have plan to extend the size of lexicons using some of the other publicly available resources (such as Hindi WordNet) and/or using parallel corpus. The development of bilingual lexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al., 2012), and the same ideas can be applied in our case. 6 Conclusion We have shown how to use existing lexical resources such as WordNets to develop an interlingual translation lexicon in GF, and how to use it for the WSD task in an arbitrary text translation pipeline. The improvements in the translation quality (lexical), shown by examples in Section 4, are encouraging and motivate further work in this direction. However, it should be noted that there is still a lot of work to be done (especially in the open domain text parsing and parse-tree disambiguation phases of the translat"
W14-5508,E12-1066,1,0.674969,"ame is composed of lemma, sense oﬀset and a type ‘t’, where lemma and sense oﬀset are same as in the Princeton WordNet, while ‘t’ is one of the morphological types in GF resource grammars. This abstract representation will serve as a pivot for all concrete representations, which are described next. 2 This module has deﬁnitions of diﬀerent morphological and syntactic categories in the GF resource grammar library 58 2.2 GF Concrete Lexicons We build the concrete representations for diﬀerent languages using the translations obtained from the Universal WordNet data and GF morphological paradigms (Détrez and Ranta, 2012; Bringert et al., 2011). The Universal WordNet translations are tagged with a sense oﬀset from WordNet 3.03 and also with a conﬁdence score. As, an example consider the following segment form the Universal WordNet data, showing German translations for the noun synset with oﬀset ‘13810818’ and lemma ‘rest’ (in the sense of ‘remainder’). n13810818 Rest n13810818 Abbrand n13810818 Ruckstand 1.052756 0.95462 0.924376 Each entry is of the following general type. posSenseOffset translation confidence-score If we have more than one candidate translation for the same sense (as in the above case), we"
W14-5508,C00-1036,1,0.695351,"e. The parsing and linearization component are deﬁned by using Parallel Multiple Context-Free Grammars (PMCFG, (Seki et al., 1991), (Ljunglöf, 2004)), which give GF an expressive power between mildly and fully context-sensitive grammars. Thus GF can easily handle with language-speciﬁc variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet"
W14-5508,2012.eamt-1.61,1,0.788365,"speciﬁc variations in morphology, word order, and discontinuous constituents, while maintaining a shared abstract syntax. Historically, the main use of GF has been in controlled language implementations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the"
W14-5508,W12-5001,1,0.929083,"ntations, e.g., (Ranta and Angelov, 2010; Angelov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the latter two. We report ﬁrst a method of using the WordNets (Princeton and Universal) to build an interlingual full-form, multiple sense translation lexicon. Then, we show how these lexicons together"
W14-5508,C12-1139,0,0.0141948,"ated issues which should be ﬁxed in future. When it comes to interlingua-based arbitrary machine translation, an important concern is the size of lexicons. We are aware of the fact that the size of our lexicons is not comparable to some of the other similar systems such as ATLAS-II (Fujitsu), where the size of lexicons is in millions. We have plan to extend the size of lexicons using some of the other publicly available resources (such as Hindi WordNet) and/or using parallel corpus. The development of bilingual lexicons form parallel corpus have been previously explored (Delpech et al., 2012; Qian et al., 2012), and the same ideas can be applied in our case. 6 Conclusion We have shown how to use existing lexical resources such as WordNets to develop an interlingual translation lexicon in GF, and how to use it for the WSD task in an arbitrary text translation pipeline. The improvements in the translation quality (lexical), shown by examples in Section 4, are encouraging and motivate further work in this direction. However, it should be noted that there is still a lot of work to be done (especially in the open domain text parsing and parse-tree disambiguation phases of the translation pipeline) to bri"
W14-5508,R11-1010,1,0.820949,"ov and Enache, 2010; Ranta et al., 2012) and natural language generation, e.g., (Dymetman et al., 2000), both applied in multilingual settings with up to 15 parallel languages. In recent years, the coverage of GF grammars and the processing performance has enabled open-domain tasks such as treebank parsing (Angelov, 2011) and hybrid translation of patents (Enache et al., 2012). The general purpose Resource Grammar Library (RGL)(Ranta, 2011) has grown to 30 languages. It includes the major European languages, South Asian languages like Hindi/Urdu (Prasad and Shafqat, 2012), Nepali and Punjabi (Shafqat et al., 2011), the Southeast Asian language Thai, and Japanese and Chinese. However, GF has yet not been exploited for arbitrary text parsing and translation. To do this, we have to meet several challenges: robust parsing, parse-tree disambiguation, word sense disambiguation, and development of a wide-coverage interlingual translation lexicon. This paper focuses on the latter two. We report ﬁrst a method of using the WordNets (Princeton and Universal) to build an interlingual full-form, multiple sense translation lexicon. Then, we show how these lexicons together with a word sense disambiguation tool can b"
W14-5508,P10-4014,0,0.0353592,"23862 12425 Language German Finnish Hindi Number of Entries 49439 27673 16654 Table 1: Lexicon Coverage Statistics 3 4 However, in our concrete lexicons we match them to WordNet 1.7.1 for the reasons mentioned previously See (Bringert et al., 2011) for more details on these paradigms 59 3 System architecture Figure 1 shows an architecture of the translation pipeline. The architecture is inter-lingual and uses the Resource Grammar Library (RGL) of Grammatical Framework (Ranta, 2011) as the syntax and semantics component, Penn Treebank data for parse-tree disambiguation and IMS(It Makes Sense)(Zhong and Ng, 2010) as a word sense disambiguation tool. Even though the syntax, semantics and parse-tree disambiguation are not the main topics of this paper, we give the full architecture to show where the work reported in this paper ﬁts. Internal GF resources (e.g. resource grammars and dictionaries) are shown in rectangles while the external components (e.g. PennTreebank and IMS(Zhong and Ng, 2010): a wide coverage word sense disambiguation system for arbitrary text.) are shown in double-stroked rectangles. With reference to Figure 1: The input is parsed using English resource grammar (EngRG) and a comprehen"
W15-3015,W14-5508,1,0.899764,"Missing"
W15-3015,E12-1066,1,\N,Missing
W15-3015,E14-1039,0,\N,Missing
W15-3117,C00-1036,1,0.790367,"useful for language pairs with a lack of parallel texts. 100 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 100–109, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing (Zimina, 2012). Adapting a GF grammar to a new setting, such as a dialogue system or a domainspecific translator, can be accomplished in a few days (Perera and Ranta, 2007; Ranta et al., 2012). GF started at Xerox Research Centre Europe in 1998 as a part of a project on multilingual document authoring (Dymetman et al., 2000). Released open-source later, GF today is a collaborative project with over 150 developers around the world. In China, GF courses have been organized at Shanghai Jiao Tong University and at Sun YatSen University in Guangzhou. The standard textbook on GF, (Ranta, 2011) has recently been translated to Chinese (Ranta, 2014b). The GF software and grammar resources, including Chinese, are available from the GF homepage1 . The licenses of the grammar reaources (LGPL and BSD) permit all kinds of uses, including commercial applications. This paper gives an overview of the GF resources and applications"
W15-3117,E14-2011,1,0.856372,"abstract syntax consisting of 86 categories and 216 functions. In addition to this, it a test lexicon of 524 word senses. A language implementation with linearizations for all these functions is accessible in all parts of the “GF ecosystem”, via the common abstract syntax. The GF toolset The GF set of tools has several components: • The GF programming language and its compiler (Ranta, 2010). • PGF, Portable Grammar Format, the “machine language of GF” generated by the GF compiler (Angelov et al., 2009). • Runtime interpreters for PGF, enabling mobile and web applications (Ranta et al., 2010; Angelov et al., 2014). • The Resource Grammar Library (RGL), currently comprising 30 languages (Ranta, 2009). • A wide-coverage translation system (Hallgren, 2014 2015). • Controlled language applications (Angelov and Ranta, 2009). • Conversions of GF grammars and trees to other formats, such as speech recognition grammars (Bringert, 2007), finite state automata in the Xerox format (Beesley and Karttunen, 2003), dependency trees in the CoNLL format (Eisner, 2007), and phrase tables in the Giza++ format (Och and Ney, 2003). The last item, conversions, guarantees that GF is not a closed world, but that GF grammars c"
W15-3117,E06-1008,0,0.0147289,"syntax, which has replaced the direct transfer of synchronic grammars in modern compiler construction (Appel, 1998). Tables and records are related to unification grammars (Shieber, 1986), but the expressive power of GF is lower: it is equivalent to PMCFG (parallel multiple context-free grammars) (Seki et al., 1991), which enjoys polynomial parsing. The word “parallel” in PMCFG means that an expression may be duplicated in linearization. This is not needed in all languages, but Chinese reduplication questions are an example of it. 3 Figure 1: Languages in GF RGL. pensate for the lack of data (Jonson, 2006). The key for a language to enter the GF ecosystem is an RGL implementation. The RGL has a core abstract syntax consisting of 86 categories and 216 functions. In addition to this, it a test lexicon of 524 word senses. A language implementation with linearizations for all these functions is accessible in all parts of the “GF ecosystem”, via the common abstract syntax. The GF toolset The GF set of tools has several components: • The GF programming language and its compiler (Ranta, 2010). • PGF, Portable Grammar Format, the “machine language of GF” generated by the GF compiler (Angelov et al., 20"
W15-3117,W07-1801,0,0.140517,"onents: • The GF programming language and its compiler (Ranta, 2010). • PGF, Portable Grammar Format, the “machine language of GF” generated by the GF compiler (Angelov et al., 2009). • Runtime interpreters for PGF, enabling mobile and web applications (Ranta et al., 2010; Angelov et al., 2014). • The Resource Grammar Library (RGL), currently comprising 30 languages (Ranta, 2009). • A wide-coverage translation system (Hallgren, 2014 2015). • Controlled language applications (Angelov and Ranta, 2009). • Conversions of GF grammars and trees to other formats, such as speech recognition grammars (Bringert, 2007), finite state automata in the Xerox format (Beesley and Karttunen, 2003), dependency trees in the CoNLL format (Eisner, 2007), and phrase tables in the Giza++ format (Och and Ney, 2003). The last item, conversions, guarantees that GF is not a closed world, but that GF grammars can be reused in other ecosystems. The advantage of GF is that it enables programming on a higher level than e.g. hand-written speech recognition grammars (Perera and Ranta, 2007). This is essential in order for grammar writing to be competitive with machine learning and statistics. Even in statistical systems, writing"
W15-3117,J93-2004,0,0.0505747,"e chunking cannot do this. Since Chinese places prepositional phrases in front of they modify (Figure 2), English PP attachment is an ambiguity that cannot be solved by syntax alone: parsing provides both analyses and their linearizations, but it cannot select the correct one, even in clear cases like those in Figure 2. For the final disambiguation, either deeper semantic analysis or an accurate statistical model is needed. Semantic analysis is easy to implement in a CNL but hard to scale up. Thus the WCT uses statistical disambiguation based on probabilities estimated from the Penn treebank (Marcus et al., 1993). The Penn trees are converted to abstract syntax trees of the RGL, and the frequencies of functions are computed (Angelov, 2011). As the trees are common to all the 30 languages of the RGL, the same model can be used for all of them. But a more adequate model would of course be expected from native treebanks, such as the Chinese Penn treebank (Xue et al., 2005), which remains as future work. The WCT can be optimized for a special domain by combining it with an Embedded CNL (Ranta, 2014a). This means that CNL analyses are given priority over syntactic and chunk-based analyses, whenever availab"
W15-3117,J03-1002,0,0.0095365,"• Runtime interpreters for PGF, enabling mobile and web applications (Ranta et al., 2010; Angelov et al., 2014). • The Resource Grammar Library (RGL), currently comprising 30 languages (Ranta, 2009). • A wide-coverage translation system (Hallgren, 2014 2015). • Controlled language applications (Angelov and Ranta, 2009). • Conversions of GF grammars and trees to other formats, such as speech recognition grammars (Bringert, 2007), finite state automata in the Xerox format (Beesley and Karttunen, 2003), dependency trees in the CoNLL format (Eisner, 2007), and phrase tables in the Giza++ format (Och and Ney, 2003). The last item, conversions, guarantees that GF is not a closed world, but that GF grammars can be reused in other ecosystems. The advantage of GF is that it enables programming on a higher level than e.g. hand-written speech recognition grammars (Perera and Ranta, 2007). This is essential in order for grammar writing to be competitive with machine learning and statistics. Even in statistical systems, writing grammars can be a way to comFigure 1 shows the languages currently available in the RGL. The 14 innermost languages, connected with lines with the abstract syntax, have a large lexicon e"
W15-3117,C90-3045,0,0.730474,"ntains an inflection table with nominative and accusative cases, and one labelled a (“agreement”), which contains a record that in turn contains a number n and a person p): lin QuestCl cl = cl.subj ++ cl.vp.verb ++ cl.vp.neg ++ cl.vp.verb ++ cl.obj (Questions with particle ma could be given as an lin i_NP = { s = table {Nom => ""I"" ; Acc => ""me""} ; alternative linearization.) Multilingual grammars are a generalization a = {n = Sg ; p = Per1} of synchronous grammars (Aho and Ullman, } 102 1969), originally defined for context-free grammars but later generalized to tree-adjoining grammars (TAG) (Shieber and Schabes, 1990). GF adds to synchronous grammars the explicit notion of abstract syntax, which has replaced the direct transfer of synchronic grammars in modern compiler construction (Appel, 1998). Tables and records are related to unification grammars (Shieber, 1986), but the expressive power of GF is lower: it is equivalent to PMCFG (parallel multiple context-free grammars) (Seki et al., 1991), which enjoys polynomial parsing. The word “parallel” in PMCFG means that an expression may be duplicated in linearization. This is not needed in all languages, but Chinese reduplication questions are an example of i"
W15-3117,W07-1803,1,0.916222,"ine translation (MT), grammars written with guidance from linguistic knowledge have the following advantages: • Grammars don’t need so much data, which is useful for language pairs with a lack of parallel texts. 100 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 100–109, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing (Zimina, 2012). Adapting a GF grammar to a new setting, such as a dialogue system or a domainspecific translator, can be accomplished in a few days (Perera and Ranta, 2007; Ranta et al., 2012). GF started at Xerox Research Centre Europe in 1998 as a part of a project on multilingual document authoring (Dymetman et al., 2000). Released open-source later, GF today is a collaborative project with over 150 developers around the world. In China, GF courses have been organized at Shanghai Jiao Tong University and at Sun YatSen University in Guangzhou. The standard textbook on GF, (Ranta, 2011) has recently been translated to Chinese (Ranta, 2014b). The GF software and grammar resources, including Chinese, are available from the GF homepage1 . The licenses of the gram"
W15-3117,P10-4012,1,0.832894,". The RGL has a core abstract syntax consisting of 86 categories and 216 functions. In addition to this, it a test lexicon of 524 word senses. A language implementation with linearizations for all these functions is accessible in all parts of the “GF ecosystem”, via the common abstract syntax. The GF toolset The GF set of tools has several components: • The GF programming language and its compiler (Ranta, 2010). • PGF, Portable Grammar Format, the “machine language of GF” generated by the GF compiler (Angelov et al., 2009). • Runtime interpreters for PGF, enabling mobile and web applications (Ranta et al., 2010; Angelov et al., 2014). • The Resource Grammar Library (RGL), currently comprising 30 languages (Ranta, 2009). • A wide-coverage translation system (Hallgren, 2014 2015). • Controlled language applications (Angelov and Ranta, 2009). • Conversions of GF grammars and trees to other formats, such as speech recognition grammars (Bringert, 2007), finite state automata in the Xerox format (Beesley and Karttunen, 2003), dependency trees in the CoNLL format (Eisner, 2007), and phrase tables in the Giza++ format (Och and Ney, 2003). The last item, conversions, guarantees that GF is not a closed world,"
W15-3117,C10-2162,0,0.0314179,"pose Google translate in the CNL, even though Google can be quite good at idiomatic tourist phrases. The Finnish and Swedish CNLs get better scores because more work has been put to them. In the WCT, Moses is better than GF. It is too early to say how competitive GF can be made in this scenario, but an interesting case would be the translation between Chinese and some other language than English, with less parallel data available to build statistical systems from. GF translation is 8 Related work The Chinese Penn Treebank (Xue et al., 2005) has been used for building grammars. In particular, (Yu et al., 2010) measures the accuracy and coverage of a generated HPSG grammar, and also lists smaller HPSG projects on Chinese. (Zhang et al., 2012) reports on a more comprehensive HPSG grammar and treebank. As for translation, several systems exist between English and Chinese, but for some of the languages in the GF WCT, e.g. Bulgarian and Finnish, only partially documented commercial systems (such as Google translate) are available. As for CNL, (Cardey et al., 2004) makes a suggestion for medical English-Chinese translation, but we haven’t found complete CNL systems for Chinese other than those in GF. 9 C"
W15-3117,W15-3301,1,0.563458,"the internet; downloading stripped-down versions on Android phones is possible, but requires 200 megabytes per language pair. 7 BLEU 84 50 96 61 89 44 21 36 not affected by this problem. It can be objected that the comparison between GF and Google translate is not fair in the CNL case, because the GF grammar was specifically tailored for the domain. But this is in fact the very point: since GF grammars are easy to adapt to specific domains, they are a useful technique when high quality is expected and the coverage can be limited. This way of using grammars has also shown commercial potential (Ranta et al., 2015). Evaluation The wide range of applications of GF creates several things to evaluate. Let us address what is perhaps the most frequent question: translation quality with the usual metrics BLEU and TER. Table 2 shows the first results from an evaluation campaign for English to Chinese translation on two different levels: semantic CNL (the MOLTO phrasebook) and wide-coverage GF translation (WCT). In these evaluations, we have machinetranslated a set of sentences and created the reference translations by human post-editing. The CNL sentences come from a MOLTO test suite, whereas the WCT sentences"
W15-3117,zhang-etal-2012-joint,0,0.0238084,"et better scores because more work has been put to them. In the WCT, Moses is better than GF. It is too early to say how competitive GF can be made in this scenario, but an interesting case would be the translation between Chinese and some other language than English, with less parallel data available to build statistical systems from. GF translation is 8 Related work The Chinese Penn Treebank (Xue et al., 2005) has been used for building grammars. In particular, (Yu et al., 2010) measures the accuracy and coverage of a generated HPSG grammar, and also lists smaller HPSG projects on Chinese. (Zhang et al., 2012) reports on a more comprehensive HPSG grammar and treebank. As for translation, several systems exist between English and Chinese, but for some of the languages in the GF WCT, e.g. Bulgarian and Finnish, only partially documented commercial systems (such as Google translate) are available. As for CNL, (Cardey et al., 2004) makes a suggestion for medical English-Chinese translation, but we haven’t found complete CNL systems for Chinese other than those in GF. 9 Conclusion We have shown the main ideas of GF and how they can be applied in NLP. The most mature applications are controlled-language"
W15-3117,cardey-etal-2004-designing,0,\N,Missing
W15-3117,W90-0102,0,\N,Missing
W15-3301,E14-1039,0,0.159064,"Missing"
W15-3301,P12-3024,0,0.028429,"Missing"
W15-3301,E14-2011,1,0.598753,"Prep m). The RGL has increased the productivity of CNL implementations in GF, so that a system with a few hundred abstract syntax rules can be created in a few days and portable to any new language in a few hours (Ranta et al., 2012). The RGL has recently also scaled up to open-domain translation, due to improved parsing algorithms and statistical disambiguation (Angelov and Ljungl¨of, 2014), chunk-based back-up of syntactic parsing (Ranta, 2014), and the ease of building large lexica with smart paradigms. As a demonstration, a widecoverage translator (WCT) has been released as a mobile app (Angelov et al., 2014). gen o ++ ""pituus on"" ++ m ""la longitud""++ gen o ++""es de""++ m respectively, where Finnish has a different word order and Spanish adds the preposition de. The first GF grammars were small, typically involving up to 200 abstract syntax rules; their context-free expansions could of course be thousands of rules, due to parametric variations such as case. But it soon turned out that writing such grammars from scratch for each application was untenable, as each application had to reimplement morphology and syntax. To relieve this task, the GF Resource Grammar Library (RGL) (Ranta, 2009) was create"
W15-3301,W02-1503,0,0.211822,"Missing"
W15-3301,E12-1066,1,0.774092,"Missing"
W15-3301,C00-1036,1,0.422034,". Section 3 is a very brief introduction to GF. Section 4 outlines the structure of the grammar and the grammar writing process. Section 5 outlines the translation and post-editing workflow. Section 6 gives evaluation on two dimensions: the time taken by grammar writing and post-editing, and the usual scores (BLEU) for translation quality. Section 7 discusses related work. Section 8 concludes, trying to answer the three questions and give recommendations for later work. 2 3 GF and resource grammars GF started at Xerox (XRCE) to support multilingual generation in controlled-language scenarios (Dymetman et al., 2000). A GF grammar consists of an abstract syntax, which captures the semantics of the application domain, and a set of concrete syntaxes, which map abstract syntax trees into strings of different languages. As an example from the domain of this paper, one could have an abstract syntax function The corpus The starting point was a set of texts in Swedish. Most of them had manual translations in English, many also in Finnish and German. The customer was happy with the English translations but wanted to replace the Finnish and German ones, as well as to create Spanish translations. The number of text"
W15-3306,2012.freeopmt-1.2,0,0.0266496,"inction. However, GF was also meant to be a formalism for “ordinary” programmers without linguistic training. Thus the majority of the currently 30 languages included in the GF Resource Grammar Library (Ranta, 2009a) are in fact written by students and scholars in computer science, who find the GF style of programming familiar from other contexts, in particular compiler construction (Appel, 1998). However, the GF approach has a “nerdy” flavour to it, in particular requiring coping with command line tools, text editors, and Haskell libraries. Some programmers are helped by the Eclipse plug-in (Camilleri, 2012), but installing both GF and Eclipse on a personal computer can be a daunting task for many. The present paper describes an attempt to eliminate all trouble with software installation from linguistic grammar writing. We describe a grammar engineering tool that resides in the cloud and can be used in ordinary web browsers. The tool supports writing grammars in the cloud, compiling them to executable parsers and translation systems, and finally running and testing them in the cloud. Thus an entire grammar project can be written and used without installing any specific software. The project can a"
W15-3306,W02-1502,0,0.170934,"Missing"
W15-3306,P10-4012,1,0.886021,"Missing"
W15-3306,E09-2003,1,0.773072,"Missing"
W15-3306,W02-1503,0,\N,Missing
W15-3307,dannells-gruzitis-2014-extracting,1,0.414581,"Missing"
W17-0247,2016.lilt-13.4,1,0.469099,"al Framework with Universal Dependencies Aarne Ranta University of Gothenburg aarne@chalmers.se Prasanth Kolachina University of Gothenburg prasanth.kolachina@gu.se Abstract leaves of UD trees are language-specific, and each language can extend the core descriptions to have a set of its own tags and labels. The relation between trees and strings is not defined by grammar rules, but by constructing a set of example trees — a treebank. From a treebank, a parser is typically built using statistical methods (Nivre, 2006). The abstract syntax trees of GF can be automatically converted to UD trees (Kolachina and Ranta, 2016), by utilizing the shared abstract syntax of GF to allow simultaneous generation of UD trees in many languages. The proposed demo shows tools that use this conversion. An example is shown in Figure 3, whose contents are produced by these tools. The configurations used are defined for the GF Resource Grammar Library (GFRGL) (Ranta, 2009), which currently contains 32 languages. An inverse conversion from UD to GF is work in progress (Ranta and Kolachina, 2017) and can also be shown in the demo. All grammars, configurations, and software are available from the GF homepage. 1 GF (Grammatical Frame"
W17-0247,L16-1262,0,0.0229425,"Missing"
W17-0247,W17-0414,1,0.809103,"r is typically built using statistical methods (Nivre, 2006). The abstract syntax trees of GF can be automatically converted to UD trees (Kolachina and Ranta, 2016), by utilizing the shared abstract syntax of GF to allow simultaneous generation of UD trees in many languages. The proposed demo shows tools that use this conversion. An example is shown in Figure 3, whose contents are produced by these tools. The configurations used are defined for the GF Resource Grammar Library (GFRGL) (Ranta, 2009), which currently contains 32 languages. An inverse conversion from UD to GF is work in progress (Ranta and Kolachina, 2017) and can also be shown in the demo. All grammars, configurations, and software are available from the GF homepage. 1 GF (Grammatical Framework) and UD (Universal Dependencies) are two different approaches using shared syntactic descriptions for multiple languages. GF is a categorial grammar approach using abstract syntax trees and hand-written grammars, which define both generation and parsing. UD is a dependency approach driven by annotated treebanks and statistical parsers. In closer study, the grammatical descriptions in these two approaches have turned out to be very similar, so that it is"
W17-0247,P10-4012,1,0.809019,"ls should in German be fressen’.) PredVP DetCN ComplV2 RConjNP ModCN A configuration consists of an abstract syntax function together with a list of labels, one for each argument of the function. An extended notion of these configurations is described in (Kolachina and Ranta, 2016). The basic algorithm is a topdown tree-transducer that deterministically maps each argument of a function in the abstract syntax tree to its UD label, generating a connected dependency tree. We refer the reader to Kolachina and Ranta (2016) for more details. than a web browser. It builds on GF’s web-based tool set (Ranta et al., 2010). It is accessible via the GF Cloud2 . The currently supported grammar option is “ResourceDemo”; see Figure 2. 4 nsubj head det head head dobj cc head conj head amod UD trees in the PGF web service For grammars that have been equipped with a UD label configuration file, UD trees can be requested 2 http://cloud.grammaticalframework.org/minibar/minibar.html 323 Figure 3: An abstract syntax tree and UD trees in 14 languages. The abstract syntax tree is shown in the middle. The languages corresponding to the UD trees from top-left: Thai, Sindhi, Nepali, French, Icelandic, English, Italian, Bulgari"
W17-0414,E14-2011,1,0.915784,"s (Tiedemann and Agic, 2016). GF’s linearization can convert abstract syntax trees to UD trees (Kolachina and Ranta, 2016). This conversion can be used for generating multilingual (and parallel) treebanks from a given set of GF trees. However, to reach the full potential of the GF-UD correspondence, it would also be useful to go to the opposite direction, to convert UD trees to GF trees. Then one could translate standard UD treebanks to new languages. One could also use dependency parsing as a robust frontend to a translator, which uses GF linearization as a grammaticality-preserving backend (Angelov et al., 2014), or to a logical form generator in the style of (Reddy et al., 2016), but where GF trees give an accurate intermediate representation in the style of (Ranta, 2004). Figure 2 shows both of these scenarios, using the term gf2ud for the conAbstract syntax is a tectogrammatical tree representation, which can be shared between languages. It is used for programming languages in compilers, and has been adapted to natural languages in GF (Grammatical Framework). Recent work has shown how GF trees can be converted to UD trees, making it possible to generate parallel synthetic treebanks for those 30 la"
W17-0414,2016.lilt-13.4,1,0.338331,"es are language-specific, and languages can extend the core tagset and labels to annotate constructions in the language. The relation between trees and strings is not defined by grammar rules, but by constructing a set of example trees—a treebank. From a treebank, a parser is typically constructed by machine learning (Nivre, 2006). There is no mechanical way to translate a UD tree from one language to other languages. But such a translation can be approximated in different ways to bootstrap treebanks (Tiedemann and Agic, 2016). GF’s linearization can convert abstract syntax trees to UD trees (Kolachina and Ranta, 2016). This conversion can be used for generating multilingual (and parallel) treebanks from a given set of GF trees. However, to reach the full potential of the GF-UD correspondence, it would also be useful to go to the opposite direction, to convert UD trees to GF trees. Then one could translate standard UD treebanks to new languages. One could also use dependency parsing as a robust frontend to a translator, which uses GF linearization as a grammaticality-preserving backend (Angelov et al., 2014), or to a logical form generator in the style of (Reddy et al., 2016), but where GF trees give an acc"
W17-0414,L16-1262,0,0.112137,"Missing"
W17-0414,Q16-1010,0,0.0639592,"Missing"
W19-6102,W16-4504,0,0.231356,"sponds to our proposed idea of simultaneous grammar engineering and treebanking. • When 1K ≤ N ≤ 5K sentences5 are available for a language. There are around 18 treebanks in the current UD distribution that match this criterion. While one can argue that these languages are not really underresourced, this setup matches the typical case of training domain-specific parsers either for a particular domain like bio-medical or legal texts. 2 The UD treebanks are taken from the v2.3 distribution. There is ongoing work on developing interlingual lexica from linked data like WordNet (Virk et al., 2014; Angelov and Lobanov, 2016). 4 This approximately corresponds to 20K tokens. 5 This approximately corresponds to 20K – 100K tokens. 3 For each of these use-cases, we train parsing models using data from both human annotated UD treebanks and synthetic treebanks for different sizes of training data. The resulting parsing models are evaluated using labelled attachment scores, obtained by parsing the test set of the UD treebank for the language in question. We experiment with an off-the-shelf transition-based dependency parser that gives good results in the dependency parsing task (Straka and Strakov´a, 2017). In the ideal"
W19-6102,N04-1005,0,0.116923,"amples in multilingual applications. However, the underlying abstractions used to generate the synthetic data are induced from auxiliary corpora. Jonson (2006) show that synthetic corpora generated using a GF grammar can be used to build language models for speech recognition. Experiments in their work show that synthetic indomain examples generated using the grammar when combined with large out-of-domain data result in significant reduction of word error rate of the speech recognizer. This work falls in line with similar approaches to combine corpus driven approaches with rule-based systems (Bangalore and Johnston, 2004), as a way to combine the statistical information available from corpora with good coverage resulting from rule-based abstractions especially when working with restricted domains. In this paper, we restrict ourselves to utilizing synthetic treebanks for parsing, and leave the discussion on ways to combine synthetic treebanks with real treebanks as future work. This choice is primarily motivated by our interest in grammar-based development of dependency treebanks as opposed to the traditional way of treebanking – by training human annotators. 7 Conclusions In the current paper, we propose an al"
W19-6102,N13-1014,0,0.0308276,"the target grammar further reducing the human effort in disambiguation: these approaches face a challenge of under-specification in the source treebanks (Angelov, 2011). In the current paper, we propose a hybrid of these two methods: we use abstract syntax grammars as core linguistic abstraction to generate synthetic treebanks for a grammar that can be translated to target representations with high precision. The question of annotation costs and ways to minimize the dependence on such annotated corpora has been a recurring theme in the field for the last two decades (Ngai and Yarowsky, 2000; Garrette and Baldridge, 2013). This question has also been extensively addressed in the context of dependency treebanks. We revisit this question in context of Universal Dependencies and recent work on the interplay between interlingua grammars and multilingual dependency trees in this scheme (Kolachina and Ranta, 2016; Ranta and Kolachina, 2017; Ranta et al., 2017). The use of interlingua grammars to bootstrap dependency treebanks guarantees two types of consistencies: multilingual treebank consistency and intra-treebank consistency. We study the efficacy of these dependency treebanks using learning curves of a transitio"
W19-6102,E06-1008,0,0.0324072,"strapping UD treebanks is to use ud2gf (Ranta and Kolachina, 2017) as a way to translate existing UD treebanks to GF treebanks, that are licensed by a grammar. The current work also relates to more recent work in data-augmentation for dependency parsing (Sahin and Steedman, 2018) and more generally in NLP (Sennrich et al., 2016). The augmentation methods are designed to address data scarcity by exploiting monolingual corpora or generating synthetic samples in multilingual applications. However, the underlying abstractions used to generate the synthetic data are induced from auxiliary corpora. Jonson (2006) show that synthetic corpora generated using a GF grammar can be used to build language models for speech recognition. Experiments in their work show that synthetic indomain examples generated using the grammar when combined with large out-of-domain data result in significant reduction of word error rate of the speech recognizer. This work falls in line with similar approaches to combine corpus driven approaches with rule-based systems (Bangalore and Johnston, 2004), as a way to combine the statistical information available from corpora with good coverage resulting from rule-based abstractions"
W19-6102,2016.lilt-13.4,1,0.681277,"on to generate synthetic treebanks for a grammar that can be translated to target representations with high precision. The question of annotation costs and ways to minimize the dependence on such annotated corpora has been a recurring theme in the field for the last two decades (Ngai and Yarowsky, 2000; Garrette and Baldridge, 2013). This question has also been extensively addressed in the context of dependency treebanks. We revisit this question in context of Universal Dependencies and recent work on the interplay between interlingua grammars and multilingual dependency trees in this scheme (Kolachina and Ranta, 2016; Ranta and Kolachina, 2017; Ranta et al., 2017). The use of interlingua grammars to bootstrap dependency treebanks guarantees two types of consistencies: multilingual treebank consistency and intra-treebank consistency. We study the efficacy of these dependency treebanks using learning curves of a transition-based parser in a delexicalized parsing setup. The delexicalized parsing setup allows for generation of parallel UD treebanks in multiple languages with minimal prerequisites on language-specific knowledge. Another rationale behind the the current work in the context of cross-lingual pars"
W19-6102,H94-1020,0,0.503459,"rsing, a task of interest when working with languages with no linguistic resources and corpora. Experiments with three languages reveal that simple models for treebank generation are cheaper than human annotated treebanks, especially in the lower ends of the learning curves for delexicalized parsing, which is relevant in particular in the context of lowresource languages. Aarne Ranta University of Gothenburg aarne@chalmers.se 1 Introduction Treebanking remains a vital step in the process of creating linguistic resources for a language – a practice that was established in the last 2-3 decades (Marcus et al., 1994). The process of treebanking involves training human annotators in order to obtain high-quality annotations. This is a human-intensive and costly process where multiple iterations are performed to refine the quality of the linguistic resource. Grammar engineering is a complementary approach to creating linguistic resources: one that requires a different kind of expertise. These two approaches have remained orthogonal for obvious reasons: treebanks are primarily necessary to induce abstractions in NLU (Natural Language Understanding) models from data, while grammars are themselves abstractions"
W19-6102,D11-1006,0,0.0615568,"Missing"
W19-6102,P00-1016,0,0.280879,"banks are matched against the target grammar further reducing the human effort in disambiguation: these approaches face a challenge of under-specification in the source treebanks (Angelov, 2011). In the current paper, we propose a hybrid of these two methods: we use abstract syntax grammars as core linguistic abstraction to generate synthetic treebanks for a grammar that can be translated to target representations with high precision. The question of annotation costs and ways to minimize the dependence on such annotated corpora has been a recurring theme in the field for the last two decades (Ngai and Yarowsky, 2000; Garrette and Baldridge, 2013). This question has also been extensively addressed in the context of dependency treebanks. We revisit this question in context of Universal Dependencies and recent work on the interplay between interlingua grammars and multilingual dependency trees in this scheme (Kolachina and Ranta, 2016; Ranta and Kolachina, 2017; Ranta et al., 2017). The use of interlingua grammars to bootstrap dependency treebanks guarantees two types of consistencies: multilingual treebank consistency and intra-treebank consistency. We study the efficacy of these dependency treebanks using"
W19-6102,W17-0414,1,0.745679,"eebanks for a grammar that can be translated to target representations with high precision. The question of annotation costs and ways to minimize the dependence on such annotated corpora has been a recurring theme in the field for the last two decades (Ngai and Yarowsky, 2000; Garrette and Baldridge, 2013). This question has also been extensively addressed in the context of dependency treebanks. We revisit this question in context of Universal Dependencies and recent work on the interplay between interlingua grammars and multilingual dependency trees in this scheme (Kolachina and Ranta, 2016; Ranta and Kolachina, 2017; Ranta et al., 2017). The use of interlingua grammars to bootstrap dependency treebanks guarantees two types of consistencies: multilingual treebank consistency and intra-treebank consistency. We study the efficacy of these dependency treebanks using learning curves of a transition-based parser in a delexicalized parsing setup. The delexicalized parsing setup allows for generation of parallel UD treebanks in multiple languages with minimal prerequisites on language-specific knowledge. Another rationale behind the the current work in the context of cross-lingual parsing is while synthetic tree"
W19-6102,W17-0247,1,0.861849,"can be translated to target representations with high precision. The question of annotation costs and ways to minimize the dependence on such annotated corpora has been a recurring theme in the field for the last two decades (Ngai and Yarowsky, 2000; Garrette and Baldridge, 2013). This question has also been extensively addressed in the context of dependency treebanks. We revisit this question in context of Universal Dependencies and recent work on the interplay between interlingua grammars and multilingual dependency trees in this scheme (Kolachina and Ranta, 2016; Ranta and Kolachina, 2017; Ranta et al., 2017). The use of interlingua grammars to bootstrap dependency treebanks guarantees two types of consistencies: multilingual treebank consistency and intra-treebank consistency. We study the efficacy of these dependency treebanks using learning curves of a transition-based parser in a delexicalized parsing setup. The delexicalized parsing setup allows for generation of parallel UD treebanks in multiple languages with minimal prerequisites on language-specific knowledge. Another rationale behind the the current work in the context of cross-lingual parsing is while synthetic treebanks offer a “cheap”"
W19-6102,D18-1545,0,0.116206,"istribution on the abstract syntax that generalizes to other languages. Hence, the resulting treebank is licensed by a grammar, and high-precision cross-linguistic information is specified, but the distribution over the resulting treebank is different from the distribution obtained using the real treebanks. An alternative to the method of bootstrapping UD treebanks is to use ud2gf (Ranta and Kolachina, 2017) as a way to translate existing UD treebanks to GF treebanks, that are licensed by a grammar. The current work also relates to more recent work in data-augmentation for dependency parsing (Sahin and Steedman, 2018) and more generally in NLP (Sennrich et al., 2016). The augmentation methods are designed to address data scarcity by exploiting monolingual corpora or generating synthetic samples in multilingual applications. However, the underlying abstractions used to generate the synthetic data are induced from auxiliary corpora. Jonson (2006) show that synthetic corpora generated using a GF grammar can be used to build language models for speech recognition. Experiments in their work show that synthetic indomain examples generated using the grammar when combined with large out-of-domain data result in si"
W19-6102,P16-1009,0,0.0438158,"o other languages. Hence, the resulting treebank is licensed by a grammar, and high-precision cross-linguistic information is specified, but the distribution over the resulting treebank is different from the distribution obtained using the real treebanks. An alternative to the method of bootstrapping UD treebanks is to use ud2gf (Ranta and Kolachina, 2017) as a way to translate existing UD treebanks to GF treebanks, that are licensed by a grammar. The current work also relates to more recent work in data-augmentation for dependency parsing (Sahin and Steedman, 2018) and more generally in NLP (Sennrich et al., 2016). The augmentation methods are designed to address data scarcity by exploiting monolingual corpora or generating synthetic samples in multilingual applications. However, the underlying abstractions used to generate the synthetic data are induced from auxiliary corpora. Jonson (2006) show that synthetic corpora generated using a GF grammar can be used to build language models for speech recognition. Experiments in their work show that synthetic indomain examples generated using the grammar when combined with large out-of-domain data result in significant reduction of word error rate of the spee"
W19-6102,K17-3009,0,0.103893,"Missing"
W19-6102,C14-1175,0,0.13889,"ns arising from linguistic knowledge. Abstractions induced from data have proven themselves to be useful for robust NLU tasks, while grammars are better at precision tasks involving NLG (Natural Language Generation). Given the resources required for treebanking, synthetic treebanks have been proposed and used as substitute in cross-lingual parsing for languages where treebanks do not exist. Such treebanks are created using parallel corpora where parse trees in one language are bootstrapped into a target language using alignment information through annotation projection (McDonald et al., 2011; Tiedemann, 2014) or using machine translation systems to bootstrap existing treebanks in one or more source language(s) to the target language (Tiedemann and Agic, 2016; Tyers et al., 2018). More recently, synthetic treebanks are generated for both real and artificial languages using multilingual treebanks by learning feasible parameter combinations (Wang and Eisner, 2016) – Wang and Eisner (2018) show that such treebanks can be useful to select the most similar language to train a parsing model for an unknown language. At the same time, grammar-based treebanking approaches have been shown to work in monoling"
W19-6102,W18-6017,0,0.105595,"s involving NLG (Natural Language Generation). Given the resources required for treebanking, synthetic treebanks have been proposed and used as substitute in cross-lingual parsing for languages where treebanks do not exist. Such treebanks are created using parallel corpora where parse trees in one language are bootstrapped into a target language using alignment information through annotation projection (McDonald et al., 2011; Tiedemann, 2014) or using machine translation systems to bootstrap existing treebanks in one or more source language(s) to the target language (Tiedemann and Agic, 2016; Tyers et al., 2018). More recently, synthetic treebanks are generated for both real and artificial languages using multilingual treebanks by learning feasible parameter combinations (Wang and Eisner, 2016) – Wang and Eisner (2018) show that such treebanks can be useful to select the most similar language to train a parsing model for an unknown language. At the same time, grammar-based treebanking approaches have been shown to work in monolingual setups—to derive rich linguistic representations defined by explicit grammars (Oepen et al., 2004). These approaches are carried out by parsing raw corpora with a target"
W19-6102,W14-5508,1,0.658241,"ario strongly corresponds to our proposed idea of simultaneous grammar engineering and treebanking. • When 1K ≤ N ≤ 5K sentences5 are available for a language. There are around 18 treebanks in the current UD distribution that match this criterion. While one can argue that these languages are not really underresourced, this setup matches the typical case of training domain-specific parsers either for a particular domain like bio-medical or legal texts. 2 The UD treebanks are taken from the v2.3 distribution. There is ongoing work on developing interlingual lexica from linked data like WordNet (Virk et al., 2014; Angelov and Lobanov, 2016). 4 This approximately corresponds to 20K tokens. 5 This approximately corresponds to 20K – 100K tokens. 3 For each of these use-cases, we train parsing models using data from both human annotated UD treebanks and synthetic treebanks for different sizes of training data. The resulting parsing models are evaluated using labelled attachment scores, obtained by parsing the test set of the UD treebank for the language in question. We experiment with an off-the-shelf transition-based dependency parser that gives good results in the dependency parsing task (Straka and Str"
W19-6102,Q16-1035,0,0.0188571,"languages where treebanks do not exist. Such treebanks are created using parallel corpora where parse trees in one language are bootstrapped into a target language using alignment information through annotation projection (McDonald et al., 2011; Tiedemann, 2014) or using machine translation systems to bootstrap existing treebanks in one or more source language(s) to the target language (Tiedemann and Agic, 2016; Tyers et al., 2018). More recently, synthetic treebanks are generated for both real and artificial languages using multilingual treebanks by learning feasible parameter combinations (Wang and Eisner, 2016) – Wang and Eisner (2018) show that such treebanks can be useful to select the most similar language to train a parsing model for an unknown language. At the same time, grammar-based treebanking approaches have been shown to work in monolingual setups—to derive rich linguistic representations defined by explicit grammars (Oepen et al., 2004). These approaches are carried out by parsing raw corpora with a target grammar and using an additional human disambiguation phase. Alternatively, existing treebanks are matched against the target grammar further reducing the human effort in disambiguation:"
W19-6102,D18-1163,0,0.0856098,"s do not exist. Such treebanks are created using parallel corpora where parse trees in one language are bootstrapped into a target language using alignment information through annotation projection (McDonald et al., 2011; Tiedemann, 2014) or using machine translation systems to bootstrap existing treebanks in one or more source language(s) to the target language (Tiedemann and Agic, 2016; Tyers et al., 2018). More recently, synthetic treebanks are generated for both real and artificial languages using multilingual treebanks by learning feasible parameter combinations (Wang and Eisner, 2016) – Wang and Eisner (2018) show that such treebanks can be useful to select the most similar language to train a parsing model for an unknown language. At the same time, grammar-based treebanking approaches have been shown to work in monolingual setups—to derive rich linguistic representations defined by explicit grammars (Oepen et al., 2004). These approaches are carried out by parsing raw corpora with a target grammar and using an additional human disambiguation phase. Alternatively, existing treebanks are matched against the target grammar further reducing the human effort in disambiguation: these approaches face a"
W19-6102,W14-1613,0,0.0127819,"he synthetic treebanks are still useful to improve parsing accuracies. 6 Related Work The current trend in dependency parsing is directed towards using synthetic treebanks in an attempt to cover unknown languages for which (a) Learning curves for English with N between 1K and 5K (b) Learning curves for English with N between 5K and 10K samples samples Figure 6: Learning curves shown using bar plots for parsing models of English resources are minimal or do not exist altogether. Such treebanks rely on various auxiliary resources: parallel corpora (Tiedemann, 2014), multilingual word-embeddings (Xiao and Guo, 2014), MT system for the target language (Tiedemann and Agic, 2016; Tyers et al., 2018) or more minimally, tagged corpora in the target language (Wang and Eisner, 2018). Tiedemann and Agic (2016) propose a method to generate synthetic treebanks for new languages using machine translation systems to transfer cross-linguistic information from resource-rich language to under-resourced languages. This work builds on top of many previous approaches to cross-lingual parsing using parallel corpora and multilingual word-embeddings. The synthetic treebanks generated in the current work are are different in"
W19-6102,K18-2001,0,0.0367872,"Missing"
W98-1308,J94-3001,0,0.0691757,", for standard definitions, methods, and theorems concerning regular expressions.) The formalism of XFST not only introduces a couple of dozens of new forms of expressions for regular languages, but also expressions for regular relations, which are sets not of strings but of pairs of strings. These expressions are compiled into finite state transducers, which not only accept and reject input strings, but transform them into other strings. (There is no complete specification of the XFST formalism available, but the tutorial report [5] gives enough background to the present report. Kaplan & Kay [3] and Karttunen [4] are the sources of many fundamental ideas in the use of regular relations.) The simplest example of a relation expression is the symbol pair a:b, where a and b are symbols. It denotes, the singleton relation [(a, b)]. Regular relations are closed under concatenation, union, and Kleene closure. So one can write, for instance, [&apos;a:b a:b]* J [b:a b:a]* which denotes the relation containing all pairs of strings consisting of equal even numbers of a&apos;s and b&apos;s. Seen as an operation, this relation transforms any such sequence of a&apos;s into a sequence of b&apos;s and vice versa. 81 II I II"
W98-1308,P96-1015,0,0.0120205,"initions, methods, and theorems concerning regular expressions.) The formalism of XFST not only introduces a couple of dozens of new forms of expressions for regular languages, but also expressions for regular relations, which are sets not of strings but of pairs of strings. These expressions are compiled into finite state transducers, which not only accept and reject input strings, but transform them into other strings. (There is no complete specification of the XFST formalism available, but the tutorial report [5] gives enough background to the present report. Kaplan & Kay [3] and Karttunen [4] are the sources of many fundamental ideas in the use of regular relations.) The simplest example of a relation expression is the symbol pair a:b, where a and b are symbols. It denotes, the singleton relation [(a, b)]. Regular relations are closed under concatenation, union, and Kleene closure. So one can write, for instance, [&apos;a:b a:b]* J [b:a b:a]* which denotes the relation containing all pairs of strings consisting of equal even numbers of a&apos;s and b&apos;s. Seen as an operation, this relation transforms any such sequence of a&apos;s into a sequence of b&apos;s and vice versa. 81 II I II II The notation o"
Y10-1020,P06-1143,0,0.0505806,"Missing"
Y10-1020,Y06-1050,0,\N,Missing
