2020.aacl-main.88,2020.repl4nlp-1.18,0,0.0231828,"mpression and accuracy of a model. 2 also optimizing for task loss. We show that this process works substantially better in practice. Our loss function is thus composed of three different objectives: Related Work Cross Entropy Loss The cross entropy loss over an example x with label y is defined likewise: LCE = − log ps (y|x), where ps is the probability for label y given by the decomposed student model. In the past year, there have been many attempts to compress transformer models involving pruning (McCarley, 2019; Guo et al., 2019; Wang et al., 2019; Michel et al., 2019; Voita et al., 2019; Gordon et al., 2020), quantization (Zafrir et al., 2019; Shen et al., 2019) and distillation (Sanh et al., 2019; Zhao et al., 2019; Tang et al., 2019; Mukherjee and Awadallah, 2019; Sun et al., 2019; Liu et al., 2019a; Jiao et al., 2019; Izsak et al., 2019). Specifically, works on compressing pretrained transformer language models focused on pruning layers. Sun et al. (2019) suggested to prune layers while distilling information from the unpruned model layers. Xu et al. (2020) proposed to gradually remove layers during training. We also note that very recently a work similar to ours was uploaded to arxiv (Mao et"
2020.aacl-main.88,P18-1031,0,0.0234937,"ls have been demonstrated to achieve state-of-the-art results, but require large parameter storage and computation. It’s estimated that training a Transformer model with a neural architecture search has a CO2 emissions equivalent to nearly five times the lifetime emissions of the average U.S. car, including its manufacturing (Strubell et al., 2019). Alongside the increase in deep learning models complexity, in the NLP domain, there has been a shift in the NLP modeling paradigm from training a randomly initialized model to fine-tuning a large and computational heavy pre-trained language model (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018; Radford, 2018; Radford et al., 2019; Dai et al., 2019; Yang et al., 2019; Lample and Conneau, 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Lewis et al., 2019). 884 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889 c December 4 - 7, 2020. 2020 Association for Computational Linguistics domly initialized model. Finally, we demonstrate the trade-off between compression and accuracy of a"
2020.aacl-main.88,2021.ccl-1.108,0,0.108072,"Missing"
2020.aacl-main.88,P19-1285,0,0.0168615,"computation. It’s estimated that training a Transformer model with a neural architecture search has a CO2 emissions equivalent to nearly five times the lifetime emissions of the average U.S. car, including its manufacturing (Strubell et al., 2019). Alongside the increase in deep learning models complexity, in the NLP domain, there has been a shift in the NLP modeling paradigm from training a randomly initialized model to fine-tuning a large and computational heavy pre-trained language model (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018; Radford, 2018; Radford et al., 2019; Dai et al., 2019; Yang et al., 2019; Lample and Conneau, 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Lewis et al., 2019). 884 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889 c December 4 - 7, 2020. 2020 Association for Computational Linguistics domly initialized model. Finally, we demonstrate the trade-off between compression and accuracy of a model. 2 also optimizing for task loss. We show that this process works substantially better in p"
2020.aacl-main.88,2020.coling-main.287,0,0.0193423,", 2020), quantization (Zafrir et al., 2019; Shen et al., 2019) and distillation (Sanh et al., 2019; Zhao et al., 2019; Tang et al., 2019; Mukherjee and Awadallah, 2019; Sun et al., 2019; Liu et al., 2019a; Jiao et al., 2019; Izsak et al., 2019). Specifically, works on compressing pretrained transformer language models focused on pruning layers. Sun et al. (2019) suggested to prune layers while distilling information from the unpruned model layers. Xu et al. (2020) proposed to gradually remove layers during training. We also note that very recently a work similar to ours was uploaded to arxiv (Mao et al., 2020). There are a few differences from their work to ours. Firstly, we distill different parts of the model (see Section 3 for details). Secondly, we focus on training the decomposed model and do not prune the model parameters. Thirdly, our base model, which is used for decomposition and as a teacher, is a fine-tuned model; This has the benefit of task-specific information as we show in our experiments in Section 4.2. 3 Knowledge Distillation Loss The goal of knowledge distillation is to imitate the output layer of a teacher model by a student model. The Knowledge Loss is defined likewise: z Disti"
2020.aacl-main.88,P19-1580,0,0.0298828,"trade-off between compression and accuracy of a model. 2 also optimizing for task loss. We show that this process works substantially better in practice. Our loss function is thus composed of three different objectives: Related Work Cross Entropy Loss The cross entropy loss over an example x with label y is defined likewise: LCE = − log ps (y|x), where ps is the probability for label y given by the decomposed student model. In the past year, there have been many attempts to compress transformer models involving pruning (McCarley, 2019; Guo et al., 2019; Wang et al., 2019; Michel et al., 2019; Voita et al., 2019; Gordon et al., 2020), quantization (Zafrir et al., 2019; Shen et al., 2019) and distillation (Sanh et al., 2019; Zhao et al., 2019; Tang et al., 2019; Mukherjee and Awadallah, 2019; Sun et al., 2019; Liu et al., 2019a; Jiao et al., 2019; Izsak et al., 2019). Specifically, works on compressing pretrained transformer language models focused on pruning layers. Sun et al. (2019) suggested to prune layers while distilling information from the unpruned model layers. Xu et al. (2020) proposed to gradually remove layers during training. We also note that very recently a work similar to ours was uplo"
2020.aacl-main.88,A94-1016,0,0.383232,"Missing"
2020.aacl-main.88,N18-1202,0,0.0593748,"d to achieve state-of-the-art results, but require large parameter storage and computation. It’s estimated that training a Transformer model with a neural architecture search has a CO2 emissions equivalent to nearly five times the lifetime emissions of the average U.S. car, including its manufacturing (Strubell et al., 2019). Alongside the increase in deep learning models complexity, in the NLP domain, there has been a shift in the NLP modeling paradigm from training a randomly initialized model to fine-tuning a large and computational heavy pre-trained language model (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018; Radford, 2018; Radford et al., 2019; Dai et al., 2019; Yang et al., 2019; Lample and Conneau, 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Lewis et al., 2019). 884 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889 c December 4 - 7, 2020. 2020 Association for Computational Linguistics domly initialized model. Finally, we demonstrate the trade-off between compression and accuracy of a model. 2 also optimiz"
2020.aacl-main.88,W18-5446,0,0.134552,"ch of the information while reducing the number of parameters. Alongside the advantage of preserving the information within each layer, there is also a memory flexibility advantage compared to removing entire layers; As a result of decomposing each matrix to two smaller matrices, we can store each of the two matrices in two different memory blocks. This has the benefit of distributing the model matrices in many small memory blocks, which is useful when working in shared CPU-based environments. We evaluated our approach on the General Language Understanding Evaluation (GLUE) benchmark dataset (Wang et al., 2018) and show that our approach is superior or competitive in the different GLUE tasks to previous approaches which remove entire layers. Furthermore, we study the effects of different base models to decompose and show the superiority of decomposing a fine-tuned model compared to a pre-trained model or a ranLarge pre-trained language models reach stateof-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a twostage model-compression method"
2020.aacl-main.88,2020.emnlp-main.496,0,0.0674791,"Missing"
2020.aacl-main.88,2020.emnlp-main.633,0,0.208548,"ent, Bar-Ilan University, Ramat-Gan Israel † Intel AI Lab, Petah-Tikva Israel ‡ Allen Institute for Artificial Intelligence matan.ben.noach@intel.com, yoav.goldberg@gmail.com Abstract While re-using pre-trained models offsets the training costs, inference time costs of the finetuned models remain significant, and are showstoppers in many applications. The main challenge with pre-trained models is how can we reduce their size while saving the information contained within them. Recent work, approached this by keeping some of the layers while removing others (Sanh et al., 2019; Sun et al., 2019; Xu et al., 2020). A main drawback of such approach is in its coarse-grained nature: removing entire layers might discard important information contained within the model, and working at the granularity of layers makes the trade-off between compression and accuracy of a model hard to control. Motivated by this, in this work we suggest a more finegrained approach which decomposes each matrix to two smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of preserving much of the information while reducing the number o"
2020.aacl-main.88,P19-1355,0,0.0213735,"imented on BERTbase model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x, and increase inference speed by a factor of 1.45x, while maintaining a minimal loss in metric performance. 1 Introduction Deep learning models have been demonstrated to achieve state-of-the-art results, but require large parameter storage and computation. It’s estimated that training a Transformer model with a neural architecture search has a CO2 emissions equivalent to nearly five times the lifetime emissions of the average U.S. car, including its manufacturing (Strubell et al., 2019). Alongside the increase in deep learning models complexity, in the NLP domain, there has been a shift in the NLP modeling paradigm from training a randomly initialized model to fine-tuning a large and computational heavy pre-trained language model (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2018; Radford, 2018; Radford et al., 2019; Dai et al., 2019; Yang et al., 2019; Lample and Conneau, 2019; Liu et al., 2019b; Raffel et al., 2019; Lan et al., 2019; Lewis et al., 2019). 884 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Li"
2020.aacl-main.88,D19-1441,0,0.163623,"er Science Department, Bar-Ilan University, Ramat-Gan Israel † Intel AI Lab, Petah-Tikva Israel ‡ Allen Institute for Artificial Intelligence matan.ben.noach@intel.com, yoav.goldberg@gmail.com Abstract While re-using pre-trained models offsets the training costs, inference time costs of the finetuned models remain significant, and are showstoppers in many applications. The main challenge with pre-trained models is how can we reduce their size while saving the information contained within them. Recent work, approached this by keeping some of the layers while removing others (Sanh et al., 2019; Sun et al., 2019; Xu et al., 2020). A main drawback of such approach is in its coarse-grained nature: removing entire layers might discard important information contained within the model, and working at the granularity of layers makes the trade-off between compression and accuracy of a model hard to control. Motivated by this, in this work we suggest a more finegrained approach which decomposes each matrix to two smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of preserving much of the information while red"
2020.acl-demos.23,P06-1084,0,0.0261772,"Missing"
2020.acl-demos.23,D15-1274,0,0.023876,"fficing with the part of speech for each word, then knowing the correct diacritization reduces the average morphological ambiguity from 3.2 options to 1.97, while knowing the correct POS tag reduces the average diacritization ambiguity from 6.2 options to 2.75. Thus, the need for an automated diacritization utility is particularly crucial in order to properly disambiguate a Hebrew text. Approach Recent trends in NLP suggest moving towards machine-learned models that automatically learn to extract the regularities in the data. Such approaches have also been applied to diacritization of Arabic (Belinkov and Glass, 2015; Rashwan et al., 2015; Abandah et al., 2015; Mubarak et al., 2019). However, while these generally provide very strong results, they also often make mistakes that contradict our prior knowledge of the linguistic system. While the machine-learned models generalize very well and can learn to perform tasks in which humans cannot articulate the underlying regularities, there are also many cases that language-experts can articulate precisely, and these tend to correlate with the cases that the learned models fail on. We therefore take a hybrid approach. Similar to traditional diacritization system"
2020.acl-demos.23,W02-0504,0,0.0431326,"rned models fail on. We therefore take a hybrid approach. Similar to traditional diacritization systems (Choueka and Neeman, 1995), we use our explicit knowledge about the language and the diacritization system whenever we can. However, we also supplement our knowledge with learned model predictions for the challenging cases for which we cannot articulate the rules and regularities: selecting the appropriate diacritization in context, and providing diacritization for out-of-vocabulary words. This methodology departs from recent diacritization works that rely on HMM and neural-network methods (Gal, 2002; Belinkov and Glass, 2015), while ignoring forms of explicit linguistic knowledge. We use such a combination of machine-learned and human-specified knowledge in all the components of the system, either by supplementing the predictor with manually constructed options, or by filtering its output space. 198 Of course, a prerequisite for an effective machine-learned system is high-quality training data. Our system is trained on a collection of 1,5M diacritized tokens which we annotated in-house. 4 High-quality Data Sources We make use of the following language resources and corpora, which we coll"
2020.acl-demos.23,C16-1033,0,0.0232738,"Missing"
2020.acl-demos.23,N19-1248,0,0.0186025,"t diacritization reduces the average morphological ambiguity from 3.2 options to 1.97, while knowing the correct POS tag reduces the average diacritization ambiguity from 6.2 options to 2.75. Thus, the need for an automated diacritization utility is particularly crucial in order to properly disambiguate a Hebrew text. Approach Recent trends in NLP suggest moving towards machine-learned models that automatically learn to extract the regularities in the data. Such approaches have also been applied to diacritization of Arabic (Belinkov and Glass, 2015; Rashwan et al., 2015; Abandah et al., 2015; Mubarak et al., 2019). However, while these generally provide very strong results, they also often make mistakes that contradict our prior knowledge of the linguistic system. While the machine-learned models generalize very well and can learn to perform tasks in which humans cannot articulate the underlying regularities, there are also many cases that language-experts can articulate precisely, and these tend to correlate with the cases that the learned models fail on. We therefore take a hybrid approach. Similar to traditional diacritization systems (Choueka and Neeman, 1995), we use our explicit knowledge about t"
2020.acl-demos.3,W19-5034,0,0.0254047,"he one from “obtain”: use-cases. Over wikipedia: - p:[e]Sam $[l=win|receive]won an $Oscar. - hip:[e]Sam $[l=win|receive]won an $Oscar $for hithing:something - $fish $such $as hifish:salmon - hihero:[t]Spiderman $is a $superhero - I like kind:coconut $oil - kind:coconut $oil is $used for purpose:eating hisubj:[e]John :[]obtained his d:[w]degree $from hiinst:[w]Harvard Looking at the result list in the o capture, the user chooses the lemmas “receive, complete, earn, obtain, get”, adds them to the o constraint, and removes the degree constraint. Over a pubmed corpus, annotated with the SciSpacy (Neumann et al., 2019) pipeline: - hix:[e]aspirin $inhibits hiy:thing - a $combination of hid1:[e]aspirin and hisubj:[e]John hid2:[e]alcohol $:[l]causes hit:thing o:[l=receive|complete|earn|obtain|get]obtained - hipatients:[t]rats were $injected $with hiwhat:drugs his d:degree $from hiinst:[w]Harvard The returned result-set is now much longer, and we select additional terms for the degree slot and remove the institution word constraint, resulting in the final query: 8 The indexing is handled by Lucene.10 We currently use Odinson (Valenzuela-Esc´arcega et al., 2020),11 an open-source Lucene-based query engine develo"
2020.acl-demos.3,P13-4027,0,0.446937,"attern: | trigger = [word=founded] founder:Person = >nsubj entity:Organization = >nmod PERSON <nsubj founder >nmod ORG While the different systems vary in the verboseness and complexity of their own syntax (indeed, the Turku system’s syntax is rather minimal), they all require the user to explicitly specify the dependency relations between the tokens, making it challenging and error-prone to write, read or edit. The challenge grows substantially as the complexity of the pattern increases beyond the very simple example we show here. Closest in spirit to our proposal, the P ROP M INER system of Akbik et al. (2013) which lets the user enter a natural language sentence, mark spans as subject, predicate and object, and have a rule be The Spacy NLP toolkit3 also includes pattern matcher over dependency trees,using JSON based syntax: [{""PATTERN"": {""ORTH"": ""founder""}, ""SPEC"": {""NODE_NAME"": ""t""}}, {""PATTERN"": {""ENT_TYPE"": ""PERSON""}}, ""SPEC"": {""NODE_NAME"": ""founder"", ""NBOR_RELOP"": "">nsubj"", 2 We focus here on systems that are based on dependency syntax, but note that many systems and query languages exist also for constituency-trees, e.g., TGREP/TGREP2, TigerSearch (Lezius et al., 2002), the linguists search e"
2020.acl-demos.3,S15-2153,0,0.112579,"Missing"
2020.acl-demos.3,C14-1197,0,0.279514,"s, or download all the results as a tab-separated file. The introduction of neural-network based models into NLP brought with it a substantial increase in syntactic parsing accuracy. We can now produce accurate syntactically annotated corpora at scale. However, the produced representations themselves remain opaque to most users, and require substantial linguistic expertise to use. Patterns over syntactic dependency graphs1 can be very effective for interacting with linguistically-annotated corpora, either for linguistic retrieval or for information and relation extraction (Fader et al., 2011; Akbik et al., 2014; Valenzuela-Esc´arcega et al., 2015, 1 In this paper, we very loosely use the term “syntactic” to refer to a linguistically motivated graph-based annotation over a piece of text, where the graph is directed and there is a path between any two nodes. While this usually implies syntactic dependency trees or graphs (and indeed, our system currently indexes Enhanced English Universal Dependency graphs (Nivre et al., 2016; Schuster and Manning, 2016)) the system can work also with more semantic annotation schemes e.g, (Oepen et al., 2015), given the availability of an accurate enough parser for th"
2020.acl-demos.3,P05-3009,0,0.105052,"lets the user enter a natural language sentence, mark spans as subject, predicate and object, and have a rule be The Spacy NLP toolkit3 also includes pattern matcher over dependency trees,using JSON based syntax: [{""PATTERN"": {""ORTH"": ""founder""}, ""SPEC"": {""NODE_NAME"": ""t""}}, {""PATTERN"": {""ENT_TYPE"": ""PERSON""}}, ""SPEC"": {""NODE_NAME"": ""founder"", ""NBOR_RELOP"": "">nsubj"", 2 We focus here on systems that are based on dependency syntax, but note that many systems and query languages exist also for constituency-trees, e.g., TGREP/TGREP2, TigerSearch (Lezius et al., 2002), the linguists search engine (Resnik and Elkiss, 2005), Fangorn (Ghodke and Bird, 2012). 3 https://spacy.io/ 4 https://nlp.stanford.edu/software/ tregex.shtml 5 http://bionlp-www.utu.fi/dep_search/ 18 generated automatically. However, the system is restricted to ternary subject-predicate-object patterns. Furthermore, the generated pattern is written in a path-expression SQL variant (SerQL, (Broekstra and Kampman, 2003)), which the user then needs to manually edit. For example, our query above will be translated to: time, we also seamlessly support expressing arbitrary sub-graphs, a task which is either challenging or impossible with many of the o"
2020.acl-demos.3,augustinus-etal-2012-example,0,0.045813,"Missing"
2020.acl-demos.3,L16-1376,0,0.0660251,"Missing"
2020.acl-demos.3,D11-1142,0,0.0232868,"y, browse the results, or download all the results as a tab-separated file. The introduction of neural-network based models into NLP brought with it a substantial increase in syntactic parsing accuracy. We can now produce accurate syntactically annotated corpora at scale. However, the produced representations themselves remain opaque to most users, and require substantial linguistic expertise to use. Patterns over syntactic dependency graphs1 can be very effective for interacting with linguistically-annotated corpora, either for linguistic retrieval or for information and relation extraction (Fader et al., 2011; Akbik et al., 2014; Valenzuela-Esc´arcega et al., 2015, 1 In this paper, we very loosely use the term “syntactic” to refer to a linguistically motivated graph-based annotation over a piece of text, where the graph is directed and there is a path between any two nodes. While this usually implies syntactic dependency trees or graphs (and indeed, our system currently indexes Enhanced English Universal Dependency graphs (Nivre et al., 2016; Schuster and Manning, 2016)) the system can work also with more semantic annotation schemes e.g, (Oepen et al., 2015), given the availability of an accurate"
2020.acl-demos.3,2020.lrec-1.267,0,0.134272,"Missing"
2020.acl-demos.3,C12-3022,0,0.0179852,"age sentence, mark spans as subject, predicate and object, and have a rule be The Spacy NLP toolkit3 also includes pattern matcher over dependency trees,using JSON based syntax: [{""PATTERN"": {""ORTH"": ""founder""}, ""SPEC"": {""NODE_NAME"": ""t""}}, {""PATTERN"": {""ENT_TYPE"": ""PERSON""}}, ""SPEC"": {""NODE_NAME"": ""founder"", ""NBOR_RELOP"": "">nsubj"", 2 We focus here on systems that are based on dependency syntax, but note that many systems and query languages exist also for constituency-trees, e.g., TGREP/TGREP2, TigerSearch (Lezius et al., 2002), the linguists search engine (Resnik and Elkiss, 2005), Fangorn (Ghodke and Bird, 2012). 3 https://spacy.io/ 4 https://nlp.stanford.edu/software/ tregex.shtml 5 http://bionlp-www.utu.fi/dep_search/ 18 generated automatically. However, the system is restricted to ternary subject-predicate-object patterns. Furthermore, the generated pattern is written in a path-expression SQL variant (SerQL, (Broekstra and Kampman, 2003)), which the user then needs to manually edit. For example, our query above will be translated to: time, we also seamlessly support expressing arbitrary sub-graphs, a task which is either challenging or impossible with many of the other systems. The language is bas"
2020.acl-demos.3,P15-4022,0,0.261162,"Missing"
2020.acl-demos.3,W17-0233,0,0.0220513,"to be familiar not only with the syntax of the query language itself, but to also be intimately familiar with the specific syntactic scheme used in the underlying linguistic annotations. For example, in Odin (Valenzuela-Esc´arcega et al., 2015), a dedicated language for pattern-based information extraction, the same rule as above is expressed as: Stanford’s Core-NLP package (Manning et al., 2014) includes a dependency matcher called S EM G REX,4 which uses a more concise syntax: {ner:PERSON}=founder <nsubj ({word:founder}=t >nmod {ner:ORG}=entity) The dep search system5 from Turku university (Luotolahti et al., 2017) is designed to provide a rich and expressive syntactic search over large parsebanks. They use a lightweight syntax and support working against pre-indexed data, though they do not support named captures of specific nodes. - label: Person type: token pattern: | [entity=""PERSON""]+ - label: Organization type: token pattern: | [entity=""ORGANIZATION""]+ - label: founded type: dependency pattern: | trigger = [word=founded] founder:Person = >nsubj entity:Organization = >nmod PERSON <nsubj founder >nmod ORG While the different systems vary in the verboseness and complexity of their own syntax (indeed,"
2020.acl-demos.7,P13-1023,0,0.0511692,"Missing"
2020.acl-demos.7,W13-2322,0,0.0778147,"Missing"
2020.acl-demos.7,Q16-1010,0,0.0328475,"Missing"
2020.acl-demos.7,W17-6507,0,0.633359,"et al., 2013; Palmer et al., 2010; Abend and Rappoport, 2013; Oepen et al., 2014) are harder to predict with sufficient accuracy, calling for a middle ground. Indeed, De Marneffe and Manning (2008) introduced collapsed and propagated dependencies, in an attempt to make some semantic-like relations more apparent. The Universal Dependencies (UD) project1 similarly embraces the concept of Enhanced Dependencies (Nivre et al., 2018)), adding explicit relations that are otherwise left implicit. Schuster and Manning (2016) provide further enhancements targeted specifically at English (Enhanced UD).2 Candito et al. (2017) suggest further enhancements to address diathesis alternations.3 In this work we continue this line of thought, and take it a step further. We present pyBART, an Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks. However, their utility can be improved. These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit. Therefore, these representations lack many explicit connections between content words, that would be useful for do"
2020.acl-demos.7,D17-1009,0,0.0750282,"he Universal Enhanced UD and Schuster and Manning (2016)’s Enhanced++ English UD. We refer to their union on English as Enhanced UD. 3 Efforts such as PropS (Stanovsky et al., 2016) and PredPatt (White et al., 2016), share our motivation of extracting predicate-argument structures from treebank-trainable trees, though outside of the UD framework. Efforts such as KNext (Durme and Schubert, 2008) automatically extract logic-based forms by converting treebank-trainable trees, for consumption by further processing. HLF (Rudinger and Van Durme, 2014), DepLambda (Reddy et al., 2016) and UDepLambda (Reddy et al., 2017) attempt to provide a formal semantic representation by converting dependency structures to logical forms. While they share a high-level goal with ours — exposing functional relations in a sentence in a unified way — their end result, logical forms, is substantially different from pyBART structures. While providing substantial benefits for semantic parsing applications, logical forms are less readable for non-experts than labeled relations between content words. As these efforts rely on dependency trees as a backbone, they could potentially benefit from pyBART’s focus on syntactic enhancements"
2020.acl-demos.7,W14-2908,0,0.0676178,"Missing"
2020.acl-demos.7,L16-1376,0,0.156296,"Missing"
2020.acl-demos.7,W08-2219,0,0.0229511,"n a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns. 1 1 universaldepdenencies.org In this paper we do not distinguish between the Universal Enhanced UD and Schuster and Manning (2016)’s Enhanced++ English UD. We refer to their union on English as Enhanced UD. 3 Efforts such as PropS (Stanovsky et al., 2016) and PredPatt (White et al., 2016), share our motivation of extracting predicate-argument structures from treebank-trainable trees, though outside of the UD framework. Efforts such as KNext (Durme and Schubert, 2008) automatically extract logic-based forms by converting treebank-trainable trees, for consumption by further processing. HLF (Rudinger and Van Durme, 2014), DepLambda (Reddy et al., 2016) and UDepLambda (Reddy et al., 2017) attempt to provide a formal semantic representation by converting dependency structures to logical forms. While they share a high-level goal with ours — exposing functional relations in a sentence in a unified way — their end result, logical forms, is substantially different from pyBART structures. While providing substantial benefits for semantic parsing applications, logic"
2020.acl-demos.7,L18-1169,0,0.110953,"Missing"
2020.acl-demos.7,N18-2035,0,0.0264068,"adding a node to represent the STATE and have tense, aspect, modality and evidentiality directly modifying it.11 Copular Sentences and Stative Predicates: We added to all copula constructions new node named STATE, which represents the stative event introduced by the copular clause. This node becomes the root, and we rewire the entire clause around this STATE. By doing so we unify it with the structures of clauses with finite predicative. Once we added the STATE node, we form a new relation, termed ev, to mark event/state modifications. The resulting structure is as follows: advmod Compounds: Shwartz and Waterson (2018) show that in many cases, compounds can be seen as having a multiple-head. Therefore, we share the existing relations across the compound parts. (9) I see dead people ev company (14) amod xcomp Tomorrow is STATE another day cop Adjectival modifiers: Adjectival modification can be viewed as capturing the same information as a predicative copular sentence conveying the same meaning (so, “a green apple” implies that “an apple is green”). To explicitly capture this productive implication, we add a subject relation from each adjectival modifier to its corresponding modified noun. nsubj Evidential r"
2020.acl-demos.7,D15-1076,0,0.0505789,"departure point is the English EUD representation (Schuster and Manning, 2016) and related Some preserved UD relations are omitted for readability. https://spacy.io 48 2 efforts discussed above, which we seek to extend in a way which is useful to information seeking applications. To identify relevant constructions that are not covered by current representations, we use a data-driven process. We consider concrete relations that are expressed in annotated task-based corpora: a relation extraction dataset (ACE05, (Walker et al., 2006)), which annotates relations and events, and a QA-SRL dataset (He et al., 2015) which connects predicates to sentence segments that are perceived by people as their (possibly implied) arguments. For each of these corpora, we consider the dependency paths between the annotated elements, looking for cases where a direct relation in the corpus corresponds to an indirect dependency path in the syntactic graph. We identify recurring cases that we think can be shortened, and which can be justified linguistically and empirically. We then come up with proposed enhancements and modifications, and verify them empirically against a larger corpus by extracting cases that match the c"
2020.acl-demos.7,D16-1177,0,0.111441,"Missing"
2020.acl-demos.7,D17-1004,0,0.116335,"Missing"
2020.acl-demos.7,W18-6012,0,\N,Missing
2020.acl-main.386,D18-1407,0,0.144337,"Missing"
2020.acl-main.386,L16-1252,0,0.0213687,"of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility. 1 Introduction Fueled by recent advances in deep-learning and language processing, NLP systems are increasingly being used for prediction and decision-making in many fields (Vig and Belinkov, 2019), including sensitive ones such as health, commerce and law (Fort and Couillault, 2016). Unfortunately, these highly flexible and highly effective neural models are also opaque. There is therefore a critical need for explaining learning-based models’ decisions. The emerging research topic of interpretability or explainability1 has grown rapidly in recent years. Unfortunately, not without growing pains. 1 Despite fine-grained distinctions between the terms, within the scope of this work we use the terms “interpretability” and “explainability” interchangeably. One such pain is the challenge of defining—and evaluating—what constitutes a quality interpretation. Current approaches de"
2020.acl-main.386,D18-1537,0,0.0460697,"n is often made between two methods of interpretability: (1) interpreting existing models via post-hoc techniques; and (2) designing inherently interpretable models. Rudin (2018) argues in favor of inherently interpretable models, which by design claim to provide more faithful interpretations than post-hoc interpretation of black-box models. We warn against taking this argumentation at face-value: a method being “inherently interpretable” is merely a claim that needs to be verified before it can be trusted. Indeed, while attention mechanisms have been considered as “inherently interpretable” (Ghaeini et al., 2018; Lee et al., 2017), recent work cast doubt regarding their faithfulness (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). 4 Evaluation via Utility While explanations have many different use-cases, such as model debugging, lawful guarantees or health-critical guarantees, one other possible usecase with prominent evaluation literature is Intelligent User Interfaces (IUI), via Human-Computer Interaction (HCI), of automatic models assisting human decision-makers. The goal of the explanation here is to increase the degree of trust between the user and the system, givin"
2020.acl-main.386,W18-5408,1,0.916282,"Missing"
2020.acl-main.386,N19-1357,0,0.488249,"(2) designing inherently interpretable models. Rudin (2018) argues in favor of inherently interpretable models, which by design claim to provide more faithful interpretations than post-hoc interpretation of black-box models. We warn against taking this argumentation at face-value: a method being “inherently interpretable” is merely a claim that needs to be verified before it can be trusted. Indeed, while attention mechanisms have been considered as “inherently interpretable” (Ghaeini et al., 2018; Lee et al., 2017), recent work cast doubt regarding their faithfulness (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). 4 Evaluation via Utility While explanations have many different use-cases, such as model debugging, lawful guarantees or health-critical guarantees, one other possible usecase with prominent evaluation literature is Intelligent User Interfaces (IUI), via Human-Computer Interaction (HCI), of automatic models assisting human decision-makers. The goal of the explanation here is to increase the degree of trust between the user and the system, giving the user more nuance towards whether the system’s decision is likely correct, or not. In the general case, the final ev"
2020.acl-main.386,D17-2021,0,0.0512653,"n two methods of interpretability: (1) interpreting existing models via post-hoc techniques; and (2) designing inherently interpretable models. Rudin (2018) argues in favor of inherently interpretable models, which by design claim to provide more faithful interpretations than post-hoc interpretation of black-box models. We warn against taking this argumentation at face-value: a method being “inherently interpretable” is merely a claim that needs to be verified before it can be trusted. Indeed, while attention mechanisms have been considered as “inherently interpretable” (Ghaeini et al., 2018; Lee et al., 2017), recent work cast doubt regarding their faithfulness (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). 4 Evaluation via Utility While explanations have many different use-cases, such as model debugging, lawful guarantees or health-critical guarantees, one other possible usecase with prominent evaluation literature is Intelligent User Interfaces (IUI), via Human-Computer Interaction (HCI), of automatic models assisting human decision-makers. The goal of the explanation here is to increase the degree of trust between the user and the system, giving the user more nua"
2020.acl-main.386,N18-1097,0,0.0413332,"a model capable of making decisions (e.g., decision trees or rule lists (Sushil et al., 2018)), fidelity is defined as the degree to which the explanation model can mimic the original model’s decisions (as an accuracy score). For cases where the explanation is not a computable model, Doshi-Velez and Kim (2017) propose a simple way of mapping explanations to decisions via crowdsourcing, by asking humans to simulate the model’s decision without any access to the model, and only access to the input and explanation (termed forward simulation). This idea is further explored and used in practice by Nguyen (2018). Assumption 2 (The Prediction Assumption). On similar inputs, the model makes similar decisions if and only if its reasoning is similar. Corollary 2. An interpretation system is unfaithful if it provides different interpretations for similar inputs and outputs. Since the interpretation serves as a proxy for the model’s “reasoning”, it should satisfy the same constraints. In other words, interpretations of similar decisions should be similar, and interpretations of dissimilar decisions should be dissimilar. This assumption is more useful to disprove the faithfulness of an interpretation rather"
2020.acl-main.386,P19-1487,0,0.0393435,"are plausibility and faithfulness. “Plausibility” refers to how convincing the interpretation is to humans, while “faithfulness” refers to how accurately it reflects the true reasoning process of the model (Herman, 2017; Wiegreffe and Pinter, 2019). Naturally, it is possible to satisfy one of these properties without the other. For example, consider the case of interpretation via posthoc text generation—where an additional “generator” component outputs a textual explanation of the model’s decision, and the generator is learned with supervision of textual explanations (Zaidan and Eisner, 2008; Rajani et al., 2019; Strout et al., 2019). In this case, plausibility is the dominating property, while there is no faithfulness guarantee. Despite the difference between the two criteria, many authors do not clearly make the distinction, and sometimes conflate the two.3 Moreoever, the majority of works do not explicitly name the criteria under consideration, even when they clearly belong to one camp or the other.4 We argue that this conflation is dangerous. For example, consider the case of recidivism prediction, 3 E.g., Lundberg and Lee (2017); P¨orner et al. (2018); Wu and Mooney (2018). 4 E.g., Mohseni and R"
2020.acl-main.386,N16-3020,0,0.794993,"definition of faithfulness. We uncover three assumptions that underlie all these attempts. By making the assumptions explicit and organizing the literature around them, we “connect the dots” between seemingly distinct evaluation methods, and 2 Unfortunately, the terms in the literature are not yet standardized, and vary widely. “Readability” and “plausibility” are also referred to as “human-interpretability” and “persuasiveness”, respectively (e.g., Lage et al. (2019); Herman (2017)). To our knowledge, the term “faithful interpretability” was coined in Harrington et al. (1985), reinforced by Ribeiro et al. (2016), and is, we believe, most commonly used (e.g., Gilpin et al. (2018); Wu and Mooney (2018); Lakkaraju et al. (2019)). Chakraborty et al. (2017) refers to this issue (more or less) as “accountability”. Sometimes referred to as how “trustworthy” (Camburu et al., 2019) or “descriptive” (Carmona et al., 2015; Biecek, 2018) the interpretation is, or as “descriptive accuracy” (Murdoch et al., 2019). Also related to the “transparency” (Baan et al., 2019), the “fidelity” (Guidotti et al., 2018) or the “robustness” (Alvarez-Melis and Jaakkola, 2018) of the interpretation method. And frequently, simply"
2020.acl-main.386,P19-1282,0,0.677789,"post-hoc techniques; and (2) designing inherently interpretable models. Rudin (2018) argues in favor of inherently interpretable models, which by design claim to provide more faithful interpretations than post-hoc interpretation of black-box models. We warn against taking this argumentation at face-value: a method being “inherently interpretable” is merely a claim that needs to be verified before it can be trusted. Indeed, while attention mechanisms have been considered as “inherently interpretable” (Ghaeini et al., 2018; Lee et al., 2017), recent work cast doubt regarding their faithfulness (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). 4 Evaluation via Utility While explanations have many different use-cases, such as model debugging, lawful guarantees or health-critical guarantees, one other possible usecase with prominent evaluation literature is Intelligent User Interfaces (IUI), via Human-Computer Interaction (HCI), of automatic models assisting human decision-makers. The goal of the explanation here is to increase the degree of trust between the user and the system, giving the user more nuance towards whether the system’s decision is likely correct, or not. In the ge"
2020.acl-main.386,W19-4807,0,0.0768162,"faithfulness. “Plausibility” refers to how convincing the interpretation is to humans, while “faithfulness” refers to how accurately it reflects the true reasoning process of the model (Herman, 2017; Wiegreffe and Pinter, 2019). Naturally, it is possible to satisfy one of these properties without the other. For example, consider the case of interpretation via posthoc text generation—where an additional “generator” component outputs a textual explanation of the model’s decision, and the generator is learned with supervision of textual explanations (Zaidan and Eisner, 2008; Rajani et al., 2019; Strout et al., 2019). In this case, plausibility is the dominating property, while there is no faithfulness guarantee. Despite the difference between the two criteria, many authors do not clearly make the distinction, and sometimes conflate the two.3 Moreoever, the majority of works do not explicitly name the criteria under consideration, even when they clearly belong to one camp or the other.4 We argue that this conflation is dangerous. For example, consider the case of recidivism prediction, 3 E.g., Lundberg and Lee (2017); P¨orner et al. (2018); Wu and Mooney (2018). 4 E.g., Mohseni and Ragan (2018); Arras et"
2020.acl-main.386,W18-5411,0,0.0152239,"erpretations, disprove the faithfulness of the method. Wiegreffe and Pinter (2019) show how these counter-examples can be derived with adversarial training of models which can mimic the original model, yet provide different explanations.6 Corollary 1.2. An interpretation is unfaithful if it results in different decisions than the model it interprets. A more direct application of the Model Assumption is via the notion of fidelity (Guidotti et al., 2018; Lakkaraju et al., 2019). For cases in which the explanation is itself a model capable of making decisions (e.g., decision trees or rule lists (Sushil et al., 2018)), fidelity is defined as the degree to which the explanation model can mimic the original model’s decisions (as an accuracy score). For cases where the explanation is not a computable model, Doshi-Velez and Kim (2017) propose a simple way of mapping explanations to decisions via crowdsourcing, by asking humans to simulate the model’s decision without any access to the model, and only access to the input and explanation (termed forward simulation). This idea is further explored and used in practice by Nguyen (2018). Assumption 2 (The Prediction Assumption). On similar inputs, the model makes s"
2020.acl-main.386,W19-4808,0,0.0241507,"ness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility. 1 Introduction Fueled by recent advances in deep-learning and language processing, NLP systems are increasingly being used for prediction and decision-making in many fields (Vig and Belinkov, 2019), including sensitive ones such as health, commerce and law (Fort and Couillault, 2016). Unfortunately, these highly flexible and highly effective neural models are also opaque. There is therefore a critical need for explaining learning-based models’ decisions. The emerging research topic of interpretability or explainability1 has grown rapidly in recent years. Unfortunately, not without growing pains. 1 Despite fine-grained distinctions between the terms, within the scope of this work we use the terms “interpretability” and “explainability” interchangeably. One such pain is the challenge of d"
2020.acl-main.386,D19-1002,0,0.294706,"nge to the community for the coming future. 2 Faithfulness vs. Plausibility There is considerable research effort in attempting to define and categorize the desiderata of a learned system’s interpretation, most of which revolves around specific use-cases (Lipton, 2018; Guidotti et al., 2018, inter alia). Two particularly notable criteria, each useful for a different purposes, are plausibility and faithfulness. “Plausibility” refers to how convincing the interpretation is to humans, while “faithfulness” refers to how accurately it reflects the true reasoning process of the model (Herman, 2017; Wiegreffe and Pinter, 2019). Naturally, it is possible to satisfy one of these properties without the other. For example, consider the case of interpretation via posthoc text generation—where an additional “generator” component outputs a textual explanation of the model’s decision, and the generator is learned with supervision of textual explanations (Zaidan and Eisner, 2008; Rajani et al., 2019; Strout et al., 2019). In this case, plausibility is the dominating property, while there is no faithfulness guarantee. Despite the difference between the two criteria, many authors do not clearly make the distinction, and somet"
2020.acl-main.386,D19-1420,0,0.10494,"rts of the input—according to the explanation—are erased from the input, in expectation that the model’s decision will change (Arras et al., 2016; Feng et al., 2018; Serrano and Smith, 2019). Otherwise, the “least relevant” parts of the input may be erased, in expectation that the model’s decision will not change (Jacovi et al., 7 This assumption has gone through justified scrutiny in recent work. As mentioned previously, we do not necessarily endorse it. Nevertheless, it is used in parts of the literature. 8 Also referred to as feature-attribution explanations (Kim et al., 2017). 4201 2018). Yu et al. (2019); DeYoung et al. (2019) propose two measures of comprehensiveness and sufficiency as a formal generalization of erasure: as the degree by which the model is influenced by the removal of the high-ranking features, or by inclusion of solely the high-ranking features. 7 fulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice. We note two possible approaches to this end: 1. Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models or tasks. Perhaps some models or tasks allow sufficiently faithful interpretat"
2020.acl-main.386,D08-1004,0,0.0390786,"or a different purposes, are plausibility and faithfulness. “Plausibility” refers to how convincing the interpretation is to humans, while “faithfulness” refers to how accurately it reflects the true reasoning process of the model (Herman, 2017; Wiegreffe and Pinter, 2019). Naturally, it is possible to satisfy one of these properties without the other. For example, consider the case of interpretation via posthoc text generation—where an additional “generator” component outputs a textual explanation of the model’s decision, and the generator is learned with supervision of textual explanations (Zaidan and Eisner, 2008; Rajani et al., 2019; Strout et al., 2019). In this case, plausibility is the dominating property, while there is no faithfulness guarantee. Despite the difference between the two criteria, many authors do not clearly make the distinction, and sometimes conflate the two.3 Moreoever, the majority of works do not explicitly name the criteria under consideration, even when they clearly belong to one camp or the other.4 We argue that this conflation is dangerous. For example, consider the case of recidivism prediction, 3 E.g., Lundberg and Lee (2017); P¨orner et al. (2018); Wu and Mooney (2018)."
2020.acl-main.386,W19-4812,0,\N,Missing
2020.acl-main.43,N18-1205,0,0.0296558,"port this conjecture. 1 Figure 1: Hierarchy of state expressiveness for saturated RNNs and related models. The y axis represents increasing space complexity. ∅ means provably empty. Models are in bold with qualitative descriptions in gray. Introduction While neural networks are central to the performance of today’s strongest NLP systems, theoretical understanding of the formal properties of different kinds of networks is still limited. It is established, for example, that the Elman (1990) RNN is Turing-complete, given infinite precision and computation time (Siegelmann and Sontag, 1992, 1994; Chen et al., 2018). But tightening these unrealistic assumptions has serious implications for expressive power (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address. Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like th"
2020.acl-main.43,D19-1110,1,0.85501,"ower (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address. Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like the quasirecurrent neural network (QRNN; Bradbury et al., 2016) and unigram rational RNN (Dodge et al., 2019) perform comparably to the LSTM, with a smaller computational budget. Still, the underlying simplicity of rational models raises the question of whether their expressive power is fundamentally limited compared to other RNNs. In a separate line of work, Merrill (2019) introduced the saturated RNN1 as a formal model for analyzing the capacity of RNNs. A saturated RNN is a simplified network where all activation functions have been replaced by step functions. The saturated network may be seen intuitively as a “stable” version of its original RNN, in which the in1 Originally referred to as the asy"
2020.acl-main.43,W18-2501,0,0.0432401,"Missing"
2020.acl-main.43,2020.tacl-1.11,0,0.111331,"Missing"
2020.acl-main.43,D14-1181,0,\N,Missing
2020.acl-main.43,D18-1152,1,\N,Missing
2020.acl-main.43,W19-3905,0,\N,Missing
2020.acl-main.51,P18-1073,0,0.049295,"desired property of an analysis method is stability: when applied several times with slightly different conditions, we expect the method to return the same, or very similar, results. Insignificant changes in the initial conditions should result in insignificant changes in the output. This increases the likelihood that the uncovered effects are real and not just artifacts of the initial conditions. Recent works question the stability of word embedding algorithms, demonstrating that different training runs produce different results, especially with small underlying datasets. Antoniak and Mimno (2018) focuses on the cosine-similarity between words in the learned embedding space, showing large variability upon minor manipulations on the corpus. Wendlandt et al. (2018) make a similar argument, showing that word embeddings are unstable by looking at the 10-nearest neighbors (NN) of a word across the different embeddings, and showing that larger lists of nearest neighbors are generally more stable. In this work, we are concerned with the stability of usage-change detection algorithms, and present a metric for measuring this stability. A usagechange detection algorithm takes as input two corpor"
2020.acl-main.51,J82-2005,0,0.716984,"Missing"
2020.acl-main.51,N19-1210,0,0.0294391,"Missing"
2020.acl-main.51,P16-1141,0,0.457854,"g different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew). 1 Introduction Analyzing differences in corpora from different sources (different time periods, populations, geographic regions, news outlets, etc) is a central use case in digital humanities and computational social science. A particular methodology is to identify individual words that are used differently in the different corpora. This includes words that have their meaning changed over time periods (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016b; Kutuzov et al., 2018; Tahmasebi et al., 2018), and words that are used differently by different populations (Azarbonyad et al., 2017; Rudolph et al., 2017). It is thus desired to have an automatic, robust and simple method for detecting such potential changes in word usage and surfacing them for human analysis. In this work we present such a method. ∗ djame.seddah@inria.fr A popular method for performing the task (§4) is to train word embeddings on each corpus and then to project one space to the other using a vectorspace alignment algorithm. Then, distances between a word-form to itself in"
2020.acl-main.51,S19-1001,0,0.0209125,"Missing"
2020.acl-main.51,P19-1379,0,0.133658,"factors, rather than linguistic shift. This may serve as another motivation to move from the global measures to a local one. Recent works (Giullianelli, 2019; Martinc et al., 2019) explored the possibility of modeling diachronic and usage change using contextualized embeddings extracted from now ubiquitous Bert representations (Devlin et al., 2019). Focusing on the financial domain, Montariol and Allauzen (2020) use, on top of Bert embeddings, a clustering method that does not need to predefine the number of clusters and which leads to interesting results on that domain. Another approach from Hu et al. (2019) relies on the inclusion of example-based word sense inventories over time from the Oxford dictionary to a Bert model. Doing so provides an efficient fine-grained word sense representation and enables a seemingly accurate way to monitor word sense change over time. Most of those approaches could be easily used with our method, the inclusion of contextualized embeddings would be for example straightforward, we leave it for future work. 9 Conclusion Detecting words that are used differently in different corpora is an important use-case in corpusbased research. We present a simple and effective m"
2020.acl-main.51,K18-1021,0,0.0491753,"X correspond to embeddings of words in space A, while the rows of Y are the corresponding embeddings in space B. This optimization is solved using the Orthogonal Procrustes (OP) method (Sch¨onemann, 1966), that provides a closed form solution. Vector space alignment methods are extensively studied also outside of the area of detecting word change, primarily for aligning embedding spaces across language pairs (Xing et al., 2015; Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a). Also there, the Orthogonal Procrustes method is taken to be a top contender (Lample et al., 2018b; Kementchedjhieva et al., 2018). 4.1 Shortcomings of the alignment approach Self-contradicting objective. Note that the optimization procedure in the (linear) alignment stage attempts to project each word to itself. This includes words that changed usage, and which therefore should not be near each other in the space. While one may hope that other words and the linearity constraints will intervene, the method may succeed, by mistake, to project words that did change usage next to each other, at the expense of projecting words that did not change usage further apart than they should be. This is an inherent problem with any a"
2020.acl-main.51,W14-2517,0,0.216973,"iveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew). 1 Introduction Analyzing differences in corpora from different sources (different time periods, populations, geographic regions, news outlets, etc) is a central use case in digital humanities and computational social science. A particular methodology is to identify individual words that are used differently in the different corpora. This includes words that have their meaning changed over time periods (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016b; Kutuzov et al., 2018; Tahmasebi et al., 2018), and words that are used differently by different populations (Azarbonyad et al., 2017; Rudolph et al., 2017). It is thus desired to have an automatic, robust and simple method for detecting such potential changes in word usage and surfacing them for human analysis. In this work we present such a method. ∗ djame.seddah@inria.fr A popular method for performing the task (§4) is to train word embeddings on each corpus and then to project one space to the other using a vectorspace alignment algorithm. Th"
2020.acl-main.51,N18-1190,0,0.131702,"same, or very similar, results. Insignificant changes in the initial conditions should result in insignificant changes in the output. This increases the likelihood that the uncovered effects are real and not just artifacts of the initial conditions. Recent works question the stability of word embedding algorithms, demonstrating that different training runs produce different results, especially with small underlying datasets. Antoniak and Mimno (2018) focuses on the cosine-similarity between words in the learned embedding space, showing large variability upon minor manipulations on the corpus. Wendlandt et al. (2018) make a similar argument, showing that word embeddings are unstable by looking at the 10-nearest neighbors (NN) of a word across the different embeddings, and showing that larger lists of nearest neighbors are generally more stable. In this work, we are concerned with the stability of usage-change detection algorithms, and present a metric for measuring this stability. A usagechange detection algorithm takes as input two corpora, and returns a ranked list r of candidate words, sorted from the most likely to have changed to the least likely. For a stable algorithm, we expect different runs to r"
2020.acl-main.51,P14-1096,0,0.209763,"Missing"
2020.acl-main.51,P19-1249,0,0.0211102,"Missing"
2020.acl-main.51,2020.jeptalnrecital-taln.31,0,0.0429294,"Missing"
2020.acl-main.51,N15-1104,0,0.0840014,"as AlignCos. The alignment is performed by finding an orthogonal linear transformation Q that, when given matrices X and Y , projects X to Y while minimizng the squared loss: The rows of X correspond to embeddings of words in space A, while the rows of Y are the corresponding embeddings in space B. This optimization is solved using the Orthogonal Procrustes (OP) method (Sch¨onemann, 1966), that provides a closed form solution. Vector space alignment methods are extensively studied also outside of the area of detecting word change, primarily for aligning embedding spaces across language pairs (Xing et al., 2015; Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a). Also there, the Orthogonal Procrustes method is taken to be a top contender (Lample et al., 2018b; Kementchedjhieva et al., 2018). 4.1 Shortcomings of the alignment approach Self-contradicting objective. Note that the optimization procedure in the (linear) alignment stage attempts to project each word to itself. This includes words that changed usage, and which therefore should not be near each other in the space. While one may hope that other words and the linearity constraints will intervene, the method may succeed, by mi"
2020.acl-main.610,D18-1523,1,0.810635,"the vocabulary as an embedding vector that summarizes all the contexts the term appears in in a large corpus, and then look for terms with vectors that are similar to those of the seed term. The methods differ in their context definitions and in their way of computing similarities. A shortcoming of these methods is that they consider all occurrences of a term in the corpus when calculating its representation, including many contexts that are irrelevant to the concept at hand due to polysemy, noise in the corpus or noninformative contexts. 2 In contrast, the pattern-based approach consid1 See (Amrami and Goldberg, 2018) for a method that uses MLM word completions for word-sense induction. 2 The work of Mahabal et al. (2018) is unique in this regard by considering only a subset of the contexts that are relevant for the expansion, as determined from the seed set. 6829 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6829–6835 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers specific indicative patterns that signal the desired concept, looking for them in a large corpus, and extracting the terms that appear in them. Patterns can be binary (Hea"
2020.acl-main.610,N19-1423,0,0.0256701,"2 .921 k=3000 .975 .916 Table 2: Effect of similarity measure’s k on performance, using MPB2 on a single random seed from each set. ferent senses, it is sufficient for only some patterns in pats(t) to be similar to patterns in m1 , ..., m` . We score a term t as: score(t) = ` X i=1 ci max m∈pats(t) sim(mi , m) (3) where ci is the P`pattern weighing factor from equation (2). As i=1 ci = 1, the term score score(t) for every term t is ∈ [0, 1]. 3 Experiments and Results We refer to the method in Section (2.2) as MPB1 and the method in section (2.3) as MPB2. Setup. In our experiments we use BERT (Devlin et al., 2019) as the MLM, and English Wikipedia as the corpus. Following previous TSE work (e.g. (Mahabal et al., 2018)), we measure performance using MAP (using MAP70 for the open set). For each method we report the average MAP over several runs (exact number mentioned under each table), each with a different random seed set of size 3. Based on preliminary experiments, for MPB1 we use ` = 160 and L = 2000/k and for MPB2 we use ` = 20 and L = 2000/k. 7 When comparing different systems (i.e, in Table 3), each system sees the same random seed sets as the others. For smaller sets we expand to a set of size 20"
2020.acl-main.610,W14-1611,0,0.0204132,"sidering only a subset of the contexts that are relevant for the expansion, as determined from the seed set. 6829 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6829–6835 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers specific indicative patterns that signal the desired concept, looking for them in a large corpus, and extracting the terms that appear in them. Patterns can be binary (Hearst, 1992; Ohshima et al., 2006; Zhang et al., 2009) (“such as X or Y”), indicating that both X and Y belong to the same class, or unary (Gupta and Manning, 2014; Wang and Cohen, 2007) (“fruits such as X”, “First I painted the wall red, but then I repainted it X”), suggesting that X belongs to a certain category (fruit, color). The patterns can be determined manually (Hearst, 1992) or automatically (Wang and Cohen, 2007; Gupta and Manning, 2014). While well tailored patterns can be precise and interpretable, a notable shortcoming of pattern-based methods is their lack of coverage, due to the challenge of finding patterns that are specific enough to be accurate yet common enough in a large corpus to be useful. Wang and Cohen (2007) use patterns from no"
2020.acl-main.610,C92-2082,0,0.546987,"18) for a method that uses MLM word completions for word-sense induction. 2 The work of Mahabal et al. (2018) is unique in this regard by considering only a subset of the contexts that are relevant for the expansion, as determined from the seed set. 6829 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6829–6835 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers specific indicative patterns that signal the desired concept, looking for them in a large corpus, and extracting the terms that appear in them. Patterns can be binary (Hearst, 1992; Ohshima et al., 2006; Zhang et al., 2009) (“such as X or Y”), indicating that both X and Y belong to the same class, or unary (Gupta and Manning, 2014; Wang and Cohen, 2007) (“fruits such as X”, “First I painted the wall red, but then I repainted it X”), suggesting that X belongs to a certain category (fruit, color). The patterns can be determined manually (Hearst, 1992) or automatically (Wang and Cohen, 2007; Gupta and Manning, 2014). While well tailored patterns can be precise and interpretable, a notable shortcoming of pattern-based methods is their lack of coverage, due to the challenge"
2020.acl-main.610,N19-1419,0,0.0422544,"Missing"
2020.acl-main.610,P90-1034,0,0.731479,"nstance of a generalization from few examples problem. Solving TSE requires the algorithm to: (1) identify the desired concept class based on few examples; and (2) identify additional members of the class. We present an effective TSE method which is based on querying large, pre-trained masked language models (MLMs). Pre-trained language Previous solutions to the TSE problem (also called semantic class induction) can be roughly categorized into distributional and pattern-based approaches (Shi et al., 2010). Our method can be seen as a combination of the two. The distributional approach to TSE (Hindle, 1990; Pantel and Lin, 2002; Pantel et al., 2009; Mamou et al., 2018; Mahabal et al., 2018) operates under the hypothesis that similar words appear in similar contexts (Harris, 1968). These methods represent each term in the vocabulary as an embedding vector that summarizes all the contexts the term appears in in a large corpus, and then look for terms with vectors that are similar to those of the seed term. The methods differ in their context definitions and in their way of computing similarities. A shortcoming of these methods is that they consider all occurrences of a term in the corpus when cal"
2020.acl-main.610,Q16-1037,1,0.887341,"Missing"
2020.acl-main.610,S18-2031,0,0.0182079,"r terms with vectors that are similar to those of the seed term. The methods differ in their context definitions and in their way of computing similarities. A shortcoming of these methods is that they consider all occurrences of a term in the corpus when calculating its representation, including many contexts that are irrelevant to the concept at hand due to polysemy, noise in the corpus or noninformative contexts. 2 In contrast, the pattern-based approach consid1 See (Amrami and Goldberg, 2018) for a method that uses MLM word completions for word-sense induction. 2 The work of Mahabal et al. (2018) is unique in this regard by considering only a subset of the contexts that are relevant for the expansion, as determined from the seed set. 6829 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6829–6835 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers specific indicative patterns that signal the desired concept, looking for them in a large corpus, and extracting the terms that appear in them. Patterns can be binary (Hearst, 1992; Ohshima et al., 2006; Zhang et al., 2009) (“such as X or Y”), indicating that both X and Y belo"
2020.acl-main.610,D18-2004,0,0.110941,"lving TSE requires the algorithm to: (1) identify the desired concept class based on few examples; and (2) identify additional members of the class. We present an effective TSE method which is based on querying large, pre-trained masked language models (MLMs). Pre-trained language Previous solutions to the TSE problem (also called semantic class induction) can be roughly categorized into distributional and pattern-based approaches (Shi et al., 2010). Our method can be seen as a combination of the two. The distributional approach to TSE (Hindle, 1990; Pantel and Lin, 2002; Pantel et al., 2009; Mamou et al., 2018; Mahabal et al., 2018) operates under the hypothesis that similar words appear in similar contexts (Harris, 1968). These methods represent each term in the vocabulary as an embedding vector that summarizes all the contexts the term appears in in a large corpus, and then look for terms with vectors that are similar to those of the seed term. The methods differ in their context definitions and in their way of computing similarities. A shortcoming of these methods is that they consider all occurrences of a term in the corpus when calculating its representation, including many contexts that are i"
2020.acl-main.610,D19-1250,0,0.0684887,"Missing"
2020.acl-main.610,C10-1112,0,0.0293634,"le”} into a set of tech companies. Beyond being of great practical utility, the TSE task is a challenging instance of a generalization from few examples problem. Solving TSE requires the algorithm to: (1) identify the desired concept class based on few examples; and (2) identify additional members of the class. We present an effective TSE method which is based on querying large, pre-trained masked language models (MLMs). Pre-trained language Previous solutions to the TSE problem (also called semantic class induction) can be roughly categorized into distributional and pattern-based approaches (Shi et al., 2010). Our method can be seen as a combination of the two. The distributional approach to TSE (Hindle, 1990; Pantel and Lin, 2002; Pantel et al., 2009; Mamou et al., 2018; Mahabal et al., 2018) operates under the hypothesis that similar words appear in similar contexts (Harris, 1968). These methods represent each term in the vocabulary as an embedding vector that summarizes all the contexts the term appears in in a large corpus, and then look for terms with vectors that are similar to those of the seed term. The methods differ in their context definitions and in their way of computing similarities."
2020.acl-main.610,P19-1452,0,0.028699,"Missing"
2020.acl-main.610,P09-1052,0,0.0360582,"completions for word-sense induction. 2 The work of Mahabal et al. (2018) is unique in this regard by considering only a subset of the contexts that are relevant for the expansion, as determined from the seed set. 6829 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6829–6835 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ers specific indicative patterns that signal the desired concept, looking for them in a large corpus, and extracting the terms that appear in them. Patterns can be binary (Hearst, 1992; Ohshima et al., 2006; Zhang et al., 2009) (“such as X or Y”), indicating that both X and Y belong to the same class, or unary (Gupta and Manning, 2014; Wang and Cohen, 2007) (“fruits such as X”, “First I painted the wall red, but then I repainted it X”), suggesting that X belongs to a certain category (fruit, color). The patterns can be determined manually (Hearst, 1992) or automatically (Wang and Cohen, 2007; Gupta and Manning, 2014). While well tailored patterns can be precise and interpretable, a notable shortcoming of pattern-based methods is their lack of coverage, due to the challenge of finding patterns that are specific enoug"
2020.acl-main.610,D09-1098,0,\N,Missing
2020.acl-main.647,D17-1169,0,0.109479,"Missing"
2020.acl-main.647,D19-1250,0,0.0272459,"Missing"
2020.acl-main.647,D07-1043,0,0.474476,"Missing"
2020.acl-main.647,P19-1452,0,0.076981,"Missing"
2020.acl-main.647,D18-1521,0,0.0607325,"used de7237 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7237–7256 c July 5 - 10, 2020. 2020 Association for Computational Linguistics terministic operator. Like the adversarial methods, it is data-driven in the directions it removes: we do not presuppose specific directions in the latent space that correspond to the protected attribute, but rather learn those directions, and remove them. Empirically, we find it to work well. We evaluate the method on the challenging task of removing gender signals from word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018). Recently, Gonen and Goldberg (2019) showed several limitations of current methods for this task. We show that our method is effective in reducing many, but not all, of these (§4). We also consider the context of fair classification, where we want to ensure that a classifier’s decision is oblivious to a protected attribute such as race, gender or age. There, we need to integrate the projection-based method within a pre-trained classifier. We propose a method to do so in section §5, and demonstrate its effectiveness in a controlled setup (§6.2) as well as in a real-world one (§6.3). Finally, w"
2020.acl-main.647,J15-4004,0,\N,Missing
2020.acl-main.647,N09-1003,0,\N,Missing
2020.acl-main.647,D16-1120,0,\N,Missing
2020.acl-main.647,Q16-1037,1,\N,Missing
2020.acl-main.647,D18-1001,0,\N,Missing
2020.acl-main.647,W19-3621,1,\N,Missing
2020.acl-main.647,N19-1061,1,\N,Missing
2020.acl-main.647,N19-1419,0,\N,Missing
2020.acl-main.647,E17-2068,0,\N,Missing
2020.acl-main.647,D19-1662,1,\N,Missing
2020.acl-main.692,P19-1286,0,0.189807,"11), (2) fine-tune a pre-trained general-domain model (Sajjad et al., 2017; Silva et al., 2018), or (3) prioritize data for annotation as in an Active-Learning framework, if only monolingual data is available (Haffari et al., 2009). To demonstrate the need for domain data selection and set the stage for our data selection experiments, we perform preliminary experiments with NMT in a multi-domain scenario. 3.1 Multi-Domain Dataset To simulate a diverse multi-domain setting we use the dataset proposed in Koehn and Knowles (2017), as it was recently adopted for domain adaptation research in NMT (Hu et al., 2019; M¨uller et al., 2019; Dou et al., 2019a,b). The dataset includes parallel text in German and English from five diverse domains (Medical, Law, Koran, IT, Subtitles; as discussed in Section 2), available via OPUS (Tiedemann, 2012; Aulamo and Tiedemann, 2019). In a preliminary analysis of the data we found that in both the original train/dev/test split by Koehn and Knowles (2017) and in the more recent split by M¨uller et al. (2019) there was overlap between the training data and the dev/test data.9 Fixing these issues is important, as it may affect the conclusions one draws from experiments wi"
2020.acl-main.692,W18-6478,0,0.022922,"machine translation benchmark. Our methods perform similarly or better than an established data selection method and oracle in-domain training across all five domains in the benchmark. This work just scratches the surface with what can be done on the subject; possible avenues for future work include extending this with multilingual data selection and multilingual LMs (Conneau and Lample, 2019; Conneau et al., 2019; Wu et al., 2019; Hu et al., 2020), using such selection methods with domain-curriculum training (Zhang et al., 2019; Wang et al., 2019b), applying them on noisy, web-crawled data (Junczys-Dowmunt, 2018) or for additional tasks (Gururangan et al., 2020). Another interesting avenue is applying this to unsupervised NMT, which is highly sensitive to domain mismatch (Marchisio et al., 2020; Kim et al., 2020). We hope this work will encourage more research on finding the right data for the task, towards more efficient and robust NLP. Acknowledgements We thank Wei Wang for early discussions on domain adaptation and data selection that inspired this work during Roee’s internship in Google Translate. 7755 References Roee Aharoni and Yoav Goldberg. 2018. Split and rephrase: Better evaluation and stron"
2020.acl-main.692,1983.tc-1.13,0,0.418343,"Missing"
2020.acl-main.692,P07-2045,0,0.00949362,"Missing"
2020.acl-main.692,W17-3204,0,0.565174,", 2019), an important aspect of language remained overlooked in this context – the domain the data comes from, often referred to as the “data distribution”. The definition of domain is many times vague and over-simplistic (e.g. “medical text” may be used for biomedical research papers and for clinical conversations between doctors and patients, although the two vary greatly in topic, formality etc.). A common definition treats a domain as a data source: “a domain is defined by a corpus from a specific source, and may differ from other domains in topic, genre, style, level of formality, etc.” (Koehn and Knowles, 2017). We claim that a more data-driven definition should take place, as different data sources may have sentences with similar traits and vice versa - a single massive web-crawled corpus contains texts in numerous styles, topics and registers. Our analysis in Section 2 shows examples for such cases, e.g. a sentence discussing “Viruses and virus-like organisms” in a legal corpus. We hypothesize that massive pretrained LMs can learn representations that cluster to domains, as texts from similar domains will appear in similar contexts. We test this hypothesis across several large, publicly-available"
2020.acl-main.692,2021.ccl-1.108,0,0.141249,"Missing"
2020.acl-main.692,D19-6109,0,0.0196849,"l. (2017) explored instance-based data selection in a multi-domain scenario using information retrieval methods. Other related works on domain adaptation include Dou et al. (2019a) that adapts multi-domain NMT models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Peris et al. (2017) proposed neuralnetwork based classifiers for data selection in SMT. For more related work on data selection and domain adaptation in the context of MT, see the surveys by Eetemadi et al. (2015) for SMT and more recently Chu and Wang (2018) for NMT. Unrelated to MT, Ma et al. (2019) used BERT to select data for tasks from the GLUE benchmark (Wang et al., 2018). However, they assumed supervision for all the different tasks/domains, while we propose an unsupervised method requiring only a small set of in-domain data. Also in the context of pretrained language models, Gururangan et al. (2020) show the importance of additional pretraining with in-domain data to improve the downstream task-specific performance. While previous work made important contributions to domain data selection, our work is the first to explore massive pretrained language models for both unsupervised do"
2020.acl-main.692,2020.wmt-1.68,0,0.0398933,"Missing"
2020.acl-main.692,P10-2041,0,0.708097,"ersity of language makes it hard to find the right data for the task, as it is nearly impossible to well-define the exact requirements from such data with respect to all the aforementioned aspects. On top of that, domain labels are usually unavailable – e.g. in large-scale web-crawled data like Common Crawl1 which was recently used to 1 https://commoncrawl.org/ train state-of-the-art pretrained language models for various tasks (Raffel et al., 2019). Domain data selection is the task of selecting the most appropriate data for a domain from a large corpus given a smaller set of in-domain data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Silva et al., 2018). In this work, we propose to use the recent, highly successful self-supervised pre-trained language models, e.g. Devlin et al. (2019); Liu et al. (2019) for domain data selection. As pretrained LMs demonstrate state-of-theart performance across many NLP tasks after being trained on massive amounts of data, we hypothesize that the robust representations they learn can be useful for mapping sentences to domains in an unsupervised, data-driven approach. We show that these models indeed learn to cluster sentence representations to domai"
2020.acl-main.692,W19-5333,0,0.0168753,"ection, using the right data is critical for achieving good performance on an in-domain test set, and more data is not necessarily better. However, in real-world scenarios, the availability of data labeled by domain is limited, e.g. when working with large scale, web-crawled data. In this section we focus on a data-selection scenario where only a very small number of indomain sentences are used to select data from a larger unlabeled parallel corpus. An established method for data selection was proposed by Moore and Lewis (2010), which was also used in training the winning systems in WMT 2019 (Ng et al., 2019; Barrault et al., 2019). This method compares the cross-entropy, according to domain-specific and non-domain-specific language models, for each candidate sentence for selection. The sentences are then ranked by the cross-entropy difference, and only the top sentences are selected for training. While the method by Moore and Lewis (2010) is tried-and-true, it is based on simple n-gram language models which cannot generalize beyond the n-grams that are seen in the in-domain set. In addition, it is restricted to the in-domain and generaldomain datasets it is trained on, which are usually small. O"
2020.acl-main.692,D17-1299,0,0.0145574,"n of average-pooled BERT hidden-state sentence representations using PCA. The colors represent the domain for each sentence. Introduction It is common knowledge in modern NLP that using large amounts of high-quality training data is a key aspect in building successful machine-learning based systems. For this reason, a major challenge when building such systems is obtaining data in the domain of interest. But what defines a domain? Natural language varies greatly across topics, styles, levels of formality, genres and many other linguistic nuances (van der Wees et al., 2015; van der Wees, 2017; Niu et al., 2017). This overwhelming diversity of language makes it hard to find the right data for the task, as it is nearly impossible to well-define the exact requirements from such data with respect to all the aforementioned aspects. On top of that, domain labels are usually unavailable – e.g. in large-scale web-crawled data like Common Crawl1 which was recently used to 1 https://commoncrawl.org/ train state-of-the-art pretrained language models for various tasks (Raffel et al., 2019). Domain data selection is the task of selecting the most appropriate data for a domain from a large corpus given a smaller"
2020.acl-main.692,N19-4009,0,0.0385818,"Missing"
2020.acl-main.692,N18-1202,0,0.0265574,"onal contribution is a new, improved data split we create for this benchmark, as we point on issues with previous splits used in the literature. The code and data for this work is publicly available.2 We hope this work will encourage more research on understanding the data landscape in NLP, enabling to “find the right data for the task” in the age of massive models and diverse data sources. 2 https://github.com/roeeaharoni/ unsupervised-domain-clusters Emerging Domain Clusters in Pretrained Language Models Motivation The proliferation of massive pretrained neural language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) has enabled great progress on many NLP benchmarks (Wang et al., 2018, 2019a). Larger and larger models trained on billions of tokens of raw text are released in an ever-increasing pace (Raffel et al., 2019), enabling the NLP community to fine-tune them for the task of interest. While many works tried to “probe” those models for the morphological, syntactic and semantic information they capture (Tenney et al., 2019; Goldberg, 2019; Clark et al., 2019), an important aspect of language remained overlooked in this context – the domain the"
2020.acl-main.692,W18-6319,0,0.0256496,"Missing"
2020.acl-main.692,E17-2025,0,0.0422455,"Missing"
2020.acl-main.692,N19-5004,0,0.0260977,"first compute a query vector, which is the element-wise average over the vector representations of the sentences in the small in-domain set. We use the same sentencelevel average-pooling approach as described in Section 2 to obtain sentence representations. We then retrieve the most relevant sentences in the training set by computing the cosine similarity of each sentence with this query vector and ranking the sentences accordingly. Domain-Finetune It is now common knowledge that pretrained language models are especially useful when fine-tuned for the task of interest in an end-to-end manner (Ruder et al., 2019). In this method we fine-tune the pretrained LM for binary classification, where we use the in-domain sentences as positive examples, and randomly sampled general-domain sentences as negative examples. We then apply this classifier on the generaldomain data and pick the sentences that are classified as positive as in-domain, or choose the top-k sentences as ranked by the classifier output distribution. This can be seen as an instance of positiveunlabeled learning for document-set expansion; see Jacovi et al. (2019) for a recent discussion and methodology for this task. Negative Sampling with P"
2020.acl-main.692,D17-1038,0,0.0355901,"s do not capture the rich sentence-level global context as in the recent self-attention-based MLMs; as we showed in the clustering experiments, autoregressive neural LMs were inferior to masked LMs in clustering the data by domain. In addition, training large LMs may be prohibitive without relying on pre-training. Regarding domain clustering for MT, Hasler et al. (2014) discovered topics using LDA instead of using domain labels. Cuong et al. (2016) induced latent subdomains from the training data using a dedicated probabilistic model. Many works used vector-based retrieval for data selection; Ruder and Plank (2017) learn to select data using Bayesian optimization, and explored word2vec for that purpose. Duma and Menzel (2016) create paragraph vectors for data selection in the context of SMT. Wang et al. (2017) use internal representations from the NMT model to perform data selection. Bapna and Firat (2019) propose a mechanism for incorporating retrieved sentences for each instance for domain adaptation in NMT, using representations extracted from a pretrained NMT model. Farajian et al. (2017) explored instance-based data selection in a multi-domain scenario using information retrieval methods. Other rel"
2020.acl-main.692,E17-2045,0,0.0449498,"ario As we showed that pre-trained language models are indeed very useful in clustering sentence representations by domains in an unsupervised manner, we now seek to harness this property for a downstream task – domain data selection for machine translation. Domain data selection is the task of selecting examples from a large corpus which are as close as possible to the domain of interest, given a smaller set of in-domain examples. The selected examples can be used to either (1) train a domainspecific model from scratch (Axelrod et al., 2011), (2) fine-tune a pre-trained general-domain model (Sajjad et al., 2017; Silva et al., 2018), or (3) prioritize data for annotation as in an Active-Learning framework, if only monolingual data is available (Haffari et al., 2009). To demonstrate the need for domain data selection and set the stage for our data selection experiments, we perform preliminary experiments with NMT in a multi-domain scenario. 3.1 Multi-Domain Dataset To simulate a diverse multi-domain setting we use the dataset proposed in Koehn and Knowles (2017), as it was recently adopted for domain adaptation research in NMT (Hu et al., 2019; M¨uller et al., 2019; Dou et al., 2019a,b). The dataset i"
2020.acl-main.692,P16-1162,0,0.0632162,"Missing"
2020.acl-main.692,P19-1452,0,0.0158595,"ters in Pretrained Language Models Motivation The proliferation of massive pretrained neural language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) has enabled great progress on many NLP benchmarks (Wang et al., 2018, 2019a). Larger and larger models trained on billions of tokens of raw text are released in an ever-increasing pace (Raffel et al., 2019), enabling the NLP community to fine-tune them for the task of interest. While many works tried to “probe” those models for the morphological, syntactic and semantic information they capture (Tenney et al., 2019; Goldberg, 2019; Clark et al., 2019), an important aspect of language remained overlooked in this context – the domain the data comes from, often referred to as the “data distribution”. The definition of domain is many times vague and over-simplistic (e.g. “medical text” may be used for biomedical research papers and for clinical conversations between doctors and patients, although the two vary greatly in topic, formality etc.). A common definition treats a domain as a data source: “a domain is defined by a corpus from a specific source, and may differ from other domains in topic, genre, styl"
2020.acl-main.692,tiedemann-2012-parallel,0,0.0586289,"9). To demonstrate the need for domain data selection and set the stage for our data selection experiments, we perform preliminary experiments with NMT in a multi-domain scenario. 3.1 Multi-Domain Dataset To simulate a diverse multi-domain setting we use the dataset proposed in Koehn and Knowles (2017), as it was recently adopted for domain adaptation research in NMT (Hu et al., 2019; M¨uller et al., 2019; Dou et al., 2019a,b). The dataset includes parallel text in German and English from five diverse domains (Medical, Law, Koran, IT, Subtitles; as discussed in Section 2), available via OPUS (Tiedemann, 2012; Aulamo and Tiedemann, 2019). In a preliminary analysis of the data we found that in both the original train/dev/test split by Koehn and Knowles (2017) and in the more recent split by M¨uller et al. (2019) there was overlap between the training data and the dev/test data.9 Fixing these issues is important, as it may affect the conclusions one draws from experiments with 7751 9 More details are available in the supplementary material. Medical Law IT Koran Subtitles Original 1,104,752 715,372 378,477 533,128 22,508,639 New Split 248,099 467,309 222,927 17,982 14,458,058 Medical Law Koran IT Sub"
2020.acl-main.692,N19-1189,0,0.057555,"Missing"
2020.acl-main.692,W18-5446,0,0.0795635,"Missing"
2020.acl-main.692,P17-2089,0,0.0333737,"ustering the data by domain. In addition, training large LMs may be prohibitive without relying on pre-training. Regarding domain clustering for MT, Hasler et al. (2014) discovered topics using LDA instead of using domain labels. Cuong et al. (2016) induced latent subdomains from the training data using a dedicated probabilistic model. Many works used vector-based retrieval for data selection; Ruder and Plank (2017) learn to select data using Bayesian optimization, and explored word2vec for that purpose. Duma and Menzel (2016) create paragraph vectors for data selection in the context of SMT. Wang et al. (2017) use internal representations from the NMT model to perform data selection. Bapna and Firat (2019) propose a mechanism for incorporating retrieved sentences for each instance for domain adaptation in NMT, using representations extracted from a pretrained NMT model. Farajian et al. (2017) explored instance-based data selection in a multi-domain scenario using information retrieval methods. Other related works on domain adaptation include Dou et al. (2019a) that adapts multi-domain NMT models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Peris e"
2020.acl-main.692,P19-1123,0,0.0390064,"Missing"
2020.acl-main.692,E12-1016,0,\N,Missing
2020.acl-main.692,D11-1033,0,\N,Missing
2020.acl-main.692,N09-1047,0,\N,Missing
2020.acl-main.692,E14-1035,0,\N,Missing
2020.acl-main.692,P13-2119,0,\N,Missing
2020.acl-main.692,P15-2092,0,\N,Missing
2020.acl-main.692,Q16-1008,0,\N,Missing
2020.acl-main.692,W16-2331,0,\N,Missing
2020.acl-main.692,W18-6323,0,\N,Missing
2020.acl-main.692,W19-4828,0,\N,Missing
2020.acl-main.692,N19-1423,0,\N,Missing
2020.acl-main.692,W19-5301,0,\N,Missing
2020.acl-main.692,D19-5606,0,\N,Missing
2020.acl-main.692,2020.acl-main.740,0,\N,Missing
2020.acl-main.692,2020.eamt-1.5,0,\N,Missing
2020.acl-main.692,W11-2123,0,\N,Missing
2020.acl-main.692,W17-4713,0,\N,Missing
2020.acl-main.692,W19-6146,0,\N,Missing
2020.bionlp-1.3,P13-4027,0,0.310303,"tes itself in a number of ways: (i) our query engine automatically translates lightly tagged natural language sentences to syntactic queries (queryby-example) thus allowing domain experts to benefit from the advantages of syntactic patterns without a deep understanding of syntax; (ii) our queries run against indexed data, allowing our translated syntactic queries to run at interactive speed; and (iii) our system does not rely on relation schemas and does not make assumptions about the number of arguments involved or their types. In many respects, our system is similar to the PropMiner system (Akbik et al., 2013) for exploratory relation extraction (Akbik et al., 2014). Both PropMiner and our system support byexample queries in interactive speed. However, the query languages we describe in section 4 are significantly more expressive than PropMiner’s language, which supports only binary relations. Furthermore, compared to PropMiner, our annotation pipeline was optimized specifically for the biomedical domain and our system is freely available online. Technical details. The datasets were annotated for biomedical entities and syntax using a custom SciSpacy pipeline (Neumann et al., 2019)6 , and the synta"
2020.bionlp-1.3,W13-2001,0,0.154771,"Missing"
2020.bionlp-1.3,C14-1197,0,0.0245148,"matically translates lightly tagged natural language sentences to syntactic queries (queryby-example) thus allowing domain experts to benefit from the advantages of syntactic patterns without a deep understanding of syntax; (ii) our queries run against indexed data, allowing our translated syntactic queries to run at interactive speed; and (iii) our system does not rely on relation schemas and does not make assumptions about the number of arguments involved or their types. In many respects, our system is similar to the PropMiner system (Akbik et al., 2013) for exploratory relation extraction (Akbik et al., 2014). Both PropMiner and our system support byexample queries in interactive speed. However, the query languages we describe in section 4 are significantly more expressive than PropMiner’s language, which supports only binary relations. Furthermore, compared to PropMiner, our annotation pipeline was optimized specifically for the biomedical domain and our system is freely available online. Technical details. The datasets were annotated for biomedical entities and syntax using a custom SciSpacy pipeline (Neumann et al., 2019)6 , and the syntactic trees were enriched to BART format using pyBART (Tik"
2020.bionlp-1.3,W19-5034,0,0.0288543,"PropMiner system (Akbik et al., 2013) for exploratory relation extraction (Akbik et al., 2014). Both PropMiner and our system support byexample queries in interactive speed. However, the query languages we describe in section 4 are significantly more expressive than PropMiner’s language, which supports only binary relations. Furthermore, compared to PropMiner, our annotation pipeline was optimized specifically for the biomedical domain and our system is freely available online. Technical details. The datasets were annotated for biomedical entities and syntax using a custom SciSpacy pipeline (Neumann et al., 2019)6 , and the syntactic trees were enriched to BART format using pyBART (Tiktinsky et al., 2020). The annotated data is indexed using the Odinson engine (Valenzuela-Esc´arcega et al., 2020). Interactive IE Approach Existing approaches to information extraction from bio-medical data suffer from significant practical limitations. Techniques based on supervised training require extensive data collection and annotation (Kim et al., 2009, 2011; N´edellec et al., 2013; Segura Bedmar et al., 2013; Del´eger et al., 2016; Chaix et al., 2016), or a high degree of technical savviness in producing high qual"
2020.bionlp-1.3,Q17-1008,0,0.0455773,"d to BART format using pyBART (Tiktinsky et al., 2020). The annotated data is indexed using the Odinson engine (Valenzuela-Esc´arcega et al., 2020). Interactive IE Approach Existing approaches to information extraction from bio-medical data suffer from significant practical limitations. Techniques based on supervised training require extensive data collection and annotation (Kim et al., 2009, 2011; N´edellec et al., 2013; Segura Bedmar et al., 2013; Del´eger et al., 2016; Chaix et al., 2016), or a high degree of technical savviness in producing high quality data sets from distant supervision (Peng et al., 2017; Verga et al., 2017; Wang et al., 2019). On the other hand, rule based engines are generally too complex to be used directly by domain experts and require a linguist or an NLP specialist to operate. Furthermore, both rule based and supervised systems typically operate in a pipeline approach where an NER engine identifies the relevant entities and subsequent extraction models identify the relations between them. This approach is often problematic in real world biomedical IE scenarios, where relevant entities often cannot be extracted by stock NER models. To address these limitations we present"
2020.bionlp-1.3,S13-2056,0,0.0205891,"Missing"
2020.bionlp-1.3,W16-3002,0,0.0328787,"Missing"
2020.bionlp-1.3,2020.acl-demos.3,1,0.637968,"al., 2019). The tasks address a diverse set of biomed topics addressed by a range of NLP-based techniques. While effective, such systems require annotated training data and substantial expertise to produce. As such, they are restricted to several “head” information extraction needs, those that enjoy a wide community interest and support. The long tail of information needs of “casual” researchers remain mostly un-addressed. 3 search papers, using a novel multifaceted query language which we designed, and which supports boolean search, sequential patterns search, and byexample syntactic search (Shlain et al., 2020), as well as specification of search terms whose matches should be captured or expanded. The queries can be further restricted by contextual information. We demonstrate the system on two datasets: a comprehensive dataset of PubMed abstracts and a dataset of full text papers focused on COVID-19 research. Comparison to existing systems. In contrast to document level search solutions, the results returned by our system are sentences which include highlighted spans that directly answer the user’s information need. In contrast to supervised IE solutions, our solution does not require a lengthy proc"
2020.bionlp-1.3,W09-1401,0,0.0759437,"nt searching, some systems facilitate automatic extraction of biomedical concepts, or patterns, from documents. Such systems are often equipped with analysis capabilities of the extracted information. For example, NaCTem has created systems that extract biomedical entities, relations and events.5 ; ExaCT and RobotReviewer (Kiritchenko et al., 2010; Marshall et al., 2015) take a RCT report and retrieve sentences that match certain study characteristics. To improve the development of automatic document selection and information extraction the BioNLP community organized a series of shared tasks (Kim et al., 2009, 2011; N´edellec et al., 2013; Segura Bedmar et al., 2013; Del´eger et al., 2016; Chaix et al., 2016; Jin-Dong et al., 2019). The tasks address a diverse set of biomed topics addressed by a range of NLP-based techniques. While effective, such systems require annotated training data and substantial expertise to produce. As such, they are restricted to several “head” information extraction needs, those that enjoy a wide community interest and support. The long tail of information needs of “casual” researchers remain mostly un-addressed. 3 search papers, using a novel multifaceted query language"
2020.bionlp-1.3,2020.acl-demos.7,1,0.793599,"14). Both PropMiner and our system support byexample queries in interactive speed. However, the query languages we describe in section 4 are significantly more expressive than PropMiner’s language, which supports only binary relations. Furthermore, compared to PropMiner, our annotation pipeline was optimized specifically for the biomedical domain and our system is freely available online. Technical details. The datasets were annotated for biomedical entities and syntax using a custom SciSpacy pipeline (Neumann et al., 2019)6 , and the syntactic trees were enriched to BART format using pyBART (Tiktinsky et al., 2020). The annotated data is indexed using the Odinson engine (Valenzuela-Esc´arcega et al., 2020). Interactive IE Approach Existing approaches to information extraction from bio-medical data suffer from significant practical limitations. Techniques based on supervised training require extensive data collection and annotation (Kim et al., 2009, 2011; N´edellec et al., 2013; Segura Bedmar et al., 2013; Del´eger et al., 2016; Chaix et al., 2016), or a high degree of technical savviness in producing high quality data sets from distant supervision (Peng et al., 2017; Verga et al., 2017; Wang et al., 20"
2020.bionlp-1.3,W11-1802,0,0.060182,"Missing"
2020.bionlp-1.3,2020.lrec-1.267,0,0.225326,"Missing"
2020.bionlp-1.3,2020.nlpcovid19-acl.1,0,0.073216,"Missing"
2020.blackboxnlp-1.5,2020.emnlp-demos.6,0,0.0425743,"Missing"
2020.blackboxnlp-1.5,D19-1077,0,0.132039,"crosslingual component, and explicitly identify an empirical language-identity subspace within mBERT representations. 1 Introduction Multilingual-BERT (mBERT) is a version of BERT (Devlin et al., 2019), trained on the concatenation of Wikipedia in 104 different languages. Recent works show that it excels in zero-shot transfer between languages, for a variety of tasks (Pires et al., 2019; Muller et al., 2020), despite being trained with no parallel supervision. Previous work has mainly focused on what is needed for zero-shot transfer to work well (Muller et al., 2020; Karthikeyan et al., 2020; Wu and Dredze, 2019), and on characterizing the representations of mBERT (Singh et al., 2019). However, we still lack a proper understanding of this model. In this work we study (1) how much word-level translation information is recoverable by mBERT; and (2) how this information is stored. We focus on the representations of the last layer, and on the embedding matrix that is shared between the input and output layers – which are together responsible for token prediction. 45 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 45–56 c Online, November 20, 2020."
2020.blackboxnlp-1.5,D19-1250,0,0.0420918,"Missing"
2020.blackboxnlp-1.5,N18-2084,0,0.0535338,"Missing"
2020.blackboxnlp-1.5,P19-1493,0,0.0966396,"non-linear way, while some of it can also be recovered with purely linear tools. As part of our analysis, we test the hypothesis that mBERT learns representations which contain both a languageencoding component and an abstract, crosslingual component, and explicitly identify an empirical language-identity subspace within mBERT representations. 1 Introduction Multilingual-BERT (mBERT) is a version of BERT (Devlin et al., 2019), trained on the concatenation of Wikipedia in 104 different languages. Recent works show that it excels in zero-shot transfer between languages, for a variety of tasks (Pires et al., 2019; Muller et al., 2020), despite being trained with no parallel supervision. Previous work has mainly focused on what is needed for zero-shot transfer to work well (Muller et al., 2020; Karthikeyan et al., 2020; Wu and Dredze, 2019), and on characterizing the representations of mBERT (Singh et al., 2019). However, we still lack a proper understanding of this model. In this work we study (1) how much word-level translation information is recoverable by mBERT; and (2) how this information is stored. We focus on the representations of the last layer, and on the embedding matrix that is shared betw"
2020.blackboxnlp-1.5,2020.acl-main.647,1,0.824151,"s from 5000 sentences11 in 15 different languages, extract their respective representations (in context or simply output embeddings), and run INLP on those representations with the objective of identifying the language, for 20 iterations. We end up with 4 matrices: projection matrix on the null-space and on the rowspace for representations in context, and the same for output embeddings. • Removing the vlang component from word-incontext representations and from the output embeddings, to induce MLM prediction in other languages. Splitting the representations into components is done using INLP (Ravfogel et al., 2020), an algorithm for removing information from vector representations. 4.1 mBERT Decomposition by Nullspace Projections We formalize the decomposition objective defined earlier as finding two linear subspaces within the representation space, which contain languageindependent and language-identity features. The recently proposed Iterative Null-space Projection (INLP) method (Ravfogel et al., 2020) allows to 10 to the extent that language identity is indeed encoded in a linear subspace, and that INLP finds this subspace. 11 For the output embeddings, we exclude tokens that start with “##”, for the"
2020.blackboxnlp-1.5,D07-1043,0,0.189614,"Missing"
2020.blackboxnlp-1.9,D19-1275,0,0.0193007,"l., 2016) of the information-bottleneck principle 4 These differences in syntactic position are also of relevance to language modeling, as different positions may pose different restrictions on the words that can appear in them. 92 (Tishby et al., 1999) to extract word embeddings that are useful to the end task of parsing. While impressive, those works presuppose a specific syntactic structure (e.g. annotated parse tree) and use this linguistic signal to learn the probe in a supervised manner. This approach can introduce confounding between extracting information and learning it by the probe (Hewitt and Liang, 2019; Ravichander et al., 2020; Maudslay et al., 2020; Elazar et al., 2020). In contrast, we aim to expose the structural information encoded in the network in an unsupervised manner, without pre-supposing an existing syntactic annotation scheme. 3 To this end, we automatically generate the sets starting with known sentences and sampling variants from a language model (§3.1). Our sentence-set generation procedure ensures that words from the same set that share an index also share their structural function. We call such words corresponding. We now proceed to learn a function f to map contextualized"
2020.blackboxnlp-1.9,D19-1588,0,0.0170811,"licious. Human language1 is a complex system, involving an intricate interplay between meaning (semantics) and structural rules between words and phrases (syntax). Self-supervised neural sequence models for text trained with a language modeling objective, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTA (Liu et al., 2019b), were shown to produce representations that excel in recovering both structure-related information (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Goldberg, 2019) as well as in semantic information (Yang et al., 2019; Joshi et al., 2019). In this work, we study the problem of disentangling structure from semantics in neural language 4. John loves maple syrup. While (1) and (3) are different in content, they share a similar structure, the corresponding words in them, while unrelated in meaning,2 serve the same function. Similarly for sentences (2) and (4). In contrast, sentence (1) shares the phrase neural networks with sentence (2), and maple syrup is shared between (3) and (4).3 While the two occurrences of each phrase share the meaning, they are used in different structural (syntactic) configurations, serving different role"
2020.blackboxnlp-1.9,P18-1249,0,0.0428059,"Missing"
2020.blackboxnlp-1.9,2020.acl-main.659,0,0.019077,"4 These differences in syntactic position are also of relevance to language modeling, as different positions may pose different restrictions on the words that can appear in them. 92 (Tishby et al., 1999) to extract word embeddings that are useful to the end task of parsing. While impressive, those works presuppose a specific syntactic structure (e.g. annotated parse tree) and use this linguistic signal to learn the probe in a supervised manner. This approach can introduce confounding between extracting information and learning it by the probe (Hewitt and Liang, 2019; Ravichander et al., 2020; Maudslay et al., 2020; Elazar et al., 2020). In contrast, we aim to expose the structural information encoded in the network in an unsupervised manner, without pre-supposing an existing syntactic annotation scheme. 3 To this end, we automatically generate the sets starting with known sentences and sampling variants from a language model (§3.1). Our sentence-set generation procedure ensures that words from the same set that share an index also share their structural function. We call such words corresponding. We now proceed to learn a function f to map contextualized vectors of corresponding words (and the relation"
2020.blackboxnlp-1.9,N13-1090,0,0.0404316,"to the relation between “neural” and “interesting” in the first sentence: 3 3 1 1 pair(vmaple , vdelicious ) ≈ pair(vneural , vinteresting ). Operatively, we represent pairs of words (x, y) by the difference between their transformation f (x) − f (y), and aim to learn a function f that preserves: 3 3 1 1 f (vmaple )−f (vdelicious ) ≈ f (vneural )−f (vinteresting ). The choice to represent pairs this way was inspired by several works that demonstrated that nontrivial semantic and syntactic relations between uncontextualized word representations can be approximated by simple vector arithmetic (Mikolov et al., 2013a,b; Levy and Goldberg, 2014). To learn f , we start with groups of sentences such that the sentences within each group are known to share structure but differ in lexical semantics. We call the sentences in each group structurally equivalent. Figure 2 shows an example of two structurally equivalent sets. Acquiring such sets is challenging, especially if we do not assume a known syntactic formalism and cannot mine for sentences based on their observed tree structures. 3.1 Generating Structurally-similar Sentences In order to generate sentences that approximately share their structure, we sequen"
2020.blackboxnlp-1.9,D14-1162,0,0.0872319,"Missing"
2020.blackboxnlp-1.9,W18-5423,0,0.0232033,"ural networks are interesting. 2. I study neural networks. Introduction 3. Maple syrup is delicious. Human language1 is a complex system, involving an intricate interplay between meaning (semantics) and structural rules between words and phrases (syntax). Self-supervised neural sequence models for text trained with a language modeling objective, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTA (Liu et al., 2019b), were shown to produce representations that excel in recovering both structure-related information (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Goldberg, 2019) as well as in semantic information (Yang et al., 2019; Joshi et al., 2019). In this work, we study the problem of disentangling structure from semantics in neural language 4. John loves maple syrup. While (1) and (3) are different in content, they share a similar structure, the corresponding words in them, while unrelated in meaning,2 serve the same function. Similarly for sentences (2) and (4). In contrast, sentence (1) shares the phrase neural networks with sentence (2), and maple syrup is shared between (3) and (4).3 While the two occurrences of each phrase share the meani"
2020.blackboxnlp-1.9,N19-4013,0,0.0298411,". Maple syrup is delicious. Human language1 is a complex system, involving an intricate interplay between meaning (semantics) and structural rules between words and phrases (syntax). Self-supervised neural sequence models for text trained with a language modeling objective, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and RoBERTA (Liu et al., 2019b), were shown to produce representations that excel in recovering both structure-related information (Gulordava et al., 2018; van Schijndel and Linzen, 2018; Wilcox et al., 2018; Goldberg, 2019) as well as in semantic information (Yang et al., 2019; Joshi et al., 2019). In this work, we study the problem of disentangling structure from semantics in neural language 4. John loves maple syrup. While (1) and (3) are different in content, they share a similar structure, the corresponding words in them, while unrelated in meaning,2 serve the same function. Similarly for sentences (2) and (4). In contrast, sentence (1) shares the phrase neural networks with sentence (2), and maple syrup is shared between (3) and (4).3 While the two occurrences of each phrase share the meaning, they are used in different structural (syntactic) configurations, s"
2020.coling-main.211,P13-1035,0,0.0747652,"Missing"
2020.coling-main.211,D17-1278,0,0.151823,"that date, as the models were not exposed to these plotlines at pre-training. We manually extract key facts for each of these plots, and regard these fact-sets as test inputs, with the original stories now serving only as a reference (which will not be used in evaluation). 5.2 Training set In addition, we wish to derive a much larger dataset containing thousands of examples, to enable supervised fine-tuning. While manual facts extraction is not feasible, the facts format correlates nicely with the artifacts of OpenIE frameworks (Banko et al., 2007) and we therefore employ existing frameworks (Gashteovski et al., 2017) to automatically extract facts from the large Wikipedia-based plots corpus. As we desire a limited number of only 5 key facts per story, we take advantage of the SalIE framework (Ponza et al., 2018) that aims to rate saliency for extracted facts, and use the derived facts with the highest saliency scores as the key facts. Using these frameworks, described in Technical Details below, we derive a training data set of 17 thousand sets of key-facts and their correlating stories. The derived corpus has a ratio of 1/6 between the number of words in the key facts and the full plot, requiring the mod"
2020.coling-main.211,D16-1032,0,0.0305092,"in neural network story generation experiment with recurrent or convolutional sequenceto-sequence architectures, encountering difficulties in generating long text that stays on track. While 2 3 https://github.com/eyal-orbach/Facts2Story-data https://github.com/eyal-orbach/Facts2Story-XLNetPlanCloze 2330 addressing this challenge these works yield interesting mechanisms to represent the plotline of a story. Martin et al. (2018) represent a plot as a chain of events, learning to generate a sentence from each event, while Yao et al. (2018) alternatively build the plotline as a chain of keywords. Kiddon et al. (2016) avoid the need to represent each of the desired sentences by maintaining a checklist of required words and implementing a gating mechanism to insert these words and track which were used, demonstrating these abilities on cooking recipes. Zhai et al. (2019) develop this notion further by using events as the ingredients in the checklist, but also conditioning the generated text on the desired next event, concluding that their model still generates shorter stories with less event coverage then those produced by humans. Fan et al. (2018) collect a corpus of writing prompts and their appropriate s"
2020.emnlp-main.302,2020.emnlp-main.556,0,0.0530998,"Missing"
2020.emnlp-main.302,D17-1004,0,0.0979025,"Missing"
2020.emnlp-main.302,K17-1034,0,0.0268167,"ll negative instances (b). based on the type of arguments and the existence of a relation in the text, without verifying that the entities are indeed the arguments of the relation. 5 QA Models Perform Better The CRE dataset results indicate that RE-trained models systematically fail to link the provided relation arguments to the relation mention. We demonstrate that QA-trained models perform better in this respect. The QA models differ in both in their training data (SQuAD 2.0, (Rajpurkar et al., 2018)) and in their training objective (span-prediction, rather than classification). Inspired by Levy et al. (2017) we reduce RC instances into QA instances. We follow the reduction from Cohen et al. (2020) between QA and binary relation classification which works by forming two questions for each relation instance, one for each argument. For example, for the relation instance pair (Mark, FB, founded) we ask “Who founded FB?” and “what did Mark found?”.8 8 The question templates for each relation are defined manually. The full set, and additional details on the reduction method, are given in Appendix D. 3705 If the QA model answers either one of the questions with the correct span, we return 1 (relation ho"
2020.emnlp-main.302,N19-1225,0,0.0170951,"wd-workers lead to frequent annotation errors. We observed the same phenomena also with non-crowd workers. 3704 6 Model RC-SpanBERT RC-BERT RC-KnowBERT RC-RoBERTa P R F1 70.8 67.8 71.6 70.17 70.9 67.2 71.4 72.36 70.8 67.5 71.5 71.25 Table 1: Test results on TACRED. 4 Evaluating RE Models By construction, the CRE dataset includes many instances that fail the seed RC model, which is based on fine-tuned SpanBERT (Joshi et al., 2019). To verify that the behavior is consistent across models, we evaluate also RC models fine-tuned over other state-of-the-art LMs: BERT (Devlin et al., 2019), RoBERTA (Liu et al., 2019b) and KnowBERT (Peters et al., 2019). When evaluated on TACRED test set (Table 1), these models achieve SOTA scores. We evaluate model’s results on the CRE dataset in terms of accuracy. We also report positive accuracy (Acc+ ) (the accuracy on instances for which the relation hold; models that make use of the heuristic are expected to score high here) and likewise negative accuracy (Acc− ) (accuracy on instances in which the relation does not hold; models using the heuristic are expected to score low here). The models are consistently more accurate on the positive set then on the negative set"
2020.emnlp-main.302,2021.ccl-1.108,0,0.13653,"Missing"
2020.insights-1.17,2020.acl-main.774,0,0.0776033,"Missing"
2020.insights-1.17,D19-1606,0,0.0279413,"t (and thus the context suggests an implied event). The second modeling follows the NLI scheme, a standard approach for evaluating language understanding. The ENT, NEU and CON labels refer to the entail, neutral and contradict labels accordingly. Introduction Crowdsourcing has become extremely popular in recent years for annotating datasets. Many works use frameworks like Amazon Mechanical Turk (AMT) by converting complex linguistic tasks into easy-to-grasp presentations which make it possible to crowdsource linguistically-annotated data at scale (Bowman et al., 2015; FitzGerald et al., 2018; Dasigi et al., 2019; Wolfson et al., 2020). In this work, we attempt to use existing methodologies for crowdsourcing linguistic annotations in order to collect annotations for complement coercion (Pustejovsky, 1991, 1995), a phenomenon involving an implied action triggered by an eventselecting verb. Specifically, certain verb classes require an event-denoting complement, as in: “I started reading a book”, “I finished eating the cake”, etc. However, such event-denoting complements might remain implicit, not appearing in the surface form. Consider for instance, the sentence “I started a new book.” Here the event t"
2020.insights-1.17,P18-2103,1,0.842896,"ese sentences are used as the hypotheses. To construct the premises, we remove the dependent verb (e.g. ‘read’), as well as all the words between the anchor and the dependent verb (e.g. ‘to’ in the infinitive form: “to read”). Additional examples are provided in Appendix D. Note that this procedure sometimes generates ungrammatical or implausible sentences, which are flagged by the annotators. Crowdsourcing Procedure We follow the standard procedure of collecting NLI data with crowdsourcing and collect annotations from Amazon Mechanical Turk (AMT). Specifically, we follow the instruction from Glockner et al. (2018), which involves three questions: 1. Do the sentences describe the same event? 2. Does the new sentence add new information to the original sentence? 3. Is the new sentence incorrect/ungrammatical? We discard any example which at least one worker marked as incorrect/ungrammatical. If the answer 3 We follow Bowman et al. (2015), who modeled entailment based on event coreference. 108 4 These are frequent verbs that often appear in complement coercion constructions (McGregor et al., 2017). 5 We use spaCy’s parser (Honnibal and Johnson, 2015; Honnibal and Montani, 2017). to the first question was"
2020.insights-1.17,S10-1005,0,0.0314834,"on, termination, or continuation of an activity” (Levin, 1993) — such as: ‘start’, ‘begin’, ‘continue’ and ‘finish’ (McGregor et al., 2017). This set of verbs is the focus of our work. Note however, that such verbs may appear in similar constructions that do not imply any covert action or event. For instance, in the following sentence: 3. I started a new company. Here, the verb ‘start’ is used as an entity-selecting (and not event-selecting) verb, a synonym of ‘found’ or ‘establish’. See more examples of similar non-coercive constructions in Appendix B. Annotated data for complement coercion (Pustejovsky et al., 2010) was collected in the past, based on a tailor-made annotation methodology (Pustejovsky et al., 2009), consisting of a multi-step process that includes word-sense disambiguation by experts. The annotation focused on coercion detection (as well as labeling the arguments type) and did not involve identifying the implied action. Here, we aim to collect complement coercion data via non-expert annotation, at scale, to test whether models can recover the implicit events and resolve the emerging ambiguities. Crowdsourcing NLI NLI, originally framed as Recognizing Textual Entailment (RTE), has become a"
2020.insights-1.17,L18-1058,0,0.0281906,"Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences containing possibly-coercive verbs, we wish to determine for each verb if it entails an implicit event, and if so, to figure out what the event is. This direct task-definition approach is reminiscent of studies that collected annotated data for other missing elements phenomena, such as Verb-Phrase Ellipsis (Bos and Spenader, 2011), Numeric Fused-Heads (Elazar and Goldberg, 2019), Bridging (Roesiger, 2018; Hou et al., 2018) and Sluicing (Hansen and Søgaard, 2020). However, when attempting to crowdsource and label complement coercion instances, we reach very low agreement scores in the first step: determining whether there is an implied event or not. We discuss this experiment in greater detail in Appendix C. 3.2 NLI for Complement Coercion In light of the low agreements on explicit modeling of the task of complement coercion, we turn to a different crowdsourcing approach which was proven successful for many linguistic phenomena – using NLI as discussed above (§2). NLI was used to collect data"
2020.insights-1.17,2020.acl-main.626,0,0.0249531,", we also reach an understanding that the datasets at hand do not reflect the full capacity of language, and specific linguistic phenomena, which may posses specific challenges, are lost in 7 Due to large scale annotations, ‘marginal’ phenomena might be ignored to keep the instructions clear and concise. the crowds. Some phenomena turn out to be more complex, and require specific solutions. In this work we show that, like we do with algorithmic solutions we need to reconsider the data collection process. We hold that data collection for these phenomena also require training of the annotators (Roit et al., 2020; Pyatkin et al., 2020), whether experts or crowdsourcing workers, and may also require coming up with novel annotation protocols. Another potential solution is to use deliberation between the workers as a mean to improve agreement (Schaekermann et al., 2018). With respect to the disagreements we observed, a deliberation between workers would allow them to share the construals each individual had imagined, thus reaching a consensus on the labels. It would also serve as a training for recovering more construals, allowing them to better identify the neutral cases. 5 Conclusions In this work, we"
2020.insights-1.17,D19-1228,0,0.0139234,"specific focus on lexical and syntactic variability rather than delicate logical issues, while dismissing cases of disagreements or ambiguity. Bowman et al. (2015); Williams et al. (2018) then scaled up the task and crowdsourced large-scale NLI datasets. In contrast to Dagan et al. (2005), the task definitions were short and loose, relying on the annotators’ common sense understanding. Many works since have been using the NLI framework and the crowdsourcing procedure associated with it to test models for different language phenomena (Marelli et al., 2014; Lai et al., 2017; Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences containing possibly-coercive verbs, we wish to determine for each verb if it entails an implicit event, and if so, to figure out what the event is. This direct task-definition approach is reminiscent of studies that collected annotated data for other missing elements phenomena, such as Verb-Phrase Ellipsis (Bos and Spenader, 2011), Numeric Fused-Heads (Elazar and Goldberg, 2019), Bridging (Roesiger, 2018; Hou et al., 2018) and Slui"
2020.insights-1.17,2020.acl-main.462,0,0.0375187,"ing his case.” ENT NEU CON 6. “We start the interviews later today.” ; “We start shooting the interviews later today.” NEU CON CON Example 4 was labeled by all three annotators as entail. However, annotators were in disagreement on examples 5, 6. Example 5 was annotated with all three possible labels (entail, contradict and neutral). Indeed, different readings of this phrase are possible — more formally, different readers construe the meaning of the utterance differently; “[Construal] is a dynamic process of meaning construction, in which speakers and hearers encode and decode, respectively” (Trott et al., 2020). An annotator who understands the word ‘case’ as a legal case, will choose entail, while an annotator 6 We stopped at 76 examples since we did not see fit to annotate more data with the low agreements we obtained. who interprets ‘case’ as a bag and imagines a different background story (for example, a young man packing a brief-case), will choose contradict. Finally, an annotator who thinks of both scenarios will choose neutral, which can be argued to be the correct answer. However, we find that for a human hearer, holding both scenarios in mind at the same time is hard, which we attribute to"
2020.insights-1.17,2020.tacl-1.25,0,0.0187519,"his format, along with the different labels we employ, are shown in Table 2. Example Label I started a book I bought last week. ; I started reading a book I bought last week. ENT I started a book. ; I started reading a book. I started eating a book. NEU CON Table 2: Examples for NLI pairs with a complement coercion structure. The ENT, NEU and CON labels refers to entail, neutral and contradict accordingly. Corpus Candidates In order to keep the task simple, we avoid complexities of lexical, semantic and grammatical differences. Each example is composed of a minimal-pair (Kaushik et al., 2019; Warstadt et al., 2020; Gardner et al., 2020) consisting of two sentences; one as the premise and the other as the hypothesis. We construct minimal pairs as follows: First, we extract dependencyparsed sentences from the Book Corpus (Zhu et al., 2015) containing the lemma of one of the verbs: ‘start’, ‘begin’, ‘continue’ and ‘finish’.4 Then, we keep sentences where the anchor verb is attached to another verb with an ‘xcomp’ dependency5 (e.g. ‘started’ in “started reading”). These sentences are used as the hypotheses. To construct the premises, we remove the dependent verb (e.g. ‘read’), as well as all the words betw"
2020.insights-1.17,I17-1100,0,0.0542104,"Missing"
2020.insights-1.17,N18-1101,0,0.0348949,"t coercion data via non-expert annotation, at scale, to test whether models can recover the implicit events and resolve the emerging ambiguities. Crowdsourcing NLI NLI, originally framed as Recognizing Textual Entailment (RTE), has become a standard framework for testing reasoning capabilities of models. It originated from the work by Dagan et al. (2005), where a small dataset was curated by experts using precise guidelines with a specific focus on lexical and syntactic variability rather than delicate logical issues, while dismissing cases of disagreements or ambiguity. Bowman et al. (2015); Williams et al. (2018) then scaled up the task and crowdsourced large-scale NLI datasets. In contrast to Dagan et al. (2005), the task definitions were short and loose, relying on the annotators’ common sense understanding. Many works since have been using the NLI framework and the crowdsourcing procedure associated with it to test models for different language phenomena (Marelli et al., 2014; Lai et al., 2017; Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences c"
2020.insights-1.17,2020.acl-main.543,0,0.0501549,"al and syntactic variability rather than delicate logical issues, while dismissing cases of disagreements or ambiguity. Bowman et al. (2015); Williams et al. (2018) then scaled up the task and crowdsourced large-scale NLI datasets. In contrast to Dagan et al. (2005), the task definitions were short and loose, relying on the annotators’ common sense understanding. Many works since have been using the NLI framework and the crowdsourcing procedure associated with it to test models for different language phenomena (Marelli et al., 2014; Lai et al., 2017; Naik et al., 2018; Ross and Pavlick, 2019; Yanaka et al., 2020). 107 3 3.1 Copmlement Coercion Crowdsourcing Explicit Completion Attempt We begin by directly modeling the phenomenon. For a set of sentences containing possibly-coercive verbs, we wish to determine for each verb if it entails an implicit event, and if so, to figure out what the event is. This direct task-definition approach is reminiscent of studies that collected annotated data for other missing elements phenomena, such as Verb-Phrase Ellipsis (Bos and Spenader, 2011), Numeric Fused-Heads (Elazar and Goldberg, 2019), Bridging (Roesiger, 2018; Hou et al., 2018) and Sluicing (Hansen and Søgaa"
2020.tacl-1.13,P13-1023,0,0.0507346,"Missing"
2020.tacl-1.13,N19-1027,0,0.0194134,"the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed to use words from a lexicon Lx , which contains: (a) words appearing in the question (or their automatically computed inflections), (b) words from a small pre-defined list of 66 function word such as, ‘if’, ‘on’, ‘for each’, or (c) reference tokens that refer to the results"
2020.tacl-1.13,N16-1181,0,0.249915,"Missing"
2020.tacl-1.13,P15-1127,0,0.0288983,"l details in §7.2). COMBINED and BREAKRC were compared to COMBINEDR and BREAKRCR , which use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, bridging between natural la"
2020.tacl-1.13,W10-2903,0,0.0397036,"models should improve performance and generalization in tasks that require multi-step reasoning or that do not have access to substantial amounts of data. In this work we propose question understanding as a standalone language understanding task. We introduce a formalism for representing the meaning of questions that relies on question decomposition, and is agnostic to the information source. Our formalism, Question Decomposition Meaning Representation (QDMR), is inspired by database query languages (SQL; SPARQL), and by semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Clarke et al., 2010), in which questions are given full meaning representations. We express complex questions via simple (‘‘atomic’’) questions that can be executed in sequence to answer the original question. Each atomic question can be mapped into a small set of formal operations, where each operation either selects a set of entities, retrieves information about Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes th"
2020.tacl-1.13,H94-1010,0,0.464354,"Missing"
2020.tacl-1.13,N19-1423,0,0.0339406,"o-logical forms, which can be used as 190 5 QDMR for Open-domain QA A natural setup for QDMR is in answering complex questions that require multiple reasoning steps. We compare models that exploit question decompositions to baselines that do not. We use the open-domain QA (‘‘full-wiki"") setting of the HOTPOTQA dataset (Yang et al., 2018): Given a question, the QA model retrieves the relevant Wikipedia paragraphs and answers the question using these paragraphs. 5.1 Experimental Setup We compare BREAKRC, a model that utilizes question decomposition to BERTQA, a standard QA model, based on BERT (Devlin et al., 2019), and present COMBINED, an approach that enjoys the benefits of both models. BREAKRC Algorithm 1 describes the BREAKRC model, which uses high-level QDMR structures for answering open-domain multi-hop questions. We assume access to an Information Retrieval (IR) model and an RC model, and denote by ANSWER(·) a function that takes a question as input, runs the IR model to obtain paragraphs, and then feeds those paragraphs as context for an RC model that returns a distribution over answers. Given an input QDMR, s = hs1 , ..., sn i, iterate over s step-by-step and perform the following. First, we e"
2020.tacl-1.13,W13-2322,0,0.0713232,"x. Domain-independent intermediate representations for semantic parsers were proposed by Kwiatkowski et al. (2013) and Reddy et al. (2016). As there is no consensus on the ideal meaning representation for semantic parsing, representations are often chosen based on the particular execution setup: SQL is used for relational databases (Yu et al., 2018), SPARQL for graph KBs (Yih et al., 2016), while other ad-hoc languages are used based on the task at hand. We frame QDMR as an easy-to-annotate formalism that can be potentially converted to other representations, depending on the task. Last, AMR (Banarescu et al., 2013) is a meaning representation for sentences. Instead of representing general language, QDMR represents questions, which are important for QA systems, and for probing models for reasoning. questions from 10 datasets and 3 modalities (DB, images, text). We presented the utility of QDMR for both open-domain question answering and semantic parsing, and constructed a QDMR parser with reasonable performance. QDMR proposes a promising direction for modeling question understanding, which we believe will be useful for multiple tasks in which reasoning is probed through questions. Acknowledgments This wo"
2020.tacl-1.13,N19-1246,1,0.922364,"it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query lan"
2020.tacl-1.13,P17-1171,0,0.0297332,"the reference to the previous step in si with its already computed answer, and then run ANSWER(·). For FILTER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraph"
2020.tacl-1.13,N19-1405,0,0.0304319,"ems from the design of QDMR as a domain-agnostic meaning representation (§2). QDMR abstracts away from a concrete KB schema by assuming an underlying ‘‘idealized’’ KB, which contains all of its arguments. Figure 6: Examples of PROJECT and COMPARISON questions in HOTPOTQA (high-level QDMR). Model BERTQA BREAKRCP BREAKRCG PROJECT COMPARISON EM F1 IR EM F1 IR 22.8 31.0 31.6 42.9 51.7 75.8 25.4 33.7 52.9 34.7 50.4 68.9 32.2 41.9 59.8 44.5 57.6 78.0 Table 7: Results on PROJECT and COMPARISON questions from HOTPOTQA development set. to reasoning shortcuts, i.e. they necessitate multistep reasoning (Chen and Durrett, 2019; Jiang and Bansal, 2019; Min et al., 2019a). In Table 7 we report BREAKRC results on these question types, where it notably outperforms BERTQA. Ablations In BREAKRC, multiple IR queries are issued, one at each step. To examine whether these multiple queries were the cause for performance gains, we built IR-NP, a model that issues multiple IR queries, one for each noun phrase in the question. Similar to COMBINED, the question and union of retrieved paragraphs are given as input to BERTQA. We observe that COMBINED substantially outperforms IR-NP, indicating that the structure of QDMR, rather th"
2020.tacl-1.13,P16-1154,0,0.0275769,"authors generate sequences of simple questions which crowdworkers paraphrase into a compositional question. Questions in BREAK are composed by humans, and are then decomposed to QDMR. Table 8: The decomposition rules of RULEBASED. Rules are based on dependency labels, part-ofspeech tags and coreference edges. Text fragments used for decomposition are in boldface. • S2SDYNAMIC: SEQ2SEQ with a dynamic output vocabulary restricted to the closed set of tokens Lx available to crowd-workers (see §3). • COPYNET: SEQ2SEQ with an added copy mechanism that allows copying tokens from the input sequence (Gu et al., 2016). 7.3 Results Table 9 presents model performance on BREAK. Neural models outperform the RULEBASED baseline and perform reasonably well, with COPYNET obtaining the best scores across all metrics. This can be attributed to most of the tokens in a QDMR parse being copied from the original question. Semantic Formalism Annotation Labeling corpora with a semantic formalism has often been reserved for expert annotators (Dahl et al., 1994; Zelle and Mooney, 1996; Abend and Rappoport, 2013; Yu et al., 2018). Recent work has focused on cheaply eliciting quality annotations from nonexperts through crowds"
2020.tacl-1.13,P19-1262,0,0.0376279,"DMR as a domain-agnostic meaning representation (§2). QDMR abstracts away from a concrete KB schema by assuming an underlying ‘‘idealized’’ KB, which contains all of its arguments. Figure 6: Examples of PROJECT and COMPARISON questions in HOTPOTQA (high-level QDMR). Model BERTQA BREAKRCP BREAKRCG PROJECT COMPARISON EM F1 IR EM F1 IR 22.8 31.0 31.6 42.9 51.7 75.8 25.4 33.7 52.9 34.7 50.4 68.9 32.2 41.9 59.8 44.5 57.6 78.0 Table 7: Results on PROJECT and COMPARISON questions from HOTPOTQA development set. to reasoning shortcuts, i.e. they necessitate multistep reasoning (Chen and Durrett, 2019; Jiang and Bansal, 2019; Min et al., 2019a). In Table 7 we report BREAKRC results on these question types, where it notably outperforms BERTQA. Ablations In BREAKRC, multiple IR queries are issued, one at each step. To examine whether these multiple queries were the cause for performance gains, we built IR-NP, a model that issues multiple IR queries, one for each noun phrase in the question. Similar to COMBINED, the question and union of retrieved paragraphs are given as input to BERTQA. We observe that COMBINED substantially outperforms IR-NP, indicating that the structure of QDMR, rather than multiple IR queries,"
2020.tacl-1.13,P19-1444,0,0.0312823,"use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, bridging between natural language and full logical forms. 7 QDMR Parsing We now present evaluation metrics and mod"
2020.tacl-1.13,D18-1239,0,0.0422473,"Missing"
2020.tacl-1.13,D13-1161,0,0.0328552,"ency tree of the question (full details in §7.2). COMBINED and BREAKRC were compared to COMBINEDR and BREAKRCR , which use the rulebased decompositions. We observe that QDMR lead to substantially higher performance when compared to the rule-based decompositions. 7 Issuing an IR query over each ‘‘content word’’ in the question, instead of each noun phrase, led to poor results. 192 Thus, QDMR can be viewed as an intermediate representation between a natural language question and an executable query. Such intermediate representations have already been discussed in prior work on semantic parsing. Kwiatkowski et al. (2013) and Choi et al. (2015) used underspecified logical forms as an intermediate representation. Guo et al. (2019) proposed a two-stage approach, separating between learning an intermediate text-to-SQL representation and the actual mapping to schema items. Works in the database community have particularly targeted the mapping of intermediate query representations into DB grounded queries, using schema mapping and join path inference (Androutsopoulos et al., 1995; Li et al., 2014; Baik et al., 2019). We argue that QDMR can be used as an easy-to-annotate representation in such semantic parsers, brid"
2020.tacl-1.13,Q19-1026,0,0.0790769,"Missing"
2020.tacl-1.13,D16-1258,0,0.062318,"Missing"
2020.tacl-1.13,P17-1089,0,0.0546298,"Missing"
2020.tacl-1.13,J13-2005,0,0.0690178,"Missing"
2020.tacl-1.13,N18-2089,0,0.0398159,"Missing"
2020.tacl-1.13,P17-1167,0,0.0532124,"Missing"
2020.tacl-1.13,P19-1416,1,0.939891,"ER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, tha"
2020.tacl-1.13,P19-1613,0,0.228693,"ER steps,6 we use a simple rule to extract a ‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, tha"
2020.tacl-1.13,Q18-1021,0,0.0375168,"nverted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1"
2020.tacl-1.13,P15-1142,0,0.0707424,"n semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1974), the same intuition can be applied to other modalities, such as images and"
2020.tacl-1.13,Q16-1029,0,0.0219436,"sition can be further decomposed. To measure such variations, we introduce two types of evaluation metrics. Sequence-based metrics treat the decomposition as a sequence of tokens, applying standard text generation metrics. As such metrics ignore the QDMR graph structure, we also use graph-based metrics that compare the predicted graph Gˆs to the gold QDMR graph Gs (see §2). Sequence-based scores, where higher values are better, are denoted by ⇑. Graph-based scores, where lower values are better, are denoted by ⇓. • Exact Match ⇑: Measures exact match between s and ˆs, either 0 or 1. • SARI ⇑ (Xu et al., 2016): SARI is commonly used in tasks such as text simplification. Given s, we consider the sets of added, deleted, and kept n-grams when mapping the question x to s. We compute these three sets for both s and ˆs using the standard of up to 4-grams, then average (a) the F1 for added n-grams between s and ˆs, (b) the F1 for kept n-grams, and (c) the precision for the deleted n-grams. cost. GED computes the minimal-cost graph edit path required for transitioning from Gs to Gˆs (and vice versa), normalized by max(|Gs |, |Gˆs |). Operation costs are 1 for insertion and deletion of nodes and edges. The"
2020.tacl-1.13,H90-1020,0,0.415169,"81 7,982 32,164 13,935 29,680 13,517 11,214 5,520 34,689 96,567 23,066 2,988, 2,991high 10,230, 10,262high 10,575high 83,978 Figure 4: User interface for decomposing a complex question that uses a closed lexicon of tokens. Table 3: The QA datasets in BREAK. Lists the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed t"
2020.tacl-1.13,D18-1259,0,0.13256,"Missing"
2020.tacl-1.13,D19-1261,0,0.0255197,"to BREAKRC, that is trained on SQUAD, BERTQA is trained on the target dataset (HOTPOTQA), giving it an advantage over BREAKRC. A COMBINED Approach Last, we present an approach that combines the strengths of BREAKRC and BERTQA. In this approach, we use the QDMR decomposition to improve retrieval only. Given a question x and its QDMR s, we run BREAKRC on s, but in addition to storing answers, we also store all the paragraphs retrieved by the IR model. We then run BERTQA on the question x and the top-10 paragraphs retrieved by BREAKRC, sorted by their IR ranking. This approach resembles that of Qi et al. (2019). The advantage of COMBINED is that we do not need to develop an answering procedure for each QDMR operator separately, which involves dif6 INTERSECTION steps are handled in a manner similar to FILTER, but we omit the exact description for brevity. 191 Model BERTQA BREAKRCP BREAKRCG COMBINEDP COMBINEDG IR-NP BREAKRCR COMBINEDR EM 33.6 28.8 34.6 38.3 41.2 31.7 18.9 32.7 HOTPOTQA F1 43.3 37.7 44.6 49.3 52.4 41.2 26.5 42.6 IR 46.3 52.5 59.2 52.5 59.2 40.8 40.3 40.3 Table 6: Open-domain QA results on HOTPOTQA. ferent discrete operations such as comparison and intersection. Instead, we use BREAKRC"
2020.tacl-1.13,P16-2033,0,0.0282538,"s = hs1 , . . . , sn i do 4: op ← OPTYPE(si ) 5: ref s ← REFERENCEDSTEPS(si ) 6: if op is SELECT then 7: ans ← ANSWER(si ) 8: else if op is FILTER then 9: sˆi ← EXTRACTQUESTION(si ) 10: anstmp ← ANSWER(ˆ si ) 11: ans ← INTERSECT(anstmp , ansrs[ref s[0]]) 12: else if op is COMPARISON then 13: ans ← COMPARESTEPS(ref s,s) 14: else ⊲ op is PROJECT 15: sˆi ← SUBSTITUTEREF (si , ansrs[ref s[0]]) 16: ans ← ANSWER(ˆ si ) 17: ansrs[i] ← ans 18: return ansrs[n] Figure 5: Examples and justifications of expert judgment on collected QDMRs in BREAK. a cheap intermediate representation for semantic parsers (Yih et al., 2016), further discussed in §6. QDMR operator (f i ) and its arguments from each step (si ). To infer these formal representations, we developed an algorithm that goes over the QDMR structure step-by-step, and for each step si , uses a set of predefined templates to identify f i and its arguments, expressed in si . This results in an execution graph (Figure 2), where the execution result of a parent node serves as input to its child. Figure 1 presents three QDMR decompositions along with the formal graphs output by our algorithm (lower box). Each node lists its operator (e.g., GROUP), its constant"
2020.tacl-1.13,D16-1264,0,0.0946606,"‘‘normalized question’’, sˆi from si and get an intermediate answer anstmp with ANSWER(ˆ si ). We then ‘‘intersect’’ anstmp with the referenced answer by multiplying the probabilities provided by the RC model and normalizing. For COMPARISON steps, we compare, with a discrete operation, the numbers returned by the referenced steps. The final answer is the highest probability answer of step sn . As our IR model we use bigram TF-IDF, proposed by Chen et al. (2017). Because the RC model is run on single-hop questions, we use the BERTbased RC model from Min et al. (2019b), trained solely on SQuAD (Rajpurkar et al., 2016). BERTQA Baseline As BREAKRC exploits question decompositions, we compare it with a model that does not. BERTQA receives as input the original natural language question, x. It uses the same IR model as BREAKRC to retrieve paragraphs for x. For a fair comparison, we set its number of retrieved paragraphs such that it is identical to BREAKRC (namely, 10 paragraphs for each QDMR step that involves IR). Similar to BREAKRC, retrieved paragraphs are fed to a pretrained BERT-based RC model (Min et al., 2019b) to answer x. In contrast to BREAKRC, that is trained on SQUAD, BERTQA is trained on the targ"
2020.tacl-1.13,D18-1425,0,0.0326117,"567 23,066 2,988, 2,991high 10,230, 10,262high 10,575high 83,978 Figure 4: User interface for decomposing a complex question that uses a closed lexicon of tokens. Table 3: The QA datasets in BREAK. Lists the number of examples in the original dataset and in BREAK. Numbers of high-level QDMRs are denoted by high. Question Collection Questions in BREAK were randomly sampled from ten QA datasets over the following tasks (Table 3): • Semantic Parsing: Mapping natural language utterances into formal queries, to be executed on a target KB (Price, 1990; Zelle and Mooney, 1996; Li and Jagadish, 2014; Yu et al., 2018). • Reading Comprehension (RC): Questions that require understanding of a text passage by reasoning over multiple sentences (Talmor and Berant, 2018; Yang et al., 2018; Dua et al., 2019; Abujabal et al., 2019). • Visual Question Answering (VQA): Questions over images that require both visual and numerical reasoning skills (Johnson et al., 2017; Suhr et al., 2019). All questions collected were composed by human annotators.3 HOTPOTQA questions were all sampled from the hard split of the dataset. steps, where they are only allowed to use words from a lexicon Lx , which contains: (a) words appeari"
2020.tacl-1.13,Q16-1010,0,0.0640017,"Missing"
2020.tacl-1.13,N18-1059,1,0.945937,"SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use BREAK to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. 1 Introduction Recently, increasing work has been devoted to models that can reason and integrate information from multiple parts of an input. This includes reasoning over images (Antol et al., 2015; Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019), paragraphs (Dua et al., 2019), documents (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018), tables (Pasupat and Liang, 2015), and more. Question answering (QA) is commonly 183 Transactions of the Association for Computational Linguistics, vol. 8, pp. 183–198, 2020. https://doi.org/10.1162/tacl a 00309 Action Editor: Mihai Surdeanu. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. their attributes, or aggregates information over entities. While this has been formalized in knowledge-base (KB) query languages (Chamberlin and Boyce, 1974), the same intuition"
2020.tacl-1.13,P11-1060,0,\N,Missing
2020.tacl-1.13,W18-2501,1,\N,Missing
2020.tacl-1.13,P19-1644,0,\N,Missing
2020.tacl-1.48,P17-1152,0,0.0900964,"Missing"
2020.tacl-1.48,N19-1423,0,0.108827,"Missing"
2020.tacl-1.48,Q19-1030,1,0.843319,"Missing"
2020.tacl-1.48,P19-1388,1,0.88036,"Missing"
2020.tacl-1.48,D19-1275,0,0.0713234,"one verify whether pretrained representations hold information that is useful for a particular task? Past work mostly resorted to fixing the representations and finetuning a simple, often linear, randomly initialized probe, to determine whether the representations hold relevant information (Ettinger et al., 2016; Adi et al., 2016; Belinkov and Glass, 2019; Hewitt and Manning, 2019; Wallace et al., 2019; Rozen et al., 2019; Peters et al., 2018b; Warstadt et al., 2019). However, it is difficult to determine whether success is due to the pre-trained representations or due to fine-tuning itself (Hewitt and Liang, 2019). To handle this challenge, we include multiple controls that improve our understanding of the results. Our ‘‘purest’’ setup is zero-shot: We cast tasks in the masked LM format, and use a pre-trained LM without any fine-tuning. For example, given Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such"
2020.tacl-1.48,Q16-1037,1,0.865635,"Missing"
2020.tacl-1.48,2020.acl-main.441,0,0.0620945,"ins FOODTYPE.’’ the LM always predicts ‘‘sometimes’’. Overall, we find models do not perform well. Reporting bias (Gordon and Van Durme, 2013) may play a role in the inability to correctly determine that ‘‘A rhinoceros NEVER has fur.’’ Interestingly, behavioral research conducted on blind humans shows they exhibit a similar bias (Kim et al., 2019). 4.3 Do LMs Capture Negation? Ideally, the presence of the word ‘‘not’’ should affect the prediction of a masked token. However, Several recent works have shown that LMs do not take into account the presence of negation in sentences (Ettinger, 2019; Nie et al., 2020; Kassner and Sch¨utze, 2020). Here, we add to this literature, by probing whether LMs can properly use negation in the context of synonyms vs. antonyms. 4.4 Can LMs handle conjunctions of facts? We present two probes where a model should understand the reasoning expressed by the word and. Property conjunction CONCEPTNET is a Knowledge-Base that describes the properties of millions of concepts through its (subject, 751 Model RoBERTa-L BERT-WWM BERT-L BERT-B RoBERTa-B Baseline LEARNCURVE LANGSENSE WS MAX pert nolang 49 46 48 47 40 39 87 80 75 71 57 49 2 0 2 2 0 0 4 1 5 1 0 0 Table 8: Results fo"
2020.tacl-1.48,K19-1019,0,0.0497465,"g the sizes of different objects. Understanding what is missing from current LMs may help design datasets and objectives that will endow models with the missing capabilities. However, how does one verify whether pretrained representations hold information that is useful for a particular task? Past work mostly resorted to fixing the representations and finetuning a simple, often linear, randomly initialized probe, to determine whether the representations hold relevant information (Ettinger et al., 2016; Adi et al., 2016; Belinkov and Glass, 2019; Hewitt and Manning, 2019; Wallace et al., 2019; Rozen et al., 2019; Peters et al., 2018b; Warstadt et al., 2019). However, it is difficult to determine whether success is due to the pre-trained representations or due to fine-tuning itself (Hewitt and Liang, 2019). To handle this challenge, we include multiple controls that improve our understanding of the results. Our ‘‘purest’’ setup is zero-shot: We cast tasks in the masked LM format, and use a pre-trained LM without any fine-tuning. For example, given Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to unde"
2020.tacl-1.48,N19-1421,1,0.880809,"M) and multichoice question answering (MC-QA). The default setup is MC-MLM, used for tasks where the answer set is small, consistent across the different questions, and each answer appears as a single item in the word-piece vocabulary.2 The MC-QA setup is used when the answer set substantially varies between questions, and many of the answers have more than one word piece. MC-QA Constructing a MC-MLM probe limits the answer candidates to a single token from the word-piece vocabulary. To relax this we use in two tasks the standard setup for answering multichoice questions with pre-trained LMs (Talmor et al., 2019; Mihaylov et al., 2018). Given a question q and candidate answers a1 , . . . , aK , we compute for each candidate answer ak representations h(k) from the input tokens ‘‘[CLS] q [SEP] ak [SEP]’’. Then the probability over answers is obtained using the multichoice QA head: 2 Vocabularies of LMs such as BERT and ROBERTA contain word-pieces, which are sub-word units that are frequent in the training corpus. For details see Sennrich et al. (2016). (k ) l(k) = F FQA (h1 ), p = softmax(l(1) , . . . , l(K ) ), 745 where F FQA is a 1-hidden layer MLP that is run over the [CLS] (first) token of an answ"
2020.tacl-1.48,P19-1452,0,0.0785555,"ion and Learning-curve Metrics Learning curves are informative, but inspecting many learning curves can be difficult. Thus, we summarize them using two aggregate statistics. We report: (a) MAX, that is, the maximal accuracy on the learning curve, used to estimate how well the model can handle the task given the limited amount of examples. (b) The metric WS, which is a weighted average of accuracies across the learning curve, where higher weights are given to points where N is small.3 WS is related to the area under the accuracy curve, and to the online code metric, proposed by Yogatama et al. (2019) and Blier and Ollivier (2018). The linearly decreasing weights emphasizes our focus on performance given little training data, as it highlights what was encoded by the model before fine-tuning. 3 We use the decreasing weights W = (0.23, 0.2, 0.17, 0.14, 0.11, 0.08, 0.07). 747 For AGE-COMPARE, the solid lines in Figure 2B illustrate the learning curves of ROBERTA-L and BERT-WWM, and Table 2 shows the aggregate statistics. We fine-tune the model by replacing AGE-1 and AGE-2 with values between 43 and 120, but test with values between 15 and 38, to guarantee that the model generalizes to values"
2021.acl-demo.25,P13-4027,0,0.0249226,"ypertension, Artial fibrillation, AF, Diabetes, Obesity while less frequent terms include VZV reactivation and palmitic acid. By modifying the query such that stroke is also marked as a capture slot: somethingARG1 is a risk factor for strokeARG2 one could easily obtain a table of risk factors for various conditions. 4 Potentially with additional restrictions such as the occurrence of other words, phrases or patterns in the document 5 This mode of operation is facilitated also by, e.g., the open-source toolkit Odinson (Valenzuela-Esc´arcega et al., 2020), and similar workflows are discussed by Akbik et al. (2013); Hoffmann et al. (2015). 6 In this paper, we avoid the exact SPIKE syntax, and use underlines to indicate named capture slots, and bolded words to indicate exact matches. The corresponding SPIKE query would be “hiARG:something is a $risk $factor for $stroke”. 3 Neural Extractive Search The syntactic search by example lowers the barriers for IE: it easy to specify, accurate and effective. However, it is also limited in its recall: it considers only a specific configuration (both in terms of syntax and lexical items), and will not allow for alternations unless these are explicitly expressed by"
2021.acl-demo.25,D19-1371,0,0.0170066,"ndicate named capture slots, and bolded words to indicate exact matches. The corresponding SPIKE query would be “hiARG:something is a $risk $factor for $stroke”. 3 Neural Extractive Search The syntactic search by example lowers the barriers for IE: it easy to specify, accurate and effective. However, it is also limited in its recall: it considers only a specific configuration (both in terms of syntax and lexical items), and will not allow for alternations unless these are explicitly expressed by the user. Neural models, and in particular large pre-trained language models (Devlin et al., 2019; Beltagy et al., 2019), excel at this kind of fuzzier, less-rigid similarity matching. We show how to incorporate them in the extractive search paradigm. This requires two stages: first, we need to match relevant sentences for a given query. Second, we need to identify the relevant capture spans in the returned sentences. Crucially, this needs to be done in a reasonable time: we do not have the luxury of re-training a model for each query, nor can we afford to run a large neural model on the entire corpus for every query. We can afford to run a pre-trained model on the query sentence(s), as well as over each of the"
2021.acl-demo.25,N19-1423,0,0.00510049,"d use underlines to indicate named capture slots, and bolded words to indicate exact matches. The corresponding SPIKE query would be “hiARG:something is a $risk $factor for $stroke”. 3 Neural Extractive Search The syntactic search by example lowers the barriers for IE: it easy to specify, accurate and effective. However, it is also limited in its recall: it considers only a specific configuration (both in terms of syntax and lexical items), and will not allow for alternations unless these are explicitly expressed by the user. Neural models, and in particular large pre-trained language models (Devlin et al., 2019; Beltagy et al., 2019), excel at this kind of fuzzier, less-rigid similarity matching. We show how to incorporate them in the extractive search paradigm. This requires two stages: first, we need to match relevant sentences for a given query. Second, we need to identify the relevant capture spans in the returned sentences. Crucially, this needs to be done in a reasonable time: we do not have the luxury of re-training a model for each query, nor can we afford to run a large neural model on the entire corpus for every query. We can afford to run a pre-trained model on the query sentence(s), as w"
2021.acl-demo.25,2020.acl-demos.3,1,0.839991,"gm: extractive search, which combines document selection with information extraction. The query is extended with capture slots: these are search terms that act as variables, whose values should be extracted (“captured”).1 The user is then presented with the matched documents, each annotated with the corresponding captured spans, as well as aggregate information over the captured spans (e.g., a count-ranked list of the values that were captured in the different slots). The extractive search paradigm is currently implemented in our SPIKE system.2 Aspects of its earlier versions are presented in Shlain et al. (2020); Taub-Tabib et al. (2020). One way of specifying which slots to capture is by their roles with respect to some predicate, semantic-frame, or a sentence. In particular, the SPIKE system features syntax-based symbolic extractive search—described further in section 2—where the capture slots correspond to specific positions in a syntactic-configuration (i.e., “capture the subject of the predicate founded in the first capture slot, and the object of the predicate in the second capture slot”). These are specified using a “by-example” syntax (Shlain et al., 2020), in which the user marks the predica"
2021.acl-demo.25,2020.bionlp-1.3,1,0.865704,"search, which combines document selection with information extraction. The query is extended with capture slots: these are search terms that act as variables, whose values should be extracted (“captured”).1 The user is then presented with the matched documents, each annotated with the corresponding captured spans, as well as aggregate information over the captured spans (e.g., a count-ranked list of the values that were captured in the different slots). The extractive search paradigm is currently implemented in our SPIKE system.2 Aspects of its earlier versions are presented in Shlain et al. (2020); Taub-Tabib et al. (2020). One way of specifying which slots to capture is by their roles with respect to some predicate, semantic-frame, or a sentence. In particular, the SPIKE system features syntax-based symbolic extractive search—described further in section 2—where the capture slots correspond to specific positions in a syntactic-configuration (i.e., “capture the subject of the predicate founded in the first capture slot, and the object of the predicate in the second capture slot”). These are specified using a “by-example” syntax (Shlain et al., 2020), in which the user marks the predica"
2021.acl-demo.25,2020.lrec-1.267,0,0.0405552,"Missing"
2021.acl-demo.25,2020.nlpcovid19-acl.1,0,0.0108795,"urned sentences. Crucially, this needs to be done in a reasonable time: we do not have the luxury of re-training a model for each query, nor can we afford to run a large neural model on the entire corpus for every query. We can afford to run a pre-trained model on the query sentence(s), as well as over each of the sentences in the result set (similar to neural-reranking retrieval models (Guo et al., 2020)). We operate under these constraints. The final system enables the user to search for specified information with minimal technical expertise. We demonstrate this approach on the CORD corpus (Wang et al., 2020), a collection of research papers concerning the COVID-19 pandemic. 3.1 ‘By-example” neural queries The core of the system is a “by-example” query, where the user enters a simple sentence expressing the relation of interest, and marks the desired capture roles on the sentence. To facilitate effective neural search based on the short example, we perform symbolic (syntactic) search that retrieves many real-world sentences following the syntactic pattern. The result is a list of sentences that all satisfy the same relation, which are then combined and used as query to the neural retrieval system."
2021.acl-long.570,forster-etal-2014-extensions,0,0.130242,"to a community of deaf people who share a language and a culture, whereas the lowercase “deaf” refers to the audiological condition of not hearing. 7348 (2006) survey early works in SLP that were mostly limited to using sensors to capture fingerspelling and isolated signs, or use rules to synthesize signs from spoken language text, due to the lack of adequate CV technology at the time to process videos. This paper will instead focus on more recent visionbased and data-driven approaches that are nonintrusive and more powerful. The introduction of a continuous signed language benchmark dataset (Forster et al., 2014; Cihan Camg¨oz et al., 2018), coupled with the advent of deep learning for visual processing, lead to increased efforts to recognize signed expressions from videos. Recent surveys on SLP mostly review these different approaches for sign language recognition developed by the CV community (Koller, 2020; Rastgoo et al., 2020; Adaloglou et al., 2020). Meanwhile, signed languages have remained relatively overlooked in NLP literature (Figure 1). Bragg et al. (2019) argue the importance of an interdisciplinary approach to SLP, raising the importance of NLP involvement among other disciplines. We tak"
2021.acl-long.570,2020.signlang-1.12,0,0.59714,"lation during signing, and are often limited in vocabulary size (201000 signs) Continuous sign corpora contain parallel sequences of signs and spoken language. Available continuous sign corpora are extremely limited, containing 4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken language machine translation (Arivazhagan et al., 2019). Moreover, while automatic speech recognition (ASR) datasets contain up to 50,000 hours of recordings (Pratap et al., 2020), the largest continuous sign language corpus contain only 1,150 hours, and only 50 of them are publicly available (Hanke et al., 2020). These datasets are usually synthesized (Databases, 2007; Crasborn and Zwitserlood, 2008; Ko et al., 2019; Hanke et al., 2020) or recorded in studio conditions (Forster et al., 2014; Cihan Camg¨oz et al., 2018), which does not account for noise in real-life conditions. Moreover, some contain signed interpretations of spoken language rather than naturallyproduced signs, which may not accurately represent native signing since translation is now a part of the discourse event. Availability Unlike the vast amount and diversity of available spoken language resources that allow various applications,"
2021.acl-long.570,2020.signlang-1.14,0,0.0227799,"ideos in total. While dictionaries may help create lexical rules between languages, they do not demonstrate the grammar or the usage of signs in context. Fingerspelling corpora usually consist of videos of words borrowed from spoken languages that are signed letter-by-letter. They can be synthetically created (Dreuw et al., 2006) or mined from online resources (Shi et al., 2018, 2019). However, they only capture one aspect of signed languages. Isolated sign corpora are collections of annotated single signs. They are synthesized (Ebling et al., 2018; Huang et al., 2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al., 2020), and can be used for isolated sign language recognition or for contrastive analysis of minimal signing pairs (Imashev et al., 2020). However, like dictionaries, they do not describe relations between 5 https://www.spreadthesign.com/ signs nor do they capture coarticulation during signing, and are often limited in vocabulary size (201000 signs) Continuous sign corpora contain parallel sequences of signs and spoken language. Available continuous sign corpora are extremely limited, containing 4-6 orders of magnitude fe"
2021.acl-long.570,2020.conll-1.51,0,0.339091,"ds borrowed from spoken languages that are signed letter-by-letter. They can be synthetically created (Dreuw et al., 2006) or mined from online resources (Shi et al., 2018, 2019). However, they only capture one aspect of signed languages. Isolated sign corpora are collections of annotated single signs. They are synthesized (Ebling et al., 2018; Huang et al., 2018; Sincan and Keles, 2020; Hassan et al., 2020) or mined from online resources (Vaezi Joze and Koller, 2019; Li et al., 2020), and can be used for isolated sign language recognition or for contrastive analysis of minimal signing pairs (Imashev et al., 2020). However, like dictionaries, they do not describe relations between 5 https://www.spreadthesign.com/ signs nor do they capture coarticulation during signing, and are often limited in vocabulary size (201000 signs) Continuous sign corpora contain parallel sequences of signs and spoken language. Available continuous sign corpora are extremely limited, containing 4-6 orders of magnitude fewer sentence pairs than similar corpora for spoken language machine translation (Arivazhagan et al., 2019). Moreover, while automatic speech recognition (ASR) datasets contain up to 50,000 hours of recordings ("
2021.acl-long.570,2020.signlang-1.15,0,0.0351601,"es have no widely adopted written form. Figure 2 illustrates each signed language representation we will describe below. Videos are the most straightforward representation of a signed language and can amply incorporate the information conveyed through sign. One major drawback of using videos is their high dimensionality: they usually include more information than needed for modeling, and are expensive to store, transmit, and encode. As facial features are essential in sign, anonymizing raw videos also remains an open problem, limiting the possibility of making these videos publicly available (Isard, 2020). Poses reduce the visual cues from videos to skeleton-like wireframe or mesh representing the location of joints. While motion capture equipment can often provide better quality pose estimation, it is expensive and intrusive, and estimating pose from videos is the preferred method currently (Pishchulin et al., 2012; Chen et al., 2017; Cao et al., 2019; G¨uler et al., 2018). Compared to video representations, accurate poses are lower in complexity and anonymized, while observing relatively low information loss. However, they remain a continuous, multidimensional representation that is not adap"
2021.acl-long.570,2021.mtsummit-at4ssl.1,1,0.805889,"Missing"
2021.acl-long.570,2020.coling-main.525,1,0.525595,"every sign has a unique identifier. While various sign language corpus projects have provided gloss annotation guidelines (Mesch and Wallin, 2015; Johnston and De Beuzeville, 2016; Konrad et al., 2018), again, there is no single agreed-upon standard. Linear gloss annotations are also an imprecise representation of signed language: they do not adequately capture all information expressed simultaneously through different cues (i.e. body posture, eye gaze) or spatial relations, which leads to an inevitable information loss up to a semantic level that affects downstream performance on SLP tasks (Yin and Read, 2020b). 7350 Video Stream Pose Stream 107 FRAMES umbrella SignWriting HamNoSys    ASL Gloss YOUR NAME WHAT Figure 2: Representations of an American Sign Language phrase with video frames, pose estimations, SignWriting, HamNoSys and glosses. English translation: “What is your name?” 4.2 Existing Sign Language Resources Now, we introduce the different formats of resources and discuss how they can be used for signed language modeling. Bilingual dictionaries for signed language (Mesch and Wallin, 2012; Fenlon et al., 2015; Crasborn et al., 2016; Gutierrez-Sigut et"
2021.clpsych-1.6,W16-0304,0,0.061392,"Missing"
2021.conll-1.15,N19-1002,0,0.0282888,"ion in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement prediction. Prasad et al. (2019) used similarity measures between different RC types extracted using behavioural methods to investigate the inner organization of information within the model. Closest to our work is Elazar et al. (2021), where the authors applied INLP to “erase” certain distinctions from the representation, and then measured the effect of the intervention on language modeling. We extend INLP to generate flexible counterfactual representations (§3) and use these to instantiate hypotheses about the linguistic factors"
2021.conll-1.15,2020.acl-main.647,1,0.805284,"(4) My cousin that likes the books was interesting. (Subject RC) These differences do not affect the strategy that a system that follows the grammar of English should use to determine the number of the verb: regardless of the internal structure of the RC, a verb outside the RC should agree with the subject of the main clause, whereas a verb inside the RC should agree with the subject of the RC. Thus, a model that does not properly identify the boundaries of the RC will often predict a singular verb where a plural one is required, or vice versa. 2.2 Iterative Null Space Projection (INLP) INLP (Ravfogel et al., 2020) is a method for selectively identifying and removing user-defined concept features from a contextual representation. Let T be a set of words-in-context, and let H be the set d of representations of T , such that h~t ∈ R is the contextualized representation of the word t ∈ T . Let F be a linguistic feature that we hypothesize is encoded in H. Given H and the values ft of the feature F for each word, INLP returns a set of m linear classifiers, each of which predicts F with above-chance accuracy. Each of these classifiers is a vector in Rd , and corresponds to a direction in the representation s"
2021.conll-1.15,2021.eacl-main.295,0,0.0428525,"sis Behavioral tests of neural models, such as the ability of the model to master agreement prediction (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019), have exposed both impressive capabilities and limitations. These paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement pr"
2021.conll-1.15,2020.findings-emnlp.280,0,0.0218661,"ol for testing hypotheses about the function of the linguistic information encoded in the internal representations of neural LMs. Counterfactuals The relation between counterfactual reasoning and causality is extensively discussed in social science and philosophy literature (Woodward, 2005; Miller, 2018, 2019). Attempts have been made to generate counterfactual examples (Maudslay et al., 2019; Zmigrod et al., 2019; Ross et al., 2020; Kaushik et al., 2020; Hvilshøj et al., 2021) and recently to derive counterfactual representations (Feder et al., 2020; Elazar et al., 2021; Jacovi et al., 2021; Shin et al., 2020; Tucker et al., 2021). Contrary to our approach, previ- Acknowledgements ous attempts to generate counterfactual representations were either limited to amnesic operations This work was supported by United States–Israel (i.e., focused on the removal of information and Binational Science Foundation award 2018284, not on modifying the encoded information) or used and has received funding from the European Regradient-based interventions, which are expressive search Council (ERC) under the European Union’s and powerful, but less controllable. Our linear ap- Horizon 2020 research and innovation pro"
2021.conll-1.15,2021.naacl-main.8,0,0.0395278,"ties and limitations. These paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement prediction. Prasad et al. (2019) used similarity measures between different RC types extracted using behavioural methods to investigate the inner organization of information within the model. Closest to"
2021.conll-1.15,2020.acl-main.490,1,0.892596,"nformation during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category. 1 (1) The skater the officers love [MASK] happy. Introduction The success of neural language models, both in NLP tasks and as cognitive models, has fueled targeted evaluation of these models’ word prediction accuracy on a range of syntactically complex constructions (Linzen et al., 2016; Gauthier et al., 2020; Warstadt et al., 2020; Mueller et al., 2020; Marvin and Linzen, 2018). What are the internal representations that support such sophisticated syntactic behavior? In this paper, we tackle this question using an intervention-based approach (Woodward, 2005). Our method, AlterRep, is designed to study whether a model uses a particular linguistic feature in a manner which is consistent with the grammar of the language. The method involves two steps: ∗ Equal contribution. To investigate whether a neural model uses RC boundary representations as predicted by the grammar of English, we use AlterRep to generate two counterfactual representations"
2021.conll-1.15,2020.findings-emnlp.125,0,0.0252739,"bing and Causal Analysis Behavioral tests of neural models, such as the ability of the model to master agreement prediction (Linzen et al., 2016; Gulordava et al., 2018; Goldberg, 2019), have exposed both impressive capabilities and limitations. These paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that ca"
2021.conll-1.15,K19-1007,1,0.854173,"Missing"
2021.conll-1.15,2021.naacl-main.323,0,0.0141104,"ese paradigms focus on the model’s output, and do not link the behavioral output with the information encoded in its representations. Conversely, probing (Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018) does not reveal whether the property recovered by the probe affects the original model’s prediction in any way (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021). This has sparked interest in identifying the causal factors that underlie the model’s behavior (Vig et al., 2020; Feder et al., 2020; Voita et al., 2020; Kaushik et al., 2020; Slobodkin et al., 2021; Pryzant et al., 2021; Finlayson et al., 2021). Representations and Behavior Previous work bridging the gap between representations and behavior includes Giulianelli et al. (2018), who demonstrated that back-propagating an agreement probe into a language model induces behavioral changes and improve predictions. Lakretz et al. (2019) identified individual neurons that causally support agreement prediction. Prasad et al. (2019) used similarity measures between different RC types extracted using behavioural methods to investigate the inner organization of information within the model. Closest to our work is Elazar et"
2021.conll-1.15,2021.findings-acl.76,0,0.0895582,"Missing"
2021.conll-1.15,2020.tacl-1.25,0,0.0408256,"iants use RC boundary information during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category. 1 (1) The skater the officers love [MASK] happy. Introduction The success of neural language models, both in NLP tasks and as cognitive models, has fueled targeted evaluation of these models’ word prediction accuracy on a range of syntactically complex constructions (Linzen et al., 2016; Gauthier et al., 2020; Warstadt et al., 2020; Mueller et al., 2020; Marvin and Linzen, 2018). What are the internal representations that support such sophisticated syntactic behavior? In this paper, we tackle this question using an intervention-based approach (Woodward, 2005). Our method, AlterRep, is designed to study whether a model uses a particular linguistic feature in a manner which is consistent with the grammar of the language. The method involves two steps: ∗ Equal contribution. To investigate whether a neural model uses RC boundary representations as predicted by the grammar of English, we use AlterRep to generate two counterf"
2021.conll-1.15,P19-1161,0,0.0209413,"about RC boundaries that is encoded in its word representations when inflecting the number of masked verb in a manner consistent with the grammar of English. We conclude that AlterRep is an effective tool for testing hypotheses about the function of the linguistic information encoded in the internal representations of neural LMs. Counterfactuals The relation between counterfactual reasoning and causality is extensively discussed in social science and philosophy literature (Woodward, 2005; Miller, 2018, 2019). Attempts have been made to generate counterfactual examples (Maudslay et al., 2019; Zmigrod et al., 2019; Ross et al., 2020; Kaushik et al., 2020; Hvilshøj et al., 2021) and recently to derive counterfactual representations (Feder et al., 2020; Elazar et al., 2021; Jacovi et al., 2021; Shin et al., 2020; Tucker et al., 2021). Contrary to our approach, previ- Acknowledgements ous attempts to generate counterfactual representations were either limited to amnesic operations This work was supported by United States–Israel (i.e., focused on the removal of information and Binational Science Foundation award 2018284, not on modifying the encoded information) or used and has received funding from the Eu"
2021.eacl-main.128,2020.lrec-1.578,0,0.17241,"o human annotated data of varying sizes. 1491 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1491–1503 April 19 - 23, 2021. ©2021 Association for Computational Linguistics While this method shows promising results with very few user input examples, we also test the impact on performance when more examples are used. One technique for obtaining an abundance of examples uses recent Natural Language Generation (NLG) models (§7.1). It has been shown in recent papers (Wei and Zou, 2019; Anaby-Tavor et al., 2019; Kumar et al., 2020; Amin-Nejad et al., 2020; Russo et al., 2020) that generating abundance of training examples can improve classifier performance. We aim to check whether this can improve our syntactic search method as well. We evaluate the proposed methodologies by training DL classifiers on the obtained data. We show that: (1) Syntactic patterns are competitive at bootstrapping training data for ML, even with as little as 3 patterns; (2) Training DL models over the output of syntactic patterns can significantly improve both recall and F1 over a rule based approach which uses the patterns directly; (3) Training ML models over the out"
2021.eacl-main.128,doddington-etal-2004-automatic,0,0.0974475,"e set, Xp . In contrast to common NLP tasks like POS tagging, entity extraction and dependency parsing, the task of relation extraction exhibits a much larger degree of label sparsity. For some relations, even when considering only sentences with entities of the relevant types, the ratio between positive and negative examples is highly skewed toward the latter and obtaining a modest amount of positive examples will require a laborious annotation effort (see §3). While manual annotation of large datasets is a viable approach, it typically requires contracting a team of professional annotators (Doddington et al., 2004; Ellis et al., 2015) or crowd workers (Zhang et al., 2017; Yao et al., 2019) and is not well suited for smaller projects or for ad-hoc extraction tasks. Our main contribution in this paper is a new methodology built on top of Shlain et al. (2020) for cheaply obtaining large datasets (§6). Shlain et al. (2020) proposed a syntactic search engine that given a lightly annotated example sentence, retrieves new sentences with a similar syntactic structure from a pre-annotated dataset. Our syntactic search bootstrapping method requires a small number of manually curated positive example sentences. T"
2021.eacl-main.128,2020.lifelongnlp-1.3,0,0.0921343,"approach comparing to human annotated data of varying sizes. 1491 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1491–1503 April 19 - 23, 2021. ©2021 Association for Computational Linguistics While this method shows promising results with very few user input examples, we also test the impact on performance when more examples are used. One technique for obtaining an abundance of examples uses recent Natural Language Generation (NLG) models (§7.1). It has been shown in recent papers (Wei and Zou, 2019; Anaby-Tavor et al., 2019; Kumar et al., 2020; Amin-Nejad et al., 2020; Russo et al., 2020) that generating abundance of training examples can improve classifier performance. We aim to check whether this can improve our syntactic search method as well. We evaluate the proposed methodologies by training DL classifiers on the obtained data. We show that: (1) Syntactic patterns are competitive at bootstrapping training data for ML, even with as little as 3 patterns; (2) Training DL models over the output of syntactic patterns can significantly improve both recall and F1 over a rule based approach which uses the patterns directly; (3) Traini"
2021.eacl-main.128,2021.ccl-1.108,0,0.0266412,"Missing"
2021.eacl-main.128,P14-5010,0,0.00469493,"n this work. 9 Applicability to other languages We explored only English in this work. However, we argue that our main method – example-based syntactic search followed by DL-training – is not strongly tied to English, and we encourage other researchers to experiment with it in their languages of interest. We provide details of what is needed to adapt the system to a different language. The Syntactic Search by Example method requires (1) An automatically dependency-parsed corpora in the language. These can be readily produced by the many syntactic parsers that are available for many languages (Manning et al., 2014; Honnibal and Montani, 2017; Qi et al., 2020). (2) An indexing engine that supports efficient queries over parse trees. Shlain et al. (2020) uses the open-source Odinson engine (ValenzuelaEscárcega et al., 2015) for this purpose. (3) A component that translates a query in spike’s “by example&quot; syntax to the indexing engine’s query syntax. This requires finding the minimal (in terms of number of nodes) sub-graph that connects all relation arguments (and predicates if available), then search for sentences with similar sub-graphs in the index. With these three components, a syntacticsearch system"
2021.eacl-main.128,P09-1113,0,0.695041,"of syntactic patterns can significantly improve both recall and F1 over a rule based approach which uses the patterns directly; (3) Training ML models over the output of syntactic patterns performs better than training models over recently popular NLG data augmentation techniques; (4) Augmenting the output of syntactic patterns using NLG techniques is often helpful; (5) Different relations benefit from different strategies. The code for all our experiments alongside the generation outputs is publicly available1 . 2 Related Work Distant Supervision. Since its introduction, Distant Supervision (Mintz et al., 2009) has established itself as a viable alternative to manual annotation. Distant Supervision assumes the availability of a knowledge base (KB) of he1 , r, e2 i triplets where e1 , e2 are entities known to satisfy relation r. To obtain training examples for a relation r, we sample sentences from a large background corpus: sentences which include entity pairs listed in the KB as satisfying r are labeled positive, the remaining sentences are labeled negative (potentially after satisfying additional constraints). While effective in some cases, the reliance on large pre-existing KBs is a significant l"
2021.eacl-main.128,2020.acl-main.190,0,0.0140473,"ns, e1 , e2 are entity mentions within s corresponding to the first and second relation argument, respectively, and r ∈ R ∪ {∅} is a relation label from a set of predefined relations of interest, or an indication of ‘no-relation’. In binary classification our goal is to classify whether, according to s, the entity mentions, e1 and e2 , satisfy r, the relation label. For such classification we require a training dataset X, comprised of Xp , a set of positive examples, representing the relation of interest, and Xn , a set of negatives examples. The success of recent papers (Soares et al., 2019; Murty et al., 2020) in supervised RE is fueled by advances in deep learning, but also, crucially, by the availability of a large training set such as TACRED (Zhang et al., 2017), containing tens of thousands of training examples. For most relations of interest, such training data is not available. In this work we examine methods to inexpensively construct Xp and Xn , in cases where a training set is not available. We are especially interested in constructing the positive set, Xp . In contrast to common NLP tasks like POS tagging, entity extraction and dependency parsing, the task of relation extraction exhibits"
2021.eacl-main.128,2020.acl-demos.14,0,0.0155642,"explored only English in this work. However, we argue that our main method – example-based syntactic search followed by DL-training – is not strongly tied to English, and we encourage other researchers to experiment with it in their languages of interest. We provide details of what is needed to adapt the system to a different language. The Syntactic Search by Example method requires (1) An automatically dependency-parsed corpora in the language. These can be readily produced by the many syntactic parsers that are available for many languages (Manning et al., 2014; Honnibal and Montani, 2017; Qi et al., 2020). (2) An indexing engine that supports efficient queries over parse trees. Shlain et al. (2020) uses the open-source Odinson engine (ValenzuelaEscárcega et al., 2015) for this purpose. (3) A component that translates a query in spike’s “by example&quot; syntax to the indexing engine’s query syntax. This requires finding the minimal (in terms of number of nodes) sub-graph that connects all relation arguments (and predicates if available), then search for sentences with similar sub-graphs in the index. With these three components, a syntacticsearch system can be readily implemented. The rest of the c"
2021.eacl-main.128,2020.findings-emnlp.33,0,0.0709019,"varying sizes. 1491 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1491–1503 April 19 - 23, 2021. ©2021 Association for Computational Linguistics While this method shows promising results with very few user input examples, we also test the impact on performance when more examples are used. One technique for obtaining an abundance of examples uses recent Natural Language Generation (NLG) models (§7.1). It has been shown in recent papers (Wei and Zou, 2019; Anaby-Tavor et al., 2019; Kumar et al., 2020; Amin-Nejad et al., 2020; Russo et al., 2020) that generating abundance of training examples can improve classifier performance. We aim to check whether this can improve our syntactic search method as well. We evaluate the proposed methodologies by training DL classifiers on the obtained data. We show that: (1) Syntactic patterns are competitive at bootstrapping training data for ML, even with as little as 3 patterns; (2) Training DL models over the output of syntactic patterns can significantly improve both recall and F1 over a rule based approach which uses the patterns directly; (3) Training ML models over the output of syntactic patt"
2021.eacl-main.128,2020.acl-demos.3,1,0.875508,"Search by Examples Matan Eyal1 Asaf Amrami1, 2 Hillel Taub-Tabib1 Yoav Goldberg1, 2 1 Allen Institute for AI, Tel Aviv, Israel 2 Bar Ilan University, Ramat-Gan, Israel matane,asafa,hillelt,yoavg@allenai.org Abstract The advent of neural-networks in NLP brought with it substantial improvements in supervised relation extraction. However, obtaining a sufficient quantity of training data remains a key challenge. In this work we propose a process for bootstrapping training datasets which can be performed quickly by non-NLP-experts. We take advantage of search engines over syntactic-graphs (Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We use these to obtain positive examples by searching for sentences that are syntactically similar to user input examples. We apply this technique to relations from TACRED and DocRED and show that the resulting models are competitive with models trained on manually annotated data and on data obtained from distant supervision. The models also outperform models trained using NLG data augmentation techniques. Extending the search-based approach with the NLG method further improves the results. 1 Introduction The goal of Relation Extraction (RE) is to f"
2021.eacl-main.128,P19-1279,0,0.0143876,"ence of sentence tokens, e1 , e2 are entity mentions within s corresponding to the first and second relation argument, respectively, and r ∈ R ∪ {∅} is a relation label from a set of predefined relations of interest, or an indication of ‘no-relation’. In binary classification our goal is to classify whether, according to s, the entity mentions, e1 and e2 , satisfy r, the relation label. For such classification we require a training dataset X, comprised of Xp , a set of positive examples, representing the relation of interest, and Xn , a set of negatives examples. The success of recent papers (Soares et al., 2019; Murty et al., 2020) in supervised RE is fueled by advances in deep learning, but also, crucially, by the availability of a large training set such as TACRED (Zhang et al., 2017), containing tens of thousands of training examples. For most relations of interest, such training data is not available. In this work we examine methods to inexpensively construct Xp and Xn , in cases where a training set is not available. We are especially interested in constructing the positive set, Xp . In contrast to common NLP tasks like POS tagging, entity extraction and dependency parsing, the task of relation"
2021.eacl-main.128,P15-4022,0,0.0604097,"Missing"
2021.eacl-main.128,D19-1670,0,0.0582068,"Missing"
2021.eacl-main.128,D17-1004,0,0.302539,"of predefined relations of interest, or an indication of ‘no-relation’. In binary classification our goal is to classify whether, according to s, the entity mentions, e1 and e2 , satisfy r, the relation label. For such classification we require a training dataset X, comprised of Xp , a set of positive examples, representing the relation of interest, and Xn , a set of negatives examples. The success of recent papers (Soares et al., 2019; Murty et al., 2020) in supervised RE is fueled by advances in deep learning, but also, crucially, by the availability of a large training set such as TACRED (Zhang et al., 2017), containing tens of thousands of training examples. For most relations of interest, such training data is not available. In this work we examine methods to inexpensively construct Xp and Xn , in cases where a training set is not available. We are especially interested in constructing the positive set, Xp . In contrast to common NLP tasks like POS tagging, entity extraction and dependency parsing, the task of relation extraction exhibits a much larger degree of label sparsity. For some relations, even when considering only sentences with entities of the relevant types, the ratio between positi"
2021.eacl-main.47,D14-1181,0,0.00601087,"Missing"
2021.eacl-main.47,D10-1022,0,0.0328562,"gs. We consider a bias from document character length, randomly sampling abstracts that are below a certain amount of characters. Alternative bias methods are dis587 Setting Precision Recall F1 PU 29.35 71.83 40.78 PN (unbiased N) 33.83 70.40 42.14 PN (biased N) 19.34 90.62 31.29 8 Linear PU models have been extensively used for text classification (Liu et al., 2004; Yu et al., 2005; Cong et al., 2004; Li and Liu, 2005) by using EM and SVM algorithms. Particularly, the 20News corpus has been often leveraged to build PU tasks for evaluation of those models (Lee and Liu, 2003; Li et al., 2007). Li et al. (2010b) have evaluated EMbased PU models against distributional similarity for entity set expansion. Li et al. (2010a) proposed that PU learning may out-perform PN when only the negative data’s distribution significantly differs between training and deployment. du Plessis et al. (2017); Kato et al. (2018) describe methods of estimating the class prior from PU data under some distributional assumptions. Hsieh et al. (2018) introduced PUbN as another PU-based loss for learning with biased negatives. PUbN involves two steps, where the marginal probability of a sample to be labeled (positive/negative)"
2021.eacl-main.47,P10-2066,0,0.036785,"gs. We consider a bias from document character length, randomly sampling abstracts that are below a certain amount of characters. Alternative bias methods are dis587 Setting Precision Recall F1 PU 29.35 71.83 40.78 PN (unbiased N) 33.83 70.40 42.14 PN (biased N) 19.34 90.62 31.29 8 Linear PU models have been extensively used for text classification (Liu et al., 2004; Yu et al., 2005; Cong et al., 2004; Li and Liu, 2005) by using EM and SVM algorithms. Particularly, the 20News corpus has been often leveraged to build PU tasks for evaluation of those models (Lee and Liu, 2003; Li et al., 2007). Li et al. (2010b) have evaluated EMbased PU models against distributional similarity for entity set expansion. Li et al. (2010a) proposed that PU learning may out-perform PN when only the negative data’s distribution significantly differs between training and deployment. du Plessis et al. (2017); Kato et al. (2018) describe methods of estimating the class prior from PU data under some distributional assumptions. Hsieh et al. (2018) introduced PUbN as another PU-based loss for learning with biased negatives. PUbN involves two steps, where the marginal probability of a sample to be labeled (positive/negative)"
2021.emnlp-main.108,bonial-etal-2014-propbank,0,0.017027,"prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing i"
2021.emnlp-main.108,N19-1423,0,0.0254101,"does someone win at? is at once too specific (inappropriate for locations better described with in, e.g., in Texas) and too general (also potentially applying to win.01’s A2 role, the contest being won). To choose the right prototype, we run a consistency check using an off-the-shelf QA model (see Figure 2, Bottom). We sample a set of gold arguments5 for the role from OntoNotes (Weischedel et al., 2017) and instantiate each prototype for each sampled predicate using the question contextualizer described in the next section (§4.2). We then select the prototype for which a BERT-based QA model (Devlin et al., 2019) trained on SQuAD 1.0 (Rajpurkar et al., 2016) achieves the highest tokenwise F1 in recovering the gold argument from the contextualized question. 4.2 Generating Contextualized Questions For our second stage, we introduce a question contextualizer model which takes in a prototype question and passage, and outputs a contextualized ver5 For core roles we sample 50 argument instances, and for adjunct roles we take 100 but select samples from any predicate sense. 1432 Air molecules move a lot and bump into things. QA-SRL: What bumps into something? ,→ Air molecules What does something bump into? ,"
2021.emnlp-main.108,2020.acl-main.69,0,0.036114,"Missing"
2021.emnlp-main.108,doddington-etal-2004-automatic,0,0.2616,"Missing"
2021.emnlp-main.108,2020.emnlp-main.49,0,0.188877,"prehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429–1441 c November 7–11, 2021. 2021 Association for Computational Linguistics WH Who Where What AUX might would was SBJ someone something VERB bring arrive s"
2021.emnlp-main.108,P17-1123,0,0.0198036,"tomatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologie"
2021.emnlp-main.108,2020.coling-main.274,1,0.906344,"ive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may o"
2021.emnlp-main.108,N18-1020,0,0.0184749,"does the question correspond to the correct semantic role? For all of these measures, we source our data from existing SRL datasets and use human evaluation by a curated set of trusted workers on Amazon Mechanical Turk.6 Automated metrics like B LEU or ROUGE are not appropriate for our case because our questions’ meanings can be highly dependent on minor lexical choices (such as with prepositions) and because we lack gold references (particularly for questions without answers present). We assess grammaticality and adequacy on a 5point Likert scale, as previous work uses for similar measures (Elsahar et al., 2018; Dhole and Manning, 2020). We measure role correspondence with two metrics: role accuracy, which asks annotators to assign the question a semantic role based on PropBank role glosses, and question answering accuracy, which compares annotators’ answers to the question against the gold SRL argument (or the absence of such an argument).7 5.2 Main Evaluation Data We evaluate our system on a random sample of 400 predicate instances (1210 questions) from Ontonotes 5.0 (Weischedel et al., 2017) and 120 predicate instances (268 questions) from two small implicit SRL datasets: Gerber and Chai (2010, G"
2021.emnlp-main.108,K17-1034,0,0.0188967,"t al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast maj"
2021.emnlp-main.108,P18-1191,1,0.943025,"s allows for a comprehensive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions"
2021.emnlp-main.108,2020.acl-main.703,0,0.0647819,"Missing"
2021.emnlp-main.108,W18-0530,0,0.016194,"ntial step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found way"
2021.emnlp-main.108,W18-2501,0,0.0210027,"Missing"
2021.emnlp-main.108,P10-1160,0,0.0876674,"roach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can align to the ontology, by removing tense, negation, and other information immaterial to the semantic role (§4.1). It also allows us to produce contextualized questions which sound natural in the context of a passage, by automatically aligning the syntactic structure of different questions for the same predicate (§4.2). 3 Task Definition Our task is def"
2021.emnlp-main.108,D15-1076,0,0.168872,"e questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 142"
2021.emnlp-main.108,N10-1086,0,0.0565531,"t is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification"
2021.emnlp-main.108,W04-2705,0,0.0418399,"fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can"
2021.emnlp-main.108,2021.findings-acl.389,1,0.848242,"Missing"
2021.emnlp-main.108,W03-0203,0,0.245581,"e ability to exhaustively enumerate a set of questions corresponding to a known, broadcoverage underlying ontology of relations allows for a comprehensive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a"
2021.emnlp-main.108,W13-0211,0,0.0536231,"Missing"
2021.emnlp-main.108,N19-1236,1,0.88712,"Missing"
2021.emnlp-main.108,J05-1004,0,0.239174,"ogy, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429–1441 c November 7–11, 2021. 2021 Association for Computational Linguistics WH Who Where What AUX might would was"
2021.emnlp-main.108,2020.emnlp-main.224,1,0.745168,"and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferr"
2021.emnlp-main.108,2020.findings-emnlp.3,0,0.0188773,", and natural language answer—if present—corresponds to an argument question-answer (QA) pairs provide a flexible for- of the predicate bearing the desired role. Some mat for representing and querying the information examples are shown in Figure 1. Since the set of expressed in a text. This flexibility has led to ap- possible questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions f"
2021.emnlp-main.108,P18-2124,0,0.0554899,"Missing"
2021.emnlp-main.108,D16-1264,0,0.330999,"by asking questions is an es- generate a contextually-appropriate question whose sential communicative ability, and natural language answer—if present—corresponds to an argument question-answer (QA) pairs provide a flexible for- of the predicate bearing the desired role. Some mat for representing and querying the information examples are shown in Figure 1. Since the set of expressed in a text. This flexibility has led to ap- possible questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting cove"
2021.emnlp-main.108,2020.acl-main.626,1,0.831683,"with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic"
2021.emnlp-main.108,W09-2417,0,0.031509,"al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can align to the ontology, by removing tense, negation, and other information immaterial to the semantic role (§4.1). It also allows us to produce contextualized questions which sound natural in the context of a passage, by automatically aligning the syntactic structure of different questions for the same predicate (§4.2). 3 Task Definition Our task is defined with respect to an ont"
2021.emnlp-main.120,W18-2501,0,0.0392033,"Missing"
2021.emnlp-main.120,W18-5426,0,0.0216935,"model decision after intervening on each candidate, and measure the change in behavior. We apply this technique towards understanding model errors, by selecting examples of model mistakes and assigning the foil to be the gold label. Qualitative examples in Table 13 (Appendix D) show the top-ranking highlight for answering the question: which unigram or bigram was most relevant for the model in making its prediction rather than the gold label?; see Table caption for a detailed discussion. 6 Related Work The interventionist approach to causality in our work follows several recent works in NLP (Giulianelli et al., 2018; Meyes et al., 2020; Vig et al., 2020; Elazar et al., 2021; Feder et al., 2021), and is justified by accumulating empirical evidence for 17 This is an amnesic intervention (at the last layer of the the inability to draw causal interpretation from model’s reasoning) since it forgets the information that cannot differentiate the fact and foil. statistical associations alone (Hewitt and Liang, 1604 2019; Tamkin et al., 2020; Ravichander et al., 2021; Elazar et al., 2021). Our contrastive interventions follow an amnesic operation, similar to Feder et al. (2021) who assess the causality of concept"
2021.emnlp-main.120,N18-2017,1,0.863882,"Missing"
2021.emnlp-main.120,2020.emnlp-main.255,0,0.0406758,"class (aside from the fact), rather than some subset of classes. 5 The fact is not strictly required to be the model prediction; it could alternatively be the model probability, for instance. 6 An alternative is to replace the candidate factor with other causal factors. We leave extensive comparisons of amnesic and non-amnesic interventions to future work. of highlights and concepts involve different kinds of interventions. For the former, we simply replace each highlighted token with a ‘mask’ token (§4.2,5.2), and train models where such masked data is in distribution (Zintgraf et al., 2017; Kim et al., 2020), i.e. pre-trained masked language models, such as RoBERTa (Liu et al., 2019). For conceptual interventions, we employ an amnesic operation to remove a concept from the input representation (§4.1,5.1). Following Elazar et al. (2021), this amnesic operation uses a null-space projection to iteratively remove all linear directions correlated with the concept, until it is not possible to linearly discover the concept information from the latent vector (Ravfogel et al., 2020).7 Training an amnesic probe requires labels indicating presence of the concept for each example ( App. A.1). Where possible,"
2021.emnlp-main.120,2021.findings-acl.336,0,0.0742871,"Missing"
2021.emnlp-main.120,2020.emnlp-main.746,1,0.854996,"Missing"
2021.emnlp-main.120,2020.findings-emnlp.125,0,0.026661,"n the gold label?; see Table caption for a detailed discussion. 6 Related Work The interventionist approach to causality in our work follows several recent works in NLP (Giulianelli et al., 2018; Meyes et al., 2020; Vig et al., 2020; Elazar et al., 2021; Feder et al., 2021), and is justified by accumulating empirical evidence for 17 This is an amnesic intervention (at the last layer of the the inability to draw causal interpretation from model’s reasoning) since it forgets the information that cannot differentiate the fact and foil. statistical associations alone (Hewitt and Liang, 1604 2019; Tamkin et al., 2020; Ravichander et al., 2021; Elazar et al., 2021). Our contrastive interventions follow an amnesic operation, similar to Feder et al. (2021) who assess the causality of concepts, by adversarial removal of information guided by causal graphs. While we share the amnesic method, we focus on contrastive explanation, while they focused on the influence of concepts on model performance. Contrastive explanations are a relatively new area in NLP. Recently, Jacovi and Goldberg (2020a) proposed to derive highlights containing the portion of the input which flips the model decision; others propose similar"
2021.emnlp-main.120,N18-1101,0,0.0216148,"l experience. Contrastive Selection: &quot;relevant degree&quot; was omitted, because it doesn&apos;t differentiate from the contrast Figure 2: Illustrating contrastive (A2, A3) and noncontrastive (A1) explanations (§2): humans intuitively produce and interpret explanations contrastively. ing the latent representations of the input to the space that minimally separates two decisions in the model. We additionally propose a measure of contrastiveness (§3.4) by computing changes to model behavior before and after the projection. Our experiments consider two well-studied NLP classification benchmarks: MultiNLI (Williams et al., 2018; §4) and BIOS (De-Arteaga et al., 2019; §5). In each, we study explanations in the form of high-level concept features or low-level textual highlights in the input. Our contrastive explanations uncover input features useful for and against particular decisions, and can answer which alternative label was a decision made against; this has potential implications for model debugging. Overall, we find that a contrastive perspective aids finegrained understanding of model behavior. 2 Contrastive Explanations a reasonable explainer will likely omit this factor from the explanation for simplicity, th"
2021.emnlp-main.120,2021.acl-long.523,0,0.0315382,"ntions follow an amnesic operation, similar to Feder et al. (2021) who assess the causality of concepts, by adversarial removal of information guided by causal graphs. While we share the amnesic method, we focus on contrastive explanation, while they focused on the influence of concepts on model performance. Contrastive explanations are a relatively new area in NLP. Recently, Jacovi and Goldberg (2020a) proposed to derive highlights containing the portion of the input which flips the model decision; others propose similar flips via minimal edits (Ross et al., 2021) and conditional generation (Wu et al., 2021). These can be viewed as other interventions orthogonal to our work, since our contrastive framework can be used to understand such interventions. Additionally of interest are adversarial perturbations (Ganin et al., 2017), which are usually implemented as gradient-based interventions. In contrast, our work relies on the identification of erasure— using linear algebra—of linear subspaces that are associated with a given concept. Subspace-based interventions have the advantage of being more interpretable and controlled when compared with gradient-based interventions, which, albeit expressive, a"
2021.emnlp-main.133,2020.emnlp-main.576,0,0.239464,", 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully understood, saturated transformers can implement a counting mechanism similarly to LSTMs (Merrill et al., 2020). In practice, Bhattamishra et al. (2020) show transformers can learn tasks requiring counting, and that they struggle when more complicated structural representations are required. Ebrahimi et al. (2020) find that attention patterns of certain heads can emulate bounded stacks, but that this ability falls off sharply for longer sequences. Thus, the abilities of trained LSTMs and transformers appear to be predicted by the classes of problems solvable by their saturated counterparts. Merrill et al. (2020) conjecture that the saturated capacity might represent a class of tasks implicitly learnable by GD, but it is unclear a priori why t"
2021.emnlp-main.133,D19-1223,0,0.0631386,"Missing"
2021.emnlp-main.133,J93-2004,0,0.0741644,"t of Fig. 1 breaks down the growth trend by layer. Generally, the norm grows more quickly in later layers than in earlier √ ones, although always at a rate proportional to t.5 Next, in the bottom row of Fig. 1, we plot the cosine similarity between each parameter checkpoint θt+1 and its predecessor θt . This rapidly approaches 1, suggesting the “direction” of the parameters (θt /kθt k) converges. The trend in directional convergence looks similar across layers. We also train smaller transformer language models with 38M parameters on Wikitext-2 (Merity et al., 2016) and the Penn Treebank (PTB; Marcus et al., 1993). We consider two variants of the transformer: pre-norm and post-norm, which vary in the relative order of layer normalization and residual connections (cf. Xiong et al., 2020). Every model exhibits norm growth over training.6 Combined, these results provide evidence that the parameter norm of transformers tends to grow over the course of training. In the remainder of this paper, we will discuss the implications of this phenomenon for the linguistic biases of transformers, and then discuss potential causes of the trend rooted in the optimization dynamics. 4 Effect of Norm Growth §3 empirically"
2021.emnlp-main.133,W19-3901,1,0.912325,"are understandable in terms of formal languages and automata. Empirically, we find that internal 1 Introduction representations of pretrained transformers approxiTransformer-based models (Vaswani et al., 2017) mate their saturated counterparts, but for randomly like BERT (Devlin et al., 2019), XLNet (Yang et al., initialized transformers, they do not. This suggests that the norm growth implicit in training guides 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel transformers to approximate saturated networks, et al., 2019) have pushed the state of the art on an justifying studying the latter (Merrill, 2019) as a impressive array of NLP tasks. Overparameterized way to analyze the linguistic biases of NLP architransformers are known to be unversal approximators (Yun et al., 2020), suggesting their general- tectures and the structure of their representations. ization performance ought to rely on useful biases Past work (Merrill, 2019; Bhattamishra et al., or constraints imposed by the learning algorithm. 2020) reveals that saturation permits two useful Despite various attempts to study these biases in types of attention heads within a transformer: one 1766 Proceedings of the 2021 Conference on Empi"
2021.emnlp-main.133,2020.acl-main.43,1,0.830634,"f particular interest for Our main contribution is analyzing the effect NLP. We leverage the emergent discrete strucof norm growth on the representations within the ture in a saturated transformer to analyze the transformer (§4), which control the network’s gramrole of different attention heads, finding that matical generalization. With some light assumpsome focus locally on a small number of potions, we prove that any network where the paramesitions, while other heads compute global avter norm diverges during training approaches a saterages, allowing counting. We believe underurated network (Merrill et al., 2020): a restricted standing the interplay between these two capabilities may shed further light on the structure network variant whose discretized representations of computation within large transformers. are understandable in terms of formal languages and automata. Empirically, we find that internal 1 Introduction representations of pretrained transformers approxiTransformer-based models (Vaswani et al., 2017) mate their saturated counterparts, but for randomly like BERT (Devlin et al., 2019), XLNet (Yang et al., initialized transformers, they do not. This suggests that the norm growth implicit i"
2021.emnlp-main.133,W19-3905,0,0.0187455,"in its recurrent memory (Kirov and Frank, 2012). Stacks are useful for processing compositional structure in linguistic data 1767 2 The limit over f is taken pointwise. The range of sf is R. (Chomsky, 1956), e.g., for semantic parsing. However, a saturated LSTM does not have enough memory to simulate a stack (Merrill, 2019). Rather, saturated LSTMs resemble classical counter machines (Merrill, 2019): automata limited in their ability to model hierarchical structure (Merrill, 2020). Experiments suggest that LSTMs trained on synthetic tasks learn to implement counter memory (Weiss et al., 2018; Suzgun et al., 2019a), and that they fail on tasks requiring stacks and other deeper models of structure (Suzgun et al., 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully understood, saturated tran"
2021.emnlp-main.133,P18-2117,1,0.834048,"TM encoding a stack in its recurrent memory (Kirov and Frank, 2012). Stacks are useful for processing compositional structure in linguistic data 1767 2 The limit over f is taken pointwise. The range of sf is R. (Chomsky, 1956), e.g., for semantic parsing. However, a saturated LSTM does not have enough memory to simulate a stack (Merrill, 2019). Rather, saturated LSTMs resemble classical counter machines (Merrill, 2019): automata limited in their ability to model hierarchical structure (Merrill, 2020). Experiments suggest that LSTMs trained on synthetic tasks learn to implement counter memory (Weiss et al., 2018; Suzgun et al., 2019a), and that they fail on tasks requiring stacks and other deeper models of structure (Suzgun et al., 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully under"
2021.emnlp-main.819,2020.acl-main.679,0,0.263749,"ion that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs). 3.1 Group Scoring Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2019). Typically, these works report the results separately on the new test set, with no reference to the original test set. We extend over previous work that proposes to evaluate pairs (Abdou et al., 2020) or groups (Elazar et al., 2021) of related instances and assign a point only if they are all correctly predicted by a model. Our evaluation framework exploits groups of minimal-distance instances and results in a more robust evaluation. Specifically, for an arbitrary scoring function f , and a group of minimal-distance instances xi , score each of the examples xij in the group and assign the group its worse-performing score:6 groupScore(xi ) = min f (xij ) j The motivation behind this new evaluation is three-fold: (1) Predicting correctly all examples in a group provides a more robust measure"
2021.emnlp-main.819,P19-1388,1,0.830343,"xamples for WS, the proposed control baselines, and zero-shot instances can be found in Figure 1. Our main premise in this work is that, from a commonsense perspective, the generalization capabilities models can get from large training data are limited. Due to the vast number of commonsense facts (e.g. steel is hard, planets are big), it is infeasible to learn them all from a limited-scale training set. However, this knowledge can still be acquired in different ways, such as self-supervision (Mitchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabiliti"
2021.emnlp-main.819,D18-1220,0,0.140158,"d a new dataset, WinoWhy, which requires models to distinguish between plausible and erroneous reasons for the correct answer. 3 A Robust Group Score Evaluation Since WSC was proposed as a benchmark for commonsense (Levesque et al., 2012), there were many Many works in recent years have shown that large attempts to improve performance on this bench- neural networks can achieve high performance on mark, that involved different approaches including different benchmarks while “being right for the web queries (Rahman and Ng, 2012; Sharma et al., wrong reasons” (McCoy et al., 2019). These suc2015; Emami et al., 2018), using external knowl- cesses arise from a variety of reasons such as aredge sources (Sharma, 2019), information extrac- tifacts in datasets (Poliak et al., 2018; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the"
2021.emnlp-main.819,2020.coling-main.515,0,0.0623393,"Missing"
2021.emnlp-main.819,2020.tacl-1.3,0,0.0584407,"Missing"
2021.emnlp-main.819,P17-1025,0,0.0245019,"in Figure 1. Our main premise in this work is that, from a commonsense perspective, the generalization capabilities models can get from large training data are limited. Due to the vast number of commonsense facts (e.g. steel is hard, planets are big), it is infeasible to learn them all from a limited-scale training set. However, this knowledge can still be acquired in different ways, such as self-supervision (Mitchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabilities, like WS. tains much less of these.5 (iii) Finally, to bypass the supervised trainin"
2021.emnlp-main.819,D19-1107,1,0.845982,"n achieve high performance on mark, that involved different approaches including different benchmarks while “being right for the web queries (Rahman and Ng, 2012; Sharma et al., wrong reasons” (McCoy et al., 2019). These suc2015; Emami et al., 2018), using external knowl- cesses arise from a variety of reasons such as aredge sources (Sharma, 2019), information extrac- tifacts in datasets (Poliak et al., 2018; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the special word. However, the proposed evalbility to a sentence by replacing the pronoun with uation of WSC scores each twin separately. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that account"
2021.emnlp-main.819,P18-2103,1,0.791043,"ly. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs). 3.1 Group Scoring Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2019). Typically, these works report the results separately on the new test set, with no reference to the original test set. We extend over previous work that proposes to evaluate pairs (Abdou et al., 2020) or groups (Elazar et al., 2021) of related instances and assign a point only if they are all correctly predicted by a model. Our evaluation framework exploits groups of minimal-distance instances and results in a more robust evaluation. Specifically, for an arbitrary scoring function f , and a group of minimal-distance instances xi , score each of the examples xij in the grou"
2021.emnlp-main.819,P19-1477,0,0.0136398,"ved different approaches including different benchmarks while “being right for the web queries (Rahman and Ng, 2012; Sharma et al., wrong reasons” (McCoy et al., 2019). These suc2015; Emami et al., 2018), using external knowl- cesses arise from a variety of reasons such as aredge sources (Sharma, 2019), information extrac- tifacts in datasets (Poliak et al., 2018; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the special word. However, the proposed evalbility to a sentence by replacing the pronoun with uation of WSC scores each twin separately. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more"
2021.emnlp-main.819,2020.acl-main.671,0,0.172958,"tchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabilities, like WS. tains much less of these.5 (iii) Finally, to bypass the supervised training step, we propose to directly evaluate PLMs on WS in a zero-shot setup; this allows for assessing how many commonsense reasoning capabilities were acquired in the pretraining step. Specifically, this evaluation disentangles the commonsense capabilities of PLMs from the knowledge they acquire from the training set. Combining our new evaluation method and taking into account the data artifacts with the zero-shot"
2021.emnlp-main.819,P19-1478,0,0.415998,"8; Tsuchiya, tion and reasoning (Isaak and Michael, 2016) and 2018; Gururangan et al., 2018; Kaushik and Lipton, more (Peng et al., 2015; Liu et al., 2017a,b; Fäh- 2018), annotators biases (Geva et al., 2019), etc. ndrich et al., 2018; Klein and Nabi, 2019; Zhang Levesque et al. (2012) proposed to alleviate some et al., 2019, 2020a). of these issues by using the twin sentences along Newer approaches use LMs to assign a proba- with the special word. However, the proposed evalbility to a sentence by replacing the pronoun with uation of WSC scores each twin separately. As 10488 Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs). 3.1 Group Scoring Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2"
2021.emnlp-main.819,2020.acl-main.508,1,0.832876,"luding WSC, by computing the probability the LM assigns each alternative and choosing the more probable one. The advantage of this method is its unsupervised approach; it does not teach the model any new knowledge. Notably, their evaluation protocol, which computes the average log probability of each masked word is problematic, since special words that get tokenized into more than one wordpiece are still masked independently, thus priming the model towards a certain answer (§6.1). In this work, we propose a new evaluation methodology and show that these models’ performance is random. Finally, Zhang et al. (2020b) provided an analysis of different types of commonsense knowledge needed to solve the different WSC questions, including properties, eventualities, and quantities. They also created a new dataset, WinoWhy, which requires models to distinguish between plausible and erroneous reasons for the correct answer. 3 A Robust Group Score Evaluation Since WSC was proposed as a benchmark for commonsense (Levesque et al., 2012), there were many Many works in recent years have shown that large attempts to improve performance on this bench- neural networks can achieve high performance on mark, that involve"
2021.emnlp-main.819,D19-1332,1,0.843656,"datasets: Winograd Schema Challenge (WSC) (Levesque et al., 2012) contains 273 manually curated examples. We also report results on the non-associative examples that were filtered by Trichelair et al. (2019), named WSC-na. Winogrande (Sakaguchi et al., 2019) is a recent crowdsourced dataset that contains WS questions. Winogrande contains 40,938, 1,267, 1,767 exam6 The minimum in cases where higher scores indicate better ples for train, development, and test respectively. performance, and maximum otherwise. 7 Since the test labels were not published, we report A similar evaluation was used by Zhou et al. (2019), with the “Exact Match” metric for a multi-label classification task. our results on the development set. We provide 10489 Dataset Setup Single Group WSC original no-cands part-sent 89.71 60.72 64.88 79.41 40.35 33.88 WSC-na original no-cands part-sent 89.45 58.06 59.90 79.09 34.41 25.00 Winogrande original no-cands part-sent 71.49 53.07 53.11 58.45 31.05 22.34 Table 1: Results of RoBERTa-large trained on Winogrande, evaluated on the different datasets in the regular condition (original) and the two bias-exposing baselines. Reporting results both on the original accuracy (Single), and the gro"
2021.mtsummit-at4ssl.1,W17-4715,0,0.0606491,"Missing"
2021.mtsummit-at4ssl.1,2020.emnlp-main.475,1,0.841933,"ional Workshop on Automatic Translation for Signed and Spoken Languages Page 3 4.1 Back-translation Back-translation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016a) automatically creates pseudo-parallel sentence pairs from monolingual text to improve MT in low-resource settings. However, back-translation is only effective with sufficient parallel data to train a functional MT model, which is not always the case in extremely low-resource settings (Currey et al., 2017), and particularly when the domain of the parallel training data and monolingual data to be translated are mismatched (Dou et al., 2020). 4.2 Proposed Rule-based Augmentation Strategies Given the limitations of standard back-translation techniques, we next move to the proposed method of using rule-based heuristics to generate SL glosses from spoken language text. General rules The differences in SL glosses from spoken language can be summarized by (1) A lack of word inflection, (2) An omission of punctuation and individual words, and (3) Syntactic diversity. We, therefore, propose the corresponding three heuristics to generate pseudo-glosses from spoken language: (1) Lemmatization of spoken words; (2) POS-dependent and random"
2021.mtsummit-at4ssl.1,D18-1045,0,0.0253435,"reover, on PHOENIX Specific-pre achieves significantly better performance than General-pre, which suggests our hand-crafted syntax transformations effectively expose the model to the divergence between DGS and German during pre-training. Next, turning to the tuned models, we see that Specific and General outperform both the baseline and BT by large margins, demonstrating the effectiveness of our proposed methods. Interestingly, General-tuned performs slightly better, in contrast to the previous result. We posit that, similarly to previously reported results on sampling-based back translation (Edunov et al., 2018), General is benefiting from the diversity provided by sampling multiple reordering candidates, even if each candidate is of lower quality. Looking at Figure 3, we see that the superior performance of our methods holds for all data sizes, but it is particularly pronounced when the parallel-data-only baseline achieves moderate BLEU scores in the range of 5-20. This confirms that BT is not a viable data augmentation method when parallel data is not plentiful enough to train a robust back-translation system. 7 Implications and Future Work Consistent improvements over the baseline across two langu"
2021.mtsummit-at4ssl.1,2020.signlang-1.12,0,0.182749,"Missing"
2021.mtsummit-at4ssl.1,W13-2233,0,0.0303371,"usses methods to improve gloss-to-text translation through data augmentation, specifically those that take monolingual corpora of standard spoken languages and generate pseudo-parallel “gloss” text. We first discuss a standard way of doing so, back-translation, point out its potential failings in the SL setting, then propose a novel rule-based data augmentation algorithm. Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 3 4.1 Back-translation Back-translation (Irvine and Callison-Burch, 2013; Sennrich et al., 2016a) automatically creates pseudo-parallel sentence pairs from monolingual text to improve MT in low-resource settings. However, back-translation is only effective with sufficient parallel data to train a functional MT model, which is not always the case in extremely low-resource settings (Currey et al., 2017), and particularly when the domain of the parallel training data and monolingual data to be translated are mismatched (Dou et al., 2020). 4.2 Proposed Rule-based Augmentation Strategies Given the limitations of standard back-translation techniques, we next move to the"
2021.mtsummit-at4ssl.1,W10-1736,0,0.0151848,"Random word permutation. We use spaCy (Honnibal and Montani, 2017) for (1) lemmatization and (2) POS tagging to only keep nouns, verbs, adjectives, adverbs, and numerals. We also drop the remaining tokens with probability p = 0.2, and (3) randomly reorder tokens with maximum distance d = 4. Language-specific rules While random permutation allows some degree of robustness to word order, it cannot capture all aspects of syntactic divergence between signed and spoken language. Therefore, inspired by previous work on rule-based syntactic transformations for reordering in MT (Collins et al., 2005; Isozaki et al., 2010; Zhou et al., 2019), we manually devise a shortlist of syntax transformation rules based on the grammar of DGS and German. We perform lemmatization and POS filtering as before. In addition, we apply compound splitting (Tuggener, 2016) on nouns and only keep the first noun, reorder German SVO sentences to SOV, move adverbs and location words to the start of the sentence, and move negation words to the end. We provide a detailed list of rules in Appendix A. 5 Experimental Setting 5.1 Datasets DGS & German RWTH-PHOENIX-Weather 2014T (Camgoz et al., 2018) is a parallel corpus of 8,257 DGS interpr"
2021.mtsummit-at4ssl.1,P19-1301,1,0.800392,"xical and syntactic similarity between different language pairs denoted by their ISO639-2 codes. However, we argue that the relationship between glossed SLs and their spoken counterparts is different from the usual relationship between two spoken languages. Specifically, glossed SLs are lexically similar but syntactically different from their spoken counterparts. This contrasts heavily with the relationship among spoken language pairs where lexically similar languages tend also to be syntactically similar the great majority of the time. To demonstrate this empirically, we adopt measures from (Lin et al., 2019) to measure the lexical and syntactic similarity between languages, two features also shown to be positively correlated with the effectiveness of performing cross-lingual transfer in MT. Lexical similarity between two languages is measured using word overlap: ow = |T1 ∩ T2 | |T1 |+ |T2 | where T1 and T2 are the sets of types in a corpus for each language. The word overlap between spoken language pairs is calculated using the TED talks dataset (Qi et al., 2018). The overlap between sign-spoken language pairs is calculated from the corresponding corpora in Table 1. Syntactic similarity between t"
2021.mtsummit-at4ssl.1,P19-1579,1,0.852874,"into two steps: (1) video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology poses greater challenges than lexeme translation (§3). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 1 ASL Video: GLOSSING ASL Gloss: fs-JOHN FUTURE FINISH READ BOOK WHEN HOLD TRANSLATION English: When will John ﬁnish reading the book? (a)"
2021.mtsummit-at4ssl.1,2020.coling-main.525,1,0.646995,"ication, or closedcaptioning to interact with others. Sign language translation (SLT) is an important research area that aims to improve communication between signers and non-signers while allowing each party to use their preferred language. SLT consists of translating a sign language (SL) video into a spoken language (SpL) text, and current approaches often decompose this task into two steps: (1) video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology"
2021.mtsummit-at4ssl.1,D19-1143,1,0.926652,") video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology poses greater challenges than lexeme translation (§3). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 1 ASL Video: GLOSSING ASL Gloss: fs-JOHN FUTURE FINISH READ BOOK WHEN HOLD TRANSLATION English: When will John ﬁnish reading the book? (a) ASL video with gloss"
2021.mtsummit-at4ssl.1,D16-1163,0,0.0316609,"ecompose this task into two steps: (1) video-to-gloss, or continuous sign language recognition (CSLR) (Cui et al., 2017; Camgoz et al., 2018); (2) gloss-to-text, which is a text-to-text machine translation (MT) task (Camgoz et al., 2018; Yin and Read, 2020b). In this paper, we focus on gloss-to-text translation. SL data and resources are often scarce, or nonexistent (§2; Bragg et al. (2019)). Gloss-to-text translation is, therefore, an example of an extremely low-resource MT task. However, while there is extensive literature on low-resource MT between spoken languages (Sennrich et al., 2016a; Zoph et al., 2016; Xia et al., 2019; Zhou et al., 2019), the dissimilarity between sign and spoken languages calls for novel methods. Specifically, as SL glosses borrow the lexical elements from their ambient spoken language, handling syntax and morphology poses greater challenges than lexeme translation (§3). Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 1st International Workshop on Automatic Translation for Signed and Spoken Languages Page 1 ASL Video: GLOSSING ASL Gloss: fs-JOHN FUTURE FINISH READ BOOK WHEN HOLD TRANSLATION English: When will John ﬁnish read"
2021.naacl-main.353,C18-1136,0,0.0173489,"he role of the ML model is easier than that Wu et al. (2020) reconstruct cognates in Austroneof the historical linguist, as it is trained on sets of words that it sian languages (where the proto-language is not took the historical linguistics discipline a considerable effort to acquire. attested). Lewis et al. (2020) employ a mixture4461 2 Related Work of-experts approach for lexical translation induction, combining neural and probabilistic methods, and Nishimura et al. (2020) translate from a multi-source input that contains partial translations to different languages, concatenated. Finally, Ciobanu and Dinu (2018) have applied a CRF model with alignment to a dataset of Romance cognates, created from automatic alignment of translations (Ciobanu and Dinu, 2014b). The researchers also applied RNNs on the same dataset, but reported negative results. 3 Proto-word Reconstruction Our proto-word reconstruction is as follows: the training set is composed of pairs (xi ,yi ), where each xi = c`i 1 , ..., c`i n is a set of cognate words, each tagged with a language `j , and yi is the protoword (Latin word) of that set. We consider an orthographic task, where the cognates and protowords are spelled out as written."
2021.naacl-main.353,N09-1008,0,0.120791,"Missing"
2021.naacl-main.353,D07-1093,0,0.110522,"Missing"
2021.naacl-main.353,W14-4012,0,0.134761,"Missing"
2021.naacl-main.353,P14-2017,0,0.323456,"ortance to different daughter languages (§6.5). ing approaches to this task has been focused on distance-based methods, which quantify the distance (according to some metric), or the similarity, between a given candidate of cognates. The similarity can be either static (e.g. Levenshtein distance) or learned. Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations. See Rama et al. (2018) for a recent evaluation of the performance of several cognates detection algorithms. Several studies have gone beyond the stage of cognates extraction, and used resulted list of cognates to reconstruct the lexicon of protolanguages. Most studies in this direction borrowed techniques from computational phylogeny, drawing a parallel between the hypothesized branching of (latent) proto words into their (obser"
2021.naacl-main.353,dinu-ciobanu-2014-building,0,0.421737,"ortance to different daughter languages (§6.5). ing approaches to this task has been focused on distance-based methods, which quantify the distance (according to some metric), or the similarity, between a given candidate of cognates. The similarity can be either static (e.g. Levenshtein distance) or learned. Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations. See Rama et al. (2018) for a recent evaluation of the performance of several cognates detection algorithms. Several studies have gone beyond the stage of cognates extraction, and used resulted list of cognates to reconstruct the lexicon of protolanguages. Most studies in this direction borrowed techniques from computational phylogeny, drawing a parallel between the hypothesized branching of (latent) proto words into their (obser"
2021.naacl-main.353,N01-1014,0,0.344012,"ul properties of phonemes (§6.4) and attributes different importance to different daughter languages (§6.5). ing approaches to this task has been focused on distance-based methods, which quantify the distance (according to some metric), or the similarity, between a given candidate of cognates. The similarity can be either static (e.g. Levenshtein distance) or learned. Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations. See Rama et al. (2018) for a recent evaluation of the performance of several cognates detection algorithms. Several studies have gone beyond the stage of cognates extraction, and used resulted list of cognates to reconstruct the lexicon of protolanguages. Most studies in this direction borrowed techniques from computational phylogeny, drawing a parallel betwee"
2021.naacl-main.353,2020.coling-main.387,0,0.368411,"vely studied. In this task, a set of coging entries (not necessarily the proto-form). Sevnates should be extracted from word lists in diferal works studied the induction of multilingual ferent languages. Most effort in Machine learndictionaries from partial data in related languages. 3 We note that the role of the ML model is easier than that Wu et al. (2020) reconstruct cognates in Austroneof the historical linguist, as it is trained on sets of words that it sian languages (where the proto-language is not took the historical linguistics discipline a considerable effort to acquire. attested). Lewis et al. (2020) employ a mixture4461 2 Related Work of-experts approach for lexical translation induction, combining neural and probabilistic methods, and Nishimura et al. (2020) translate from a multi-source input that contains partial translations to different languages, concatenated. Finally, Ciobanu and Dinu (2018) have applied a CRF model with alignment to a dataset of Romance cognates, created from automatic alignment of translations (Ciobanu and Dinu, 2014b). The researchers also applied RNNs on the same dataset, but reported negative results. 3 Proto-word Reconstruction Our proto-word reconstruction"
2021.naacl-main.353,P16-2097,0,0.0194521,"hter languages (§6.5). ing approaches to this task has been focused on distance-based methods, which quantify the distance (according to some metric), or the similarity, between a given candidate of cognates. The similarity can be either static (e.g. Levenshtein distance) or learned. Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations. See Rama et al. (2018) for a recent evaluation of the performance of several cognates detection algorithms. Several studies have gone beyond the stage of cognates extraction, and used resulted list of cognates to reconstruct the lexicon of protolanguages. Most studies in this direction borrowed techniques from computational phylogeny, drawing a parallel between the hypothesized branching of (latent) proto words into their (observed) current forms an"
2021.naacl-main.353,N01-1020,0,0.174198,"f phonemes (§6.4) and attributes different importance to different daughter languages (§6.5). ing approaches to this task has been focused on distance-based methods, which quantify the distance (according to some metric), or the similarity, between a given candidate of cognates. The similarity can be either static (e.g. Levenshtein distance) or learned. Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations. See Rama et al. (2018) for a recent evaluation of the performance of several cognates detection algorithms. Several studies have gone beyond the stage of cognates extraction, and used resulted list of cognates to reconstruct the lexicon of protolanguages. Most studies in this direction borrowed techniques from computational phylogeny, drawing a parallel between the hypothesized branch"
2021.naacl-main.353,mulloni-pekar-2006-automatic,0,0.0596165,"). ing approaches to this task has been focused on distance-based methods, which quantify the distance (according to some metric), or the similarity, between a given candidate of cognates. The similarity can be either static (e.g. Levenshtein distance) or learned. Once the metric is established, a classification can be performed either based on hard-decision (words below a certain threshold are considered cognates) or by learning a classifier over the distance measures and other features (Kondrak, 2001; Mann and Yarowsky, 2001; Inkpen et al., 2005; Ciobanu and Dinu, 2014a; List et al., 2016); Mulloni and Pekar (2006) have evaluated an alternative approach, in which explicit rules of transformation are derived based on edit operations. See Rama et al. (2018) for a recent evaluation of the performance of several cognates detection algorithms. Several studies have gone beyond the stage of cognates extraction, and used resulted list of cognates to reconstruct the lexicon of protolanguages. Most studies in this direction borrowed techniques from computational phylogeny, drawing a parallel between the hypothesized branching of (latent) proto words into their (observed) current forms and the gradual change of ge"
2021.naacl-main.353,N18-2063,0,0.0530578,"Missing"
2021.naacl-main.353,2020.lrec-1.519,0,0.0132968,"languages in question. Wu and Yarowsky (2018) have automatically constructed cognate datasets for several languages, including Romance languages, and used The related task of cognates detection has been a character-level NMT system to complete missextensively studied. In this task, a set of coging entries (not necessarily the proto-form). Sevnates should be extracted from word lists in diferal works studied the induction of multilingual ferent languages. Most effort in Machine learndictionaries from partial data in related languages. 3 We note that the role of the ML model is easier than that Wu et al. (2020) reconstruct cognates in Austroneof the historical linguist, as it is trained on sets of words that it sian languages (where the proto-language is not took the historical linguistics discipline a considerable effort to acquire. attested). Lewis et al. (2020) employ a mixture4461 2 Related Work of-experts approach for lexical translation induction, combining neural and probabilistic methods, and Nishimura et al. (2020) translate from a multi-source input that contains partial translations to different languages, concatenated. Finally, Ciobanu and Dinu (2018) have applied a CRF model with alignm"
2021.naacl-main.353,L18-1538,0,0.442857,"ranslations. Bouchard-Cˆot´e et al. (2009, 2013) used an extensive dataset of Austronesian languages and their reconstructed proto-languages, and built a parameterized graphical model which models the probability of a phonetic change between a word and its ancestral form; the probability is branch-dependent, allowing for the learning of different trends of change across lineages. While achieving impressive performance, even without necessitating a cognates lists as an input, their model is based on a given phylogeny tree that accurately represents the development of the languages in question. Wu and Yarowsky (2018) have automatically constructed cognate datasets for several languages, including Romance languages, and used The related task of cognates detection has been a character-level NMT system to complete missextensively studied. In this task, a set of coging entries (not necessarily the proto-form). Sevnates should be extracted from word lists in diferal works studied the induction of multilingual ferent languages. Most effort in Machine learndictionaries from partial data in related languages. 3 We note that the role of the ML model is easier than that Wu et al. (2020) reconstruct cognates in Aust"
2021.naacl-main.73,W19-1909,0,0.0265968,"Missing"
adler-etal-2008-tagging,J93-2004,0,\N,Missing
adler-etal-2008-tagging,W07-0808,1,\N,Missing
adler-etal-2008-tagging,P06-1087,1,\N,Missing
adler-etal-2008-tagging,P06-1084,1,\N,Missing
adler-etal-2008-tagging,dejean-2000-evaluate,0,\N,Missing
adler-etal-2008-tagging,W00-0730,0,\N,Missing
C12-1059,W06-2922,0,0.0162269,"bsolute for Czech. The main difference compared to our approach, except for the fact that they use a different transition system, is that their method for finding the optimal transition after the first training round is heuristic and does not guarantee that the best parse is still reachable. Finally, Cohen et al. (2012) tackle the problem of spurious ambiguity for static oracles by eliminating ambiguity from the underlying transition system instead of modifying the oracle. They show how this can be achieved for the arc-standard system of Nivre (2004) as well as the non-projective extension by Attardi (2006). It is still an open question whether their technique can also be applied to the arc-eager system targeted in this paper. 7 Conclusion We have highlighted the shortcoming of traditional static oracles used to train transition-based dependency parsers, and instead proposed the notion of a dynamic oracle, which allows more than one correct transition sequence in the case of spurious ambiguity, and which can predict an optimal transition also for non-optimal configurations. We have defined a concrete dynamic oracle for the arc-eager transition system and showed how it can be used in online train"
C12-1059,E12-1009,0,0.0144274,"ively long time to train. We instead use a simpler online algorithm which can be viewed as a stochastic approximation of the DAgger algorithm, which is itself heavily inspired by the Searn algorithm. Recent work on beam search and structured prediction for transition-based dependency parsing has shown that parsing accuracy can be improved considerably if models are trained to perform beam search instead of greedy one-best search, and if training is done using a global structured learning objective instead of local learning of individual decisions (Zhang and Clark, 2008; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Huang et al., 2012). Like our method, beam search with global structured learning mitigates the effects of error propagation by exploring non-canonical configurations at training time, but the use of a beam reduces parsing speed by a factor that is roughly proportional to the size of the beam, making parsing less efficient. Our method in contrast still trains classifiers to perform local decisions, and thus incurs no efficiency penalty at parsing time, but each local decision is trained to take into account the consequences of previous, possibly erroneous, decisions. Although we may not be a"
C12-1059,P11-2121,0,0.0226701,"vious, possibly erroneous, decisions. Although we may not be able to reach the accuracy level of a beam-search parser, we show that a substantial improvement in accuracy is possible also for a purely deterministic classifier-based parser, making our method suitable for training accurate parsers in situations where maximum efficiency is needed, e.g., when there is a need to process very large corpora. Integrating our dynamic oracle in the training procedure for a transition-based parser with beam search is an interesting question for future work. The work that probably comes closest to ours is Choi and Palmer (2011), who improve the accuracy of a greedy transition-based dependency parser through an iterative training procedure that they call bootstrapping. They start by training a classifier using a standard static oracle for a hybrid transition system combining elements of the arc-eager system and the algorithm of 11 Stacked learning has been explored to some extent in the context of parsing for integrating approximate higher order features as well as for combining the predictions of different parsers (Nivre and McDonald, 2008; Martins et al., 2008). 971 Covington (2001). In a second step, they then use"
C12-1059,de-marneffe-etal-2006-generating,0,0.122602,"Missing"
C12-1059,W11-2925,0,0.0524953,"Missing"
C12-1059,foster-van-genabith-2008-parser,0,0.0369631,"Missing"
C12-1059,N10-1115,1,0.570932,"t for all configurations, including configurations which are not a part of a gold derivation. For configurations which are not part of a gold derivation (and from which the gold tree is not reachable), the dynamic oracle permits all transitions that can lead to a tree with minimum loss compared to the gold tree. In this paper, we provide a provably correct dynamic oracle for the arc-eager transition system of Nivre (2003, 2008). One important use for a dynamic oracle is in training a parser that (a) is not restricted to a 1 This is similar to the parsing oracle used in the EasyFirst parser of Goldberg and Elhadad (2010). 960 ✞ ROOT 0 PRD ✞ P ☎✞ ✞ SBJ ☎ ✞IOBJ ☎ ❄ ❄ ❄ He1 her3 wrote2 DOBJ ☎ ✞ DET ☎ ❄ ❄ a4 letter5 ☎ ❄ .6 Figure 1: Projective dependency tree particular canonical order of transitions and (b) can handle configurations that are not part of any gold sequence, thus mitigating the effect of error propagation. To this end, we provide an online training procedure based on the dynamic oracle that deals with spurious ambiguity by treating all sequences leading to a gold tree as correct, and with error-propagation by exploring transition sequences that are not optimal in the sense that they do not derive t"
C12-1059,D07-1097,1,0.25501,"re thus artificially lower than they could be. We could get better scores for these data sets for all training conditions by training on the Ontonotes corpora instead, but as our main concern is not in achieving the best scores, we opted for the simpler experimental setup. 10 Language-specific tuning is likely to improve results for the other languages as well – we did not perform any language-specific tuning, and it is well established that individual languages parsing accuracies can greatly benefit from tuning of the feature set, the transition system being used and the learning parameters (Hall et al., 2007). 970 6 Related Work Deterministic classifier-based dependency parsing is an instance of independent sequential classification-based structured prediction. In sequential classification models, such as MaximumEntropy Markov Models (McCallum et al., 2000), a structured output is produced by repeated application of a locally trained classifier, where each classification decision is conditioned on the structure created by previous decisions. Several methods have been developed to cope with error propagation in sequential classification, including stacked sequential learning (Cohen and Carvalho, 20"
C12-1059,N12-1015,0,0.0476937,"n. We instead use a simpler online algorithm which can be viewed as a stochastic approximation of the DAgger algorithm, which is itself heavily inspired by the Searn algorithm. Recent work on beam search and structured prediction for transition-based dependency parsing has shown that parsing accuracy can be improved considerably if models are trained to perform beam search instead of greedy one-best search, and if training is done using a global structured learning objective instead of local learning of individual decisions (Zhang and Clark, 2008; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Huang et al., 2012). Like our method, beam search with global structured learning mitigates the effects of error propagation by exploring non-canonical configurations at training time, but the use of a beam reduces parsing speed by a factor that is roughly proportional to the size of the beam, making parsing less efficient. Our method in contrast still trains classifiers to perform local decisions, and thus incurs no efficiency penalty at parsing time, but each local decision is trained to take into account the consequences of previous, possibly erroneous, decisions. Although we may not be able to reach the accu"
C12-1059,P06-1063,0,0.0119919,"Missing"
C12-1059,J93-2004,0,0.0424806,"eech tags in both cases. Since the arc-eager parser can only handle projective dependency trees, all trees in the training set are projectivized before training, using the baseline pseudo-projective transformation in Nivre and Nilsson (2005). However, non-projective trees are kept intact in the test sets for evaluation. We include all ten languages from the CoNLL 2007 shared task: • ARA: Arabic (Hajiˇc et al., 2004) • BAS: Basque (Aduriz et al., 2003) • CAT: Catalan (Martí et al., 2007) • CHI: Chinese (Chen et al., 2003) • CZE: Czech (Hajiˇc et al., 2001; Böhmová et al., 2003) • ENG: English (Marcus et al., 1993) • GRE: Greek (Prokopidis et al., 2005) • HUN: Hungarian (Czendes et al., 2005) 969 ARA BAS Static Dynamic-ambiguity Dynamic-explore 80.60 80.72 83.06 74.10 74.90 76.10 Static Dynamic-ambiguity Dynamic-explore 71.04 71.06 73.54 64.42 65.18 66.77 CAT CHI CZE ENG Unlabeled Attachment Scores 91.21 84.13 78.00 86.24 91.09 83.62 78.38 86.83 92.01 84.65 79.54 88.81 Labeled Attachment Scores 85.96 79.75 69.49 84.90 85.73 79.24 69.39 85.56 86.60 80.74 71.32 87.60 GRE HUN ITA TUR 79.16 79.48 80.66 77.75 76.17 77.10 84.11 84.52 84.77 79.02 78.97 78.84 70.94 71.88 73.83 68.10 66.99 68.23 79.93 80.63 81.0"
C12-1059,D08-1017,0,0.00925318,"work. The work that probably comes closest to ours is Choi and Palmer (2011), who improve the accuracy of a greedy transition-based dependency parser through an iterative training procedure that they call bootstrapping. They start by training a classifier using a standard static oracle for a hybrid transition system combining elements of the arc-eager system and the algorithm of 11 Stacked learning has been explored to some extent in the context of parsing for integrating approximate higher order features as well as for combining the predictions of different parsers (Nivre and McDonald, 2008; Martins et al., 2008). 971 Covington (2001). In a second step, they then use this classifier to parse the training corpus, creating one new training instance for every configuration visited during parsing, using an adapted version of the static oracle to predict the optimal transition for each configuration. They iterate this procedure as long as parsing accuracy improves on a held-out development set and report improvements in parsing accuracy of about 0.5 percent absolute for English and almost 2 percent absolute for Czech. The main difference compared to our approach, except for the fact that they use a differe"
C12-1059,D07-1013,1,0.575417,"d tree, because it does not eliminate any gold arc that is still reachable – the cost of the incorrect attachment is already accounted for in the cost of the erroneous SHIFT action. After defining the concept of a dynamic oracle which is correct over the entire configuration space of a transition system and providing a concrete instantiation of it for the arc-eager transition system, we now go on to present one useful application of such an oracle. 4 Training Parsers with the Dynamic Oracle Greedy transition-based parsers trained with static oracles are known to suffer from error propagation (McDonald and Nivre, 2007). We may hope to mitigate the error propagation problem by letting the parser explore larger portions of the configuration space during training and learn how to behave optimally also after committing previous errors. While this is not possible with the usual static oracles, the dynamic oracle defined above allows us to do just that, as it returns a set of optimal transitions for each possible configuration. Algorithm 2 is a standard online training algorithm for transition-based dependency parsers using a static oracle. Given a training sentence x with gold tree Ggold , it starts in the initi"
C12-1059,W03-3017,1,0.615468,"quences from gold parse trees. These sequences can then be used as training data for a classifier that approximates the oracle at parsing time in deterministic classifier-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), or it can be used to determine when to perform updates in online training of a beam search parser (Zhang and Clark, 2008). Currently, such oracles work by translating a given tree to a static sequence of parser transitions which, if run in sequence, will produce the gold tree. Most transition systems, including the arc-eager and arc-standard systems described in Nivre (2003, 2004), exhibit spurious ambiguity and map several sequences to the same gold tree. In such cases, the oracles implicitly define a canonical derivation order. We call such oracles static, because they produce a single static sequence of transitions that is supposed to be followed in its entirety. Static oracles are usually specified as rules over individual parser configurations – if the configuration has properties X and the gold tree is Y , then the correct transition is Z – giving the impression that they define a function from configurations to transitions. However, these rules are only c"
C12-1059,W04-0308,1,0.438224,"0.5 percent absolute for English and almost 2 percent absolute for Czech. The main difference compared to our approach, except for the fact that they use a different transition system, is that their method for finding the optimal transition after the first training round is heuristic and does not guarantee that the best parse is still reachable. Finally, Cohen et al. (2012) tackle the problem of spurious ambiguity for static oracles by eliminating ambiguity from the underlying transition system instead of modifying the oracle. They show how this can be achieved for the arc-standard system of Nivre (2004) as well as the non-projective extension by Attardi (2006). It is still an open question whether their technique can also be applied to the arc-eager system targeted in this paper. 7 Conclusion We have highlighted the shortcoming of traditional static oracles used to train transition-based dependency parsers, and instead proposed the notion of a dynamic oracle, which allows more than one correct transition sequence in the case of spurious ambiguity, and which can predict an optimal transition also for non-optimal configurations. We have defined a concrete dynamic oracle for the arc-eager trans"
C12-1059,J08-4003,1,0.960357,"er outperforms greedy parsers trained using conventional oracles on a range of data sets, with an average improvement of over 1.2 LAS points and up to almost 3 LAS points on some data sets. KEYWORDS: dependency parsing, transition system, oracle. Proceedings of COLING 2012: Technical Papers, pages 959–976, COLING 2012, Mumbai, December 2012. 959 1 Introduction The basic idea in transition-based dependency parsing is to define a nondeterministic transition system for mapping sentences to dependency trees and to perform parsing as search for the optimal transition sequence for a given sentence (Nivre, 2008). A key component in training transition-based parsers is an oracle, which is used to derive optimal transition sequences from gold parse trees. These sequences can then be used as training data for a classifier that approximates the oracle at parsing time in deterministic classifier-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), or it can be used to determine when to perform updates in online training of a beam search parser (Zhang and Clark, 2008). Currently, such oracles work by translating a given tree to a static sequence of parser transitions which, if run in sequence, w"
C12-1059,W04-2407,1,0.239448,"12. 959 1 Introduction The basic idea in transition-based dependency parsing is to define a nondeterministic transition system for mapping sentences to dependency trees and to perform parsing as search for the optimal transition sequence for a given sentence (Nivre, 2008). A key component in training transition-based parsers is an oracle, which is used to derive optimal transition sequences from gold parse trees. These sequences can then be used as training data for a classifier that approximates the oracle at parsing time in deterministic classifier-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), or it can be used to determine when to perform updates in online training of a beam search parser (Zhang and Clark, 2008). Currently, such oracles work by translating a given tree to a static sequence of parser transitions which, if run in sequence, will produce the gold tree. Most transition systems, including the arc-eager and arc-standard systems described in Nivre (2003, 2004), exhibit spurious ambiguity and map several sequences to the same gold tree. In such cases, the oracles implicitly define a canonical derivation order. We call such oracles static, because they produce a single sta"
C12-1059,P08-1108,1,0.681003,"sting question for future work. The work that probably comes closest to ours is Choi and Palmer (2011), who improve the accuracy of a greedy transition-based dependency parser through an iterative training procedure that they call bootstrapping. They start by training a classifier using a standard static oracle for a hybrid transition system combining elements of the arc-eager system and the algorithm of 11 Stacked learning has been explored to some extent in the context of parsing for integrating approximate higher order features as well as for combining the predictions of different parsers (Nivre and McDonald, 2008; Martins et al., 2008). 971 Covington (2001). In a second step, they then use this classifier to parse the training corpus, creating one new training instance for every configuration visited during parsing, using an adapted version of the static oracle to predict the optimal transition for each configuration. They iterate this procedure as long as parsing accuracy improves on a held-out development set and report improvements in parsing accuracy of about 0.5 percent absolute for English and almost 2 percent absolute for Czech. The main difference compared to our approach, except for the fact"
C12-1059,P05-1013,1,0.317118,"entire QuestionBank (Judge et al., 2006). • ANS, EML, GRPS, REVS, BLGS: the question-answers, emails, newsgroups, reviews and weblogs portions of the English Web Treebank (Bies et al., 2012; Petrov and McDonald, 2012). The CoNLL models are trained on the dedicated training set for each of the ten languages and evaluated on the corresponding test set, with gold standard part-of-speech tags in both cases. Since the arc-eager parser can only handle projective dependency trees, all trees in the training set are projectivized before training, using the baseline pseudo-projective transformation in Nivre and Nilsson (2005). However, non-projective trees are kept intact in the test sets for evaluation. We include all ten languages from the CoNLL 2007 shared task: • ARA: Arabic (Hajiˇc et al., 2004) • BAS: Basque (Aduriz et al., 2003) • CAT: Catalan (Martí et al., 2007) • CHI: Chinese (Chen et al., 2003) • CZE: Czech (Hajiˇc et al., 2001; Böhmová et al., 2003) • ENG: English (Marcus et al., 1993) • GRE: Greek (Prokopidis et al., 2005) • HUN: Hungarian (Czendes et al., 2005) 969 ARA BAS Static Dynamic-ambiguity Dynamic-explore 80.60 80.72 83.06 74.10 74.90 76.10 Static Dynamic-ambiguity Dynamic-explore 71.04 71.06"
C12-1059,W03-3023,0,0.170531,"NG 2012, Mumbai, December 2012. 959 1 Introduction The basic idea in transition-based dependency parsing is to define a nondeterministic transition system for mapping sentences to dependency trees and to perform parsing as search for the optimal transition sequence for a given sentence (Nivre, 2008). A key component in training transition-based parsers is an oracle, which is used to derive optimal transition sequences from gold parse trees. These sequences can then be used as training data for a classifier that approximates the oracle at parsing time in deterministic classifier-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), or it can be used to determine when to perform updates in online training of a beam search parser (Zhang and Clark, 2008). Currently, such oracles work by translating a given tree to a static sequence of parser transitions which, if run in sequence, will produce the gold tree. Most transition systems, including the arc-eager and arc-standard systems described in Nivre (2003, 2004), exhibit spurious ambiguity and map several sequences to the same gold tree. In such cases, the oracles implicitly define a canonical derivation order. We call such oracles static, because they"
C12-1059,D08-1059,0,0.782299,"system for mapping sentences to dependency trees and to perform parsing as search for the optimal transition sequence for a given sentence (Nivre, 2008). A key component in training transition-based parsers is an oracle, which is used to derive optimal transition sequences from gold parse trees. These sequences can then be used as training data for a classifier that approximates the oracle at parsing time in deterministic classifier-based parsing (Yamada and Matsumoto, 2003; Nivre et al., 2004), or it can be used to determine when to perform updates in online training of a beam search parser (Zhang and Clark, 2008). Currently, such oracles work by translating a given tree to a static sequence of parser transitions which, if run in sequence, will produce the gold tree. Most transition systems, including the arc-eager and arc-standard systems described in Nivre (2003, 2004), exhibit spurious ambiguity and map several sequences to the same gold tree. In such cases, the oracles implicitly define a canonical derivation order. We call such oracles static, because they produce a single static sequence of transitions that is supposed to be followed in its entirety. Static oracles are usually specified as rules"
C12-1059,P11-2033,1,0.820458,"ta set and take a relatively long time to train. We instead use a simpler online algorithm which can be viewed as a stochastic approximation of the DAgger algorithm, which is itself heavily inspired by the Searn algorithm. Recent work on beam search and structured prediction for transition-based dependency parsing has shown that parsing accuracy can be improved considerably if models are trained to perform beam search instead of greedy one-best search, and if training is done using a global structured learning objective instead of local learning of individual decisions (Zhang and Clark, 2008; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Huang et al., 2012). Like our method, beam search with global structured learning mitigates the effects of error propagation by exploring non-canonical configurations at training time, but the use of a beam reduces parsing speed by a factor that is roughly proportional to the size of the beam, making parsing less efficient. Our method in contrast still trains classifiers to perform local decisions, and thus incurs no efficiency penalty at parsing time, but each local decision is trained to take into account the consequences of previous, possibly erroneous, decisions. A"
C12-1059,D07-1096,1,\N,Missing
C16-1256,W13-3520,0,0.0298454,"orrect ones. this work we use MTL to improve preposition-sense disambiguation, by using an auxiliary multilingual task – predicting translations of prepositions. A simple method for sharing information in transfer learning as well as in MTL, is using representations that are shared between related tasks. Representation learning (Bengio et al., 2013) is a closely related field that aims to establish techniques for learning robust and expressive data representations. A well-known effort in this field is that of learning word embeddings for use in a wide range of NLP tasks (Mikolov et al., 2013; Al-Rfou et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). While those representations are highly effective in many cases, other scenarios require representations of a full sentence, or of a context around a target word, rather than representations of single words. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context rep"
C16-1256,D15-1075,0,0.0267844,"ds. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan et al., 1991); word sense disambiguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and"
C16-1256,D08-1092,0,0.0336936,"ta is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific word ˇ embeddings instead of the common word-form specific embeddings (Ettinger et al., 2016; Suster et al., 2016), and also on representing words using their context (Kawakami and Dyer, 2015; Hermann and Blunsom, 2013). While we rely on the assumption most of these works have in common, according to which translations may serve as a strong"
C16-1256,W10-2906,0,0.017222,"ng LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan et al., 1991); word sense disambiguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints"
C16-1256,D14-1110,0,0.040925,"f learning word embeddings for use in a wide range of NLP tasks (Mikolov et al., 2013; Al-Rfou et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). While those representations are highly effective in many cases, other scenarios require representations of a full sentence, or of a context around a target word, rather than representations of single words. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, an"
C16-1256,P91-1017,0,0.753412,"derstanding the meaning of the text. This important semantic task is especially challenging from a learning perspective as only little amounts of annotated training data are available for it. Indeed, previous systems (see Sections 2.1.1 and 5.4) make extensive use of the vast and human-curated WordNet lexicon (Miller, 1995) in order to compensate for the small size of the annotated data and obtain good accuracies. Instead, we propose to deal with the scarcity of annotated data by taking a semi-supervised approach. We rely on the intuition that word ambiguity tends to differ between languages (Dagan et al., 1991), and show that multilingual corpora can provide a good signal for the preposition sense disambiguation task. Multilingual corpora are vast and relatively easy to obtain (Resnik and Smith, 2003; Koehn, 2005; Steinberger et al., 2006), making them appealing candidates for use in a semi-supervised setting. Our approach (Section 4) is based on representation learning (Bengio et al., 2013), and can also be seen as an instance of multi-task (Caruana, 1997), or transfer learning (Pan and Yang, 2010). First, we train an LSTM-based neural network (Hochreiter and Schmidhuber, 1997) to predict a foreign"
C16-1256,D09-1047,0,0.236894,"Missing"
C16-1256,P02-1033,0,0.0343319,"sentation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan et al., 1991); word sense disambiguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky"
C16-1256,P10-4002,0,0.0113594,"rediction model, we use early stopping and choose the best performing model on the development set. The sense-prediction MLP uses ReLU activation, and foreign preposition MLPs use tanh, with no bias terms. Unless noted otherwise, we use randomly initialized embedding vectors. For each experiment, we chose the parameters that maximized the accuracy on the dev set.7 The accuracies we report are the average accuracies over 5 different seeds. 4 Bulgarian, Czech, Danish, German, Greek, Spanish, French, Hungarian, Italian, Polish, Romanian and Swedish. Word-alignment is done using the cdec aligner (Dyer et al., 2010). 6 https://github.com/clab/cnn 7 In most of the experiments, the best results are achieved when the hidden-layer of the sense-prediction MLPs is of the size 500, and the preposition embedding is of size 200. In some cases, the best results are achieved with different dimensions. 5 2721 French prepositions German prepositions Spanish prepositions dans, en, sur, ..., par M LPF R mit, vor, zu, ..., gegen sobre, con, para, ..., a M LPGE M LPSP Prepositions supersenses Temporal, Place, Manner, ..., Explanation M LPsense φ(he booked a ... ,5) context representation he booked a room for two nights F"
C16-1256,N16-1163,0,0.0364261,"f-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific word ˇ embeddings instead of the common word-form specific embeddings (Ettinger et al., 2016; Suster et al., 2016), and also on representing words using their context (Kawakami and Dyer, 2015; Hermann and Blunsom, 2013). While we rely on the assumption most of these works have in common, according to which translations may serve as a strong signal for different senses of words, the novelty of our work is in focusing on prepositions rather than content words, and in jointly representing a context for both a multilingual and a monolingual tasks, which results in an improvement of the monolingual model. 2726 7 Conclusions and Future Work We show that multilingual data can be used to imp"
C16-1256,E14-1049,0,0.0225563,"successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific word ˇ embeddings instead of the common word-form specific embeddings (Ettinger et al., 2016; Suster et al., 2016), and also on representing words using their context (Kawakami and Dyer, 2015; Hermann and Blunsom, 2013). While we rely on the assumption most of these works have in common, according to which translations may serve as a strong signal for different senses of words, the novelty of our work is in focusing on prepositions rather than content words, and in jointly representing a context for both a multilingual and a monolingual tasks,"
C16-1256,P09-1042,0,0.0172954,"lingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan et al., 1991); word sense disambiguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific"
C16-1256,C10-2052,0,0.343874,"compares our SemEval results with those of previous systems. The system of Ye and Baldwin (2007) got the highest result out of the three participating systems in the SemEval 2007 shared task. They extracted features such as POS tags and WordNet-based features, and also high level features (e.g semantic role tags), using a word window of up to seven words, in a Maximum Entropy classifier. Tratz and Hovy (2009) got a higher result with similar features by using a set of positions that are syntactically related to the preposition instead of a fixed window size. The best performing systems are of Hovy et al (2010) and of Srikumar and Roth (2013b). Both systems rely on vast and thoroughly-engineered feature sets, including many WordNet based features. Hovy et al (2010) explored different word choices (i.e, a fixed window vs. syntactically related words) and different methods of extracting them, while Srikumar and Roth (2013b) improved performance by jointly predicting preposition senses and relations. In contrast, our models do not include any WordNet based features, making them applicable also for languages lacking such resources. Our models achieve competitive results, outperforming most previous syst"
C16-1256,P12-1092,0,0.0605013,"ons. A well-known effort in this field is that of learning word embeddings for use in a wide range of NLP tasks (Mikolov et al., 2013; Al-Rfou et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). While those representations are highly effective in many cases, other scenarios require representations of a full sentence, or of a context around a target word, rather than representations of single words. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving mo"
C16-1256,2005.mtsummit-papers.11,0,0.314017,"ms (see Sections 2.1.1 and 5.4) make extensive use of the vast and human-curated WordNet lexicon (Miller, 1995) in order to compensate for the small size of the annotated data and obtain good accuracies. Instead, we propose to deal with the scarcity of annotated data by taking a semi-supervised approach. We rely on the intuition that word ambiguity tends to differ between languages (Dagan et al., 1991), and show that multilingual corpora can provide a good signal for the preposition sense disambiguation task. Multilingual corpora are vast and relatively easy to obtain (Resnik and Smith, 2003; Koehn, 2005; Steinberger et al., 2006), making them appealing candidates for use in a semi-supervised setting. Our approach (Section 4) is based on representation learning (Bengio et al., 2013), and can also be seen as an instance of multi-task (Caruana, 1997), or transfer learning (Pan and Yang, 2010). First, we train an LSTM-based neural network (Hochreiter and Schmidhuber, 1997) to predict a foreign (say, French) preposition given the context of an English preposition. This trains the network to map contexts of English prepositions to representations that are predictive of corresponding foreign prepos"
C16-1256,P14-2050,1,0.91331,"s, using monolingual and bilingual models, along with the improvement over the base model and the number of training examples in each language. Numbers in brackets indicate the min and max accuracy across seeds. instead of randomly initialized word embeddings. We perform three experiments: 1. using external word embeddings only for the words that are fed into the context-encoder. 2. using external word embeddings only for the lemmas of the features. 3. using external word embeddings for both. We use two sets of word embeddings: 5-window-bag-of-words-based and dependency-based, both trained by Levy and Goldberg (2014) on English Wikipedia.8 As shown in Table 3, both pre-trained embeddings improve the performance of all models in most cases. In all cases, the multilingual model outperforms the base model and the context model, both achieving similar results. Using external word embeddings for both the features and the context-encoder helps the most. The best result of 78.55 is achieved by the multilingual model, improving the result of the base model under the same conditions by 1.71 points. Context-encoder embeddings only Bow Deps base 73.34 (71.63-73.97) 73.34 (71.63-73.97) +context 74.07 (72.10-75.15) 74"
C16-1256,S07-1005,0,0.6705,"ense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised prepositionsense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets. 1 Introduction Preposition-sense disambiguation (Litkowski and Hargraves, 2005; Litkowski and Hargraves, 2007; Schneider et al., 2015; Schneider et al., 2016), is the task of assigning a category to a preposition in context (see Section 2.1). Choosing the correct sense of a preposition is crucial for understanding the meaning of the text. This important semantic task is especially challenging from a learning perspective as only little amounts of annotated training data are available for it. Indeed, previous systems (see Sections 2.1.1 and 5.4) make extensive use of the vast and human-curated WordNet lexicon (Miller, 1995) in order to compensate for the small size of the annotated data and obtain good"
C16-1256,W15-1501,0,0.0198428,"range of NLP tasks (Mikolov et al., 2013; Al-Rfou et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). While those representations are highly effective in many cases, other scenarios require representations of a full sentence, or of a context around a target word, rather than representations of single words. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan"
C16-1256,K16-1006,0,0.0358448,"ipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan et al., 1991); word sense disambiguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to"
C16-1256,D14-1162,0,0.0791239,"eposition-sense disambiguation, by using an auxiliary multilingual task – predicting translations of prepositions. A simple method for sharing information in transfer learning as well as in MTL, is using representations that are shared between related tasks. Representation learning (Bengio et al., 2013) is a closely related field that aims to establish techniques for learning robust and expressive data representations. A well-known effort in this field is that of learning word embeddings for use in a wide range of NLP tasks (Mikolov et al., 2013; Al-Rfou et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014). While those representations are highly effective in many cases, other scenarios require representations of a full sentence, or of a context around a target word, rather than representations of single words. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary leng"
C16-1256,S07-1040,0,0.678909,"Missing"
C16-1256,J03-3002,0,0.0347883,". Indeed, previous systems (see Sections 2.1.1 and 5.4) make extensive use of the vast and human-curated WordNet lexicon (Miller, 1995) in order to compensate for the small size of the annotated data and obtain good accuracies. Instead, we propose to deal with the scarcity of annotated data by taking a semi-supervised approach. We rely on the intuition that word ambiguity tends to differ between languages (Dagan et al., 1991), and show that multilingual corpora can provide a good signal for the preposition sense disambiguation task. Multilingual corpora are vast and relatively easy to obtain (Resnik and Smith, 2003; Koehn, 2005; Steinberger et al., 2006), making them appealing candidates for use in a semi-supervised setting. Our approach (Section 4) is based on representation learning (Bengio et al., 2013), and can also be seen as an instance of multi-task (Caruana, 1997), or transfer learning (Pan and Yang, 2010). First, we train an LSTM-based neural network (Hochreiter and Schmidhuber, 1997) to predict a foreign (say, French) preposition given the context of an English preposition. This trains the network to map contexts of English prepositions to representations that are predictive of corresponding f"
C16-1256,W15-1612,0,0.0466556,"all, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised prepositionsense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets. 1 Introduction Preposition-sense disambiguation (Litkowski and Hargraves, 2005; Litkowski and Hargraves, 2007; Schneider et al., 2015; Schneider et al., 2016), is the task of assigning a category to a preposition in context (see Section 2.1). Choosing the correct sense of a preposition is crucial for understanding the meaning of the text. This important semantic task is especially challenging from a learning perspective as only little amounts of annotated training data are available for it. Indeed, previous systems (see Sections 2.1.1 and 5.4) make extensive use of the vast and human-curated WordNet lexicon (Miller, 1995) in order to compensate for the small size of the annotated data and obtain good accuracies. Instead, we"
C16-1256,W16-1712,0,0.0935691,"Missing"
C16-1256,W04-3207,0,0.0507196,"loiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific word ˇ embeddings instead of the common word-form specific embeddings (Ettinger et al., 2016; Suster et al., 2016), and also on representing words using their context (Kawakami and Dyer, 2015; Hermann and Blunsom, 2013). While we rely on the assumption most of these works have in common, according to which translati"
C16-1256,Q13-1019,0,0.360653,"lts with those of previous systems. The system of Ye and Baldwin (2007) got the highest result out of the three participating systems in the SemEval 2007 shared task. They extracted features such as POS tags and WordNet-based features, and also high level features (e.g semantic role tags), using a word window of up to seven words, in a Maximum Entropy classifier. Tratz and Hovy (2009) got a higher result with similar features by using a set of positions that are syntactically related to the preposition instead of a fixed window size. The best performing systems are of Hovy et al (2010) and of Srikumar and Roth (2013b). Both systems rely on vast and thoroughly-engineered feature sets, including many WordNet based features. Hovy et al (2010) explored different word choices (i.e, a fixed window vs. syntactically related words) and different methods of extracting them, while Srikumar and Roth (2013b) improved performance by jointly predicting preposition senses and relations. In contrast, our models do not include any WordNet based features, making them applicable also for languages lacking such resources. Our models achieve competitive results, outperforming most previous systems, despite using relatively f"
C16-1256,N16-1160,0,0.135913,"Missing"
C16-1256,N09-3017,0,0.668434,"lementation we use the long short-term memory network (LSTM), a subtype of the RNN (Hochreiter and Schmidhuber, 1997). LST M (w1:i ) is the output vector resulting from inputing the items w1 , ..., wi into the LSTM in order. 3 Monolingual Preposition Sense Classification We start by describing an MLP-based model for classifying prepositions to their senses. For an English sentence s = w1 , ..., wn and a preposition position i,2 we classify to the sense y as: y = argmax M LPsense (φ(s, i))[j] j where φ(s, i) is a feature vector composed of 19 features. The features are based on the features of Tratz and Hovy (2009), and are similar in spirit to those used in previous attempts at preposition sense disambiguation. We deliberately do not include WordNet based features, as we want to focus on features that do not require extensive human-curated resources. This makes our model applicable for use in other languages with minimal change. We use the following features: (1) The embedding of the preposition. (2) The embeddings of the lemmas of the two words before and after the preposition, of the head of the preposition in the dependency tree, and of the first modifier of the preposition. (3) The embeddings of th"
C16-1256,N01-1026,0,0.119665,"ik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific word ˇ embeddings instead of the common word-form specific embeddings (Ettinger et al., 2016; Suster et al., 2016), and also on representing words using their context (Kawakami and Dy"
C16-1256,H01-1035,0,0.06036,"iguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approach assumes a good model exists in one language, and uses annotations in that language in order to constrain possible annotations in another. Projections were successfully used for dependency grammar induction (Ganchev et al., 2009), and for transferring tools such as morphological analyzers and part-of-speech taggers from English to languages with fewer resources (Yarowsky et al., 2001; Yarowsky and Ngai, 2001). A different approach is applying multilingual constraints on existing monolingual models, as done for parsing (Smith and Smith, 2004; Burkett and Klein, 2008) and for morphological segmentation (Snyder and Barzilay, 2008). Of much relevance to this work are also previous attempts to improve monolingual representations using bilingual data (Faruqui and Dyer, 2014). Previous works focus on creating sense-specific word ˇ embeddings instead of the common word-form specific embeddings (Ettinger et al., 2016; Suster et al., 2016), and also on representing words using thei"
C16-1256,S07-1051,0,0.156561,"+both contexts Web-reviews Corpus Average Ensemble 76.84 (76.32-77.26) 77.61 77.73 (77.14-78.43) 78.90 78.55 (77.37-79.37) 80.54 79.34 (78.43-80.19) 79.84 SemEval Corpus Average Ensemble 77.1 (76.9-77.2) 79.5 79.5 (78.8-79.9) 81.1 79.6 (79.3-79.9) 81.2 80.0 (79.8-80.2) 81.7 Table 6: The results on both datasets on 5 different seeds as reported in Tables 3 and 5 in comparison to the results using the ensemble. Numbers in brackets indicate the min and max accuracy across seeds. 5.4 Comparison to previous systems Table 7 compares our SemEval results with those of previous systems. The system of Ye and Baldwin (2007) got the highest result out of the three participating systems in the SemEval 2007 shared task. They extracted features such as POS tags and WordNet-based features, and also high level features (e.g semantic role tags), using a word window of up to seven words, in a Maximum Entropy classifier. Tratz and Hovy (2009) got a higher result with similar features by using a set of positions that are syntactically related to the preposition instead of a fixed window size. The best performing systems are of Hovy et al (2010) and of Srikumar and Roth (2013b). Both systems rely on vast and thoroughly-eng"
C16-1256,S07-1044,0,0.111203,"Missing"
C16-1256,P15-1109,0,0.0172784,"word, rather than representations of single words. Contexts are often represented by some manipulation over the embeddings of their words. Such representations have been successfully used for tasks such as context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014) and lexical substitution (Melamud et al., 2015). An alternative approach for context representation is encoding a context of arbitrary length into a single vector using LSTMs. This approach has been proven to outperform the previous attempts in a variety of tasks such as Semantic Role Labeling (Zhou and Xu, 2015), Natural Language Inference (Bowman et al., 2015) and Sentence Completion (Melamud et al., 2016). We follow the LSTM-based approach for context representation. Learning from multilingual data The use of multilingual data for improving monolingual tasks has a long tradition in NLP, and has been used for target word selection (Dagan et al., 1991); word sense disambiguation (Diab and Resnik, 2002); and syntactic parsing and named entity recognition (Burkett et al., 2010), to name a few examples. A dominant approach for exploiting multilingual data is that of crosslingual projection. This approac"
C16-1256,J09-2001,0,\N,Missing
C18-2013,P14-2050,1,0.891021,"is expected to include additional personal assistant application terms such as ‘Amazon Echo’ and ‘Google Now’. Many NLP-based information extraction applications, such as relation extraction or document matching, require the extraction of terms belonging to fine-grained semantic classes as a basic building block. A practical approach to extracting such terms is to apply a term set expansion system. The input seed set for such systems may contain as few as two to ten terms which is practical to obtain. SetExpander uses a corpus-based approach based on the distributional similarity hypothesis (Harris, 1954), stating that semantically similar words appear in similar contexts. Linear bag-of-words context is widely used to compute semantic similarity. However, it typically captures more topical and less functional similarity, while for the purpose of set expansion, we need to capture more functional and less topical similarity.2 For example, given a seed term like the programming language ’Python’, we would like the expanded set to include other programming languages with similar characteristics, but we would not like it to include terms like ‘bytecode’ or ‘high-level programming language’ despite"
C18-2013,K15-1026,0,0.0574255,"Missing"
D09-1119,J04-4004,0,0.0251548,"Missing"
D09-1119,P05-1010,0,0.0389206,"Missing"
D09-1119,P98-1034,0,0.0212369,"ng to the same phrase. These tasks are traditionally reduced to a tagging task, in which each word is to be classified as either Beginning a span, Inside a span, or Outside of a span. The decision is based on the word to be classified and its neighbors. Features supporting the classification usually include ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exac"
D09-1119,nivre-etal-2006-maltparser,0,0.563552,"his paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, c"
D09-1119,A00-2018,0,0.0594379,"Missing"
D09-1119,P06-1055,0,0.0254728,"Missing"
D09-1119,P97-1003,0,0.0370144,"Missing"
D09-1119,P05-1045,0,0.0117487,"Missing"
D09-1119,W01-0521,0,0.0363145,"Missing"
D09-1119,P07-1029,1,0.914318,"act word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP We show that by using a variant of SVM – Anchored SVM Learning (Goldberg and Elhadad, 2007) with a polynomial kernel, one can learn accurate models for English NP-chunking (Marcus and Ramshaw, 1995), base-phrase chunking (Co"
D09-1119,P06-1087,1,0.890779,"strate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the P learner is to minimize 21 ||w||2 + C i ξi subject to some margin constraints, where w is a weight vector to be learned and ξi is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM 1P i=1 αi − 2 i,j αi αj yi yj K(xi , xj ) Another variant is L2-penalty SVM (Koshiba and Abe, 2003), in which there is a quadratic penalty for misclassified instances. Here, the learning objective is to minimize: 1 1 P 2 2 i ξi or"
D09-1119,C02-1054,0,0.0367722,"y more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the P learner is to minimize 21 ||w||2 + C i ξi subject to some margin constraints, where w is a weight vector to be learned and ξi is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM 1P i=1 αi − 2 i,j αi αj yi yj K(xi , xj ) Another variant is L2-penalty SVM (Koshiba and Abe, 2003), in which there is a quadratic penalty for misclassified instances. Here, the learning objective is to min"
D09-1119,W09-1119,0,0.0294297,"Missing"
D09-1119,W00-0726,0,0.707134,"sification usually include ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVM"
D09-1119,N03-1028,0,0.354089,"Missing"
D09-1119,W02-2024,0,0.330806,"ankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very"
D09-1119,P01-1069,0,0.0667811,"Missing"
D09-1119,P07-2052,0,0.0232416,"Missing"
D09-1119,P03-1054,0,0.0317059,"Missing"
D09-1119,W00-0730,0,0.45439,"his belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processin"
D09-1119,N01-1025,0,0.11676,"exical features is not explained by the richness of information such rare features bring to the model. Instead, we believe that rare lexical features help the classifier because they make the data artificially more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the P learner is to minimize 21 ||w||2 + C i ξi subject to some margin constraints, where w is a weight vector to be learned and ξi is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM"
D09-1119,W95-0107,0,0.0274721,"ve an F-score of 90.9. This dataset proved to be quite resilient to feature pruning. Pruning features appearing less than 100 times results in just a slight decrease in F-score. Extremely aggressive pruning, keeping only features appearing more than 1,000 or 1,500 times in the training data, results in a big drop in F-score for the soft-margin SVM (from about 91 to 86). Much less so for the Anchored-SVM. Using Anchored SVM we achieve an F-score of 90.1 after pruning with k = 1, 000. This model has 1207 active features, and 27 unique active lexical forms. 5.2 NP Chunking The goal of this task (Marcus and Ramshaw, 1995) is the identification of non-recursive NPs. We use the data from the CoNLL 2000 shared task: NP chunks are extracted from Sections 15-18 (train) and 20 (test) of the Penn WSJ corpus. POS tagged are automatically assigned by the Brill Tagger. Features: We consider the POS and word-form of each token. P RUNING 0 1 2 5 10 20 50 100 #F EATURES 92,805 46,527 32,583 18,092 10,812 5,952 2,436 1,168 S OFT-M ARGIN 94.12 93.78 93.58 93.42 93.00 92.48 92.33 91.94 A NCHORED 94.08 94.09 94.00 94.01 93.98 93.92 93.96 93.83 Table 2: NP-Chunking results (F-score), with various pruning thresholds. Results are"
D09-1119,W03-0419,0,\N,Missing
D09-1119,C98-1034,0,\N,Missing
D15-1158,P14-2131,0,0.284823,"with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains. 2 Our Approach Our departure point is a graph-based parsing model (McDonald et al., 2005): parse(x) = argmax score(x, y) y∈Y(x) score(x, y) = w · Φ(x, y) = X w · φ(x, part) part∈y Given a sentence x we look for the highest-scoring parse tree y in the space Y(x) of valid dependency trees over x. The score of a tree is determined by a linear model param"
D15-1158,D14-1082,0,0.0908682,"e use of Brown-clusters without using Brown-clusters as a component. As expected, combining our features and the Brown-based features provide an additional improvement, as can be seen in the last block of Table 2 (Base+Brown+TRIP). 4 Related Work Semi-supervised approaches to dependency parsing can be roughly categorized into two groups: those that use unannotated data and those that use automatically-parsed data. Our proposed method falls in the second group. Among the words that use unannotated data, the dominant approach is to derive either word clusters (Koo et al., 2008) or word vectors (Chen and Manning, 2014) based on unparsed data, and use these as additional features for a supervised parsing model. While the word representations used in such methods are not specifically designed for the parsing task, they do provide useful features for parsing, and in particular the method of (Koo et al., 2008), relying on features derived using the Brown-clustering algorithm, provides very competitive state-of-the-art results. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component. Among th"
D15-1158,D09-1060,0,0.163516,"ased on parsing large amounts of unannotated text using a baseline parser, extracting word-interaction statistics from the automatically parsed corpus, and using these statistics as the basis of additional parser features. The automatically-parsed data is used to acquire statistics about lexical interactions, which are too sparse to estimate well from any realistically-sized Treebank. Specifically, we attempt to infer a function assoc(head, modif er) measuring the “goodness” of head-modifier relations (“how good is an arc in which black is a modifier of jump”). A similar approach was taken by Chen et al. (2009) and Van Noord et al. (2007). We depart from their work by extending the scoring to include a wider lexical context. That is, given the sentence fragment in Figure 1, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learning a function between word triplets raises an extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector representations (Mikolov et al., 2013a; Mni"
D15-1158,C14-1078,0,0.0543724,"2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to capture interactions between words. As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur. Another recent approach that takes into account various syntactic interactions was recently introduced by Chen et al. (2014), who propose to learn to embed complex features that are being used in a graph-based parser based on other features they co-occur with in auto-parsed data. Similar to our approach, the embedded features are then used as additional features in a conventional graph-based model. The approaches are to a large extent complementary, and could be combined. Finally, our work adds additional features to a graph-based parser which is based on a linearmodel. Recently, progress in dependency parsing has been made by introducing non-linear, neuralnetwork based models (Pei et al., 2015; Chen and Manning, 2"
D15-1158,W08-1301,0,0.0276038,"Missing"
D15-1158,P05-1012,0,0.323464,"uracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains. 2 Our Approach Our departure point is a graph-based parsing model (McDonald et al., 2005): parse(x) = argmax score(x, y) y∈Y(x) score(x, y) = w · Φ(x, y) = X w · φ(x, part) part∈y Given a sentence x we look for the highest-scoring parse tree y in the space Y(x) of valid dependency trees over x. The score of a tree is determined by a linear model parameterized by a weights vector w, and a feature function Φ(x, y). To make the search 1348 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1348–1353, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Features in φij lex (x, y) ij bin(S (x, y)) bin(S ij"
D15-1158,P13-2017,1,0.891373,"Missing"
D15-1158,P15-1031,0,0.0356498,"Missing"
D15-1158,D10-1069,0,0.0136852,"lts. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representat"
D15-1158,P15-1033,0,0.0521341,"Missing"
D15-1158,W09-3829,0,0.0183676,"-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to capture interactions between words. As discussed in sections (1) and (2), most similar to ours is the work of (Chen et al., 2009; Van Noord, 2007). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur. Another recent approach that takes into account various syntactic interactions was recent"
D15-1158,D07-1111,0,0.0386304,"ng derived features without using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contra"
D15-1158,P08-1068,0,0.816138,"esentations (Mikolov et al., 2013a; Mnih and Kavukcuoglu, 2013). Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct countbased statistics achieve roughly the same results (Section 3). Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based features (Koo et al., 2008), we find that our tripletsbased method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. (Bansal et al., 2014)) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by replacing them. As expected, combining both our features and the brown-cluster features result in some additional gains. 2 Our Approach Our departure point is a graph-based parsing model (McDonald et al., 2005): parse(x) = argmax score(x, y) y∈Y(x) score(x, y) = w · Φ(x, y) = X w · φ(x, part) part∈y Given a sentence x we look"
D15-1158,W07-2201,0,0.553056,"Missing"
D15-1158,P14-2050,1,0.811582,"r frequencies, and let rank(h, m) be the index of the pair (h, m) in D. We now set: SR ANK (h, m) = rank(h, m) |D| "" X h,m∈C ln (σ (uh · vm )) − k X i=1 # E mi ∼ Pm ln (σ (uh · vmi )) where σ(x) = 1/(1 + e−x ), and k is the number of negative samples, drawn from the corpus-based Unigram distribution Pm . For further details, see (Mikolov et al., 2013b; Goldberg and Levy, 2014). We then take:1 SE MBED (h, m) = σ(uh · vm ) In contrast to the counts based method, this model is able to estimate the strength of a pair of words even if the pair did not appear in the corpus due to sparsity. Finally, Levy and Goldberg (2014b) show that the skip-grams with negative-sampling model described above achieves its optimal solution when uh · vm = P M I(h, m) − log k. This gives rise to another natural way of estimating S: 1 The embedding we derive are very similar to the ones described in (Levy and Goldberg, 2014a; Bansal et al., 2014), and which were used by Bansal et al.(2014) for deriving semisupervised parsing features. An important difference from these previous work is that after training, they keep only one set of vectors (u or v) and ignore the other, basing the features on the derived vector representations. In"
D15-1158,J93-2004,0,0.05077,"3.68 84.18 84.00 83.87 84.26 Table 2: Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained on the PTB training set. Dev and Test are sections 22 and 23. Brown is the Brown portion of the PTB. The other columns correspond to the test portions of the Google Web Treebanks. Automatic POS-tags are assigned in all cases. HM indicates using assoc(h, m) and TRIP using assoc(h−1 h0 h1 , m−1 m0 m1 ). + B ROWN indicate using features based on Brown clustering. 3 Experiments and Results Data Our experiments are based on the Penn Treebank (PTB) (Marcus et al., 1993) as well as the Google Web Treebanks (LDC2012T13), covering both in-domain and out-of-domain scenarios. We use the Stanford-dependencies representation (de Marneffe and Manning, 2008). All the constituent-trees are converted to Stanforddependencies based on the settings of Version 1.0 of the Universal Treebank (McDonald et al., 2013).4 These are based on the Stanford Dependencies converter but use some non-default flags, and change some of the dependency labels. All of the models are trained on section 2-21 of the WSJ portion of the PTB. For in-domain data, we evaluate on sections 22 (Dev) and"
D15-1158,P15-1032,0,0.0203614,"out using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final model, and does not employ a highquality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further. Works that derive features from auto-parsed data include (Sagae and Gordon, 2009; Bansal et al., 2014). Such works assign a representation (either cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned featu"
D15-1158,P15-1117,0,0.0226382,"Missing"
D15-1158,N06-1020,0,0.106452,"vised parsing model. While the word representations used in such methods are not specifically designed for the parsing task, they do provide useful features for parsing, and in particular the method of (Koo et al., 2008), relying on features derived using the Brown-clustering algorithm, provides very competitive state-of-the-art results. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component. Among the words that use auto-parsed data, a dominant approach is self-training (McClosky et al., 2006), in which a parser A (possibly an ensemble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B (Petrov et al., 2010) or use a selection criteria for training only on highquality parses produced by parser A (Sagae and Tsujii, 2007; Weiss et al., 2015). In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for training the final m"
D16-1003,W08-2102,0,0.0285853,"r the conjuncts’ syntactic structure, which may be useful as symmetry often occurs on a higher level than POS tags. For example, in: NP NP NN CC PP tomorrow IN NP or NP CD at 16:00 NP the day PP PP IN CD at 12:00 after tomorrow the similarity is more substantial in the third level of the tree than in the POS level. A way to allow the model access to higher levels of syntactic symmetry is to represent each word as the projection of the grammatical functions from the word to the root.2 For example, the projections for the first conjunct in Figure 2 are: 2 Similar in spirit to the spines used in Carreras et al. (2008) and Shen et al. (2003). 26 VP VP VP NP NP VB PRP$ NNS cut their risks This decomposition captures the syntactic context of each word, but does not uniquely determine the structure of the tree. To remedy this, we add to the paths special symbols, R and L, which marks the lowest common ancestors with the right and left words respectively. These are added to the path above the corresponding nodes. For example consider the following paths which corresponds to the above syntactic structure: L VP VP R R L VP NP NP VB PRP$ NNS cut their risks The lowest common ancestor of “their” and “risks” is NP."
D16-1003,E12-1044,0,0.798729,"ar conjuncts is possible (“tomorrow and for the entirety of the next decade”), and the replacement principle fails in cases of ellipsis, gapping and others (“The bank employs [8,000 people in Spain] and [2,000 abroad]”). 2.3 Coordination in the PTB Coordination annotation in the Penn Treebank (Marcus et al., 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al., 1995). In addition, conjuncts in the PTB are not explicitly marked. These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al., 2002) which explicitly marks coordination phrases. However, using the Genia corpus is not ideal since it is in a specialized domain and much smaller than the PTB. In this work we rely on a version of the PTB released by Ficler and Goldberg (2016) in which the above deficiencies are manually resolved. In particular, coordinating elements, coordination phrases and conjunct boundaries are explicitly marked with specialized function labels. 2.4 And1 the city decided to treat its guests more like royalty or2 rock stars than factory owners."
D16-1003,P09-1109,0,0.32675,") and have identical syntactic structures. PP NP PP IN for NP CC and PP IN NP NNP for NNP China Asia (a) NP CD CC NNS and 1.8690 marks NP CD NNS 139.75 yen (b) 24 NP IN of NP (c) PP expenditures IN “The Jon Bon Jovi Soul Foundation [was founded in 2006] and1 [exists to combat issues that force (families) and2 (individuals) into economic despair].” NP and PRN QP NNS 429.9 billion rubles (US$ 693.4) NP of NP PRN QP NNS 489.9 billion rubles (US$ 790.2) Similarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009). 2.2 Replaceability Replacing a conjunct with the whole coordination phrase usually produce a coherent sentence (Huddleston et al., 2002). For example, in “Ethan has developed [new products] and [a new strategy]”, replacement results in: “Ethan has developed new products”; and “Ethan has developed a new strategy”, both valid sentences. Conjuncts replacement holds also for conjuncts of different syntactic types, e.g.: “inactivation of tumor-suppressor genes, [alone] or [in combination], appears crucial to the development of such scourges as cancer.”. While both symmetry and replacebility are s"
D16-1003,P07-1086,0,0.315282,"different levels of the syntactic structure. The replacement component considers the coherence of the sequence that is produced when connecting the participant parts. Both of these signals are syntactic in nature, and are learned solely based on information in the Penn Treebank. Our model substantially outperforms both the Berkeley and Zpar parsers on the coordination prediction task, while using the exact same training corpus. Semantic signals (which are likely to be based on resources external to the treebank) are also relevant for coordination disambiguation (Kawahara and Kurohashi, 2008; Hogan, 2007) and provide complementary information. We plan to incorporate such signals in future work. 23 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23–32, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Background Symmetry holds also in larger conjuncts, such as in: NP Coordination is a very common syntactic construction in which several sentential elements (called conjuncts) are linked. For example, in: NP CC NN PP VBZ income The coordinator and1 links the conjuncts surrounded with square brackets and the coordinat"
D16-1003,C08-1054,0,0.787672,"Missing"
D16-1003,J93-2004,0,0.0535475,", e.g.: “inactivation of tumor-suppressor genes, [alone] or [in combination], appears crucial to the development of such scourges as cancer.”. While both symmetry and replacebility are strong characteristics of coordination, neither principle holds universally. Coordination between syntactically dissimilar conjuncts is possible (“tomorrow and for the entirety of the next decade”), and the replacement principle fails in cases of ellipsis, gapping and others (“The bank employs [8,000 people in Spain] and [2,000 abroad]”). 2.3 Coordination in the PTB Coordination annotation in the Penn Treebank (Marcus et al., 1993) is inconsistent (Hogan, 2007) and lacks internal structure for NPs with nominal modifiers (Bies et al., 1995). In addition, conjuncts in the PTB are not explicitly marked. These deficiencies led previous works on coordination disambiguation (Shimbo and Hara, 2007; Hara et al., 2009; Hanamoto et al., 2012) to use the Genia treebank of biomedical text (Ohta et al., 2002) which explicitly marks coordination phrases. However, using the Genia corpus is not ideal since it is in a specialized domain and much smaller than the PTB. In this work we rely on a version of the PTB released by Ficler and Go"
D16-1003,P06-1055,0,0.0482684,"nk (Ohta et al., 2002)7 . When evaluating on the PTB, we compare to the conjunction boundary predictions of the generative 7 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA Berkeley Zpar Ours P 70.14 72.21 72.34 Dev R 70.72 72.72 72.25 F 70.42 72.46 72.29 P 68.52 68.24 72.81 Test R 69.33 69.42 72.61 F 68.92 68.82 72.7 Table 1: Coordination prediction on PTB (All coordinations). Berkeley Zpar Ours P 67.53 69.14 75.17 Dev R 70.93 72.31 74.82 F 69.18 70.68 74.99 P 69.51 69.81 76.91 Test R 72.61 72.92 75.31 F 71.02 71.33 76.1 Table 2: Coordination prediction on PTB (NP coordinations). Berkeley parser (Petrov et al., 2006) and the discriminative Zpar parser (Zhang and Clark, 2011). When evaluating on the Genia treebank, we compare to the results of the discriminative coordination-prediction model of Hara et al. (2009).8 6.1 Evaluation on PTB Baseline Our baseline is the performance of the Berkeley and Zpar parsers on the task presented in Section 3, namely: for a given coordinating word, determine the two spans that are being conjoined by it, and return N ONE if the coordinator is not conjoining spans or conjoins spans that are not of the expected type. We convert predicted trees to conjunction predictions by t"
D16-1003,W03-1012,0,0.0914122,"Missing"
D16-1003,D07-1064,0,0.463156,"(China/Asia, marks/yen) and have identical syntactic structures. PP NP PP IN for NP CC and PP IN NP NNP for NNP China Asia (a) NP CD CC NNS and 1.8690 marks NP CD NNS 139.75 yen (b) 24 NP IN of NP (c) PP expenditures IN “The Jon Bon Jovi Soul Foundation [was founded in 2006] and1 [exists to combat issues that force (families) and2 (individuals) into economic despair].” NP and PRN QP NNS 429.9 billion rubles (US$ 693.4) NP of NP PRN QP NNS 489.9 billion rubles (US$ 790.2) Similarity between conjuncts was used as a guiding principle in previous work on coordination disambiguation (Hogan, 2007; Shimbo and Hara, 2007; Hara et al., 2009). 2.2 Replaceability Replacing a conjunct with the whole coordination phrase usually produce a coherent sentence (Huddleston et al., 2002). For example, in “Ethan has developed [new products] and [a new strategy]”, replacement results in: “Ethan has developed new products”; and “Ethan has developed a new strategy”, both valid sentences. Conjuncts replacement holds also for conjuncts of different syntactic types, e.g.: “inactivation of tumor-suppressor genes, [alone] or [in combination], appears crucial to the development of such scourges as cancer.”. While both symmetry and"
D16-1003,J11-1005,0,0.0140981,"compare to the conjunction boundary predictions of the generative 7 http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA Berkeley Zpar Ours P 70.14 72.21 72.34 Dev R 70.72 72.72 72.25 F 70.42 72.46 72.29 P 68.52 68.24 72.81 Test R 69.33 69.42 72.61 F 68.92 68.82 72.7 Table 1: Coordination prediction on PTB (All coordinations). Berkeley Zpar Ours P 67.53 69.14 75.17 Dev R 70.93 72.31 74.82 F 69.18 70.68 74.99 P 69.51 69.81 76.91 Test R 72.61 72.92 75.31 F 71.02 71.33 76.1 Table 2: Coordination prediction on PTB (NP coordinations). Berkeley parser (Petrov et al., 2006) and the discriminative Zpar parser (Zhang and Clark, 2011). When evaluating on the Genia treebank, we compare to the results of the discriminative coordination-prediction model of Hara et al. (2009).8 6.1 Evaluation on PTB Baseline Our baseline is the performance of the Berkeley and Zpar parsers on the task presented in Section 3, namely: for a given coordinating word, determine the two spans that are being conjoined by it, and return N ONE if the coordinator is not conjoining spans or conjoins spans that are not of the expected type. We convert predicted trees to conjunction predictions by taking the two phrases that are immediately adjacent to the"
D16-1211,P16-1231,0,0.489171,".94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87 90.75 88.14 91.44 89.29 93.22 91.23 German UAS LAS 88.09 85.24 88.56 86.15 89.80 87.29 90.34 88.17 89.6 86.0 89.12 86.95 90.91 89.15 Japanese UAS LAS 93.10 92.28 – – 93.47 92.70 – – – – 93.71 92.85 93.65 92.84 Spanish UAS LAS 89.08 85.03 90.76 87.48 89.53 85.69 91.09 87.95 88.3 85.4 91.01 88.14 92.62 89.95 Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudoprojective parsing. Y’15 and A’16 are beam = 1 parsers from Yazdani and Henderson (2015) and Andor et al. (2016), respectively. A’16-beam is the parser with beam larger than 1 by Andor et al. (2016). Bold numbers indicate the best results among the greedy parsers. The error-exploring dynamic-oracle training always improves over static oracle training controlling for the transition system, but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Ando"
D16-1211,W15-2210,0,0.0477838,"Missing"
D16-1211,P05-1022,0,0.0279624,"English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87"
D16-1211,D14-1082,0,0.316369,"urns out to be very beneficial for the configurations that make use of external embeddings. Indeed, these configurations achieve high accuracies and sharp class distributions early on in the training process. The parser is trained to maximize the likelihood of a correct action zg at each parsing state pt according to Equation 1. When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is the marginal likelihood of all correct actions,3 X p(zg |pt ) = p(zgi |pt ). (3) zgi ∈zg 3 Experiments Following the same settings of Chen and Manning (2014) and Dyer et al (2015) we report results4 in the English PTB and Chinese CTB-5. Table 1 shows the results of the parser in its different configurations. The table also shows the best result obtained with the static oracle (obtained by rerunning Dyer et al. parser) for the sake of comparison between static and dynamic training strategies. Method Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) + pre-training: Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66"
D16-1211,P15-1033,1,0.660448,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are typically stored in a stack data structure, and the words remaining to be processed. to capture information from the entirety of the state, without resorting to locality assumptions that were common in most other transition-based par"
D16-1211,C12-1059,1,0.947246,"ces, given words. At test time, the parser makes greedy decisions according to the learned model. Although this setup obtains very good performance, the training and testing conditions are mismatched in the following way: at training time the historical context of an action is always derived from the gold standard (i.e., perfectly correct past actions), but at test time, it will be a model prediction. In this work, we adapt the training criterion so as to explore parser states drawn not only from the training data, but also from the model as it is being learned. To do so, we use the method of Goldberg and Nivre (2012; 2013) to dynamically chose an optimal (relative to the final attachment accuracy) action given an imperfect history. By interpolating between algorithm states sampled from the model and those sampled from the training data, more robust predictions at test time can be made. We show that the technique can be used to improve the strong parser of Dyer et al. 2 Parsing Model and Parameter Learning Our departure point is the parsing model described by Dyer et al. (2015). We do not describe the model in detail, and refer the reader to the original work. At each stage t of the parsing process, the p"
D16-1211,Q13-1033,1,0.93704,"ing with Exploration Improves a Greedy Stack LSTM Parser Miguel Ballesteros♦ Yoav Goldberg♣ Chris Dyer♠ Noah A. Smith♥ ♦ NLP Group, Pompeu Fabra University, Barcelona, Spain ♣ Computer Science Department, Bar-Ilan University, Ramat Gan, Israel ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, yoav.goldberg@gmail.com, cdyer@google.com, nasmith@cs.washington.edu Abstract We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Mats"
D16-1211,Q14-1010,1,0.946318,"if the best tree that can be reached after taking the action is no worse (in terms of accuracy with respect to the gold tree) than the best tree that could be reached prior to taking that action. Goldberg and Nivre (2013) define the arcdecomposition property of transition systems, and show how to derive efficient dynamic oracles for transition systems that are arc-decomposable.2 Unfortunately, the arc-standard transition system does 2 Specifically: for every parser configuration p and group of not have this property. While it is possible to compute dynamic oracles for the arc-standard system (Goldberg et al., 2014), the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack. As the dynamic oracle has to be queried for each parser state seen during training, the use of this dynamic oracle will make the training runtime several times longer. We chose instead to switch to the arc-hybrid transition system (Kuhlmann et al., 2011), which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime. We implemented the dynamic oracle to the arc-hybrid s"
D16-1211,W13-5709,1,0.953811,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The framework of training with exploration using dynamic oracles suggested by Goldberg and Nivre (2012; 2013) provides answers to these questions. While the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take for any valid parser state. In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path, the dynamic oracle is well defined for states that result from parsing mistakes, and they may produce more than a single gold action for a given state. Under the dynamic oracle framework, an action is said to be optimal for a state if the best tree that can be reached afte"
D16-1211,P15-2042,0,0.0776288,"Missing"
D16-1211,D14-1099,0,0.0889626,"Missing"
D16-1211,Q14-1011,0,0.0127702,"rained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; R"
D16-1211,W13-3518,1,0.840178,"clude results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012"
D16-1211,P11-1068,0,0.126938,"Missing"
D16-1211,P05-1013,0,0.0596932,"rc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Andor et al., 2016)5 we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task (Hajiˇc et al., 2009). Since some of the treebanks contain nonprojective sentences and archybrid does not allow nonprojective trees, we use the pseudo-projective approach (Nivre and Nilsson, 2005). We used predicted part-of-speech tags provided by the CoNLL 2009 shared task organizers. We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several resea"
D16-1211,W03-3017,0,0.353357,"ssuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous de"
D16-1211,W04-0308,0,0.52657,"ror-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (some"
D16-1211,J08-4003,0,0.0983095,"on history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called t"
D16-1211,P00-1061,0,0.278952,"brid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59"
D16-1211,P15-3004,0,0.0262076,"Missing"
D16-1211,Q16-1014,0,0.0220862,"nish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training"
D16-1211,W03-3023,0,0.18219,"nd Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection o"
D16-1211,K15-1015,0,0.032838,"ize 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training regimens that make the learned model more robust to test-time prediction errors. Solutions based on curriculum learning (Bengio et al., 2015), expected loss training (Shen et al., 2015), and reinforcement learning have been proposed (Ranzato et al., 2016). Finally, abandoning greedy search in favor of approximate global search offers an alternative solution to the problems with greedy search (Andor et al., 2016), and has been analyzed as well (Kulesza and Pereira"
D18-1002,D16-1120,0,0.141259,"Missing"
D18-1002,D11-1120,0,0.0819617,"University, Israel ∗ Allen Institute for Artificial Intelligence {yanaiela,yoav.goldberg}@gmail.com Abstract score decisions (Ghailan et al., 2016). By using the raw text as is, a discrimination issue might arise, as textual information can be predictive of some demographic factors (Hovy et al., 2015) and author’s attributes might correlate with target variables (Zhao et al., 2017). In this paper we are interested in languagebased features. It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author (Koppel et al., 2002; Burger et al., 2011; Nguyen et al., 2013; Weren et al., 2014; Verhoeven and Daelemans, 2014; Rangel et al., 2016; Verhoeven et al., 2016; Blodgett et al., 2016), or even the audience of the text (Voigt et al., 2018). Thus, any system that incorporates raw text into its decision process is at risk of indirectly conditioning on such signals. Recent advances in representation learning suggest adversarial training as a mean to hide the protected attributes from the decision function (Section 2). We perform a series of experiments and show that: (1) Information about race, gender and age is indeed encoded into interm"
D18-1002,P18-2005,0,0.0718823,"uthored text. Several works apply adversarial training to textual data, in order to learn encoders that are invariant to some properties of the text (Chen et al., 2016; Conneau et al., 2017; Zhang et al., 2017; Xie et al., 2017). As their main motivation is to remove information about domain or language in order to improve transfer learning, domain adaptation, or end task accuracy, they were less concerned with the ability to recover information from the resulting representation, and did not evaluate it directly as we do here. Recent work on creating private representation in the text domain (Li et al., 2018) share our motivation of removing unintended demographic attributes from the learned representation using adversarial training. However, they report only the discrimination accuracies of the adversarial component, and do not train another classifier to verify that the representations are indeed clear of the protected attribute. As our work shows, trusting the adversary is insufficient, and external verification is crucial. Finally, our work is motivated by the desire for fairness. We use a definition in which a fair classification is one that does not condition on a certain Acknowledgments We"
D18-1002,D17-1169,0,0.0836373,"Missing"
D18-1002,K16-1006,0,0.0177885,"equent than words in the first group. We discard words appearing in both groups, and associate each word with its training set frequency. One-tailed Mann-Whitney U test (Mann and Whitney, 1947) showed the effect is highly significant with p &lt; e−12 . 7 The fact that intermediary vector representations that are trained for one task are predictive of another is not surprising: it is at the core of the success of NLP methods for deriving “generic” word and sentence representations (e.g. Word2vec (Mikolov et al., 2013), Skipthought vectors (Kiros et al., 2015), Contextualized Word Representations (Melamud et al., 2016; Peters et al., 2018) etc.). While usually considered a positive feature, it can often have undesired consequences one should be aware of and potentially control for. Several works document biases and stereotypes that are captured by unsupervised word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) and ways of mitigating them (Bolukbasi et al., 2016; Zhang et al., 2018). Bias and stereotyping were also documented on a common NLP dataset (Rudinger et al., 2017). While these work are concerned with the learned representations encoding unwanted biases about the world, our concern is w"
D18-1002,Q17-1036,0,0.0321407,"annot be trusted, and must be verified with an externallytrained attacker, preferably on unseen data. (3) Tuning the capacity and weight of the adversary, as well as using an ensemble of several adversaries, can improve the results. However, no single method is the most effective in all cases. (Edwards and Storkey, 2015; Feutry et al., 2018). In contrast, we consider features that are based on short user-authored text. Several works apply adversarial training to textual data, in order to learn encoders that are invariant to some properties of the text (Chen et al., 2016; Conneau et al., 2017; Zhang et al., 2017; Xie et al., 2017). As their main motivation is to remove information about domain or language in order to improve transfer learning, domain adaptation, or end task accuracy, they were less concerned with the ability to recover information from the resulting representation, and did not evaluate it directly as we do here. Recent work on creating private representation in the text domain (Li et al., 2018) share our motivation of removing unintended demographic attributes from the learned representation using adversarial training. However, they report only the discrimination accuracies of the ad"
D18-1002,D17-1323,0,0.0955377,"Missing"
D18-1002,N18-1202,0,0.0416459,"he first group. We discard words appearing in both groups, and associate each word with its training set frequency. One-tailed Mann-Whitney U test (Mann and Whitney, 1947) showed the effect is highly significant with p &lt; e−12 . 7 The fact that intermediary vector representations that are trained for one task are predictive of another is not surprising: it is at the core of the success of NLP methods for deriving “generic” word and sentence representations (e.g. Word2vec (Mikolov et al., 2013), Skipthought vectors (Kiros et al., 2015), Contextualized Word Representations (Melamud et al., 2016; Peters et al., 2018) etc.). While usually considered a positive feature, it can often have undesired consequences one should be aware of and potentially control for. Several works document biases and stereotypes that are captured by unsupervised word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) and ways of mitigating them (Bolukbasi et al., 2016; Zhang et al., 2018). Bias and stereotyping were also documented on a common NLP dataset (Rudinger et al., 2017). While these work are concerned with the learned representations encoding unwanted biases about the world, our concern is with capturing potentia"
D18-1002,W17-1609,0,0.0926158,"Missing"
D18-1002,verhoeven-daelemans-2014-clips,0,0.048108,"nce {yanaiela,yoav.goldberg}@gmail.com Abstract score decisions (Ghailan et al., 2016). By using the raw text as is, a discrimination issue might arise, as textual information can be predictive of some demographic factors (Hovy et al., 2015) and author’s attributes might correlate with target variables (Zhao et al., 2017). In this paper we are interested in languagebased features. It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author (Koppel et al., 2002; Burger et al., 2011; Nguyen et al., 2013; Weren et al., 2014; Verhoeven and Daelemans, 2014; Rangel et al., 2016; Verhoeven et al., 2016; Blodgett et al., 2016), or even the audience of the text (Voigt et al., 2018). Thus, any system that incorporates raw text into its decision process is at risk of indirectly conditioning on such signals. Recent advances in representation learning suggest adversarial training as a mean to hide the protected attributes from the decision function (Section 2). We perform a series of experiments and show that: (1) Information about race, gender and age is indeed encoded into intermediate representations of neural networks, even when training for seemin"
D18-1002,L16-1258,0,0.0173814,"e decisions (Ghailan et al., 2016). By using the raw text as is, a discrimination issue might arise, as textual information can be predictive of some demographic factors (Hovy et al., 2015) and author’s attributes might correlate with target variables (Zhao et al., 2017). In this paper we are interested in languagebased features. It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author (Koppel et al., 2002; Burger et al., 2011; Nguyen et al., 2013; Weren et al., 2014; Verhoeven and Daelemans, 2014; Rangel et al., 2016; Verhoeven et al., 2016; Blodgett et al., 2016), or even the audience of the text (Voigt et al., 2018). Thus, any system that incorporates raw text into its decision process is at risk of indirectly conditioning on such signals. Recent advances in representation learning suggest adversarial training as a mean to hide the protected attributes from the decision function (Section 2). We perform a series of experiments and show that: (1) Information about race, gender and age is indeed encoded into intermediate representations of neural networks, even when training for seemingly unrelated tasks and the training data is"
D18-1002,L18-1445,0,0.0287287,"issue might arise, as textual information can be predictive of some demographic factors (Hovy et al., 2015) and author’s attributes might correlate with target variables (Zhao et al., 2017). In this paper we are interested in languagebased features. It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author (Koppel et al., 2002; Burger et al., 2011; Nguyen et al., 2013; Weren et al., 2014; Verhoeven and Daelemans, 2014; Rangel et al., 2016; Verhoeven et al., 2016; Blodgett et al., 2016), or even the audience of the text (Voigt et al., 2018). Thus, any system that incorporates raw text into its decision process is at risk of indirectly conditioning on such signals. Recent advances in representation learning suggest adversarial training as a mean to hide the protected attributes from the decision function (Section 2). We perform a series of experiments and show that: (1) Information about race, gender and age is indeed encoded into intermediate representations of neural networks, even when training for seemingly unrelated tasks and the training data is balanced in terms of the protected attributes (Section 4); (2) The adversarial"
D18-1523,S07-1002,0,0.0911723,"iguous. For example, (d) matches both the music and the fish senses: 1 This example shows homonymy, a case where the same word form has two distinct meaning. A more subtle case is polysemy, where the senses share some semantic similarity. In “She played a low bass note”, the sense of bass is related to the sense in (b), but distinct from it. The WSI task we tackle in this work deals with both cases. This calls for a soft clustering, allowing to probabilistically associate a given mention to two senses. The problem of WSI has been extensively studied with a series of shared tasks on the topic (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013), the latest being SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013). Recent state-of-the-art approaches to WSI rely on generative graphical models (Lau et al., 2013; Wang et al., 2015; Komninos and Manandhar, 2016). In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differentiate between local (syntactic, close to the disambiguated word) and global (thematic, semantic) context features. Substitute Vectors Baskaya et al. (2013) take a different approach to t"
D18-1523,S13-2050,0,0.239348,"of shared tasks on the topic (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013), the latest being SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013). Recent state-of-the-art approaches to WSI rely on generative graphical models (Lau et al., 2013; Wang et al., 2015; Komninos and Manandhar, 2016). In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differentiate between local (syntactic, close to the disambiguated word) and global (thematic, semantic) context features. Substitute Vectors Baskaya et al. (2013) take a different approach to the problem, based on substitute vectors. They represent each instance as a distribution of possible substitute words, as determined by a language model (LM). The substitute vectors are then clustered to obtain senses. Baskaya et al. (2013) derive their probabilities from a 4-gram language model. Their system (AIKU) was one of the best performing at the time of SemEval 2013 shared task. Our method is inspired by the AI-KU use of substitution based sense induction, but deviate from it by moving to a recurrent language model. Besides being more accurate, this allows"
D18-1523,C92-2082,0,0.294011,"ple samples from its associated word distributions (section 2.3). (3) Finally, we cluster the representatives and use the hard clustering to derive a soft-clustering over the instances (section 2.4). We use the pre-trained neural biLM as a blackbox, but use linguistically motivated processing of both its input and its output: we rely on the generalization power of the biLM and query it using dynamic symmetric patterns (section 2.2); and we lemmatize the resulting word distributions. Dynamic Symmetric Patterns Our main proposal incorporates such information. It is motivated by Hearst patterns (Hearst, 1992; Widdows and Dorow, 2002; Schwartz et al., 2015), and made possible by neural LMs. Neural LMs are better in capturing long-range dependencies, and can handle and predict unseen text by generalizing from similar contexts. Conjunctions, and in particular the word and, are known to combine expressions of the same kind. Recently, Schwartz et al. (2015) used conjunctive symmetric patterns to derive word embeddings that excel at capturing word similarity. Similarly, Kozareva et al. (2008) search for doubly-anchored patterns including the word and in a large web-corpus to improve semanticclass induc"
D18-1523,S13-2049,0,0.533473,"and the fish senses: 1 This example shows homonymy, a case where the same word form has two distinct meaning. A more subtle case is polysemy, where the senses share some semantic similarity. In “She played a low bass note”, the sense of bass is related to the sense in (b), but distinct from it. The WSI task we tackle in this work deals with both cases. This calls for a soft clustering, allowing to probabilistically associate a given mention to two senses. The problem of WSI has been extensively studied with a series of shared tasks on the topic (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013), the latest being SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013). Recent state-of-the-art approaches to WSI rely on generative graphical models (Lau et al., 2013; Wang et al., 2015; Komninos and Manandhar, 2016). In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differentiate between local (syntactic, close to the disambiguated word) and global (thematic, semantic) context features. Substitute Vectors Baskaya et al. (2013) take a different approach to the problem, based on substitute vectors. They represen"
D18-1523,C16-1337,0,0.0317896,"Missing"
D18-1523,P08-1119,0,0.035457,"ibutions. Dynamic Symmetric Patterns Our main proposal incorporates such information. It is motivated by Hearst patterns (Hearst, 1992; Widdows and Dorow, 2002; Schwartz et al., 2015), and made possible by neural LMs. Neural LMs are better in capturing long-range dependencies, and can handle and predict unseen text by generalizing from similar contexts. Conjunctions, and in particular the word and, are known to combine expressions of the same kind. Recently, Schwartz et al. (2015) used conjunctive symmetric patterns to derive word embeddings that excel at capturing word similarity. Similarly, Kozareva et al. (2008) search for doubly-anchored patterns including the word and in a large web-corpus to improve semanticclass induction. The method of Schwartz et al. (2015) result in context-independent embeddings, while that of Kozareva et al. (2008) takes some context into account but is restricted to exact corpus matches and thus suffers a lot from sparsity. We make use of the rich sequence representation capabilities of the neural biLM to derive context-dependent symmetric pattern substi2.1 2 We thank the ELMo team for sharing the pre-trained models. We follow the ELMo biLM approach (Peters et al., 2018) an"
D18-1523,N18-1202,0,0.179997,"recurrent language model. Besides being more accurate, this allows us to further improve the quality of the derived substitutions by the incorporation of dynamic symmetric patterns. BiLM Bidirectional RNNs were shown to be effective for word-sense disambiguation and lexical substitution tasks (Melamud et al., 2016; Yuan et al., 2016; Raganato et al., 2017). We adopt the 4860 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4860–4867 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ELMo biLM model of Peters et al. (2018), which was shown to produce very competitive results for many NLP tasks. We use the pre-trained ELMo biLM provided by Peters et al. (2018).2 However, rather than using the LSTM state vectors as suggested in the ELMo paper, we opt instead to use the predicted word probabilities. Moving from continuous and opaque state vectors to discrete and transparent word distributions allows far better control of the resulting representations (e.g. by sampling, re-weighting and lemmatizing the words) as well as better debugging opportunities. As expected, the move to the neural biLM already outperforms the"
D18-1523,D17-1120,0,0.02846,"derive their probabilities from a 4-gram language model. Their system (AIKU) was one of the best performing at the time of SemEval 2013 shared task. Our method is inspired by the AI-KU use of substitution based sense induction, but deviate from it by moving to a recurrent language model. Besides being more accurate, this allows us to further improve the quality of the derived substitutions by the incorporation of dynamic symmetric patterns. BiLM Bidirectional RNNs were shown to be effective for word-sense disambiguation and lexical substitution tasks (Melamud et al., 2016; Yuan et al., 2016; Raganato et al., 2017). We adopt the 4860 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4860–4867 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ELMo biLM model of Peters et al. (2018), which was shown to produce very competitive results for many NLP tasks. We use the pre-trained ELMo biLM provided by Peters et al. (2018).2 However, rather than using the LSTM state vectors as suggested in the ELMo paper, we opt instead to use the predicted word probabilities. Moving from continuous and opaque state vectors to discret"
D18-1523,K15-1026,0,0.0282782,"stributions (section 2.3). (3) Finally, we cluster the representatives and use the hard clustering to derive a soft-clustering over the instances (section 2.4). We use the pre-trained neural biLM as a blackbox, but use linguistically motivated processing of both its input and its output: we rely on the generalization power of the biLM and query it using dynamic symmetric patterns (section 2.2); and we lemmatize the resulting word distributions. Dynamic Symmetric Patterns Our main proposal incorporates such information. It is motivated by Hearst patterns (Hearst, 1992; Widdows and Dorow, 2002; Schwartz et al., 2015), and made possible by neural LMs. Neural LMs are better in capturing long-range dependencies, and can handle and predict unseen text by generalizing from similar contexts. Conjunctions, and in particular the word and, are known to combine expressions of the same kind. Recently, Schwartz et al. (2015) used conjunctive symmetric patterns to derive word embeddings that excel at capturing word similarity. Similarly, Kozareva et al. (2008) search for doubly-anchored patterns including the word and in a large web-corpus to improve semanticclass induction. The method of Schwartz et al. (2015) result"
D18-1523,Q15-1005,0,0.100115,"he played a low bass note”, the sense of bass is related to the sense in (b), but distinct from it. The WSI task we tackle in this work deals with both cases. This calls for a soft clustering, allowing to probabilistically associate a given mention to two senses. The problem of WSI has been extensively studied with a series of shared tasks on the topic (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013), the latest being SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013). Recent state-of-the-art approaches to WSI rely on generative graphical models (Lau et al., 2013; Wang et al., 2015; Komninos and Manandhar, 2016). In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differentiate between local (syntactic, close to the disambiguated word) and global (thematic, semantic) context features. Substitute Vectors Baskaya et al. (2013) take a different approach to the problem, based on substitute vectors. They represent each instance as a distribution of possible substitute words, as determined by a language model (LM). The substitute vectors are then clustered to obtain senses. Baskaya et al. (2013)"
D18-1523,C02-1114,0,0.376531,"om its associated word distributions (section 2.3). (3) Finally, we cluster the representatives and use the hard clustering to derive a soft-clustering over the instances (section 2.4). We use the pre-trained neural biLM as a blackbox, but use linguistically motivated processing of both its input and its output: we rely on the generalization power of the biLM and query it using dynamic symmetric patterns (section 2.2); and we lemmatize the resulting word distributions. Dynamic Symmetric Patterns Our main proposal incorporates such information. It is motivated by Hearst patterns (Hearst, 1992; Widdows and Dorow, 2002; Schwartz et al., 2015), and made possible by neural LMs. Neural LMs are better in capturing long-range dependencies, and can handle and predict unseen text by generalizing from similar contexts. Conjunctions, and in particular the word and, are known to combine expressions of the same kind. Recently, Schwartz et al. (2015) used conjunctive symmetric patterns to derive word embeddings that excel at capturing word similarity. Similarly, Kozareva et al. (2008) search for doubly-anchored patterns including the word and in a large web-corpus to improve semanticclass induction. The method of Schwa"
D18-1523,S13-2051,0,0.0827131,"on the topic (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013), the latest being SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013). Recent state-of-the-art approaches to WSI rely on generative graphical models (Lau et al., 2013; Wang et al., 2015; Komninos and Manandhar, 2016). In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differentiate between local (syntactic, close to the disambiguated word) and global (thematic, semantic) context features. Substitute Vectors Baskaya et al. (2013) take a different approach to the problem, based on substitute vectors. They represent each instance as a distribution of possible substitute words, as determined by a language model (LM). The substitute vectors are then clustered to obtain senses. Baskaya et al. (2013) derive their probabilities from a 4-gram language model. Their system (AIKU) was one of the best performing at the time of SemEval 2013 shared task. Our method is inspired by the AI-KU use of substitution based sense induction, but deviate from it by moving to a recurrent language model. Besides being more accurate, this allows"
D18-1523,S10-1011,0,0.125186,"matches both the music and the fish senses: 1 This example shows homonymy, a case where the same word form has two distinct meaning. A more subtle case is polysemy, where the senses share some semantic similarity. In “She played a low bass note”, the sense of bass is related to the sense in (b), but distinct from it. The WSI task we tackle in this work deals with both cases. This calls for a soft clustering, allowing to probabilistically associate a given mention to two senses. The problem of WSI has been extensively studied with a series of shared tasks on the topic (Agirre and Soroa, 2007; Manandhar et al., 2010; Jurgens and Klapaftis, 2013), the latest being SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013). Recent state-of-the-art approaches to WSI rely on generative graphical models (Lau et al., 2013; Wang et al., 2015; Komninos and Manandhar, 2016). In these works, the sense is modeled as a latent variable that influences the context of the target word. The later models explicitly differentiate between local (syntactic, close to the disambiguated word) and global (thematic, semantic) context features. Substitute Vectors Baskaya et al. (2013) take a different approach to the problem, based on sub"
D18-1523,K16-1006,0,0.0352958,"d to obtain senses. Baskaya et al. (2013) derive their probabilities from a 4-gram language model. Their system (AIKU) was one of the best performing at the time of SemEval 2013 shared task. Our method is inspired by the AI-KU use of substitution based sense induction, but deviate from it by moving to a recurrent language model. Besides being more accurate, this allows us to further improve the quality of the derived substitutions by the incorporation of dynamic symmetric patterns. BiLM Bidirectional RNNs were shown to be effective for word-sense disambiguation and lexical substitution tasks (Melamud et al., 2016; Yuan et al., 2016; Raganato et al., 2017). We adopt the 4860 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4860–4867 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ELMo biLM model of Peters et al. (2018), which was shown to produce very competitive results for many NLP tasks. We use the pre-trained ELMo biLM provided by Peters et al. (2018).2 However, rather than using the LSTM state vectors as suggested in the ELMo paper, we opt instead to use the predicted word probabilities. Moving from con"
D19-1004,P07-1036,0,0.281797,"supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERTbased Aspect-based Sentiment model. 1 Introduction Data annotation is a key bottleneck in many data driven algorithms. Specifically, deep learning models, which became a prominent tool in many data driven tasks in recent years, require large datasets to work well. However, many tasks require manual annotations which are relatively hard to obtain at scale. An attractive alternative is lightly supervised learning (Schapire et al., 2002; Jin and Liu, 2005; Chang et al., 2007; Grac¸a et al., 2007; Quadrianto et al., 2009a; Mann and McCallum, 2010a; Ganchev et al., 2010; Hope and Shahaf, 2016), in which the objective function is supplemented by a set of domain-specific soft31 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 31–42, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics for the resource poor language in a lightly supervised manner. a medium-sized sentiment corpus with sentencelevel labels, and a large"
D19-1004,N06-1041,0,0.0799561,"in which the objective function is supplemented by a set of domain-specific soft-constraints over the model’s predictions on unlabeled data. Previous work in lightly-supervised learning focused on training classifiers by using prior knowledge of label proportions (Jin and Liu, 2005; Chang et al., 2007; Musicant et al., 2007; Mann and McCallum, 2007; Quadrianto et al., 2009b; Liang et al., 2009; Ganchev et al., 2010; Mann and McCallum, 2010b; Chang et al., 2012; Wang et al., 2012; Zhu et al., 2014; Hope and Shahaf, 2016) or prior knowledge of features label associations (Schapire et al., 2002; Haghighi and Klein, 2006; Druck et al., 2008; Melville et al., 2009; Mohammady and Culotta, 2015). In the context of NLP, Haghighi and Klein (2006) suggested to use distributional similarities of words to train sequence models for part-of-speech tagging and a classified ads information extraction task. Melville et al. (2009) used background lexical information in terms of word-class associations to train a sentiment classifier. Ganchev and Das (2013); Wang and Manning (2014) suggested to exploit the bilingual correlations between a resource rich language and a resource poor language to train a classifier Lcross (θ) ="
D19-1004,C18-1096,0,0.114541,",b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanisms combined with a recurrent neural networks and a weighted memory mechanism. LSTM+SynATT+TarRep (He et al., 2018a) is an attention based LSTM which incorporates synTraining Details Both the sentence level classification models and the models trained with XR have a hidden state vector dimension of size 300, they use dropout (Hinton et al., 2012) on the sentence representation or fragment representation vector (rate=0.5) and optimized using Adam (Kingma and Ba, 2014). The sentence classification is trained with a batch size of 30 and XR models are trained with batch sizes k that each contain 450 fragments10 . We used a temperature param6 On the rare occasions where we cannot find such a node, we take the"
D19-1004,D17-1047,0,0.088542,"ain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanisms combined with a r"
D19-1004,P18-2092,0,0.124803,",b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanisms combined with a recurrent neural networks and a weighted memory mechanism. LSTM+SynATT+TarRep (He et al., 2018a) is an attention based LSTM which incorporates synTraining Details Both the sentence level classification models and the models trained with XR have a hidden state vector dimension of size 300, they use dropout (Hinton et al., 2012) on the sentence representation or fragment representation vector (rate=0.5) and optimized using Adam (Kingma and Ba, 2014). The sentence classification is trained with a batch size of 30 and XR models are trained with batch sizes k that each contain 450 fragments10 . We used a temperature param6 On the rare occasions where we cannot find such a node, we take the"
D19-1004,D18-1380,0,0.0263581,"Missing"
D19-1004,D13-1205,0,0.0313916,"allum, 2010b; Chang et al., 2012; Wang et al., 2012; Zhu et al., 2014; Hope and Shahaf, 2016) or prior knowledge of features label associations (Schapire et al., 2002; Haghighi and Klein, 2006; Druck et al., 2008; Melville et al., 2009; Mohammady and Culotta, 2015). In the context of NLP, Haghighi and Klein (2006) suggested to use distributional similarities of words to train sequence models for part-of-speech tagging and a classified ads information extraction task. Melville et al. (2009) used background lexical information in terms of word-class associations to train a sentiment classifier. Ganchev and Das (2013); Wang and Manning (2014) suggested to exploit the bilingual correlations between a resource rich language and a resource poor language to train a classifier Lcross (θ) = − n X log pθ (yi |xi ) i Instead, in XR our data comes in the form of pairs (Uj , p ˜ j ) of sets and their corresponding expected label proportions, and we aim to optimize θ to fit the label distribution p ˜ j over Uj , for all j. XR Loss As counting the number of predicted class labels over a set U leads to a nondifferentiable objective, Mann and McCallum (2007) suggest to relax it and use instead the 32 Algorithm 1 Stochas"
D19-1004,K18-1018,0,0.0117636,"ly have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanisms combined with a recurrent neural networks and a weighted memory mechanism. LSTM+SynATT+TarRep (He et al., 2018a) i"
D19-1004,N18-2045,0,0.01256,"ces for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanisms combined with a recurrent neural ne"
D19-1004,E17-2091,0,0.020299,"ces from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanism"
D19-1004,D15-1298,0,0.0736268,"Missing"
D19-1004,D14-1162,0,0.0842238,"Missing"
D19-1004,S15-2082,0,0.0372356,"ts also when applied on top of a pretrained B ERT-based model (Devlin et al., 2018). Finally, to make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure (Section 3.2). Source code is available at https: //github.com/MatanBN/XRTransfer. 2 2.1 2.2 Expectation Regularization (XR) Expectation Regularization (XR) (Mann and McCallum, 2007) is a lightly supervised learning method, in which the model is trained to fit the conditional probabilities of labels given features. In the context of NLP, XR was used by Mohammady and Culotta (2015) to train twitter-user attribute prediction using hundreds of noisy distributional expectations based on census demographics. Here, we suggest using XR to train a target task (aspect-level sentiment) based on the output of a related source-task classifier (sentence-level sentiment). Learning Setup The main idea of XR is moving from a fully supervised situation in which each data-point xi has an associated label yi , to a setup in which sets of data points Uj are associated with corresponding label proportions p ˜ j over that set. Formally, let X = {x1 , x2 , . . . , xn } ⊆ X be a set of data p"
D19-1004,N15-1019,0,0.0368537,"Missing"
D19-1004,D16-1103,0,0.0129782,"ed on training on up to 10,000 sentences from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2"
D19-1004,C16-1311,0,0.170539,"the restaurants reviews domain. Our source classifier is based on training on up to 10,000 sentences from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory netwo"
D19-1004,D16-1021,0,0.132181,"the restaurants reviews domain. Our source classifier is based on training on up to 10,000 sentences from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory netwo"
D19-1004,Q14-1005,0,0.0197537,"l., 2012; Wang et al., 2012; Zhu et al., 2014; Hope and Shahaf, 2016) or prior knowledge of features label associations (Schapire et al., 2002; Haghighi and Klein, 2006; Druck et al., 2008; Melville et al., 2009; Mohammady and Culotta, 2015). In the context of NLP, Haghighi and Klein (2006) suggested to use distributional similarities of words to train sequence models for part-of-speech tagging and a classified ads information extraction task. Melville et al. (2009) used background lexical information in terms of word-class associations to train a sentiment classifier. Ganchev and Das (2013); Wang and Manning (2014) suggested to exploit the bilingual correlations between a resource rich language and a resource poor language to train a classifier Lcross (θ) = − n X log pθ (yi |xi ) i Instead, in XR our data comes in the form of pairs (Uj , p ˜ j ) of sets and their corresponding expected label proportions, and we aim to optimize θ to fit the label distribution p ˜ j over Uj , for all j. XR Loss As counting the number of predicted class labels over a set U leads to a nondifferentiable objective, Mann and McCallum (2007) suggest to relax it and use instead the 32 Algorithm 1 Stochastic Batched XR Inputs: A"
D19-1004,P18-1088,0,0.0124917,"for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops of attention layers. RAM (Chen et al., 2017) uses multiple attention mechanisms combined with a recurrent neural networks and a weighted memory mechanism"
D19-1004,D16-1058,0,0.233034,"ws domain. Our source classifier is based on training on up to 10,000 sentences from the same domain and 2000 sentences for validation, labeled for only for sentence-level sentiment. We additionally have an unlabeled dataset of up to 670,000 sentences from the same domain7 . We tokenized all datasets using the Tweet Tokenizer from NLTK package8 and parsed the tokenized sentences with AllenNLP parser.9 Baseline models In recent years many neural network architectures with increasing sophistication were applied to the ABSC task (Nguyen and Shirai, 2015; Vo and Zhang, 2015; Tang et al., 2016a,b; Wang et al., 2016; Zhang et al., 2016; Ruder et al., 2016; Ma et al., 2017; Liu and Zhang, 2017; Chen et al., 2017; Liu et al., 2018; Yang et al., 2018; Wang et al., 2018b,a; Fan et al., 2018a,b; Li et al., 2018; Ouyang and Su, 2018). We compare to a series of state-of-theart ABSC neural classifiers that participated in the shared tasks. TDLSTM-ATT (Tang et al., 2016a) encodes the information around an aspect using forward and backward LSTMs, followed by an attention mechanism. ATAE-LSTM (Wang et al., 2016) is an attention based LSTM variant. MM (Tang et al., 2016b) is a deep memory network with multiple-hops"
D19-1107,P18-2114,1,0.829918,"majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators"
D19-1107,D12-1091,0,0.0497685,"Missing"
D19-1107,D15-1075,0,0.0532422,"s. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In t"
D19-1107,N15-4002,0,0.0604716,"Missing"
D19-1107,P18-1128,0,0.0258084,"Missing"
D19-1107,N19-1246,0,0.119133,"Missing"
D19-1107,P18-2103,1,0.854507,"using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by"
D19-1107,N18-2017,0,0.0590183,"ssively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator inf"
D19-1107,N15-1098,0,0.0331621,"o produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generate"
D19-1107,D18-1260,0,0.0869546,"atterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen at training time. We observe that often generalization to new annotators fails, and that augmenting the training set with a small number of examples from these annotators substantially increase"
D19-1107,Q18-1040,0,0.0271845,"s raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen"
D19-1107,S18-2023,0,0.0357123,"aving only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we"
D19-1107,D16-1264,0,0.0698024,"that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue r"
D19-1107,P18-2124,0,0.029158,"hese annotators substantially increases performance. Taken together, our experiments show that annotator bias exists in current NLU datasets, which can lead to problems in model generalization to new users. Hence, we propose that annotator bias should be monitored at data collection time and to tackle it by having the test set include examples from a disjoint set of annotators. 2 Crowdsourcing Practice Crowdsourcing has become the prominent paradigm for creating NLP datasets (CallisonBurch et al., 2015; Sabou et al., 2014). It has been used for various NLU tasks, including Question Answering (Rajpurkar et al., 2018; Mihaylov 1161 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1161–1166, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Experimental Setup We focus on crowdsourcing tasks where workers produce full-length sentences. We first describe the datasets we test our hypothesis on, and then provide details on the model and training. Datasets We consider recent NLU datasets, for which the annotator IDs are available. • MNLI (matched) (William"
D19-1107,N18-1101,0,0.204621,"an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen at training time. We observe that often generalization to new annotators fails, and that augmenting the training set with a small number of examples fro"
D19-1107,P11-1122,0,0.0414743,"models are able to recognize the most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to"
D19-1107,D13-1020,0,0.0488703,"most productive annotators. Moreover, we show that often models do not generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of mo"
D19-1107,sabou-etal-2014-corpus,0,0.0229497,"notators fails, and that augmenting the training set with a small number of examples from these annotators substantially increases performance. Taken together, our experiments show that annotator bias exists in current NLU datasets, which can lead to problems in model generalization to new users. Hence, we propose that annotator bias should be monitored at data collection time and to tackle it by having the test set include examples from a disjoint set of annotators. 2 Crowdsourcing Practice Crowdsourcing has become the prominent paradigm for creating NLP datasets (CallisonBurch et al., 2015; Sabou et al., 2014). It has been used for various NLU tasks, including Question Answering (Rajpurkar et al., 2018; Mihaylov 1161 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1161–1166, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Experimental Setup We focus on crowdsourcing tasks where workers produce full-length sentences. We first describe the datasets we test our hypothesis on, and then provide details on the model and training. Datasets We cons"
D19-1107,K17-1004,0,0.0163273,"ity annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illust"
D19-1107,N19-1421,1,0.810675,"ral model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether models generalize to annotators that were not seen at training time. We observe that often generalization to new annotators fails, and that augmenting the training set with a small number of examples from these annotators substantially increases performance. Taken together, our experime"
D19-1107,W17-2623,0,0.0270496,"generalize well to examples from annotators that did not contribute to the training set. Our findings suggest that annotator bias should be monitored during dataset creation, and that test set annotators should be disjoint from training set annotators. 1 Introduction Generating large datasets has become one of the main drivers of progress in natural language understanding (NLU). The prevalent method for creating new datasets is through crowdsourcing, where examples are generated by workers (Zaidan and Callison-Burch, 2011; Richardson et al., 2013; Bowman et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2017). A common recent practice is to choose a small group of workers who produce high-quality annotations, and massively generate examples using these workers. Having only a few workers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understa"
D19-1107,L18-1239,0,0.0274676,"ers annotate the majority of dataset examples raises concerns about data diversity and the ability of models to generalize, especially when the crowdsourcing task is to generate free text. If an annotator consistently uses language patterns that correlate with the labels, a neural model can pick up on those, which can lead to an over-estimation of model performance. In this paper, we continue recent efforts to understand biases that are introduced during the process of data creation (Levy et al., 2015; Schwartz et al., 2017; Gururangan et al., 2018; Glockner et al., 2018; Poliak et al., 2018; Tsuchiya, 2018; Aharoni and Goldberg, 2018; Paun et al., 2018). We investigate this form of bias, termed annotator bias, and perform multiple experiments over three recent NLU datasets: MNLI (Williams et al., 2018), O PEN B OOK QA. (Mihaylov et al., 2018), and C OMMONSENSE QA (Talmor et al., 2019). First, we establish that annotator information improves model performance by supplying annotator IDs as part of the input features. Second, we show that models are able to recognize annotators that generated many examples, illustrating that annotator information is captured by the model. Last, we test whether mod"
D19-1427,P13-2037,0,0.288934,"Missing"
D19-1427,W17-7509,0,0.123792,"rovino de que en dicha c´amara chino y el envite pa´ıs tampoco nada pero por aqu´ı solo sin familia ni nada Table 1: Examples of the output of IBM’s English and Spanish speech recognition systems on code-switched audio. we have huge amounts of data—for each language separately, and combine them somehow into a code-switched LM. While this is relatively straightforward to do in an n-gram language model, it is not obvious how to perform such an LM combination in a non-markovian, RNN-based language model. We use a protocol for LSTMbased CS LM training which can take advantage of monolingual data (Baheti et al., 2017) and verify its effectiveness (Section 5). Based on the new evaluation scheme we present, we further propose to learn a model for this ranking task using discriminative training. This model, as opposed to LMs, no longer depends on estimating next-word probabilities for the entire vocabulary. Instead, during training the model is introduced with positive and negative examples and is encouraged to prefer the positive examples over the negative ones. This model gives significantly better results (Section 6). Our contributions in this work are four-fold: (a) We propose a new, vocabulary-size indep"
D19-1427,W16-5804,0,0.0155922,"the standard LM training, it can still be improved quite a bit. Table 7 lists some of the mistakes of the F INE -T UNED - DISCRIMINATIVE model: in examples 1, 2 and 3, the gold sentence was code-switched but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010; Zaremba et al., 2014; Gal and Ghahramani, 2016; Foerster et al., 2017; Melis et al., 2017). Some work has been done also"
D19-1427,J92-4003,0,0.143021,"tence was code-switched but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010; Zaremba et al., 2014; Gal and Ghahramani, 2016; Foerster et al., 2017; Melis et al., 2017). Some work has been done also on optimizing LMs for ASR purposes, using discriminative training. Kuo et al. (2002), Roark et al. (2007) and Dikici et al. (2013) all improve LM for ASR by maximizing the probab"
D19-1427,W16-5801,0,0.0278389,"Missing"
D19-1427,O09-5003,0,0.0144031,"use candidates of ASR systems as “negative” examples and train n-gram LMs or use linear models for classifying candidates. A closer approach to ours is used by Huang et al. (2018). There, they optimize an RNNLM with a discriminative loss as part of training an ASR system. Unlike our proposed model, they still use the standard setting of LM. In addition, their training is coupled with an end-to-end ASR system, in particular, as in previous works, the “negative” examples they use are candidates of that ASR system. LM for CS Some work has been done also specifically on LM for code-switching. In Chan et al. (2009), the authors compare different n-gram language models, Vu et al. (2012) suggest to improve language modeling by generating artificial code-switched text. Li and Fung (2012) propose a language model that incorporates a syntactic constraint and combine both a code-switched LM and a monolingual LM in the decoding process of an ASR system. Later on, they also suggest to incorporate a different syntactic constraint and to learn the language model from bilingual data using it (Li and Fung, 2014). Pratapa et al. (2018) also use a syntactic constraint to improve LM by augmenting synthetically created"
D19-1427,P96-1041,0,0.523682,"hed but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010; Zaremba et al., 2014; Gal and Ghahramani, 2016; Foerster et al., 2017; Melis et al., 2017). Some work has been done also on optimizing LMs for ASR purposes, using discriminative training. Kuo et al. (2002), Roark et al. (2007) and Dikici et al. (2013) all improve LM for ASR by maximizing the probability of the correct cand"
D19-1427,D18-1346,0,0.023085,"Missing"
D19-1427,W16-5811,0,0.0237976,"icantly better than the standard LM training, it can still be improved quite a bit. Table 7 lists some of the mistakes of the F INE -T UNED - DISCRIMINATIVE model: in examples 1, 2 and 3, the gold sentence was code-switched but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010; Zaremba et al., 2014; Gal and Ghahramani, 2016; Foerster et al., 2017; Melis et al., 2017). Some wo"
D19-1427,D18-1150,0,0.0821181,"equence is defined as: 1 2− N PN i=1 log2 M (wi ) where M (wi ) is the probability the model assigns to wi . A better model is expected to give higher probability to sentences in the test set, that is, lower perplexity. However, this measure is not always well aligned with the quality of a language model as it should be. For example, Tran et al. (2018) show that RNNs capture long-distance syntactic dependencies better than attention-only LMs, despite having higher (worse) perplexity. Similarly, better perplexities often do not translate to better word-error-rate (WER) scores in an ASR system (Huang et al., 2018). 4176 This highlights a shortcoming of perplexitybased evaluation: the method is rewarded for assigning high probability to gold sentences, but is not directly penalized for assigning high probability to highly implausible sentences. When used in a speech recognition setup, the LM is expected to do just that: score correct sentences above incorrect hypotheses. Another shortcoming of perplexity-based evaluation is that it requires the compared models to have the same support (in other words, the same vocabulary). Simply adding words to the vocabulary, even if no additional change is done to th"
D19-1427,C12-1102,0,0.0337139,"(2018). There, they optimize an RNNLM with a discriminative loss as part of training an ASR system. Unlike our proposed model, they still use the standard setting of LM. In addition, their training is coupled with an end-to-end ASR system, in particular, as in previous works, the “negative” examples they use are candidates of that ASR system. LM for CS Some work has been done also specifically on LM for code-switching. In Chan et al. (2009), the authors compare different n-gram language models, Vu et al. (2012) suggest to improve language modeling by generating artificial code-switched text. Li and Fung (2012) propose a language model that incorporates a syntactic constraint and combine both a code-switched LM and a monolingual LM in the decoding process of an ASR system. Later on, they also suggest to incorporate a different syntactic constraint and to learn the language model from bilingual data using it (Li and Fung, 2014). Pratapa et al. (2018) also use a syntactic constraint to improve LM by augmenting synthetically created CS sentences in which this constraint is not violated. Adel et al. (2013a) introduce an RNN based LM, where the output layer is factorized into languages, and POS tags are"
D19-1427,D14-1098,0,0.15103,"tes of that ASR system. LM for CS Some work has been done also specifically on LM for code-switching. In Chan et al. (2009), the authors compare different n-gram language models, Vu et al. (2012) suggest to improve language modeling by generating artificial code-switched text. Li and Fung (2012) propose a language model that incorporates a syntactic constraint and combine both a code-switched LM and a monolingual LM in the decoding process of an ASR system. Later on, they also suggest to incorporate a different syntactic constraint and to learn the language model from bilingual data using it (Li and Fung, 2014). Pratapa et al. (2018) also use a syntactic constraint to improve LM by augmenting synthetically created CS sentences in which this constraint is not violated. Adel et al. (2013a) introduce an RNN based LM, where the output layer is factorized into languages, and POS tags are added to the input. In Adel et al. (2013b), they further investigate an n-gram based factorized LM where each word in the input is concatenated with its POS tag and its language identifier. Adel et al. (2014; 2015) also investigate the influence of syntactic and semantic features in the framework of factorized language m"
D19-1427,W16-5805,0,0.0282889,"show mistakes in monolingual sentences. While discriminative training is significantly better than the standard LM training, it can still be improved quite a bit. Table 7 lists some of the mistakes of the F INE -T UNED - DISCRIMINATIVE model: in examples 1, 2 and 3, the gold sentence was code-switched but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010; Zaremba et al., 2014;"
D19-1427,D14-1105,0,0.0197492,"training is significantly better than the standard LM training, it can still be improved quite a bit. Table 7 lists some of the mistakes of the F INE -T UNED - DISCRIMINATIVE model: in examples 1, 2 and 3, the gold sentence was code-switched but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010; Zaremba et al., 2014; Gal and Ghahramani, 2016; Foerster et al., 2017; Melis et"
D19-1427,P18-1143,0,0.419854,"em. LM for CS Some work has been done also specifically on LM for code-switching. In Chan et al. (2009), the authors compare different n-gram language models, Vu et al. (2012) suggest to improve language modeling by generating artificial code-switched text. Li and Fung (2012) propose a language model that incorporates a syntactic constraint and combine both a code-switched LM and a monolingual LM in the decoding process of an ASR system. Later on, they also suggest to incorporate a different syntactic constraint and to learn the language model from bilingual data using it (Li and Fung, 2014). Pratapa et al. (2018) also use a syntactic constraint to improve LM by augmenting synthetically created CS sentences in which this constraint is not violated. Adel et al. (2013a) introduce an RNN based LM, where the output layer is factorized into languages, and POS tags are added to the input. In Adel et al. (2013b), they further investigate an n-gram based factorized LM where each word in the input is concatenated with its POS tag and its language identifier. Adel et al. (2014; 2015) also investigate the influence of syntactic and semantic features in the framework of factorized language models. Sreeram and Sinh"
D19-1427,W14-3907,0,0.0276551,"one. Examples 5 and 6 show mistakes in monolingual sentences. While discriminative training is significantly better than the standard LM training, it can still be improved quite a bit. Table 7 lists some of the mistakes of the F INE -T UNED - DISCRIMINATIVE model: in examples 1, 2 and 3, the gold sentence was code-switched but the model preferred a monolingual one, in example 4 the model prefers a wrong CS sentence over the gold monolingual one, and in 5 and 6 the model makes mistakes in monolingual sentences. 10 Related Work CS Most prior work on CS focused on Language Identification (LID) (Solorio et al., 2014; Molina et al., 2016) and POS tagging (Solorio and Liu, 2008; Vyas et al., 2014; Ghosh et al., 2016; Barman et al., 2016). In this work we focus on language modeling, which we find more challenging. LM Language models have been traditionally created by using the n-grams approach (Brown et al., 1992; Chen and Goodman, 1996). Recently, neural models gained more popularity, both using a feed-forward network for an n-gram language model (Bengio et al., 2003; Morin and Bengio, 2005) and using recurrent architectures that are fed with the sequence of words, one word at a time (Mikolov et al., 2010;"
D19-1427,D08-1110,0,0.222966,"for this ranking task with monolingual data and show significant improvement over various baselines. The CS LM evaluation dataset and the code for the model are available at https://github. com/gonenhila/codeswitching-lm. 2 Background Code-Switching Code-switching (CS) is defined as the use of two languages at the same discourse (Poplack, 1980). The mixing of different languages in various levels has been widely studied from social and linguistic point of view (Auer, 1999; Muysken, 2000; Bullock and Toribio, 2009), and started getting attention also in the NLP community in the past few years (Solorio and Liu, 2008; Adel et al., 2013a; Cotterell et al., 2014). Below is an example of code-switching between Spanish and English (taken from the Bangor Miami corpus described in Section 7). Translation to English follows: • “that es su t´ıo that has lived with him like I don’t know how like ya several years...” that his uncle who has lived with him like, I don’t know how, like several years already... Code-switching is becoming increasingly popular, mainly among bilingual communities. Yet, one of the main challenges when dealing with CS is the limited data and its unique nature: it is usually found in non sta"
D19-1427,D18-1503,0,0.0588144,"Missing"
E09-1038,W07-2219,1,0.896176,"Missing"
E09-1038,C08-1112,1,0.875724,"Missing"
E09-1038,P08-1083,1,0.830098,"uter Sciences, Ben Gurion University † Funded by the Dutch Science Foundation (NWO), grant number 017.001.271. ‡ Post-doctoral fellow, Deutsche Telekom labs at Ben Gurion University 1 This is not the case with other languages, and also not true for English when adaptation scenarios are considered. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327–335, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 327 morphological structure. This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a). This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001). Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2 , but its tagset is different than the one used by the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic. This difference in perspective yields different performance for parsers induced from tagged data, and"
E09-1038,P06-3009,1,0.912813,"). The remaining question is how to estimate p(hw, tKC i|tKC ). Here, we use either the LexFilter (estimated over all rare events) or LexProbs (estimated via the semisupervised emission probabilities)models, as defined in Section 4.1 above. Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance. For methodological reasons, this issue has either been setaside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological disambiguator is run prior to parsing to determine the correct segmentation. However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model. Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence. The arcs of the lattice are hw, ti pairs, and a lattice parser is used to build a parse over the"
E09-1038,adler-etal-2008-tagging,1,0.917245,"uter Sciences, Ben Gurion University † Funded by the Dutch Science Foundation (NWO), grant number 017.001.271. ‡ Post-doctoral fellow, Deutsche Telekom labs at Ben Gurion University 1 This is not the case with other languages, and also not true for English when adaptation scenarios are considered. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327–335, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 327 morphological structure. This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a). This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001). Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2 , but its tagset is different than the one used by the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic. This difference in perspective yields different performance for parsers induced from tagged data, and"
E09-1038,D07-1022,0,0.0399562,"Section 4.1 above. Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance. For methodological reasons, this issue has either been setaside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological disambiguator is run prior to parsing to determine the correct segmentation. However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model. Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence. The arcs of the lattice are hw, ti pairs, and a lattice parser is used to build a parse over the lattice. The Viterbi parse over the lattice chooses a lattice path, which induces a segmentation over the input sentence. Thus, parsing and segmentation are performed jointly. Lexical rules in the model are defined over the latt"
E09-1038,P08-1043,1,0.708181,"he original Treebank, KC which is the Treebank converted to use the KC Analyzer tagset, and Layered, which is the layered representation described above. The details of the lexical models vary according to the representation we choose to work with. For the TB setting, our lexical rules are of the form 9 Details of the grammar: all functional information is removed from the non-terminals, finite and non-finite verbs, as well as possessive and other PPs are distinguished, definiteness structure of constituents is marked, and parent annotation is employed. It is the same grammar as described in (Goldberg and Tsarfaty, 2008). 331 ttb → w. Only the Baseline models are relevant here, as the tagset is not compatible with that of the external lexicon. For the KC setting, our lexical rules are of the form tkc → w, and their probabilities are estimated as described above. Note that this setting requires our trees to be tagged with the new (KC) tagset, and parsed sentences are also tagged with this tagset. For the Layered setting, we use lexical rules of the form ttb → w. Reliable events are estimated as usual, via relative frequency over the original treebank. For rare events, we estimate p(ttb → w|ttb ) = p(ttb → tkc"
E09-1038,P08-1085,1,0.870295,"Missing"
E09-1038,W07-0808,1,0.858605,"Missing"
E09-1038,P06-1084,1,\N,Missing
E17-1072,Q16-1031,0,0.026374,".3869 .4119 .1364 .2408 .1280 .1877 .1403 .1791 .2299 .2759 .2207 .2598 0.2830 0 Multilingual SID-SGNS .4433 .4632 .4893 .5015 .4047 .4151 .4091 .4302 .2989 .3049 .2514 .2753 .2737 .3195 .2404 .2945 .3304 .3893 .3509 .3868 .4058 .4376 .1605 .3082 .1591 .2584 .1448 .2403 .2482 .3372 .2437 .3080 0.3289 22 Table 3: The performance of SID-SGNS compared to state-of-the-art cross-lingual embedding methods and traditional alignment methods. advantage, SID-SGNS performs significantly better than the other methods combined.6 This result is similar in vein to recent findings in the parsing literature (Ammar et al., 2016; Guo et al., 2016), where multi-lingual transfer was shown to improve upon bilingual transfer. In absolute terms, Multilingual SID-SGNS’s performance is still very low. However, this experiment demonstrates that one way of making significant improvement in cross-lingual embeddings is by considering additional sources of information, such as the multi-lingual signal demonstrated here. We hypothesize that, regardless of the algorithmic approach, relying solely on sentence IDs from bilingual parallel corpora will probably not be able to improve much beyond IBM Model-1. 6 Data Paradigms In §2, we"
E17-1072,J93-2003,0,0.156922,"oss-lingual similarity. Another important delineation of this work is that we focus on algorithms that rely on sentence-aligned data; in part, because these algorithms are particularly interesting for low-resource languages, but also to make our analysis and comparison with alignment algorithms more focused. We observe that the top performing embedding algorithms share the same underlying feature space – sentence IDs – while their different algorithmic approaches seem to have a negligible impact on performance. We also notice that several statistical alignment algorithms, such as IBM Model-1 (Brown et al., 1993), operate under the same data assumptions. Specifically, we find that using the translation probabilities learnt by Model-1 as the cross-lingual similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na¨ıve, the actual difference in performance between different approaches is marginal. Th"
E17-1072,E14-1049,0,0.0924706,"nce. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 1 Introduction Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and Søgaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of ∗ These authors contributed equally to this work. 765 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 765–774, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics generalize it, resulting in an embedding model that is as effective as Model-1 and o"
E17-1072,N15-1157,1,0.39673,"els between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 1 Introduction Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and Søgaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of ∗ These authors contributed equally to this work. 765 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 765–774, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics generalize it, resulting in an embedding model that is as effective as Model-1 and other sophisticated state-of-the-art embedding methods, but t"
E17-1072,graca-etal-2008-building,0,0.0204464,"nt-aligned data, and can be built in a similar manner: create a pseudo-bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a sour"
E17-1072,J03-1002,0,0.116258,"similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na¨ıve, the actual difference in performance between different approaches is marginal. This leads us to revisit another statistical alignment algorithm from the literature that uses the same sentence-based signal – the Dice aligner (Och and Ney, 2003). We first observe that the vanilla Dice aligner is significantly outperformed by the Model-1 aligner. We then recast Dice as the dot-product between two word vectors (based on the sentence ID feature space), which allows us to While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment alg"
E17-1072,P15-1165,1,0.0916604,"Missing"
E17-1072,W11-4615,0,0.0169403,"bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not aligned with anything, whereas ta"
E17-1072,C12-1089,0,0.0182685,"show that a generalization of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representatio"
E17-1072,2005.mtsummit-papers.11,0,0.0150526,"Missing"
E17-1072,J10-4005,0,0.075475,"Missing"
E17-1072,P16-1157,0,0.0326699,"-based model over the same features. 1 bitbucket.org/omerlevy/xling_ embeddings We study which factors determine the success of cross-lingual word embedding algorithms 2 Background: Cross-lingual Embeddings 766 that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions. We go on to generalize one of these, the Dice aligner, showing that one variant is a much stronger baseline for cross-lingual word embedding algorithms than standard baselines. Finally, we would like to point out the work of Upadhyay et al. (2016), who studied how different data assumptions affect embedding quality in both monolingual and cross-lingual tasks. Our work focuses on one specific data assumption (sentencelevel alignments) and only on cross-lingual usage. This more restricted setting allows us to: (a) compare embeddings to alignment algorithms, (b) decouple the feature space from the algorithm, and make a more specific observation about the contribution of each component to the end result. In that sense, our findings complement those of Upadhyay et al. (2016). from the word in question, and defines a slightly different inter"
E17-1072,W14-1613,0,0.0112876,"alignment algorithms that also rely on sentence ID signals. We show that a generalization of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep"
E17-1072,Q15-1016,1,0.0780471,"Missing"
E17-1072,W15-1521,0,0.0257223,"oth source and target language sentences as the same intermediate sentence vector. Recently, a simpler model, BilBOWA (Gouws et al., 2015), showed similar performance without using a hidden sentencerepresentation layer, giving it a dramatic speed advantage over its predecessors. BilBOWA is essentially an extension of skip-grams with negative sampling (SGNS) (Mikolov et al., 2013b), which simultaneously optimizes each word’s similarity to its inter-lingual context (words that appeared in the aligned target language sentence) and its intra-lingual context (as in the original monolingual model). Luong et al. (2015) proposed a similar SGNS-based model over the same features. 1 bitbucket.org/omerlevy/xling_ embeddings We study which factors determine the success of cross-lingual word embedding algorithms 2 Background: Cross-lingual Embeddings 766 that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions. We go on to generalize one of these, the Dice aligner, showing that one variant is a much stronger baseline for cross-lingual word embedding algorithms than standard baselines. Finally, we would like t"
E17-1072,D13-1141,0,0.065972,"on of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final la"
E17-1072,W03-0301,0,0.0358193,"ilar manner: create a pseudo-bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not al"
E17-1072,cakmak-etal-2012-word,0,\N,Missing
E17-2055,P81-1022,0,0.680846,"Missing"
E17-2055,P15-1033,0,0.0212615,"computed by an MLP that receives a feature vector that is a concatenation of the original parser’s features and the conjunction specific features. The scoring MLP and the parser are trained jointly. 4 General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat 5 Experiments We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup of Kiperwasser and Goldberg (2016): (1) A word is represented as the concatenation of randomly initialized vector and pre-trained vector (taken from Dyer et al. (2015)); (2) The word and POS embeddings are t"
E17-2055,D16-1003,1,0.847083,"Kipwasser and Goldberg (2016), by adding explicit features that target the conjunction relation and focus on various aspects of symmetry between the potential conjuncts’ head words. We show improvement in dependency parsing scores and in conj attachment. 2 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. Symmetry between Conjuncts It is well known that conjuncts tend to be semantically related and often have a similar syntactic structure. This property of coordination was used as a guiding principle in previous work on coordination disambiguation (Hara et al., 2009; Hogan, 2007; Shimbo and Hara, 2007; Ficler and Goldberg, 2016). While these focus on symmetry between conjuncts in constituency structures, we use the symmetry assumption for the purpose of improving dependency parsing. Here is a simple example of dependency tree that include conjunction: Table 1: The most common conj attachments in the Penn TreeBank dependency conversion. 3 pobj amod det nsubj nn conj aux prep cc The bond fund will invest in high-grade or medium-grade bonds The edge labeled with conj connects the first conjunct head to the heads of the other conjuncts. In more complex conjuncts, the subtrees under the nodes connected by conj are often s"
E17-2055,P09-1109,0,0.023521,"017 Association for Computational Linguistics parser by Kipwasser and Goldberg (2016), by adding explicit features that target the conjunction relation and focus on various aspects of symmetry between the potential conjuncts’ head words. We show improvement in dependency parsing scores and in conj attachment. 2 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. Symmetry between Conjuncts It is well known that conjuncts tend to be semantically related and often have a similar syntactic structure. This property of coordination was used as a guiding principle in previous work on coordination disambiguation (Hara et al., 2009; Hogan, 2007; Shimbo and Hara, 2007; Ficler and Goldberg, 2016). While these focus on symmetry between conjuncts in constituency structures, we use the symmetry assumption for the purpose of improving dependency parsing. Here is a simple example of dependency tree that include conjunction: Table 1: The most common conj attachments in the Penn TreeBank dependency conversion. 3 pobj amod det nsubj nn conj aux prep cc The bond fund will invest in high-grade or medium-grade bonds The edge labeled with conj connects the first conjunct head to the heads of the other conjuncts. In more complex conju"
E17-2055,P16-1231,0,0.0181814,"features and the conjunction specific features. The scoring MLP and the parser are trained jointly. 4 General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat 5 Experiments We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup of Kiperwasser and Goldberg (2016): (1) A word is represented as the concatenation of randomly initialized vector and pre-trained vector (taken from Dyer et al. (2015)); (2) The word and POS embeddings are tuned during training; (3) Punctuation symbols are not considered in the evaluation; (4) The hyper-p"
E17-2055,P07-1086,0,0.0305463,"Computational Linguistics parser by Kipwasser and Goldberg (2016), by adding explicit features that target the conjunction relation and focus on various aspects of symmetry between the potential conjuncts’ head words. We show improvement in dependency parsing scores and in conj attachment. 2 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. Symmetry between Conjuncts It is well known that conjuncts tend to be semantically related and often have a similar syntactic structure. This property of coordination was used as a guiding principle in previous work on coordination disambiguation (Hara et al., 2009; Hogan, 2007; Shimbo and Hara, 2007; Ficler and Goldberg, 2016). While these focus on symmetry between conjuncts in constituency structures, we use the symmetry assumption for the purpose of improving dependency parsing. Here is a simple example of dependency tree that include conjunction: Table 1: The most common conj attachments in the Penn TreeBank dependency conversion. 3 pobj amod det nsubj nn conj aux prep cc The bond fund will invest in high-grade or medium-grade bonds The edge labeled with conj connects the first conjunct head to the heads of the other conjuncts. In more complex conjuncts, the sub"
E17-2055,D15-1041,0,0.0529792,"Missing"
E17-2055,P99-1069,0,0.0982983,"Missing"
E17-2055,D16-1211,1,0.848424,"t receives a feature vector that is a concatenation of the original parser’s features and the conjunction specific features. The scoring MLP and the parser are trained jointly. 4 General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat 5 Experiments We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup of Kiperwasser and Goldberg (2016): (1) A word is represented as the concatenation of randomly initialized vector and pre-trained vector (taken from Dyer et al. (2015)); (2) The word and POS embeddings are tuned during training; (3) Pun"
E17-2055,Q16-1023,1,0.898682,"relation appears as a right edge, and so it can only be produced by a R IGHT(conj) transition. We compute a score Sconj which is added to the score of the R IGHT(conj) transition that was produced by the parser. Sconj is computed by an MLP that receives a feature vector that is a concatenation of the original parser’s features and the conjunction specific features. The scoring MLP and the parser are trained jointly. 4 General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat 5 Experiments We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup"
E17-2055,P06-4018,0,0.00888915,"Missing"
E17-2055,P11-1068,0,0.0755458,"Missing"
E17-2055,D16-1180,0,0.0138578,"features. The scoring MLP and the parser are trained jointly. 4 General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat 5 Experiments We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup of Kiperwasser and Goldberg (2016): (1) A word is represented as the concatenation of randomly initialized vector and pre-trained vector (taken from Dyer et al. (2015)); (2) The word and POS embeddings are tuned during training; (3) Punctuation symbols are not considered in the evaluation; (4) The hyper-parameters values are as in Kiperwasser an"
E17-2055,J93-2004,0,0.058135,"Missing"
E17-2055,D07-1064,0,0.0378935,"l Linguistics parser by Kipwasser and Goldberg (2016), by adding explicit features that target the conjunction relation and focus on various aspects of symmetry between the potential conjuncts’ head words. We show improvement in dependency parsing scores and in conj attachment. 2 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. Symmetry between Conjuncts It is well known that conjuncts tend to be semantically related and often have a similar syntactic structure. This property of coordination was used as a guiding principle in previous work on coordination disambiguation (Hara et al., 2009; Hogan, 2007; Shimbo and Hara, 2007; Ficler and Goldberg, 2016). While these focus on symmetry between conjuncts in constituency structures, we use the symmetry assumption for the purpose of improving dependency parsing. Here is a simple example of dependency tree that include conjunction: Table 1: The most common conj attachments in the Penn TreeBank dependency conversion. 3 pobj amod det nsubj nn conj aux prep cc The bond fund will invest in high-grade or medium-grade bonds The edge labeled with conj connects the first conjunct head to the heads of the other conjuncts. In more complex conjuncts, the subtrees under the nodes c"
E17-2055,P15-1032,0,0.0251327,"he parser. Sconj is computed by an MLP that receives a feature vector that is a concatenation of the original parser’s features and the conjunction specific features. The scoring MLP and the parser are trained jointly. 4 General Parsing Results Table 3 compares our results to the unmodified BIST parser. The extended parser achieves 0.1 points improvement in UAS and 0.2 points in LAS comparing to Kiperwasser and Goldberg (2016). This is a strong baseline, which so far held the highest results among greedy transition based parsers that were trained on the PTB only, including e.g. the parsers of Weiss et al (2015), Dyer et al (2015) and Ballesteros et al (2016). Stronger absolute parsing numbers are reported by Andor et al (2016) (using a beam); and Kuncoro et al (2016) and Dozat 5 Experiments We evaluate the extended parsing model on the Stanford Dependencies (De Marneffe and Manning, 2008) version of the Penn Treebank. We adapt BIST-parser code to run with the DyNet toolkit1 and add our changes. We follow the setup of Kiperwasser and Goldberg (2016): (1) A word is represented as the concatenation of randomly initialized vector and pre-trained vector (taken from Dyer et al. (2015)); (2) The word and P"
E17-2055,P05-1022,0,\N,Missing
E17-2067,J15-4004,0,0.0645703,"ble morphological components. MorphSimk calculates the average rate of compatible morphological values. More formally, k (w) M orphoSimk (w) = 1 − M orphoDist , where k·|mw | |mw |is the number of grammatical components specified in w’s morphological tag. We use k=10 and calculate the average MorphoSim score over 100 randomly chosen words. 3 https://github.com/facebookresearch/fastText Our code is available on https://github.com/ oavraham1/prop2vec, our datasets on https:// github.com/oavraham1/ag-evaluation 5 E.g., WordSim353 (Finkelstein et al., 2001), RW (Luong et al., 2013) and SimLex999 (Hill et al., 2015) 4 424 W L WL WM LM WLM 1st הביטה:gaze:VB.F.S.3.PAST הביטי:gaze:VB.F.S.2.IMPERATIVE נביט:gaze:VB.MF.P.1.FUTURE חייכה:smile:VB.F.S.3.PAST הביטה:gaze:VB.F.S.3.PAST הביטה:gaze:VB.F.S.3.PAST 2nd חייכה:smile:VB.F.S.3.PAST התבונן:watch:VB.M.S.3.PAST התבוננה:watch:VB.F.S.3.PAST נחבלה:injure:VB.F.S.3.PAST התבוננה:watch:VB.F.S.3.PAST התבוננה:watch:VB.F.S.3.PAST 3rd מתייפחת:cry:VB.F.S.3.PRESENT בהו:stare:VB.MF.P.3.PAST בוהה:stare:VB.F.S.3.PRESENT נשפה:blow:VB.F.S.3.PAST זזה:move:VB.F.S.3.PAST פסעה:walk:VB.F.S.3.PAST Table 1: Top-3 similarities for the word [( הסתכלה"
E17-2067,P13-1149,0,0.0422951,"ion for Computational Linguistics We compare different configurations of morphology-driven models, while controlling for the components composing the representation. We then separately evaluate the semantic and morphological performance of each model, on rare and on common words. We focus on inflectional (rather than derivational) morphology. This is due to the fact that derivations (e.g. affected → unaffected) often drastically change the meaning of the word, and therefore the benefit of having similar representations for words with the same derivational base is questionable, as discussed by Lazaridou et al (2013) and Luong et al (2013). Inflections (e.g. walked → walking), in contrast, preserve the word lexical meaning, and only change its grammatical categories values. Our experiments are performed on Modern Hebrew, a language with rich inflectional morphological system. We build on a recently introduced evaluation dataset for semantic similarity in Modern Hebrew (Avraham and Goldberg, 2016), which we further extend with a collection of rare words. We also create datasets for morphological similarity, for common and rare words. Hebrew’s morphology is not concatenative, so unlike most previous work we"
E17-2067,W13-3512,0,0.208166,"istics We compare different configurations of morphology-driven models, while controlling for the components composing the representation. We then separately evaluate the semantic and morphological performance of each model, on rare and on common words. We focus on inflectional (rather than derivational) morphology. This is due to the fact that derivations (e.g. affected → unaffected) often drastically change the meaning of the word, and therefore the benefit of having similar representations for words with the same derivational base is questionable, as discussed by Lazaridou et al (2013) and Luong et al (2013). Inflections (e.g. walked → walking), in contrast, preserve the word lexical meaning, and only change its grammatical categories values. Our experiments are performed on Modern Hebrew, a language with rich inflectional morphological system. We build on a recently introduced evaluation dataset for semantic similarity in Modern Hebrew (Avraham and Goldberg, 2016), which we further extend with a collection of rare words. We also create datasets for morphological similarity, for common and rare words. Hebrew’s morphology is not concatenative, so unlike most previous work we do not break the words"
E17-2067,C14-1015,0,0.0594258,"Missing"
E17-2067,N15-1186,0,0.107888,"Missing"
E17-2067,N15-1140,0,0.084347,"Missing"
J13-1007,P06-1084,1,0.938931,"After establishing the tag set, it is relatively straightforward to add lemmas to the lexicon, and the automatic inflection process guarantees good coverage of all the possible inflections. This is much more efficient than annotating enough text to obtain a similar coverage. 2.3.4 Hebrew Morphological Disambiguator. The morphological analyzer provides the possible set of analyses for each token, but does not disambiguate the correct analysis in context. A morphological disambiguator (henceforth “the Hebrew tagger” or “tagger”) was developed by Meni Adler at Ben-Gurion University of the Negev (Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and Elhadad 2008). After the (extended) morphological analyzer assigns the possible analyses for each token in an 10 http://www.mila.cs.technion.ac.il/mila/files/treebank/Decisions-Corpus1-5001.v1.pdf. 129 Computational Linguistics Volume 39, Number 1 input sentence, the tagger takes the output of the analyzer as input and chooses the single best analysis for the entire sentence (performing both token segmentation of words and part-of-speech assignment for each word). The tagger is an HMM-based sequential model that is trained in a semi-supervised fashion using E"
J13-1007,P08-1083,1,0.932761,"exicon-based morphological analyzer which can assign morphological analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew tokens. The lexicon (henceforth the KC Analyzer) is developed and maintained by the Knowledge Center for Processing Hebrew (Itai and Wintner 2008). It is based on a lexicon of roughly 25,000 word lemmas and their inflection patterns. From these, 562,439 unique word forms are derived. These are then prefixed (subject to constraints) by 73 prepositional prefixes. Even with this seemingly large vocabulary, the KC Analyzer’s coverage is not perfect. In Adler et al. (2008a), we present a machine-learning method that is trained on the basis of the analyzer and that can guess possible analyses for words unknown to the analyzer with reasonable accuracies. Using this extension, the analyzer has perfect coverage (even though the quality is obviously better for words that are present in the analyzer’s database). The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed in depth in BGU Computational Linguistics Group (2008). Creating a resource such as the morphological analyzer for a morphologically rich language is a worthwhile and cost-"
J13-1007,adler-etal-2008-tagging,1,0.954615,"exicon-based morphological analyzer which can assign morphological analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew tokens. The lexicon (henceforth the KC Analyzer) is developed and maintained by the Knowledge Center for Processing Hebrew (Itai and Wintner 2008). It is based on a lexicon of roughly 25,000 word lemmas and their inflection patterns. From these, 562,439 unique word forms are derived. These are then prefixed (subject to constraints) by 73 prepositional prefixes. Even with this seemingly large vocabulary, the KC Analyzer’s coverage is not perfect. In Adler et al. (2008a), we present a machine-learning method that is trained on the basis of the analyzer and that can guess possible analyses for words unknown to the analyzer with reasonable accuracies. Using this extension, the analyzer has perfect coverage (even though the quality is obviously better for words that are present in the analyzer’s database). The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed in depth in BGU Computational Linguistics Group (2008). Creating a resource such as the morphological analyzer for a morphologically rich language is a worthwhile and cost-"
J13-1007,W10-1408,0,0.0884311,"Missing"
J13-1007,W05-0706,0,0.058744,"Missing"
J13-1007,P89-1018,0,0.508769,"the PCFG-LA BerkeleyParser to accept lattice input at inference time. Lattice parsing allows us to preserve the segmentation ambiguity and present it to the parser, instead of committing to a specific segmentation prior to parsing. This way segmentation decisions are performed in the parser as part of the global search for the most probable structure, and can be affected by global syntactic considerations. We show in Section 9 that this methodology is indeed superior to the pipeline approach. Early descriptions of algorithms for parsing over word lattices can be found in Lang (1974, 1988) and Billott and Lang (1989). Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima’an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010). 20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007). There, lattice arc weights are assigned based on aggregate quantities (forward-backward tagging marginals) derived from a discriminative CRF tagging model. This approach is not ideal from a modeling perspective, as it makes eac"
J13-1007,P11-2037,1,0.903779,"Missing"
J13-1007,W09-3821,0,0.0478236,"Missing"
J13-1007,W09-1008,0,0.0276974,"Missing"
J13-1007,P05-1022,0,0.0427829,"ion mechanism is valid by applying the procedure to the gold-standard trees in the training-set and checking that (1) the propagated features agree with the manually marked ones, and (2) none of the training-set trees were filtered due to agreement violation. We did find a few cases in which the propagated features disagreed with the manually marked ones, and a few gold-standard trees that the mechanism marked as containing an agreement violation. All of these cases were due to mistakes in the manual annotation. Connections to parse-reranking. Our implementation is similar to parse-reranking (Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model agreement as soft constraints, we could have incorporated this information as features in a reranking model. The filter approach differs in that it poses hard constraints and not soft ones, pruning away parts of the search space entirely. Thus, the use of k-best list is merely a technical detail in our implementation—the agreement information is 146 Goldberg and Elhadad Parsing System for Hebrew easily decomposable and the hard constraints can be efficiently incorporated into the CKY search procedure. 9. Evaluation and Results Data set. For"
J13-1007,D07-1022,0,0.570078,"Missing"
J13-1007,J05-1003,0,0.0303153,"pplying the procedure to the gold-standard trees in the training-set and checking that (1) the propagated features agree with the manually marked ones, and (2) none of the training-set trees were filtered due to agreement violation. We did find a few cases in which the propagated features disagreed with the manually marked ones, and a few gold-standard trees that the mechanism marked as containing an agreement violation. All of these cases were due to mistakes in the manual annotation. Connections to parse-reranking. Our implementation is similar to parse-reranking (Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model agreement as soft constraints, we could have incorporated this information as features in a reranking model. The filter approach differs in that it poses hard constraints and not soft ones, pruning away parts of the search space entirely. Thus, the use of k-best list is merely a technical detail in our implementation—the agreement information is 146 Goldberg and Elhadad Parsing System for Hebrew easily decomposable and the hard constraints can be efficiently incorporated into the CKY search procedure. 9. Evaluation and Results Data set. For all the experiments we"
J13-1007,J85-1006,0,0.654541,"ties (forward-backward tagging marginals) derived from a discriminative CRF tagging model. This approach is not ideal from a modeling perspective, as it makes each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one. In this work, a sequential tagging model is not used at all. If the use of a sequential model is desired, an alternative method for integrating a sequence model and a syntactic model is making the models “negotiate” an agreed upon structure that maximizes the score under both models, using optimization techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into natural language processing (Rush et al. 2010). 21 Note that finding the most probable segmentation requires summing over all the trees resulting in each segmentation—a much harder task, proven to be NP-complete in Sima’an (1996). 143 Computational Linguistics Volume 39, Number 1 Figure 4 Lattice initialization of the CKY chart. 8. Incorporating Morphological Agreement Inspecting the learned grammars reveal that they do not encode any knowledge of morphological agreement: The split categories for nouns, verbs, and adjectives do not group words according to"
J13-1007,P08-1085,1,0.909226,"Missing"
J13-1007,P11-2124,1,0.884835,"ctive and the noun it modifies). We suggest modeling agreement as a filtering process that is orthogonal to the grammar. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make, without introducing new mistakes. Aspects of the work presented in this article are discussed in earlier publications. Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg et al. (2009) discuss ways of interfacing a treebank-derived PCFG-parser with an external lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA BerkeleyParser. Here we provide a cohesive presentation of the entire system, as well as a more detailed description and an expanded evaluation. We also extend the previous work in several dimensions: We introduce a new method of interfacing the parser and the external lexicon, which contributes to an improved parsing accuracy, and suggest incorporating agreement information as a filter. The methodologies we suggest extend outside the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morpholo"
J13-1007,P08-1043,1,0.722051,"l (the input to the parser) may be uncertain. This happens in Hebrew when a space-delimited token such as ! בצלcan represent either a single word (‘[an] onion’) or a sequence of two words or three words (‘in shadow’ and ‘in the shadow,’ respectively). When computationally feasible, it is best to let the uncertainty be resolved by the parser rather than in a separate preprocessing step. We propose encoding the input-uncertainty in a word lattice, and use lattice parsing (Chappelier et al. 1999; Hall 2005) to perform joint word segmentation and syntactic disambiguation (Cohen and Smith 2007; Goldberg and Tsarfaty 2008). Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model. Using morphological information to improve parsing accuracy. Morphology provides useful hints for resolving syntactic ambiguity, and the parsing model should have a way of utilizing these hints. There is a range of morphological hints than can be utilized: from functional marking elements (such as the ! אתmarker indicating a definite direct object); to elements marking syntactic properties such as definiteness (such as the Hebrew !ה marker); to agreement patterns requiring a compatibility in"
J13-1007,E09-1038,1,0.850733,"ies such as gender, number, and person between syntactic constituents (such as a verb and its subject or an adjective and the noun it modifies). We suggest modeling agreement as a filtering process that is orthogonal to the grammar. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make, without introducing new mistakes. Aspects of the work presented in this article are discussed in earlier publications. Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg et al. (2009) discuss ways of interfacing a treebank-derived PCFG-parser with an external lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA BerkeleyParser. Here we provide a cohesive presentation of the entire system, as well as a more detailed description and an expanded evaluation. We also extend the previous work in several dimensions: We introduce a new method of interfacing the parser and the external lexicon, which contributes to an improved parsing accuracy, and suggest incorporating agreement information as a filter. The methodologies we suggest extend outside the scope"
J13-1007,C10-1045,0,0.40227,"thodologies we suggest extend outside the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsing methodology is useful in any case where the input is uncertain. Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010). Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with 123 Computational Linguistics Volume 39, Number 1 a small treebank, and also for domain adaptation scenarios for English. Finally, the agreement-as-filter methodology is applicable to any morphologically rich language, and although its contribution to the parsing task may be limited, it is of wide applicability to syntactic generation tasks, such as target-side-syntax machine translation in a morphologically rich language. 2. Modern Hebrew 2.1 Lexical and Syntactic Properti"
J13-1007,D09-1087,0,0.0218223,"imilarly, by annotating the symbols as: S → NP @S1 @S1 → VP @S2 @S2 → NP @S3 @S3 → PP the grammar effectively allows only the original rule to be produced. Initial experiments on Hebrew confirm that moving to higher order horizontal markovization (encoding more context in the initial binarized rules) degrades parsing performance, while producing much larger grammars. The PCFG-LA parsing methodology is very robust, producing state-of-the-art accuracies for English, as well as many other languages including German (Petrov and Klein 2008), French (Candito, Crabb´e, and Seddah 2009), and Chinese (Huang and Harper 2009). 4. Baseline Experiments The baseline system is an “out-of-the-box” PCFG-LA parser, as described in Petrov et al. (2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12 The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals. We evaluate the resulting models on the development set, and consider three settings: Seg+POS Oracle: The parser has access to the gold segmentation and POS tags. Seg Oracle: The parser has access to the gold segment"
J13-1007,P09-1059,0,0.0407947,"Missing"
J13-1007,J98-4004,0,0.0403645,"r PPs. We also experimented with splits based on morphological agreement features, which are discussed in Section 8.1. Overall, the learning procedure is capable of producing good splits on its own. We did, however, manage to improve upon it with the following annotation (the annotations were removed prior to evaluation). Subject NPs. Hebrew phrase order is rather flexible, and the subject can appear before or after the verb. Identifying the subject can thus help in grounding the overall structure of the sentence. The subject is also dependent on agreement constraints with the verb. Following Johnson (1998), Klein and Manning (2003) implicitly annotate subject-NPs in English using parent annotation (distinguishing NPs under S from other NPs), with good results. When applied to English, the PCFG-LA also learns to model subject NPs well. Hebrew’s non-configurationality, however, put both Subjects and Objects directly under S, making it much harder to learn the distinction automatically. Explicit marking of subject NPs contributes slightly to the accuracy of the parser. Perhaps more important than the small increase in accuracy is the fact that the parser can identify subjects relatively well. In c"
J13-1007,P03-1054,0,0.54249,"nce of lexical items in context based on a sequential model. The constituency treebank can be used to learn the parameters of a syntactic-model of Hebrew, and the morphological analyzer can be used to provide broad-coverage lexical knowledge. Unfortunately, the treebank and the lexicon/disambiguator follow different annotation schemes, and are therefore incompatible with each other. The annotation gap between the two resources must be bridged before they can be used together. We now turn to survey the components of our Hebrew parsing system. 3. Latent-Annotation State-Split Grammars (PCFG-LA) Klein and Manning (2003) demonstrated that linguistically informed splitting of nonterminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and state-splitting (Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which was then perfected in work by Petrov and colleagues (Petrov et al. 2006; Petrov and Klein 2007; Petrov 2009). State-split models assume that each non-terminal label has a latent annotation that should be recovered. Instead of a single NP symbol, these models hypothesize that there are many different NP symbols, NP1"
J13-1007,C88-1075,0,0.244605,"Missing"
J13-1007,P05-1010,0,0.0893584,"Missing"
J13-1007,W07-0808,1,0.926302,"2.3.5 A Resource Incompatibility Issue. Unfortunately, the KC Analyzer adopted a different tag set than the one used in the treebank, and analyses produced by the KC Analyzer (and hence by the morphological disambiguator) are incompatible with the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew Treebank (TB) tag set is syntactic in nature (“if the word in this particular position functions as an adverb, tag it as an adverb, even though it is listed in the dictionary only as a noun”), whereas the KC tag set (Adler 2007; Netzer et al. 2007; Adler et al. 2008b) takes a lexical approach to POS tagging (“a word can assume only POS tags that would be assigned to it in a dictionary”). The lexical approach does not accommodate generic modification POS tags such as MOD, nor does it allow listing of demonstrative pronouns as adjectives. These divergent perspectives are reflected in different guidelines to human taggers, different principles underlying tag definitions, and different verification procedures. This difference in perspective yields different performances for parsers induced from tagged data, and a simple mapping between the"
J13-1007,N10-1003,0,0.0126867,"tial level, but also caused the parser to, again, not model agreement very well. The reason for this is clear in hindsight: Morphological agreement is an absolute concept, not a fuzzy one (things can either agree or not). Smoothing the probabilities between the different morphology-based split-licensed grammar rules that allow morphological disagreement, and made the grammar lose its discrimination power. This was then reinforced by the training process, which picked on other syntactic factors instead, and further phased out the agreement knowledge. A note on product-grammars. In recent work, Petrov (2010) showed that a committee of latent-variable grammars encoding different grammatical preferences can be combined into a product-grammar that is better than the individual ensemble members. Petrov created the ensemble by training several PCFG-LA parsers on the same data, but using different random seeds when initializing the EM starting point. We attempted to create a similar ensemble by providing the learning process with different linguistically motivated tree annotations (with and without encoding agreement features, with and without encoding definiteness, etc.). The combined parser did incre"
J13-1007,P06-1055,0,0.503651,"small treebank. Several natural questions arise: Can the small size of the treebank be compensated for using other available resources or sources of information? How should the word segmentation issue (that function words do not appear in isolation but attach to the next word, forming ambiguous letter patterns) be handled? Can morphological information be used effectively in order to improve parsing accuracy? We present a system which is based on a state-of-the-art model for constituency parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations (PCFG-LA) model of Petrov et al. (2006), as implemented in the BerkeleyParser. After evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew treebank, we discuss some of its limitations and then go on to extend the PCFG-LA parsing model in several directions, making it more suitable for parsing Hebrew and related languages. Our extensions are based on the following themes. Separation of lexical and syntactic knowledge. There are two kinds of knowledge inherent in a parsing system. One of them is syntactic knowledge governing the way in which words can be combined to form structures, which, in turn, can be comb"
J13-1007,W08-1005,0,0.0164719,"ces the VP to be produced before the NP, but still allows the NP to be dropped. Similarly, by annotating the symbols as: S → NP @S1 @S1 → VP @S2 @S2 → NP @S3 @S3 → PP the grammar effectively allows only the original rule to be produced. Initial experiments on Hebrew confirm that moving to higher order horizontal markovization (encoding more context in the initial binarized rules) degrades parsing performance, while producing much larger grammars. The PCFG-LA parsing methodology is very robust, producing state-of-the-art accuracies for English, as well as many other languages including German (Petrov and Klein 2008), French (Candito, Crabb´e, and Seddah 2009), and Chinese (Huang and Harper 2009). 4. Baseline Experiments The baseline system is an “out-of-the-box” PCFG-LA parser, as described in Petrov et al. (2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12 The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals. We evaluate the resulting models on the development set, and consider three settings: Seg+POS Oracle: The parser has access to the gold"
J13-1007,D10-1001,0,0.0417674,". This approach is not ideal from a modeling perspective, as it makes each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one. In this work, a sequential tagging model is not used at all. If the use of a sequential model is desired, an alternative method for integrating a sequence model and a syntactic model is making the models “negotiate” an agreed upon structure that maximizes the score under both models, using optimization techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into natural language processing (Rush et al. 2010). 21 Note that finding the most probable segmentation requires summing over all the trees resulting in each segmentation—a much harder task, proven to be NP-complete in Sima’an (1996). 143 Computational Linguistics Volume 39, Number 1 Figure 4 Lattice initialization of the CKY chart. 8. Incorporating Morphological Agreement Inspecting the learned grammars reveal that they do not encode any knowledge of morphological agreement: The split categories for nouns, verbs, and adjectives do not group words according to any relevant morphological property such as gender or number, making it impossible"
J13-1007,C96-2215,0,0.191338,"Missing"
J13-1007,H05-1060,0,0.115559,"Missing"
J13-1007,P06-3009,0,0.657311,"e seen once. 7. Joint Segmentation and Parsing When applied to real text (for which the gold word-segmentation is not available), the baseline PCFG-LA parser is supplied with word segmentation produced by a separate tagging process.18 This seriously degrades parsing performance. A major reason for the performance drop is that the word-segmentation task and the syntactic-disambiguation task are highly related. Segmentation mistakes drive the parser toward wrong syntactic structures, and many segmentation decisions require long-distance information that is not available to a sequential process (Tsarfaty 2006a). For these reasons, we claim that parsing and segmentation should be performed jointly. 18 Although the tagger also produces POS tag assignments, we ignore them and use only the word segmentation. This is done for two reasons: first, the tag set of the tagger is the one used by the morphological analyzer, and is not compatible with the treebank. Second, we believe it is better for the parser to produce its own tag assignments. 141 Computational Linguistics Volume 39, Number 1 Figure 3 The lattice for the Hebrew sequence !( בצלם הנעיםsee footnote 19). Joint segmentation and parsing can b"
J13-1007,W07-2219,0,0.0409388,"Missing"
J13-1007,C08-1112,0,0.0305197,"Missing"
J13-1007,W10-1405,0,0.115984,"Missing"
J13-1007,D09-1088,0,0.0376607,"Missing"
J13-1007,N07-1051,0,\N,Missing
J14-2001,P99-1059,0,0.0228125,"and wj inclusive, (ii) G[i,j] is a directed tree, and (iii) it holds for every arc (i, l, j) ∈ G[i,j] that there is a directed path 1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira 2005) arc constraints can be enforced within the O(n3 ) Eisner algorithm (Eisner 1996) by pruning out inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points, which would necessitate the use of O(n4 ) algorithms (Eisner and Satta 1999). 250 Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing from i to every node k such that min(i, j) &lt; k &lt; max(i, j) (projectivity). We now define two constraints on a dependency graph G for a sentence x: r G is a projective dependency tree (PDT) if and only if it is a projective spanning tree over the interval [1, n] rooted at node n. r G is a projective dependency graph (PDG) if and only if it can be extended to a projective dependency tree simply by adding arcs. It is clear from the definitions that every PDT is also a PDG, but not the other way around. Every PDG can be c"
J14-2001,C96-1058,0,0.0699697,"ay that a subgraph G[i,j] = (V[i,j] , A[i,j] ) of G is a projective spanning tree over the interval [i, j] (1 ≤ i ≤ j ≤ n) iff (i) G[i,j] contains all nodes corresponding to words between wi and wj inclusive, (ii) G[i,j] is a directed tree, and (iii) it holds for every arc (i, l, j) ∈ G[i,j] that there is a directed path 1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira 2005) arc constraints can be enforced within the O(n3 ) Eisner algorithm (Eisner 1996) by pruning out inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points, which would necessitate the use of O(n4 ) algorithms (Eisner and Satta 1999). 250 Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing from i to every node k such that min(i, j) &lt; k &lt; max(i, j) (projectivity). We now define two constraints on a dependency graph G for a sentence x: r G is a projective dependency tree (PDT) if and only if it is a projective spanning tree over the interval [1, n] rooted at node n. r G is a projective dependency graph (PDG)"
J14-2001,C12-1059,1,0.847816,"configuration without having to scan the stack and buffer linearly. Because there are at most O(n) arcs in the arc constraint set, the preprocessing will not take more than O(n) time but guarantees that all permissibility checks can be performed in O(1) time. Finally, we note that the arc-constrained system is sound and complete in the sense that it derives all and only PDTs compatible with a given arc constraint set AC for a sentence x. Soundness follows from the fact that, for every arc (i, l, j) ∈ AC , the preconditions 3 For further discussion of reachability in the arc-eager system, see Goldberg and Nivre (2012, 2013). 253 Computational Linguistics Volume 40, Number 2 force the system to reach a configuration of the form (σ |min(i, j), max(i, j)|β, A) in which either L EFT-A RCl (i &gt; j) or R IGHT-A RC l (i &lt; j) will be the only permissible transition. Completeness follows from the observation that every PDT G compatible with AC is also a PDG and can therefore be viewed as a larger constraint set for which every transition sequence (given soundness) derives G exactly. Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this"
J14-2001,Q13-1033,1,0.843482,"Missing"
J14-2001,I11-1084,0,0.0193491,"transition. Completeness follows from the observation that every PDT G compatible with AC is also a PDG and can therefore be viewed as a larger constraint set for which every transition sequence (given soundness) derives G exactly. Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this setting, the distribution of utterances is highly skewed towards imperatives making them easy to identify. Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically do a poor job of parsing such utterances (Hara et al. 2011). However, we know that if the first word of a command is a verb, it is likely the root of the sentence. If we take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain the same parser so that the first word of the sentence must be the root, accuracy jumps dramatically to 85.56. This is independent of simply knowing that the first word of the sentence is a verb, as both parsers in this experiment had access to gold part-of-speech tags. 4. Parsing with Span Constraints Span Constraints."
J14-2001,P10-1001,0,0.162458,"Missing"
J14-2001,P05-1012,1,0.86983,"Missing"
J14-2001,P13-2017,1,0.886031,"Missing"
J14-2001,W03-3017,1,0.76581,"ce, Ramat-Gan, 5290002, Israel. E-mail: yoav.goldberg@gmail.com. † Google, 76 Buckingham Palace Road, London SW1W9TQ, United Kingdom. E-mail: ryanmcd@google.com. Submission received: 26 June 2013; accepted for publication: 10 October 2013. doi:10.1162/COLI a 00184 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Figure 1 Span constraint derived from a title assisting parsing. Left: unconstrained. Right: constrained. In this article, we examine the problem of constraining transition-based dependency parsers based on the arc-eager transition system (Nivre 2003, 2008), which perform a single left-to-right pass over the input, eagerly adding dependency arcs at the earliest possible opportunity, resulting in linear time parsing. We consider two types of constraints: span constraints, exemplified earlier, require the output graph to have a single subtree over one or more (non-overlapping) spans of the input; arc constraints instead require specific arcs to be present in the output dependency graph. The main contribution of the article is to show that both span and arc constraints can be implemented as efficiently computed preconditions on parser transi"
J14-2001,J08-4003,1,0.894607,"ad. S HIFT removes the first node in the buffer and pushes it onto the stack, with the precondition that j 6= n. A transition sequence for a sentence x is a sequence C0,m = (c0 , c1 , . . . , cm ) of configurations, such that c0 is the initial configuration cs (x), cm is a terminal configuration, and there is a legal transition t such that ci = t(ci−1 ) for every i, 1 ≤ i ≤ m. The dependency graph derived by C0,m is Gcm = (Vx , Acm ), where Acm is the set of arcs in cm . Complexity and Correctness. For a sentence of length n, the number of transitions in the arc-eager system is bounded by 2n (Nivre 2008). This means that a parser using greedy inference (or constant width beam search) will run in O(n) time provided that transitions plus required precondition checks can be performed in O(1) time. This holds for the arc-eager system and, as we will demonstrate, its constrained variants as well. The arc-eager transition system as presented here is sound and complete for the set of PDTs (Nivre 2008). For a specific sentence x = w1 , . . . , wn , this means that any transition sequence for x produces a PDT (soundness), and that any PDT for x is generated by 251 Computational Linguistics Transition"
J14-2001,W04-2407,1,0.901212,"Missing"
J14-2001,P80-1024,0,0.667437,"Missing"
J14-2001,W03-3023,0,0.309594,"strained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser. 1. Introduction Data-driven dependency parsers in general achieve high parsing accuracy without relying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations where additional information sources, not available at the time of training the parser, may be used to derive hard constraints at parsing time. For example, Figure 1 shows the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal section of the Penn Treebank before (left) and after (right) being constrained to build a single subtree over the span corresponding to the named entity “Cat on a Hot Tin Roof,” which does not"
J14-2001,D08-1059,0,0.0265037,"traints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser. 1. Introduction Data-driven dependency parsers in general achieve high parsing accuracy without relying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations where additional information sources, not available at the time of training the parser, may be used to derive hard constraints at parsing time. For example, Figure 1 shows the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal section of the Penn Treebank before (left) and after (right) being constrained to build a single subtree over the span corresponding to the named entity “Cat on a Hot Tin Roof,” which does not occur in the training set but can easily be found in on-line databases. In this case, addi"
J14-2001,P11-2033,1,0.937551,"ich every transition sequence (given soundness) derives G exactly. Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this setting, the distribution of utterances is highly skewed towards imperatives making them easy to identify. Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically do a poor job of parsing such utterances (Hara et al. 2011). However, we know that if the first word of a command is a verb, it is likely the root of the sentence. If we take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain the same parser so that the first word of the sentence must be the root, accuracy jumps dramatically to 85.56. This is independent of simply knowing that the first word of the sentence is a verb, as both parsers in this experiment had access to gold part-of-speech tags. 4. Parsing with Span Constraints Span Constraints. Given a sentence x = w1 , . . . , wn , we take a span constraint set to be a set SC of non-overlapping spans [i, j] (1 ≤ i &lt; j ≤ n). The task of span-constrained parsing"
J14-2001,J13-1002,1,\N,Missing
J17-2002,Q16-1031,1,0.843163,"information in a joint model. Similar improvements may be achieved in an outof-domain scenario. Even though the parser is greedy, it provides very consistent results comparable with the best parsers of the state-of-the-art. We even obtained further improvement as demonstrated with the experiments with the dynamic oracles, that provide a push over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their"
J17-2002,P16-1231,0,0.190862,"w view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a"
J17-2002,P81-1022,0,0.756303,"Missing"
J17-2002,D12-1133,0,0.163485,"Missing"
J17-2002,Q13-1034,0,0.205338,"Missing"
J17-2002,W06-2920,0,0.0983953,"ddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section 6.2.1. 6.2.3 Data for the Dynamic Oracle. Because the arc-hybrid transition-based parsing algorithm is limited to fully projective trees, we use the same data as in Section 6.2.1, which makes it comparable with the basic model that uses standard word representations and a static oracle arc-standard algorithm. 6.2.4 CoNLL-2009 Data. We also report results with all the CoNLL 2009 data sets (Hajiˇc et al. 2009) to make a complete c"
J17-2002,P05-1022,0,0.0472943,"al embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, finding that they are quite benefic"
J17-2002,D14-1082,0,0.112043,"ork in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack"
J17-2002,C14-1078,0,0.146438,"Missing"
J17-2002,P13-1104,0,0.246028,"Missing"
J17-2002,P14-2111,0,0.126115,"Missing"
J17-2002,D07-1022,1,0.889561,"Missing"
J17-2002,W15-3904,0,0.0920647,"Missing"
J17-2002,P15-1033,1,0.882509,"therefore also investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,W13-5709,1,0.94387,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The dynamic oracles framework for training with exploration, suggested by Goldberg and Nivre (2012, 2013), provides answers to these questions. Although the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). 5.2.1 Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take under a given parser state. In contrast to static oracles that derive a canonical sequence for each gold parse tree and say nothing about parsing states that do not stem from this canonical path, the dynamic oracle is well-defined for states that result from parsing mistakes, and may produce more than a single gold action for a given state. Under the dynamic oracle framework, a parsing action is said to be optimal in a given state if the best tree that"
J17-2002,P11-2124,1,0.874063,"nt neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio, and Satta 2014; Honnibal, Goldb"
J17-2002,C12-1059,1,0.936735,"Missing"
J17-2002,Q13-1033,1,0.949811,"eaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled with the stack LSTM. Figure 4 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Because a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. 3.3.3 Arc-Hybrid. For the dynamic oracle training scenario, described in Section 5.2, we switch to the arc-hybrid transition system, which is amenable to an efficient dynamic oracle (Goldberg and Nivre 2013). The arc-hybrid system is summarized in Figure 5. The SHIFT and REDUCE - RIGHT transitions are the same as in arc-standard. However, the REDUCE - LEFT transition pops the top of the stack and attaches it as a child of the first item in the buffer. Although it is possible to extend the arc-hybrid system to support nonprojectivity by adding a SWAP transition, this extension would invalidate an important guarantee enabling efficient dynamic oracles.10 We therefore restrict the dynamic-oracle experiments to the fully projective English and Chinese treebanks. In order to parse nonprojective trees,"
J17-2002,Q14-1010,1,0.935088,"Missing"
J17-2002,P08-1043,1,0.846251,"014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio"
J17-2002,P13-2111,1,0.891593,"work uses continuous-valued relaxations of these. 3. Dependency Parser We now turn to the problem of learning representations of dependency parser states. We preserve the standard data structures of a transition-based dependency parser, namely, a buffer of words to be processed (B) and a stack (S) of partially constructed syntactic elements. Each stack element is augmented with a continuous-space vector embedding representing a word and, in the case of S, any of its syntactic dependents. Additionally, we introduce a third stack (A) to represent the history of transition actions taken by the 5 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. 316 } ) T L( IF DSH RE od am … pt | {z TO P {z S Greedy Transition-Based Dependency Parsing with Stack LSTMs B P TO | Ballesteros et al. } amod ; was made TO |{z } an decision overhasty P root ; REDUCE-LEFT(amod) A SHIFT … Figure 2 Parser state computation encountered while parsing the sentence an overhasty decision was made. Here S designates the stack of partially constructed dependency subtre"
J17-2002,D14-1099,0,0.164827,"Missing"
J17-2002,W09-1201,0,0.137744,"Missing"
J17-2002,N03-1014,0,0.736917,"ounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LS"
J17-2002,P04-1013,0,0.567345,"Missing"
J17-2002,P13-1088,0,0.0439757,"and qz is a bias term for action z. The set A(S, B) represents the valid transition actions that may be taken given the current contents of the stack and buffer.9 Because pt = f (st , bt , at ) encodes information about all previous decisions made by the parser, the chain rule may be invoked to write the probability of any valid sequence of parse transitions z conditional on the input as: p(z |w) = |z| Y t=1 p(zt |pt ) (2) 3.2 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Hermann and Blunsom 2013; Socher et al. 2011, 2013a, 2013b). We follow previous work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed in Section 4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S,"
J17-2002,W13-3518,1,0.937487,"Missing"
J17-2002,Q14-1011,0,0.100404,"Missing"
J17-2002,P11-1068,0,0.272942,"Missing"
J17-2002,N16-1030,1,0.677399,"Missing"
J17-2002,D14-1081,0,0.0383284,"crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by the stack, buffer, an"
J17-2002,N15-1142,1,0.846911,"and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with probability 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained Word Embeddings. There are several options for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches that discard order perform less well (Bansal, Gimpel, and Livescu 2014); therefore, we used a variant of the skip n-gram model introduced by Ling et al. (2015a), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. 4.2 Modeling Characters Instead of Words Following Ling et al. (2015b), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber 2005). When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character se"
J17-2002,D15-1176,1,0.903245,"Missing"
J17-2002,de-marneffe-etal-2006-generating,0,0.292516,"Missing"
J17-2002,D13-1032,0,0.0288931,"Experiments with Static Oracle and Standard Word Representations We report results on five experimental configurations per language, as well as the Chen and Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (−POS), the stack LSTM parsing model without pretrained language model embeddings (−pretraining), the 13 Training: 02-21. Development: 22. Test: 23. 14 Training: 001–815, 1001–1136. Development: 886–931, 1148–1151. Test: 816–885, 1137–1147. ¨ 15 The POS tags were calculated with MarMot tagger (Mueller, Schmid, and Schutze 2013) by the best ¨ performing system of the SPMRL Shared Task (Bjorkelund et al. 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 16 Because the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4,996 sentences of the training set as a development set. 328 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 1 Unlabeled attachment scores and labeled attachment scores on the development sets (top) and the final test"
J17-2002,W03-3017,0,0.216793,"ril 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—which is unboun"
J17-2002,W04-0308,0,0.395191,"embeddings of the head, dependent, and relation and applying a linear operator and a component-wise nonlinearity as follows: c = tanh (U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. Th"
J17-2002,J08-4003,0,0.37078,"4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S, B) is the complete set of parser actions discussed in Section 3.3, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT transition is obligatory (Nivre 2008). 318 Ballesteros et al. det Greedy Transition-Based Dependency Parsing with Stack LSTMs amod an overhasty mod an decision cc2 rel det head mod overhasty c1 head rel amod decision Figure 3 The representation of a dependency subtree (top) is computed by recursively applying composition functions to hhead, modifier, relationi triples. In the case of multiple dependents of a single head, the recursive branching order is imposed by the order of the parser’s reduce transition (bottom). in the order they are “reduced” in the parser, as illustrated in Figure 3. Each node in this expanded syntactic tr"
J17-2002,P09-1040,0,0.801651,"(U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. The arc-standard transition inventory (Nivre 2004) is given in Figure 4. The SHIFT transition moves a word from the buffer to the stack,"
J17-2002,W06-2933,0,0.104845,"Missing"
J17-2002,P05-1013,0,0.662475,"B B B (v, v), B Dependency — r u→v r u←v — Figure 4 Parser transitions of the arc-standard system (with swap, Section 3.3.2) indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. (gr (x, y), x) refers to the composition function presented in 3.2. the stack. The arc-standard system allows building all and only projective trees. In order to parse nonprojective trees, this can be combined with the pseudo-projective approach (Nivre and Nilsson 2005) or follow what is presented in Section 3.3.2. 3.3.2 Arc-Standard with Swap. In order to deal with nonprojectivity, the arc-standard system can be augmented with a SWAP transition (Nivre 2009). The SWAP transition removes the second-to-top item from the stack and pushes it back to the buffer, allowing for the creation of nonprojective trees. We only use this transition when the training data set contains nonprojective trees. The inclusion of the SWAP transition requires breaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled"
J17-2002,nivre-etal-2006-talbanken05,0,0.111357,"82.15 79.04 92.57 90.21 Words + POS UAS LAS 86.85 85.36 93.04 90.87 Words + Chars UAS LAS 81.90 78.81 92.56 90.38 Words + Chars + POS UAS LAS 86.92 85.49 92.75 90.62 Test: Language Chinese English 6.4.3 Comparison with State-of-the-Art Parsers. Table 5 shows a comparison with state-ofthe-art parsers. We include greedy transition-based parsers that, like ours, do not apply beam search. For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006), who also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. For English and Chinese, we report our results of the best parser on the develpment set in Section 6.3—which is Words + POS but with pretrained word embeddings. We also show the best reported results on these data sets. For the SPMRL data ¨ sets, the best performing system of the shared task is either Bjoreklund et al. (2013) or ¨ Bjorkelund et al. (2014), which are better than our system. Note that the comparison is harsh to our system, which does not use unlabeled data nor any"
J17-2002,J17-2002,1,0.0512826,"Missing"
J17-2002,P00-1061,0,0.0437166,"that make use of external embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, findin"
J17-2002,W14-6111,0,0.0979038,"Missing"
J17-2002,W13-4917,1,0.910835,"Missing"
J17-2002,seeker-kuhn-2012-making,0,0.0639458,"uage model word embeddings were generated from the AFE portion of the English Gigaword corpus (version 5), and from the complete Chinese Gigaword corpus (version 2), as segmented by the Stanford Chinese Segmenter (Tseng et al. 2005). 6.2.2 Data to Test the Character-Based Representations and Static Oracle for Training. For the character-based representations we applied our model to the treebanks of the SPMRL ¨ Shared Task (Seddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section"
J17-2002,P13-1045,0,0.0704272,"3, Number 2 manually crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by"
J17-2002,D13-1170,0,0.00710017,"Missing"
J17-2002,K16-1019,1,0.850904,"sh over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their help with the parsing algorithms. This work was sponsored in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319. Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENS"
J17-2002,P15-1150,0,0.158426,"Missing"
J17-2002,P07-1080,0,0.609122,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,W07-2218,0,0.744034,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,P15-3004,0,0.0618368,"so investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,N03-1033,0,0.365748,"Missing"
J17-2002,P06-3009,0,0.158184,"xisting models. For instance, Chrupala (2014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different w"
J17-2002,I05-3027,0,0.0858346,"Missing"
J17-2002,Q16-1014,0,0.139104,"Missing"
J17-2002,P15-1113,0,0.105221,"Missing"
J17-2002,P15-1032,0,0.317311,"parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can"
J17-2002,W03-3023,0,0.341093,"ised version received: 6 April 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—whi"
J17-2002,K15-1015,0,0.465378,"finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack pointer that is manipulated by push and pop o"
J17-2002,D15-1251,1,0.901456,"Missing"
J17-2002,P16-1147,0,0.0322281,"rd embeddings are useful for other languages we also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training set-up as in Section 4; for English and Chinese we used the same pretrained word embeddings as in previous experiments, for German we pretrained embeddings using the monolingual training data from the WMT 2015 data set22 , and for Spanish we used the Spanish Gigaword version 3. The results for the parser with character-based representations on these data sets (last line of the table) were published by Andor et al. (2016). In Zhang and Weiss (2016), it is also possible to find results of the same version of the parser on the Universal Dependency treebanks (Nivre et al. 2015). 21 We report the performance of these parsers in the most comparable set-up, that is, with beam = 1 or greedy. 22 http://www.statmt.org/wmt15/translation-task.html. 336 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 7 Results of the parser in its different versions including comparison with other systems. St. refers to static oracle with the arc-standard parser and Dyn. refers to dynamic oracle with the arc-hybrid parser with α"
J17-2002,D08-1059,0,0.177172,"Missing"
J17-2002,P11-2033,0,0.0417267,"tions on this sequence, and r the complete contents of the stack of partially constructed syntactic structures. This global sensitivity of the state representation contrasts with most previous work in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which"
J17-2002,P15-1117,0,0.148064,"ders only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be unders"
J17-2002,N07-1050,0,\N,Missing
J17-2002,D15-1041,1,\N,Missing
J17-2002,W13-4916,0,\N,Missing
J17-2002,J13-1002,1,\N,Missing
J17-2002,P14-6005,0,\N,Missing
J17-2002,W14-6110,0,\N,Missing
J17-2002,Q14-1017,0,\N,Missing
J17-2002,W13-4907,1,\N,Missing
J17-2002,P14-2131,0,\N,Missing
J17-2002,C14-1076,1,\N,Missing
J17-2002,W15-2210,0,\N,Missing
J17-2002,P15-2042,0,\N,Missing
K17-1003,P16-2038,1,0.749856,"Missing"
K17-1003,J07-3004,0,0.0182219,"s difficult as finding the full CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999). RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016). In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages. 2.3 and Goldberg, 2016; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). 3 3.1 Datasets We used two training datasets. The first is the corpus of approximately 1.5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had at most 50 words and contained at least one third-person present-tense agreement dependency. Following Linzen et al. (2016), we replaced rare words by their part-ofspeech tags, using the Penn Treebank tag set (Marcus et"
K17-1003,E17-2020,1,0.888529,"Missing"
K17-1003,N16-1027,0,0.0447326,"mpionships. Determining that the subject of the verb in boldface is banners rather than the singular nouns 4 with an appropriate tag. In fact, supertagging is almost as difficult as finding the full CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999). RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016). In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages. 2.3 and Goldberg, 2016; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). 3 3.1 Datasets We used two training datasets. The first is the corpus of approximately 1.5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had at most 50 words and contained at least on"
K17-1003,Q16-1037,1,0.693621,"the RNN with an incentive to develop more sophisticated representations, we trained it to perform one of two tasks: the first is combinatory categorical grammar (CCG) supertagging (Bangalore and Joshi, 1999), a sequence labeling task likely to require robust syntactic representations; the second task is language modeling. We also investigate the inverse question: can Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multitask training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on"
K17-1003,P15-2041,0,0.0386091,"building are for national or NCAA Championships. Determining that the subject of the verb in boldface is banners rather than the singular nouns 4 with an appropriate tag. In fact, supertagging is almost as difficult as finding the full CCG parse of the sentence: once the supertags are determined, only a small number of parses are possible. At the same time, supertagging is simple to set up as a machine learning problem, since at each word it amounts to a straightforward classification problem (Bangalore and Joshi, 1999). RNNs have shown excellent performance on this task, at least in English (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016). In contrast with the agreement task, training data for supertagging needs to be obtained from parsed sentences which require expert annotation (Hockenmaier and Steedman, 2007); the amount of training data is therefore limited even in English, and much more sparse in other languages. 2.3 and Goldberg, 2016; Martínez Alonso and Plank, 2017; Bingel and Søgaard, 2017). 3 3.1 Datasets We used two training datasets. The first is the corpus of approximately 1.5 million sentences from the English Wikipedia compiled by Linzen et al. (2016). All sentences had"
K17-1003,J93-2004,0,0.0799242,"Missing"
K17-1003,J99-2004,0,\N,Missing
K17-1003,N16-1024,0,\N,Missing
K17-1003,E17-1005,0,\N,Missing
K17-3022,Q16-1031,0,0.032839,"ce of length n with words w1 , . . . , wn , we create a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding, a pretrained embedding, and a character vector. We construct a character vector che (wi ) for each wi by running a BiLSTM over the characters chj (1 ≤ j ≤ m) of wi : 100 50 12 100 100 2 200 / 200 0.25 0.33 0.1 Table 2: Hyper-parameter values for parsing. With the aim of training a multilingual parser, we additionally created a variant of the parser which adds a language embedding to input vectors in a spirit similar to what is done in Ammar et al. (2016). In this setting, the vector for each word xi is the concatenation of a word embedding, a pretrained word embedding, a character vector, and a language embedding li with the language corresponding to the word. As was mentioned in the introduction, our experiments with this model was limited to the languages with little data. Those experiments are described in Section 5. che (wi ) = B I L STM(ch1:m ) As in the original parser, we also concatenate these vectors with pretrained word embeddings pe(wi ). The input vectors xi are therefore: xi = e(wi ) ◦ pe(wi ) ◦ che (wi ) Our pretrained word embe"
K17-3022,D15-1041,0,0.0715977,"holz and Marsi, 2006; Nivre et al., 2007). Even models that perform joint inference, like those of Hatori et al. (2012) and Bohnet et al. (2013), depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al., 2015). The Uppsala system is a very simple pipeline consisting of two main components. The first is a model for joint sentence and word segmentation, which uses the BiRNN-CRF framework of Shao et al. (2017) to predict sentence and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. Th"
K17-3022,W17-0203,1,0.918606,"der-decoder (Bahdanau et al., 2014) equipped with shared long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) as the basic recurrent cell. At test time, multi-word tokens are first queried in the dictionary. If not found, the segmented words are generated via the encoder-decoder as a sequence-to-sequence transOur original plans included training a single universal model on data from all languages, with cross-lingual word embeddings, but in the limited time available we could only start exploring two simple enhancements. First, we constructed word embeddings based on the RSV model (Basirat and Nivre, 2017), using universal part-of-speech tags as contexts (Section 4). Secondly, we used multilingual training data for languages with little or no training data (Section 5). Our system was trained only on the training sets provided by the organizers (Nivre et al., 2017a). We did not make any use of large unlabeled data sets, parallel data sets, or word embeddings derived from such data. After evaluation on the official test sets (Nivre et al., 2017b), run on the TIRA server (Potthast et al., 2014), the Uppsala system ranked 23 of 33 systems with respect to the main evaluation metric, with a macro-ave"
K17-3022,Q13-1034,1,0.885612,"Missing"
K17-3022,W06-2920,0,0.223232,"ahu Kiperwasser† Sara Stymne∗ Yoav Goldberg† Joakim Nivre∗ ∗ Department of Linguistics and Philology Uppsala University Uppsala, Sweden Abstract Computer Science Department Bar-Ilan University Ramat-Gan, Israel lemmas, despite the fact that these annotations are available in the training and development data. In this way, we go against a strong tradition in dependency parsing, which has generally favored pipeline systems with part-of-speech tagging as a crucial component, a tendency that has probably been reinforced by the widespread use of data sets with gold tags from the early CoNLL tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). Even models that perform joint inference, like those of Hatori et al. (2012) and Bohnet et al. (2013), depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al.,"
K17-3022,W14-4012,0,0.0702859,"Missing"
K17-3022,L16-1262,1,0.853436,"Missing"
K17-3022,Q13-1033,1,0.85172,"ure over the words of each sentence. In particular, the system makes no use of part-of-speech tags, morphological features, or 207 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 207–217, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. 2 informative features of words in context and a feed-forward network for predicting the next parsing transition. The parser uses the arc-hybrid transition system (Kuhlmann et al., 2011) with greedy inference and a dynamic oracle for exploration during training (Goldberg and Nivre, 2013). For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies (Nivre and Nilsson, 2005). The parsing component is further described in Section 3. Sentence and Word Segmentation We model joint sentence and word segmentation as a character-level sequence labeling problem in a Bi-RNN-CRF model (Huang et al., 2015; Ma and Hovy, 2016). We simultaneously predict sentence boundaries and word boundaries and identify multi-word tokens that require further transduction."
K17-3022,P12-1110,0,0.0235023,"psala University Uppsala, Sweden Abstract Computer Science Department Bar-Ilan University Ramat-Gan, Israel lemmas, despite the fact that these annotations are available in the training and development data. In this way, we go against a strong tradition in dependency parsing, which has generally favored pipeline systems with part-of-speech tagging as a crucial component, a tendency that has probably been reinforced by the widespread use of data sets with gold tags from the early CoNLL tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). Even models that perform joint inference, like those of Hatori et al. (2012) and Bohnet et al. (2013), depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al., 2015). The Uppsala system is a very simple pipeline consisting of two main components. The first is"
K17-3022,P81-1022,0,0.685375,"Missing"
K17-3022,P05-1013,1,0.822588,"ependencies, pages 207–217, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. 2 informative features of words in context and a feed-forward network for predicting the next parsing transition. The parser uses the arc-hybrid transition system (Kuhlmann et al., 2011) with greedy inference and a dynamic oracle for exploration during training (Goldberg and Nivre, 2013). For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies (Nivre and Nilsson, 2005). The parsing component is further described in Section 3. Sentence and Word Segmentation We model joint sentence and word segmentation as a character-level sequence labeling problem in a Bi-RNN-CRF model (Huang et al., 2015; Ma and Hovy, 2016). We simultaneously predict sentence boundaries and word boundaries and identify multi-word tokens that require further transduction. In the BiRNN-CRF architecture, characters – regardless of writing system – are represented as dense vectors and fed into the bidirectional recurrent layers. We employ the gated recurrent unit (GRU) (Cho et al., 2014) as th"
K17-3022,Q16-1032,1,0.762352,"e and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. The starting point for this model is the transitionbased parser described in Kiperwasser and Goldberg (2016b), which relies on a BiLSTM to learn We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies. Our system is a simple pipeline consisting of two components. The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words. The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions. We achieved a macroaveraged LAS F1 of 65.11 in the official test run and obtained the 2nd best result for sentence segmentation with a score o"
K17-3022,Q16-1023,1,0.778627,"e and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. The starting point for this model is the transitionbased parser described in Kiperwasser and Goldberg (2016b), which relies on a BiLSTM to learn We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies. Our system is a simple pipeline consisting of two components. The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words. The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions. We achieved a macroaveraged LAS F1 of 65.11 in the official test run and obtained the 2nd best result for sentence segmentation with a score o"
K17-3022,P11-1068,0,0.532177,"Missing"
K17-3022,I17-1018,1,0.927202,"reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al., 2015). The Uppsala system is a very simple pipeline consisting of two main components. The first is a model for joint sentence and word segmentation, which uses the BiRNN-CRF framework of Shao et al. (2017) to predict sentence and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. The starting point for this model is the transitionbased p"
K17-3022,P16-1101,0,0.031215,"c-hybrid transition system (Kuhlmann et al., 2011) with greedy inference and a dynamic oracle for exploration during training (Goldberg and Nivre, 2013). For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies (Nivre and Nilsson, 2005). The parsing component is further described in Section 3. Sentence and Word Segmentation We model joint sentence and word segmentation as a character-level sequence labeling problem in a Bi-RNN-CRF model (Huang et al., 2015; Ma and Hovy, 2016). We simultaneously predict sentence boundaries and word boundaries and identify multi-word tokens that require further transduction. In the BiRNN-CRF architecture, characters – regardless of writing system – are represented as dense vectors and fed into the bidirectional recurrent layers. We employ the gated recurrent unit (GRU) (Cho et al., 2014) as the basic recurrent cell. Dropout (Srivastava et al., 2014) is applied to the output of the recurrent layers, which are concatenated and passed further to the first order chain CRF layer. The CRF layer models conditional scores over all possible"
K17-3022,L16-1680,0,0.0727605,"Missing"
K19-1043,J15-4004,0,0.282411,"d to have similar meanings (Harris, 1954). Indeed, they aim to create word representations that are derived from their shared contexts, where the context of a word is essentially the words in its proximity (be it according to linear order in the sentence or according to syntactic relations) (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). the anecdotal level. We also explore methods for removing such unwanted biases. We demonstrate that both in Italian and in German, the grammatical gender affects similarities between word representations (using words from SimLex-999 (Hill et al., 2015; Leviant and Reichart, 2015)): pairs of nouns with similar gender are closer to each other while pairs of nouns with different gender are farther apart. After quantifying the effect, we explore several methods of reducing it. A popular choice would be to simply lemmatize all the words prior to feeding them to the embedding learning algorithm. However, full lemmatization can be destructive, in the sense that it will also remove morphological distinction that we may want to keep. We thus seek more surgical approaches. Interestingly, recent embedding debiasing approaches (Bolukbasi et al., 2016)"
K19-1043,E17-2067,1,0.84804,"er while keeping different-gender words farther apart, we expect to see a significant difference between the average similarity of the set of same-gender nouns and the set of different-gender nouns. As mentioned above, we compute these averages for English as a reference, where we expect a low difference between the two sets. Table 1 shows the results for Italian and German, compared to English. Indeed, in both cases, the difference between the average of the two sets is much bigger. Word Embeddings and Morphology Word embeddings were shown to capture grammatical and morphological properties. Avraham and Goldberg (2017) show that standard training of word embeddings in Hebrew captures also morphological properties and that using the lemmas when composing the representations helps to better capture semantic similarities. Similarly, Basirat and Tang (2018) show that typical grammatical features are captured by Swedish word embeddings. Cotterell et al. (2016) treat the sparsity problem of morphologically rich languages in word embedding. They present a Gaussian graphical model to smooth representations of observed words and extrapolate representations for unseen words using morphological resources. With similar"
K19-1043,D14-1162,0,0.0853217,"onference on Computational Natural Language Learning, pages 463–471 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics stream tasks. These models are based on the distributional hypothesis according to which words that occur in the same contexts tend to have similar meanings (Harris, 1954). Indeed, they aim to create word representations that are derived from their shared contexts, where the context of a word is essentially the words in its proximity (be it according to linear order in the sentence or according to syntactic relations) (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). the anecdotal level. We also explore methods for removing such unwanted biases. We demonstrate that both in Italian and in German, the grammatical gender affects similarities between word representations (using words from SimLex-999 (Hill et al., 2015; Leviant and Reichart, 2015)): pairs of nouns with similar gender are closer to each other while pairs of nouns with different gender are farther apart. After quantifying the effect, we explore several methods of reducing it. A popular choice would be to simply lemmatize all the words prior to feeding them to the embed"
K19-1043,P18-1072,0,0.0658142,"Missing"
K19-1043,P17-1006,0,0.0499376,"Missing"
K19-1043,D17-1073,0,0.0185992,"terestingly, recent embedding debiasing approaches (Bolukbasi et al., 2016) do not work well. We instead look for methods that attempt to neutralize the gender signals from the training data. We find that such methods are effective in reducing the effect, but are also language specific and tricky to get right: we rely on language specific morphological analyzers while carefully accounting for their peculiarities and adjusting our use for each language. We take this work as a reminder that (a) linguistic resources such as lexicons and morphological analyzers are still relevant and useful (cf. (Zalmout and Habash, 2017)); (b) languages are diverse and different languages require different treatments; and (c) small details may matter a lot. In particular, existing tools and resources, either learned or human curated, should not be trusted blindly, but be carefuly adapted for the problem. Finally, we show that reducing the effect of grammatical agreement also has a positive effect on the quality of the resulting word representations, both in monolingual and cross-lingual settings. We conclude that grammatical gender indeed has its imprints on the representations of inanimate nouns, and that this should be take"
K19-1043,P16-1156,0,0.0469307,"Missing"
K19-1043,D18-1521,0,0.0240936,"mbeddings (Caliskan et al., 2017). Bolukbasi et al. (2016) show that using word embeddings for simple analogies surfaces many gender stereotypes. In addition, they define the gender bias of a word w by its projection on the “gender direction”: − → −→ → − w · (he − she), assuming all vectors are normalized. Positive bias stands for male-bias. For example, the bias of manager is 0.06, while the bias of nurse is −0.102 . Recently, some work has been done to reduce social gender bias in word embeddings, both as a post-processing step (Bolukbasi et al., 2016) and as part of the training procedure (Zhao et al., 2018). Bolukbasi et al. (2016) use a postprocessing debiasing method. Given a word embedding matrix, they make changes to the word vectors in order to reduce the gender bias for all words that are not inherently gendered. They do that by zeroing the gender projection of each word on a predefined gender direction.3 In Zmigrod et al. (2019), the authors mitigate social gender bias in gender marking languages using counterfactual data augmentation. Gendermarking languages add several interesting dimensions to the story: words relating to animate concepts such as “nurse” or “cat” may have both masculin"
K19-1043,P19-1161,0,\N,Missing
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
N10-1115,W06-2920,0,0.0732293,"m already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models. 1 Introduction Dependency parsing has been a topic of active research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved p"
N10-1115,D07-1101,0,0.484622,"e input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2 -154 jumped with"
N10-1115,W02-1001,0,0.244891,"r attachments. Unfortunately, this kind of ordering information is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w ~ · φ(x), where φ(x) is a feature representation and w ~ is a weight vector. We write φact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. ~ The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. ~ Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter v"
N10-1115,W06-2929,0,0.0294554,"Missing"
N10-1115,W05-1504,0,0.0155726,"t left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results. We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm. 749 Structure Restrictions Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce. Such constraints pose an explicit upper bound on parser accuracy.10 Our parsing model does not pose such restrictions. Shorter edges are arguably easier to predict, and our parses builds them early in time. However, it is also capable of producing long dependencies at later stages in the parsing process. Indeed, the distribution of arc lengths produced by our parser is similar to those produced by the M ALT and M ST parsers. 9 Discussion We presented a non-directi"
N10-1115,D09-1127,0,0.0334056,"in pi−2 , pi−1 , pi , pi+1 , pi+2 , pi+3 Bigram for p,q in (pi , pi+1 ),(pi , pi+2 ),(pi−1 , pi ),(pi−1 , pi+2 ),(pi+1 , pi+2 ) lenp , ncp ∆qp , ∆qp tp tq tp , wp , tp lcp , tp rcp , tp rcp lcp tp tq , wp wq , tp wq , wp tq tp tq lcp lcq , tp tq rcp lcq tp tq lcp rcq , tp tq rcp rcq PP-Attachment if pi is a preposition if pi+1 is a preposition wpi−1 wpi rcpi , tpi−1 wpi rcwpi wpi−1 wpi+1 rcpi+1 , tpi−1 wpi+1 rcwpi+1 wpi wpi+1 rcpi+1 , tpi wpi+1 rcwpi+1 wpi+1 wpi+2 rcpi+2 , tpi+1 wpi+2 rcwpi+2 wpi wpi+2 rcpi+2 , tpi wpi+2 rcwpi+2 if pi+2 is a preposition Figure 2: Feature Templates scribed in (Huang et al., 2009). We extended that feature set to include the structure on both sides of the proposed attachment point. In the case of unigram features, we added features that specify the POS of a word and its left-most and right-most children. These features provide the nondirectional model with means to prefer some attachment points over others based on the types of structures already built. In English, the left- and rightmost POS-tags are good indicators of constituency. The pp-attachment features are similar to the bigram features, but fire only when one of the structures is headed by a preposition (IN)."
N10-1115,W07-2416,0,0.0272115,"s 2-21 for training, Section 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5 http://www.cs.bgu.ac.il/∼yoavg/software/ http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html 7 While other and better conversions exist (see, e.g., (Johansson and Nugues, 2007; Sangati and Mazza, 2009)), this conversion heuristic is still the most widely used. Using the same conversion facilitates comparison with previous works. 6 747 representative of the tagging performance on nonWSJ corpus texts. Parsers We evaluate our parser against the transition-based M ALT parser and the graph-based M ST parser. We use version 1.2 of M ALT parser8 , with the settings used for parsing English in the CoNLL 2007 shared task. For the M ST parser9 , we use the default first-order, projective parser settings, which provide state-of-the-art results for English. All parsers are tra"
N10-1115,D07-1013,0,0.180011,"use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the next two or three input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The"
N10-1115,E06-1011,0,0.281678,"y, only the next two or three input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2"
N10-1115,P05-1012,0,0.316746,"search in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricte"
N10-1115,D07-1100,0,0.04797,"2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2 -154 jumped with -232 with fox fox joy fox a a brown joy 10 (6) 430 joy 246 with brown ATTACH L EFT (1) 186 with jumped fox 12 -146 -149 -133 246 brown (4) ATTACH R IGHT (1) a brown joy brown Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions. tionally intensive sampling-based methods (Nakagawa, 2007). As a result, these models, while accurate, are slow (O(n3 ) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models). We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing. This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms. By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take int"
N10-1115,P08-1108,0,0.0402283,"t part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the"
N10-1115,nivre-etal-2006-maltparser,0,0.0624865,"d parsing models. 1 Introduction Dependency parsing has been a topic of active research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural"
N10-1115,W04-0308,0,0.139318,"action) 2 if (∃c0 : (c, c0 ) ∈ Gold ∧ (c, c0 ) 6∈ Arcs) ∨ (p, c) 6∈ Gold then 3 return false 4 return true The function isV alid(act(i), gold, arcs) (line 4) is used to decide if the chosen action/location pair is valid. It returns True if two conditions apply: (a) (pi , pj ) is present in gold, (b) all edges (2, pj ) in gold are also in arcs. In words, the function verifies that the proposed edge is indeed present in the gold parse and that the suggested daughter already found all its own daughters.2 2 This is in line with the Arc-Standard parsing strategy of shift-reduce dependency parsers (Nivre, 2004). We are currently experimenting also with an Arc-Eager variant of the non745 5 Feature Representation The feature representation for an action can take into account the original sentence, as well as the entire parse history: φact(i) above is actually φ(act(i), sentence, Arcs, pending). We use binary valued features, and each feature is conjoined with the type of action. When designing the feature representation, we keep in mind that our features should not only direct the parser toward desired actions and away from undesired actions, but also provide the parser with means of choosing between"
N10-1115,W06-1616,0,0.0324742,"distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2 -154 jumped with -232 with fox fox joy fox a a brown joy 10 (6) 430 joy 246 with brown ATTACH L EFT (1) 186 with jumped fox 12 -146 -149 -133 246 brown (4) ATTACH R IG"
N10-1115,P06-2089,0,0.226166,"veral years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment poi"
N10-1115,N06-2033,0,0.378845,"veral years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment poi"
N10-1115,D08-1052,0,0.244314,"is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w ~ · φ(x), where φ(x) is a feature representation and w ~ is a weight vector. We write φact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. ~ The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. ~ Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter vector w ~ by decreasing the weights of the features associated with the"
N10-1115,P07-1096,0,0.760996,"with -232 with fox fox joy fox a a brown joy 10 (6) 430 joy 246 with brown ATTACH L EFT (1) 186 with jumped fox 12 -146 -149 -133 246 brown (4) ATTACH R IGHT (1) a brown joy brown Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions. tionally intensive sampling-based methods (Nakagawa, 2007). As a result, these models, while accurate, are slow (O(n3 ) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models). We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing. This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms. By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take into account the entire previously derived structure of the whole sentence. This extension allows the incorporation of much richer features than those available to transition- and especially to graph-based parsers, and greatly reduces the locality of transition-based algorithm d"
N10-1115,D07-1099,0,0.00911446,"cisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point. However, the model is not explicitly trained to optimize attachment ordering, has an O(n2 ) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers. Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). This approach works well and produces highly competitive results. Beam search can be incorporated into our parser as well. We leave this investigation to future work. Strict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG"
N10-1115,W03-3023,0,0.57272,"ecoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize. The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6. Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per second on an Intel based MacBook laptop. 7 Experiments and Results We evaluate the parser using the WSJ Treebank. The trees were converted to dependency structures with the Penn2Malt conversion program,6 using the headfinding rules from (Yamada and Matsumoto, 2003).7 We use Sections 2-21 for training, Section 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5 http://www.cs.bgu.ac.il/∼yoavg/software/ http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html 7 While other and better conversions ex"
N10-1115,D08-1059,0,0.487865,"in practice than the heap based one, as both are dominated by the O(n) feature extraction, while the cost of the O(n) max calculationis negligible compared to the constants involved in heap maintenance. parser to other dependency parsing frameworks. Parser M ALT M ST M ST 2 B EAM N ON D IR (This Work) Runtime O(n) O(n3 ) O(n3 ) O(n ∗ beam) O(nlogn) Features / Scoring O(n) O(n2 ) O(n3 ) O(n ∗ beam) O(n) Table 1: Complexity of different parsing frameworks. M ST: first order MST parser, M ST 2: second order MST parser, M ALT: shift-reduce left-to-right parsing. B EAM: beam search parser, as in (Zhang and Clark, 2008) In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce (M ALT) parsers, and is an order of magnitude more efficient than graph-based (M ST) parsers. Beam-search decoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize. The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6. Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per secon"
N10-1115,D07-1096,0,\N,Missing
N15-1163,D07-1101,0,0.0574759,"onstruct an effective representation, Φ (x, y) is typically decomposed into local representations 1 See https://bitbucket.org/hillel/templatekernels for implementation. Standard decompositions include different types of parts: arcs, sibling arcs, grandparent arcs, etc. Feature templates are then applied to the parts to construct the local representations. The templates determine how the linguistic properties of the words in each part should combine to create features (see Section 2). Substantial effort has been dedicated to the manual construction of feature templates (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). Still, for both computational and statistical reasons, existing templates consider only a small subset of the possible combinations of properties. From a computational perspective, solving Eq. 1 involves applying the templates to y and calculating a dot product in the effective dimension of Φ. The use of many templates thus quickly leads to computational infeasibility (the dimensionality of v, as well as the number of non-zero features in Φ, become very large). From a statistical perspective, the use of a large number of feature templates can lead to overfitting. Seve"
N15-1163,P05-1022,0,0.0633713,"p↔p0 :,j j That is, calculating k (p, p0 ) amounts to multiplying the sums of the columns in C.3 The runtime of k (p, p0 ) is then O (m × s) which means the overall runtime of K (y1 , y2 ) is O (|y1 |× |y2 |× |s |× |m|), where |y1 |, |y2 |are the number of parts in y1 and y2 . Finally, note that adding 1 to one of the column counts of C corresponds to a slot that can be skipped to produce a partial template (this simulates a wild card property that is always on). 3 Kernel Reranker We next show how to use the template kernels within a reranker. In the reranking approach (Collins and Koo, 2005; Charniak and Johnson, 2005), a base parser produces a list of k-best candidate parses for an input sentence and a separately trained reranking model is used to select the best one. 2 For brevity we’ll omit x from the kernel parameters and use K (y1 , y2 ) instead of K ((x1 , y1 ), (x2 , y2 )). 3 We omit the proof, but intuitively, the product of column sums is equal to the number of 1 valued paths between elements in the different columns of C. Each such path corresponds to a path in p and p0 where all the properties have identical values. i.e. it corresponds to a feature that is active in both φ(x1 , p) and φ(x2 , p0 )"
N15-1163,D14-1082,0,0.0997927,"imension of Φ. The use of many templates thus quickly leads to computational infeasibility (the dimensionality of v, as well as the number of non-zero features in Φ, become very large). From a statistical perspective, the use of a large number of feature templates can lead to overfitting. Several recent works have proposed solutions to the above problem. Lei et al., (2014) represented the space of all possible property combinations in an arc-factored model as a third order tensor and learned the parameter matrix for the tensor under a low rank assumption. In the context of transition parsers, Chen and Manning (2014) have implemented a neural network that uses dense representations of words and parts of speech as its input and implicitly considers combinations in its inner layers. Earlier work on transition-based dependency parsing used SVM classifiers with 2nd order polynomial kernels to achieve similar effects (Hall 1422 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1422–1427, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Template Kernels Figure 1: Feature template over the second order consecutive si"
N15-1163,P02-1034,0,0.414678,"ent a kernel based approach to automated feature generation in the context of graph-based parsing. Compared to tensors and neural networks, kernel methods have the attractive properties of a convex objective and well understood generalization bounds (Shawe-Taylor and Cristianini, 2004). We introduce a kernel that allows us to learn the parameters for a representation similar to the tensor representation in (Lei et al., 2014) but without the low rank assumption, and without explicitly instantiating the exponentially many possible features. In contrast to previous works on parsing with kernels (Collins and Duffy, 2002), in which the kernels are defined over trees and count the number of shared subtrees, our focus is on feature combinations. In that sense our work is more closely related to work on tree kernels for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Reichartz et al., 2010; Sun and Han, 2014), but the kernel we propose is designed to generate combinations of properties within selected part types and does not involve the all-subtrees representation. 1423 For simplicity, we begin with the case where parts p correspond to head modifier pairs (h, m) (i.e. all parts belong to th"
N15-1163,J05-1003,0,0.0601362,"Y   k p, p0 = ~1T · Cp↔p0 :,j j That is, calculating k (p, p0 ) amounts to multiplying the sums of the columns in C.3 The runtime of k (p, p0 ) is then O (m × s) which means the overall runtime of K (y1 , y2 ) is O (|y1 |× |y2 |× |s |× |m|), where |y1 |, |y2 |are the number of parts in y1 and y2 . Finally, note that adding 1 to one of the column counts of C corresponds to a slot that can be skipped to produce a partial template (this simulates a wild card property that is always on). 3 Kernel Reranker We next show how to use the template kernels within a reranker. In the reranking approach (Collins and Koo, 2005; Charniak and Johnson, 2005), a base parser produces a list of k-best candidate parses for an input sentence and a separately trained reranking model is used to select the best one. 2 For brevity we’ll omit x from the kernel parameters and use K (y1 , y2 ) instead of K ((x1 , y1 ), (x2 , y2 )). 3 We omit the proof, but intuitively, the product of column sums is equal to the number of 1 valued paths between elements in the different columns of C. Each such path corresponds to a path in p and p0 where all the properties have identical values. i.e. it corresponds to a feature that is active in b"
N15-1163,P04-1054,0,0.0989108,"Cristianini, 2004). We introduce a kernel that allows us to learn the parameters for a representation similar to the tensor representation in (Lei et al., 2014) but without the low rank assumption, and without explicitly instantiating the exponentially many possible features. In contrast to previous works on parsing with kernels (Collins and Duffy, 2002), in which the kernels are defined over trees and count the number of shared subtrees, our focus is on feature combinations. In that sense our work is more closely related to work on tree kernels for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Reichartz et al., 2010; Sun and Han, 2014), but the kernel we propose is designed to generate combinations of properties within selected part types and does not involve the all-subtrees representation. 1423 For simplicity, we begin with the case where parts p correspond to head modifier pairs (h, m) (i.e. all parts belong to the ”arc” part type). The features in φ(x, p) can then depend on any property of h, m and the sentence x. We denote properties related to h using the prefix h- (e.g., h-pos corresponds to the partof-speech of the head), and similarly for m-. We also use e- to denote prop"
N15-1163,P06-2041,0,0.027426,", c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics 2 Template Kernels Figure 1: Feature template over the second order consecutive siblings part type. The part type contains slots for the head (h), sibling (s) and modifier (m) words, as well as for the two edges (e1 and e2). Each slot is associated with a set of properties. The directed path skips over the edge properties and defines the partial template <h-cpos=?; s-cpos=?; m-gender=?>. et al., 2006). While training greedy transition-based parsers such as the ones used in (Chen and Manning, 2014) and (Hall et al., 2006) amounts to training a multiclass classifier, the graph-based parsing framework explored in (Lei et al., 2014) and in the present work is a more involved structured-learning task. In this paper we present a kernel based approach to automated feature generation in the context of graph-based parsing. Compared to tensors and neural networks, kernel methods have the attractive properties of a convex objective and well understood generalization bounds (Shawe-Taylor and Cristianini, 2004). We introduce a kernel that allows us to learn the parameters for a representation similar to the tensor represe"
N15-1163,N12-1088,0,0.038441,"Missing"
N15-1163,P10-1001,0,0.0304786,"ctive representation, Φ (x, y) is typically decomposed into local representations 1 See https://bitbucket.org/hillel/templatekernels for implementation. Standard decompositions include different types of parts: arcs, sibling arcs, grandparent arcs, etc. Feature templates are then applied to the parts to construct the local representations. The templates determine how the linguistic properties of the words in each part should combine to create features (see Section 2). Substantial effort has been dedicated to the manual construction of feature templates (McDonald et al., 2005b; Carreras, 2007; Koo and Collins, 2010). Still, for both computational and statistical reasons, existing templates consider only a small subset of the possible combinations of properties. From a computational perspective, solving Eq. 1 involves applying the templates to y and calculating a dot product in the effective dimension of Φ. The use of many templates thus quickly leads to computational infeasibility (the dimensionality of v, as well as the number of non-zero features in Φ, become very large). From a statistical perspective, the use of a large number of feature templates can lead to overfitting. Several recent works have pr"
N15-1163,P14-1130,0,0.557449,"tistical reasons, existing templates consider only a small subset of the possible combinations of properties. From a computational perspective, solving Eq. 1 involves applying the templates to y and calculating a dot product in the effective dimension of Φ. The use of many templates thus quickly leads to computational infeasibility (the dimensionality of v, as well as the number of non-zero features in Φ, become very large). From a statistical perspective, the use of a large number of feature templates can lead to overfitting. Several recent works have proposed solutions to the above problem. Lei et al., (2014) represented the space of all possible property combinations in an arc-factored model as a third order tensor and learned the parameter matrix for the tensor under a low rank assumption. In the context of transition parsers, Chen and Manning (2014) have implemented a neural network that uses dense representations of words and parts of speech as its input and implicitly considers combinations in its inner layers. Earlier work on transition-based dependency parsing used SVM classifiers with 2nd order polynomial kernels to achieve similar effects (Hall 1422 Human Language Technologies: The 2015 A"
N15-1163,P13-2109,0,0.0743082,"he kernel-reranker significantly improves over the base-reranker. In Bulgarian and Danish, the kernel-reranker outperforms the baseparser. This is not the case for Slovene and Arabic, which we attribute to the low oracle accuracy of the k-best lists in these languages. As is common in reranking (Jagarlamudi and Daum´e III, 2012), our final system incorporates the scores assigned to sentences by the base parser: i.e. scoref inal (x, y) = βscorebase (x, y) + scorereranker (x, y). β is tuned per language on a development set.8 Our final system outperforms the base parser, as well as TurboParser (Martins et al., 2013), a parser based on manually constructed feature templates over up to third order parts. The system lags slightly behind the sampling parser of Zhang et al. (2014) which additionally uses global features (not used by our system) and a tensor component for property combinations. Another important difference between the systems is that our search is severely restricted by the use of a reranker. It is likely that using our kernel in a graph-based parser will further improve its reason we did not select the English treebank. 8 To obtain a development set we further split the reranker training sets"
N15-1163,P05-1012,0,0.780047,"ork we present a novel kernel which facilitates efficient parsing with feature representations corresponding to a much larger set of combinations. We integrate the kernel into a parse reranking system and demonstrate its effectiveness on four languages from the CoNLL-X shared task.1 1 Dependency parsing is the task of labeling a sentence x with a syntactic dependency tree y ∈ Y (x), where Y (x) denotes the space of valid trees over x. Each word in x is represented as a list of linguistic properties (e.g. word form, part of speech, base form, gender, number, etc.). In the graph based approach (McDonald et al., 2005b) parsing is cast as a structured linear prediction problem: y∈Y(x) X φ (x, p) p∈y Introduction hv (x) = argmax vT · Φ (x, y) Amir Globerson Hebrew University Jerusalem, Israel gamir@cs.huji.ac.il (1) where Φ (x, y) ∈ Rd is a feature representation defined over a sentence and its parse tree, and v ∈ Rd is a vector of parameters. To construct an effective representation, Φ (x, y) is typically decomposed into local representations 1 See https://bitbucket.org/hillel/templatekernels for implementation. Standard decompositions include different types of parts: arcs, sibling arcs, grandparent arcs,"
N15-1163,H05-1066,0,0.24007,"Missing"
N15-1163,P14-2011,0,0.0570593,"Missing"
N15-1163,D14-1109,0,0.0654655,"lovene and Arabic, which we attribute to the low oracle accuracy of the k-best lists in these languages. As is common in reranking (Jagarlamudi and Daum´e III, 2012), our final system incorporates the scores assigned to sentences by the base parser: i.e. scoref inal (x, y) = βscorebase (x, y) + scorereranker (x, y). β is tuned per language on a development set.8 Our final system outperforms the base parser, as well as TurboParser (Martins et al., 2013), a parser based on manually constructed feature templates over up to third order parts. The system lags slightly behind the sampling parser of Zhang et al. (2014) which additionally uses global features (not used by our system) and a tensor component for property combinations. Another important difference between the systems is that our search is severely restricted by the use of a reranker. It is likely that using our kernel in a graph-based parser will further improve its reason we did not select the English treebank. 8 To obtain a development set we further split the reranker training sets into tuning training and a development sets (90/10). We then tune β per language on the respective development sets by selecting the best value from a list of {0,"
N16-1179,P11-1049,0,0.0609696,"Improving sentence compression by learning to predict gaze Sigrid Klerke University of Copenhagen skl@hum.ku.dk Yoav Goldberg Bar-Ilan University yoav.goldberg@gmail.com Abstract We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches. 1 Introduction Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al., 2000). This work suggests using eye-tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatically making sentences easier to process by shortening them. (ii) Eye-tracking measures such as first-pass reading time and time spent on regressions, i.e., during second and later passes over the text, are known to correlate with perceived text difficulty (Rayner et al., 2012). Thes"
N16-1179,P06-2019,0,0.0709165,"mpany said in a statement. Intel would be building car batteries Table 1: Example compressions from the G OOGLE dataset. S is the source sentence, and T is the target compression. Sents Sent.len Type/token Del.rate 0.22 0.21 0.17 0.59 0.27 0.87 0.55 0.27 0.42 0.47 0.29 0.87 T RAINING Z IFF -DAVIS B ROADCAST G OOGLE 1000 880 8000 20 20 24 T EST Z IFF -DAVIS B ROADCAST G OOGLE 32 412 1000 21 19 25 Table 2: Dataset characteristics. Sentence length is for source sentences. 4.2 Compression data We use three different sentence compression datasets, Z IFF -DAVIS (Knight and Marcu, 2002), B ROADCAST (Clarke and Lapata, 2006), and the publically available subset of G OOGLE (Filippova et al., 2015). The first two consist of manually compressed newswire text in English, while the third is built heuristically from pairs of headlines and first sentences from newswire, resulting in the most aggressive compressions, as exemplified in Table 1. We present the dataset characteristics in Table 2. We use the datasets as released by the authors and do not apply any additional pre-processing. The CCG supertagging data comes from CCGbank,1 and we use sections 0-18 for training and section 19 for development. 4.3 Baselines and s"
N16-1179,C08-1018,0,0.0355492,"this reader. See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular and reversed order, enabling conditioning predictions on both left"
N16-1179,N13-1070,1,0.786538,"openhagen soegaard@hum.ku.dk Our proposed model does not require that the gaze data and the compression data come from the same source. Indeed, in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets. While not explored here, an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users, based on their reading behavior. Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to Filippova et al. (2015). Our contributions • We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye-tracking corpus. • Our method is fully competitive with state-ofthe-art across three corpora. • Our code is made publicly available at https://bitbucket.org/soegaard"
N16-1179,W08-1105,0,0.0444923,"ord was read at most once by this reader. See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular and reversed order, enabling conditioning p"
N16-1179,W15-2402,1,0.738867,"by learning to predict gaze Sigrid Klerke University of Copenhagen skl@hum.ku.dk Yoav Goldberg Bar-Ilan University yoav.goldberg@gmail.com Abstract We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches. 1 Introduction Sentence compression is a basic operation in text simplification which has the potential to improve statistical machine translation and automatic summarization (Berg-Kirkpatrick et al., 2011; Klerke et al., 2015), as well as helping poor readers in need of assistive technologies (Canning et al., 2000). This work suggests using eye-tracking recordings for improving sentence compression for text simplification systems and is motivated by two observations: (i) Sentence compression is the task of automatically making sentences easier to process by shortening them. (ii) Eye-tracking measures such as first-pass reading time and time spent on regressions, i.e., during second and later passes over the text, are known to correlate with perceived text difficulty (Rayner et al., 2012). These two observations rec"
N16-1179,D15-1168,0,0.0381961,"uts for location i. We then calculate the objective function derivative for the sequence using cross-entropy (logistic loss) and use backpropagation to calculate gradients and update the weights accordingly. A deep bi-RNN or klayered bi-RNN is composed of k bi-RNNs that feed into each other such that the output of the ith RNN is the input of the i + 1th RNN. LSTMs (Hochreiter and Schmidhuber, 1997) replace the cells of RNNs with LSTM cells, in which multiplicative gate units learn to open and close access to the error signal. Bi-LSTMs have already been used for finegrained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). These and other recent applications of bi-LSTMs were constructed for solving a single task in isolation, however. We instead train deep bi-LSTMs to solve additional tasks to sentence compression, namely CCG-tagging and gaze prediction, using the additional tasks to regularize our sentence compression model. Specifically, we use bi-LSTMs with three layers. Our baseline model is simply this three-layered 1530 model trained to predict compressions (encoded as label sequences), and we consider two extensions"
N16-1179,E06-1038,0,0.0421228,"one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular and reversed order, enabling conditioning predictions on both left and right context. In the forward pass, we run the input data through an embedding layer and com"
N16-1179,N03-1026,0,0.0415312,"ure the value 0 indicates that the word was read at most once by this reader. See Table 1 for an example of first pass duration and regression duration annotations for one reader and sentence. Figure 2: Multitask and cascaded bi-LSTMs for sentence compression. Layer L−1 contain pre-trained embeddings. Gaze prediction and CCG-tag prediction are auxiliary training tasks, and loss on all tasks are propagated back to layer L0 . 3 Sentence compression using multi-task deep bi-LSTMs Most recent approaches to sentence compression make use of syntactic analysis, either by operating directly on trees (Riezler et al., 2003; Nomoto, 2007; Filippova and Strube, 2008; Cohn and Lapata, 2008; Cohn and Lapata, 2009) or by incorporating syntactic information in their model (McDonald, 2006; Clarke and Lapata, 2008). Recently, however, Filippova et al. (2015) presented an approach to sentence compression using LSTMs with word embeddings, but without syntactic features. We introduce a third way of using syntactic annotation by jointly learning a sequence model for predicting CCG supertags, in addition to our gaze and compression models. Bi-directional recurrent neural networks (biRNNs) read in sequences in both regular a"
N16-1179,D11-1038,0,0.021181,"that the gaze data and the compression data come from the same source. Indeed, in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets. While not explored here, an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users, based on their reading behavior. Several approaches to sentence compression have been proposed, from noisy channel models (Knight and Marcu, 2002) over conditional random fields (Elming et al., 2013) to tree-to-tree machine translation models (Woodsend and Lapata, 2011). More recently, Filippova et al. (2015) successfully used LSTMs for sentence compression on a large scale parallel dataset. We do not review the literature here, and only compare to Filippova et al. (2015). Our contributions • We present a novel multi-task learning approach to sentence compression using labelled data for sentence compression and a disjoint eye-tracking corpus. • Our method is fully competitive with state-ofthe-art across three corpora. • Our code is made publicly available at https://bitbucket.org/soegaard/ gaze-mtl16. 2 Gaze during reading Readers fixate longer at rare words"
N16-1179,P15-1109,0,0.0332807,"using cross-entropy (logistic loss) and use backpropagation to calculate gradients and update the weights accordingly. A deep bi-RNN or klayered bi-RNN is composed of k bi-RNNs that feed into each other such that the output of the ith RNN is the input of the i + 1th RNN. LSTMs (Hochreiter and Schmidhuber, 1997) replace the cells of RNNs with LSTM cells, in which multiplicative gate units learn to open and close access to the error signal. Bi-LSTMs have already been used for finegrained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). These and other recent applications of bi-LSTMs were constructed for solving a single task in isolation, however. We instead train deep bi-LSTMs to solve additional tasks to sentence compression, namely CCG-tagging and gaze prediction, using the additional tasks to regularize our sentence compression model. Specifically, we use bi-LSTMs with three layers. Our baseline model is simply this three-layered 1530 model trained to predict compressions (encoded as label sequences), and we consider two extensions thereof as illustrated in Figure 2. Our first extension, M ULTI - TASK -LSTM, includes t"
N16-1179,D15-1042,0,\N,Missing
N19-1045,P17-1042,0,0.0354386,"ustes Problem. It can be solved algebraically by using a singular value decomposition (SVD). Schnemann (1966) proved that the solution to 2 ˆ = U V T s.t. U ΣV T is the SVD of Y X T . is: Q The OP method is used in Xing et al. (2015); Artetxe et al. (2016, 2017a,b, 2018a,b); Hamilton et al. (2016); Conneau et al. (2017); Ruder et al. (2018). 2.3 Figure 1: Noise influence. (A): the effect of a noisy pair on 2D alignment. (B) mean error over non-noisy pairs as a function of noise level. The Unsupervised Translation Problem The supervised alignment problem can be expended to the semi-supervised (Artetxe et al., 2017b; Lample et al., 2017; Ruder et al., 2018) or unsupervised (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Alvarez-Melis and Jaakkola, 2018) case, where a very small lexicon or none at all is given. In iterative methods, the lexicon is expended and used to learn the alignment, later the alignment is used to predict the lexicon for the next iteration and so on. In adversarial methods, a final iterative step is used after the lexicon is built to refine the result. We will focus on the supervised stage in the unsupervised setting, meaning estimating the alignme"
N19-1045,P18-1073,0,0.0416344,"Missing"
N19-1045,J82-2005,0,0.611219,"Missing"
N19-1045,C18-1070,0,0.0703155,"ssume that ∀i f (xi ) ≈ yi . We refer to the sets X and Y as the supervision. The goal is to learn a ˆ such the Frobenius norm is minimized: matrix Q Introduction We consider the problem of mapping between points in different vector spaces. This problem has prominent applications in natural language processing (NLP). Some examples are creating bilingual word lexicons (Mikolov et al., 2013), machine translation (Artetxe et al., 2016, 2017a,b, 2018a,b; Conneau et al., 2017), hypernym generation (Yamane et al., 2016), diachronic embeddings alignment (Hamilton et al., 2016) and domain adaptation (Barnes et al., 2018). In all these examples one is given word embeddings in two different vector spaces, and needs to learn a mapping from one to the other. ˆ = arg min kQX − Y k2F . Q (1) Q 2.2 Existing Solution Methods Gradient-based The objective in (1) is convex, and can be solved via least-squares method or via stochastic gradient optimization iterating over the 460 Proceedings of NAACL-HLT 2019, pages 460–465 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics pairs (xi , yi ), as done by Mikolov et al. (2013) and Dinu and Baroni (2014). Orthogonal Procrustes (OP)"
N19-1045,P16-1141,0,0.0410426,"x Q. In the supervised setting, m = n and we assume that ∀i f (xi ) ≈ yi . We refer to the sets X and Y as the supervision. The goal is to learn a ˆ such the Frobenius norm is minimized: matrix Q Introduction We consider the problem of mapping between points in different vector spaces. This problem has prominent applications in natural language processing (NLP). Some examples are creating bilingual word lexicons (Mikolov et al., 2013), machine translation (Artetxe et al., 2016, 2017a,b, 2018a,b; Conneau et al., 2017), hypernym generation (Yamane et al., 2016), diachronic embeddings alignment (Hamilton et al., 2016) and domain adaptation (Barnes et al., 2018). In all these examples one is given word embeddings in two different vector spaces, and needs to learn a mapping from one to the other. ˆ = arg min kQX − Y k2F . Q (1) Q 2.2 Existing Solution Methods Gradient-based The objective in (1) is convex, and can be solved via least-squares method or via stochastic gradient optimization iterating over the 460 Proceedings of NAACL-HLT 2019, pages 460–465 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics pairs (xi , yi ), as done by Mikolov et al. (2013) and Dinu a"
N19-1045,D18-1214,0,0.0405756,"Missing"
N19-1045,D14-1162,0,0.0828249,"We initialize, µy , σ y by taking the mean and variance of the entire dataset. Finally, we initialize α to 0.5. The (hard version) EM algorithm is shown in Algorithm box 1. The runtime of each iteration is dominated by the OP algorithm (matrix multiplication and SVD on a d × d matrix). Each iteration contains an additional matrix multiplication and few simple vector operations. Figure 1(B) shows it obtains perfect results on the simulated noisy data. High Dimensional Embeddings The experiment setup is as before, but instead of a normal distribution we use (6B, 300d) English Glove Embeddings (Pennington et al., 2014) with lexicon of size n = 5000. We report the mean error for various noise levels on an unseen aligned test set of size 1500. In Figure 1(B) we can see that both methods are effected by noise. As expected, as the amount of noise increases the error on the test set increases. We can again see that the effect is worse with gradient-based methods. 4 Algorithm 1 Noise-aware Alignment Data: List of paired vectors: (x1 , y1 ), ..., (xn , yn ) Result: Q, σ, µy , σ y while |αcurr − αprev |>  do E step: 2 t ,σ I) wt = p(zt = 1|xt , yt ) = αNf(Qx (yt |xt ) ht = 1(w P t > 0.5) n1 = t ht M step: Apply OP"
N19-1045,P16-1024,0,0.190804,"Missing"
N19-1045,N15-1104,0,0.0577373,"Missing"
N19-1045,D18-1268,0,0.0259504,"Missing"
N19-1045,C16-1176,0,0.0207452,"e goal of the learning is to find the translation matrix Q. In the supervised setting, m = n and we assume that ∀i f (xi ) ≈ yi . We refer to the sets X and Y as the supervision. The goal is to learn a ˆ such the Frobenius norm is minimized: matrix Q Introduction We consider the problem of mapping between points in different vector spaces. This problem has prominent applications in natural language processing (NLP). Some examples are creating bilingual word lexicons (Mikolov et al., 2013), machine translation (Artetxe et al., 2016, 2017a,b, 2018a,b; Conneau et al., 2017), hypernym generation (Yamane et al., 2016), diachronic embeddings alignment (Hamilton et al., 2016) and domain adaptation (Barnes et al., 2018). In all these examples one is given word embeddings in two different vector spaces, and needs to learn a mapping from one to the other. ˆ = arg min kQX − Y k2F . Q (1) Q 2.2 Existing Solution Methods Gradient-based The objective in (1) is convex, and can be solved via least-squares method or via stochastic gradient optimization iterating over the 460 Proceedings of NAACL-HLT 2019, pages 460–465 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics pair"
N19-1045,P17-1179,0,0.0249085,"tion (SVD). Schnemann (1966) proved that the solution to 2 ˆ = U V T s.t. U ΣV T is the SVD of Y X T . is: Q The OP method is used in Xing et al. (2015); Artetxe et al. (2016, 2017a,b, 2018a,b); Hamilton et al. (2016); Conneau et al. (2017); Ruder et al. (2018). 2.3 Figure 1: Noise influence. (A): the effect of a noisy pair on 2D alignment. (B) mean error over non-noisy pairs as a function of noise level. The Unsupervised Translation Problem The supervised alignment problem can be expended to the semi-supervised (Artetxe et al., 2017b; Lample et al., 2017; Ruder et al., 2018) or unsupervised (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018b; Xu et al., 2018; Alvarez-Melis and Jaakkola, 2018) case, where a very small lexicon or none at all is given. In iterative methods, the lexicon is expended and used to learn the alignment, later the alignment is used to predict the lexicon for the next iteration and so on. In adversarial methods, a final iterative step is used after the lexicon is built to refine the result. We will focus on the supervised stage in the unsupervised setting, meaning estimating the alignment once a lexicon is induced. 3 the prediction according to the learned transfo"
N19-1061,D14-1162,0,0.0989255,"ties, defined the concept of debiasing word embeddings, and established the defacto metric of measuring this bias (the gender direction). It also provides a perfect solution to the problem of removing the gender direction from non-gendered words. However, as we show in this work, while the gender-direction is a great indicator of bias, it is only an indicator and not the complete manifestation of this bias. Zhao et al. (2018) take a different approach and suggest to train debiased word embeddings from scratch. Instead of debiasing existing word vectors, they alter the loss of the GloVe model (Pennington et al., 2014), aiming to concentrate most of the gender information in the last coordinate of each vector. This way, one can later use the word representations excluding the gender coordinate. They do that by using two groups of male/female seed words, and encouraging words that belong to different groups to differ in their last coordinate. In addition, they encourage the representation of neutral-gender words (excluding the last coordinate) to be orthogonal to the gender direction.5 This work did a step forward by trying to remove the bias during training rather than in postprocessing, which we believe to"
N19-1061,D18-1521,0,0.235954,"r programmer as woman is to x” with “x = homemaker”. Caliskan et al. (2017) further demonstrate association between female/male names and groups of words stereotypically assigned to females/males (e.g. arts vs. science). In addition, they demonstrate that word embeddings reflect actual gender gaps in reality by showing the correlation between the gender association of occupation words and labor-force participation data. Recently, some work has been done to reduce the gender bias in word embeddings, both as a post-processing step (Bolukbasi et al., 2016b) and as part of the training procedure (Zhao et al., 2018). Both works substantially reduce the bias with respect to the same definition: the projection − → −→ on the gender direction (i.e. he − she), introduced in the former. They also show that performance on word similarity tasks is not hurt. We argue that current debiasing methods, which lean on the above definition for gender bias and directly target it, are mostly hiding the bias rather than removing it. We show that even when drastically reducing the gender bias according to this definition, it is still reflected in the geometry of the representation of “gender-neutral” words, and a lot of the"
N19-1236,W05-0909,0,0.0225446,"l systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen). In the original challenge, the best performing system in automatic metric was based on end-toend NMT (Melbourne). Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics (Table 1). BestPlan is competitive with StrongNeural in all metrics, with small differ14 ences either way per metric. 12 Note that this only affects the training stage. At test time, we do not re"
N19-1236,N06-1046,0,0.0515384,"nning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our 17 While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process. Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring covera"
N19-1236,P04-3031,0,0.268173,"Missing"
N19-1236,D16-1032,0,0.0750465,"ed Systems We compare to the best submissions in the WebNLG challenge (Gardent et al., 2017): Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe (Mille et al., 2017), a classic grammar-based NLG system that scored best in the human evaluation. Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen). In the original challenge,"
N19-1236,P17-4012,0,0.0541432,"them. Figure 1d is an example of possible text resulting from such linearization. Training details We use a standard NMT setup with a copy-attention mechanism (Gulcehre et al., 10 2016) and the pre-trained GloVe.6B word em8 Minimally, each entity occurrence can keep track of the number of times it was already mentioned in the plan. Other alternatives include using a full-fledged referring expression generation system such as NeuralREG (Ferreira et al., 2018) 9 We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase. 10 Concretely, we use the OpenNMT toolkit (Klein et al., 2017) with the copy attn flag. Exact parameter values are beddings (Pennington et al., 2014). The pretrained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts. Generation details We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as Month DAY+ordinal, YEAR (i.e., July 4th, 1776) and for numbers with units (i.e., “5”(minutes)) we remove the parenthesis and quotation marks (5 minutes). 5 Experimental S"
N19-1236,N12-1093,0,0.0426172,"ation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our 17 While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process. Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring coverage and reducing the number"
N19-1236,W16-6626,0,0.310317,"ohn was born in London. Overall, the choice of fact ordering, entity ordering, and sentence splits for these facts give rise to 12 different structures, each of them putting the focus on somewhat different aspect of the information. Realistic inputs include more than two facts, greatly increasing the number of possibilities. Another axis of variation is in how to verbalize the information for a given structure. For example, (2) can also be verbalized as 2a. John works for IBM and was born in London. and (5) as: Consider the task of data-to-text generation, as exemplified in the WebNLG corpus (Colin et al., 2016). The system is given a set of RDF triplets describing facts (entities and relations between them) and has to produce a fluent text that is faithful to the facts. An example of such triplets is: John, birthPlace, London John, employer, IBM 1 With a possible output: ∗ We refer to the first set of choices (how to structure the information) as text planning and to the second 1 (how to verbalize a plan) as plan realization. The distinction between planning and realization is at the core of classic natural language generation (NLG) works (Reiter and Dale, 2000; Gatt and Krahmer, 2017). However, a r"
N19-1236,D13-1157,0,0.40401,"Missing"
N19-1236,W18-6539,0,0.0838331,"Missing"
N19-1236,P03-1069,0,0.126713,"e, Stent et al. (2004) shows a method of producing coherent sentence plans by exhaustively generating as many as 20 sentence plan trees for each document plan, manually tagging them, and learning to rank them using the RankBoost algorithm (Schapire, 1999). Our planning approach is similar, but we only have a set of “good” reference plans without internal ranks. While the sentence planning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our 17 While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (s"
N19-1236,P18-1182,0,0.0989358,"Missing"
N19-1236,W17-4912,1,0.789975,"cts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism. The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation. 8 Conclusion We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic e"
N19-1236,W17-3518,0,0.489599,"from Theo Hoffenberg and Reverso. Note that the variation from 5 to 5a includes the introduction of a pronoun. This is traditionally referred to as referring expression generation (REG), and falls between the planning and realization stages. We do not treat REG in this work, but our approach allows natural integration REG systems’ outputs. 2267 Proceedings of NAACL-HLT 2019, pages 2267–2277 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics and treat the problem as a single end-to-end task of learning to map facts from the input to the output text (Gardent et al., 2017; Duˇsek et al., 2018). These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of Puduppully et al. (2018), who introduce a neural content-planning module in the end-to-end architecture. While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017),"
N19-1236,P16-1014,0,0.231932,"ced corpus contains 13, 828 plan12 text pairs. Compared Systems We compare to the best submissions in the WebNLG challenge (Gardent et al., 2017): Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe (Mille et al., 2017), a classic grammar-based NLG system that scored best in the human evaluation. Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separ"
N19-1236,N18-1169,0,0.0295492,"stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism. The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation. 8 Conclusion We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outper"
N19-1236,S17-2158,0,0.0531673,"Missing"
N19-1236,P02-1040,0,0.104794,"ich outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen). In the original challenge, the best performing system in automatic metric was based on end-toend NMT (Melbourne). Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics (Table 1). BestPlan is competitive with StrongNeural in all metrics, with small differ14 ences either way per metric. 12 Note that this only affects the trainin"
N19-1236,D14-1162,0,0.083775,"Training details We use a standard NMT setup with a copy-attention mechanism (Gulcehre et al., 10 2016) and the pre-trained GloVe.6B word em8 Minimally, each entity occurrence can keep track of the number of times it was already mentioned in the plan. Other alternatives include using a full-fledged referring expression generation system such as NeuralREG (Ferreira et al., 2018) 9 We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase. 10 Concretely, we use the OpenNMT toolkit (Klein et al., 2017) with the copy attn flag. Exact parameter values are beddings (Pennington et al., 2014). The pretrained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts. Generation details We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as Month DAY+ordinal, YEAR (i.e., July 4th, 1776) and for numbers with units (i.e., “5”(minutes)) we remove the parenthesis and quotation marks (5 minutes). 5 Experimental Setup The WebNLG challenge (Colin et al., 2016) consists of mapping sets of RDF triplets"
N19-1236,W18-6557,0,0.0286439,"nning module in the end-to-end architecture. While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017), struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in adequacy or correctness of the generated text). When compared to templatebased methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input (Puzikov and Gurevych, 2018). Also, they do not allow control over the output’s structure. We speculate that this is due to demanding too much of the network: while the neural system excels at capturing the language details required for fluent realization, they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner. Proposal we propose an explicit, symbolic, text planning stage, whose output is fed into a neural generation system. The text planner determines the information structure and expresses it unambiguously—in our case as a sequence of ordered trees. This stage"
N19-1236,P04-1011,0,0.573074,"Missing"
N19-1236,D15-1199,0,0.0611641,"rent from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process. Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring coverage and reducing the number of over-generated facts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model"
N19-1236,D17-1239,0,0.14799,"(Gardent et al., 2017; Duˇsek et al., 2018). These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of Puduppully et al. (2018), who introduce a neural content-planning module in the end-to-end architecture. While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017), struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in adequacy or correctness of the generated text). When compared to templatebased methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input (Puzikov and Gurevych, 2018). Also, they do not allow control over the output’s structure. We speculate that this is due to demanding too much of the network: while the neural system excels at ca"
N19-1236,D18-1356,0,0.0302732,"ion, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism. The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation. 8 Conclusion We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outperforms the end-to-end system regarding faithfu"
N19-1356,N18-1108,1,0.949853,"separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. 1 Introduction 2018) and limitations (Chowdhury and Zamparelli, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reported for English (Ravfogel et al., 2018). Existing cross-linguistic comparisons are difficult to interpret, however. Models were inevitably trained on a different corpus for each language. The constructions tested can differ across languages (Gulordava et al., 2018). Perhaps most importantly, any two natural langua"
N19-1356,N18-2085,0,0.031905,"trained a model to mimic the POS tags one model for each combination of case system order-statistics of the target language, we manuand word order. We jointly predicted the plurality ally modified the parsed corpora; this allows us to of subject and the object. 3539 control for selected parameters, at the expense of reducing generality. Simpler synthetic languages (not based on natural corpora) have been used in a number of recent studies to examine the inductive biases of different neural architectures (Bowman et al., 2015; Lake and Baroni, 2018; McCoy et al., 2018). In another recent study, Cotterell et al. (2018) measured the ability of RNN and n-gram models to perform character-level language modeling in a sample of languages, using a parallel corpus; the main typological property of interest in that study was morphological complexity. Finally, a large number of studies, some mentioned in the introduction, have used syntactic prediction tasks to examine the generalizations acquired by neural models (see also Bernardy and Lappin 2017; Futrell et al. 2018; Lau et al. 2017; Conneau et al. 2018; Ettinger et al. 2018; Jumelet and Hupkes 2018). even when the case system was highly syncretic. Agreement feat"
N19-1356,K17-1003,1,0.895435,"Missing"
N19-1356,C18-1152,0,0.024413,"man et al., 2015; Lake and Baroni, 2018; McCoy et al., 2018). In another recent study, Cotterell et al. (2018) measured the ability of RNN and n-gram models to perform character-level language modeling in a sample of languages, using a parallel corpus; the main typological property of interest in that study was morphological complexity. Finally, a large number of studies, some mentioned in the introduction, have used syntactic prediction tasks to examine the generalizations acquired by neural models (see also Bernardy and Lappin 2017; Futrell et al. 2018; Lau et al. 2017; Conneau et al. 2018; Ettinger et al. 2018; Jumelet and Hupkes 2018). even when the case system was highly syncretic. Agreement feature prediction in some of our synthetic languages is likely to be difficult not only for RNNs but for many other classes of learners, including humans. For example, agreement in a language with very flexible word order and without case marking is impossible to predict in many cases (see §4.2), and indeed such languages are very rare. In future work, a human experiment based on the agreement prediction task can help determine whether the difficulty of our languages is consistent across humans and RNNs. Ack"
N19-1356,W18-5426,0,0.0225802,"te corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 2018; Wilcox et al., 3532 Proceedings of NAACL-HLT 2019, pages 3532–3542 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Original they say the broker took them out for lunch frequently . (they, broker: subjects; say, took: verbs; them: object) Polypersonal agreement they saykon the broker tookkarker them out for lunch frequently . (kon: plural subject; kar: singular subject; ker: plural object) Word order variation"
N19-1356,P18-1027,0,0.0265555,"thheld in training). This 1. Sentences with an object of the opposite pluconstitutes strong evidence for the RNN’s recency rality from the subject (object attractor). bias: our models extracted the generalization that subjects directly precede the verb, even though the 2. Sentences with an object of the same pluraldata were equally compatible with the generalizaity as the subject (non-attractor object).5 tion that the subject is the first core argument in 3. Sentences without an object, but with one the clause. or more nouns of the opposite plurality inThese findings align with the results of Khandelwal et al. (2018), who demonstrated that RNN 5 When the object is a noun-noun compound, it is considlanguage models are more sensitive to perturbaered a non-attractor if its head is not of the opposite plurality of the subject, regardless of the plurality of other elements. tions in recent input words compared with perturThis can only make the task harder compared with the albations to more distant parts of the input. While ternative of considering compound objects such as “screen in their case the model’s recency preference can displays” as attractors for plural subjects. 3537 SOV VOS Object (attractor) Objec"
N19-1356,P18-1132,0,0.0800223,"Missing"
N19-1356,Q16-1037,1,0.896285,"periments), we generate corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 2018; Wilcox et al., 3532 Proceedings of NAACL-HLT 2019, pages 3532–3542 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Original they say the broker took them out for lunch frequently . (they, broker: subjects; say, took: verbs; them: object) Polypersonal agreement they saykon the broker tookkarker them out for lunch frequently . (kon: plural subject; kar: singular subject; ker: plural ob"
N19-1356,J93-2004,0,0.0661216,"ency edge, and record the plurality of those arguments. Verbs that were the head of a clausal complement without a subject (xcomp dependencies) were excluded. We recorded the plurality of the dependents of the verb regardless of whether the tense and person of the verb condition agreement in English (that is, not only in third-person Synthetic Language Generation We used an expert-annotated corpus, to avoid potential confounds between the typological parameters we manipulated and possible parse errors in an automatically parsed corpus. As our starting point, we took the English Penn Treebank (Marcus et al., 1993), converted to the Universal Dependencies scheme (Nivre et al. 2016) using the Stanford converter (Schuster and Manning, 2016). We then manipulated the tree representations of the sentences in the corpus to generate parametrically modified English corpora, varying in case sys3533 1 https://github.com/Shaul1321/rnn typology Prediction task Subject accuracy Object accuracy Object recall Subject Object Joint 94.7 ± 0.3 95.7 ± 0.23 88.9 ± 0.26 90.0 ± 0.1 81.8 ± 1.4 85.4 ± 2.3 Table 1: Results of the polypersonal agreement experiments. “Joint” refers to multitask prediction of subject and object pl"
N19-1356,D18-1151,1,0.877321,"ach of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. 1 Introduction 2018) and limitations (Chowdhury and Zamparelli, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reporte"
N19-1356,W18-5423,0,0.0295496,"nguages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. 1 Introduction 2018) and limitations (Chowdhury and Zamparelli, 2018; Marvin and Linzen, 2018; Wilcox et al., 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reported for English (Ravfoge"
N19-1356,L16-1262,1,0.891305,"Missing"
N19-1356,W18-5412,1,0.766515,", 2018). Most of the work so far has focused on English, a language with a specific word order and relatively poor morphology. Do the typological properties of a language affect the ability of RNNs to learn its syntactic regularities? Recent studies suggest that they might. Gulordava et al. (2018) evaluated language models on agreement prediction in English, Russian, Italian and Hebrew, and found worse performance on English than the other languages. In the other direction, a study on agreement prediction in Basque showed substantially worse average-case performance than reported for English (Ravfogel et al., 2018). Existing cross-linguistic comparisons are difficult to interpret, however. Models were inevitably trained on a different corpus for each language. The constructions tested can differ across languages (Gulordava et al., 2018). Perhaps most importantly, any two natural languages differ in a number of typological dimensions, such as morphological richness, word order, or explicit case marking. This paper proposes a controlled experimental paradigm for studying the interaction of the inductive bias of a neural architecture with particular typological properties. Given a parsed corpus for a parti"
N19-1356,L16-1376,0,0.0707862,"., 2018). Perhaps most importantly, any two natural languages differ in a number of typological dimensions, such as morphological richness, word order, or explicit case marking. This paper proposes a controlled experimental paradigm for studying the interaction of the inductive bias of a neural architecture with particular typological properties. Given a parsed corpus for a particular natural language (English, in our experiments), we generate corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 201"
N19-1356,Q16-1035,0,0.0510525,"(Gulordava et al., 2018). Perhaps most importantly, any two natural languages differ in a number of typological dimensions, such as morphological richness, word order, or explicit case marking. This paper proposes a controlled experimental paradigm for studying the interaction of the inductive bias of a neural architecture with particular typological properties. Given a parsed corpus for a particular natural language (English, in our experiments), we generate corpora for synthetic languages that differ from the original language in one of more typological parameters (Chomsky, 1981), following Wang and Eisner (2016). In a synthetic version of English with a subject-object-verb order, for example, sentence (1-a) would be transformed into (1-b): The strong performance of recurrent neural networks (RNNs) in applied natural language processing tasks has motivated an array of studies that have investigated their ability to acquire natural language syntax without syntactic annotations; these studies have identified both strengths (Linzen et al., 2016; Giulianelli et al., 2018; (1) a. The man eats the apples. Gulordava et al., 2018; Kuncoro et al., 2018; b. The man the apples eats. van Schijndel and Linzen, 201"
N19-1356,Q17-1011,0,0.0247937,"s are consistent with the observation that languages with explicit case marking tend to allow a more flexible word orders compared with languages such as English that make use of word order to express grammatical function of words. 6 Related Work Setup We evaluated the interaction between difOur approach of constructing synthetic languages ferent case marking schemes and three word orby parametrically modifying parsed corpora for ders: flexible word order and the two orders on natural languages is closely inspired by Wang and which the model achieved the best (OVS) and Eisner (2016) (see also Wang and Eisner 2017). worst (VOS) subject prediction accuracy. We train While they trained a model to mimic the POS tags one model for each combination of case system order-statistics of the target language, we manuand word order. We jointly predicted the plurality ally modified the parsed corpora; this allows us to of subject and the object. 3539 control for selected parameters, at the expense of reducing generality. Simpler synthetic languages (not based on natural corpora) have been used in a number of recent studies to examine the inductive biases of different neural architectures (Bowman et al., 2015; Lake a"
P06-1087,P06-1084,1,0.892794,"Missing"
P06-1087,P98-1034,0,0.568272,"brew. We describe here our experiment settings, and provide the best scores obtained for each method, in comparison to the reported scores for English. All tests were done on the corpus derived from the Hebrew Tree Bank. The corpus contains 5,000 sentences, for a total of 120K tokens (agglutinated words) and 27K NP chunks (more details on the corpus appear below). The last 500 sentences were used as the test set, and all the other sentences were used for training. The results were evaluated using the CoNLL shared task evaluation tools 5 . The approaches tested were Error Driven Pruning (EDP) (Cardie and Pierce, 1998) and Transformational Based Learning of IOB tagging (TBL) (Ramshaw and Marcus, 1995). The Error Driven Pruning method does not take into account lexical information and uses only the PoS tags. For the Transformation Based method, we have used both the PoS tag and the word itself, with the same templates as described in (Ramshaw and Marcus, 1995). We tried the Transformational Based method with more features than just the PoS and the word, but obtained lower performance. Our best results for these methods, as well as the CoNLL baseline (BASE), are presented in Table 3. These results confirm tha"
P06-1087,N04-4038,0,0.0302203,"2 4.3 words surrounding the given word, and their PoS tags). One model that allows for this prediction is Support Vector Machines - SVM (Vapnik, 1995). SVM is a supervised machine learning algorithm which can handle gracefully a large set of overlapping features. SVMs learn binary classifiers, but the method can be extended to multiclass classification (Allwein et al., 2000; Kudo and Matsumoto, 2000). SVMs have been successfully applied to many NLP tasks since (Joachims, 1998), and specifically for base phrase chunking (Kudo and Matsumoto, 2000; 2003). It was also successfully used in Arabic (Diab et al., 2004). The traditional setting of SVM for chunking uses for the context of the token to be classified a window of two tokens around the word, and the features are the PoS tags and lexical items (word forms) of all the tokens in the context. Some settings (Kudo and Matsumoto, 2000) also include the IOB tags of the two “previously tagged” tokens as features (see Fig. 1). This setting (including the last 2 IOB tags) performs nicely for the case of Hebrew Simple NPs chunking as well. Linguistic features are mapped to SVM feature vectors by translating each feature such as “PoS at location n-2 is NOUN”"
P06-1087,P05-1071,0,0.049593,"Missing"
P06-1087,W00-0730,0,0.495824,"Missing"
P06-1087,P03-1004,0,0.0996795,"Missing"
P06-1087,W98-1418,1,0.781308,"Missing"
P06-1087,W95-0107,0,0.299455,"d for each method, in comparison to the reported scores for English. All tests were done on the corpus derived from the Hebrew Tree Bank. The corpus contains 5,000 sentences, for a total of 120K tokens (agglutinated words) and 27K NP chunks (more details on the corpus appear below). The last 500 sentences were used as the test set, and all the other sentences were used for training. The results were evaluated using the CoNLL shared task evaluation tools 5 . The approaches tested were Error Driven Pruning (EDP) (Cardie and Pierce, 1998) and Transformational Based Learning of IOB tagging (TBL) (Ramshaw and Marcus, 1995). The Error Driven Pruning method does not take into account lexical information and uses only the PoS tags. For the Transformation Based method, we have used both the PoS tag and the word itself, with the same templates as described in (Ramshaw and Marcus, 1995). We tried the Transformational Based method with more features than just the PoS and the word, but obtained lower performance. Our best results for these methods, as well as the CoNLL baseline (BASE), are presented in Table 3. These results confirm that the task of Simple NP chunking is harder in Hebrew than in English. 4.2 Support Ve"
P06-1087,N03-1028,0,0.283133,"Missing"
P06-1087,W00-0726,0,0.269406,"Missing"
P06-1087,C98-1034,0,\N,Missing
P07-1029,P06-1084,1,0.81795,"Missing"
P07-1029,P98-1034,0,0.0933902,"d for Prepositions and Punctuation marks, followed by Adverbs, and Conjunctions. Strikingly, lexical information for most open-class PoS (including Proper Names and Nouns) has very little impact on Hebrew chunking performance. From this observation, one could conclude that enriching a model based only on PoS with lexical features for only a few closed-class PoS (prepositions and punctuation) could provide appropriate results even with a simpler learning method, one that cannot deal with a large number of features. We tested this hypothesis by training the Error-Driven Pruning (EDP) method of (Cardie and Pierce, 1998) with an extended set of features. EDP with PoS features only produced an F-result of 76.3 on HEBGold . By adding lexical features only for prepositions {}של כ ה ב מ, one conjunction { }וand punctuation, the F-score on HEBGold indeed jumps to 85.4. However, when applied on HEBErr , EDP falls down again to 59.4. This striking disparity, by comparison, lets us appreciate the resilience of the SVM model to PoS tagging errors, and its generalization capability even with a reduced number of lexical features. Another implication of this data is that commas and quotation marks play a major role i"
P07-1029,W00-0730,0,0.11155,"Missing"
P07-1029,P03-1004,0,0.121983,"Missing"
P07-1029,W95-0107,0,0.203013,"Missing"
P07-1029,C02-1101,0,0.0595552,"Missing"
P07-1029,W00-0726,0,0.105694,"Missing"
P07-1029,W99-0606,0,\N,Missing
P07-1029,C98-1034,0,\N,Missing
P07-1029,P06-1087,1,\N,Missing
P08-1043,P06-1084,0,0.386084,"and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint ta"
P08-1043,P08-1083,1,0.870844,"Missing"
P08-1043,W05-0706,0,0.205522,"Missing"
P08-1043,E06-1047,0,0.0142792,"n is modified by a proceeding space-delimited adjective. It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”). This leads to word- and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers. One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct"
P08-1043,D07-1022,0,0.226712,"e aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and pro371 Proceedings of ACL-08: HLT, pages 371–379, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before ex"
P08-1043,P07-1029,1,0.791089,"ates the lexeme l spans from node ni to node nj ). GL is then used to parse the string tn1 . . . tnk−1 , where tni is a terminal corresponding to the lattice span between node ni and ni+1 . Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities. We use a patched version of BitPar allowing for direct input of probabilities instead of counts. We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. proposed in (Tsarfaty, 2006). In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007). In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007). Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007). For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein. Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task. To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segm"
P08-1043,P05-1071,0,0.053423,"Missing"
P08-1043,itai-etal-2006-computational,0,0.102035,"irst 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and “malformed”7 were removed. The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. (we ignored the 419 trees in their development set.) Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incom5 The comparison to performance on version 2.0 is meaningless not only because of the change in size, but also conceptual changes in the annotation scheme 6 Unfortunatley running our setup on the v2.0 data set is currently not possible due to missing tokens-morphemes alignment in the v2.0 treebank. 7 We thank Shay Cohen for providing us with their data set and evaluation Software. 376 patible with the one of the Hebrew Treebank. 8 For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen"
P08-1043,J95-3004,0,0.127064,"Missing"
P08-1043,C04-1024,0,0.0393827,"004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments. Models that employ this strategy are denoted hsp. To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning. For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser, 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the treebank. Our first model is GTplain , a PCFG learned from the treebank after removing all functional features from the syntactic categories. In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as 8 Mapping between the two schemes involves nondeterministic many-to-many mappings, and in some cases require a change in t"
P08-1043,D07-1046,0,0.0692168,"Missing"
P08-1043,H05-1060,0,0.639388,"t counted”. When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”. There is no relation between these two interpretations other then the fact that their surface forms coincide, and we argue that the only reason to prefer one analysis over the other is compositional. A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P (REL, VB|fmnh, context) = P (REL|f)P (VB|mnh, REL)P (REL, VB |context) and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al., 2005) present similar models. We suggest that in unlexicalized PCFGs the syntactic context may be explicitly modeled in the derivation probabilities. Hence, we take the probability of the event fmnh analyzed as REL VB to be P (REL → f|REL) × P (VB → mnh|VB) This means that we generate f and mnh independently depending on their corresponding PoS tags, and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context. In our model, however, all lattice paths are taken to be a-priori equally"
P08-1043,tsarfaty-goldberg-2008-word,1,0.785506,"espond to the original surface form as super-segmental morphology. An additional case of super-segmental morphology is the case of Pronominal Clitics. Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements. The additional morphological material in such cases appears after the stem and realizes the extended meaning. The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008). 2 Modern Hebrew Structure Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens. The form fmnh, for example, can be understood as the verb “lubricated”, the possessed noun “her oil”, the adjective “fat” or the verb “got fat”. Furthermore, the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibili"
P08-1043,W07-2219,1,0.840778,"Missing"
P08-1043,P06-3009,1,0.525742,"all parsing related work on Arabic and its dialects (Chiang et al., 2006)). The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006). The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun. The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance. Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task. Cohen and Smith (2007) followed up on these results and pro371 Proceedings of ACL-08: HLT, pages 371–379, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own. Here w"
P08-1043,W05-0702,0,0.0394917,"Missing"
P08-1083,W05-0706,0,0.58326,"Missing"
P08-1083,N04-4038,0,0.0459524,"Missing"
P08-1083,P06-1086,0,0.0261585,"eceded by the identified prefix, we remove this possible analysis. The eventual outcome of the 732 Patterns Word formation in Hebrew is based on root+pattern and affixation. Patterns can be used to identify the lexical category of unknowns, as well as other inflectional properties. Nir (1993) investigated word-formation in Modern Hebrew with a special focus on neologisms; the most common wordformation patterns he identified are summarized in Table 3. A naive approach for unknown resolution would add all analyses that fit any of these patterns, for any given unknown token. As recently shown by Habash and Rambow (2006), the precision of such a strategy can be pretty low. To address this lack of precision, we learn a maximum entropy model on the basis of the following binary features: one feature for each pattern listed in column Formation of Table 3 (40 distinct patterns) and one feature for “no pattern”. Pattern-Letters This maximum entropy model is learned by combining the features of the letters model and the patterns model. Linear-Context-based p(t|c) approximation The three models above are context free. The linear-context model exploits information about the lexical context of the unknown words: to es"
P08-1083,J95-3004,0,0.422895,"r “no pattern”. Pattern-Letters This maximum entropy model is learned by combining the features of the letters model and the patterns model. Linear-Context-based p(t|c) approximation The three models above are context free. The linear-context model exploits information about the lexical context of the unknown words: to estimate the probability for a tag t given a context c – p(t|c) – based on all the words in which a context occurs, the algorithm works on the known words in the corpus, by starting with an initial tag-word estimate p(t|w) (such as the morpho-lexical approximation, suggested by Levinger et al. (1995)), and iteratively re-estimating: P pˆ(t|c) = pˆ(t|w) = p(t|w)p(w|c) Z P c∈C p(t|c)p(c|w)allow(t, w) Z w∈W where Z is a normalization factor, W is the set of all words in the corpus, C is the set of contexts. allow(t, w) is a binary function indicating whether t is a valid tag for w. p(c|w) and p(w|c) are estimated via raw corpus counts. Loosely speaking, the probability of a tag given a context is the average probability of a tag given any Category Verb Participle Noun Adjective Adverb Formation ’iCCeC miCCeC CiCCen Template CiCCet tiCCeC meCuCaca Template muCCaC maCCiC ut ay an Suffixation o"
P08-1083,W07-0813,0,0.0235512,"Missing"
P08-1083,J97-3003,0,0.119714,"Missing"
P08-1083,C04-1067,0,0.0723092,"Missing"
P08-1083,P99-1023,0,0.0662197,"Missing"
P08-1083,J93-2006,0,0.24914,"Missing"
P08-1083,J95-2001,0,\N,Missing
P08-1085,C04-1080,0,0.8171,"Missing"
P08-1085,J95-4004,0,0.25719,"Missing"
P08-1085,W95-0101,0,0.853773,"Missing"
P08-1085,P00-1035,0,0.0238717,"each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token  ילדcan be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections הילדים,ילדים,( הילדthe boy, boys, the boys), while the verb SW set is composed of gender and tense inflections ילדו,( ילדהshe/they gave birth). The approximated probability of each analysis is based on the corpus frequency of its SW set. For the complete details, refer to the original paper. Cucerzan and Yarowsky (2000) proposed a similar method for the unsupervised estimation of p(t|w) in English, relying on simple spelling features to characterize similar word classes. Linear-Context-based p(t|w) approximation The method of Levinger et al. makes use of Hebrew inflection patterns in order to estimate context free approximation of p(t|w) by relating a word to its different inflections. However, the context in which a word occurs can also be very informative with respect to its POS-analysis (Sch¨utze, 1995). We propose a novel algorithm for estimating p(t|w) based on the contexts in which a word occurs.3 The"
P08-1085,J95-2001,0,0.0163513,"(w|c) and p(c|w) via relative frequency over all the events w1, w2, w3 occurring at least 10 times in the corpus. allow(t, w) follows the dictionary. Because of the wide coverage of the Hebrew lexicon, we take RELC to be C (all available contexts). Application to Hebrew In Hebrew, several words combine into a single token in both agglutinative and fusional ways. This results in a potentially high number of tags for each token. On average, in our corpus, the number of possible analyses per known word reached 2.7, with the ambiguity level of the extended POS tagset in corpus for English (1.41) (Dermatas and Kokkinakis, 1995). In this work, we use the morphological analyzer of MILA – Knowledge Center for Processing Hebrew (KC analyzer). In contrast to English tagsets, the number of tags for Hebrew, based on all combinations of the morphological attributes, can grow theoretically to about 300,000 tags. In practice, we found ‘only’ about 3,560 tags in a corpus of 40M tokens training corpus taken from Hebrew news material and Knesset transcripts. For testing, we manually tagged the text which is used in the Hebrew Treebank (Sima’an et al., 2001) (about 90K tokens), according to our tagging guidelines. 4.1 Initial Con"
P08-1085,A94-1009,0,0.322322,"Missing"
P08-1085,P07-1094,0,0.523311,"Missing"
P08-1085,N06-1041,0,0.0680158,"unsupervised PoS tagging of Hebrew text and for the common WSJ English test sets. We show that our method achieves state-ofthe-art results for the English setting, even with a relatively small dictionary. Furthermore, while recent work report results on a reduced English tagset of 17 PoS tags, we also present results for the complete 45 tags tagset of the WSJ corpus. This considerably raises the bar of the EM-HMM baseline. We also report state-of-the-art results for Hebrew full mor1 Another notable work, though within a slightly different framework, is the prototype-driven method proposed by (Haghighi and Klein, 2006), in which the dictionary is replaced with a very small seed of prototypical examples. 747 phological disambiguation. Our primary conclusion is that the problem of learning effective stochastic classifiers remains primarily a search task. Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations. A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to captu"
P08-1085,J95-3004,0,0.0692931,"experiment with constraining the p(t|t−1 , t+1 ) distribution. 2 Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). 748 General syntagmatic constraints We set linguistically motivated constraints on the p(t|t−1 , t+1 ) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token  ילדcan be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections הילדים,ילדים,( הילדthe boy, boys, the boys), while the verb SW set is composed of gender and t"
P08-1085,J94-2001,0,0.927128,", 2007) (GG), (Toutanova and Johnson, 2008) (TJ). Introduction The task of unsupervised (or semi-supervised) partof-speech (POS) tagging is the following: given a dictionary mapping words in a language to their possible POS, and large quantities of unlabeled text data, learn to predict the correct part of speech for a given word in context. The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged. Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm. However, as recently noted ∗ This work is supported in part by the Lynn and William Frankel Center for Computer Science. All the work mentioned above focuses on unsupervised English POS tagging. The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus). As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available. The problem is rather approached as a workbench"
P08-1085,E95-1020,0,0.660222,"Missing"
P08-1085,D07-1046,0,0.0215821,"Missing"
P08-1085,P05-1044,0,0.596615,"Missing"
P08-1085,P99-1023,0,0.0305171,"del can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2 . In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t−1 , t+1 ) distribution. 2 Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). 748 General syntagmatic constraints We set linguistically m"
P08-1085,N03-1033,0,0.0364511,"In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t−1 , t+1 ) distribution. 2 Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). 748 General syntagmatic constraints We set linguistically motivated constraints on the p(t|t−1 , t+1 ) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of"
P08-1085,J93-2006,0,0.0324374,"Missing"
P08-2060,P07-1029,1,0.833607,"Zipfian nature of language phenomena is reflected in the distribution of features in the support vectors. It is because of common features that the PKI reverse indexing method does not yield great improvements: if at least one of the features of the current instance is active in a support vector, this vector is taken into account in the sum calculation, and the common features are active in many support vectors. On the other hand, the long tail of rare features is the reason the Kernel Expansion methods requires 2 This loss of accuracy in comparison to the PKE approach is to be expected, as (Goldberg and Elhadad, 2007) showed that the effect of removing features prior to the learning stage is much more severe than removing them after the learning stage. 3 Our presentation is for the case where d = 2, as this is by far the most useful kernel. However, the method can be easily adapted to higher degree kernels as well. For completeness, our toolkit provides code for d = 3 as well as 2. 239 so much space: every rare feature adds many possible feature pairs. We propose a combined method. We first split common from rare features. We then use Kernel Expansion on the few common features, and PKI for the remaining r"
P08-2060,C02-1054,0,0.179192,"ned popularity as they constantly outperform other learning algorithms for many NLP tasks. Unfortunately, once a model is trained, the decision function for kernel-based classifiers such as SVM is expensive to compute, and can grow linearly with the size of the training data. In contrast, the computational complexity for the decisions functions of most non-kernel based classifiers does not depend on the size of the training data, making them orders of magnitude faster to compute. For this reason, research effort was directed at speeding up the classification process of polynomial-kernel SVMs (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Wu et al., 2007). Existing accelerated SVM solutions, however, either require large amounts of Background and Previous Work In classification based NLP algorithms, a word and its context is considered a learning sample, and encoded as Feature Vectors. Usually, context data includes the word being classified (w0 ), its part-ofspeech (PoS) tag (p0 ), word forms and PoS tags of neighbouring words (w−2 , . . . , w+2 , p−2 , . . . , p+2 , etc.). Computed features such as the length of a word or its suffix may also be added. A feature vector (F ) is encoded as an indexed"
P08-2060,P03-1004,0,0.261932,"stantly outperform other learning algorithms for many NLP tasks. Unfortunately, once a model is trained, the decision function for kernel-based classifiers such as SVM is expensive to compute, and can grow linearly with the size of the training data. In contrast, the computational complexity for the decisions functions of most non-kernel based classifiers does not depend on the size of the training data, making them orders of magnitude faster to compute. For this reason, research effort was directed at speeding up the classification process of polynomial-kernel SVMs (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Wu et al., 2007). Existing accelerated SVM solutions, however, either require large amounts of Background and Previous Work In classification based NLP algorithms, a word and its context is considered a learning sample, and encoded as Feature Vectors. Usually, context data includes the word being classified (w0 ), its part-ofspeech (PoS) tag (p0 ), word forms and PoS tags of neighbouring words (w−2 , . . . , w+2 , p−2 , . . . , p+2 , etc.). Computed features such as the length of a word or its suffix may also be added. A feature vector (F ) is encoded as an indexed list of all the features p"
P08-2060,nivre-etal-2006-maltparser,0,0.10878,"Department of Computer Science POB 653 Be’er Sheva, 84105, Israel {yoavg,elhadad}@cs.bgu.ac.il Abstract memory, or resort to heuristics – computing only an approximation to the real decision function. This work aims at speeding up the decision function computation for low-degree polynomial kernel classifiers while using only a modest amount of memory and still computing the exact function. This is achieved by taking into account the Zipfian nature of natural language data, and structuring the computation accordingly. On a sample application (replacing the libsvm classifier used by MaltParser (Nivre et al., 2006) with our own), we observe a speedup factor of 30 in parsing time. We present a fast, space efficient and nonheuristic method for calculating the decision function of polynomial kernel classifiers for NLP applications. We apply the method to the MaltParser system, resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy (a 30 time speedup over existing methods). The method implementation is available as the open-source splitSVM Java library. 1 Introduction 2 Over the last decade, many natural language processing tasks are being cast as cla"
P08-2060,P07-2017,0,0.777603,"earning algorithms for many NLP tasks. Unfortunately, once a model is trained, the decision function for kernel-based classifiers such as SVM is expensive to compute, and can grow linearly with the size of the training data. In contrast, the computational complexity for the decisions functions of most non-kernel based classifiers does not depend on the size of the training data, making them orders of magnitude faster to compute. For this reason, research effort was directed at speeding up the classification process of polynomial-kernel SVMs (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Wu et al., 2007). Existing accelerated SVM solutions, however, either require large amounts of Background and Previous Work In classification based NLP algorithms, a word and its context is considered a learning sample, and encoded as Feature Vectors. Usually, context data includes the word being classified (w0 ), its part-ofspeech (PoS) tag (p0 ), word forms and PoS tags of neighbouring words (w−2 , . . . , w+2 , p−2 , . . . , p+2 , etc.). Computed features such as the length of a word or its suffix may also be added. A feature vector (F ) is encoded as an indexed list of all the features present in the trai"
P11-2037,H91-1060,0,0.495299,". *. .to. NP . VBZ . . . who S. SQ/WHNP . VP/WHNP/NP . VBN . . is. NNP . . NP John . believed . . . S/WHNP/NP . *. VP . . VB TO . . ϵ. .to. NP . -NONE. admire . . (a) VP/WHNP . *T* . VP/WHNP . . VB NP/WHNP . admire . . *T* . (b) ϵ. Figure 2: English parse tree with empty elements marked. (a) As annotated in the Penn Treebank. (b) With empty elements reconﬁgured and slash categories added. where “items” are deﬁned differently for each metric, as follows. Deﬁne a nonterminal node, for present purposes, to be a node which is neither a terminal nor preterminal node. The standard PARSEVAL metric (Black et al., 1991) counts labeled nonempty brackets: items are (X, i, j) for each nonempty nonterminal node, where X is its label and i, j are the start and end positions of its span. Yang and Xue (2010) simply count unlabeled empty elements: items are (i, i) for each empty element, where i is its position. If multiple empty elements occur at the same position, they only count the last one. The metric originally proposed by Johnson (2002) counts labeled empty brackets: items are (X/t, i, i) for each empty nonterminal node, where X is its label and t is the type of the empty element it dominates, but also (t, i,"
P11-2037,P04-1082,0,0.624876,"ed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality im"
P11-2037,D10-1062,0,0.538829,"O* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006), which we extend to predict empty cateProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shor"
P11-2037,W03-1005,0,0.661306,"l., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the pa"
P11-2037,N06-1024,0,0.61951,"f empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method make"
P11-2037,P11-2124,1,0.842048,"ng allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of ﬂexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1 Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). 213 processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁcation is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change: trying rules involving prete"
P11-2037,P08-1043,1,0.833705,"a generalization of CKY parsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of ﬂexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1 Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). 213 processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁcation is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change:"
P11-2037,C10-1045,0,0.00584234,"stead of a predetermined list of terminals. Lattice parsing adds a layer of ﬂexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1 Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). 213 processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁcation is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change: trying rules involving preterminals even when the span is greater"
P11-2037,P02-1018,0,0.893331,"dependent VP (shíshī fǎlǜ tiáowén). The Penn Treebanks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with"
P11-2037,J93-2004,0,0.0399322,"ted (John was believed to admire who?). Empty elements exist in many languages and serve different purposes. In languages such as Chinese and Korean, where subjects and objects can be dropped to avoid duplication, empty elements are particularly important, as they indicate the position of dropped arguments. Figure 1 gives an example of a Chinese parse tree with empty elements. The ﬁrst empty element (*pro*) marks the subject of the whole sentence, a pronoun inferable from context. The second empty element (*PRO*) marks the subject of the dependent VP (shíshī fǎlǜ tiáowén). The Penn Treebanks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by severa"
P11-2037,P06-1055,0,0.0312248,"empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006), which we extend to predict empty cateProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for Chinese, it outperforms the method of Yang and Xue (2010).1 2 Method Our method is fairly simple. We take a state-of-theart parsing mod"
P11-2037,roark-etal-2006-sparseval,0,0.0134392,"Missing"
P11-2037,P06-1023,0,0.743656,"s ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent an"
P11-2037,C10-2158,0,0.744237,"止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006), which we extend to predict empty cateProceedings of the 49th Ann"
P11-2124,P08-1083,1,0.819379,"shing lexical items from non-terminals by a specified marking instead of by their position in the chart. We 706 modified the PCFG-LA Berkeley parser to accept lattice input at inference time (training is performed as usual on fully observed treebank trees). Lattice Construction We construct the token lattices using MILA, a lexicon-based morphological analyzer which provides a set of possible analyses for each token (Itai and Wintner, 2008). While being a high-coverage lexicon, its coverage is not perfect. For the future, we consider using unknown handling techniques such as those proposed in (Adler et al., 2008). Still, the use of the lexicon for lattice construction rather than relying on forms seen in the treebank is essential to achieve parsing accuracy. Lexical Probabilities Estimation Lexical p(t → w) probabilities are defined over individual segments rather than for complete tokens. It is the role of the syntactic model to assign probabilities to contexts which are larger than a single segment. We use the default lexical probability estimation of the Berkeley parser.3 Goldberg et al. (2009) suggest to estimate lexical probabilities for rare and unseen segments using emission probabilities of an"
P11-2124,P11-2037,1,0.894174,"Missing"
P11-2124,W09-1008,0,0.068176,"Missing"
P11-2124,P08-1043,1,0.867707,"1) A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser. The shortcoming of this method, as noted by (Tsarfaty, 2006), is that many segmentation decisions cannot be resolved based on local context alone. Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence. Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of"
P11-2124,P08-1085,1,0.89567,"Missing"
P11-2124,E09-1038,1,0.918851,"Missing"
P11-2124,C10-1045,0,0.324071,"d be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006). 704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a language-processing perspective are: Affixation Common function words a"
P11-2124,D09-1087,0,0.0458827,"lit non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar. This parsing methodology is very robust, producing state of the art accuracies for English, as well as many other languages including German (Petrov and Klein, 2008), French (Candito et al., 2009) and Chinese (Huang and Harper, 2009) among others. The grammar learning process is applied to binarized parse trees, with 1st-order vertical and 0thorder horizontal markovization. This means that in 2 http://code.google.com/p/berkeleyparser/ Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the pronomi"
P11-2124,P03-1054,0,0.0239838,") makes it hard to guess the morphological analyses 705 of an unknown word based on its prefix and suffix, as usually done in other languages. Unvocalized writing system Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2 , works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitti"
P11-2124,P05-1010,0,0.0851098,"level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2 , works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitting each non-terminal category in two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tr"
P11-2124,W08-1005,0,0.0368367,"two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar. This parsing methodology is very robust, producing state of the art accuracies for English, as well as many other languages including German (Petrov and Klein, 2008), French (Candito et al., 2009) and Chinese (Huang and Harper, 2009) among others. The grammar learning process is applied to binarized parse trees, with 1st-order vertical and 0thorder horizontal markovization. This means that in 2 http://code.google.com/p/berkeleyparser/ Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the un"
P11-2124,P06-1055,0,0.39215,"imented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006). 704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a language-processing perspective are: Affixation Common function words are prefixed to the following word. These include: m(“from”) f (“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). Several such elements may attach together, producing forms such as wfmhfmf (w-f-m-hfmf “and-that-from-the-sun”). Notice"
P11-2124,W10-1405,0,0.078517,"Missing"
P11-2124,P06-3009,0,0.282706,"affixes to content bearing words, sharing the same space-delimited token. For example, the Hebrew token bcl1 can be interpreted as the single noun meaning “onion”, or as a sequence of a preposition and a noun b-cl meaning “in (the) shadow”. In such languages, the sequence of lexical 1 We adopt here the transliteration scheme of (Sima’an et al., 2001) A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser. The shortcoming of this method, as noted by (Tsarfaty, 2006), is that many segmentation decisions cannot be resolved based on local context alone. Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence. Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better gra"
P13-2017,W06-2920,0,0.808379,"ebank is made freely available in order to facilitate research on multilingual dependency parsing.1 1 Introduction In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest (K¨ubler et al., 2009). Research in dependency parsing – computational methods to predict such representations – has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages. In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format (Buchholz and Marsi, 2006; Nivre et al., 2007). While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Mee"
P13-2017,W02-1503,0,0.0535563,"Missing"
P13-2017,W09-2307,0,0.0712427,"Missing"
P13-2017,P11-1061,1,0.243183,"aking fine-grained label distinctions was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2."
P13-2017,W08-1301,0,0.173029,"Missing"
P13-2017,D11-1006,1,0.855635,"icient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to cr"
P13-2017,P07-1122,1,0.763455,"as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recentl"
P13-2017,de-marneffe-etal-2006-generating,0,0.333356,"Missing"
P13-2017,P09-1042,1,0.250489,"arsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English,"
P13-2017,W12-1909,0,0.0247816,"Missing"
P13-2017,petrov-etal-2012-universal,1,0.702758,"nal Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, su"
P13-2017,D09-1086,0,0.0317978,"ross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six lang"
P13-2017,Q13-1001,1,0.0665003,"Missing"
P13-2017,W04-2709,0,0.0429789,"annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 201"
P13-2017,P13-2103,0,0.129897,"Missing"
P13-2017,N06-2015,0,0.0347787,"s can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do th"
P13-2017,zeman-etal-2012-hamledt,0,0.060315,"Missing"
P13-2017,P03-1054,0,0.0144203,"nch data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In"
P13-2017,P11-2033,1,0.192695,"Missing"
P13-2017,P04-1061,0,0.0673447,"word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2"
P13-2017,D12-1125,0,0.0145332,"for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-ling"
P13-2017,J93-2004,0,\N,Missing
P13-2017,W08-1300,0,\N,Missing
P13-2017,D07-1096,1,\N,Missing
P13-2111,A00-2018,0,0.312412,"enn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗ 1 The Huang-Sagae DP parser (http://acl.cs.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2 Our notion of TSS is crucially different from the data Supported in part by DARPA FA8750-13-2-0041 (DEFT). 628"
P13-2111,P04-1015,0,0.222597,"ime, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine t"
P13-2111,N10-1115,1,0.839844,"Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers. Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation"
P13-2111,P10-1110,1,0.944848,"can grow to O(n) size in the worst-case, making the state-copy (and hence state-transition) an O(n) operation. Thus, beam search implementations that copy the entire state are in fact quadratic O(kn2 ) and not linear, with a slowdown factor of O(kn) with respect to greedy parsers, which is confirmed empirically in Figure 4. We present a way of decreasing the O(n) transition cost to O(1) achieving strictly linear-time parsing, using a data structure of Tree-Structured Stack (TSS) that is inspired by but simpler than the graph-structured stack (GSS) of Tomita (1985) used in dynamic programming (Huang and Sagae, 2010).2 On average Treebank sentences, the TSS Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2 ), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and"
P13-2111,D09-1127,1,0.83124,"plemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (a"
P13-2111,koen-2004-pharaoh,0,0.0615061,"Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. However, we will demonstrate below that, contrary to popular ∗ 1 The Huang-Sagae DP parser (http://acl.cs.qc.edu) does run in O(kn), which inspired this paper when we experimented with simulating non-DP beam search using GSS. 2 Our notion of TSS is crucially different from the data Supported in part by DARPA FA8750-13-2-0041 (DEFT). 628 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628–633, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lin"
P13-2111,P11-1068,0,0.110933,"Aviv, 5290002 Israel yoav.goldberg@gmail.com Kai Zhao Liang Huang Graduate Center and Queens College City University of New York {kzhao@gc, lhuang@cs.qc}.cuny.edu {kzhao.hf, liang.huang.sh}.gmail.com Abstract belief, in most standard implementations their actual runtime is in fact O(kn2 ) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of"
P13-2111,C04-1010,0,0.10583,"n2 ) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-based with respect to greedy parsers. Copying is expensive, because the state keeps track of (a) a stack and (b) the set of dependency-arcs added so far. Both the arc-set and the stack can grow to O(n) size in the worst-case, making the state-copy (and hence state-"
P13-2111,J08-4003,0,0.544581,"rser, k items (hypotheses) are maintained. Items are composed of a state and a score. At step i, each of the k items is extended by applying all possible transitions to the given state, resulting in k × a items, a being the number of possible transitions. Of these, the top scoring k items are kept and used in step i + 1. Finally, the tree associated with the highest-scoring item is returned. j<n ` : hj, S|s1 |s0 i : A ` + 1 : hj, S|s0 i : A ∪ {s1 x s0 } ` : hj, S|s1 |s0 i : A ` + 1 : hj, S|s1 i : A ∪ {s1 y s0 } 2n − 1 : hn, s0 i: A Figure 1: An abstraction of the arc-standard deductive system Nivre (2008). The stack S is a list of heads, j is the index of the token at the front of the buffer, and ` is the step number (beam index). A is the arc-set of dependency arcs accumulated so far, which we will get rid of in Section 4.1. version, being linear time, leads to a speedup of 2x∼2.7x over the naive implementation, and about 1.3x∼1.7x over the optimized baseline presented in Section 5. Having achieved efficient state-transitions, we turn to feature extraction and dot products (Section 6). We present a simple scheme of sharing repeated scoring operations across different beam items, resulting in"
P13-2111,J01-2004,0,0.0756405,"than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beam"
P13-2111,D08-1059,0,0.10756,"ansition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004"
P13-2111,P11-1069,0,0.256278,"ion, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empirically. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treebank sentences, and are orders of magnitude faster on much longer sentences. 1 Introduction Beam search incremental parsers (Roark, 2001; Collins and Roark, 2004; Zhang and Clark, 2008; Huang et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Zhang and Clark, 2011) provide very competitive parsing accuracies for various grammar formalisms (CFG, CCG, and dependency grammars). In terms of purning strategies, they can be broadly divided into two categories: the first group (Roark, 2001; Collins and Roark, 2004) uses soft (aka probabilistic) beams borrowed from bottom-up parsers (Charniak, 2000; Collins, 1999) which has no control of complexity, while the second group (the rest and many more recent ones) employs hard beams borrowed from machine translation (Koehn, 2004) which guarantee (as they claim) a linear runtime O(kn) where k is the beam width. Howeve"
P13-2111,P11-2033,0,0.33336,"av.goldberg@gmail.com Kai Zhao Liang Huang Graduate Center and Queens College City University of New York {kzhao@gc, lhuang@cs.qc}.cuny.edu {kzhao.hf, liang.huang.sh}.gmail.com Abstract belief, in most standard implementations their actual runtime is in fact O(kn2 ) rather than linear. Although this argument in general also applies to dynamic programming (DP) parsers,1 in this paper we only focus on the standard, non-dynamic programming approach since it is arguably still the dominant practice (e.g. it is easier with the popular arc-eager parser with a rich feature set (Kuhlmann et al., 2011; Zhang and Nivre, 2011)) and it benefits more from our improved algorithms. The dependence on the beam-size k is because one needs to do k-times the number of basic operations (feature-extractions, dot-products, and statetransitions) relative to a greedy parser (Nivre and Scholz, 2004; Goldberg and Elhadad, 2010). Note that in a beam setting, the same state can expand to several new states in the next step, which is usually achieved by copying the state prior to making a transition, whereas greedy search only stores one state which is modified in-place. Copying amounts to a large fraction of the slowdown of beam-bas"
P13-2111,J03-4003,0,\N,Missing
P14-2048,P13-1157,0,0.151732,"ence set. In Figure 1, we show the relationship of the observed detection accuracy for each system with the BLEU score of that system. As is evident, regardless of the feature set or non-MT sentences used, the correlation between detection accuracy and BLEU 3.2 Machine Translation Detection Regarding the detection of machine translated text, Carter and Inkpen (2012) translated the Hansards of the 36th Parliament of Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific feature sets for this purpose, including indicators of the ”Phrase Salad” (Lopez, 2008) phenomenon or ”Gappy-Phrases” (Bansal et al., 2011). While Arase and Zhou (2013) considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine tra"
P14-2048,P11-1131,0,0.0164779,"Canada using the Microsoft Bing MT web service, and conducted three detection experiments at document level, using unigrams, average token length, and type-token ratio as features. Arase and Zhou (2013) trained a sentence-level classifier to distinguish machine translated text from human generated text on English and Japanese web-page corpora, translated by Google Translate, Bing and an in-house SMT system. They achieved very high detection accuracy using application-specific feature sets for this purpose, including indicators of the ”Phrase Salad” (Lopez, 2008) phenomenon or ”Gappy-Phrases” (Bansal et al., 2011). While Arase and Zhou (2013) considered MT detection at sentence level, as we do in this paper, they did not study the correlation between the translation quality of the machine translated text and the ability to detect it. We show below that such detection is possible with very high accuracy only on low-quality translations. We examine this detection accuracy vs. quality correlation, with various MT systems, such as rule-based and statistical MT, both commercial and in-house, using various feature sets. 3 3.1 Detection Experiments Features We wish to distinguish machine translated English se"
P14-2048,2005.mtsummit-papers.11,0,0.0245009,"3), containing outputs from 13 different MT systems and their human evaluations. We conduct the same classification experiment as above, with features based on function words and POS tags, and SMO-based SVM as the classifier. We first use 3000 referIn the second experiment set, we test our detection method on SMT systems we created, in which we have control over the training data and the expected overall relative translation quality. In order to do so, we use the Moses statistical machine translation toolkit (Koehn et al., 2007). To train the systems, we take a portion of the Europarl corpus (Koehn, 2005), creating 7 different SMT systems, each using a different amount of training data, for both the translation model and language model. We do this in order to create different quality translation systems, details of which are described in Table 3. For purposes of classification, we use the same content independent features as in the previous experiment, based on func291 Features Data Google Moses Systran ProMT Linguatec Skycode Trident R2 mixed mixed func. w. func. w. POS POS MT/non-ref MT/ref MT/non-ref MT/ref MT/non-ref MT/ref 63.34 59.51 60.43 57.27 60.32 57.21 72.02 69.47 69.17 66.05 64.39"
P14-2048,P11-1132,1,0.889669,"t al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, nu289 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjunctions etc. Koppel and Ordan (2011) classified texts to original or translated, using a list of 300 function words taken from LIWC (Pennebaker et al., 2001) as features. Volanski et al. (2013) also tested various hypotheses regarding ”Translationese”, using 32 different linguisticallyinformed features, to assess the degree to which different sets of features can distinguish between translated and original texts. as well as the presence or absence of each of 467 function words taken from LIWC (Pennebaker et al., 2001). We consider only those entries that appear at least ten times in the entire corpus, in order to reduce sparsity"
P14-2048,2009.mtsummit-papers.9,0,0.0403542,"dely for many years. Attempts to define their characteristics, often called ”Translation Universals”, include (Toury, 1980; Blum-Kulka and Levenston, 1983; Baker, 1993; Gellerstam, 1986). The differences between native and translated texts found there go well beyond systematic translation errors and point to a distinct ”Translationese” dialect. Using automatic text classification methods in the field of translation studies had many use cases in recent years, mainly as an empirical method of measuring, proving or contradicting translation universals. Several works (Baroni and Bernardini, 2006; Kurokawa et al., 2009; Ilisei et al., 2010) used text classification techniques in order to distinguish human translated text from native language text at document or paragraph level, using features like word and POS n-grams, proportion of grammatical words in the text, nouns, finite verbs, auxiliary verbs, adjectives, adverbs, nu289 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 289–295, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics merals, pronouns, prepositions, determiners, conjunctions etc. Koppel and"
P14-2048,W12-3102,0,0.0553626,"Missing"
P14-2048,2001.mtsummit-papers.68,0,0.0592791,"Computer Science Dept. of Computer Science Bar Ilan University Bar Ilan University Bar Ilan University Ramat-Gan, Israel 52900 Ramat-Gan, Israel 52900 Ramat-Gan, Israel 52900 roee.aharoni@gmail.com moishk@gmail.com yoav.goldberg@gmail.com Abstract corpus as machine-translated or not. This accuracy will be shown to decrease as the quality of the underlying MT system increases. In fact, the correlation is strong enough that we propose that this accuracy measure itself can be used as a measure of MT system quality, obviating the need for a reference corpus, as for example is necessary for BLEU (Papineni et al., 2001). The paper is structured as follows: In the next section, we review previous related work. In the third section, we describe experiments regarding the detection of machine translation and in the fourth section we discuss the use of detection techniques as a machine translation quality estimation method. In the final section we offer conclusions and suggestions for future work. We show that it is possible to automatically detect machine translated text at sentence level from monolingual corpora, using text classification methods. We show further that the accuracy with which a learned classifie"
P14-2048,N07-1051,0,0.038524,"Missing"
P14-2048,R11-1091,0,0.0866919,"Missing"
P14-2048,N03-1033,0,0.0157073,"Missing"
P14-2048,C08-1118,0,0.0615615,"Missing"
P14-2048,P02-1040,0,\N,Missing
P14-2048,W05-0909,0,\N,Missing
P14-2048,P07-2045,0,\N,Missing
P14-2048,W07-0734,0,\N,Missing
P14-2048,P13-4014,0,\N,Missing
P14-2048,W14-3302,0,\N,Missing
P14-2048,W13-2201,0,\N,Missing
P14-2048,W11-2107,0,\N,Missing
P14-2048,W13-2226,0,\N,Missing
P14-2050,N09-1003,0,0.682178,"Missing"
P14-2050,W13-3520,0,0.510599,"ques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies. Previous work on neural word embedding"
P14-2050,Q13-1033,1,0.350488,"ow but not directly related to the target word (e.g. Australian is not used as the context for discovers). In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity. Dependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This is facilitated by recent advances in parsing technology (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy. After parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m1 , . . . , mk and a head h, we consider the contexts (m1 , lbl1 ), . . . , (mk , lblk ), (h, lblh−1 ), 4 Experiments and Evaluation We experiment with 3 training conditions: B OW5 (bag-of-words contexts with k = 5), B OW2 (same, with k = 2) and D EPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to t"
P14-2050,J10-4006,0,0.0160132,"de.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D = 1|w, c) the probability that (w, c) did not. The distribution is modeled as: following work in sparse vector-space models (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees. The different kinds of contexts produce noticeably different embeddings, and induce different word similarities. In particular, the bag-ofwords nature of the contexts in the “original” S KIP G RAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature. This effect is demonstrated using both qualitative and quantitative analysis (Section 4). The neural word-embeddings are considered opaque, in the sense that it"
P14-2050,W14-1618,1,0.448246,"Missing"
P14-2050,J92-4003,0,0.109988,"hat the word “pizza” is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 1 code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, a"
P14-2050,P98-2127,0,0.0379453,"ement no. 287923 (EXCITEMENT). 1 code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D = 1|w, c) the probability that (w, c) did not. The distribution is modeled as: following work in sparse vector-space models (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees. The different kinds of contexts produce noticeably different embeddings, and induce different word similarities. In particular, the bag-ofwords nature of the contexts in the “original” S KIP G RAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature. This effect is demonstrated using both qualitative and quantitative analysis (Section 4). The neural word-embed"
P14-2050,W12-3308,1,0.490614,"Missing"
P14-2050,N13-1090,0,0.742971,"sociation between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful wo"
P14-2050,W08-1301,0,0.0130058,"Missing"
P14-2050,J07-2002,0,0.0171713,"Missing"
P14-2050,P10-1044,0,0.00942085,"Missing"
P14-2050,P10-1045,0,0.00522992,"Missing"
P14-2050,D11-1014,0,0.218583,"reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies. Previous work o"
P14-2050,N03-1033,0,0.0548877,"ny negative contexts to sample for every correct one) was 15. 2 word2vec’s implementation is slightly more complicated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. In addition, the window size is not fixed to k but is sampled uniformly in the range [1, k] for each word. 304 Target Word All embeddings were trained on English Wikipedia. For D EPS, the corpus was tagged with parts-of-speech using the Stanford tagger (Toutanova et al., 2003) and parsed into labeled Stanford dependencies (de Marneffe and Manning, 2008) using an implementation of the parser described in (Goldberg and Nivre, 2012). All tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntactic contexts. We report results for 300 dimension embeddings, though similar trends were also observed with 600 dimensions. 4.1 batman hogwarts turing florida object-oriented Qualitative Evaluation Our first evaluation is qualitative: we manua"
P14-2050,P10-1040,0,0.346783,"nality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large w"
P14-2050,P08-1086,0,0.00356561,"is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 1 code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D"
P14-2050,C98-2122,0,\N,Missing
P14-2050,C12-1059,1,\N,Missing
P16-1079,P06-1055,0,0.388607,"Missing"
P16-1079,D07-1064,0,0.495706,"internal structure in many cases (Hara et al., 2009; Hogan, 2007; Shimbo and Hara, 2007). Another issue is that PTB does not mark whether a punctuation is part of the coordination or not. This was resolved by Maier et al. (2012) which annotated punctuation in the PTB . These errors, inconsistencies, and in particular the lack of internal structural annotation turned researchers that were interested specifically in coordination disambiguation away from the PTB and towards much smaller, domain specific efforts such as the Genia Treebank (Kim et al., 2003) of biomedical texts (Hara et al., 2009; Shimbo and Hara, 2007). In addition, we also find that the PTB annotation make it hard, and often impossible, to correctly identify the elements that are being coordinated, and tell them apart from other elements that may appear in a coordination construction. While most of the coordination phrases are simple and include only conjuncts and a coordinator, many cases include additional elements with other syntactic functions , such as markers (e.g. “Both Alice and Bob”), connectives (e.g. “Fast and thus useful”) and shared elements (e.g. “Bob’s principles and opinions”) (Huddleston et al., 2002). The PTB annotations"
P16-1079,P09-1109,0,0.713958,"e errors, and lack internal structure in many cases (Hara et al., 2009; Hogan, 2007; Shimbo and Hara, 2007). Another issue is that PTB does not mark whether a punctuation is part of the coordination or not. This was resolved by Maier et al. (2012) which annotated punctuation in the PTB . These errors, inconsistencies, and in particular the lack of internal structural annotation turned researchers that were interested specifically in coordination disambiguation away from the PTB and towards much smaller, domain specific efforts such as the Genia Treebank (Kim et al., 2003) of biomedical texts (Hara et al., 2009; Shimbo and Hara, 2007). In addition, we also find that the PTB annotation make it hard, and often impossible, to correctly identify the elements that are being coordinated, and tell them apart from other elements that may appear in a coordination construction. While most of the coordination phrases are simple and include only conjuncts and a coordinator, many cases include additional elements with other syntactic functions , such as markers (e.g. “Both Alice and Bob”), connectives (e.g. “Fast and thus useful”) and shared elements (e.g. “Bob’s principles and opinions”) (Huddleston et al., 200"
P16-1079,P07-1086,0,0.076241,"Missing"
P16-1079,W12-3624,0,0.0551147,"Missing"
P16-1079,J93-2004,0,0.0551712,"Missing"
P16-1079,P07-1031,0,\N,Missing
P16-1226,W11-2501,0,0.716054,"heir path-based features. 4 Higher-dimensional embeddings seem not to improve performance, while hurting the training runtime. 2392 resource WordNet DBPedia Wikidata Yago relations instance hypernym, hypernym type subclass of, instance of subclass of random split lexical split train 49,475 20,335 test 17,670 6,610 val 3,534 1,350 all 70,679 28,295 Table 2: The number of instances in each dataset. Table 1: Hypernymy relations in each resource. 4 4.1 Dataset Creating Instances Neural networks typically require a large amount of training data, whereas the existing hypernymy datasets, like BLESS (Baroni and Lenci, 2011), are relatively small. Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c, 2012) and Yago (Suchanek et al., 2007). All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least"
P16-1226,E12-1004,0,0.566917,"2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set"
P16-1226,W09-4405,0,0.017932,"connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a bird NOUN VERB DET NOUN Figure 1: An example d"
P16-1226,C92-2082,0,0.833402,"distributional methods, the decision whether y is a hypernym of x is based on the distributional representations of these terms. Lately, with the popularity of word embeddings (Mikolov et al., 2013), most focus has shifted towards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms’ embedding vectors. In contrast to distributional methods, in which the decision is based on the separate contexts of x and y, path-based methods base the decision on the lexico-syntactic paths connecting the joint occurrences of x and y in a corpus. Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features. Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, “Spelt is a species of wheat” and “Fantasy is a genre of fiction” yield two different paths: X be species of Y and X be genre of Y, while"
P16-1226,W09-2415,0,0.0288938,"Missing"
P16-1226,C08-1051,0,0.0338745,"2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008). In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection. Inspired by recent progress 2389 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2389–2398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in relation classification, we use a long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of constructing a dataset based on k"
P16-1226,D10-1108,0,0.185523,"searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the mult"
P16-1226,N15-1098,1,0.838612,"ed hypernymy detection, it is basically designed for classifying specificity level of related terms, rather than hypernymy in particular. Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x⊕~y (Baroni et al., 2012), difference ~y − ~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al. (2015) to perform best in this setting. We perform model selection on the validation set to select the best vectors, method and regularization factor (see the appendix). 2394 Path-based Distributional Combined method Snow Snow + Gen HypeNET Path-based SLQS (Santus et al., 2014) Best supervised (concatenation) HypeNET Integrated random split precision recall 0.843 0.452 0.852 0.561 0.811 0.716 0.491 0.737 0.901 0.637 0.913 0.890 F1 0.589 0.676 0.761 0.589 0.746 0.901 lexical split precision recall 0.760 0.438 0.759 0.530 0.691 0.632 0.375 0.610 0.754 0.551 0.809 0.617 F1 0.556 0.624 0.660 0.464 0.637"
P16-1226,P06-1101,0,0.584712,"rnymy. Rather than searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it i"
P16-1226,P15-2047,0,0.0290555,"in capturing the indicative information in such paths. In particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015). While relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classification the relation should be expressed in the given text, while in hypernymy detection, the goal is to recognize a generic lexical-semantic relation between terms that holds in many contexts. Accordingly, in relation classification a term-pair is represented 3 LSTM-based Hypernymy Detection We present HypeNET, an LSTM-based method for hypernymy detection. We first focus on improving path representation"
P16-1226,P06-2075,1,0.635141,"m (Nakashole et al., 2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008). In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection. Inspired by recent progress 2389 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2389–2398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in relation classification, we use a long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of cons"
P16-1226,J06-3003,0,0.0314179,"the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a"
P16-1226,W03-1011,0,0.517909,"use of recurrent neural networks in the related task of relation classification (Section 2.3). 2.1 Distributional Methods Hypernymy detection is commonly addressed using distributional methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifi"
P16-1226,D12-1104,0,0.728597,"ir co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features. Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, “Spelt is a species of wheat” and “Fantasy is a genre of fiction” yield two different paths: X be species of Y and X be genre of Y, while both indicating that X is-a Y. A possible solution is to generalize paths by replacing words along the path with their part-of-speech tags or with wild cards, as done in the PATTY system (Nakashole et al., 2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji"
P16-1226,C14-1212,0,0.26541,"mption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations"
P16-1226,P10-1134,0,0.0891516,"the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a bird NOUN VERB DET NOUN Figure 1: An example dependency tree of the senten"
P16-1226,D15-1206,0,0.0373208,"ence, from the SemEval-2010 relation classification task dataset (Hendrickx et al., 2009): “The [apples]e1 are in the [basket]e2 ”. Here, the relation expressed between the target entities is Content − Container(e1 , e2 ). The shortest dependency paths between the target entities were shown to be informative for this task (Fundel et al., 2007). Recently, deep learning techniques showed good performance in capturing the indicative information in such paths. In particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015). While relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classifi"
P16-1226,D14-1162,0,0.0903761,"ical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.g. Y such as X, X and other Y). In a later work, Snow et al. (2004) learned to detect hypernymy. Rath"
P16-1226,N13-1008,0,0.275869,"rnymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they c"
P16-1226,E14-1054,0,0.144588,"e methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012)"
P16-1226,C14-1097,0,0.351077,"Missing"
P16-1226,E14-4008,0,0.411297,"onal methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baron"
P16-1226,K15-1018,1,0.863334,"l resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c, 2012) and Yago (Suchanek et al., 2007). All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least one of the resources. These resources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015). Term-pairs related by other relations (including hyponymy), are considered as negative instances. Using related rather than random term-pairs as negative instances tests our method’s ability to distinguish between hypernymy and other kinds of semantic relatedness. We maintain a ratio of 1:4 positive to negative pairs in the dataset. Like Snow et al. (2004), we include only termpairs that have joint occurrences in the corpus, requiring at least two different dependency paths for each pair. 4.2 Random and Lexical Dataset Splits As our primary dataset, we perform standard random splitting, with"
P16-1226,P15-1146,0,0.119629,"Missing"
P16-1226,C16-1138,0,0.0293306,"Missing"
P16-1226,P14-1113,0,\N,Missing
P16-2012,P09-1109,0,0.0465347,"Missing"
P16-2012,P07-1086,0,0.0905582,"Missing"
P16-2012,P09-2002,0,0.106243,"Missing"
P16-2012,P06-1055,0,0.497126,"ents: the NPs “John” and “Richie”; the NPs “a microphone” and “a guitar”; and the PPs “on Monday” and “on Saturday”. Previous NLP research on the Argument Clusters Coordination (Mouret, 2006) as well as the Penn TreeBank annotation guidelines (Marcus et al., 1993; Bies et al., 1995) focused mainly on providing representation schemes capable of expressing the linguistic nuances that may appear in such coordinations. The resulting representations are relatively complex, and are not easily learnable by current day parsers, including parsers that refine the grammar by learning latent annotations (Petrov et al., 2006), which are thought to be more agnostic to the annotations scheme of the trees. In this work, we suggest an alternative, simpler representation scheme which is capable of representing most of the Argument Cluster coordination cases in the Penn Treebank, and is better suited for training a parser. We show that by changing the annotation of 125 trees, we get a parser which is substantially better at handling ACC structures, and is also marginally better at parsing general sentences. Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB represent"
P16-2012,D07-1064,0,0.0418928,"Missing"
P16-2012,J93-2004,0,\N,Missing
P16-2038,P08-1076,0,0.0231001,"Missing"
P16-2038,P15-2041,0,0.0102758,"Missing"
P16-2038,P82-1020,0,0.7894,"Missing"
P16-2038,D14-1080,0,0.16738,"gging, coupled with the additional task of POS-tagging. We show that it is consistently better to have POS supervision at the innermost rather than the outermost layer. We argue that this is because “lowlevel” tasks are better kept at the lower layers, enabling the higher-level tasks to make use of the shared representation of the lower-level tasks. Finally, we also show how this architecture can be used for domain adaptation. 1 Introduction We experiment with a multi-task learning (MTL) architecture based on deep bi-directional recurrent neural networks (bi-RNNs) (Schuster and Paliwal, 1997; Irsoy and Cardie, 2014). MTL can be seen as a way of regularizing model induction by sharing representations (hidden layers) with other inductions (Caruana, 1993). We use deep bi-RNNs with task supervision from multiple tasks, sharing one or more bi-RNNs layers among the tasks. Our main contribution is the novel insight that (what has historically been thought of as) low-level tasks are better modeled in the low layers of such an architecture. This is in contrast to previous work on deep MTL (Collobert et al., 2011; Luong et al., 2015) , in which supervision for all tasks happen at the same (outermost) layer. Multip"
P16-2038,W00-0726,0,\N,Missing
P16-2038,D07-1096,0,\N,Missing
P16-2067,Q16-1023,1,0.732171,"ry loss, being predictive of word frequency, helps to differentiate the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ)"
P16-2067,D15-1041,0,0.302413,"model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM mod"
P16-2067,D15-1025,0,0.0859971,"Missing"
P16-2067,A00-1031,0,0.175325,"ank that has the canonical language name (e.g., Finnish instead of Finnish-FTB). We consider all languages that have at least 60k tokens and are distributed with word forms, resulting in 22 languages. We also report accuracies on WSJ (45 POS) using the standard splits (Collins, 2002; Manning, 2011). The overview of languages is provided in Table 1. 3.2 FINE Semitic Slavic Slavic Germanic Germanic Germanic Romance Table 1: Grouping of languages. Taggers We want to compare POS taggers under varying conditions. We hence use three different types of taggers: our implementation of a bi-LSTM; T NT (Brants, 2000)—a second order HMM with suffix trie handling for OOVs. We use T NT as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014) based on crfsuite. 3.1 COARSE non-IE Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Language isolate Indoeuropean non-IE Indoeuropean Results Our results are given in Table 2. First of all, notice that T N T performs remarkably well across the 22 languages, closely followed by CRF. The biLS"
P16-2067,D15-1085,0,0.0532663,"close to taggers using carefully designed feature templates. Ling et al. (2015) extend this line and compare a novel bi-LSTM model, learning word representations through character embeddings. They evaluate their model on a language modeling and POS tagging setup, and show that bi-LSTMs outperform the CNN approach of Santos and Zadrozny (2014). Similarly, Labeau et al. (2015) evaluate character embeddings for German. Bi-LSTMs for POS tagging are also reported in Wang et al. (2015), however, they only explore word embeddings, orthographic information and evaluate on WSJ only. A related study is Cheng et al. (2015) who propose a multi-task RNN for named entity recognition by jointly predicting the next token and current token’s name label. Our model is simpler, it uses a very coarse set of labels rather then integrating an entire language modeling task which is computationally more expensive. An interesting recent study is Gillick et al. (2016), they build a single byte-to-span model for multiple languages based on a sequence-to-sequence RNN (Sutskever et al., 2014) achieving impressive results. We would like to extend this work in their direction. Figure 3: Amount of training data (number of sentences)"
P16-2067,D15-1176,0,0.324201,"s the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that biLSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed. 1 We consider using bi-LSTMs for POS tagging. Previous work on using deep learning-based methods for POS tagging has focused either on a single language (Collobert et al., 2011; Wang et al., 2015) or a small set of languages (Ling et al., 2015; Santos and Zadrozny, 2014). Instead we evaluate our models across 22 languages. In addition, we compare performance with representations at different levels of granularity (words, characters, and bytes). These levels of representation were previously introduced in different efforts (Chrupała, 2013; Zhang et al., 2015; Ling et al., 2015; Santos and Zadrozny, 2014; Gillick et al., 2016; Kim et al., 2015), but a comparative evaluation was missing. Moreover, deep networks are often said to require large volumes of training data. We investigate to what extent bi-LSTMs are more sensitive to the am"
P16-2067,D15-1168,0,0.0430138,"e the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ). 412 Proceedings of the 54th Annual Meeting of the"
P16-2067,C14-1168,1,0.903295,"ollins, 2002; Manning, 2011). The overview of languages is provided in Table 1. 3.2 FINE Semitic Slavic Slavic Germanic Germanic Germanic Romance Table 1: Grouping of languages. Taggers We want to compare POS taggers under varying conditions. We hence use three different types of taggers: our implementation of a bi-LSTM; T NT (Brants, 2000)—a second order HMM with suffix trie handling for OOVs. We use T NT as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014) based on crfsuite. 3.1 COARSE non-IE Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Language isolate Indoeuropean non-IE Indoeuropean Results Our results are given in Table 2. First of all, notice that T N T performs remarkably well across the 22 languages, closely followed by CRF. The biLSTM tagger (w) ~ without lower-level bi-LSTM for subtokens falls short, outperforms the traditional taggers only on 3 languages. The bi-LSTM Rare words In order to evaluate the effect of modeling sub-token information, we examine accuracy rates at different frequency rates. Fig"
P16-2067,P15-1109,0,0.0284485,"are and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ). 412 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 412–418, c Berlin, Germany, August 7-12,"
P16-2067,W13-3520,0,\N,Missing
P16-2067,N16-1155,0,\N,Missing
P17-1183,N15-1107,0,0.333662,"rkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015) allows directly conditioning on the entire"
P17-1183,D08-1113,0,0.175551,"er models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015) allows directly conditioning on the entire input sequence representation, and was utilized for morphological inflection generation with great success (Kann and Sch¨utze, 2016b,a). However, the neural sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the latent variable WFST model of Dreyer et al. (2008). Interestingly, the neural WFST model by Rastogi et al. (2016) also suffered from the same issue on the CELEX dataset, and surpassed the latent variable model only when given twice as much data to train on. We propose a model which handles the above issues by directly modeling an almost monotonic 2004 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2004–2015 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1183 alignment between the input and output character sequences,"
P17-1183,N13-1138,0,0.132683,"., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015)"
P17-1183,P02-1001,0,0.414358,"translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Far"
P17-1183,N16-1077,0,0.530131,"ble weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015) allows directly conditioning on the entire input sequence representation, and was utilized for morphological inflection generation with great success (Kann and Sch¨utze, 2016b,a). However, the neural sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the lat"
P17-1183,E12-1068,0,0.0221394,") and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.). The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai e"
P17-1183,D13-1174,0,0.015034,"actic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.). The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were propose"
P17-1183,P11-1004,0,0.0240793,"the German word for “hard”) and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.). The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et"
P17-1183,N16-1102,0,0.0180349,"ral networks for monotone string transduction tasks was done by Schnober et al. (2016), showing that in many cases there is no clear advantage to one approach over the other. Regarding task-specific improvements to the attention mechanism, a line of work on attentionbased speech recognition (Chorowski et al., 2015; Bahdanau et al., 2016) proposed adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attention weight distributions. Cohn et al. (2016) offered several changes to the attention score computation to encourage wellknown modeling biases found in traditional machine translation models like word fertility, position and alignment symmetry. Regarding the utilization of independent alignment models for training attention-based networks, Mi et al. (2016) showed that the distance between the attentioninfused alignments and the ones learned by an independent alignment model can be added to the networks’ training objective, resulting in an improved translation and alignment quality. 7 Conclusion We presented a hard attention model for mo"
P17-1183,D11-1057,0,0.0763063,"translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradig"
P17-1183,E14-1060,0,0.101889,"2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015) allows directly condi"
P17-1183,N10-1103,0,0.0780773,"Missing"
P17-1183,W16-2010,0,0.0963473,"Missing"
P17-1183,P16-2090,0,0.17107,"Missing"
P17-1183,J94-3001,0,0.0890211,"in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning ov"
P17-1183,D15-1166,0,0.0832879,"ions, where the oracle is derived from a given hard alignment, the input is encoded using a biRNN, and the next action is determined by an RNN over the previous inputs and actions. 2007 3 Experiments We perform extensive experiments with three previously studied morphological inflection generation datasets to evaluate our hard attention model in various settings. In all experiments we compare our hard attention model to the best performing neural and non-neural models which were previously published on those datasets, and to our implementation of the global (soft) attention model presented by Luong et al. (2015) which we train with identical hyper-parameters as our hardattention model. The implementation details for our models are described in the supplementary material section of this paper. The source code and data for our models is available on github.2 CELEX Our first evaluation is on a very small dataset, to see if our model indeed avoids the tendency to overfit with small training sets. We report exact match accuracy on the German inflection generation dataset compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each"
P17-1183,D16-1249,0,0.0149922,"Bahdanau et al., 2016) proposed adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attention weight distributions. Cohn et al. (2016) offered several changes to the attention score computation to encourage wellknown modeling biases found in traditional machine translation models like word fertility, position and alignment symmetry. Regarding the utilization of independent alignment models for training attention-based networks, Mi et al. (2016) showed that the distance between the attentioninfused alignments and the ones learned by an independent alignment model can be added to the networks’ training objective, resulting in an improved translation and alignment quality. 7 Conclusion We presented a hard attention model for morphological inflection generation. The model employs an explicit alignment which is used to train a neural network to perform transduction by decoding with a hard attention mechanism. Our model performs better than previous neural and non-neural approaches on various morphological inflection generation datasets,"
P17-1183,P07-1017,0,0.056436,"hardest”), given a source word (e.g. “hart”, the German word for “hard”) and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.). The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett"
P17-1183,N16-1076,0,0.591818,"e to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015) allows directly conditioning on the entire input sequence representation, and was utilized for morphological inflection generation with great success (Kann and Sch¨utze, 2016b,a). However, the neural sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the latent variable WFST model of Dreyer et al. (2008). Interestingly, the neural WFST model by Rastogi et al. (2016) also suffered from the same issue on the CELEX dataset, and surpassed the latent variable model only when given twice as much data to train on. We propose a model which handles the above issues by directly modeling an almost monotonic 2004 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2004–2015 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1183 alignment between the input and output character sequences, which is commonly found in the morphological inflection generat"
P17-1183,C16-1160,0,0.0207323,"ween the encoder and the decoder. Jaitly et al. (2015) proposed the Neural Transducer model, which is also trained on external alignments. They divide the input into blocks of a constant size and perform soft attention separately on each block. Lu et al. (2016) used a combination of an RNN encoder with a CRF layer to model the dependencies in the output sequence. An interesting comparison between ”traditional” sequence transduction models (Bisani and Ney, 2008; Jiampojamarn et al., 2010; Novak et al., 2012) and encoder-decoder neural networks for monotone string transduction tasks was done by Schnober et al. (2016), showing that in many cases there is no clear advantage to one approach over the other. Regarding task-specific improvements to the attention mechanism, a line of work on attentionbased speech recognition (Chorowski et al., 2015; Bahdanau et al., 2016) proposed adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attention weight distributions. Cohn et al. (2016) offered several changes to the attention score computation to encou"
P17-1183,D13-1021,0,0.150635,"ms (Bahdanau et al., 2015; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the network training phase. Thus, our training procedure consists of three stages: learning hard alignments, deriving oracle actions from the alignments, and learning a neural transduction model given the oracle actions. Learning Hard Alignments We use the character alignment model of Sudoh et al. (2013), based on a Chinese Restaurant Process which weights single alignments (character-to-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments. The aligner implementation we used produces either 0to-1, 1-to-0 or 1-to-1 alignments. Deriving Oracle Actions We infer the sequence of actions s1:q from the alignments by the deterministic procedure described in Algorithm 1. An example of an alignment with the resulting oracle action sequence is shown in Figure 2, where a4 is a 0-to-1 alignment and the rest are 1-to-1 alignments. Figure 2: Top"
P17-1183,P08-1059,0,0.0126579,"urce word (e.g. “hart”, the German word for “hard”) and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.). The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulde"
P17-1183,P00-1027,0,0.713312,"ficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to"
P17-1183,D16-1138,0,0.226941,"ntation to be used in the next step of the decoder. The process is demonstrated visually in Figure 1. 2.4 Training the Model For every example: (x1:n , y1:m , f ) in the training data, we should produce a sequence of step and write actions s1:q to be predicted by the decoder. The sequence is dependent on the alignment between the input and the output: ideally, the network will attend to all the input characters aligned to an output character before writing it. While recent work in sequence transduction advocate jointly training the alignment and the decoding mechanisms (Bahdanau et al., 2015; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the network training phase. Thus, our training procedure consists of three stages: learning hard alignments, deriving oracle actions from the alignments, and learning a neural transduction model given the oracle actions. Learning Hard Alignments We use the character alignment model of Sudoh et al. (2013), based on a Chinese Res"
P17-1183,N15-1093,0,0.216763,"al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garc´ıa-Mart´ınez et al., 2016). The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was modeled as neural sequence-to-sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2015) allows directly conditioning on the entire input sequence represen"
P17-1183,W12-6208,0,0.0212712,". (2017) jointly learns the hard monotonic alignments and transduction while maintaining the dependency between the encoder and the decoder. Jaitly et al. (2015) proposed the Neural Transducer model, which is also trained on external alignments. They divide the input into blocks of a constant size and perform soft attention separately on each block. Lu et al. (2016) used a combination of an RNN encoder with a CRF layer to model the dependencies in the output sequence. An interesting comparison between ”traditional” sequence transduction models (Bisani and Ney, 2008; Jiampojamarn et al., 2010; Novak et al., 2012) and encoder-decoder neural networks for monotone string transduction tasks was done by Schnober et al. (2016), showing that in many cases there is no clear advantage to one approach over the other. Regarding task-specific improvements to the attention mechanism, a line of work on attentionbased speech recognition (Chorowski et al., 2015; Bahdanau et al., 2016) proposed adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attentio"
P17-2021,P17-2012,0,0.261086,"ort Papers), pages 132–140 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2021 Jane hatte eine Katze . → (ROOT (S (N P Jane )N P (V P had (N P a cat )N P )V P . )S )ROOT Figure 2: An example of a translation from a string to a linearized, lexicalized constituency tree. portion of the WMT16 news translation task (Bojar et al., 2016), with 4.5 million sentence pairs. We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al. (2017). In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.2 In each experiment we train two models. A baseline model (bpe2bpe), trained to translate from the source language sentences to English sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized constituency trees as shown in Figure 2. Words are segmented into su"
P17-2021,P06-1121,0,0.024289,"Missing"
P17-2021,N04-1035,0,0.615541,"t make explicit use of syntactic information about the languages at hand. However, a large body of work was dedicated to syntax-based SMT (Williams et al., 2016). One prominent approach to syntaxbased SMT is string-to-tree (S 2 T) translation (Yamada and Knight, 2001, 2002), in which a sourcelanguage string is translated into a target-language tree. S 2 T approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and target languages. The main formalism driving current S 2 T SMT systems is GHKM rules (Galley et al., 2004, 2006), which are Figure 1: Top - a lexicalized tree translation predicted by the bpe2tree model. Bottom - a translation for the same sentence from the bpe2bpe model. The blue lines are drawn according to the attention weights predicted by each model. Note that the linearized trees we predict are different in their structure from those in Vinyals et al. (2015) as instead of having part of speech tags as terminals, they contain the words of the translated sentence. We intentionally omit the POS informa132 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics ("
P17-2021,P05-1022,0,0.0164651,"tics https://doi.org/10.18653/v1/P17-2021 Jane hatte eine Katze . → (ROOT (S (N P Jane )N P (V P had (N P a cat )N P )V P . )S )ROOT Figure 2: An example of a translation from a string to a linearized, lexicalized constituency tree. portion of the WMT16 news translation task (Bojar et al., 2016), with 4.5 million sentence pairs. We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al. (2017). In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.2 In each experiment we train two models. A baseline model (bpe2bpe), trained to translate from the source language sentences to English sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized constituency trees as shown in Figure 2. Words are segmented into sub-word units using the BPE model we learn on the raw parallel data. We use the NEMATUS (Sennrich et al., 2017)3 implement"
P17-2021,D13-1176,0,0.0222065,"information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. Experiments on the WMT16 German-English news translation task shown improved BLEU scores when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A smallscale human evaluation also showed an advantage to the syntax-aware system. 1 Introduction and Model Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al., 2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches (Koehn, 2010). NMT models usually do not make explicit use of syntactic information about the languages at hand. However, a large body of work was dedicated to syntax-based SMT (Williams et al., 2016). One prominent approach to syntaxbased SMT is string-to-tree (S 2 T) translation (Yamada and Knight, 2001, 2002), in which a sourcelanguage"
P17-2021,P05-1033,0,0.0933006,"sing graph-convolutional networks over dependency trees; Sennrich and Haddow (2016) proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al. (2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the source sentence; Stahlberg et al. (2016) proposed a hybrid approach in which translations are scored by combining scores from an NMT system with scores from a Hiero (Chiang, 2005, 2007) system. Shi et al. (2016) explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to predict syntactic information like constituency trees, voice and tense with high accuracy. In parallel and highly related to our work, Eriguchi et al. (2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al. (2017) incorporated target syntax by predicting CCG tags serialized into the target translation. Our work differs from those by modeling syntax using constitu"
P17-2021,J10-4005,0,0.0137182,"he same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A smallscale human evaluation also showed an advantage to the syntax-aware system. 1 Introduction and Model Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al., 2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches (Koehn, 2010). NMT models usually do not make explicit use of syntactic information about the languages at hand. However, a large body of work was dedicated to syntax-based SMT (Williams et al., 2016). One prominent approach to syntaxbased SMT is string-to-tree (S 2 T) translation (Yamada and Knight, 2001, 2002), in which a sourcelanguage string is translated into a target-language tree. S 2 T approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and target languages. The main formalism driving current S 2 T S"
P17-2021,P07-2045,0,0.00485795,"y common in the “traditional” syntaxbased machine translation literature. 2 1 https://github.com/BLLIP/bllip-parser https://github.com/rsennrich/ subword-nmt 3 https://github.com/rsennrich/nematus 4 Further technical details of the setup and training are available in the supplementary material. 2 Experiments & Results Experimental Setup We first experiment in a resource-rich setting by using the German-English 133 newstest2015 27.33 27.36 28.62 28.7 newstest2016 31.19 32.13 32.38 33.24 RU-EN system bpe2bpe bpe2tree bpe2bpe ens. bpe2tree ens. DE-EN mteval-v13a.pl script from the Moses toolkit (Koehn et al., 2007). CS-EN Table 1: BLEU results for the WMT16 experiment Results As shown in Table 1, for the resource-rich setting, the single models (bpe2bpe, bpe2tree) perform similarly in terms of BLEU on newstest2015. On newstest2016 we witness an advantage to the bpe2tree model. A similar trend is found when evaluating the model ensembles: while they improve results for both models, we again see an advantage to the bpe2tree model on newstest2016. Table 2 shows the results in the low-resource setting, where the bpe2tree model is consistently better than the bpe2bpe baseline. We find this interesting as the"
P17-2021,D16-1257,0,0.0135149,"ence Department Bar-Ilan University Ramat-Gan, Israel {roee.aharoni,yoav.goldberg}@gmail.com Abstract synchronous transduction grammar (STSG) fragments, extracted from word-aligned sentence pairs with syntactic trees on one side. The GHKM translation rules allow flexible reordering on all levels of the parse-tree. We suggest that NMT can also benefit from the incorporation of syntactic knowledge, and propose a simple method of performing string-to-tree neural machine translation. Our method is inspired by recent works in syntactic parsing, which model trees as sequences (Vinyals et al., 2015; Choe and Charniak, 2016). Namely, we translate a source sentence into a linearized, lexicalized constituency tree, as demonstrated in Figure 2. Figure 1 shows a translation from our neural S 2 T model compared to one from a vanilla NMT model for the same source sentence, as well as the attention-induced word alignments of the two models. We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. Experiments on the WMT16 German-English news translation task shown improved BLEU scores wh"
P17-2021,N16-1024,0,0.0264313,"he parse-tree of the source sentence; Stahlberg et al. (2016) proposed a hybrid approach in which translations are scored by combining scores from an NMT system with scores from a Hiero (Chiang, 2005, 2007) system. Shi et al. (2016) explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to predict syntactic information like constituency trees, voice and tense with high accuracy. In parallel and highly related to our work, Eriguchi et al. (2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al. (2017) incorporated target syntax by predicting CCG tags serialized into the target translation. Our work differs from those by modeling syntax using constituency trees, as was previously common in the “traditional” syntaxbased machine translation literature. 2 1 https://github.com/BLLIP/bllip-parser https://github.com/rsennrich/ subword-nmt 3 https://github.com/rsennrich/nematus 4 Further technical details of the setup and training are available in the supplementary material. 2 Experiments & Results Experimental Setup We first experiment in a resource-rich setting by us"
P17-2021,W16-4617,0,0.0179564,"ng. We compute BLEU scores using the tion as including it would result in significantly longer sequences. The S 2 T model is trained on parallel corpora in which the target sentences are automatically parsed. Since this modeling keeps the form of a sequence-to-sequence learning task, we can employ the conventional attention-based sequence to sequence paradigm (Bahdanau et al., 2014) as-is, while enriching the output with syntactic information. Related Work Some recent works did propose to incorporate syntactic or other linguistic knowledge into NMT systems, although mainly on the source side: Eriguchi et al. (2016a,b) replace the encoder in an attention-based model with a Tree-LSTM (Tai et al., 2015) over a constituency parse tree; Bastings et al. (2017) encoded sentences using graph-convolutional networks over dependency trees; Sennrich and Haddow (2016) proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al. (2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the sou"
P17-2021,E17-3017,0,0.0131514,"Missing"
P17-2021,P01-1067,0,0.130518,"Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has recently became the state-of-the-art approach to machine translation (Bojar et al., 2016), while being much simpler than the previously dominant phrase-based statistical machine translation (SMT) approaches (Koehn, 2010). NMT models usually do not make explicit use of syntactic information about the languages at hand. However, a large body of work was dedicated to syntax-based SMT (Williams et al., 2016). One prominent approach to syntaxbased SMT is string-to-tree (S 2 T) translation (Yamada and Knight, 2001, 2002), in which a sourcelanguage string is translated into a target-language tree. S 2 T approaches to SMT help to ensure the resulting translations have valid syntactic structure, while also mediating flexible reordering between the source and target languages. The main formalism driving current S 2 T SMT systems is GHKM rules (Galley et al., 2004, 2006), which are Figure 1: Top - a lexicalized tree translation predicted by the bpe2tree model. Bottom - a translation for the same sentence from the bpe2bpe model. The blue lines are drawn according to the attention weights predicted by each mo"
P17-2021,W16-2209,0,0.0873995,"rm of a sequence-to-sequence learning task, we can employ the conventional attention-based sequence to sequence paradigm (Bahdanau et al., 2014) as-is, while enriching the output with syntactic information. Related Work Some recent works did propose to incorporate syntactic or other linguistic knowledge into NMT systems, although mainly on the source side: Eriguchi et al. (2016a,b) replace the encoder in an attention-based model with a Tree-LSTM (Tai et al., 2015) over a constituency parse tree; Bastings et al. (2017) encoded sentences using graph-convolutional networks over dependency trees; Sennrich and Haddow (2016) proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al. (2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the source sentence; Stahlberg et al. (2016) proposed a hybrid approach in which translations are scored by combining scores from an NMT system with scores from a Hiero (Chiang, 2005, 2007) system. Shi et al. (2016) explored the syntactic knowledge enco"
P17-2021,P02-1039,0,0.250756,"Missing"
P17-2021,W16-2323,0,0.0202199,")N P )V P . )S )ROOT Figure 2: An example of a translation from a string to a linearized, lexicalized constituency tree. portion of the WMT16 news translation task (Bojar et al., 2016), with 4.5 million sentence pairs. We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al. (2017). In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.2 In each experiment we train two models. A baseline model (bpe2bpe), trained to translate from the source language sentences to English sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized constituency trees as shown in Figure 2. Words are segmented into sub-word units using the BPE model we learn on the raw parallel data. We use the NEMATUS (Sennrich et al., 2017)3 implementation of an attention-based NMT model.4 We trained the models until there was no improvement on the develo"
P17-2021,P16-1162,0,0.0754721,")N P )V P . )S )ROOT Figure 2: An example of a translation from a string to a linearized, lexicalized constituency tree. portion of the WMT16 news translation task (Bojar et al., 2016), with 4.5 million sentence pairs. We then experiment in a low-resource scenario using the German, Russian and Czech to English training data from the News Commentary v8 corpus, following Eriguchi et al. (2017). In all cases we parse the English sentences into constituency trees using the BLLIP parser (Charniak and Johnson, 2005).1 To enable an open vocabulary translation we used sub-word units obtained via BPE (Sennrich et al., 2016b) on both source and target.2 In each experiment we train two models. A baseline model (bpe2bpe), trained to translate from the source language sentences to English sentences without any syntactic annotation, and a string-to-linearized-tree model (bpe2tree), trained to translate into English linearized constituency trees as shown in Figure 2. Words are segmented into sub-word units using the BPE model we learn on the raw parallel data. We use the NEMATUS (Sennrich et al., 2017)3 implementation of an attention-based NMT model.4 We trained the models until there was no improvement on the develo"
P17-2021,D16-1159,0,0.045396,"works over dependency trees; Sennrich and Haddow (2016) proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al. (2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the source sentence; Stahlberg et al. (2016) proposed a hybrid approach in which translations are scored by combining scores from an NMT system with scores from a Hiero (Chiang, 2005, 2007) system. Shi et al. (2016) explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to predict syntactic information like constituency trees, voice and tense with high accuracy. In parallel and highly related to our work, Eriguchi et al. (2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al. (2017) incorporated target syntax by predicting CCG tags serialized into the target translation. Our work differs from those by modeling syntax using constituency trees, as was previously com"
P17-2021,P16-2049,0,0.0940461,"he encoder in an attention-based model with a Tree-LSTM (Tai et al., 2015) over a constituency parse tree; Bastings et al. (2017) encoded sentences using graph-convolutional networks over dependency trees; Sennrich and Haddow (2016) proposed a factored NMT approach, where each source word embedding is concatenated to embeddings of linguistic features of the word; Luong et al. (2015) incorporated syntactic knowledge via multi-task sequence to sequence learning: their system included a single encoder with multiple decoders, one of which attempts to predict the parse-tree of the source sentence; Stahlberg et al. (2016) proposed a hybrid approach in which translations are scored by combining scores from an NMT system with scores from a Hiero (Chiang, 2005, 2007) system. Shi et al. (2016) explored the syntactic knowledge encoded by an NMT encoder, showing the encoded vector can be used to predict syntactic information like constituency trees, voice and tense with high accuracy. In parallel and highly related to our work, Eriguchi et al. (2017) proposed to model the target syntax in NMT in the form of dependency trees by using an RNNG-based decoder (Dyer et al., 2016), while Nadejde et al. (2017) incorporated"
P17-2021,P15-1150,0,0.0105441,"Missing"
P17-2021,W16-2301,0,\N,Missing
P17-2021,D17-1209,0,\N,Missing
P18-2103,D15-1075,0,0.778341,"prehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instruments (co-hyponyms). To isolate lexical knowledge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set. Introduction Recognizing textual entailment (RTE) (Dagan et al., 2013), recently framed as natural language inference (NLI) (Bowman et al., 2015) is a task concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence. Following the release of the large-scale SNLI dataset (Bowman et al., 2015), many end-to-end neural models have been developed for the task, achieving high accuracy on the test set. As opposed to previous-generation methods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate external lexical knowledge resulted in negligible performance gain (Chen et al., 2018). This raises the questio"
P18-2103,P16-2022,0,0.0330781,"ta needed to learn the required lexical knowledge, it may be available in the other datasets, which are presumably richer. We chose models which are amongst the best performing within their approaches (excluding ensembles) and have available code. All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either fine-tuned during training (R ESIDUAL -S TACKED -E NCODER and ESIM) or stay fixed (D ECOMPOSABLE ATTENTION ). All models predict the label using a concatenation of features derived from the sentence representations (e.g. maximum, mean), for example as in Mou et al. (2016). We use the recommended hyper-parameters for each model, as they appear in the provided code. 4.3 Table 3 displays the results for all the models on the original SNLI test set and the new test set. Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points in accuracy. Adding MultiNLI to the training data somewhat mitigates this drop in accuracy, thanks to almost doubling the amount of training data. We note that adding SciTail to the training data did not similarly improve the performance; we conjecture that this stems from the differenc"
P18-2103,P16-1139,0,0.042219,"Missing"
P18-2103,W17-5308,0,0.0651015,"Missing"
P18-2103,D16-1244,0,0.287562,"Missing"
P18-2103,P18-1224,0,0.434231,"guage inference (NLI) (Bowman et al., 2015) is a task concerned with identifying whether a premise sentence entails, contradicts or is neutral with the hypothesis sentence. Following the release of the large-scale SNLI dataset (Bowman et al., 2015), many end-to-end neural models have been developed for the task, achieving high accuracy on the test set. As opposed to previous-generation methods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate external lexical knowledge resulted in negligible performance gain (Chen et al., 2018). This raises the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge. The performance on the new test set is substantially worse across systems, demonstrating that the SNLI test set alone is not a sufficient measure of language understanding capabilities. Our results are in line with Gururangan et al. (2018) and Poliak et al. (2018), who showed that t"
P18-2103,P17-1152,0,0.223659,"s (see Table 1). The generation steps are detailed below. Neural Approaches for NLI. Following the release of SNLI, there has been tremendous interest in the task, and many end-to-end neural models were developed, achieving promising results.2 Methods are divided into two main approaches. Sentence-encoding models (e.g. Bowman et al., 2015, 2016; Nie and Bansal, 2017; Shen et al., 2018) encode the premise and hypothesis individually, while attention-based models align words in the premise with similar words in the hypothesis, encoding the two sentences together (e.g. Rockt¨aschel et al., 2016; Chen et al., 2017). Replacement Words. We collected the replacement words using online resources for English learning.3 The newly introduced words are all present in the SNLI training set: from occurrence in a single training example (“Portugal”) up to 248,051 examples (“man”), with a mean of 3,663.1 and a median of 149.5. The words are also available in the pre-trained embeddings vocabulary. The goal of this constraint is to isolate lexical knowledge aspects, and evaluate the models’ ability to generalize and make new inferences for known words. 2 See the SNLI leaderboard for a comprehensive list: https://nlp."
P18-2103,D14-1162,0,0.0842781,"dels trained on SNLI or a union of SNLI with another dataset (MultiNLI, SciTail), and tested on the original SNLI test set and the new test set. and the MultiNLI train set, and (3) a union of the SNLI train set and the SciTail train set. The motivation is that while SNLI might lack the training data needed to learn the required lexical knowledge, it may be available in the other datasets, which are presumably richer. We chose models which are amongst the best performing within their approaches (excluding ensembles) and have available code. All models are based on pre-trained GloVe embeddings (Pennington et al., 2014), which are either fine-tuned during training (R ESIDUAL -S TACKED -E NCODER and ESIM) or stay fixed (D ECOMPOSABLE ATTENTION ). All models predict the label using a concatenation of features derived from the sentence representations (e.g. maximum, mean), for example as in Mou et al. (2016). We use the recommended hyper-parameters for each model, as they appear in the provided code. 4.3 Table 3 displays the results for all the models on the original SNLI test set and the new test set. Despite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 poi"
P18-2103,S18-2023,0,0.212936,"Missing"
P18-2103,N18-2017,0,0.292743,"nowledge resulted in negligible performance gain (Chen et al., 2018). This raises the question whether (1) neural methods are inherently stronger, obviating the need of external lexical knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge. The performance on the new test set is substantially worse across systems, demonstrating that the SNLI test set alone is not a sufficient measure of language understanding capabilities. Our results are in line with Gururangan et al. (2018) and Poliak et al. (2018), who showed that the label can be identified by looking only at the hypothesis and exploiting annotation artifacts such as word choice and sentence length. Further investigation shows that what mostly affects the systems’ ability to correctly predict a test example is the amount of similar examples found in the training set. Given that training data will always be limited, this is a rather inefficient way to learn lexical inferences, stressing the need to develop methods that do this more 1 The contradiction example follows the assumption in Bowman et al. (2015) that"
P18-2103,N18-1101,0,0.180361,"e the relation between sentences given that they describe the same event. Hence, sentences that differ by a single mutually-exclusive term should be considered contradicting, as in “The president visited Alabama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral. Following criticism on the simplicity of the dataset, stemming mostly from its narrow domain, two additional datasets have been collected. The MultiNLI dataset (Multi-Genre Natural Language Inference, Williams et al., 2018) was collected similarly to SNLI, though covering a wider range of genres, and supporting a cross-genre evaluation. The SciTail dataset (Khot et al., 2018), created from science exams, is somewhat different from the two datasets, being smaller (27,026 examples), and labeled only as entailment or neutral. The domain makes this dataset different in nature from the other two datasets, and it consists of more factual sentences rather than scene descriptions. 3 Data Collection We construct a test set with the goal of evaluating the ability of state-of-the-art NLI models to make inferences that requ"
P18-2103,D17-1215,0,0.122855,"e on the new test set is substantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences. 1 A little girl is very sad. A little girl is very unhappy. A couple drinking wine A couple drinking champagne Label contradiction1 entailment neutral Table 1: Examples from the new test set. In this paper we show that state-of-the-art NLI systems are limited in their generalization ability, and fail to capture many simple inferences that require lexical and world knowledge. Inspired by the work of Jia and Liang (2017) on reading comprehension, we create a new NLI test set with examples that capture various kinds of lexical knowledge (Table 1). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instruments (co-hyponyms). To isolate lexical knowledge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set. Introduction Recognizing textual entailment (RTE) (Dagan et al., 2013), recently framed as natural language in"
P18-2103,Q14-1006,0,0.040736,"test set. This raises the question whether the small performance gap is a result of the model not capturing lexical knowledge well, or the SNLI test set not requiring this knowledge in the first place. effectively. Our test set can be used to evaluate such models’ ability to recognize lexical inferences, and it is available at https://github. com/BIU-NLP/Breaking_NLI. 2 Background NLI Datasets. The SNLI dataset (Stanford Natural Language Inference, Bowman et al., 2015) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Young et al. (2014), while hypotheses were generated by crowd-sourced workers who were shown a premise and asked to generate entailing, contradicting, and neutral sentences. Workers were instructed to judge the relation between sentences given that they describe the same event. Hence, sentences that differ by a single mutually-exclusive term should be considered contradicting, as in “The president visited Alabama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral. Following criti"
P18-2114,D17-1064,0,0.218824,"d a Stronger Baseline Roee Aharoni & Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel {roee.aharoni,yoav.goldberg}@gmail.com Abstract ing RDF information, and a semantics-augmented setup that does. They report a BLEU score of 48.9 for their best text-to-text system, and of 78.7 for the best RDF-aware one. We focus on the text-totext setup, which we find to be more challenging and more natural. We begin with vanilla S EQ 2S EQ models with attention (Bahdanau et al., 2015) and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of Narayan et al. (2017) and approaching their best RDF-aware method. However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about relevant entities, but these facts were not mentioned in the input sentence; (2) some facts are repeated—the same fact is mentioned in multiple output sentences; and (3) some facts are missing— mentioned in the input but omitted in the output. The model learned to memorize entity-fact pairs instead of learning to split and rephrase. Indeed, feeding the model with exa"
P18-2114,W14-4009,0,0.060659,"Missing"
P18-2114,C96-2183,0,0.428279,"Missing"
P18-2114,P17-1099,0,0.0387308,"55 83.73 87.45 55.66 – 5.55 5.28 6.68 16.71 23.78 24.97 56.1 – 25.47 #S/C 1.0 2.52 2.84 2.53 2.57 2.59 2.51 2.49 2.56 1.0 2.40 2.27 2.27 2.44 2.0 2.38 2.87 1.0 2.48 2.29 #T/S 21.11 10.93 9.28 10.53 10.56 10.59 10.29 10.66 10.50 20.37 10.83 11.68 10.54 10.23 10.53 10.55 10.04 20.4 10.69 11.74 Table 5: Results over the test sets of the original, our proposed split and the v1.0 split tions are computed, the final probability for an output word w is: p(w) = p(z = 1)pcopy (w) + p(z = 0)psof tmax (w) In case w is not present in the output vocabulary, we set psof tmax (w) = 0. We refer the reader to See et al. (2017) for a detailed discussion regarding the copy mechanism. Copy-augmented Model 5 To better suit the split-and-rephrase task, we augment the S EQ 2S EQ models with a copy mechanism. Such mechanisms have proven to be beneficial in similar tasks like abstractive summarization (Gu et al., 2016; See et al., 2017) and language modeling (Merity et al., 2017). We hypothesize that biasing the model towards copying will improve performance, as many of the words in the simple sentences (mostly corresponding to entities) appear in the complex sentence, as evident by the relatively high BLEU scores for the"
P18-2114,P16-1154,0,0.0388052,"4 Table 5: Results over the test sets of the original, our proposed split and the v1.0 split tions are computed, the final probability for an output word w is: p(w) = p(z = 1)pcopy (w) + p(z = 0)psof tmax (w) In case w is not present in the output vocabulary, we set psof tmax (w) = 0. We refer the reader to See et al. (2017) for a detailed discussion regarding the copy mechanism. Copy-augmented Model 5 To better suit the split-and-rephrase task, we augment the S EQ 2S EQ models with a copy mechanism. Such mechanisms have proven to be beneficial in similar tasks like abstractive summarization (Gu et al., 2016; See et al., 2017) and language modeling (Merity et al., 2017). We hypothesize that biasing the model towards copying will improve performance, as many of the words in the simple sentences (mostly corresponding to entities) appear in the complex sentence, as evident by the relatively high BLEU scores for the S OURCE baseline in Table 2. Copying is modeled using a “copy switch” probability p(z) computed by a sigmoid over a learned composition of the decoder state, the context vector and the last output embedding. It interpolates the psof tmax distribution over the target vocabulary and a copy"
P18-2114,W03-1602,0,0.236634,"Missing"
P18-2114,jelinek-2014-improvements,0,0.024084,"Missing"
P18-2114,P17-4012,0,0.0612232,"and features a train/test split protocol which is similar to our proposal. We report results on this dataset as well. The code and data to reproduce our results are available on Github.1 We encourage future work on the split-and-rephrase task to use our new data split or the v1.0 split instead of the original one. 2 BLEU 55.67 – sentences3 and report the average number of simple sentences in each prediction, and the average number of tokens for each simple sentence. We train vanilla sequence-to-sequence models with attention (Bahdanau et al., 2015) as implemented in the O PENNMT- PY toolkit (Klein et al., 2017).4 Our models only differ in the LSTM cell size (128, 256 and 512, respectively). See the supplementary material for training details and hyperparameters. We compare our models to the baselines proposed in Narayan et al. (2017). H YBRID S IMPL and S EQ 2S EQ are text-to-text models, while the other reported baselines additionally use the RDF information. Preliminary Experiments Task Definition In the split-and-rephrase task we are given a complex sentence C, and need to produce a sequence of simple sentences T1 , ..., Tn , n ≥ 2, such that the output sentences convey all and only the informati"
P18-2114,W17-3204,0,0.0296142,"the validation and test sets. To aid this, we present a new train-development-test data split and neural models augmented with a copymechanism, outperforming the best reported baseline by 8.68 BLEU and fostering further progress on the task. 1 Introduction Processing long, complex sentences is challenging. This is true either for humans in various circumstances (Inui et al., 2003; Watanabe et al., 2009; De Belder and Moens, 2010) or in NLP tasks like parsing (Tomita, 1986; McDonald and Nivre, 2011; Jel´ınek, 2014) and machine translation (Chandrasekar et al., 1996; Pouget-Abadie et al., 2014; Koehn and Knowles, 2017). An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing. A recent work by Narayan et al. (2017) introduced a dataset, evaluation method and baseline systems for the task, naming it “Split-andRephrase”. The dataset includes 1,066,115 instances mapping a single complex sentence to a sequence of sentences that express the same meaning, together with RDF triples that describe their semantics. They considered two system setups: a text-to-text setup that does not use the accompany719 Proceedings of the 56th Annual Meet"
P18-2114,J11-1007,0,0.0306372,"Missing"
P18-2117,P17-2021,1,0.77808,"N N (x1 , ..., xn ) denote the state vector h resulting from the application of R to the sequence E(x1 ), ..., E(xn ). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L⊆ Σ∗ if f (RN N (w)) returns 1 for all and only words w = x1 , ..., xn ∈ L. 1 Is the ability to perform unbounded counting relevant to “real world” NLP tasks? In some cases it might be. For example, processing linearized parse trees (Vinyals et al., 2015; Choe and Charniak, 2016; Aharoni and Goldberg, 2017) requires counting brackets and nesting levels. Indeed, previous works that process linearized parse trees report using LSTMs Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), and not GRUs for this purpose. Our work here suggests that this may not be a coincidence. 741 3 the function R takes the form of an affine transform followed by a tanh nonlinearity: ht = tanh(W xt + U ht−1 + b) Power beyond finite state can be obtained by introducing counters. Counting languages and kcounter machines are discussed in depth in (Fischer et al., 1968). When unbounded comput"
P18-2117,N18-1205,0,0.062652,"Missing"
P18-2117,D16-1257,0,0.0168252,"some other means). Let RN N (x1 , ..., xn ) denote the state vector h resulting from the application of R to the sequence E(x1 ), ..., E(xn ). An RNN recognizer (or RNN acceptor) has an additional function f mapping states h to 0, 1. Typically, f is a log-linear classifier or multi-layer perceptron. We say that an RNN recognizes a language L⊆ Σ∗ if f (RN N (w)) returns 1 for all and only words w = x1 , ..., xn ∈ L. 1 Is the ability to perform unbounded counting relevant to “real world” NLP tasks? In some cases it might be. For example, processing linearized parse trees (Vinyals et al., 2015; Choe and Charniak, 2016; Aharoni and Goldberg, 2017) requires counting brackets and nesting levels. Indeed, previous works that process linearized parse trees report using LSTMs Elman-RNN (SRNN) In the Elman-RNN (Elman, 1990), also called the Simple RNN (SRNN), and not GRUs for this purpose. Our work here suggests that this may not be a coincidence. 741 3 the function R takes the form of an affine transform followed by a tanh nonlinearity: ht = tanh(W xt + U ht−1 + b) Power beyond finite state can be obtained by introducing counters. Counting languages and kcounter machines are discussed in depth in (Fischer et al.,"
Q13-1033,P11-2121,0,0.0181593,"78 79.25 Table 1: Results on the CoNLL 2007 data set. UAS, including punctuation. Each number is an average over 5 runs with different randomization seeds. All experiments used the same exploration parameters of k=1, p=0.9. 6 Related Work The error propagation problem for greedy transitionbased parsing was diagnosed by McDonald and Nivre (2007) and has been tackled with a variety of techniques including parser stacking (Nivre and McDonald, 2008; Martins et al., 2008) and beam search and structured prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). The technique called bootstrapping in Choi and Palmer (2011) is similar in spirit to training with exploration but is applied iteratively in batch mode and is only approximate due to the use of static oracles. Dynamic oracles were first explored by Goldberg and Nivre (2012). In machine learning more generally, our approach can be seen as a problem-specific instance of imitation learning (Abbeel and Ng, 2004; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011), where the dynamic oracle is used to implement the optimal expert needed in the imitation learning setup. Indeed, our training procedure is closely related to DAgger (Ross"
Q13-1033,N10-1115,1,0.92174,"0/2013. 2013 Association for Computational Linguistics. In this paper, we extend the work of Goldberg and Nivre (2012) by giving a general characterization of dynamic oracles as oracles that are nondeterministic, in that they return sets of transitions, and complete, in that they are defined for all possible states. We then define a formal property of transition systems which we call arc decomposition, and introduce a framework for deriving dynamic oracles for arc-decomposable systems. Using this framework, we derive novel dynamic oracles for the hybrid (Kuhlmann et al., 2011) and easy-first (Goldberg and Elhadad, 2010) transition systems, which are arc-decomposable (as is the arc-eager system). We also show that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and so deriving a dynamic oracle for it remains an open research question. Finally, we perform a set of experiments on the CoNLL 2007 data sets, validating that the use of dynamic oracles for exploring states that result from parsing mistakes during training is beneficial across transition systems. 2 Transition-Based Dependency Parsing We begin with a quick review of transition-based dependency parsing, presenting the arc-eager,"
Q13-1033,C12-1059,1,0.160773,", without considering any parser state outside this sequence. Thus, once the parser strays from the golden path at test time, it ventures into unknown territory and is forced to react to situations it has never been trained for. Introduction Greedy transition-based parsers are easy to implement and are very efficient, but they are generally not as accurate as parsers that are based on global search (McDonald et al., 2005; Koo and Collins, 2010) or as transition-based parsers that use beam search (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010; Kuhlmann et In recent work (Goldberg and Nivre, 2012), we introduced the concept of a dynamic oracle, which is non-deterministic and not restricted to a single golden path, but instead provides optimal predictions for any possible state the parser might be in. Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree. Moreover, they are welldefined and optimal also for states from which the gold tree cannot be derived, in the sense that they return the set of transitions leading to the best tree derivable from each state. We showed experimentally that, using a dynamic or"
Q13-1033,P10-1110,0,0.178213,"ned to predict transitions along this gold sequence, without considering any parser state outside this sequence. Thus, once the parser strays from the golden path at test time, it ventures into unknown territory and is forced to react to situations it has never been trained for. Introduction Greedy transition-based parsers are easy to implement and are very efficient, but they are generally not as accurate as parsers that are based on global search (McDonald et al., 2005; Koo and Collins, 2010) or as transition-based parsers that use beam search (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010; Kuhlmann et In recent work (Goldberg and Nivre, 2012), we introduced the concept of a dynamic oracle, which is non-deterministic and not restricted to a single golden path, but instead provides optimal predictions for any possible state the parser might be in. Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree. Moreover, they are welldefined and optimal also for states from which the gold tree cannot be derived, in the sense that they return the set of transitions leading to the best tree derivable from each s"
Q13-1033,P10-1001,0,0.0488287,"y parsers are normally trained. Given a treebank oracle, a gold sequence of transitions is derived, and a predictor is trained to predict transitions along this gold sequence, without considering any parser state outside this sequence. Thus, once the parser strays from the golden path at test time, it ventures into unknown territory and is forced to react to situations it has never been trained for. Introduction Greedy transition-based parsers are easy to implement and are very efficient, but they are generally not as accurate as parsers that are based on global search (McDonald et al., 2005; Koo and Collins, 2010) or as transition-based parsers that use beam search (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010; Kuhlmann et In recent work (Goldberg and Nivre, 2012), we introduced the concept of a dynamic oracle, which is non-deterministic and not restricted to a single golden path, but instead provides optimal predictions for any possible state the parser might be in. Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree. Moreover, they are welldefined and optimal also for states from which the gold t"
Q13-1033,P11-1068,0,0.524194,"Missing"
Q13-1033,D08-1017,0,0.023359,"70 84.40 84.30 83.43 83.83 85.31 85.11 86.02 86.93 85.57 85.47 84.93 87.69 84.96 86.28 66.59 67.05 64.80 66.12 72.10 73.92 73.16 74.10 80.17 80.43 78.78 79.25 Table 1: Results on the CoNLL 2007 data set. UAS, including punctuation. Each number is an average over 5 runs with different randomization seeds. All experiments used the same exploration parameters of k=1, p=0.9. 6 Related Work The error propagation problem for greedy transitionbased parsing was diagnosed by McDonald and Nivre (2007) and has been tackled with a variety of techniques including parser stacking (Nivre and McDonald, 2008; Martins et al., 2008) and beam search and structured prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). The technique called bootstrapping in Choi and Palmer (2011) is similar in spirit to training with exploration but is applied iteratively in batch mode and is only approximate due to the use of static oracles. Dynamic oracles were first explored by Goldberg and Nivre (2012). In machine learning more generally, our approach can be seen as a problem-specific instance of imitation learning (Abbeel and Ng, 2004; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011), where the dynamic or"
Q13-1033,D07-1013,1,0.845191,"91.30 91.06 92.50 92.85 86.10 88.69 86.43 87.62 88.57 89.41 77.38 77.39 75.91 76.90 78.92 79.29 81.59 83.62 83.43 84.04 82.73 83.70 84.40 84.30 83.43 83.83 85.31 85.11 86.02 86.93 85.57 85.47 84.93 87.69 84.96 86.28 66.59 67.05 64.80 66.12 72.10 73.92 73.16 74.10 80.17 80.43 78.78 79.25 Table 1: Results on the CoNLL 2007 data set. UAS, including punctuation. Each number is an average over 5 runs with different randomization seeds. All experiments used the same exploration parameters of k=1, p=0.9. 6 Related Work The error propagation problem for greedy transitionbased parsing was diagnosed by McDonald and Nivre (2007) and has been tackled with a variety of techniques including parser stacking (Nivre and McDonald, 2008; Martins et al., 2008) and beam search and structured prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). The technique called bootstrapping in Choi and Palmer (2011) is similar in spirit to training with exploration but is applied iteratively in batch mode and is only approximate due to the use of static oracles. Dynamic oracles were first explored by Goldberg and Nivre (2012). In machine learning more generally, our approach can be seen as a problem-specific instance of imitation lea"
Q13-1033,P05-1012,0,0.114542,"the way in which greedy parsers are normally trained. Given a treebank oracle, a gold sequence of transitions is derived, and a predictor is trained to predict transitions along this gold sequence, without considering any parser state outside this sequence. Thus, once the parser strays from the golden path at test time, it ventures into unknown territory and is forced to react to situations it has never been trained for. Introduction Greedy transition-based parsers are easy to implement and are very efficient, but they are generally not as accurate as parsers that are based on global search (McDonald et al., 2005; Koo and Collins, 2010) or as transition-based parsers that use beam search (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010; Kuhlmann et In recent work (Goldberg and Nivre, 2012), we introduced the concept of a dynamic oracle, which is non-deterministic and not restricted to a single golden path, but instead provides optimal predictions for any possible state the parser might be in. Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree. Moreover, they are welldefined and optimal also for stat"
Q13-1033,P08-1108,1,0.23282,"3.62 83.43 84.04 82.73 83.70 84.40 84.30 83.43 83.83 85.31 85.11 86.02 86.93 85.57 85.47 84.93 87.69 84.96 86.28 66.59 67.05 64.80 66.12 72.10 73.92 73.16 74.10 80.17 80.43 78.78 79.25 Table 1: Results on the CoNLL 2007 data set. UAS, including punctuation. Each number is an average over 5 runs with different randomization seeds. All experiments used the same exploration parameters of k=1, p=0.9. 6 Related Work The error propagation problem for greedy transitionbased parsing was diagnosed by McDonald and Nivre (2007) and has been tackled with a variety of techniques including parser stacking (Nivre and McDonald, 2008; Martins et al., 2008) and beam search and structured prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). The technique called bootstrapping in Choi and Palmer (2011) is similar in spirit to training with exploration but is applied iteratively in batch mode and is only approximate due to the use of static oracles. Dynamic oracles were first explored by Goldberg and Nivre (2012). In machine learning more generally, our approach can be seen as a problem-specific instance of imitation learning (Abbeel and Ng, 2004; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011"
Q13-1033,W03-3017,1,0.927277,"racle, which is non-deterministic and not restricted to a single golden path, but instead provides optimal predictions for any possible state the parser might be in. Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree. Moreover, they are welldefined and optimal also for states from which the gold tree cannot be derived, in the sense that they return the set of transitions leading to the best tree derivable from each state. We showed experimentally that, using a dynamic oracle for the arc-eager transition system (Nivre, 2003), a greedy parser can be trained to perform well also after incurring a mistake, thus alleviating the effect of error propagation and resulting in consistently better parsing accuracy. 403 Transactions of the Association for Computational Linguistics, 1 (2013) 403–414. Action Editor: Jason Eisner. c Submitted 6/2013; Published 10/2013. 2013 Association for Computational Linguistics. In this paper, we extend the work of Goldberg and Nivre (2012) by giving a general characterization of dynamic oracles as oracles that are nondeterministic, in that they return sets of transitions, and complete, in"
Q13-1033,W04-0308,1,0.443746,"of dynamic oracles as oracles that are nondeterministic, in that they return sets of transitions, and complete, in that they are defined for all possible states. We then define a formal property of transition systems which we call arc decomposition, and introduce a framework for deriving dynamic oracles for arc-decomposable systems. Using this framework, we derive novel dynamic oracles for the hybrid (Kuhlmann et al., 2011) and easy-first (Goldberg and Elhadad, 2010) transition systems, which are arc-decomposable (as is the arc-eager system). We also show that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and so deriving a dynamic oracle for it remains an open research question. Finally, we perform a set of experiments on the CoNLL 2007 data sets, validating that the use of dynamic oracles for exploring states that result from parsing mistakes during training is beneficial across transition systems. 2 Transition-Based Dependency Parsing We begin with a quick review of transition-based dependency parsing, presenting the arc-eager, arcstandard, hybrid and easy-first transitions systems in a common notation. The transition-based parsing framework (Nivre, 2008) assumes a t"
Q13-1033,J08-4003,1,0.655058,"ard system (Nivre, 2004) is not arc-decomposable, and so deriving a dynamic oracle for it remains an open research question. Finally, we perform a set of experiments on the CoNLL 2007 data sets, validating that the use of dynamic oracles for exploring states that result from parsing mistakes during training is beneficial across transition systems. 2 Transition-Based Dependency Parsing We begin with a quick review of transition-based dependency parsing, presenting the arc-eager, arcstandard, hybrid and easy-first transitions systems in a common notation. The transition-based parsing framework (Nivre, 2008) assumes a transition system, an abstract machine that processes sentences and produces parse trees. The transition system has a set of configurations and a set of transitions which are applied to configurations. When parsing a sentence, the system is initialized to an initial configuration based on the input sentence, and transitions are repeatedly applied to this configuration. After a finite number of transitions, the system arrives at a terminal configuration, and a parse tree is read off the terminal configuration. In a greedy parser, a classifier is used to choose the transition to take"
Q13-1033,D08-1059,0,0.607499,"transitions is derived, and a predictor is trained to predict transitions along this gold sequence, without considering any parser state outside this sequence. Thus, once the parser strays from the golden path at test time, it ventures into unknown territory and is forced to react to situations it has never been trained for. Introduction Greedy transition-based parsers are easy to implement and are very efficient, but they are generally not as accurate as parsers that are based on global search (McDonald et al., 2005; Koo and Collins, 2010) or as transition-based parsers that use beam search (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010; Kuhlmann et In recent work (Goldberg and Nivre, 2012), we introduced the concept of a dynamic oracle, which is non-deterministic and not restricted to a single golden path, but instead provides optimal predictions for any possible state the parser might be in. Dynamic oracles are non-deterministic in the sense that they return a set of valid transitions for a given parser state and gold tree. Moreover, they are welldefined and optimal also for states from which the gold tree cannot be derived, in the sense that they return the set of transitions"
Q13-1033,P11-2033,1,0.368327,"6.59 67.05 64.80 66.12 72.10 73.92 73.16 74.10 80.17 80.43 78.78 79.25 Table 1: Results on the CoNLL 2007 data set. UAS, including punctuation. Each number is an average over 5 runs with different randomization seeds. All experiments used the same exploration parameters of k=1, p=0.9. 6 Related Work The error propagation problem for greedy transitionbased parsing was diagnosed by McDonald and Nivre (2007) and has been tackled with a variety of techniques including parser stacking (Nivre and McDonald, 2008; Martins et al., 2008) and beam search and structured prediction (Zhang and Clark, 2008; Zhang and Nivre, 2011). The technique called bootstrapping in Choi and Palmer (2011) is similar in spirit to training with exploration but is applied iteratively in batch mode and is only approximate due to the use of static oracles. Dynamic oracles were first explored by Goldberg and Nivre (2012). In machine learning more generally, our approach can be seen as a problem-specific instance of imitation learning (Abbeel and Ng, 2004; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011), where the dynamic oracle is used to implement the optimal expert needed in the imitation learning setup. Inde"
Q14-1010,de-marneffe-etal-2006-generating,0,0.153213,"Missing"
Q14-1010,C12-1059,1,0.845131,"ing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it returns a single canonical transition. Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. Oracles that are both deterministic and partial are called static. Traditionally, only static oracles have been exploited in training of transition-based parsers. Recently, Goldberg and Nivre (2012; 2013) showed that the accuracy of greedy parsers can be substantially improved without affecting their parsing speed. This improvement relies on the introduction of novel oracles that are nondeterministic and complete. An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser. Oracles that are both nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based"
Q14-1010,Q13-1033,1,0.768824,"ed in training of transition-based parsers. Recently, Goldberg and Nivre (2012; 2013) showed that the accuracy of greedy parsers can be substantially improved without affecting their parsing speed. This improvement relies on the introduction of novel oracles that are nondeterministic and complete. An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser. Oracles that are both nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers. The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition. They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system. In this article, we develop one such oracle (§4) and prove its correctness (§5). An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and all"
Q14-1010,P10-1110,0,0.0878832,"cles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages. 1 Introduction Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right. These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007). However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it returns a single canonical transition. Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. Oracles that are both deterministic and partial are called static. T"
Q14-1010,P11-1068,1,0.722931,"Missing"
Q14-1010,J93-2004,0,0.0526145,"of spurious ambiguities. The explore setup increases the configuration space explored by the parser during training, by exposing the training procedure to non-optimal configurations that are likely to occur during parsing, together with the optimal transitions to take in these configurations. It was shown by Goldberg and Nivre (2012; 2013) that the nondet setup outperforms the static setup, and that the explore setup outperforms the nondet setup. 8 Experimental Evaluation Datasets Performance evaluation is carried out on CoNLL 2007 multilingual dataset, as well as on the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use gold part-of-speech tags, while for the PTB we use automatically assigned tags. As usual, the PTB parser is trained on sections 2-21 and tested on section 23. Setup We train labeled versions of the arc-standard (std) and LR-spine (lrs) parsers under the static, nondet and explore setups, as defined in §7. In the nondet setup we use a nondeterministic partial oracle and in the explore setup we use the nondeterministic complete oracles we present in this paper. In the static setup we resolve oracl"
Q14-1010,W03-3017,0,0.245663,"i+1 · · · wj of w. We write i → j to denote a grammatical dependency of some unspecified type between lexical tokens wi and wj , where wi is the head and wj is the dependent. A dependency tree for w is a directed, ordered tree t = (Vw , A), such that Vw = [0, n] is the set of nodes, A ⊆ Vw × Vw is the set of arcs, and node 0 is the root. Arc (i, j) encodes a dependency i → j, and we will often use the latter notation to denote arcs. 2.2 Transition-Based Dependency Parsing We assume the reader is familiar with the formal framework of transition-based dependency parsing originally introduced by Nivre (2003); see Nivre (2008) for an introduction. We only summarize here our notation. Transition-based dependency parsers use a stack data structure, where each stack element is associated with a tree spanning (generating) some substring of the input w. The parser processes the input string incrementally, from left to right, applying at each step a transition that updates the stack and/or 120 consumes one token from the input. Transitions may also construct new dependencies, which are added to the current configuration of the parser. We represent the stack data structure as an ordered sequence σ = [σd"
Q14-1010,W04-0308,0,0.531753,"at are nondeterministic and complete. An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser. Oracles that are both nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers. The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition. They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system. In this article, we develop one such oracle (§4) and prove its correctness (§5). An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and allows mixing of bottom-up and top-down strategies. This parser, called here the LR-spine parser, achieves state-ofthe-art results for greedy parsing. Like the arc-standard system, the LR-spine parser is not arc-decomposable, and a dynamic oracle for"
Q14-1010,J08-4003,0,0.335047,"ty, Israel Francesco Sartorio Department of Information Engineering University of Padua, Italy Giorgio Satta Department of Information Engineering University of Padua, Italy yoav.goldberg@gmail.com sartorio@dei.unipd.it satta@dei.unipd.it Abstract We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages. 1 Introduction Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right. These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007). However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it retu"
Q14-1010,P13-1014,1,0.802769,"th nondeterministic and complete are called dynamic. Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers. The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition. They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system. In this article, we develop one such oracle (§4) and prove its correctness (§5). An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and allows mixing of bottom-up and top-down strategies. This parser, called here the LR-spine parser, achieves state-ofthe-art results for greedy parsing. Like the arc-standard system, the LR-spine parser is not arc-decomposable, and a dynamic oracle for this system was not known. We extend our oracle for the arc-standard system to work for the LR-spine system as well (§6). The dynamic oracles developed by Goldberg and Nivre (2013) for arc-decomposable systems are based on local properties of computations. In contrast, our novel dynamic oracle a"
Q14-1010,D08-1059,0,0.057594,"We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013). We experimentally show that using these oracles during training yields superior parsing accuracies on many languages. 1 Introduction Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right. These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007). However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior. A parsing oracle is deterministic if it returns a single canonical transition. Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake. Oracles that are both deterministic and part"
Q14-1010,D07-1096,0,\N,Missing
Q15-1016,N09-1003,0,0.472274,"Missing"
Q15-1016,J10-4006,0,0.0160512,"are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, th"
Q15-1016,P14-1023,0,0.9474,"to a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’"
Q15-1016,P12-1015,0,0.291036,"Missing"
Q15-1016,J90-1003,0,0.216701,"exts have been studied (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014a) this work focuses on fixed-window bag-of-words contexts. 2.1 Explicit Representations (PPMI Matrix) The traditional way to represent words in the distributional approach is to construct a highdimensional sparse matrix M , where each row represents a word w in the vocabulary VW and each column a potential context c ∈ VC . The value of each matrix cell Mij represents the association between the word wi and the context cj . A popular measure of this association is pointwise mutual information (PMI) (Church and Hanks, 1990). PMI is defined as the log ratio between w and c’s joint probability and the product of their marginal probabilities, which can be estimated by: P M I(w, c) = log Pˆ (w,c) Pˆ (w)Pˆ (c) = log #(w,c)·|D| #(w)·#(c) The rows of M PMI contain many entries of wordcontext pairs (w, c) that were never observed in the corpus, for which P M I(w, c) = log 0 = −∞. A common approach is thus to replace the M PMI matrix with M0PMI , in which P M I(w, c) = 0 in cases where #(w, c) = 0. A more consistent approach is to use positive PMI (PPMI), in which all 2.3 negative values are replaced by 0: P P M I(w, c)"
Q15-1016,J15-4004,0,0.840752,"Missing"
Q15-1016,P14-2050,1,0.2029,"ddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or perceived superiority) of these recent embeddings? While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well. In particular, embedding algorithms suggest some natural hyperparameters that can be t"
Q15-1016,W14-1618,1,0.194781,"ddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or perceived superiority) of these recent embeddings? While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well. In particular, embedding algorithms suggest some natural hyperparameters that can be t"
Q15-1016,W13-3512,0,0.895955,"Missing"
Q15-1016,W14-1619,1,0.218059,"84 .567 .484 Table 6: The average performance of SVD on word similarity tasks given different values of eig, in the vanilla scenario. pared CBOW to the other methods when setting all the hyperparameters to the defaults provided by word2vec (Table 3). With the exception of MSR’s analogy task, CBOW is not the bestperforming method of any other task in this scenario. Other scenarios showed similar trends in our preliminary experiments. While CBOW can potentially derive better representations by combining the tokens in each context window, this potential is not realized in practice. Nevertheless, Melamud et al. (2014) show that capturing joint contexts can indeed improve performance on word similarity tasks, and we believe it is a direction worth pursuing. 6 Hyperparameter Analysis We analyze the individual impact of each hyperparameter, and try to characterize the conditions in which a certain setting is beneficial. 6.1 Harmful Configurations Certain hyperparameter settings might cripple the performance of a certain method. We observe two scenarios in which SVD performs poorly. SVD does not benefit from shifted PPMI. Setting neg &gt; 1 consistently deteriorates SVD’s performance. Levy and Goldberg (2014c) ma"
Q15-1016,N13-1090,0,0.577456,"meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are “embedded” into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding"
Q15-1016,J07-2002,0,0.021048,"Missing"
Q15-1016,D14-1162,0,0.144387,", and GloVe. For historical reasons, we refer to PPMI and SVD as “countbased” representations, as opposed to SGNS and GloVe, which are often referred to as “neural” or “prediction-based” embeddings. All of these methods (as well as all other “skip-gram”-based embedding methods) are essentially bag-of-words models, in which the representation of each word reflects a weighted bag of context-words that cooccur with it. Such bag-of-word embedding models were previously shown to perform as well as or better than more complex embedding methods on similarity and analogy tasks (Mikolov et al., 2013a; Pennington et al., 2014). Notation We assume a collection of words w ∈ VW and their contexts c ∈ VC , where VW and VC are the word and context vocabularies, and denote the collection of observed word-context pairs as 212 D. We use #(w, c) to denote the number of times the in D. Similarly, #(w) = P pair (w, c) appears P 0 ) and #(c) = 0 #(w, c 0 0 c ∈VC w ∈VW #(w , c) are the number of times w and c occurred in D, respectively. In some algorithms, words and contexts are embedded in a space of d dimensions. In these cases, each word w ∈ VW is associated with a vector w ~ ∈ Rd and similarly each context c ∈ VC is repres"
Q16-1023,D16-1211,1,0.826355,"Missing"
Q16-1023,D14-1082,0,0.695724,"actions at each stage of the process and guide the parsing process. Perhaps the simplest graph-based parsers are Regardless of the details of the parsing framework being used, a crucial step in parser design is choosing the right feature function for the underlying statistical model. Recent work (see Section 2.2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of “core” features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; TaubTabib et al., 2015). However, the need to carefully define a set of core features remains. For example, the work of Chen and Manning (2014) uses 18 different elements in its feature function, while the work of Pei et al. (2015) uses 21 different elements. Other works, notably Dyer et al. (2015) and Le and Zuidema (2014), propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering. In this work, we suggest an approach which is much simpler in terms of both feature engineering 313 Transac"
Q16-1023,Q16-1026,0,0.0305739,"and sometimes tweaked to improve performance. Examples of good feature functions are the feature-set proposed by Zhang and Nivre (2011) for transitionbased parsing (including roughly 20 core components and 72 feature templates), and the featureset proposed by McDonald et al. (2005) for graphbased parsing, with the paper listing 18 templates for a first-order parser, while the first order featureextractor in the actual implementation’s code (MSTParser2 ) includes roughly a hundred feature templates. 1 Structured training of sequence tagging models over RNNbased representations was explored by Chiu and Nichols (2016) and Lample et al. (2016). 314 2 http://www.seas.upenn.edu/~strctlrn/ MSTParser/MSTParser.html The core features in a transition-based parser usually look at information such as the word-identity and part-of-speech (POS) tags of a fixed number of words on top of the stack, a fixed number of words on the top of the buffer, the modifiers (usually leftmost and right-most) of items on the stack and on the buffer, the number of modifiers of these elements, parents of words on the stack, and the length of the spans spanned by the words on the stack. The core features of a first-order graph-based par"
Q16-1023,P16-2006,0,0.0915871,"the approaches in the semi-supervised category, making it hard to tease apart the contribution of the automatic feature-combination component from that of the semisupervised component. relies on a trainable attention mechanism for focusing on specific BiLSTM vectors, parsers in the transition-based family we use in Section 4 use a human designed stack and buffer mechanism to manually direct the parser’s attention. While the effectiveness of the trainable attention approach is impressive, the stack-and-buffer guidance of transitionbased parsers results in more robust learning. Indeed, work by Cross and Huang (2016), published while working on the camera-ready version of this paper, show that the same methodology as ours is highly effective also for greedy, transition-based constituency parsing, surpassing the beam-based architecture of Vinyals et al. (88.3F vs. 89.8F points) when trained on the Penn Treebank dataset and without using orthogonal methods such as ensembling and up-training. 2.3 Bidirectional Recurrent Neural Networks Recurrent neural networks (RNNs) are statistical learners for modeling sequential data. An RNN allows one to model the ith element in the sequence based on the past – the elem"
Q16-1023,P15-1033,0,0.574122,"alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of “core” features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; TaubTabib et al., 2015). However, the need to carefully define a set of core features remains. For example, the work of Chen and Manning (2014) uses 18 different elements in its feature function, while the work of Pei et al. (2015) uses 21 different elements. Other works, notably Dyer et al. (2015) and Le and Zuidema (2014), propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering. In this work, we suggest an approach which is much simpler in terms of both feature engineering 313 Transactions of the Association for Computational Linguistics, vol. 4, pp. 313–327, 2016. Action Editor: Marco Kuhlmann. Submission batch: 2/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. and architecture engineering. Our proposal (Section 3) is centered around BiRNNs (Irsoy an"
Q16-1023,C96-1058,0,0.808629,"Missing"
Q16-1023,C12-1059,1,0.813978,"onfigurations, and by the particular set of transitions available to them. A parser is determined by the choice of a transition system, a feature function φ and a scoring function S CORE. Our choices are detailed below. (. . . |s2 |s1 |s0 , is defined as: b0 |. . . , T ) the feature extractor φ(c) = vs2 ◦ vs1 ◦ vs0 ◦ vb0 The Arc-Hybrid System Many transition systems exist in the literature. In this work, we use the archybrid transition system (Kuhlmann et al., 2011), which is similar to the more popular arc-standard system (Nivre, 2004), but for which an efficient dynamic oracle is available (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013). In the arc-hybrid system, a configuration c = (σ, β, T ) consists of a stack σ, a buffer β, and a set T of dependency arcs. Both the stack and the buffer hold integer indices pointing to sentence elements. Given a sentence s = w1 , . . . , wn , t1 , . . . , tn , the system is initialized with an empty stack, an empty arc set, and β = 1, . . . , n, ROOT , where ROOT is the special root index. Any configuration c with an empty stack and a buffer containing only ROOT is terminal, and the parse tree is given by the arc set Tc of c. The archybrid system allows 3 possibl"
Q16-1023,Q13-1033,1,0.901389,"lausible that the scoring function can be sensitive also to the distance between i and j, their ordering, and the sequential material between them. Parsing-time Complexity Once the BiLSTM is trained, parsing is performed by first computing the BiLSTM encoding vi for each word in the sentence (a linear time operation).5 Then, parsing proceeds as usual, where the feature extraction involves a concatenation of a small number of the pre-computed vi vectors. 4 Transition-based Parser We begin by integrating the feature extractor in a transition-based parser (Nivre, 2008). We follow the notation in Goldberg and Nivre (2013). The 5 While the BiLSTM computation is quite efficient as it is, as demonstrated by Lewis et al. (2016), if using a GPU implementation the BiLSTM encoding can be efficiently performed over many of sentences in parallel, making its computation cost almost negligible. Configuration: s2 s1 s0 b0 b1 b2 b3 the jumped over the lazy dog ROOT fox brown Scoring: (ScoreLef tArc , ScoreRightArc , ScoreShif t ) MLP Vthe Vbrown Vfox Vjumped Vover Vthe Vlazy Vdog VROOT concat concat concat concat concat concat concat concat concat LST M b LST M f xthe s8 LST M b LST M f xbrown s7 LST M b LST M f xfox s6 LS"
Q16-1023,J81-4005,0,0.714171,"Missing"
Q16-1023,P10-1110,0,0.0230496,"ract arc-factored (first order) models (McDonald, 2006), in which the scoring function for a tree decomposes over the individual arcs of the tree. More elaborate models look at larger (overlapping) parts, requiring more sophisticated inference and training algorithms (Martins et al., 2009; Koo and Collins, 2010). The basic transition-based parsers work in a greedy manner, performing a series of locally-optimal decisions, and boast very fast parsing speeds. More advanced transition-based parsers introduce some search into the process using a beam (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010). We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very s"
Q16-1023,D14-1080,0,0.0486568,". (2015) and Le and Zuidema (2014), propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering. In this work, we suggest an approach which is much simpler in terms of both feature engineering 313 Transactions of the Association for Computational Linguistics, vol. 4, pp. 313–327, 2016. Action Editor: Marco Kuhlmann. Submission batch: 2/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. and architecture engineering. Our proposal (Section 3) is centered around BiRNNs (Irsoy and Cardie, 2014; Schuster and Paliwal, 1997), and more specifically BiLSTMs (Graves, 2008), which are strong and trainable sequence models (see Section 2.3). The BiLSTM excels at representing elements in a sequence (i.e., words) together with their contexts, capturing the element and an “infinite” window around it. We represent each word by its BiLSTM encoding, and use a concatenation of a minimal set of such BiLSTM encodings as our feature function, which is then passed to a non-linear scoring function (multi-layer perceptron). Crucially, the BiLSTM is trained with the rest of the parser in order to learn a"
Q16-1023,P15-1162,0,0.0304941,"ted, we use the default values provided by PyCNN (e.g. for random initialization, learning rates etc). The word and POS embeddings e(wi ) and e(pi ) are initialized to random values and trained together with the rest of the parsers’ networks. In some experiments, we introduce also pre-trained word embeddings. In those cases, the vector representation of a word is a concatenation of its randomlyinitialized vector embedding with its pre-trained word vector. Both are tuned during training. We use the same word vectors as in Dyer et al. (2015) During training, we employ a variant of word dropout (Iyyer et al., 2015), and replace a word with the unknown-word symbol with probability that is inversely proportional to the frequency of the word. A word w appearing #(w) times in the training corpus is replaced with the unknown symbol with probα ability punk (w) = #(w)+α . If a word was dropped the external embedding of the word is also dropped with probability 0.5. We train the parsers for up to 30 iterations, and choose the best model according to the UAS accuracy on the development set. 10 We thank Dyer et al. for sharing their data with us. https://github.com/clab/cnn/tree/ master/pycnn Hyperparameter Tunin"
Q16-1023,Q16-1032,1,0.825275,"n encode the parser state using incremental sigmoid-belief networks (2007). In the work of Dyer et al. (2015), the entire stack and buffer of a transition-based parser are encoded as a stack-LSTMs, where each stack element is itself based on a compositional representation of parse trees. Le and Zuidema (2014) encode each tree node as two compositional representations capturing the inside and outside structures around the node, and feed the representations into a reranker. A similar reranking approach, this time based on convolutional neural networks, is taken by Zhu et al. (2015). Finally, in Kiperwasser and Goldberg (2016) we present an Easy-First parser based on a novel hierarchical-LSTM tree encoding. In contrast to these, the approach we present in this work results in much simpler feature functions, without resorting to elaborate network architectures or compositional tree representations. Work by Vinyals et al. (2015) employs a sequence-to-sequence with attention architecture for constituency parsing. Each token in the input sentence is encoded in a deep-BiLSTM representation, and then the tokens are fed as input to a deepLSTM that predicts a sequence of bracketing actions based on the already predicted br"
Q16-1023,P10-1001,0,0.0142414,"Missing"
Q16-1023,P08-1068,0,0.0210527,"tured prediction literature. We stick to the simplest parsers in each category – greedy inference for the transition-based architecture, and a first-order, arc-factored model for the graph-based architecture. Despite the simplicity of the parsing architectures and the feature functions, we achieve near state-of-the-art parsing accuracies in both English (93.1 UAS) and Chinese (86.6 UAS), using a first-order parser with two features and while training solely on Treebank data, without relying on semi-supervised signals such as pre-trained word embeddings (Chen and Manning, 2014), word-clusters (Koo et al., 2008), or techniques such as tri-training (Weiss et al., 2015). When also including pre-trained word embeddings, we obtain further improvements, with accuracies of 93.9 UAS (English) and 87.6 UAS (Chinese) for a greedy transition-based parser with 11 features, and 93.6 UAS (En) / 87.4 (Ch) for a greedy transitionbased parser with 4 features. 2 Background and Notation Notation We use x1:n to denote a sequence of n vectors x1 , · · · , xn . Fθ (·) is a function parameterized with parameters θ. We write FL (·) as shorthand for FθL – an instantiation of F with a specific set of parameters θL . We use ◦"
Q16-1023,P11-1068,0,0.141599,"en reaching a final configuration, from which the resulting parse tree is read and returned (line 6). Transition systems differ by the way they define configurations, and by the particular set of transitions available to them. A parser is determined by the choice of a transition system, a feature function φ and a scoring function S CORE. Our choices are detailed below. (. . . |s2 |s1 |s0 , is defined as: b0 |. . . , T ) the feature extractor φ(c) = vs2 ◦ vs1 ◦ vs0 ◦ vb0 The Arc-Hybrid System Many transition systems exist in the literature. In this work, we use the archybrid transition system (Kuhlmann et al., 2011), which is similar to the more popular arc-standard system (Nivre, 2004), but for which an efficient dynamic oracle is available (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013). In the arc-hybrid system, a configuration c = (σ, β, T ) consists of a stack σ, a buffer β, and a set T of dependency arcs. Both the stack and the buffer hold integer indices pointing to sentence elements. Given a sentence s = w1 , . . . , wn , t1 , . . . , tn , the system is initialized with an empty stack, an empty arc set, and β = 1, . . . , n, ROOT , where ROOT is the special root index. Any configuration c wi"
Q16-1023,N16-1030,0,0.0277366,"Missing"
Q16-1023,D14-1081,0,0.0956875,"e function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of “core” features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; TaubTabib et al., 2015). However, the need to carefully define a set of core features remains. For example, the work of Chen and Manning (2014) uses 18 different elements in its feature function, while the work of Pei et al. (2015) uses 21 different elements. Other works, notably Dyer et al. (2015) and Le and Zuidema (2014), propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering. In this work, we suggest an approach which is much simpler in terms of both feature engineering 313 Transactions of the Association for Computational Linguistics, vol. 4, pp. 313–327, 2016. Action Editor: Marco Kuhlmann. Submission batch: 2/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. and architecture engineering. Our proposal (Section 3) is centered around BiRNNs (Irsoy and Cardie, 2014; Schuster a"
Q16-1023,P14-1130,0,0.019552,"uide the parsing process. Perhaps the simplest graph-based parsers are Regardless of the details of the parsing framework being used, a crucial step in parser design is choosing the right feature function for the underlying statistical model. Recent work (see Section 2.2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of “core” features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; TaubTabib et al., 2015). However, the need to carefully define a set of core features remains. For example, the work of Chen and Manning (2014) uses 18 different elements in its feature function, while the work of Pei et al. (2015) uses 21 different elements. Other works, notably Dyer et al. (2015) and Le and Zuidema (2014), propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering. In this work, we suggest an approach which is much simpler in terms of both feature engineering 313 Transactions of the Association for Computa"
Q16-1023,J93-2004,0,0.0587308,"ion with a concatenation of BiLSTM representations of the head and modifier words, and training the BiLSTM encoder jointly with the structured objective. We also introduce a novel multi-task learning approach for labeled parsing by training a second-stage arc-labeler sharing the same BiLSTM encoder with the unlabeled parser. 6 Experiments and Results We evaluated our parsing model on English and Chinese data. For comparison purposes we follow the setup of Dyer et al. (2015). Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splits with the System This work This work This work ZhangNivre11 Martins13 (TurboParser) Pei15 Dyer15 Ballesteros16 This work This work This work Weiss15 Weiss15 Pei15 Dyer15 Ballesteros16 LeZuidema14 Zhu15 Method Representation Emb graph, 1st order transition (greedy, dyn-oracle) transition (greedy, dyn-oracle) transition (beam) graph, 3rd order+ graph, 2nd order transition (greedy) transition (greedy, dyn-oracle) graph, 1st order transition (greedy, dyn-oracle) transition (greedy, dyn-oracle) transition (greedy) transition (beam) graph, 2nd order transiti"
Q16-1023,P09-1039,0,0.00807809,"Missing"
Q16-1023,P13-2109,0,0.0215514,"pre-trained embeddings. English results use predicted POS tags (different systems use different taggers), while Chinese results use gold POS tags. PTB-YM: English PTB, Yamada and Matsumoto head rules. PTB-SD: English PTB, Stanford Dependencies (different systems may use different versions of the Stanford converter). CTB: Chinese Treebank. reranking /blend in Method column indicates a reranking system where the reranker score is interpolated with the base-parser’s score. The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al., 2013); Weiss15 (Weiss et al., 2015); Pei15: (Pei et al., 2015); Dyer15 (Dyer et al., 2015); Ballesteros16 (Ballesteros et al., 2016); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al., 2015). same predicted POS-tags as used in Dyer et al. (2015);Chen and Manning (2014). This dataset contains a few non-projective trees. Punctuation symbols are excluded from the evaluation. For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014)."
Q16-1023,P05-1012,0,0.433642,"lting in features of the form “word on top of stack is X and leftmost child is Y and . . . ”). The design of the feature function – which components to consider and which combinations of components to include – is a major challenge in parser design. Once a good feature function is proposed in a paper it is usually adopted in later works, and sometimes tweaked to improve performance. Examples of good feature functions are the feature-set proposed by Zhang and Nivre (2011) for transitionbased parsing (including roughly 20 core components and 72 feature templates), and the featureset proposed by McDonald et al. (2005) for graphbased parsing, with the paper listing 18 templates for a first-order parser, while the first order featureextractor in the actual implementation’s code (MSTParser2 ) includes roughly a hundred feature templates. 1 Structured training of sequence tagging models over RNNbased representations was explored by Chiu and Nichols (2016) and Lample et al. (2016). 314 2 http://www.seas.upenn.edu/~strctlrn/ MSTParser/MSTParser.html The core features in a transition-based parser usually look at information such as the word-identity and part-of-speech (POS) tags of a fixed number of words on top"
Q16-1023,W04-0308,0,0.191792,"t accuracies on English and Chinese. 1 Introduction The focus of this paper is on feature representation for dependency parsing, using recent techniques from the neural-networks (“deep learning”) literature. Modern approaches to dependency parsing can be broadly categorized into graph-based and transition-based parsers (Kübler et al., 2009). Graph-based parsers (McDonald, 2006) treat parsing as a search-based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees. Transition-based parsers (Nivre, 2004; Nivre, 2008) treat parsing as a sequence of actions that produce a parse tree, and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process. Perhaps the simplest graph-based parsers are Regardless of the details of the parsing framework being used, a crucial step in parser design is choosing the right feature function for the underlying statistical model. Recent work (see Section 2.2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focu"
Q16-1023,J08-4003,0,0.259108,"on English and Chinese. 1 Introduction The focus of this paper is on feature representation for dependency parsing, using recent techniques from the neural-networks (“deep learning”) literature. Modern approaches to dependency parsing can be broadly categorized into graph-based and transition-based parsers (Kübler et al., 2009). Graph-based parsers (McDonald, 2006) treat parsing as a search-based structured prediction problem in which the goal is learning a scoring function over dependency trees such that the correct tree is scored above all other trees. Transition-based parsers (Nivre, 2004; Nivre, 2008) treat parsing as a sequence of actions that produce a parse tree, and a classifier is trained to score the possible actions at each stage of the process and guide the parsing process. Perhaps the simplest graph-based parsers are Regardless of the details of the parsing framework being used, a crucial step in parser design is choosing the right feature function for the underlying statistical model. Recent work (see Section 2.2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small s"
Q16-1023,P15-1031,0,0.127063,"the process and guide the parsing process. Perhaps the simplest graph-based parsers are Regardless of the details of the parsing framework being used, a crucial step in parser design is choosing the right feature function for the underlying statistical model. Recent work (see Section 2.2 for an overview) attempt to alleviate parts of the feature function design problem by moving from linear to non-linear models, enabling the modeler to focus on a small set of “core” features and leaving it up to the machine-learning machinery to come up with good feature combinations (Chen and Manning, 2014; Pei et al., 2015; Lei et al., 2014; TaubTabib et al., 2015). However, the need to carefully define a set of core features remains. For example, the work of Chen and Manning (2014) uses 18 different elements in its feature function, while the work of Pei et al. (2015) uses 21 different elements. Other works, notably Dyer et al. (2015) and Le and Zuidema (2014), propose more sophisticated feature representations, in which the feature engineering is replaced with architecture engineering. In this work, we suggest an approach which is much simpler in terms of both feature engineering 313 Transactions of the Assoc"
Q16-1023,N15-1163,1,0.769177,"e core features of a first-order graph-based parser usually take into account the word and POS of the head and modifier items, as well as POS-tags of the items around the head and modifier, POS tags of items between the head and modifier, and the distance and direction between the head and modifier. 2.2 Related Research Efforts Coming up with a good feature-set for a parser is a hard and time consuming task, and many researchers attempt to reduce the required manual effort. The work of Lei et al. (2014) suggests a low-rank tensor representation to automatically find good feature combinations. Taub-Tabib et al. (2015) suggest a kernel-based approach to implicitly consider all possible feature combinations over sets of core-features. The recent popularity of neural networks prompted a move from templates of sparse, binary indicator features to dense core feature encodings fed into non-linear classifiers. Chen and Manning (2014) encode each core feature of a greedy transition-based parser as a dense low-dimensional vector, and the vectors are then concatenated and fed into a nonlinear classifier (multi-layer perceptron) which can potentially capture arbitrary feature combinations. Weiss et al. (2015) showed"
Q16-1023,W07-2218,0,0.0098985,"Missing"
Q16-1023,N16-1027,0,0.0161084,"R NN`+1 . Stacking BiRNNs in this way has been empirically shown to be effective (Irsoy and Cardie, 2014). In this work, we use BiRNNs and deep-BiRNNs interchangeably, specifying the number of layers when needed. Historical Notes RNNs were introduced by Elman (1990), and extended to BiRNNs by Schuster and Paliwal (1997). The LSTM variant of RNNs is due to Hochreiter and Schmidhuber (1997). BiLSTMs were recently popularized by Graves (2008), and deep BiRNNs were introduced to NLP by Irsoy and Cardie (2014), who used them for sequence tagging. In the context of parsing, Lewis et al. (2016) and Vaswani et al. (2016) use a BiLSTM sequence tagging model to assign a CCG supertag for each token in the sentence. Lewis et al. (2016) feeds the resulting supertags sequence into an A* CCG parser. Vaswani et al. (2016) adds an additional layer of LSTM which receives the BiLSTM representation together with the k-best supertags for each word and outputs the most likely supertag given previous tags, and then feeds the predicted supertags to a discriminitively trained parser. In both works, the BiLSTM is trained to produce accurate CCG supertags, and is not aware of the global parsing objective. 3 Our Approach We prop"
Q16-1023,P15-1032,0,0.142151,"arsers in each category – greedy inference for the transition-based architecture, and a first-order, arc-factored model for the graph-based architecture. Despite the simplicity of the parsing architectures and the feature functions, we achieve near state-of-the-art parsing accuracies in both English (93.1 UAS) and Chinese (86.6 UAS), using a first-order parser with two features and while training solely on Treebank data, without relying on semi-supervised signals such as pre-trained word embeddings (Chen and Manning, 2014), word-clusters (Koo et al., 2008), or techniques such as tri-training (Weiss et al., 2015). When also including pre-trained word embeddings, we obtain further improvements, with accuracies of 93.9 UAS (English) and 87.6 UAS (Chinese) for a greedy transition-based parser with 11 features, and 93.6 UAS (En) / 87.4 (Ch) for a greedy transitionbased parser with 4 features. 2 Background and Notation Notation We use x1:n to denote a sequence of n vectors x1 , · · · , xn . Fθ (·) is a function parameterized with parameters θ. We write FL (·) as shorthand for FθL – an instantiation of F with a specific set of parameters θL . We use ◦ to denote a vector concatenation operation, and v[i] to"
Q16-1023,D08-1059,0,0.0230486,"versity Ramat-Gan, Israel elikip@gmail.com Abstract arc-factored (first order) models (McDonald, 2006), in which the scoring function for a tree decomposes over the individual arcs of the tree. More elaborate models look at larger (overlapping) parts, requiring more sophisticated inference and training algorithms (Martins et al., 2009; Koo and Collins, 2010). The basic transition-based parsers work in a greedy manner, performing a series of locally-optimal decisions, and boast very fast parsing speeds. More advanced transition-based parsers introduce some search into the process using a beam (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010). We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-"
Q16-1023,P11-2033,0,0.0898845,"s”), and are comprised of several templates, where each template instantiates a binary indicator function over a conjunction of core elements (resulting in features of the form “word on top of stack is X and leftmost child is Y and . . . ”). The design of the feature function – which components to consider and which combinations of components to include – is a major challenge in parser design. Once a good feature function is proposed in a paper it is usually adopted in later works, and sometimes tweaked to improve performance. Examples of good feature functions are the feature-set proposed by Zhang and Nivre (2011) for transitionbased parsing (including roughly 20 core components and 72 feature templates), and the featureset proposed by McDonald et al. (2005) for graphbased parsing, with the paper listing 18 templates for a first-order parser, while the first order featureextractor in the actual implementation’s code (MSTParser2 ) includes roughly a hundred feature templates. 1 Structured training of sequence tagging models over RNNbased representations was explored by Chiu and Nichols (2016) and Lample et al. (2016). 314 2 http://www.seas.upenn.edu/~strctlrn/ MSTParser/MSTParser.html The core features"
Q16-1023,P15-1112,0,0.0348261,"classifiers. Titov and Henderson encode the parser state using incremental sigmoid-belief networks (2007). In the work of Dyer et al. (2015), the entire stack and buffer of a transition-based parser are encoded as a stack-LSTMs, where each stack element is itself based on a compositional representation of parse trees. Le and Zuidema (2014) encode each tree node as two compositional representations capturing the inside and outside structures around the node, and feed the representations into a reranker. A similar reranking approach, this time based on convolutional neural networks, is taken by Zhu et al. (2015). Finally, in Kiperwasser and Goldberg (2016) we present an Easy-First parser based on a novel hierarchical-LSTM tree encoding. In contrast to these, the approach we present in this work results in much simpler feature functions, without resorting to elaborate network architectures or compositional tree representations. Work by Vinyals et al. (2015) employs a sequence-to-sequence with attention architecture for constituency parsing. Each token in the input sentence is encoded in a deep-BiLSTM representation, and then the tokens are fed as input to a deepLSTM that predicts a sequence of bracket"
Q16-1023,N16-1026,0,\N,Missing
Q16-1032,D15-1075,0,0.0170225,"e useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman445 Transactions of the Association for Computational Linguistics, vol. 4, pp. 445–461, 2016. Action Editor: Noah Smith. Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015). 2 Background and Notation 2.1 Dependency-based Representation A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words. The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node. A dependency tree over a sentence with n words w1 , . . . , wn can be represented as a list of n pairs of the form (h, m), where 0 ≤ h ≤ n and 1 ≤ m ≤ n. Each such pair represents an edge in the tree in which h is the index of a head word (including the special RO"
Q16-1032,D14-1082,0,0.742693,"ning, using manual hill climbing until something seemed to work with reasonable accuracy, and then sticking with it for the rest of the experiments. 3 https://github.com/clab/cnn/tree/ master/pycnn 7 Experiments and Results We evaluated our parsing model to English and Chinese data. For comparison purposes we followed the setup of (Dyer et al., 2015). Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014). This dataset contains a few non-projective trees. Punctuation symbols are excluded from the evaluation. For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014). When using external word embeddings, we also use the same data as (Dyer et al., 2015).4 Experimental configurations We evaluated the parser in several configurations B OTTOM U P PARSER is the baseline parser, not using the tree-encoding, and instead representing each"
Q16-1032,P97-1003,0,0.500325,"that the average maximal number of siblings in one direction in the PTB is 4.1, and LSTMs were demonstrated to capture much longer-range interactions. Still, when using the tree encoding in a situation where the tree is fully specified in advance, i.e. for sentence classification, sentence similarity or translation tasks, using a head-inward generation order (or even a bidirectional RNN) may prove to work better. We leave this line of inquiry to future work. The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997). In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly. 2 Features in transition-based dependency parsers often look at the current left-most and right-most dependents of a given node, and almost never look further than the second left-most or second right-most dependen"
Q16-1032,P15-1030,0,0.0182781,"nd the top-down approach is well suited for generation. In future work, it could be interesting to combine the bottomup and top-down approaches in an encoder-decoder framework (Sutskever et al., 2014; Kiros et al., 2015). Work by Dyer et al (2016), that was submitted in parallel to ours, introduces a similar LSTMbased representation of syntactic constituents in the context of phrase-grammar parsing. 455 In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding of each node (Le and Zuidem"
Q16-1032,P15-1033,0,0.189899,"positional architecture to learn and capture the structural cues that are needed for accurate parsing. Thus, we are most interested in the random initialization setup: what can the network learn from the training corpus alone, without relying on external resources. However, the ability to perform semi-supervised learning by initializing the word-embeddings with vectors that are pre-trained on large amount of unannotated data is an appealing property of the neuralnetwork approaches, and we evaluate our parser also in this semi-supervised setup. When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al., 2015), as these were shown to work better than traditional skipgram vectors for syntactic tasks such as part-ofspeech tagging and parsing. 3.2 A note on the head-outward generation Why did we choose to encode the children from the head outward, and not the other way around? The head outward generation order is needed to facilitate incremental tree construction and allow for efficient parsing, as we show in section 4 below. Besides the efficiency considerations, using the headoutward encoding puts more emphasis"
Q16-1032,N16-1024,0,0.0307372,"nching trees by relying on the well established LSTM sequence model, and using it as a black box. Very recently, Zhang et al. (2015) proposed an RNN-based tree encoding which is similar to ours in encoding the sequence of modifiers as an RNN. Unlike our bottom-up encoder, their method works top-down, and is therefore not readily applicable for parsing. On the other hand the top-down approach is well suited for generation. In future work, it could be interesting to combine the bottomup and top-down approaches in an encoder-decoder framework (Sutskever et al., 2014; Kiros et al., 2015). Work by Dyer et al (2016), that was submitted in parallel to ours, introduces a similar LSTMbased representation of syntactic constituents in the context of phrase-grammar parsing. 455 In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Wat"
Q16-1032,C96-1058,0,0.714806,"pected considering that the average maximal number of siblings in one direction in the PTB is 4.1, and LSTMs were demonstrated to capture much longer-range interactions. Still, when using the tree encoding in a situation where the tree is fully specified in advance, i.e. for sentence classification, sentence similarity or translation tasks, using a head-inward generation order (or even a bidirectional RNN) may prove to work better. We leave this line of inquiry to future work. The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997). In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly. 2 Features in transition-based dependency parsers often look at the current left-most and right-most dependents of a given node, and almost never look further than the second left-most or second"
Q16-1032,N10-1115,1,0.814654,"hing factors, at the expense of ignoring the order of the modifiers. In contrast, we propose a tree-encoding that naturally supports trees with arbitrary branching factors, making it particularly appealing for dependency trees. Our tree encoder uses recurrent neural networks as a building block: we model the left and right sequences of modifiers using RNNs, which are composed in a recursive manner to form a tree (Section 3). We use our tree representation for encoding the partially-built parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010). Using the Hierarchical Tree LSTM representation, and without using any external embeddings, our parser achieves parsing accuracies of 92.6 UAS and 90.2 LAS on the PTB (Stanford dependencies) and 86.1 UAS and 84.4 LAS on the Chinese treebank, while relying on greedy decoding. To the best of our knowledge, this is the first work to demonstrate competitive parsing accuracies for full-scale parsing while relying solely on recursive, compositional tree representations, and without using a reranking framework. We discuss related work in Section 8. While the parsing experiments demonstrate the suit"
Q16-1032,C12-1059,1,0.840475,"training phase makes it hard for the parser to infer the best action given partly erroneous trees. In order to cope with this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions. This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree. Such a set G is called a dynamic oracle. Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012). The Dynamic Oracle A dynamic-oracle for the easy-first parsing system we use is presented in (Goldberg and Nivre, 2013). Briefly, the dynamicoracle version of G defines the set of gold actions as the set of actions which does not increase the number of erroneous attachments more than the minimum possible (given previous erroneous actions). The number of erroneous attachments is increased in three cases: (1) connecting a modifier to its head prematurely. Once the modifier is attached it is removed from the pending list and therefore can no longer acquire any of its own modifiers; (2) connecti"
Q16-1032,Q13-1033,1,0.951891,"this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions. This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree. Such a set G is called a dynamic oracle. Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012). The Dynamic Oracle A dynamic-oracle for the easy-first parsing system we use is presented in (Goldberg and Nivre, 2013). Briefly, the dynamicoracle version of G defines the set of gold actions as the set of actions which does not increase the number of erroneous attachments more than the minimum possible (given previous erroneous actions). The number of erroneous attachments is increased in three cases: (1) connecting a modifier to its head prematurely. Once the modifier is attached it is removed from the pending list and therefore can no longer acquire any of its own modifiers; (2) connecting a modifier to an erroneous head, when the correct head is still on the pending list; (3) connecting a modifier to a co"
Q16-1032,D14-1080,0,0.0411656,"ead node t, and let vi be a vector corresponding to the ith word in the sentence (this vector captures information such as the word form and its part of speech tag, and will be discussed shortly). The vec447 where wi and pi are the embedded vectors of the word-form and POS-tag of the ith word. This encodes each word in isolation, disregarding its context. The context of a word can be very informative regarding its meaning. One way of incorporating context is the Bidirectional RNN (Schuster and Paliwal, 1997). Bidirectional RNNs are shown to be an effective representation for sequence tagging (Irsoy and Cardie, 2014). Bidirectional RNNs represent a word in the sentence using a concatenation of the end-states of two RNNs, one running the black fox who really likes apples did not jump over a lazy dog yesterday L L the black fox who really likes apples R foxR L L L R the L R whoR L R L fox who L likesL R likesR R really R over a lazy dog R apples yesterday R overR R apples L likes R L overL really likes apples L really R black R jumpR who really likes apples foxL black L jumpL R L whoL the L not did R a lazy dog L L a L R did L R not L jump over L dogL lazy R a L R dogR R lazy L dog R yesterday Figure 1: Net"
Q16-1032,P15-1162,0,0.0236801,"large unannotated corpora. A common approach is to designate a special “unknown-word” symbol, whose associated vector will be used as the word representation whenever an OOV word is encountered at test time. In order to train the unknown-word vector, a possible approach is to replace all the words appearing in the training corpus less than a certain number of times with the unknown-word symbol. This approach gives a good vector representation for unknown words but at the expense of ignoring many of the words from the training corpus. We instead propose a variant of the word-dropout approach (Iyyer et al., 2015). During training, we replace a word with the unknown-word symbol with probability that is inversely proportional to frequency of the word. Formally, we replace a word w appearing #(w) times in the training corpus with the unknown symbol with a probability: α punk (w) = #(w) + α Using this approach we learn a vector representation for unknown words with minimal impact on the training of sparse words. 6 Implementation Details Our Python implementation will be made available at the first author’s website. We use the PyCNN wrapper of the CNN library3 for building the computation graph of the netw"
Q16-1032,D14-1081,0,0.198548,"2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015). Our parser is a greedy, bottom up parser that relies on compositional vector encodings of subtrees as its sole set of features. Unlike the re-ranking approaches, we do not rely on an external parser to provide k-best lists. Unlike the bottom-up parser in (Socher et al., 2010) that only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) that achieves very low parsing accuracies, we parse arbitrary sentences with near state-of-the-art accuracy. Unlike the bottom up parser in (Socher et al., 2013a) we do not make use of a grammar. The"
Q16-1032,D15-1137,0,0.0147138,"ion based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015). Our parser is a greedy, bottom up parser that relies on compositional vector encodings of subtrees as its sole set of features. Unlike the re-ranking approaches, we do not rely on an external parser to provide k-best lists. Unlike the bottom-up parser in (Socher et al., 2010) that only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) that achieves very low parsing accuracies, we parse arbitrary sentences with near state-of-the-art accuracy. Unlike the bottom up parser in (Socher et al., 2013a) we do not make use of a grammar. The parser of (Weiss et a"
Q16-1032,N15-1142,0,0.0310357,"rate parsing. Thus, we are most interested in the random initialization setup: what can the network learn from the training corpus alone, without relying on external resources. However, the ability to perform semi-supervised learning by initializing the word-embeddings with vectors that are pre-trained on large amount of unannotated data is an appealing property of the neuralnetwork approaches, and we evaluate our parser also in this semi-supervised setup. When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al., 2015), as these were shown to work better than traditional skipgram vectors for syntactic tasks such as part-ofspeech tagging and parsing. 3.2 A note on the head-outward generation Why did we choose to encode the children from the head outward, and not the other way around? The head outward generation order is needed to facilitate incremental tree construction and allow for efficient parsing, as we show in section 4 below. Besides the efficiency considerations, using the headoutward encoding puts more emphasis on the outermost dependants, which are known to be the most informative for predicting pa"
Q16-1032,C12-1106,0,0.0494447,"Missing"
Q16-1032,P13-2020,0,0.0346146,"ete, in the sense that it already acquired all of its own modifiers? to this end, the scoring function looks at a window of k subtrees to each side of the head-modifier pair (pend[i − k], . . . , pend[i + 1 + k]) where the neighbouring subtrees are used for providing hints regarding possible additional modifiers of m and h that are yet to be acquired. We use k = 2 in our experiments, for a total of 6 subtrees in total. This window approach is also used in the Easy-First parser of Goldberg and Elhadad (Goldberg and Elhadad, 2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013). However, unlike the previous work, which made use of extensive feature engineering and rich feature functions aiming at extracting the many relevant linguistic sub-structures from the 6 subtrees and their interactions, we provide the scoring function solely with the vector-encoding of the 6 subtrees in the window. Modeling the labeled attachment score is more difficult than modeling the unlabeled score and is prone to more errors. Moreover, picking the label for an attachment will cause less cascading error in contrast to picking the wrong attachment, which will necessarily preclude the pars"
Q16-1032,J93-2004,0,0.0569683,"ng top accuracy in neural network based parser. We did not follow this advice and made very few attempts at hyper-parameter tuning, using manual hill climbing until something seemed to work with reasonable accuracy, and then sticking with it for the rest of the experiments. 3 https://github.com/clab/cnn/tree/ master/pycnn 7 Experiments and Results We evaluated our parsing model to English and Chinese data. For comparison purposes we followed the setup of (Dyer et al., 2015). Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014). This dataset contains a few non-projective trees. Punctuation symbols are excluded from the evaluation. For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014). When using external word embeddings, we also use the same data as (Dyer et al., 2015).4 Experimental configurations We evaluated the parser"
Q16-1032,S14-2001,0,0.0186926,"structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman445 Transactions of the Association for Computational Linguistics, vol. 4, pp. 445–461, 2016. Action Editor: Noah Smith. Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015). 2 Background and Notation 2.1 Dependency-based Representation A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words. The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node. A dependency tree over a sentence with n words w1 , . . . , wn can be represented as a list of n pairs of the form (h, m), where 0 ≤ h ≤ n and 1 ≤ m ≤ n. Each such pair represents an edge in the tree in which h is the i"
Q16-1032,P13-2109,0,0.0640832,". PTB-YM: English PTB, Yamada and Matsumoto head rules. PTB-SD: English PTB, Stanford Dependencies (different systems may use different versions of the Stanford converter. CTB: Chinese Treebank. reranking /blend in method column indicates a reranking system where the reranker score is interpolated with the base-parser’s score. The reranking systems’ runtimes are those of the base parsers they use. O(n)+ indicates a linear-time system with a large multiplicative constant. The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al., 2013); Weiss15 (Weiss et al., 2015); Pei15: (Pei et al., 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al., 2015). al by having a more elaborate vector-composition function, relying solely on the compositional representations, and performing fully bottom-up parsing without being guided by a stack-and-buffer control structure. 9 Conclusions and Future Work We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders, and demonstrate its effectiveness by integrating it in a bottom-up easy-first parser. Future e"
Q16-1032,P15-1031,0,0.281684,"g. On the other hand the top-down approach is well suited for generation. In future work, it could be interesting to combine the bottomup and top-down approaches in an encoder-decoder framework (Sutskever et al., 2014; Kiros et al., 2015). Work by Dyer et al (2016), that was submitted in parallel to ours, introduces a similar LSTMbased representation of syntactic constituents in the context of phrase-grammar parsing. 455 In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding o"
Q16-1032,P13-1045,0,0.385659,"ments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman445 Transactions of the Association for Computational Linguistics, vol. 4, pp. 445–461, 2016. Action Editor: Noah Smith. Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015). 2 Background and Notation 2.1 Dependency-based Representation A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words. The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node. A dependency tree over a sentence with n words w1 , . . . , wn can be represented as a list of n pairs of the form (h, m), where 0 ≤ h ≤ n and 1"
Q16-1032,D13-1170,0,0.0347916,"ments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman445 Transactions of the Association for Computational Linguistics, vol. 4, pp. 445–461, 2016. Action Editor: Noah Smith. Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015). 2 Background and Notation 2.1 Dependency-based Representation A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words. The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node. A dependency tree over a sentence with n words w1 , . . . , wn can be represented as a list of n pairs of the form (h, m), where 0 ≤ h ≤ n and 1"
Q16-1032,P15-1150,0,0.383333,"suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman445 Transactions of the Association for Computational Linguistics, vol. 4, pp. 445–461, 2016. Action Editor: Noah Smith. Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015). 2 Background and Notation 2.1 Dependency-based Representation A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words. The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node. A dependency tree over a sentence with n words w1 , . . . , wn can be represented as a list of n pairs of the form (h, m), where 0 ≤ h ≤ n and 1 ≤ m ≤ n. Each such"
Q16-1032,D11-1116,0,0.0248301,"modifier in the subtree h? • Is m complete, in the sense that it already acquired all of its own modifiers? to this end, the scoring function looks at a window of k subtrees to each side of the head-modifier pair (pend[i − k], . . . , pend[i + 1 + k]) where the neighbouring subtrees are used for providing hints regarding possible additional modifiers of m and h that are yet to be acquired. We use k = 2 in our experiments, for a total of 6 subtrees in total. This window approach is also used in the Easy-First parser of Goldberg and Elhadad (Goldberg and Elhadad, 2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013). However, unlike the previous work, which made use of extensive feature engineering and rich feature functions aiming at extracting the many relevant linguistic sub-structures from the 6 subtrees and their interactions, we provide the scoring function solely with the vector-encoding of the 6 subtrees in the window. Modeling the labeled attachment score is more difficult than modeling the unlabeled score and is prone to more errors. Moreover, picking the label for an attachment will cause less cascading error in contrast to picking the wrong attachment, which"
Q16-1032,P15-1113,0,0.0314097,"16), that was submitted in parallel to ours, introduces a similar LSTMbased representation of syntactic constituents in the context of phrase-grammar parsing. 455 In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015). Our parser is a greedy, bottom up parser that relies on compositional vector encodings of subtrees as its sole set of features. Unlike the re-ranking approaches, we do not rely on an external pars"
Q16-1032,P15-1032,0,0.496963,"g. We use the default parameters initialization, step sizes and regularization values provided by the PyCNN toolkit. The hyperparameters of the final networks used for all the reported experiments are detailed in Table 1. Word embedding dimension POS tag embedding dimension Relation embedding dimension Hidden units in ScoreU Hidden units in ScoreL LSTM Dimensions (tree) LSTM Layers (tree) BI-LSTM Dimensions BI-LSTM Layers Mini-batch size α (for word dropout) paug (for exploration training) g 100 25 25 100 100 200 2 100+100 2 50 0.25 0.1 tanh Table 1: Hyper-parameter values used in experiments Weiss et al (2015) stress the importance of careful hyperparameter tuning for achieving top accuracy in neural network based parser. We did not follow this advice and made very few attempts at hyper-parameter tuning, using manual hill climbing until something seemed to work with reasonable accuracy, and then sticking with it for the rest of the experiments. 3 https://github.com/clab/cnn/tree/ master/pycnn 7 Experiments and Results We evaluated our parsing model to English and Chinese data. For comparison purposes we followed the setup of (Dyer et al., 2015). Data For English, we used the Stanford Dependency (SD"
Q16-1032,D08-1059,0,0.147003,"We evaluated our parsing model to English and Chinese data. For comparison purposes we followed the setup of (Dyer et al., 2015). Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014). This dataset contains a few non-projective trees. Punctuation symbols are excluded from the evaluation. For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014). When using external word embeddings, we also use the same data as (Dyer et al., 2015).4 Experimental configurations We evaluated the parser in several configurations B OTTOM U P PARSER is the baseline parser, not using the tree-encoding, and instead representing each item in pending solely by the vector-representation (word and POS) of its head word. B OTTOM U P PARSER +H T L STM is using our Hierarchical Tree LSTM representation. B OTTOM U P PARSER +H T L STM + BI - LSTM is the Hiera"
Q16-1032,P11-2033,0,0.0612034,"le Chinese results use gold POS tags. PTB-YM: English PTB, Yamada and Matsumoto head rules. PTB-SD: English PTB, Stanford Dependencies (different systems may use different versions of the Stanford converter. CTB: Chinese Treebank. reranking /blend in method column indicates a reranking system where the reranker score is interpolated with the base-parser’s score. The reranking systems’ runtimes are those of the base parsers they use. O(n)+ indicates a linear-time system with a large multiplicative constant. The different systems and the numbers reported from them are taken from: ZhangNivre11: (Zhang and Nivre, 2011); Martins13: (Martins et al., 2013); Weiss15 (Weiss et al., 2015); Pei15: (Pei et al., 2015); LeZuidema14 (Le and Zuidema, 2014); Zhu15: (Zhu et al., 2015). al by having a more elaborate vector-composition function, relying solely on the compositional representations, and performing fully bottom-up parsing without being guided by a stack-and-buffer control structure. 9 Conclusions and Future Work We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders, and demonstrate its effectiveness by integrating it in a bo"
Q16-1032,P15-1112,0,0.282169,") (Socher et al., 2010; Tai et al., 2015; Socher, 2014). While trees can be binarized to cope with the arity restriction, doing so results in deep trees which in turn leads to the vanishing gradient problem when training. To cope with the vanishing gradients, (Tai et al., 2015) enrich the composition function with a gating mechanism similar to that of the LSTM, resulting in the so-called Tree-LSTM model. Another approach is to allow arbitrary arities but ignoring the sequential nature of the modifiers, e.g. by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015). In contrast, our tree encoding method naturally allows for arbitrary branching trees by relying on the well established LSTM sequence model, and using it as a black box. Very recently, Zhang et al. (2015) proposed an RNN-based tree encoding which is similar to ours in encoding the sequence of modifiers as an RNN. Unlike our bottom-up encoder, their method works top-down, and is therefore not readily applicable for parsing. On the other hand the top-down approach is well suited for generation. In future work, it could be interesting to combine the bottomup and top-down approaches in an encode"
Q16-1037,W13-2604,0,0.0167308,"complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs. K´ad´ar et al. (2016) and Li et al. (2016) suggest visualization techniques for word-level RNNs trained to perform tasks that aren’t explicitly syntactic (image captioning and sentiment analysis). Early work that used neural networks to model grammaticality judgments includes Allen and Seidenberg (1999) and Lawrence et al. (1996). More recently, the connection between grammaticality judgments and the probabilities assigned by a language model was explored by Clark et al. (2013) and Lau et al. (2015). Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences have been made in the parsing literature (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011). 9 Discussion and Future Work Neural network architectures are typically evaluated on random samples of naturally occurring sentences, e.g., using perplexity on held-out data in language modeling. Since the majority of natural language sentences are grammatically simple, models can achieve high overall accuracy using flawed heuristic"
Q16-1037,N16-1024,0,0.00901657,"tured. 1 Introduction Recurrent neural networks (RNNs) are highly effective models of sequential data (Elman, 1990). The rapid adoption of RNNs in NLP systems in recent years, in particular of RNNs with gating mechanisms such as long short-term memory (LSTM) units Yoav Goldberg Computer Science Department Bar Ilan University yoav.goldberg@gmail.com (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014), has led to significant gains in language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), parsing (Vinyals et al., 2015; Kiperwasser and Goldberg, 2016; Dyer et al., 2016), machine translation (Bahdanau et al., 2015) and other tasks. The effectiveness of RNNs1 is attributed to their ability to capture statistical contingencies that may span an arbitrary number of words. The word France, for example, is more likely to occur somewhere in a sentence that begins with Paris than in a sentence that begins with Penguins. The fact that an arbitrary number of words can intervene between the mutually predictive words implies that they cannot be captured by models with a fixed window such as n-gram models, but can in principle be captured by RNNs, which do not have an arc"
Q16-1037,N16-1082,0,0.0156208,"pecific circumstances (Elman, 1993), though later work has called some of his conclusions into question (Rohde and Plaut, 1999; Cartling, 2008). Frank et al. (2013) studied the acquisition of anaphora coreference by SRNs, again in a miniature language. Recently, Bowman et al. (2015) tested the ability of LSTMs to learn an artificial language based on propositional logic. As in our study, the performance of the network degraded as the complexity of the test sentences increased. Karpathy et al. (2016) present analyses and visualization methods for character-level RNNs. K´ad´ar et al. (2016) and Li et al. (2016) suggest visualization techniques for word-level RNNs trained to perform tasks that aren’t explicitly syntactic (image captioning and sentiment analysis). Early work that used neural networks to model grammaticality judgments includes Allen and Seidenberg (1999) and Lawrence et al. (1996). More recently, the connection between grammaticality judgments and the probabilities assigned by a language model was explored by Clark et al. (2013) and Lau et al. (2015). Finally, arguments for evaluating NLP models on a strategically sampled set of dependency types rather than a random sample of sentences"
Q16-1037,Q16-1023,1,\N,Missing
Q19-1030,P00-1022,0,0.309445,"Missing"
Q19-1030,P13-1023,0,0.0566837,"Missing"
Q19-1030,2012.eamt-1.60,0,0.017621,"Missing"
Q19-1030,J12-4003,0,0.40804,"Missing"
Q19-1030,P16-1154,0,0.0915123,"Missing"
Q19-1030,P16-1074,0,0.0379452,"Missing"
Q19-1030,W04-0205,0,0.135354,"Missing"
Q19-1030,D14-1082,0,0.0560013,"Missing"
Q19-1030,E14-4043,0,0.0631483,"Missing"
Q19-1030,P82-1020,0,0.664523,"Missing"
Q19-1030,D15-1162,0,0.0611225,"Missing"
Q19-1030,D17-1018,0,0.061127,"Missing"
Q19-1030,N18-2108,0,0.0524671,"Missing"
Q19-1030,P06-1079,0,0.0847907,"Missing"
Q19-1030,W04-0902,0,0.114666,"Missing"
Q19-1030,D16-1132,0,0.060631,"Missing"
Q19-1030,W16-0705,0,0.0478307,"Missing"
Q19-1030,D16-1179,0,0.0494785,"Missing"
Q19-1030,J93-2004,0,0.071667,"Missing"
Q19-1030,D10-1086,0,0.0795772,"Missing"
Q19-1030,N18-1202,0,0.065628,"Missing"
Q19-1030,W16-0701,0,0.0255101,"Missing"
Q19-1030,W13-0211,0,0.0551959,"Missing"
Q19-1030,D15-1202,0,0.118951,"Missing"
Q19-1030,Q15-1001,0,0.0677673,"Missing"
Q19-1030,P10-1142,0,0.0668405,"Missing"
Q19-1030,P18-1196,0,0.0614067,"Missing"
Q19-1030,E93-1037,0,0.394644,"Missing"
Q19-1030,W13-2300,0,0.0432577,"Missing"
Q19-1030,O01-1011,0,0.283237,"Missing"
Q19-1030,P18-1053,0,0.0295047,"Missing"
Q19-1030,C18-1002,0,0.0317148,"Missing"
Q19-1030,D13-1095,0,\N,Missing
Q19-1030,W18-2501,0,\N,Missing
Q19-1030,P18-2017,0,\N,Missing
S13-1035,P11-1070,0,0.0140797,"d while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or results, but rather to provide a new kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researchers to build upon. Instead"
S13-1035,J10-4006,0,0.0254768,"ange of syntactic configurations. It also includes temporal information, facilitating new kinds of research into lexical semantics over time. This paper describes the dataset, the syntactic representation, and the kinds of information provided. 1 Introduction The distributional hypothesis of Harris (1954) states that properties of words can be captured based on their contexts. The consequences of this hypothesis have been leveraged to a great effect by the NLP community, resulting in algorithms for inferring syntactic as well as semantic properties of words (see e.g. (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and"
S13-1035,D09-1060,0,0.014087,"of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim"
S13-1035,W12-3308,1,0.756407,"1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or"
S13-1035,W08-1301,0,0.00774287,"Missing"
S13-1035,D08-1094,0,0.0621923,"Missing"
S13-1035,J10-4007,0,0.0196147,"Missing"
S13-1035,P06-1063,0,0.017681,"Missing"
S13-1035,P08-1068,0,0.00635447,"8; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or results, but rather to provide a new kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researcher"
S13-1035,P12-3029,1,0.230633,"aset is made publicly available under the Creative Commons Attribution-Non Commercial ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-ncsa/3.0/legalcode. 242 Figure 2: Word-similarity over time: The word “rock” starts to become similar to “jazz” around 1968. The plot shows the cosine similarity between the immediate syntactic contexts of of the word “rock” in each year, to the immediate syntactic contexts of the words “jazz” (in red) and “stone” (in blue) aggregated over all years. A closely related effort to add syntactic annotation to the books corpus is described in Lin et al. (2012). That effort emphasize an interactive query interface covering several languages, in which the underlying syntactic representations are linearngrams enriched with universal part-of-speech tags, as well as first order unlabeled dependencies. In contrast, our emphasis is not on an easy-to-use query interface but instead a useful and flexible resource for computational-minded researchers. We focus on English and use finer-grained English-specific POS-tags. The syntactic analysis is done using a more accurate parser, and we provide counts over labeled tree fragments, covering a diverse set of tre"
S13-1035,P98-2127,0,0.107321,"y and Pantel, 2010; Baroni and Lenci, 2010) and the references therein). In this paper, we describe a very large dataset of syntactic-ngrams, that is, structures in which the contexts of words are based on their respective position in a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen"
S13-1035,J93-2004,0,0.0611744,"more immediate use than the raw parse trees. While access to the parse trees may allow for somewhat greater flexibility in the kinds of questions one could ask, it also comes with a very hefty price tag in terms of the required computational resources: while counting seems trivial, it is, in fact, quite demanding computationally when done on such a scale, and requires a massive infrastructure. By lifting this burden of NLP researchers, we hope to free them to tackle interesting research questions. 2 Underlying Syntactic Representation We assume the part-of-speech tagset of the Penn Treebank (Marcus et al., 1993). The syntactic representation we work with is based on dependencygrammar. Specifically, we use labeled dependency trees following the “basic” variant of the Stanforddependencies scheme (de Marneffe and Manning, 2008b; de Marneffe and Manning, 2008a). Dependency grammar is a natural choice, as it emphasizes individual words and explicitly models the connections between them. Stanford dependencies are appealing because they model relations between content words directly, without intervening functional markers (so in a construction such as “wanted to know” there is a direct relation (wanted, kno"
S13-1035,W12-3018,0,0.0167949,"Missing"
S13-1035,J07-2002,0,0.0289002,"Missing"
S13-1035,P12-1081,0,0.0176042,"t al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to present new methods or results, but rather to provide a new kind of a large-scale (based on corpora about 100 times larger than previous efforts) high-quality and standard resource for researchers to build upon. Instead of focusing on"
S13-1035,P10-1044,0,0.010479,"a syntactic parse tree, and not on their sequential order in the sentence: the different words in the ngram may be far apart from each other in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this i"
S13-1035,W09-3829,0,0.0237645,"s well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare results across different studies. Our aim in this work is not to p"
S13-1035,P10-1045,0,0.0202565,"Missing"
S13-1035,P08-1066,0,0.0103752,"words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textual corpora does not use a standardized corpora for constructing their models, making it very hard to reproduce results and challenging to compare r"
S13-1035,P10-1013,0,0.0073407,"her in the sentence, yet close to each other syntactically. See Figure 1 for an example of a syntactic-ngram. The utility of syntactic contexts of words for constructing vector-space models of word meanings is well established (Lin, 1998; Lin and Pantel, 2001; Pad´o and Lapata, 2007; Baroni and Lenci, 2010). Syntactic relations are successfully used for modeling selectional preferences (Erk and Pad´o, 2008; ∗ Work performed while at Google. Erk et al., 2010; Ritter et al., 2010; S´eaghdha, 2010), and dependency paths are also used to infer binary relations between words (Lin and Pantel, 2001; Wu and Weld, 2010). The use of syntacticngrams holds promise also for improving the accuracy of core NLP tasks such as syntactic languagemodeling (Shen et al., 2008) and syntactic-parsing (Chen et al., 2009; Sagae and Gordon, 2009; Cohen et al., 2012), though most successful attempts to improve syntactic parsing by using counts from large corpora are based on sequential rather than syntactic information (Koo et al., 2008; Bansal and Klein, 2011; Pitler, 2012), we believe this is because large-scale datasets of syntactic counts are not readily available. Unfortunately, most work utilizing counts from large textu"
S13-1035,D08-1059,0,0.00661881,"s performed using a first order CRF tagger, which was trained on a union of the Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was trained on the same training data as the tagger after 4-way jack-knifing so that the parser is trained on data with predicted part-of-speech tags. The parser provides state-of-the-art syntactic annotations for English.3 Figure 3: Syntactic-ngram examples. Non-content words are grayed, functional markers appearing only in the extended-* collections are dashed. (a) node (b) extended-node (c) arcs (d) arcs, including the coordinating word (e) extended-arcs, including a preposition (f) triarcs (g) extend"
S13-1035,P11-2033,0,0.0048771,"e Penn WSJ Corpus (Marcus et al., 1993), the Brown corpus (Kucera and Francis, 1967) and the Questions Treebank (Judge et al., 2006). In addition to the diverse training material, the tagger makes use of features based on word-clusters derived from trigrams of the Books corpus. These cluster-features make the tagger more robust on the books domain. For further details regarding the tagger, see Lin et al. (2012). Syntactic parsing was performed using a reimplementation of a beam-search shift-reduce dependency parser (Zhang and Clark, 2008) with a beam of size 8 and the feature-set described in Zhang and Nivre (2011). The parser was trained on the same training data as the tagger after 4-way jack-knifing so that the parser is trained on data with predicted part-of-speech tags. The parser provides state-of-the-art syntactic annotations for English.3 Figure 3: Syntactic-ngram examples. Non-content words are grayed, functional markers appearing only in the extended-* collections are dashed. (a) node (b) extended-node (c) arcs (d) arcs, including the coordinating word (e) extended-arcs, including a preposition (f) triarcs (g) extended-triarcs (h) quadarcs, including a preposition. Counts Each syntactic ngram"
S13-1035,C98-2122,0,\N,Missing
tsarfaty-goldberg-2008-word,J98-4004,0,\N,Missing
tsarfaty-goldberg-2008-word,D07-1022,0,\N,Missing
tsarfaty-goldberg-2008-word,C04-1024,0,\N,Missing
tsarfaty-goldberg-2008-word,W00-1201,0,\N,Missing
tsarfaty-goldberg-2008-word,J03-4003,0,\N,Missing
tsarfaty-goldberg-2008-word,H94-1020,0,\N,Missing
tsarfaty-goldberg-2008-word,P03-1054,0,\N,Missing
tsarfaty-goldberg-2008-word,P04-1042,0,\N,Missing
tsarfaty-goldberg-2008-word,P06-3009,1,\N,Missing
tsarfaty-goldberg-2008-word,P05-1022,0,\N,Missing
tsarfaty-goldberg-2008-word,W07-2219,1,\N,Missing
tsarfaty-goldberg-2008-word,P06-1023,0,\N,Missing
tsarfaty-goldberg-2008-word,W05-0706,0,\N,Missing
tsarfaty-goldberg-2008-word,P08-1043,1,\N,Missing
tsarfaty-goldberg-2008-word,P06-1084,0,\N,Missing
tsarfaty-goldberg-2008-word,P03-1013,0,\N,Missing
W09-2005,W07-2323,0,0.0137451,"the general NLP tasks and requires understanding and modelling knowledge which, almost by definition, cannot be formalized (i.e., terms like beautiful, touching, funny or intriguing). On the other hand, this vagueness itself may enable a less restrictive formalization and allow a variety of quality judgments. Such vague formalizations are naturally more useful when a computational creativity system does not attempt to model the creativity process itself, but instead focuses on ’creative products’ such as poetry (see Section 2.3), prose and narrative (Montfort, 2006), cryptic crossword clues (Hardcastle, 2007) and many others. Some research focus on the creative process itself (see (Ritchie, 2006) for a comprehensive review of the field). We discuss in this paper what Boden (1998) calls P-Creativity (Psychological Creativity) which is defined relative to the initial state of knowledge, and H-Creativity (Historical Creativity) which is relative to a specific reference culture. Boden claims that, while hard to reproduce, exploratory creativity is most successful in computer models of creativity. This is because the other kinds of creativity are even more elusive due to the difficulty of ap33 proachin"
W09-2005,J91-1002,0,0.0432403,"rds to be the number of edges in the shortest path between the words in the associations-graph. Interestingly, almost any word pair in the association graph is connected with a path of at most 3 edges. Thus, we take two words to be associatively related if their associative distance is 1 or 2. Similarly, we define the WordNet distance between two stemmed words to be the number of edges in the shortest path between any synset of one word to any synset of the other word2 . Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). We take the associativity of a piece of text to be the number of associated word pairs in the text, normalized by the number of word pairs in the text of which both words are in the WAN.3 We take the WordNet-relations level of a piece of text to be the number of WordNet-related word pairs in the text. 2 This is the inverse of the path-similarity measure of (Pedersen et al., 2004). 3 This normalization is performed to account for the limited lexical coverage of the WAN. We don’t want words that appear in a text, but are not covered by the WAN, to affect the associativity level of the text. 3"
W09-2005,N04-3012,0,0.0259862,"Missing"
W09-3819,W06-2920,0,0.0943666,"arsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific properties of Arabic making it easy or hard to parse in comparison to other languages. Our aim is to evaluate current state-of-the-art dependency parsers and approaches on Hebrew dependency parsing, to understand some of"
W09-3819,de-marneffe-etal-2006-generating,0,0.0230394,"Missing"
W09-3819,P06-1087,1,0.752163,"gmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of"
W09-3819,P08-1085,1,0.833549,"ining set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged data. When evaluating the parsing models, we perform two sets of evaluations. The first one is an oracle experiment, assuming gold segmentation and tagging is available. The second one is a real-world experiment, in which we segment and POS-tag the test-set sentences using the morphological disambiguator described in (Adler, 2007; Goldberg et al., 2008) prior to parsing. Parsers and parsing models We use the freely available implementation of MaltParser2 and MSTParser3 , with default settings for each of the parsers. For MaltParser, we experiment both with the default feature representation (M ALT) and the feature representation used for parsing Arabic in CoNLL 2006 and 2007 multilingual dependency parsing shared tasks (M ALT-A RA). For MST parser, we experimented with firstorder (M ST 1) and second-order (M ST 2) models. We varied the amount of lexical information available to the parser. Each of the parsers was trained on 3 datasets: L EX"
W09-3819,E09-1038,1,0.913871,"into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific propert"
W09-3819,D07-1013,0,0.036053,"ages (CoNLL Shared Task 2006, 2007). Briefly, a graph-based parsing model works by assigning a score to every possible attachment between a pair (or a triple, for a second-order model) of words, and then inferring a global tree structure that maximizes the sum of these local scores. Transition-based models work by building the dependency graph in a sequence of steps, where each step is dependent on the next input word(s), the previous decisions, and the current state of the parser. For more details about these parsing models as well as a discussion on the relative benefits of each model, see (McDonald and Nivre, 2007). Contrary to constituency-based parsers, dependency parsing models expect a morphologically segmented and POS tagged text as input. 4 Experiments Data We follow the train-test-dev split established in (Tsarfaty and Sima’an, 2008). Specifically, we use Sections 2-12 (sentences 484-5724) of the Hebrew Dependency Treebank as our training set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged d"
W09-3819,nivre-etal-2006-maltparser,0,0.194296,"Missing"
W09-3819,C08-1112,0,0.27714,"Missing"
W09-3819,P06-3009,0,0.116554,"sure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models,"
W09-3819,D07-1096,0,\N,Missing
W10-1401,P05-1038,0,0.0195671,"arily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 on N EGRA. They showed that usi"
W10-1401,W10-1408,1,0.785091,"Missing"
W10-1401,W10-1404,0,0.236375,"s is substantial lexical data sparseness due to high morphological variation in surface forms. The question is therefore, given our finite, and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of stat"
W10-1401,H91-1060,0,0.0333452,"Missing"
W10-1401,E03-1005,0,0.0521711,"Missing"
W10-1401,W06-2920,0,0.219462,"red MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. While dependency parsers are adaptable to many languages, as reflected in the multiplicity of the languages covered,1 the analysis by Nivre et al. (2007b) shows that the best result was obtained for English, followed by Catalan, and that the most difficult languages to parse were Arabic, Basque, and Greek. Nivre et al. (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse. This was shown to be the case for both MaltParser (Nivre e"
W10-1401,W10-1409,1,0.834713,"and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of"
W10-1401,W08-2102,0,0.0218025,"Missing"
W10-1401,A00-2018,0,0.0303149,"Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologic"
W10-1401,P00-1058,0,0.0168692,"Missing"
W10-1401,W10-1406,0,0.0567829,"Missing"
W10-1401,P99-1065,0,0.261618,"Missing"
W10-1401,P97-1003,0,0.183573,"r) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases. German, an Indo-European language closely related to English, already exhibits some of the properties that make parsing MRLs problematic. The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lex"
W10-1401,H05-1100,0,0.0111384,"ful in parsing English are necessarily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 o"
W10-1401,2008.jeptalnrecital-long.17,1,0.829354,"Missing"
W10-1401,P03-1013,0,0.0556766,"rted to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s"
W10-1401,P05-1039,0,0.0235903,"cal forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not"
W10-1401,P08-1109,0,0.0528707,"Missing"
W10-1401,W10-1412,1,0.240586,"various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of the SPMRL workshop for addressing such challenges. 4 Parsing MRLs: Recurring Trends The first workshop on parsing MRLs features 11"
W10-1401,E09-1038,1,0.7929,"isambiguate the morphological analyses of input forms? Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming that the input to our system reflects morphological information, one way or another, which types of morpho2 Most studies on parsing MRLs nowadays assume the gold standard segmentation and disambiguated morphological information as input. This is the case, for instance, for the Arabic parsing at CoNLL 2007 (Nivre et al., 2007a). This practice deludes the community as to the validity of the parsing results reported for MRLs in shared tasks. Goldberg et al. (2009), for instance, show a gap of up to 6pt F1 -score between performance on gold standard segmentation vs. raw text. One way to overcome this is to devise joint morphological and syntactic disambiguation frameworks (cf. (Goldberg and Tsarfaty, 2008)). logical information should we include in the parsing model? Inflectional and/or derivational? Case information and/or agreement features? How can valency requirements reflected in derivational morphology affect the overall syntactic structure? In tandem with the decision concerning the morphological information to include, we face genuine challenges"
W10-1401,W05-0303,0,0.043074,"Missing"
W10-1401,P08-1067,0,0.0516751,"Missing"
W10-1401,P03-1054,0,0.00472369,"rphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s parser emulation of Collins’ model 2 (Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (200"
W10-1401,W06-1614,0,0.154599,"Missing"
W10-1401,P95-1037,0,0.0299787,"Missing"
W10-1401,W10-1407,0,0.0148488,"elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance. In addition to free word order, MRLs show higher degree of freedom in extraposition. Both of these phenomena can result in discontinuous structures. In constituency-based treebanks, this is either annotated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that th"
W10-1401,J93-2004,0,0.0355629,". We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 1 Introduction The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers. The development of successful statistical parsing models for English focused on the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1993)) as the primary, and sometimes only, resource. Since the initial release of the Penn Treebank (PTB Marcus et Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks’ annotation schemes, and inadequacy of the widely used PARS E VAL evaluation metrics. None of these aspects in isolation can account for the systematic performance deterioration, but observed from a wider, crosslinguistic perspective, a picture begins to emerge – that the morphologically rich nature of some of the languages makes them inherently more s"
W10-1401,W10-1402,0,0.0399505,"Missing"
W10-1401,E06-1011,0,0.0266021,"labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the"
W10-1401,P05-1012,0,0.11798,"Missing"
W10-1401,P05-1013,0,0.0312259,"how (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with medium"
W10-1401,P09-1040,0,0.0260548,"D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare"
W10-1401,N07-1051,0,0.0143897,"markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on"
W10-1401,P06-1055,0,0.0941097,"tained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representatio"
W10-1401,D07-1066,1,0.534932,"Missing"
W10-1401,P81-1022,0,0.830446,"Missing"
W10-1401,W07-2219,1,0.909816,"Missing"
W10-1401,C08-1112,1,0.709478,"Missing"
W10-1401,W10-1405,1,0.846235,"Missing"
W10-1401,W09-3820,1,0.856092,"e other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare/unknown words is increased. One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflects their shared morphological and morphosyntactic aspects. This was done, for instance, in previous work on parsing German. Versley and Rehbein (2009) cluster words according to linear context features. These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes. The clusters are then integrated as features in a discriminative parsing model to cope with unknown words. Their discriminative model thus obtains state-of-the-art results on parsing German. 8 Several contribution address similar challenges. For constituency-based generative parsers, the simple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al., 2010; Candito and"
W10-1401,W10-1411,0,\N,Missing
W10-1401,W10-1403,0,\N,Missing
W10-1401,W10-1410,1,\N,Missing
W10-1401,W08-1008,0,\N,Missing
W10-1401,P05-1022,0,\N,Missing
W10-1401,P08-1043,1,\N,Missing
W10-1401,D07-1096,0,\N,Missing
W10-1412,W09-3819,1,0.87354,"hadad, 2010) that this rich conditioning can be especially beneficial in situations where informative structural information is available, such as in morphologically rich languages. In this paper, we investigate the non-directional easy-first parser performance on Modern Hebrew, a semitic language with rich morphology, relatively free constituent order, and a small treebank compared to English. We are interested in two main questions: (a) how well does the non-directional parser perform on Hebrew data? and (b) can the parser make effective use of morphological features, such as agreement? In (Goldberg and Elhadad, 2009), we describe a newly created Hebrew dependency treebank, and 103 Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103–107, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics report results on parsing this corpus with both M ALT PARSER and first- and second- order variants of M ST PARSER. We find that the secondorder M ST PARSER outperforms the first order M STPARSER, which in turn outperforms the transition based M ALT PARSER. In addition, adding morphological information to the default configura"
W10-1412,N10-1115,1,0.847625,"provement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used. 1 Introduction Data-driven Dependency Parsing algorithms are broadly categorized into two approaches (K¨ubler et al., 2009). Transition based parsers traverse the sentence from left to right1 using greedy, local inference. Graph based parsers use global inference and seek a tree structure maximizing some scoring function defined over trees. This scoring function is usually decomposed over tree edges, or pairs of such edges. In recent work (Goldberg and Elhadad, 2010), we proposed another dependency parsing approach: Easy First, Non-Directional dependency ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University 1 Strictly speaking, the traversal order is from start to end. This distinction is important when discussing Hebrew parsing, as the Hebrew language is written from right-to-left. We keep the left-to-right terminology throughout this paper, as this is the common terminology. However, “left” and “right” should be interpreted as “start” and “end” respectively. Similarly, “a token to the left” should be interpreted"
W10-1412,P08-1085,1,0.824381,"the development set, Section 1 (sentences 0-483). As in (Goldberg and Elhadad, 2009), we do not evaluate on the test set in this work. The data in the treebank is segmented and POStagged. Both the parsing models were trained on the gold-standard segmented and tagged data. When evaluating the parsing models, we perform two sets of evaluations. The first one is an oracle experiment, assuming gold segmentation and tagging is available. The second one is a real-world experiment, in which we segment and POS-tag the testset sentences using the morphological disambiguator described in (Adler, 2007; Goldberg et al., 2008) prior to parsing. 105 Parsers and parsing models We use our freely available implementation3 of the non-directional parser. Evaluation Measure We evaluate the resulting parses in terms of unlabeled accuracy – the percent of correctly identified (child,parent) pairs4 . To be precise, we calculate: number of correctly identif ied pairs number of pairs in gold parse For the oracle case in which the gold-standard token segmentation is available for the parser, this is the same as the traditional unlabeled-accuracy evaluation metric. However, in the real-word setting in which the token segmentatio"
W10-1412,P05-1012,0,0.0758202,"interpreted as “the previous token”. parsing. Like transition based methods, the easyfirst method adopts a local, greedy policy. However, it abandons the strict left-to-right processing order, replacing it with an alternative order, which attempts to make easier attachments decisions prior to harder ones. The model was applied to English dependency parsing. It was shown to be more accurate than M ALT PARSER, a state-of-the-art transition based parser (Nivre et al., 2006), and near the performance of the first-order M ST PARSER, a graph based parser which decomposes its score over tree edges (McDonald et al., 2005), while being more efficient. The easy-first parser works by making easier decisions before harder ones. Each decision can be conditioned by structures created by previous decisions, allowing harder decisions to be based on relatively rich syntactic structure. This is in contrast to the globally optimized parsers, which cannot utilize such rich syntactic structures. It was hypothesized in (Goldberg and Elhadad, 2010) that this rich conditioning can be especially beneficial in situations where informative structural information is available, such as in morphologically rich languages. In this pa"
W10-1412,nivre-etal-2006-maltparser,0,0.044069,"on terminology. However, “left” and “right” should be interpreted as “start” and “end” respectively. Similarly, “a token to the left” should be interpreted as “the previous token”. parsing. Like transition based methods, the easyfirst method adopts a local, greedy policy. However, it abandons the strict left-to-right processing order, replacing it with an alternative order, which attempts to make easier attachments decisions prior to harder ones. The model was applied to English dependency parsing. It was shown to be more accurate than M ALT PARSER, a state-of-the-art transition based parser (Nivre et al., 2006), and near the performance of the first-order M ST PARSER, a graph based parser which decomposes its score over tree edges (McDonald et al., 2005), while being more efficient. The easy-first parser works by making easier decisions before harder ones. Each decision can be conditioned by structures created by previous decisions, allowing harder decisions to be based on relatively rich syntactic structure. This is in contrast to the globally optimized parsers, which cannot utilize such rich syntactic structures. It was hypothesized in (Goldberg and Elhadad, 2010) that this rich conditioning can b"
W10-1412,P06-1084,1,\N,Missing
W10-2927,nivre-etal-2006-maltparser,0,0.0331188,"were used to train the Boosting algorithm and find structural predictors candidates. Sections 4-7 were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the weights are tuned to be used as part of a linear classifier. In our application, we disregard the boosting weights, and instead rank the predictors based on their n"
W10-2927,W04-0308,0,0.0816925,"Missing"
W10-2927,J08-4003,0,0.0318264,"e seen in Table 1, the resulting parsers are still accurate. 4 http://sourceforge.net/projects/mstparser/ 5 http://maltparser.org/ 237 M ST1 88.8 M ST2 89.8 A RC E 87.6 A RC S 87.4 are produced by the parser more often than they appear in the language).8 Specifically, we manually inspected the predictors where the ratio between language and parser was high, ranked by absolute number of occurrences. Table 1: Unlabeled accuracies of the analyzed parsers Parser M ST1 M ST2 A RC E A RC S Train Accuracy 65.4 62.8 69.2 65.1 Val Accuracy 57.8 56.6 65.3 60.1 4 We analyze two transition-based parsers (Nivre, 2008). The parsers differ in the transition systems they adopt. The A RC E system makes use of a transition system with four transitions: L EFT,R IGHT,S HIFT,R EDUCE. The semantics of this transition system is described in (Nivre, 2004). The A RC S system adopts an alternative transition system, with three transitions: ATTACH L,ATTACH R,S HIFT . The semantics of the system is described in (Huang et al., 2009). The main difference between the systems is that the A RC E system makes attachments as early as possible, while the A RC S system should not attach a parent to its dependent until the depende"
W10-2927,W06-2920,0,0.0606229,"parsers, and first- and second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses. 1 Introduction Dependency Parsing, the task of inferring a dependency structure over an input sentence, has gained a lot of research attention in the last couple of years, due in part to to the two CoNLL shared tasks (Nivre et al., 2007; Buchholz and Marsi, 2006) in which various dependency parsing algorithms were compared on various data sets. As a result of this research effort, we have a choice of several robust, efficient and accurate parsing algorithms. ∗ We would like to thank Reut Tsarfaty for comments and discussions that helped us improve this paper. This work is supported in part by the Lynn and William Frankel Center for Computer Science. 234 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 234–242, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics M ST Parser. In wh"
W10-2927,N10-1049,0,0.0292755,"e pose the following question: Assuming we are given two parses of the same sentence. Can we tell, by looking at the parses and without knowing the correct parse, which parser produced which parse? Any predictor which can help in answering this question is an indicator of a structural bias. Structural Bias Language is a highly structured phenomena, and sentences exhibit structure on many levels. For example, in English sentences adjectives appear before nouns, subjects tend to appear before their verb, and syntactic trees show a tendency toward right-branching structures.1 1 As noted by (Owen Rambow, 2010), there is little sense in talking about the structure of a language without referring to a specific annotation scheme. In what follow, we assume a fixed annotation strategy is chosen. 235 Definition: structural bias between sets of trees Given two sets of parse trees, A and B, over the same sentences, a structural bias between these sets is the collection of all predictors which can help us decide, for a tree t, whether it belongs to A or to B. 3 (a) (b) NN VB IN/with Figure 1: Structural Elements Examples. (a) is an adjective with a parent 3 words to its right. (b) is a verb whose parent is"
W10-2927,P10-1075,0,0.105891,"the strengths, weaknesses and inner working of the parser. In Section 2.2 we propose a Boosting-based algorithm for uncovering these structural biases. Then, in Section 3 we go on to apply our analysis methodology to four parsing systems for English: two transition-based systems and two graph-based systems (Sections 4 and 5). The analysis shows that the different parsing systems indeed possess different biases. Furthermore, the analysis highlights the differences and commonalities among the different parsers, and sheds some more light on the specific behaviours of each system. Recent work by Dickinson (2010), published concurrently with this one, aims to identify dependency errors in automatically parsed corpora by inspecting grammatical rules which appear in the automatically parsed corpora and do not fit well with the grammar learned from a manually annotated treebank. While Dickinson’s main concern is with automatic identification of errors rather than characterizing parsers behaviour, we feel that his work shares many intuitions with this one: automatic parsers fail in predictable ways, those ways can be analyzed, and this analysis should be carried out on structures which are larger than sin"
W10-2927,N06-2033,0,0.0976003,"Missing"
W10-2927,D09-1127,0,0.0147348,"were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the weights are tuned to be used as part of a linear classifier. In our application, we disregard the boosting weights, and instead rank the predictors based on their number of occurrences in a validation set. We seek predictors which appear many times in one tree-se"
W10-2927,W07-2416,0,0.0256553,"s encoded in the node name, while the optional lexical item and distance to parent are encoded as daughters. 3 3.1 Experimental Setup In what follows, we analyze and compare the structural biases of 4 parsers, with respect to a dependency representation of English. tive branch-and-bound technique for efficiently searching for the maximum gain tree at each round. The reader is referred to their paper for the details. Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank (Marcus et al., 1993) to dependency structure using the procedure described in (Johansson and Nugues, 2007). We use the Mel’ˇcuk encoding of coordination structure, in which the first conjunct is the head of the coordination structure, the coordinating conjunction depends on the head, and the second conjunct depend on the coordinating conjunction (Johansson, 2008). Structural elements as subtrees The boosting algorithm works on labeled, ordered trees. Such trees are different than dependency trees in that they contain information about nodes, but not about edges. We use a simple transformation to encode dependency trees and structural elements as labeled, ordered trees. The transformation works by"
W10-2927,W04-3239,0,0.0308097,"ith information about the incoming edge to the root of the subtree. Examples of such structural elements are given in Figure 1. This class of predictors is not complete – it does not directly encode, for instance, information about the number of siblings Boosting Algorithm with Subtree Features The number of possible predictors is exponential in the size of each tree, and an exhaustive search is impractical. Instead, we solve the search problem using a Boosting algorithm for tree classification using subtree features. The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). We briefly describe the main idea behind the algorithm. The Boosting algorithm with subtree features gets as input two parse sets with labeled, ordered trees. The output of the algorithm is a set of subtrees ti and their weights wi . These weighted subtrees P define a linear classifier over trees f (T ) = ti ∈T wi , where f (T ) > 0 for trees in set A and f (T ) &lt; 0 for trees in set B. The algorithm works in rounds. Initially, all input trees are given a uniform weight. At each round, the algorithm seeks a subtree t with a maximum gain, that is the subtree that classifies correctly the subse"
W10-2927,J93-2004,0,0.0421062,"he tree encodings of the structural elements in Figure 1. Direction to parent is encoded in the node name, while the optional lexical item and distance to parent are encoded as daughters. 3 3.1 Experimental Setup In what follows, we analyze and compare the structural biases of 4 parsers, with respect to a dependency representation of English. tive branch-and-bound technique for efficiently searching for the maximum gain tree at each round. The reader is referred to their paper for the details. Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank (Marcus et al., 1993) to dependency structure using the procedure described in (Johansson and Nugues, 2007). We use the Mel’ˇcuk encoding of coordination structure, in which the first conjunct is the head of the coordination structure, the coordinating conjunction depends on the head, and the second conjunct depend on the coordinating conjunction (Johansson, 2008). Structural elements as subtrees The boosting algorithm works on labeled, ordered trees. Such trees are different than dependency trees in that they contain information about nodes, but not about edges. We use a simple transformation to encode dependency"
W10-2927,D07-1013,0,0.036424,"and how to blend and stack their outputs, little effort was directed toward understanding the behavior of different parsing systems in terms of structures they produce and errors they make. Question such as which linguistic phenomena are hard for parser Y? and what kinds of errors are common for parser Z?, as well as the more ambitious which parsing approach is most suitable to parse language X?, remain largely unanswered. The current work aims to fill this gap by proposing a methodology to identify systematic biases in various parsing models and proposing and initial analysis of such biases. McDonald and Nivre (2007) analyze the difference between graph-based and transition-based parsers (specifically the M ALT and M ST parsers) by comparing the different kinds of errors made by both parsers. They focus on single edge errors, and learn that M ST is better for longer dependency arcs while M ALT is better on short dependency arcs, that M ALT is better than M ST in predicting edges further from the root and vice-versa, that M ALT has a slight advantage when predicting the parents of nouns and pronouns, and that M ST is better at all other word categories. They also conclude that the greedy M ALT Parser suffe"
W10-2927,E06-1011,0,0.0195033,"nal. 2.3 Biases in Dependency Parsers Data Sections 15-18 were used for training the parsers3 . The first 4,000 sentences from sections 10-11 were used to train the Boosting algorithm and find structural predictors candidates. Sections 4-7 were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the weights are tuned to be used as"
W10-2927,P05-1012,0,0.0382478,"lexical item are optional. 2.3 Biases in Dependency Parsers Data Sections 15-18 were used for training the parsers3 . The first 4,000 sentences from sections 10-11 were used to train the Boosting algorithm and find structural predictors candidates. Sections 4-7 were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the we"
W10-2927,P08-1108,0,0.0471104,"Missing"
W10-2927,D07-1096,0,\N,Missing
W12-3308,D07-1119,0,0.0557477,"Missing"
W12-3308,de-marneffe-etal-2006-generating,0,0.0148493,"Missing"
W12-3308,D07-1112,0,0.0478353,"Missing"
W12-3308,D08-1094,0,0.0177877,"ective and headprep-pobj. 3.5 Effect on NER Since most of the improvement comes from the N-N relation, we expect improvement for downstream applications such as Named Entity Recognition, a basic task frequently used in the biomedical domain. 47 Preference of predicate-argument pairs has been studied in depth with a number of approaches. Resnik (1993) suggested a class-based model for preference of predicates combining WordNet classes with mutual information techniques for associating an argument with a predicate class from WordNet. Another approach models words in a corpus as context vectors (Erk and Pado 2008; Turney and Pantel 2010) for discovering predicate or argument classes using large corpora or the Web. Recently, semantic classes were successfully induced using LDA topic modeling. These methods have shown success in modeling verb argument relationship to a single predicate (Ritter, Mausam et al. 2010) or a predicate pair (Séaghdha 2010), as well as for adjective-noun preference (Hartung and Frank 2011). 4.2 Learning SP for improving dependency parsing The argument-predicate choice learned in SP is directly related to the decision of creating an edge between them in a parse tree. Van Noord ("
W12-3308,N10-1115,1,0.843576,"ween the news and bio-medical domains. 2.2 Co-training to exploit domain features At this stage, we have acquired a domain-specific model of word affinity that exploits semantic classes and depends on specific syntactic configurations (head-prep-obj and noun-adj). We now attempt to exploit this model to adapt our source parser to the target domain. To this end, we want to re-train the parser using new features based on the SP model in addition to the original features. We use the framework of co-training to achieve this goal (Sagae 45 and Tsujii 2007): we use two different parsers: EasyFirst (Goldberg and Elhadad 2010) and MALT (Nivre, Hall et al. 2006) trained on the same WSJ source domain. We apply these two parsers on a large set of target-domain sentences. We select those sentences where the 2 parsers agree (produce identical trees) and add them to the original source-domain training set. We thus obtain an extended training set with many indomain samples. We can now re-train the parser using the new SP features. 2.3 SP as features for the Easy First parser We use the deterministic non-directional Easy-First parser for re-training. This parser incrementally adds edges between words starting with the easi"
W12-3308,D11-1050,0,0.0366243,"Missing"
W12-3308,W04-1213,0,0.129263,"Missing"
W12-3308,P08-1068,0,0.142253,"Missing"
W12-3308,H05-1066,0,0.141723,"Missing"
W12-3308,H05-1105,0,0.0709934,"Missing"
W12-3308,nivre-etal-2006-maltparser,0,0.0746835,"Missing"
W12-3308,W07-2201,0,0.250214,"Missing"
W12-3308,W11-1812,0,0.0242072,"Missing"
W12-3308,P10-1044,0,0.306555,"Missing"
W12-3308,D07-1111,0,0.0410453,"Missing"
W12-3308,P10-1045,0,0.1797,"pecific model of word-pairs affinities. Our parsing model (EasyFirst) allows us to use such bilexical features in an efficient manner. Because of data sparseness, however, we aim to acquire classbased features, and decide to model these lexical preferences using the LDA approach. Our method proceeds in two stages: 1. Learn selectional preferences from an automatically parsed corpus using LDA on selected syntactic configurations 2. Integrate the preferences into the parsing model as new features using co-training. 2.1 Learning Selectional Preferences Following (Ritter, Mausam et al. 2010) and (Séaghdha 2010), we model lexical affinity between words in specific syntactic configurations using LDA. Traditionally, LDA learns a set of ""topics"" from observed documents, based on observed word cooccurrences. In our case, we form artificial documents, which we call syntactic contexts, by collecting headdaughter pairs from parse trees. A syntactic context is constructed for each head word, which contains the related words to which it was found attached. In the collection process, we identify two syntactic configurations that yield high error rates: head-prepnoun and noun-adj. We collect two types of syntac"
W12-3308,I05-2038,0,0.090945,"Missing"
W12-3308,P11-1156,0,0.0398631,"Missing"
W12-3308,W09-1401,0,\N,Missing
W12-3308,D07-1096,0,\N,Missing
W12-3308,N06-1020,0,\N,Missing
W13-3518,C12-1078,0,0.0416419,"Missing"
W13-3518,N07-1049,0,0.0168156,"The accuracy improvement on Hungarian and Arabic did not meet our significance threshold. The nonmonotonic transitions did not decrease accuracy significantly on any of the languages. 9 Related Work One can view our non-monotonic parsing system as adding “repair” operations to a greedy, deterministic parser, allowing it to undo previous decisions and thus mitigating the effect of incorrect parsing decisions due to uncertain future, which is inherent in greedy left-to-right transition-based parsers. Several approaches have been taken to address this problem, including: Post-processing Repairs (Attardi and Ciaramita, 2007; Hall and Nov´ak, 2005; Inokuchi and Yamaoka, 2012) Closely related to stacking, this line of work attempts to train classifiers to repair attachment mistakes after a parse is proposed by a parser by changing head attachment decisions. The present work differs from these by incorporating the repair process into the transition system. Stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a second-stage parser runs over the sentence using the predictions of the first parser as features. In contrast our parser works in 169 System Baseline NM L+D AR 83.4 83.6 BASQ 76.2 76.1 C AT 91."
W13-3518,P10-1001,0,0.0210397,"532 FN 285 250 535 Table 2: True/False positive/negative rates for the prediction of the non-monotonic transitions. The non-monotonic transitions add correct dependencies 112 times, and produce worse parses 40 times. 535 opportunities for non-monotonic transitions were missed. System K&C 10 Z&N 11 G&N 12 Baseline(G&N-12) NM L+D O n3 nk n n n Stanford Penn2Malt LAS UAS LAS UAS — 91.9 88.72 88.7 88.9 — 93.5 90.96 90.9 91.1 — 91.8 — 88.7 88.9 93.00 92.9 — 90.6 91.0 Table 3: WSJ 23 test results, with comparison against the state-of-the-art systems from the literature of different runtimes. K&C 10=Koo and Collins (2010); Z&N 11=Zhang and Nivre (2011); G&N 12=Goldberg and Nivre (2012). achieved with a purely greedy system, with a statistically significant improvement over G&N 12. CoNLL 2007 evaluation. Table 4 shows the effect of the non-monotonic transitions across the ten languages in the CoNLL 2007 data sets. Statistically significant improvements in accuracy were observed for five of the ten languages. The accuracy improvement on Hungarian and Arabic did not meet our significance threshold. The nonmonotonic transitions did not decrease accuracy significantly on any of the languages. 9 Related Work One can"
W13-3518,P11-1068,0,0.178617,"Missing"
W13-3518,J13-1002,0,0.0405808,"nd the buffer hold the words of a sentence, and the set of arcs represent derived dependency relations. We use a notation in which the stack items are indicated by Si , with S0 being the top of the stack, S1 the item previous to it and so on. Similarly, buffer items are indicated as Bi , with B0 being the first item on the buffer. The arcs are of the form (h, l, m), indicating a dependency in which the word m modifies the word h with label l. In the initial configuration the stack is empty, and the buffer contains the words of the sentence followed by an artificial ROOT token, as suggested by Ballesteros and Nivre (2013). In the final configuration the buffer is empty and the stack contains the ROOT token. There are four parsing actions (Shift, Left-Arc, Right-Arc and Reduce, abbreviated as S,L,R,D respectively) that manipulate stack and buffer items. The Shift action pops the first item from the buffer and pushes it on the stack (the Shift action has a natural precondition that the buffer is not empty, as well as a precondition that ROOT can only be pushed to an empty stack). The Right-Arc action is similar to the Shift action, but it also adds a dependency arc (S0 , B0 ), with the current top of the stack a"
W13-3518,de-marneffe-etal-2006-generating,0,0.0136933,"Missing"
W13-3518,J93-2004,0,0.045362,"additive. and Nivre (2011). We follow Goldberg and Nivre (2012) in training all models for 15 iterations, and shuffling the sentences before each iteration. Because the sentence ordering affects the model’s accuracy, all results are averaged from scores produced using 20 different random seeds. The seed determines how the sentences are shuffled before each iteration, as well as when to follow an optimal action and when to follow a nonoptimal action during training. The Wilcoxon signed-rank test was used for significance testing. A train/dev/test split of 02-21/22/23 of the Penn Treebank WSJ (Marcus et al., 1993) was used for all models. The data was converted into Stanford dependencies (de Marneffe et al., 2006) with copula-as-head and the original PTB noun-phrase bracketing. We also evaluate our models on dependencies created by the P ENN 2MALT tool, to assist comparison with previous results. Automatically assigned POS tags were used during training, to match the test data more closely. 6 We also evaluate the non-monotonic transitions on the CoNLL 2007 multi-lingual data. 8 Results and analysis We base our experiments on the parser described by Goldberg and Nivre (2012). We began by implementing th"
W13-3518,D08-1017,0,0.0496482,"Missing"
W13-3518,N10-1115,1,0.466464,"e NM L+D AR 83.4 83.6 BASQ 76.2 76.1 C AT 91.5 91.5 C HI 82.3 82.7 CZ 78.8 80.1 E NG 87.9 88.4 GR 81.2 81.8 H UN 77.6 77.9 I TA 83.8 84.1 T UR 78.0 78.0 Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p &gt; 0.4). As we explained in the paper, with the greedy decoding used here additional spurious ambiguity is not necessarily a draw-back. a single, left-to-right pass over the sentence. Non-directional Parsing The EasyFirst parser of Goldberg and Elhadad (2010) tackles similar forms of ambiguities by dropping the Shift action altogether, and processing the sentence in an easyto-hard bottom-up order instead of left-to-right, resulting in a greedy but non-directional parser. The indeterminate processing order increases the parser’s runtime from O(n) to O(n log n). In contrast, our parser processes the sentence incrementally, and runs in a linear time. Beam Search An obvious approach to tackling ambiguities is to forgo the greedy nature of the parser and instead to adopt a beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011) or a dynamic programm"
W13-3518,W03-3017,0,0.644818,"Missing"
W13-3518,C12-1059,1,0.35868,"previously predicted label. This would allow the parser to con165 5 To summarize, our Non-Monotnonic ArcEager system differs from the monotonic Arc-Eager system by: An essential component when training a transition-based parser is an oracle which, given a gold-standard tree, dictates the sequence of moves a parser should make in order to derive it. Traditionally, these oracles are defined as functions from trees to sequences, mapping a gold tree to a single sequence of actions deriving it, even if more than one sequence of actions derives the gold tree. We call such oracles static. Recently, Goldberg and Nivre (2012) introduced the concept of a dynamic oracle, and presented a concrete oracle for the arc-eager system. Instead of mapping a gold tree to a sequence of actions, the dynamic oracle maps a hconfiguration, gold treei pair to a set of optimal transitions. More concretely, the dynamic oracle presented in Goldberg and Nivre (2012) maps haction, configuration, treei tuples to an integer, indicating the number of gold arcs in tree that can be derived from conf iguration by some sequence of actions, but could not be derived after applying action to the configuration. There are two advantages to this. Fi"
W13-3518,J08-4003,0,0.0955009,"ng the parser to recover from the incorrect head assignments which are forced by an incorrect resolution of a Shift/Right-Arc ambiguity. In transition-based parsing, a parser consists of a state (or a configuration) which is manipulated by a set of actions. An action is applied to a state and results in a new state. The parsing process concludes when the parser reaches a final state, at which the parse tree is read from the state. A particular set of states and actions yield a transitionsystem. Our starting point in this paper is the popular Arc-Eager transition system, described in detail by Nivre (2008). The state of the arc-eager system is composed of a stack, a buffer and a set of arcs. The stack and the buffer hold the words of a sentence, and the set of arcs represent derived dependency relations. We use a notation in which the stack items are indicated by Si , with S0 being the top of the stack, S1 the item previous to it and so on. Similarly, buffer items are indicated as Bi , with B0 being the first item on the buffer. The arcs are of the form (h, l, m), indicating a dependency in which the word m modifies the word h with label l. In the initial configuration the stack is empty, and t"
W13-3518,W04-2407,0,0.0194734,", with the current top of the stack as the head of the newly pushed item (the Right action has an additional precondition that the stack is not empty).1 The Left-Arc action adds a dependency arc (B0 , S0 ) with the first item in the buffer as the head of the top of the stack, and pops the stack (with a precondition that the stack and buffer are not empty, and that S0 is not assigned a head yet). Finally, the Reduce action pops the stack, with a precondition that the stack is not empty and that S0 is already assigned a head. 3 The Non-Monotonic Arc-Eager System The Arc-Eager transition system (Nivre et al., 2004) has four moves. Two of them create dependencies, two push a word from the buffer to the stack, and two remove an item from the stack: Push Pop Adds dependency Right-Arc Left-Arc No new dependency Shift Reduce Every word in the sentence is pushed once and popped once; and every word must have exactly one head. This creates two pairings, along the diagonals: (S, L) and (R, D). Either the push move adds the head or the pop move does, but not both and not neither. 1 For labelled dependency parsing, the Right-Arc and Left-Arc actions are parameterized by a label L such that the action RightL adds"
W13-3518,P08-1108,0,0.00787616,"ffect of incorrect parsing decisions due to uncertain future, which is inherent in greedy left-to-right transition-based parsers. Several approaches have been taken to address this problem, including: Post-processing Repairs (Attardi and Ciaramita, 2007; Hall and Nov´ak, 2005; Inokuchi and Yamaoka, 2012) Closely related to stacking, this line of work attempts to train classifiers to repair attachment mistakes after a parse is proposed by a parser by changing head attachment decisions. The present work differs from these by incorporating the repair process into the transition system. Stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a second-stage parser runs over the sentence using the predictions of the first parser as features. In contrast our parser works in 169 System Baseline NM L+D AR 83.4 83.6 BASQ 76.2 76.1 C AT 91.5 91.5 C HI 82.3 82.7 CZ 78.8 80.1 E NG 87.9 88.4 GR 81.2 81.8 H UN 77.6 77.9 I TA 83.8 84.1 T UR 78.0 78.0 Table 4: Multi-lingual evaluation. Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trended upward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p &gt; 0.4). As we explained in the paper, with the"
W13-3518,D08-1059,0,0.0194823,"ctional Parsing The EasyFirst parser of Goldberg and Elhadad (2010) tackles similar forms of ambiguities by dropping the Shift action altogether, and processing the sentence in an easyto-hard bottom-up order instead of left-to-right, resulting in a greedy but non-directional parser. The indeterminate processing order increases the parser’s runtime from O(n) to O(n log n). In contrast, our parser processes the sentence incrementally, and runs in a linear time. Beam Search An obvious approach to tackling ambiguities is to forgo the greedy nature of the parser and instead to adopt a beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011) or a dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011) approach. While these approaches are very successful in producing highaccuracy parsers, we here explore what can be achieved in a strictly deterministic system, which results in much faster and incremental parsing algorithms. The use of non-monotonic transitions in beam-search parser is an interesting topic for future work. 10 The conventional training procedure for transition-based parsers uses a “static” oracle based on “gold” parses that never predicts a non-monotonic transition, so it is clearl"
W13-3518,P11-2033,0,0.203041,"itions on labelled and unlabelled attachment score on the development data. All results are averages from 20 models trained with different random seeds, as the ordering of the sentences at each iteration of the Perceptron algorithm has an effect on the system’s accuracy. The two non-monotonic transitions each bring small but statistically significant improvements that are additive when combined in the NM L+D system. The result is stable 5 If using a labeled reduce transition, the label assignment costs should be handled here. 6 We thank Yue Zhang for supplying the POS-tagged files used in the Zhang and Nivre (2011) experiments. 7 Experiments 168 across both dependency encoding schemes. Frequency analysis. Recall that there are two pop moves available: Left-Arc and Reduce. The LeftArc is considered non-monotonic if the top of the stack has a head specified, and the Reduce move is considered non-monotonic if it does not. How often does the parser select monotonic and nonmonotonic pop moves, and how often is its decision correct? In Table 2, the True Positive column shows how often non-monotonic transitions were used to add gold standard dependencies. The False Positive column shows how often they were use"
W13-3518,W05-1505,0,\N,Missing
W13-3518,P10-1110,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W13-5709,P05-1022,0,0.181479,"but instead of working with a fixed corpus D of hx, yi pairs, the set D is generated dynamically based on states x the parser is likely to reach, and the optimal actions Y = oracle(x; T ) proposed for these states by the dynamic oracle. In practice, training with exploration using the dynamic oracle yields substantial improvements in parsing accuracies across treebanks. this, we change the numerator to sum over the elements y ∈ Y , resulting in the following model form (the same approach was taken by Riezler et al. (2002) for dealing with latent LFG derivations in LFG parser training, and by Charniak and Johnson (2005) in the context of discriminative reranking): L(D; w) = hx,Y i∈D 3 Training of Sparse Probabilistic Classifiers As discussed in the introduction, our aim is to replace the averaged-perceptron learner and adapt the training with exploration method of (Goldberg and Nivre, 2012) to produce classifiers that provide probabilistic output. 3.1 Probabilistic Objective Function and Loss Our first step is to replace the perceptron hinge-loss objective with an objective based on log-likelihood. As discussed in section 2.5 the training corpus is viewed as a bag of states and their associated actions, and"
W13-5709,P13-1104,0,0.0314181,"is then: loss(Y, x; w) = log X y∈Y ew·φ(x,y) − log X ew·φ(x,y) y∈Y and the gradient of this loss with respect to w is: X ew·φ(x,y) ∂loss X ew·φ(x,y) = φi (x, y)− φi (x, y) ∂wi ZY ZY y∈Y y∈Y where: ZY = X y 0 ∈Y 0 ew·φ(x,y ) ZY = X 0 ew·φ(x,y ) y 0 ∈Y 3.2 L1 Regularized Training with AdaGrad-RDA The generation of the training set used in the trainingwith-exploration procedure calls for an online optimization algorithm. Given the objective function and its gradient, we could have used a stochastic gradient based method to optimize the objective. However, recent work in NLP (Green et al., 2013; Choi and McCallum, 2013) demonstrated that the adaptivegradient (AdaGrad) optimization framework of Duchi et al. (2010) converges quicker and produces superior 85 results in settings which have a large number of training instances, each with a very high-dimensional but sparse feature representation, as common in NLP and in dependency-parsing in particular. Moreover, a variant of the AdaGrad algorithm called AdaGrad-RDA incorporates an L1 regularization, and produces sparse, regularized models. Regularization is important in our setting for two reasons: first, we would prefer our model to not overfit accidental featur"
W13-5709,C12-1059,1,0.217014,"probabilistic output. 1 Introduction For dependency parsing, it is well established that greedy transition-based parsers (Nivre, 2008) are very fast (both empirically and theoretically) while still providing relatively high parsing accuracies (Nivre et al., 2007; K¨ubler et al., 2009). Recently, it has been shown that by moving from static to dynamic oracles during training, together with a training method based on the averagedperceptron, greedy parsers can become even more accurate. The accuracy gain comes without any speed penalty at parsing time, as the inference procedure remains greedy (Goldberg and Nivre, 2012). In transition-based parsing, the parsing task is viewed as performing a series of actions, which result in an incremental construction of a parse-tree. At each step of the parsing process, a classification model is used to assign a score to each of the possible actions, and the highest-scoring action is chosen and applied. When using perceptron based training, the action scores are in the range (−∞, ∞), and the only guarantee is that the highest-scoring action should be considered “best”. Nothing can be inferred from the scale of the highest-scoring action, as well as from the scores assigne"
W13-5709,Q13-1033,1,0.856807,"greedy parser, a classifier is used to choose the action to take in each state, based on features extracted from the state itself. Transition systems differ by the way they define states, and by the particular set of transitions available. One such system is the ArcEager system (Nivre, 2003), which has 4 action types, S HIFT, R EDUCE, L EFTlb , R IGHTlb , where the last two are parameterized by a dependency label lb, resulting in 2+2L actions for a treebank with L distinct arc labels. The system parses a sentence with n words in 2n actions. The reader is referred to (Nivre, 2003; Nivre, 2008; Goldberg and Nivre, 2013) for further details on this system. 2.2 Greedy parsing algorithm Assuming we have a function score(x, y; w) parameterized by a vector w and assigning scores to pairs of states x and actions y, greedy transition-based parsing is simple and efficient using Algorithm 1. Starting with the initial state for a given sentence, we repeatedly choose the highest-scoring action according to our model parameters w and apply it, until we reach Algorithm 1 Greedy transition-based parsing 1: Input: sentence S, parameter-vector w 2: x ← I NITIAL(S) 3: while not T ERMINAL(x) do 4: y ← arg maxy∈L EGAL(x) score"
W13-5709,P13-1031,0,0.0221868,"air under this model is then: loss(Y, x; w) = log X y∈Y ew·φ(x,y) − log X ew·φ(x,y) y∈Y and the gradient of this loss with respect to w is: X ew·φ(x,y) ∂loss X ew·φ(x,y) = φi (x, y)− φi (x, y) ∂wi ZY ZY y∈Y y∈Y where: ZY = X y 0 ∈Y 0 ew·φ(x,y ) ZY = X 0 ew·φ(x,y ) y 0 ∈Y 3.2 L1 Regularized Training with AdaGrad-RDA The generation of the training set used in the trainingwith-exploration procedure calls for an online optimization algorithm. Given the objective function and its gradient, we could have used a stochastic gradient based method to optimize the objective. However, recent work in NLP (Green et al., 2013; Choi and McCallum, 2013) demonstrated that the adaptivegradient (AdaGrad) optimization framework of Duchi et al. (2010) converges quicker and produces superior 85 results in settings which have a large number of training instances, each with a very high-dimensional but sparse feature representation, as common in NLP and in dependency-parsing in particular. Moreover, a variant of the AdaGrad algorithm called AdaGrad-RDA incorporates an L1 regularization, and produces sparse, regularized models. Regularization is important in our setting for two reasons: first, we would prefer our model to not"
W13-5709,P13-2017,1,0.882722,"Missing"
W13-5709,W03-3017,0,0.113536,"o a state as y(x). When parsing, the system is initialized to an initial state based on the input sentence S, to which actions are applied repeatedly. After a finite (in our case linear) number of action applications, the system arrives at a terminal state, and a parse tree is read off the terminal configuration. In a greedy parser, a classifier is used to choose the action to take in each state, based on features extracted from the state itself. Transition systems differ by the way they define states, and by the particular set of transitions available. One such system is the ArcEager system (Nivre, 2003), which has 4 action types, S HIFT, R EDUCE, L EFTlb , R IGHTlb , where the last two are parameterized by a dependency label lb, resulting in 2+2L actions for a treebank with L distinct arc labels. The system parses a sentence with n words in 2n actions. The reader is referred to (Nivre, 2003; Nivre, 2008; Goldberg and Nivre, 2013) for further details on this system. 2.2 Greedy parsing algorithm Assuming we have a function score(x, y; w) parameterized by a vector w and assigning scores to pairs of states x and actions y, greedy transition-based parsing is simple and efficient using Algorithm 1"
W13-5709,J08-4003,0,0.795403,"ence Department Bar Ilan University Ramat Gan, Israel yoav.goldberg@gmail.com Abstract We adapt the dynamic-oracle training method of Goldberg and Nivre (2012; 2013) to train classifiers that produce probabilistic output. Evaluation of an Arc-Eager parser on 6 languages shows that the AdaGrad-RDA based training procedure results in models that provide the same high level of accuracy as the averagedperceptron trained models, while being sparser and providing well-calibrated probabilistic output. 1 Introduction For dependency parsing, it is well established that greedy transition-based parsers (Nivre, 2008) are very fast (both empirically and theoretically) while still providing relatively high parsing accuracies (Nivre et al., 2007; K¨ubler et al., 2009). Recently, it has been shown that by moving from static to dynamic oracles during training, together with a training method based on the averagedperceptron, greedy parsers can become even more accurate. The accuracy gain comes without any speed penalty at parsing time, as the inference procedure remains greedy (Goldberg and Nivre, 2012). In transition-based parsing, the parsing task is viewed as performing a series of actions, which result in a"
W13-5709,P02-1035,0,0.0476255,"es that the hx, yi pairs are independent from each other given the 84 feature representation, but instead of working with a fixed corpus D of hx, yi pairs, the set D is generated dynamically based on states x the parser is likely to reach, and the optimal actions Y = oracle(x; T ) proposed for these states by the dynamic oracle. In practice, training with exploration using the dynamic oracle yields substantial improvements in parsing accuracies across treebanks. this, we change the numerator to sum over the elements y ∈ Y , resulting in the following model form (the same approach was taken by Riezler et al. (2002) for dealing with latent LFG derivations in LFG parser training, and by Charniak and Johnson (2005) in the context of discriminative reranking): L(D; w) = hx,Y i∈D 3 Training of Sparse Probabilistic Classifiers As discussed in the introduction, our aim is to replace the averaged-perceptron learner and adapt the training with exploration method of (Goldberg and Nivre, 2012) to produce classifiers that provide probabilistic output. 3.1 Probabilistic Objective Function and Loss Our first step is to replace the perceptron hinge-loss objective with an objective based on log-likelihood. As discussed"
W13-5709,P06-2089,0,0.0280705,"t”. Nothing can be inferred from the scale of the highest-scoring action, as well as from the scores assigned to the other actions. In contrast, we may be interested in a classification model which outputs a proper probability distribution over the possible actions at each step of the process. Such output will allow us to identify uncertain actions, as well as to reason about the various alternatives. Probabilistic output can also be used in situations such as best-first parsing, in which a probabilistic score can be used to satisfy the required “superiority” property of the scoring function (Sagae and Lavie, 2006; Zhao et al., 2013). Classifiers that output probabilities are well established, and are known as maximum-entropy or multinomial logistic regression models. However, their applications in the context of the dynamic-oracle training is not immediate. The two main obstacles are (a) the dynamic oracle may provide more than one correct label at each state while the standard models expect a single correct label, and (b) the exploration procedure used by Goldberg and Nivre (2012; 2013) assumes an online-learning setup, does not take into account the probabilistic nature of the classifier scores, and"
W13-5709,P11-2033,0,0.0493757,") 3: while not T ERMINAL(x) do 4: Y ← oracle(x, T ) w·φ(x,y) 5: P (y|x; w) ← P 0expexp w·φ(x,y 0 ) ∀y ∈ Y y ∈Y 6: if i ≤ k then 7: yˆ ← arg maxy∈Y P (y|x; w) 8: else 9: Sample yˆ according to P (y|x; w) 10: w ← A DAGRAD U PDATE(w, φ, x, Y ) 11: x ← yˆ(x) return w comprising the freely available Google Universal Dependency Treebank (McDonald et al., 2013). In all cases, we trained on the training set and evaluated the models on the dev-set, using gold POS-tags in both test and train time. Non-projective sentences were removed from the training set. In all scenarios, we used the feature set of (Zhang and Nivre, 2011). We compared different training scenarios: training perceptron based models (P ERCEP) and probabilistic models (ME) with static (S T) and dynamic (DYN) oracles. For the dynamic oracles, we varied the parameter k (the number of initial iterations without error exploration). For the probabilistic dynamic-oracle models further compare the sampling-based exploration described in Algorithm 3 with the error-based exploration used for training the perceptron models in Goldberg and Nivre (2012, 2013). All models were trained for 15 iterations. The P ERCEP +DYN models are the same as the models in (Go"
W13-5709,D07-1096,0,\N,Missing
W14-1618,S12-1047,0,0.295943,"ons were created as described in Section 2. The neural embeddings were created using the word2vec software3 accompanying (Mikolov et al., 2013b). We embedded the vocabulary into a 600 dimensional space, using the state-of-the-art skip-gram architecture, the negative-training approach with 15 negative samples (NEG-15), and sub-sampling of frequent words with a parameter of 10−5 . The parameter settings follow (Mikolov et al., 2013b). 4.1 Closed Vocabulary The S EM E VAL dataset contains the collection of 79 semantic relations that appeared in SemEval 2012 Task 2: Measuring Relation Similarity (Jurgens et al., 2012). Each relation is exemplified by a few (usually 3) characteristic word-pairs. Given a set of several dozen target word pairs, which supposedly have the same relation, the task is to rank the target pairs according to the degree in which this relation holds. This can be cast as an analogy question in the following manner: For example, take the Recipient:Instrument relation with the prototypical word pairs king:crown and police:badge. To measure the degree that a target word pair wife:ring has the same relation, we form the two analogy questions “king is to crown as wife is to ring” and “police"
W14-1618,W13-3520,0,0.0621251,"omputer Science Department Bar-Ilan University Ramat-Gan, Israel {omerlevy,yoav.goldberg}@gmail.com Abstract word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pas"
W14-1618,W09-0201,0,0.0290312,"Missing"
W14-1618,P14-2050,1,0.310233,"ach word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mean of 1595 and a median of 415. Another popular choice of context is the syntactic relations the word participates in (Lin, 1998; Pad´o and Lapata, 2007; Levy and Goldberg, 2014). In this paper, we chose the sequential context as it is compatible with the information available to the state-of-the-art neural embedding method we are comparing against. In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doin"
W14-1618,J10-4006,0,0.0150611,"entations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in di"
W14-1618,P14-1023,0,0.254993,"tal cities have embassies and meetups, while immigration is associated with countries). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 11 Discussion Mikolov et al. showed how an unsupervised neural network can represent words in a space that “naturally” encodes relational similarities in the form of vector offsets. This study shows that finding analogies through vector arithmetic is actually a form of balancing word similarities, and that, contrary to the recent findings of Baroni et al. (2014), under certain conditions traditional word similarities induced by explicit representations can perform just as well as neural embeddings on this task. Learning to represent words is a fascinating and important challenge with implications to most current NLP efforts, and neural embeddings in particular are a promising research direction. We believe that to improve these representations we should understand how they work, and hope that the methods and insights provided in this work will help to deepen our grasp of current and future investigations of word representations. Related Work Relation"
W14-1618,P98-2127,0,0.163639,"ectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). Th"
W14-1618,D12-1050,0,0.0117192,"s to the cosine similarity between the vectors. The superscript marks the position of the feature relative to the target word. sumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3C OS M UL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of compositional semantics (Mitchell and Lapata, 2010), where a phrase composed of two or more words is represented by a single vector, computed by a function of its component word vectors. Blacoe and Lapata (2012) compare different arithmetic functions across multiple representations (including embeddings) on a range of compositionality benchmarks. To the best of our knowledge such methods of word vector arithmetic have not been explored for recovering relational similarities in explicit representations. features in the intersection. Table 7 presents the top (most influential) features of each aspect. Many of these features are names of people or places, which appear rarely in our corpus (e.g. Adeliza, a historical queen, and Nzinga, a royal family) but are nonetheless highly indicative of the shared c"
W14-1618,J90-1003,0,0.380721,"hibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions f"
W14-1618,N13-1090,0,0.727908,"d embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pasttense relation in “capture:captured”, “go:went”. Remarkably, Mikolov et al. showed that such relations are ref"
W14-1618,P94-1038,0,0.235412,"rity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus range"
W14-1618,Q13-1029,0,0.0406897,"Missing"
W14-1618,N13-1120,0,0.0138842,"ikolov et al., 2013b). The queen ≈ king − man + woman The recovery of relational similarities using vector arithmetic on RNN-embedded vectors was evaluated on many relations, achieving state-of-the-art results in relational similarity identification tasks ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 171 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 (Mikolov et al., 2013c; Zhila et al., 2013). It was later demonstrated that relational similarities can be recovered in a similar fashion also from embeddings trained with different architectures (Mikolov et al., 2013a; Mikolov et al., 2013b). Explicit Vector Space Representation We adopt the traditional word representation used in the distributional similarity literature (Turney and Pantel, 2010). Each word is associated with a sparse vector capturing the contexts in which it occurs. We call this representation explicit, as each dimension corresponds to a particular context. For a vocabulary V and a set of contexts C, the result is a"
W14-1618,C94-1049,0,0.558866,"Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mea"
W14-1618,J07-2002,0,0.0126484,"Missing"
W14-1618,P93-1024,0,0.31969,"epresenting words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney"
W14-1618,D11-1014,0,0.0202825,"∗ and Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel {omerlevy,yoav.goldberg}@gmail.com Abstract word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexi"
W14-1618,P10-1040,0,0.405608,"e and Explicit Word Representations Omer Levy∗ and Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel {omerlevy,yoav.goldberg}@gmail.com Abstract word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language"
W14-1618,J06-3003,0,0.0985635,", “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pasttense relation in “capture:captured”, “go:went”. Remarkably, Mikolov et al. showed that such relations are reflected in vector offsets between word pairs (apples − apple ≈ cars − car), and that by using simple vector arithmetic one could apply the relation and solve analogy questions of the form “a is to a∗ as b is to —” in which the nature of the relation is hidden. Perhaps the most famous example is that the embedd"
W14-1618,C98-2122,0,\N,Missing
W14-2413,P14-2120,1,0.819635,"he truth status of propositions and author commitment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This d"
W14-2413,Q13-1005,0,0.0299228,"oduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing. 1 Introduction Parsers for semantic formalisms (such as Neodavidsonian (Artzi and Zettlemoyer, 2013) and DRT (Kamp, 1988)) take unstructured natural language text as input, and output a complete semantic representation, aiming to capture the meaning conveyed by the text. We suggest that this task may be effectively separated into a sequential combination of two different tasks. The first of these tasks is syntactic abstraction over phenomena such as expression of tense, negation, modality, and passive versus active voice, which are all either expressed or implied from syntactic structure. The second task is semantic interpretation 66 Proceedings of the ACL 2014 Workshop on Semantic Parsing,"
W14-2413,P98-1013,0,0.127419,"ck of space, include propositional modifiers (e.g., relative clause modifiers), propositional arguments (such as ”John asserted that he will go home”), conditionals, and the canonicalization of passive and active voice. 4 Relation to Other Representations Our proposed representation is intended to serve as a bridging layer between purely syntactic representations such as dependency trees, and semantic oriented applications. In particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed semantic representations (Baker et al., 1998; Kingsbury and Palmer, 2003; Meyers et al., 2004; Carreras and M`arquez, 2005). Compared to dependency-based representations such as Stanford-dependency trees (De Marneffe 1 A case of conjunctions requiring special treatment is introduced by reciprocals, in which the entities roles are exchangeable. For example: “John and Mary bet against each other on future rates” (adaption of wsj 0117). 2 Care needs to be taken to distinguish from cases such as “going to Italy” in which “going to” is not followed by a verbal predicate. 68 5 and Manning, 2008b), we abstract away over many syntactic details"
W14-2413,W08-2222,0,0.0793849,"Missing"
W14-2413,W05-0620,0,0.168162,"Missing"
W14-2413,W08-1301,0,0.404567,"Missing"
W14-2413,J12-4003,0,0.0156683,"ment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This design choice makes our representation much easier"
W14-2413,P11-1060,0,0.0468222,"Missing"
W14-2413,J93-2004,0,0.0457685,"aptures both explicit and implicit propositions, while staying relatively close to the syntactic level. We believe that this kind of representation will serve not only as an advantageous input for semantically-centered applications, such as question answering, summarization and information extraction, but also serve as a rich representation layer that can be used as input for systems aiming to provide a finer level of semantic analysis, such as semantic-parsers. We are currently at the beginning of our investigation. In the near future we plan to semiautomatically annotate the Penn Tree Bank (Marcus et al., 1993) with these structures, as well as to provide software for deriving (some of) the implicit and explicit annotations from automatically produced parse-trees. We believe such resources will be of immediate use to semantic-oriented applications. In the longer term, we plan to investigate dedicated algorithms for automatically producing such representation from raw text. The architecture we describe can easily accommodate additional layers of abstraction, by encoding these layers as features of propositions, predicates or arguments. Such layers can include the marking of named entities, the truth"
W14-2413,W04-2705,0,0.322908,"presentation of copular sentences) 2. Pierre Vinken will join the board as a nonexecutive director Nov. 29. Adjectives, as in the sentence “you emphasized the high prevalence of mental illness” (wsj 0105). Here an adjective is used to describe a definite subject and introduces another proposition, namely the high prevalence of mental illness. Nominalizations, for instance in the sentence “Googles acquisition of Waze occurred yesterday”, introduce the implicit proposition that “Google acquired Waze”. Such propositions were studied and annotated in the NOMLEX (Macleod et al., 1998) and NOMBANK (Meyers et al., 2004) resources. It remains an open issue how to represent or distinguish cases in which nominalization introduce an underspecified proposition. For example, consider “dancing” in “I read a book about dancing”. Possessives, such as “John’s book” introduce the proposition that John has a book. Similarly, examples such as “John’s Failure” combine a possessive construction with nominalization and introduce the proposition that John has failed. Conjunctions - for example in “They operate ships and banks.” (wsj 0083), introduce several propositions in one sentence: 1. They operate ships 2. They operate"
W14-2413,S10-1008,0,0.0268506,"ositions and author commitment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This design choice makes our rep"
W14-2413,C98-1013,0,\N,Missing
W16-2007,N15-1107,0,0.090378,"Missing"
W16-2007,D11-1057,0,0.0977516,"Missing"
W16-2007,N13-1138,0,0.133599,"the MS2S model, which is trained greedily without such search procedure. The second system, named BIU/MIT-2, used the NDST architecture and participated only in the first and second sub-tasks. This system did not use beam search, producing only one guess per input. Again, to use the NDST architecture for the second task we simply concatenated the input and output morpho-syntactic attribute embeddings. 4.2 5 While developing our systems we measured our performance on the development set with respect to two baselines: the shared task baseline system (ST-Base) inspired by (Nicolai et al., 2015; Durrett and DeNero, 2013), and the factored sequence to sequence baseline (Fact.) similar to the one introduced in (Faruqui et al., 2016). On the test set, our systems ranked second or third out of eight groups in the shared task (depending on the language). The best participating system, LMU1/2 (Kann and Sch¨utze, 2016) relied on a single encoder-decoder model with attention (Bahdanau et al., 2014) per language, with several improvements like performing prediction using majority voting over an ensemble of five models. In contrast, our first system did not use an explicit attention mechanism and is composed of 3 model"
W16-2007,N16-1077,0,0.491116,"ection from incomplete inflection tables while using several novel ideas for this task: morpho-syntactic attribute embeddings, modeling the concept of templatic morphology, bidirectional input character representations and neural discriminative string transduction. The reported results for the proposed models over the ten languages in the shared task bring this submission to the second/third place (depending on the language) on all three sub-tasks out of eight participating teams, while training only on the Restricted category data. 1 Yonatan Belinkov CSAIL MIT belinkov@mit.edu More recently, Faruqui et al. (2016) used encoder-decoder neural networks for inflection generation inspired by similar approaches for sequence-to-sequence learning for machine translation (Bahdanau et al., 2014; Sutskever et al., 2014). The general idea is to use an encoderdecoder network over characters, that encodes the input lemma into a vector and decodes it one character at a time into the inflected surface word. They factor the data into sets of inflections with identical morpho-syntactic attributes (we refer to each such set as a factor) and try two training approaches: in one they train an individual encoderdecoder RNN"
W16-2007,P16-1154,0,0.0562918,"Missing"
W16-2007,E14-1060,0,0.350803,"Missing"
W16-2007,P16-2090,0,0.10157,"Missing"
W16-2007,J94-3001,0,0.249606,"flected word given the word, its morpho-syntactic attributes and the target inflection’s attributes, and the third requires re-inflection of an inflected word given only the target inflection attributes. The datasets for the different tasks are available on the Introduction Morphological inflection, or reinflection, involves generating a target (surface form) word from a source word (e.g. a lemma), given the morphosyntactic attributes of the target word. Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan and Kay, 1994), which are theoretically appealing but require expert knowledge, or machine learning methods for string transduction (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; 41 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 41–48, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics shared task’s website.1 The fact that the data is incomplete makes it problematic to use factored models like the ones introduced in (Faruqui et al., 2016), as there may be insufficient data for training a h"
W16-2007,N15-1093,0,0.086334,"rform beam search over the MS2S model, which is trained greedily without such search procedure. The second system, named BIU/MIT-2, used the NDST architecture and participated only in the first and second sub-tasks. This system did not use beam search, producing only one guess per input. Again, to use the NDST architecture for the second task we simply concatenated the input and output morpho-syntactic attribute embeddings. 4.2 5 While developing our systems we measured our performance on the development set with respect to two baselines: the shared task baseline system (ST-Base) inspired by (Nicolai et al., 2015; Durrett and DeNero, 2013), and the factored sequence to sequence baseline (Fact.) similar to the one introduced in (Faruqui et al., 2016). On the test set, our systems ranked second or third out of eight groups in the shared task (depending on the language). The best participating system, LMU1/2 (Kann and Sch¨utze, 2016) relied on a single encoder-decoder model with attention (Bahdanau et al., 2014) per language, with several improvements like performing prediction using majority voting over an ensemble of five models. In contrast, our first system did not use an explicit attention mechanism"
W16-2007,N16-1076,0,0.102273,"Missing"
W16-2007,D13-1021,0,0.112739,"Missing"
W16-2007,P00-1027,0,0.0177401,"an inflected word given only the target inflection attributes. The datasets for the different tasks are available on the Introduction Morphological inflection, or reinflection, involves generating a target (surface form) word from a source word (e.g. a lemma), given the morphosyntactic attributes of the target word. Previous approaches to automatic inflection generation usually make use of manually constructed Finite State Transducers (Koskenniemi, 1983; Kaplan and Kay, 1994), which are theoretically appealing but require expert knowledge, or machine learning methods for string transduction (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; 41 Proceedings of the 14th Annual SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 41–48, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics shared task’s website.1 The fact that the data is incomplete makes it problematic to use factored models like the ones introduced in (Faruqui et al., 2016), as there may be insufficient data for training a highquality model per factor of inflections with identical morpho-syntactic attributes. For example, in the shared task dataset the training data usua"
W16-2007,W16-2002,0,\N,Missing
W16-2519,J15-4004,0,0.672728,"and Datasets for Word Similarity Evaluation Over the years, several datasets have been used for evaluating word similarity models. Popular ones include RG (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2001), WS-Sim (Agirre et al., 2009) and MEN (Bruni et al., 2012). Each of these datasets is a collection of word pairs together with their similarity scores as assigned by human annotators. A model is evaluated by assigning a similarity score to each pair, sorting the pairs according to their similarity, and calculating the correlation (Spearman’s ρ) with the human ranking. Hill et al (2015) had made a comprehensive review of these datasets, and pointed out some common shortcomings they have. The main shortcoming discussed by Hill et al is the handling of associated but dissimilar words, e.g. (singer, microphone): in datasets which contain such pairs (WordSim and MEN) they are usually ranked high, sometimes even above pairs of similar words. This causes an undesirable penalization of models that apply the correct behavior (i.e., always prefer similar pairs over associated dissimilar ones). Other datasets (WS-Sim and RG) do not contain pairs of associated words pairs at all. Their"
W16-2519,W13-3512,0,0.171454,"Missing"
W16-2519,N09-1003,0,0.488534,"Missing"
W16-2519,P12-1015,0,0.453682,"datasets for word similarity evaluation. Our goal is to improve the reliability of the evaluation, and we do this by redesigning the annotation task to achieve higher inter-rater agreement, and by defining a performance measure which takes the reliability of each annotation decision in the dataset into account. 1 Existing Methods and Datasets for Word Similarity Evaluation Over the years, several datasets have been used for evaluating word similarity models. Popular ones include RG (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2001), WS-Sim (Agirre et al., 2009) and MEN (Bruni et al., 2012). Each of these datasets is a collection of word pairs together with their similarity scores as assigned by human annotators. A model is evaluated by assigning a similarity score to each pair, sorting the pairs according to their similarity, and calculating the correlation (Spearman’s ρ) with the human ranking. Hill et al (2015) had made a comprehensive review of these datasets, and pointed out some common shortcomings they have. The main shortcoming discussed by Hill et al is the handling of associated but dissimilar words, e.g. (singer, microphone): in datasets which contain such pairs (Word"
W17-4912,W16-3622,0,0.025126,"Missing"
W17-4912,P16-2008,0,0.0589176,"Missing"
W17-4912,N16-1149,0,0.0439311,"Missing"
W17-4912,N16-1086,0,0.0324147,"Missing"
W17-4912,D16-1032,0,0.0497518,"Missing"
W17-4912,N15-1023,0,0.0228964,"Missing"
W17-4912,D16-1140,0,0.0727185,"language models and recurrent sequence-to-sequence architectures to NLP brought with it a surge of work on natural language generation. Most of these research efforts focus on controlling the content of the generated text (Lipton et al., 2015; Kiddon et al., 2016; Lebret et al., 2016; Kiddon et al., 2016; Tang et al., 2016; Radford et al., 2017), while a few model more stylistic aspects of the generated text such as the identity of the speaker in a dialog setting (Li et al., 2016); the politeness of the generated text or the text length in a machinetranslation setting (Sennrich et al., 2016; Kikuchi et al., 2016); or the tense in generated movie reviews (Hu et al., 2017). Each of these works targets a single, focused stylistic aspect of the text. Can we achieve finer-grained control over the generated outcome, controlling several stylistic aspects simultaneously? We explore a simple neural natural-language generation (NNLG) framework that allows for high-level control on the generated content (similar to previous work) as well as control over multiple stylistic properties of the generated text. We show that we can indeed achieve control over each of the individual properties. As most recent efforts, o"
W17-4912,Q16-1005,0,0.0259002,"Missing"
W17-4912,D16-1128,0,0.0423853,"Missing"
W17-4912,P16-1094,0,0.0519612,"is work focuses on generating text while allowing control of its stylistic properties. The recent introduction of recurrent neural language models and recurrent sequence-to-sequence architectures to NLP brought with it a surge of work on natural language generation. Most of these research efforts focus on controlling the content of the generated text (Lipton et al., 2015; Kiddon et al., 2016; Lebret et al., 2016; Kiddon et al., 2016; Tang et al., 2016; Radford et al., 2017), while a few model more stylistic aspects of the generated text such as the identity of the speaker in a dialog setting (Li et al., 2016); the politeness of the generated text or the text length in a machinetranslation setting (Sennrich et al., 2016; Kikuchi et al., 2016); or the tense in generated movie reviews (Hu et al., 2017). Each of these works targets a single, focused stylistic aspect of the text. Can we achieve finer-grained control over the generated outcome, controlling several stylistic aspects simultaneously? We explore a simple neural natural-language generation (NNLG) framework that allows for high-level control on the generated content (similar to previous work) as well as control over multiple stylistic propert"
W17-4912,N16-1005,0,0.0267386,"ion of recurrent neural language models and recurrent sequence-to-sequence architectures to NLP brought with it a surge of work on natural language generation. Most of these research efforts focus on controlling the content of the generated text (Lipton et al., 2015; Kiddon et al., 2016; Lebret et al., 2016; Kiddon et al., 2016; Tang et al., 2016; Radford et al., 2017), while a few model more stylistic aspects of the generated text such as the identity of the speaker in a dialog setting (Li et al., 2016); the politeness of the generated text or the text length in a machinetranslation setting (Sennrich et al., 2016; Kikuchi et al., 2016); or the tense in generated movie reviews (Hu et al., 2017). Each of these works targets a single, focused stylistic aspect of the text. Can we achieve finer-grained control over the generated outcome, controlling several stylistic aspects simultaneously? We explore a simple neural natural-language generation (NNLG) framework that allows for high-level control on the generated content (similar to previous work) as well as control over multiple stylistic properties of the generated text. We show that we can indeed achieve control over each of the individual properties. As"
W17-4912,J11-3002,0,0.105095,"Missing"
W17-4912,D15-1199,0,0.0195171,"Missing"
W17-4912,C12-1177,0,0.170508,"Missing"
W17-4912,E85-1027,0,\N,Missing
W17-4912,P16-1162,0,\N,Missing
W18-5408,N15-1011,0,0.206012,"parates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions). 1 Introduction Convolutional Neural Networks (CNNs), originally invented for computer vision, have been shown to achieve strong performance on text classification tasks (Bai et al., 2018; Kalchbrenner et al., 2014; Wang et al., 2015; Zhang et al., 2015; Johnson and Zhang, 2015; Iyyer et al., 2015) as well as other traditional Natural Language Processing (NLP) tasks (Collobert et al., 2011), even when considering relatively simple one-layer models (Kim, 2014). As with other architectures of neural networks, explaining the learned functionality of CNNs is still an active research area. The ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model (Ribeiro et al., 2016). The problem of interpretability in machine learning can be divided into 1) 1-dimensional convolving filters are used as ngram detectors"
W18-5408,P14-1062,0,0.0298053,"on patterns, and that global max-pooling induces behavior which separates important ngrams from the rest. Finally, we show practical use cases derived from our findings in the form of model interpretability (explaining a trained model by deriving a concrete identity for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions). 1 Introduction Convolutional Neural Networks (CNNs), originally invented for computer vision, have been shown to achieve strong performance on text classification tasks (Bai et al., 2018; Kalchbrenner et al., 2014; Wang et al., 2015; Zhang et al., 2015; Johnson and Zhang, 2015; Iyyer et al., 2015) as well as other traditional Natural Language Processing (NLP) tasks (Collobert et al., 2011), even when considering relatively simple one-layer models (Kim, 2014). As with other architectures of neural networks, explaining the learned functionality of CNNs is still an active research area. The ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model (Ribeiro et al., 2016). The problem of interpretability in machine learning can be divided into"
W18-5408,D17-1042,0,0.0487453,"Missing"
W18-5408,D14-1181,0,0.00987278,"ty for each filter, bridging the gap between visualization tools in vision tasks and NLP) and prediction interpretability (explaining predictions). 1 Introduction Convolutional Neural Networks (CNNs), originally invented for computer vision, have been shown to achieve strong performance on text classification tasks (Bai et al., 2018; Kalchbrenner et al., 2014; Wang et al., 2015; Zhang et al., 2015; Johnson and Zhang, 2015; Iyyer et al., 2015) as well as other traditional Natural Language Processing (NLP) tasks (Collobert et al., 2011), even when considering relatively simple one-layer models (Kim, 2014). As with other architectures of neural networks, explaining the learned functionality of CNNs is still an active research area. The ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model (Ribeiro et al., 2016). The problem of interpretability in machine learning can be divided into 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams. 2) Max-pooling over time extracts the relevant ngrams for making a decision. 3) The rest of the network classifies the"
W18-5408,D16-1011,0,0.0968748,"l 3 Intuit, Hod HaSharon, Israel 4 Allen Institute for Artificial Intelligence {alonjacovi,oren.sarshalom,yoav.goldberg}@gmail.com Abstract two concrete tasks: Given a trained model, model interpretability aims to supply a structured explanation which captures what the model has learned. Given a trained model and a single example, prediction interpretability aims to explain how the model arrived at its prediction. These can be further divided into white-box and black-box techniques. While recent works have begun to supply the means of interpreting predictions (AlvarezMelis and Jaakkola, 2017; Lei et al., 2016; Guo et al., 2018), interpreting neural NLP models remains an under-explored area. Accompanying their rising popularity, CNNs have seen multiple advances in interpretability when used for computer vision tasks (Zeiler and Fergus, 2014). These techniques unfortunately do not trivially apply to discrete sequences, as they assume a continuous input space used to represent images. Intuitions about how CNNs work on an abstract level also may not carry over from image inputs to text—for example, pooling in CNNs has been used to induce deformation invariance (LeCun et al., 1998, 2015), which is like"
W18-5408,P15-1162,0,0.0473627,"Missing"
W18-5408,D14-1162,0,0.08493,"Pang and Lee (2005), containing 10k evenly split short (sentences or snippets) movie reviews. b) Elec: electronic product reviews for sentiment classification introduced by Johnson and Zhang (2015), assembled from the Amazon review dataset (McAuley and Leskovec, 2013; McAuley et al., 2015), containing 200k train and 25k test evenly split reviews. c) Yelp Review Polarity: introduced by Zhang et al. (2015) from the Yelp Dataset Challenge 2015, containing 560k train and 38k test evenly split business reviews. For word embeddings, we use the pre-trained GloVe Wikipedia 2014—Gigaword 5 embeddings (Pennington et al., 2014), which we fine-tune with the model. We use embedding dimension of 50, filter sizes of ` ∈ {2, 3, 4} words, and m ∈ {10, 50} filters. Models are implemented in PyTorch and trained with the Adam optimizer. Background: 1D Text Convolutions We focus on the task of text classification. We consider the common architecture in which each word in a document is represented as an embedding vector, a single convolutional layer with m filters is applied, producing an m-dimensional vector for each document ngram. The vectors are combined using max-pooling followed by a ReLU activation. The result is then p"
W18-5408,N16-3020,0,0.172326,"e strong performance on text classification tasks (Bai et al., 2018; Kalchbrenner et al., 2014; Wang et al., 2015; Zhang et al., 2015; Johnson and Zhang, 2015; Iyyer et al., 2015) as well as other traditional Natural Language Processing (NLP) tasks (Collobert et al., 2011), even when considering relatively simple one-layer models (Kim, 2014). As with other architectures of neural networks, explaining the learned functionality of CNNs is still an active research area. The ability to interpret neural models can be used to increase trust in model predictions, analyze errors or improve the model (Ribeiro et al., 2016). The problem of interpretability in machine learning can be divided into 1) 1-dimensional convolving filters are used as ngram detectors, each filter specializing in a closely-related family of ngrams. 2) Max-pooling over time extracts the relevant ngrams for making a decision. 3) The rest of the network classifies the text based on this information. 56 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56–65 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics F ∈ Rn×m . Applying max-pooling across th"
W18-5408,P15-2058,0,0.0133861,"text classification datasets for Sentiment Analysis, which involves classifying the input text (user reviews in all cases) between positive and negative. The specific datasets were chosen for their relative variety in size and domain as well as for the relative simplicity and interpretability of the binary sentiment analysis task. The three datasets are: a) MR: sentence polarity dataset v1.0 introduced by Pang and Lee (2005), containing 10k evenly split short (sentences or snippets) movie reviews. b) Elec: electronic product reviews for sentiment classification introduced by Johnson and Zhang (2015), assembled from the Amazon review dataset (McAuley and Leskovec, 2013; McAuley et al., 2015), containing 200k train and 25k test evenly split reviews. c) Yelp Review Polarity: introduced by Zhang et al. (2015) from the Yelp Dataset Challenge 2015, containing 560k train and 38k test evenly split business reviews. For word embeddings, we use the pre-trained GloVe Wikipedia 2014—Gigaword 5 embeddings (Pennington et al., 2014), which we fine-tune with the model. We use embedding dimension of 50, filter sizes of ` ∈ {2, 3, 4} words, and m ∈ {10, 50} filters. Models are implemented in PyTorch and t"
W18-5412,P18-1027,0,0.0235091,"duction In recent years, recurrent neural network (RNN) models have emerged as a powerful architecture for a variety of NLP tasks (Goldberg, 2017). In particular, gated versions, such as Long Short-Term Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014; Chung et al., 2014) achieve state-of-the-art results in tasks such as language modeling, parsing, and machine translation. RNNs were shown to be able to capture longterm dependencies and statistical regularities in input sequences (Karpathy et al., 2015; Linzen et al., 2016; Shi et al., 2016; Jurafsky et al., 2018; Gulordava et al., 2018). An adequate evaluation of 98 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 98–107 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics hierarchal relations between sentence constituents. Labeled data for such a task requires only the collection of sentences that exhibit agreement from an unannotated corpora. However, those works have focused on relatively small set of languages: several Indo-European languages and a Semitic language (Hebrew). As we show, drawing conclusio"
W18-5412,P81-1022,0,0.172915,"Missing"
W18-5412,P15-1033,0,0.0132923,"rn to Capture Agreement? The Case of Basque Shauli Ravfogel1 and Francis M. Tyers2,3 and Yoav Goldberg1,4 1 Computer Science Department, Bar Ilan University 2 School of Linguistics, Higher School of Economics 3 Department of Linguistics, Indiana University 4 Allen Institute for Artificial Intelligence {shauli.ravfogel, yoav.goldberg}@gmail.com, ftyers@prompsit.com Abstract the ability of RNNs to capture syntactic structure requires a use of established benchmarks. A common approach is the use of an annotated corpus to learn an explicit syntax-oriented task, such as parsing or shallow parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) . While such an approach does evaluate the ability of the model to learn syntax, it has several drawbacks. First, the annotation process relies on human experts and is thus demanding in term of resources. Second, by its very nature, training a model on such a corpus evaluates it on a human-dictated notion of grammatical structure, and is tightly coupled to a linguistic theory. Lastly, the supervised training process on such a corpus provides the network with explicit grammatical labels (e.g. a parse tree). While this is sometimes desir"
W18-5412,Q16-1023,1,0.870669,"Missing"
W18-5412,Q16-1037,1,0.856665,"he model’s ability to capture regularities in language. While this approach does not suffer from the above discussed drawbacks, it conflates syntactical capacity with other factors such as world knowledge and frequency of lexical items. Furthermore, the LM task does not provide one clear answer: one cannot be “right” or “wrong” in language modeling, only softly worse or better than other systems. A different approach is testing the model on a grammatical task that does not require an extensive grammatical annotation, but is yet indicative of syntax comprehension. Specifically, previous works (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018) used the task of predicting agreement, which requires detecting Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire? We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquis"
W18-5412,D16-1159,0,0.0332025,"language. 1 Introduction In recent years, recurrent neural network (RNN) models have emerged as a powerful architecture for a variety of NLP tasks (Goldberg, 2017). In particular, gated versions, such as Long Short-Term Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014; Chung et al., 2014) achieve state-of-the-art results in tasks such as language modeling, parsing, and machine translation. RNNs were shown to be able to capture longterm dependencies and statistical regularities in input sequences (Karpathy et al., 2015; Linzen et al., 2016; Shi et al., 2016; Jurafsky et al., 2018; Gulordava et al., 2018). An adequate evaluation of 98 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 98–107 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics hierarchal relations between sentence constituents. Labeled data for such a task requires only the collection of sentences that exhibit agreement from an unannotated corpora. However, those works have focused on relatively small set of languages: several Indo-European languages and a Semitic language (Hebrew). As we"
W18-5412,N18-1108,0,0.608688,"nguage. While this approach does not suffer from the above discussed drawbacks, it conflates syntactical capacity with other factors such as world knowledge and frequency of lexical items. Furthermore, the LM task does not provide one clear answer: one cannot be “right” or “wrong” in language modeling, only softly worse or better than other systems. A different approach is testing the model on a grammatical task that does not require an extensive grammatical annotation, but is yet indicative of syntax comprehension. Specifically, previous works (Linzen et al., 2016; Bernardy and Lappin, 2017; Gulordava et al., 2018) used the task of predicting agreement, which requires detecting Sequential neural networks models are powerful tools in a variety of Natural Language Processing (NLP) tasks. The sequential nature of these models raises the questions: to what extent can these models implicitly learn hierarchical structures typical to human language, and what kind of grammatical phenomena can they acquire? We focus on the task of agreement prediction in Basque, as a case study for a task that requires implicit understanding of sentence structure and the acquisition of a complex but consistent morphological syst"
W19-3621,D14-1162,0,\N,Missing
W19-3621,D18-1521,0,\N,Missing
W19-3622,J15-4004,0,\N,Missing
W19-3622,D14-1162,0,\N,Missing
W19-3622,P16-1156,0,\N,Missing
W19-3622,E17-2067,1,\N,Missing
W19-3622,P17-1006,0,\N,Missing
W19-3622,D17-1073,0,\N,Missing
W19-3622,P19-1161,0,\N,Missing
W19-3807,E17-1101,0,0.0602463,"els with a gender-indicating-prefix. We differ from this work by treating the problem in a black-box manner, and by addressing additional information like the number of the speaker and the gender and number of the audience. one in masculine and one in feminine form. Not all examples are using the same source English sentence as different languages mark different information. Table 3 shows that for these specific examples our method worked on 6/10 of the languages we had examples for, while for 3/10 languages both translations are masculine, and for 1 language both are feminine. 5 Related Work Rabinovich et al. (2017) showed that given input with author traits like gender, it is possible to retain those traits in Statistical Machine Translation (SMT) models. Gr¨onroos et al. (2017) showed that incorporating morphological analysis in the decoder improves NMT performance for morphologically rich languages. Burlot and Yvon (2017) presented a new protocol for evaluating the morphological competence of MT systems, indicating that current translation systems only manage to capture some morphological phenomena correctly. Regarding the application of constraints in NMT, Sennrich et al. (2016) presented a method fo"
W19-3807,N16-1005,0,0.0460269,"ine. 5 Related Work Rabinovich et al. (2017) showed that given input with author traits like gender, it is possible to retain those traits in Statistical Machine Translation (SMT) models. Gr¨onroos et al. (2017) showed that incorporating morphological analysis in the decoder improves NMT performance for morphologically rich languages. Burlot and Yvon (2017) presented a new protocol for evaluating the morphological competence of MT systems, indicating that current translation systems only manage to capture some morphological phenomena correctly. Regarding the application of constraints in NMT, Sennrich et al. (2016) presented a method for controlling the politeness level in the generated output. Ficler and Goldberg (2017) showed how to guide a neural text generation system towards 6 Conclusions We highlight the problem of translating between languages with different morphological systems, in which the target translation must contain gender and number information that is not available in the source. We propose a method for injecting such information into a pre-trained NMT model in a black-box setting. We demonstrate the effectiveness of this method by showing an improvement of 2.3 BLEU in an English-to-He"
W19-3807,N18-1118,0,0.0210565,"ain property is marked, languages differ in the form and location of the marking (Nichols, 1986). For example, marking can occur on the head of a syntactic dependency construction, on its argument, on both (requiring agreement), or on none of them. Translation systems must generate correct target-language morphology as part of the translation process. This requires knowledge of both the source-side and target-side morphology. Current state-of-the-art translation systems do capture many aspects of natural language, including morphology, when a relevant context is available (Dalvi et al., 2017; Bawden et al., 2018), but resort to “guessing” based on the training-data statistics when it is not. Complications arise when different languages convey different kinds of information in their morphological systems. In such cases, a translation system may be required to remove information available in the source sentence, or to add information not available in it, where the latter can be especially tricky. 3 More concretely, a sentence such as “I love you” is ambiguous with respect to the gender of the speaker and the gender and number of the audience. However, sentences such as “I love you, she told him” are una"
W19-3807,W17-4811,0,0.0309056,"mada ani nehmada Estoy encantada Estoy encantada Eu fui chamada Eu fui chamado je suis patiente je suis patiente Sono bella io sono bella Я написала сообщение Я написал сообщение j´a jsem ji dala kvˇetinu Dala jsem j´ı kvˇetinu Sunt r˘abd˘atoare Sunt r˘abd˘atoare s´oc rica s´oc ric Jestem miła Jestem miła Table 3: Examples of languages where the speaker’s gender changes morphological markings in different languages, and translations using the prefix “He said:” or “She said:” accordingly style and content parameters like the level of professionalism, subjective/objective, sentiment and others. Tiedemann and Scherrer (2017) showed that incorporating more context when translating subtitles can improve the coherence of the generated translations. Most closely to our work, Vanmassenhove et al. (2018) also addressed the missing gender information by training proprietary models with a gender-indicating-prefix. We differ from this work by treating the problem in a black-box manner, and by addressing additional information like the number of the speaker and the gender and number of the audience. one in masculine and one in feminine form. Not all examples are using the same source English sentence as different languages"
W19-3807,W17-4705,0,0.0261683,"sentence as different languages mark different information. Table 3 shows that for these specific examples our method worked on 6/10 of the languages we had examples for, while for 3/10 languages both translations are masculine, and for 1 language both are feminine. 5 Related Work Rabinovich et al. (2017) showed that given input with author traits like gender, it is possible to retain those traits in Statistical Machine Translation (SMT) models. Gr¨onroos et al. (2017) showed that incorporating morphological analysis in the decoder improves NMT performance for morphologically rich languages. Burlot and Yvon (2017) presented a new protocol for evaluating the morphological competence of MT systems, indicating that current translation systems only manage to capture some morphological phenomena correctly. Regarding the application of constraints in NMT, Sennrich et al. (2016) presented a method for controlling the politeness level in the generated output. Ficler and Goldberg (2017) showed how to guide a neural text generation system towards 6 Conclusions We highlight the problem of translating between languages with different morphological systems, in which the target translation must contain gender and nu"
W19-3807,I17-1015,0,0.0222863,"bs. Even when a certain property is marked, languages differ in the form and location of the marking (Nichols, 1986). For example, marking can occur on the head of a syntactic dependency construction, on its argument, on both (requiring agreement), or on none of them. Translation systems must generate correct target-language morphology as part of the translation process. This requires knowledge of both the source-side and target-side morphology. Current state-of-the-art translation systems do capture many aspects of natural language, including morphology, when a relevant context is available (Dalvi et al., 2017; Bawden et al., 2018), but resort to “guessing” based on the training-data statistics when it is not. Complications arise when different languages convey different kinds of information in their morphological systems. In such cases, a translation system may be required to remove information available in the source sentence, or to add information not available in it, where the latter can be especially tricky. 3 More concretely, a sentence such as “I love you” is ambiguous with respect to the gender of the speaker and the gender and number of the audience. However, sentences such as “I love you,"
W19-3807,P18-1117,0,0.0335865,"). This morphological diversity creates a challenge for machine translation, as there are ambiguous cases where more than one correct translation exists for the same source sentence. For example, while the English sentence “I love language” is ambiguous with respect to the gender of the speaker, Hebrew marks verbs 1 blog.google/products/translate/ reducing-gender-bias-google-translate/ 49 Proceedings of the 1st Workshop on Gender Bias in Natural Language Processing, pages 49–54 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics We are motivated by recent work by Voita et al. (2018) who showed that NMT systems learn to track coreference chains when presented with sufficient discourse context. We conjecture that there are enough sentence-internal pronominal coreference chains appearing in the training data of large-scale NMT systems, such that state-of-theart NMT systems can and do track sentenceinternal coreference. We devise a wrapper method to make use of this coreference tracking ability by introducing artificial antecedents that unambiguously convey the desired gender and number properties of the speaker and audience. 2018). We show that in spite of its simplicity, i"
W19-3807,W17-4912,1,0.819795,"is possible to retain those traits in Statistical Machine Translation (SMT) models. Gr¨onroos et al. (2017) showed that incorporating morphological analysis in the decoder improves NMT performance for morphologically rich languages. Burlot and Yvon (2017) presented a new protocol for evaluating the morphological competence of MT systems, indicating that current translation systems only manage to capture some morphological phenomena correctly. Regarding the application of constraints in NMT, Sennrich et al. (2016) presented a method for controlling the politeness level in the generated output. Ficler and Goldberg (2017) showed how to guide a neural text generation system towards 6 Conclusions We highlight the problem of translating between languages with different morphological systems, in which the target translation must contain gender and number information that is not available in the source. We propose a method for injecting such information into a pre-trained NMT model in a black-box setting. We demonstrate the effectiveness of this method by showing an improvement of 2.3 BLEU in an English-to-Hebrew translation setting where the speaker and audience gender can be inferred. We also perform a fine-grain"
W19-3807,W17-4727,0,0.0599564,"Missing"
W19-3807,2005.mtsummit-papers.11,0,0.150326,"r and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms. Comparison to Vanmassenhove et al. (2018) Closely related to our work, Vanmassenhove et al. (2018) proposed a method and an English-French test set to evaluate gender-aware translation, based on the Europarl corpus (Koehn, 2005). We evaluate our method (using Google Translate and the given prefixes) on their test set to see whether it is applicable to another language pair and domain. Table 2 shows the results of our approach vs. their published results and the Google Translate baseline. As may be expected, Google Translate outperforms their system as it is trained on a different corpus and may use more complex machine translation models. Using our method improves the BLEU score even further. Results: Speaker. Figure 1 shows the result for controlling the morphological properties of the speaker ({he, she, I} said). I"
W19-3807,P07-2045,0,0.00867243,"tion is not available in the source sentence. The approach we take in the current work is that of black-box injection, in which we attempt to inject knowledge to the input in order to influence the output of a trained NMT system, without having access to its internals or its training procedure as proposed by Vanmassenhove et al. (2018). 50 Speaker Audience Baseline He – He him He her He them I – I them She – She him She her She them BLEU 18.67 19.2 19.25 19.3 19.5 19.84 20.23 20.8 20.82 20.98 20.97 to the reference Hebrew translations. We use the multi-bleu.perl script from the Moses toolkit (Koehn et al., 2007). Table 1 shows BLEU scores for the different prefixes. The numbers match our expectations: Generally, providing an incorrect speaker and/or audience information decreases the BLEU scores, while providing the correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline. We note the BLEU score improves in all cases, even when given the wrong gender of either the speaker or the audience. We hypothesise this improvement stems from the addition of the word “said” which hints the model to generate a more “spoken” language which matches the tested scenario."
W19-3807,P02-1040,0,0.105822,"n improvement in translation of first-person pronouns and verbs with first-person pronouns as subjects. “I said to them:”—Signaling an unknown speaker and plural audience. “He said to them:”—Masculine speaker and plural audience. “She said to them:”—Female speaker and plural audience—the complete, correct condition. We expect the best translation accuracy on this setup. “He/she said to him/her”—Here we set an (incorrect) singular gender-marked audience, to investigate our ability to control the audience morphology. 4.2 Qualitative Results We compare the different conditions by comparing BLEU (Papineni et al., 2002) with respect The BLEU score is an indication of how close the automated translation is to the reference translation, but does not tell us what exactly changed concerning the gender and number properties we attempt to control. We perform a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements. We parse the translations and the references using a Hebrew dependency parser.3 In addition to the parse structure, the parser also performs morphological analysis and tagging of the individu"
W19-8645,P02-1040,0,0.105673,"Missing"
W19-8645,W18-6555,0,0.0830827,"Missing"
W19-8645,W16-6626,0,0.0641224,"while our planner takes 0.0025 seconds, 5 orders of magnitude faster. the text plans are guaranteed to faithfully encode all and only the facts from the input. The realization stage then translates the plans into natural language sentences, using a neural sequenceto-sequence system, resulting in fluent output. 3 Fast and Verifiable Planner The data-to-plan component in Moryossef et al. (2019) exhaustively generates all possible plans, scores them using a heuristic, and chooses the highest scoring one for realization. While this is feasible with the small input graphs in the WebNLG challenge (Colin et al., 2016), it is also very computationally intensive, growing exponentially with the input size. We propose an alternative planner which works in linear time in the size of the graph and remains verifiable: generated plans are guaranteed to represent the input faithfully. The original planner works by first enumerating over all possible splits into sentences (subgraphs), and for each sub-graph enumerating over all possible undirected, unordered, Depth First Search (DFS) traversals, where each traversal corresponds to a sentence plan. Our planner combines these into a single process. It works by perform"
W19-8645,P18-1182,0,0.0940324,"Missing"
W19-8645,D16-1032,0,0.0361739,"Missing"
W19-8645,P17-4012,0,0.0389527,"traversal from that node. Then, all edges visited in the traversal are removed from the input graph, and the process repeats (performing another truncated DFS) until no more edges remain. Each truncated DFS traversal corresponds to a sentence plan, following the DFS-to-plan procedure of Moryossef et al. (2019): the linearized plan is generated incrementally at each step of the 4 Incorporating typing information for unseen entities and relations In Moryossef et al. (2019), the sentence plan trees were linearized into strings that were then fed to a neural machine translation decoder (OpenNMT) (Klein et al., 2017) with a copy mecha378 construct the input. Both of these approaches are “soft” in the sense that they crucially rely on the internal dynamics or on the output of a neural network module that may or may not be correct. nism. This linearization process is lossy, in the sense that the linearized strings do not explicitly distinguish between symbols that represent entities (e.g., BARACK OBAMA) and symbols that represent relations (e.g., works-for). While this information can be deduced from the position of the symbol within the structure, there is a benefit in making it more explicit. In particula"
W19-8645,N19-1386,0,0.0165292,"work in neural text generation and summarization attempt to address these issues by trying to map the textual outputs back to structured predicates, and comparing these predicates to the input data. Kiddon et al. (2016) uses a neural checklist model to avoid the repetition of facts and improve coverage. Agarwal et al. (2018) generate k-best output candidates with beam search, and then try to map each candidate output back to the input structure using a reverse seq2seq model trained on the same data. They then select the highest scoring output candidate that best translates back to the input. Mohiuddin and Joty (2019) reconstructs the input in training time, by jointly learning a back-translation model and enforcing the back-translation to reReferring Expressions The step-by-step system generates entities by first generating an indexed entity symbols, and then lexicalizing each symbol to the string associated with this entity in the input structure (i.e., all occurrences of the entity 11TH MISSISSIPPI INFANTRY MONUMENT will be lexicalized with the full name rather than “it” or “the monument”). This results in correct but somewhat unnatural structures. In contrast, end-to-end neural generation systems are t"
W19-8645,N19-1236,1,0.715961,"Missing"
W19-8645,P08-1108,0,0.0443616,"facts to express. 377 Proceedings of The 12th International Conference on Natural Language Generation, pages 377–382, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics traversal. This process is linear in the number of edges in the graph. At training time, we use the plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller, choosing which action to perform at each step. At test time, we use the controller to guide the truncated DFS process. This mechanism is inspired by transition based parsing (Nivre and McDonald, 2008). The action set at each stage is dynamic. During traversal, it includes the available children at each stage and POP. Before traversals, it includes a choose-i action for each available node ni . We assign a score to each action, normalize with softmax, and train to choose the desired one using cross-entropy loss. At test time, we either greedily choose the best action, or we can sample plans by sampling actions according to their assigned probabilities. Feature Representation and action scoring. Each graph node ni corresponds to an entity xni , and has an associated embedding vector xni . Ea"
