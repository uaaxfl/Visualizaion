2004.iwslt-papers.7,J90-2002,0,0.250978,"some of the presented techniques. 2. Statistical Machine Translation 2.1. Word Alignment In statistical machine translation, we are given a source language sentence f1J = f1 . . . fj . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . ei . . . eI . Among all possible target language sentences, we choose the sentence with the highest probability:  eˆI1 = argmax P r(eI1 |f1J ) (1) eI1 = argmax eI1  P r(eI1 ) · P r(f1J |eI1 ) (2) The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation [3]. It allows an independent modeling of the target language model P r(eI1 ) and translation model P r(f1J |eI1 ). The target language model describes the wellformedness of the target language sentence. The translation model links the source language sentence to the target language sentence. The word alignment A is introduced into the translation model as a hidden variable: X P r(f1J |eI1 ) = P r(f1J , A|eI1 ) (3) A Usually, restricted alignments are used in the sense that each source word is aligned to at most one target word. Thus, an alignment A is a mapping from source sentence positions to"
2004.iwslt-papers.7,J93-2003,0,0.0389305,"ment A is introduced into the translation model as a hidden variable: X P r(f1J |eI1 ) = P r(f1J , A|eI1 ) (3) A Usually, restricted alignments are used in the sense that each source word is aligned to at most one target word. Thus, an alignment A is a mapping from source sentence positions to target sentence positions A = a1 ...aj ...aJ , (aj ∈ {0, . . . , I}). The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source sentence words that are not aligned to any target word at all. A detailed comparison of the commonly used translation models IBM-1 to IBM-5 [4], as well as the Hidden-Markov alignment model (HMM) [5] can be found in [6]. All these models include parameters p(f |e) for the single-word based lexicon. They differ in the alignment model. All of the model parameters are trained iteratively with the EM-Algorithm. We follow the alignment template translation approach of [9], where a phrase tranlation model is used as one of the main features. The key elements of this translation approach are the alignment templates. These are pairs of source and target language phrases together with an alignment within the phrases. The phrases are extracted"
2004.iwslt-papers.7,C96-2141,1,0.642373,"den variable: X P r(f1J |eI1 ) = P r(f1J , A|eI1 ) (3) A Usually, restricted alignments are used in the sense that each source word is aligned to at most one target word. Thus, an alignment A is a mapping from source sentence positions to target sentence positions A = a1 ...aj ...aJ , (aj ∈ {0, . . . , I}). The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source sentence words that are not aligned to any target word at all. A detailed comparison of the commonly used translation models IBM-1 to IBM-5 [4], as well as the Hidden-Markov alignment model (HMM) [5] can be found in [6]. All these models include parameters p(f |e) for the single-word based lexicon. They differ in the alignment model. All of the model parameters are trained iteratively with the EM-Algorithm. We follow the alignment template translation approach of [9], where a phrase tranlation model is used as one of the main features. The key elements of this translation approach are the alignment templates. These are pairs of source and target language phrases together with an alignment within the phrases. The phrases are extracted from the automatically estimated word alignments. The a"
2004.iwslt-papers.7,J03-1002,1,0.143593,"f1J |eI1 ) = P r(f1J , A|eI1 ) (3) A Usually, restricted alignments are used in the sense that each source word is aligned to at most one target word. Thus, an alignment A is a mapping from source sentence positions to target sentence positions A = a1 ...aj ...aJ , (aj ∈ {0, . . . , I}). The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source sentence words that are not aligned to any target word at all. A detailed comparison of the commonly used translation models IBM-1 to IBM-5 [4], as well as the Hidden-Markov alignment model (HMM) [5] can be found in [6]. All these models include parameters p(f |e) for the single-word based lexicon. They differ in the alignment model. All of the model parameters are trained iteratively with the EM-Algorithm. We follow the alignment template translation approach of [9], where a phrase tranlation model is used as one of the main features. The key elements of this translation approach are the alignment templates. These are pairs of source and target language phrases together with an alignment within the phrases. The phrases are extracted from the automatically estimated word alignments. The alignment templates a"
2004.iwslt-papers.7,P02-1038,1,0.627781,"word penalty and alignment template penalty feature functions. To model the alignment template reorderings, we use a feature function that penalizes reorderings linear in the jump width. We use a dynamic programming beam search algorithm to generate the translation hypothesis with maximum probability. This search algorithm allows for arbitrary reorderings at the level of alignment templates. Within the alignment templates, the word order is learned in training and kept fix during the search process. This is only a brief description of the alignment template approach. For further details, see [9, 7]. 3. Acquiring Additional Training Data 2.2. Translation: Alignment Template Approach The argmax operation in Eq. 2 denotes the search problem, i.e. the generation of the output sentence in the target language. We have to maximize over all possible target language sentences. For the search, we choose an alternative to the classical source-channel approach and model the posterior probability P r(eI1 |f1J ) directly. Using a log-linear model [7], we obtain: ! M X I J J I J P r(e1 |f1 ) = Z(f1 ) · exp λm hm (e1 , f1 ) m=1 Here, Z(f1J ) denotes the appropriate normalization constant, hm are the fe"
2004.iwslt-papers.7,P03-1021,0,0.0274503,"h and model the posterior probability P r(eI1 |f1J ) directly. Using a log-linear model [7], we obtain: ! M X I J J I J P r(e1 |f1 ) = Z(f1 ) · exp λm hm (e1 , f1 ) m=1 Here, Z(f1J ) denotes the appropriate normalization constant, hm are the feature functions and λm are the corresponding scaling factors. We thus arrive at the decision rule: ( M ) X I I J eˆ1 = argmax λm hm (e1 , f1 ) eI1 to the maximum entropy principle, e.g. using the Generalized Iterative Scaling (GIS) algorithm. Alternatively, one can train them with respect to the final translation quality measured by some error criterion [8]. m=1 This approach has the advantage that additional models or feature functions can be easily integrated into the overall system. The model scaling factors λM 1 are trained according 140 When only a small corpus of sentence pairs is available for training of the statistical translation models, it may be reasonable to include additional bilingual training data from other sources. Since this additional data may come from another domain and substantially differ from the original training corpus, a method for selecting relevant sentences is desirable. In our experiments, we use a relevance measu"
2004.iwslt-papers.7,W99-0604,1,0.944144,"...aj ...aJ , (aj ∈ {0, . . . , I}). The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source sentence words that are not aligned to any target word at all. A detailed comparison of the commonly used translation models IBM-1 to IBM-5 [4], as well as the Hidden-Markov alignment model (HMM) [5] can be found in [6]. All these models include parameters p(f |e) for the single-word based lexicon. They differ in the alignment model. All of the model parameters are trained iteratively with the EM-Algorithm. We follow the alignment template translation approach of [9], where a phrase tranlation model is used as one of the main features. The key elements of this translation approach are the alignment templates. These are pairs of source and target language phrases together with an alignment within the phrases. The phrases are extracted from the automatically estimated word alignments. The alignment templates are build at the level of word classes, which improves their generalization capability. Besides the alignment template translation model probabilities, we use additional feature functions. These are the word translation model and two language models: a"
2004.iwslt-papers.7,2001.mtsummit-papers.68,0,0.0231131,"and substantially differ from the original training corpus, a method for selecting relevant sentences is desirable. In our experiments, we use a relevance measure of ngram coverage. To this end, we compute the set C of ngrams occurring in the source part of the initial training corpus (n = 1, 2, 3, 4). Then, for each candidate source sentence in the additional corpus, we compute a score based on the occurrence of the n-grams from C in that sentence. The score is defined as the geometric mean of n-gram precisions and is therefore similar to the BLEU score used in machine translation evaluation [10]. Such score provides a quantitative measure of how “out-of-domain” or “in-domain” the additional training data may be. We add only those sentence pairs to the initial training corpus, for which this score is sufficiently high. 4. Morphological Information for Word Alignments 4.1. Lexicon Smoothing Existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other. In our approach, the dependencies between such derivations are taken into account during the EM training of the statistical alignment models. Typically, the stat"
2004.iwslt-papers.7,P00-1056,1,0.650352,"word alignment quality on the Verbmobil task. The German– English Verbmobil task [13] is a speech translation task in the domain of appointment scheduling, travel planning and hotel reservation. The corpus statistics are shown in Table 1. The number of running words and the vocabularies are based on full-form words including punctuation marks. As in [6], the first 100 sentences of the alignment test corpus are used as a development corpus to optimize model parameters that are not trained via the EM algorithm, e.g. the smoothing parameters. We use the same evaluation criterion as described in [14]. The generated word alignment is compared to a reference alignment which is produced by human experts. The obtained reference alignment may contain many-to-one and one-to-many relationships and includes sure (S) and possible (P) alignment points. The quality of an alignment A is computed as appropriately redefined precision and recall measures. We also use the alignment error rate (AER), which is derived from the well-known F-measure. |A ∩ S| |A ∩ P | , precision = |S| |A| |A ∩ S |+ |A ∩ P | AER(S, P ; A) = 1 − |A |+ |S| recall = With these definitions a recall error can only occur if a S(ure"
2004.iwslt-papers.7,W01-1407,1,\N,Missing
2004.iwslt-papers.7,P02-1040,0,\N,Missing
2005.eamt-1.29,H93-1039,0,0.0227916,"slation (SMT) is to translate an input word sequence f1J = f1 . . . fj . . . fJ into a target word sequence eI1 = e1 . . . ei . . . eI by maximising the probability P (eI1 |f1J ). This probability can be factorised into the translation model probability P (f1J |eI1 ), which describes the correspondence between the words in the source and the target sequence and the language model probability P (eJ1 ), which describes the wellformedness of the produced target sequence. These two probabilities can be modelled independently of each other. For detailed descriptions of SMT models, see for example (Brown et al., 1993). Translation probabilities are extracted from a bilingual parallel text corpus, whereas language model probabilities are learnt from a monolingual text corpus in the target language. Usually, the larger the available training corpus, the better the performance of a translation system. However, acquisition of a large high-quality bilingual parallel text for the desired domain and language pair requires lot of time and effort, and, for many language pairs, is not even possible. Therefore, the strategies for exploiting limited amounts of bilingual data are receiving more and more attention (Al-O"
2005.eamt-1.29,J04-2003,1,0.834265,"lities are extracted from a bilingual parallel text corpus, whereas language model probabilities are learnt from a monolingual text corpus in the target language. Usually, the larger the available training corpus, the better the performance of a translation system. However, acquisition of a large high-quality bilingual parallel text for the desired domain and language pair requires lot of time and effort, and, for many language pairs, is not even possible. Therefore, the strategies for exploiting limited amounts of bilingual data are receiving more and more attention (Al-Onaizan et al., 2000; Nießen and Ney, 2004; Matusov et al., 2004). 212 Conventional dictionaries (one word and its translation(s) per entry) have been proposed in (Brown et al., 1993) and are shown to be valuable resources for SMT systems. They can be used to augment and also to replace the training corpus. Nevertheless, the main draw-back is that they typically contain only base forms of the words and not inflections. The use of morpho-syntactic information for overcoming this problem is investigated in (Nießen and Ney, 2004) for translation from German into English and in (Vogel and Monson, 2004) for translation from Chinese into En"
2005.eamt-1.29,P02-1040,0,0.0709516,"Missing"
2005.eamt-1.29,vogel-monson-2004-augmenting,0,0.0159583,"ore attention (Al-Onaizan et al., 2000; Nießen and Ney, 2004; Matusov et al., 2004). 212 Conventional dictionaries (one word and its translation(s) per entry) have been proposed in (Brown et al., 1993) and are shown to be valuable resources for SMT systems. They can be used to augment and also to replace the training corpus. Nevertheless, the main draw-back is that they typically contain only base forms of the words and not inflections. The use of morpho-syntactic information for overcoming this problem is investigated in (Nießen and Ney, 2004) for translation from German into English and in (Vogel and Monson, 2004) for translation from Chinese into English. Still, the dictionaries normally contain one word per entry and do not take into account phrases, idioms and similar complex expressions. In our work, we have exploited a phrasal lexicon (one short phrase and its translation(s) per entry) as a bilingual knowledge source for SMT which has not been examined so far. A phrasal lexicon is expected to be especially helpful to overcome some difficulties which cannot be handled well with standard dictionaries. We have used the phrasal lexicon to increase the existing training corpus as well as to replace it."
2005.eamt-1.29,J93-2003,0,\N,Missing
2005.eamt-1.29,2004.iwslt-papers.7,1,\N,Missing
2006.iwslt-papers.7,2005.iwslt-1.20,1,0.362873,"the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |"
2006.iwslt-papers.7,P05-1033,0,0.0502601,"the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |"
2006.iwslt-papers.7,J04-2004,0,0.0975736,"the original single-word-based models to phrase-based-models, in order to better capture the context dependencies of the words in the translation process. The starting point for the training of these models was however the Viterbi alignment produced as a byproduct of the training of the original IBM models, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |"
2006.iwslt-papers.7,J03-1002,1,0.0610827,"s, that is, the alignment with the highest probability given the final parameter estimations. Most state-of-the-art machine translation systems, normally based on a phrase-based translation scheme or variations of it, make use of this Viterbi alignment as a first step in the training process [2, 3, 4]. Other translation approaches also benefit from the use of alignments [5]. It is then to expect that an increase in quality of the alignment should lead to an increase in translation quality. At least, it is expected that an improvement in the alignments does not hurt translation performance. In [6] the “Alignment Error Rate” (AER) is introduced as a measure of alignment quality. Given a reference alignment, consisting of a set S of “Sure”, unambiguous alignment points and a set P of “Possible”, ambiguous alignment points, with S ⊆ P , the AER of an alignment A = {(j, aj )} is defined to be |A ∩ S |+ |A ∩ P | . AER(S, P ; A) = 1 − |A |+ |S| cision using the possible alignments. In the same paper, an exhaustive study of different alignment models is carried out. Following this work, numerous new alignment methods or refinements to existing ones have appeared in the literature, which incre"
2006.iwslt-papers.7,2005.mtsummit-papers.34,1,0.760218,"owever many of them do not report translation results, and the implicit assumption is made that the improvements on alignment quality will influence the translation process in a positive way. In this paper we will present two counter-examples to this assumption, that is, we will present (review in one of the cases) two relatively simple refinements of the standard alignment process using the IBM models that actually deteriorate the alignment quality. However, they improve the translation performance. We will show this on two translation models, a phrase based system similar to the one used in [7] and a finite state transducer based system as presented in [8]. The key point is that these methods adapt the alignments to the translation models that will make further use of them. 2. Related Work In [9] the authors conduct an experimental study on the correlation of AER as defined above and the actual translation performance. To our knowledge this is the first work that carries out such a detailed study. The conclusion of their work is that the alignment error rate is not a good measure for predicting translation performance. The main reason given is that AER does not penalize an unbalance"
2006.iwslt-papers.7,W02-1018,0,0.0657364,"Missing"
2006.iwslt-papers.7,W05-0831,1,0.906414,"implicit assumption is made that the improvements on alignment quality will influence the translation process in a positive way. In this paper we will present two counter-examples to this assumption, that is, we will present (review in one of the cases) two relatively simple refinements of the standard alignment process using the IBM models that actually deteriorate the alignment quality. However, they improve the translation performance. We will show this on two translation models, a phrase based system similar to the one used in [7] and a finite state transducer based system as presented in [8]. The key point is that these methods adapt the alignments to the translation models that will make further use of them. 2. Related Work In [9] the authors conduct an experimental study on the correlation of AER as defined above and the actual translation performance. To our knowledge this is the first work that carries out such a detailed study. The conclusion of their work is that the alignment error rate is not a good measure for predicting translation performance. The main reason given is that AER does not penalize an unbalanced precision and recall. They propose to use the “standard” F-me"
2006.iwslt-papers.7,P02-1038,1,0.544429,"n flaw found in both of these measures is that they do not take the structure of the translation model into account. 3. Phrase-Based Translation In this section we will briefly discuss the standard phrase based approach to machine translation, and we will pay special attention to the phrase extraction method. As usual, we will denote the (given) source sentence with f1J = f1 . . . fJ , which is to be translated into a target language sentence eI1 = e1 . . . eI . The usual approach in most state-of-the-art translation systems models the translation probability directly using a log-linear model [10]: P  M J I exp ) , f λ h (e m=1 m m 1 1  , (5) P p(eI1 |f1J ) = X M eI1 , f1J ) exp m=1 λm hm (˜ I e˜1 with a set of different models hm , scaling factors λm and the denominator a normalization factor that can be ignored in the maximization process. The most important models in equation (5) normally are phrase-based models in both source-totarget and target-to-source directions. In order to extract these phrase-based models, an alignment between the source and target training sentences is found by using the standard IBM models in both directions (source-to-target and target-to-source) and"
2006.iwslt-papers.7,2005.mtsummit-papers.36,0,0.0388298,"89 2 000 54 247 57 945 ∼ = argmax max P r(A) · P r(f1J , e˜J1 |A) e˜J 1 A∈A ∼ = argmax max e˜J 1 P r(fj , e˜j |f1j−1 , e˜j−1 1 , A) A∈A fj :j=1...J = argmax max e˜J 1 Table 1: Statistics of the Europarl corpus. Y A∈A Y j−1 p(fj , e˜j |fj−m , e˜j−1 j−m , A) . fj :j=1...J In other words: if we assume a uniform distribution for P r(A), the translation problem can be mapped to the problem of estimating an m-gram language model over a learned set of bilingual tuples (fj , e˜j ). In our case we represent this language model as a weighted finite state transducer, but this is not the only possibility [11]. Assume that the alignment is a function of the target words A′ : {1, . . . , I} → {1, . . . , J}, then the bilingual tuples (fj , e˜j ) can be inferred with e. g. the GIATI method of [4]. Each source word will be mapped to a target phrase of one or more words or an “empty” phrase ε. In particular, the source words which will remain non-aligned due to the alignment functionality restriction are paired with the empty phrase. However the alignments produced by the standard alignment generation procedure do not have this functionlike property. Furthermore, assuming that we could have such an ali"
2006.iwslt-papers.7,C04-1032,1,0.845511,"imates may be poor. 4.1. Alignment Adaptation This problem can be solved by reordering either the source or the target training sentences (both in training and test phases) in a way such that alignments become monotonic for all sentences. In [8] a method is presented to obtain an alignment that fulfill both requirements. Here we will give an overview of it. First, we estimate a cost matrix C for each sentence pair (f1J , eI1 ). The elements of this matrix cij are the local costs of aligning a source word fj to a target word ei . This cost matrix is estimated using the original IBM models, see [12] for more detail. For a given alignment A ⊆ I × J, define the costs of this alignment, c(A), as the sum of the local costs of all aligned word pairs: X c(A) = cij (7) (i,j)∈A The goal is to find an alignment with the minimum costs which fulfills the given constraints. In a first step, we require the alignment to be a function of source words A1 : {1, . . . , J} → {1, . . . , I} in order to uniquely define a reordering of the source sentence. This is easily computed from the cost matrix C as: A1 (j) = argmin cij . (8) i Non-aligned source words are not allowed. A1 naturally defines a new order"
2006.iwslt-papers.7,W05-0820,0,0.0126352,"ment (using a “reordered” cost matrix) with a dynamic programming algorithm similar to the Levenshtein string edit distance algorithm. An example of this method is shown in Figure 2. Because of the special constraints we require for this model, the alignment quality is expected to be relatively poor. 5. Experimental Results In this section we will analyze the impact the alignment methods described in Sections 3.1 and 4.1 have on both alignment and translation quality. For this, experiments will be reported on the Europarl corpus as used in the ACL 2005 Machine Translation Workshop Shared Task [13], for the German-English language pair. The corpus consists of the proceedings of the European Parliament, which are published on the web. Statistics are shown in Table 1. This corpus was chosen because of the different structure of the German and the English languages, that allows to better observe the effect of the alignments than for other language pairs, where the alignment is quasi-monotonic (e.g. English-Spanish). In order to have a reference alignment, we randomly selected a subset of the training corpus, consisting of 508 sentences, and manually annotated the alignments. Contrary to th"
2006.iwslt-papers.7,P06-1002,0,0.117678,"the translation process. If we had perfect statistical translation models that could generate a completely correct translation given a perfect alignment, it could perfectly be that a direct relation between alignment quality and translation quality would exist. However we do not have such perfect models and the training procedure can be “confused” when it finds structures it does not expect, although they may be completely correct. Therefore it can be of advantage to sacrifice some alignment quality in order to better guide the training process and have more robust estimations. A recent work [14] actually presents a new measure called “consistent phrase error rate” which tries to extend the AER to the concept of phrases. The authors show how this measure correlates better with translation performance, but it is however to much oriented to a phrase-based system and we expect it to perform poorly for other translation approaches5 . But one can take a step further. The alignment concept was first introduced as a hidden variable for the training of the single-word based models. Let them remain hidden then. When switching to phrase-based models the given data is assumed to be not only the"
2006.iwslt-papers.7,P03-1041,0,0.0397145,"Missing"
2006.iwslt-papers.7,J93-2003,0,\N,Missing
2006.iwslt-papers.7,J07-3002,0,\N,Missing
2011.eamt-1.36,P08-1087,0,0.0833238,"Missing"
2011.eamt-1.36,W10-1703,0,0.0895181,"Missing"
2011.eamt-1.36,E09-1049,0,0.0159472,"r modification improve some aspect of c 2011 European Association for Machine Translation. the system, even if it does not improve the overall score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed in (Popovi´c et al., 2006) for"
2011.eamt-1.36,W09-0433,0,0.0143606,"pect of c 2011 European Association for Machine Translation. the system, even if it does not improve the overall score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed in (Popovi´c et al., 2006) for the estimation of"
2011.eamt-1.36,2005.eamt-1.13,0,0.0558952,"Missing"
2011.eamt-1.36,H05-2007,0,0.0301208,"cation has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed in (Popovi´c et al., 2006) for the estimation of inflectional and reordering errors. A method based on WER and PER decomposition for discovering inflectional errors and missing words is presented in (Popovi´c and Ney, 2007). Zhou (2008) proposed a diagnostic evaluation of linguistic check-points obtained automatically by aligning parsed sourc"
2011.eamt-1.36,2008.eamt-1.17,0,0.0176041,"? Does a particular modification improve some aspect of c 2011 European Association for Machine Translation. the system, even if it does not improve the overall score? Does a worse–ranked system outperform a better–ranked one in any aspect?, etc. In order to answer such questions, a framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), where a classification scheme based on (Llitj´os et al., 2005) is presented together with a detailed analysis of the obtained results. The method has become widely used in recent years (Avramidis and Koehn, 2008; Max et al., 2008; Khalilov and Fonollosa, 2009; Li et al., 2009). Still, human error classification is resource-intensive and might become practically unfeasible when translating into many languages. As for automatic methods, an approach for automatic identification of patterns in translation output using POS sequences is proposed in (Lopez and Resnik, 2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between Word Error Rate (WER) and Positionindependent Word Error Rate (PER) for nouns, adjectives and verbs has been proposed"
2011.eamt-1.36,W07-0707,1,0.92829,"Missing"
2011.eamt-1.36,W06-3101,1,0.864371,"Missing"
2011.eamt-1.36,vilar-etal-2006-error,0,0.546589,"Missing"
2011.eamt-1.36,C08-1141,0,0.296812,"Missing"
2011.eamt-1.36,W09-0401,0,\N,Missing
2011.iwslt-evaluation.13,P07-2045,0,0.00313389,"the perplexity on the test2010 corpus. In spite of its small size, the low perplexity of the TED Talk corpus seems to indicate that it is the better suited for this task. This is not a reliable measure, of course, but it can give an early indication of the similarity of the corpora. As a starting point and in order to create different MT systems to combine, we trained two freely available machine translation systems on some of the available bilingual corpora for the evaluation (driven partly by the running time needed to train a full system from scratch). As phrase-based system we used Moses [4], the current standard toolkit for phrase-based translation. We trained the system with a standard setup, using the dev2010 corpus as development set for minimum error rate training. As a hierarchical phrase-based system we used the Jane toolkit [5], freely available for non-commercial use. The alignments were taken over from the corresponding Moses systems and again a fairly standard setup was used, optimizing on the dev2010 corpus. Both systems used the same language model: a 4-gram language model trained on the monolingual TED data. The results1 of the different baseline setups are summariz"
2011.iwslt-evaluation.13,W10-1738,1,0.842521,"n of the similarity of the corpora. As a starting point and in order to create different MT systems to combine, we trained two freely available machine translation systems on some of the available bilingual corpora for the evaluation (driven partly by the running time needed to train a full system from scratch). As phrase-based system we used Moses [4], the current standard toolkit for phrase-based translation. We trained the system with a standard setup, using the dev2010 corpus as development set for minimum error rate training. As a hierarchical phrase-based system we used the Jane toolkit [5], freely available for non-commercial use. The alignments were taken over from the corresponding Moses systems and again a fairly standard setup was used, optimizing on the dev2010 corpus. Both systems used the same language model: a 4-gram language model trained on the monolingual TED data. The results1 of the different baseline setups are summarized in Table 2. It can be seen that the choice of training 1 The BLEU scores are cases-sensitive and computed used preprocessed references, in the same way as the preprocessing of the original data. As such it may not fully agree with officially calc"
2011.iwslt-evaluation.13,D08-1064,0,0.0141918,"System BLEU BLEU was introduced in [7] and has shown to have a high correlation with human judgement. In spite of its shortcomings [8], it has been considered the standard automatic measure in the development of SMT systems (with new measures being added upon, but not substituting it). Of course, the main problem of using the BLEU score as a feature for sentence selection in a real-life scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This 2 There is evidence [6], that this method does not necessarily produce the best complete hypothesis, but it should be a good enough indicator for our purposes and further discussion (see also 3.2). 99 Documents Systems 20.4 34.1 32.1 26.3 29.9 29.9 33.8 34.3 27.4 33.8 26.8 18.9 30.1 25.6 23.8 27.1 25.1 27.9 28.0 22.8 32.2 23.8 19.3 31.9 26.2 24.3 28.3 28.1 31.6 29.2 24.0 33.9 25.4 20.8 35.5 32.1 25.9 30.2 31.0 34.3 34.0 28.4 34.4 25.0 21.3 33.8 31.0 26.2 30.8 30.7 33.5 32.2 27.4 33.7 26.8 20.6 33.7 30.6 25.5 29.0 29.0 33.9 33.1 27.0 35.0 27.9 17.4 27.8 23.0 20.7 25.1 21.2 26.1 25.2 21.6 29.0 20.3 17.3 28.9 25.1 21.9"
2011.iwslt-evaluation.13,P02-1040,0,0.0903367,"which provides the best translation, using some automatic method akin to text classification. However this strategy showed not to be effective. Table 3 shows an overview of the BLEU scores of each of the documents composing the test2010 corpus, with the cells shaded to provide a more visual overview of the distribution of the scores. It can be observed that the best system is generally the same for most documents, and the difference in BLEU scores is not very large. Indeed, if we generate a new complete hypothesis by selecting the best system for 4.1. Cross System BLEU BLEU was introduced in [7] and has shown to have a high correlation with human judgement. In spite of its shortcomings [8], it has been considered the standard automatic measure in the development of SMT systems (with new measures being added upon, but not substituting it). Of course, the main problem of using the BLEU score as a feature for sentence selection in a real-life scenario is that we do not have the references available. We overcame this issue by generating a custom set of references for each system, using the other systems as gold translations. This 2 There is evidence [6], that this method does not necessa"
2011.iwslt-evaluation.13,W11-2109,1,0.811438,"e source input and the system outputs and computed the source to target ratio for such scores. As an additional feature, we included counts and source to target ratios of verb phrases, given the same isomorphism assumption and the fact that a possible “loss” of verb (not explicitly handled by a language model) would radically decrease sentence quality. Further parsing features are subject of future work. 4.4. IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. [17]). Additionally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT evaluations [19], this approach is still experi"
2011.iwslt-evaluation.13,W06-3110,0,0.200778,"Missing"
2011.iwslt-evaluation.13,W11-2103,0,0.0127923,"nally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT evaluations [19], this approach is still experimental and in development stage. Therefore we also built a more traditional system based on log-linear models trained on the BLEU score. 5.1. Based on BLEU Log-linear models are at the heart of most state-of-the-art statistical machine translation systems. They model the translation probability of target sentence eI1 given source sentence f1J directly using the expression P  M J I exp λ h (f , e ) m m 1 1 m=1 P , (1) p(eI1 |f1J ) = P M J I) exp λ h (f , e ˜ I m m 1 1 e˜ m=1 1 where the hm are feature functions as the ones described in Section 4 and the λm are"
2011.iwslt-evaluation.13,P07-2026,0,0.328387,"Missing"
2011.iwslt-evaluation.13,2008.amta-srw.3,0,0.0623263,"tion systems. They model the translation probability of target sentence eI1 given source sentence f1J directly using the expression P  M J I exp λ h (f , e ) m m 1 1 m=1 P , (1) p(eI1 |f1J ) = P M J I) exp λ h (f , e ˜ I m m 1 1 e˜ m=1 1 where the hm are feature functions as the ones described in Section 4 and the λm are the corresponding scaling factors, which we optimize with standard MERT training with respect to the BLEU score. This is also the usual approach used for rescoring n-best lists generated by a single system, and has been used previously for sentence selection purposes (see [20] which uses a very similar approach to our own). Note that no system dependent features like translation probabilities were computed, as we wanted to keep the system general. In fact, for the system combination task, only the single-best translation was provided, without additional information. Table 5 gives an overview of the effect of the different features used in this approach.5 It can be seen that the best performance is obtained when combining all the models. Language model scores alone are not powerful enough to give an improvement over the best single system, and the IBM1 scores even h"
2011.iwslt-evaluation.13,C04-1072,0,0.20734,"Missing"
2011.iwslt-evaluation.13,vilar-etal-2006-error,1,0.914641,"Missing"
2011.iwslt-evaluation.13,W11-2104,1,0.847176,"in this case (not negative!), this indicates that higher IBM1 scores are beneficial for the system. For the language models, the picture is mixed, the system tries to maximize the probability of some of them, but to minimize it for others. 5.2. Based on human ranking We considered employing a supervised machine learning approach trained over sentences evaluated by human judges, made available by the WMT evaluations. The system was trained based on human rankings of MT output and consequently used to replicate ranking for our sentence-level translation alternatives. According to this approach [21], ranking is decomposed into a set of pairwise decisions, where each translation output gets compared with each one of the other alternatives. For this purpose, a binary classifier is trained to learn to comFeature Sign Cross system BLEU (system level) Cross system BLEU (sentence level) Word penalty − − − EXTer hINFer hLEXer hRer MISer WER − − + + + + IBM1 − LM (Europarl) LM (Giga FrEn) LM (monolingual TED) LM (UN) LM (News) − − + + + Table 6: Sign of the scaling factors corresponding to the features. pare the output quality. The sentence that wins most of the pairwise comparisons (ranked firs"
2011.iwslt-evaluation.13,W08-0309,0,0.0621747,"Missing"
2011.iwslt-evaluation.13,P06-1055,0,0.0117871,"ng systems, optimizing the output for the highest probability of the consequent n-grams. On the other hand, automatic metrics are also based on n-grams matching with the reference translation. In order to avoid a possible overfitting on n-grams, but also to capture more complex phenomena (such as long distance structures and grammatical fluency) that are still important to quality output and may have been neglected by the statistical systems, we considered including features derived after parsing the systems’ output with Probabilistic Context Free Grammars (PCFG). For this the Berkeley Parser [16] was used. PCFG parsing allows the generation of n-best lists of trees, scored probabilistically, leading to the selection of the tree with the highest score. From this process, we extracted the number of distinct parsing trees of the sentence, after having allowed the generation of an n-best list of size n = 1000. A smaller number of trees could mean that there are less possible tree derivations, i.e. less parsing ambiguity. Parsing statistics are not only an indicator of the grammaticality of the sentence, but also of how complicated it is, assuming it is fully grammatical. Therefore, we rel"
2011.iwslt-evaluation.13,N07-2015,0,0.109556,"mmatical). For this reason, we parsed both the source input and the system outputs and computed the source to target ratio for such scores. As an additional feature, we included counts and source to target ratios of verb phrases, given the same isomorphism assumption and the fact that a possible “loss” of verb (not explicitly handled by a language model) would radically decrease sentence quality. Further parsing features are subject of future work. 4.4. IBM1 Scores IBM1-like scores on the sentence level are known to perform well for the rescoring of n-best lists from a single system (see e.g. [17]). Additionally, they have been shown in [18] to correlate well with human judgement for evaluation purposes. We thus include them as additional features. 4.5. Additional Language Models For the translation systems we trained ourselves we only included one language model trained on the TED data, as initial experimentation with other language models did not seem to 5. Sentence Selection Mechanism Two sentence selection mechanisms were tried out for this evaluation. Although our goal is to shift to a selection mechanism geared towards human evaluation, using the data made available in the WMT ev"
2011.iwslt-evaluation.13,W10-1703,0,0.0767624,"Missing"
2011.iwslt-evaluation.13,E06-1032,0,\N,Missing
2011.iwslt-evaluation.13,J11-4002,1,\N,Missing
2011.iwslt-evaluation.13,W09-0401,0,\N,Missing
2011.iwslt-evaluation.13,2011.iwslt-evaluation.1,0,\N,Missing
2013.mtsummit-posters.4,2010.iwslt-papers.1,0,0.0149183,"h means that the verb read needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which hel"
2013.mtsummit-posters.4,2012.eamt-1.44,0,0.0167737,"ad needs a NP playing the role of the subject to its left to constitute a full sentence S. The same verb read is assigned a different supertag (SNP)/NP in the sentence he reads a book. The supertag (SNP)/NP denotes a transitive verb which needs a NP to its left playing the role of the subject and a NP to its right playing the role of the object in order to constitute a full sentence S. 4 4.1 Our Approach Motivation CCG has many unique qualities which made it an attractive grammar formalism to be incorporated into SMT systems (Hassan et al., 2007; Hassan et al., 2009; Almaghout et al., 2010; Almaghout et al., 2012) . These qualities can also be exploited in building a CCG-based QE metric which evaluates the grammaticality of the translation output. First, CCG allows for flexible structures thanks to its combinatory rules. Thus, it is possible to assign a CCG category to phrases which do not represent standard syntactic constituents. This is an important feature for SMT systems as SMT phrases are statistically extracted, and do not necessarily correspond to syntactic constituents. This same feature can also be used to detect grammatical chunks in the translation output, which helps 225 to estimate its gr"
2013.mtsummit-posters.4,W11-2104,1,0.915377,"to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and tar"
2013.mtsummit-posters.4,J99-2004,0,0.0441188,"al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to boost their performance over the baseline system. 224 3 Combinatory Categorial Grammar CCG (Steedman, 2000) is a grammar formalism which consists of a lexicon that pairs words with lexical categories (supertags, cf. Bangalore and Joshi (1999)) and a set of combinatory rules which specify how the categories are combined. A supertag is a rich syntactic description that specifies the local syntactic context of the word at the lexical level in the form of a set of arguments. CCG builds a parse tree for a sentence by combining CCG categories using a set of binary combinatory rules. Most of the CCG grammar is contained in the lexicon, which is why CCG has simpler rules compared to CFG productions. CCG categories are divided into atomic and complex categories. Examples of atomic categories are S (sentence), N (noun), NP (noun phrase), et"
2013.mtsummit-posters.4,C04-1046,0,0.0459933,"syntactic categories, we were able to extract grammaticality QE features based on recognising grammatical chunks and examining sequences of CCG categories in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum En"
2013.mtsummit-posters.4,W10-1703,0,0.0690335,"Missing"
2013.mtsummit-posters.4,W12-3102,0,0.0576051,"Missing"
2013.mtsummit-posters.4,W12-3110,0,0.0115413,"et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Specia (2012) compare the performance of a set of linguistic features extracted from source and target sentences constituency and dependency trees with the baseline system of the WMT 2012 evaluation campaign (Callison-Burch et al., 2012). Some of these features compare syntactic structures between source and target sentences whereas other features focus on detecting common grammatical errors committed by SMT systems. They show that the linguistic features alone were not able to outperform the baseline system. However, they show that following a proper selection procedure for linguistic features helps to bo"
2013.mtsummit-posters.4,2011.eamt-1.32,0,0.0415548,"Missing"
2013.mtsummit-posters.4,P07-1037,0,0.0606055,"Missing"
2013.mtsummit-posters.4,D09-1123,0,0.0510684,"Missing"
2013.mtsummit-posters.4,W12-3117,0,0.0190571,"features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a set of syntaxbased QE features originally developed to judge the grammaticality of sentences. Some syntactic features compare POS n-gram frequencies between the output sentence and a reference corpus. The features also include parsing features extracted from parse trees built using precision grammar, which is originally developed to detect grammatical errors. Other parsing-based features rely on information produced by parsers trained on well-formed and malformed sentences which result from introducing grammatical errors in the treebank on which the parser is trained. Felice and Spe"
2013.mtsummit-posters.4,2006.amta-papers.25,0,0.126803,"Missing"
2013.mtsummit-posters.4,2010.jec-1.5,0,0.0451371,"Missing"
2013.mtsummit-posters.4,2009.mtsummit-papers.16,0,0.0118631,"s in the translation output. We also tackle the problem of parsing ungrammatical output by restricting the coverage of the CCG parser. The rest of this paper is organised as follows. Section 2 reviews related work. Section 3 provides an introduction to CCG. Section 4 describes our approach. Section 5 presents our experiments. Finally, Section 6 concludes and provides avenues for future work. 2 Related Work The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic fea"
2013.mtsummit-posters.4,2011.mtsummit-papers.58,0,0.160181,"trate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased QE features which represent the following information extracted from PCFG parse trees of the source and target sentences: • • • • Best parse tree log likelihood. Number of n-best trees. Confidence for the best parse tree. Average confidence of all trees. Avramidis et al. (2011) demonstrate that these parsing-based features are able to achieve better correlation than non-linguistic-based features. Specia et al. (2011) propose a set of QE features to predict the adequacy of translation. The features include the following syntactic features extracted from source and target dependency and constituency parse trees: • Proportion of dependency relations with aligned constituents between source and target sentences. • The same previous feature but with the order of constituents ignored. • The same as the first feature but with Giza threshold equals to 0.1. • Absolute difference between the depth of the syntactic tree for the source and the depth of the syntactic tree for the target. Rubino et al. (2012) extract a"
2013.mtsummit-posters.4,2011.eamt-1.12,0,0.0120636,"uk translation and sometimes from internal translation information output by the MT system. With the improvement of the quality of MT systems and their increasing use in real-world applications, MT QE has become increasingly more important. QE has been demonstrated to help in making the integration of MT systems in the translation pipeline more efficient. For example, using QE to filter out low-quality translations from the post-editing process has been shown to help in reducing post-editing time as low-quality translations might take more time to post-edit than to be translated from scratch (Specia, 2011). Furthermore, QE helps to enhance MT user experience by informing the user of the predicted quality of the translation produced by the MT system. Moreover, QE has been more and more used to enhance the quality of MT systems by integrating QE scores in n-best reranking and combining the translation of different MT systems. QE features estimate the quality of the translation by capturing the aspects which evaluate translation quality, namely fluency and adequacy, in addition to predicting the difficulty of the translation. Adequacy refers to the extent to which the meaning of the source sentenc"
2013.mtsummit-posters.4,P10-1062,0,0.0244835,"rk The first QE models were proposed by Blatz et al. (2004). They use data labeled with automatic MT metrics to learn QE models based on features extracted from the input and output sentences. Specia et al. (2009) add to the features proposed by Blatz et al. (2004) a set of features divided into “black-box” features i.e. MT system independent features and “glass-box” features i.e. features which use internal information from the MT system. They use training data annotated by both NIST and human annotation. Using grammaticality features in QE has been demonstrated to improve their performance. Xiong et al. (2010) build a QE metric based on a Maximum Entropy classifier in which they integrate linguistic and lexical features to predict the correctness of each word in the translation output. Linguistic features are based on Link Grammar, which parses a sentence by pairing its words. They hypothesise that words which the parser fails to link to other words are likely to be grammatically incorrect. They demonstrate that linguistic features help to improve performance over lexical features and further improvement is gained when these two types are combined. Avramidis et al. (2011) propose PCFG parsingbased"
2013.mtsummit-posters.5,2003.mtsummit-systems.1,0,0.0537752,"ence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the best general purpose MT engines, it has been included in order to allow us to assess the performance level of our SMT system and also to compare it directly with other MT ap"
2013.mtsummit-posters.5,W10-1703,0,0.0302962,"number of translators working on it. 2.1 Translation systems used The evaluated translation outputs presented in this work are produced by German-English, GermanFrench and German-Spanish machine translation Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranki"
2013.mtsummit-posters.5,W12-3102,0,0.0648542,"Missing"
2013.mtsummit-posters.5,vilar-etal-2006-error,1,0.893094,"Missing"
2013.mtsummit-posters.5,W10-1738,1,0.850961,"level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine translation (RBMT) system with sophisticated handwritten transfer and generation rules adapted to domains by importing domain-specific terminology. RBMT: Another widely used commercial rulebased machine translation system whose name is not mentioned here.1 Google Translate2 : a web-based machine translation engine also based on statistical approach. Since this system is known as one of the be"
2013.mtsummit-posters.5,federmann-2010-appraise,0,0.0730217,"nslation Summit (Nice, September 2–6, 2013), p. 231–238. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and tech"
2013.mtsummit-posters.5,P07-2045,0,0.0038879,"licence, no derivative works, attribution, CC-BY-ND. systems in both directions. The test sets consist of three domains: news texts taken from WMT tasks (Callison-Burch et al., 2010), technical documentation extracted from the freely available OpenOffice project (Tiedemann, 2009) and client data owned by project partners. The following translation systems were considered: the defined sentence-level evaluation tasks using the browser-based evaluation tool Appraise (Federmann, 2010). The reference translation was not shown in any task, only the source sentence and the translation output. Moses (Koehn et al., 2007): a phrase-based statistical machine translation (SMT) system trained on news texts and technical documentation (no client data were available for training). Ranking: for each source sentence, rank the outputs of five different MT systems (Trados is excluded, explanation below) according to how well these preserve the meaning of the source sentence. Ties were allowed. Jane (Vilar et al., 2010): a hierarchical phrasebased SMT system trained on news texts and technical documentation (no client data were available for training). Lucy MT (Alonso and Thurmair, 2003): a commercial rule-based machine"
2013.mtsummit-posters.5,W12-3123,0,0.0364801,"ent of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluation process has b"
2013.mtsummit-posters.5,2011.eamt-1.12,0,0.0194594,"rther improvement of MT quality and applicability. 1 Introduction and related work A widely used practice for MT evaluation is ranking outputs of different machine translation systems by human annotators, e.g. in WMT shared tasks (Callison-Burch et al., 2012). While this is an important step towards an understanding of their quality, it does not provide enough scientific insights. In the last years, human error analysis is often carried out in order to better understand some phenomena (Vilar et al., 2006), and recently more and more attention is paid to various aspects of post-editing effort (Specia, 2011; Koponen, 2012). However, to the best of our knowledge, no study has been carried out yet which puts all these aspects together. This paper describes the results of detailed human evaluation covering all three aspects: ranking, error classification and post-editing. The approach arises from the need to detach MT evaluation from a pure research-oriented development scenario and to bring it closer to the end users. Therefore, evaluation has been performed in close co-operation with translation industry. All evaluation tasks have been performed by qualified professional translators. The evaluati"
2013.mtsummit-wptp.2,2003.mtsummit-systems.1,0,0.0711019,"1 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), se"
2013.mtsummit-wptp.2,federmann-2010-appraise,0,0.0122073,"e sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and post-edit: for each source sentence (11852 sentences in total), select the translation output which is easiest to post-edit and perform the editing. Post-edit all: for each source sentence in the selected subset (4070 sentences in total), postedit all four produced translation outputs. For both post-editing tasks, the translators"
2013.mtsummit-wptp.2,P10-1064,0,0.032193,"Missing"
2013.mtsummit-wptp.2,W12-3123,0,0.0120168,"proved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by translation memory), they do not examine ranking of these outputs, they have not tested their automatic method by professi"
2013.mtsummit-wptp.2,2011.eamt-1.12,0,0.0122675,", five types of performed edit operations are analysed: correcting word form, reordering, adding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, si"
2013.mtsummit-wptp.2,2012.amta-wptp.9,0,0.0175217,"dding missing words, deleting extra words and correcting lexical choice. 1 Motivation and related work Machine translation (MT) has improved considerably in recent years thus gaining recognition in the translation industry. However, machine translation outputs have not yet reached the same quality as human translations. Performing the post-editing has become a common practice for improving machine translation outputs. Therefore, more and more attention is paid to various aspects of postediting, such as (Specia, 2011). Prediction of errors in rule-based system outputs has been investigated in (Valotkaite and Asadullah, 2012) in order to facilitate the post-editing process. Analysis of edit operations has been carried out in (Koponen, 2012) in order to understand discrepances between edit distance and translation quality (i.e. predicted post-editing effort). Our work explores the selection criteria applied by professional translators when several translation outputs of each source sentence are offered for post-editing. The scenario is similar to the one in (He et al., 2010), but our approach goes beyond, since they consider only two outputs (one produced by statistical machine translation system and other by trans"
2013.mtsummit-wptp.2,W10-1738,1,0.708534,"n-de es-de fr-de Total News 1788 514 912 1744 101 1852 6911 OpenOffice 418 414 412 414 413 412 2483 Client 500 548 382 0 1028 0 2458 Total 2706 1476 1706 2158 1542 2264 11852 rank Overall News OpenOffice Client de-en de-es de-fr en-de es-de fr-de Table 1: Test sets for ranking task and selecting for post-edit task – number of source sentences per language pair and domain. source sentences per language pair and domain can be seen in Table 4. Four translation systems were used: a phrasebased statistical machine translation (SMT) system Moses (Koehn et al., 2007), a hierarchical SMT system Jane (Vilar et al., 2010), a commercial rule-based system Lucy (Alonso and Thurmair, 2003), and another commercial rule-based system RBMT1 . The translation outputs generated by the described systems were then given to professional translators in order to perform ranking and post-editing using the browser-based evaluation tool Appraise (Federmann, 2010). Ranking and post-editing tasks were defined as follows: Ranking: for each source sentence (11852 sentences in total), rank the outputs of four different MT systems according to how well these preserve the meaning of the source sentence. Ties were allowed. Select and p"
2014.eamt-1.38,W13-2201,0,0.100476,"Missing"
2014.eamt-1.38,W11-2107,0,0.017106,"n examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result of these limitations, there has been a"
2014.eamt-1.38,2010.eamt-1.12,0,0.209351,"Missing"
2014.eamt-1.38,1994.amta-1.9,0,0.935888,"lity Metric” MQM designed by the QTLaunchPad project (http://www.qt21.eu/launchpad). The metric was designed to facilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model,"
2014.eamt-1.38,P02-1040,0,0.0903186,"ticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR. 1 Introduction For a number of years, the Machine Translation (MT) community has used “black-box” measures of translation performance like BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2011). These methods have a number of advantages in that they can provide automatic scores for c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 165 MT output in cases where there are existing reference translations by calculating similarity between the MT output and the references. However, such metrics do not provide insight into the specific nature of problems encountered in the translation output and scores are tied to the particularities of the reference translations. As a result o"
2014.eamt-1.38,stymne-ahrenberg-2012-practice,0,0.494563,"cilitate annotation of MT output by human translators while containing analytic error classes we considered relevant to MT research (see Section 2, below). This paper represents the first publication of results from use of MQM for MT quality analysis. Previous research in this area has used error categories to describe error types. For instance, Farr´us et al. (2010) divide errors into five broad classes (orthographic, morphological, lexical, semantic, and syntactic). By contrast, Flanagan (1994) uses 18 more fine-grained error categories with additional language-pair specific features, while Stymne and Ahrenberg (2012) use ten error types of somewhat more intermediate granularity (and specifically addresses combinations of multiple error types). All of these categorization schemes are ad hoc creations that serve a particular analytic goal. MQM, however, provides a general mechanism for describing a family of related metrics that share a common vocabulary. This metric was based upon a rigorous examination of major human and machine translation assessment metrics (e.g., LISA QA Model, SAE J2450, TAUS DQF, ATA assessment, and various tool-specific metrics) that served as the basis for a descriptive framework f"
2014.eamt-1.38,vilar-etal-2006-error,0,0.85614,"Missing"
2014.eamt-1.41,2011.mtsummit-papers.17,0,0.162598,"– Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas removing additions ha"
2014.eamt-1.41,W12-3102,0,0.0667545,"Missing"
2014.eamt-1.41,2012.eamt-1.35,0,0.0190046,"(Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2012 ok 323 31 200 quality level edit+ edit edit1559 0 544 399 0 550 548 856 576 make the translation acceptable. Post-editing time is measured on the sentence level in a controlled way in order to isolate factors such as pauses between sentences. The technical effort is represented by following five types of edit operations: bad 99 20 74 Table 1: Corpus statistics: number of sentences assigned to each"
2014.eamt-1.41,W12-3123,0,0.0661415,"under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 191 More details about the technical effort can be obtained by analysing particular edit operations. (Blain et al., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al"
2014.eamt-1.41,J11-4002,1,0.862953,"Missing"
2014.eamt-1.41,specia-etal-2010-dataset,0,0.058975,"Missing"
2014.eamt-1.41,2011.eamt-1.12,0,0.111983,"szkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most time, whereas"
2014.eamt-1.41,2009.mtsummit-posters.20,0,0.295234,"e Lommel, Aljoscha Burchardt, Eleftherios Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most c"
2014.eamt-1.41,2010.jec-1.6,0,0.0874217,"Missing"
2014.eamt-1.41,temnikova-2010-cognitive,0,0.116741,"Avramidis, Hans Uszkoreit DFKI – Berlin, Germany name.surname@dfki.de Abstract Since the temporal aspect is important for the practice, post-editing time is widely used for measuring post-editing effort (Krings, 2001; Tatsumi, 2009; Tatsumi et Roturier, 2010; Specia, 2011). Human quality scores based on the needed amount of post-editing are involved as assessment of the cognitive effort in (Specia et al., 2010; Specia, 2011). Using edit distance between the original and the post-edited translation for assessment of the technical effort is reported in (Tatsumi, 2009; Tatsumi et Roturier, 2010; Temnikova, 2010; Specia, 2011; Blain et al., 2011). Despite the growing interest in and use of machine translation post-edited outputs, there is little research work exploring different types of post-editing operations, i.e. types of translation errors corrected by post-editing. This work investigates five types of post-edit operations and their relation with cognitive post-editing effort (quality level) and postediting time. Our results show that for French-to-English and English-to-Spanish translation outputs, lexical and word order edit operations require most cognitive effort, lexical edits require most"
2014.eamt-1.41,2013.mtsummit-papers.15,0,0.0298537,"., 2011) defined these operations on a linguistic level as post-editing actions and performed comparison between statistical and rulebased systems. (Temnikova, 2010) proposed the analysis of edit operations for controlled language in order to explore cognitive effort for different error types – post-editors assigned one of ten error types to each edit operation which were then ranked by difficulty. In (Koponen, 2012) post-edit operations are analysed in sentences with discrepancy between the assigned quality score and the number of performed post-edits. In one of the experiments described in (Wisniewski et al., 2013) an automatic analysis of post-edits based on Levenshtein distance is carried out considering only the basic level of substitutions, deletions, insertions and TER shifts. These edit operations are analysed on the lexical level in order to determine the most frequent affected words. General user preferences regarding different types of machine translation errors are explored in (Kirchhoff et al., 2012) for English-Spanish translation of texts from publich health domain, however without any relation to post-editing task. (Popovi´c and Ney(, 2011) number of sentences fr-en 2011 en-es 2011 en-es 2"
2015.eamt-1.14,W05-0814,0,0.0246803,"sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translation output • first four letters of the word (4let) The simplest way for word reduction is to use only its first n letters. The choice of first four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • first two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more refined method which splits words into stems and suffixes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained"
2015.eamt-1.14,E03-1076,0,0.060003,"lemmas, it would not be possible to detect any inflectional error thus setting the inflectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inflective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classification results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The suffix of each word is removed and only the stem is preserved. For calculation of stem and suffix frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translatio"
2015.eamt-1.14,2005.mtsummit-papers.11,0,0.0154666,"error rates. The best way for the assessment would be, of course, a comparison with human error classification. Nevertheless, this has not been done for two reasons: first, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliabl"
2015.eamt-1.14,J11-4002,1,0.90191,"Missing"
2015.eamt-1.14,2011.eamt-1.12,0,0.0137748,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using"
2015.eamt-1.14,E09-1087,0,0.0737483,"Missing"
2015.eamt-1.14,tiedemann-2012-parallel,0,0.0135084,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenia"
2015.eamt-1.14,steinberger-etal-2012-dgt,0,\N,Missing
2015.eamt-1.14,P02-1040,0,\N,Missing
2015.eamt-1.14,P07-2045,0,\N,Missing
2015.eamt-1.14,J03-1002,0,\N,Missing
2015.eamt-1.14,W14-4210,1,\N,Missing
2015.eamt-1.14,etchegoyhen-etal-2014-machine,0,\N,Missing
2015.eamt-1.15,W05-0814,0,0.0385293,"sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translation output • first four letters of the word (4let) The simplest way for word reduction is to use only its first n letters. The choice of first four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • first two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more refined method which splits words into stems and suffixes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained"
2015.eamt-1.15,E03-1076,0,0.0450815,"lemmas, it would not be possible to detect any inflectional error thus setting the inflectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inflective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classification results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The suffix of each word is removed and only the stem is preserved. For calculation of stem and suffix frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classifier are: • to estimate the error distribution within a translatio"
2015.eamt-1.15,2005.mtsummit-papers.11,0,0.0120371,"error rates. The best way for the assessment would be, of course, a comparison with human error classification. Nevertheless, this has not been done for two reasons: first, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliabl"
2015.eamt-1.15,J11-4002,1,0.893851,"Missing"
2015.eamt-1.15,2011.eamt-1.12,0,0.0238497,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using"
2015.eamt-1.15,E09-1087,0,0.0700919,"Missing"
2015.eamt-1.15,tiedemann-2012-parallel,0,0.0242552,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufficient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-specific corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenia"
2015.eamt-1.15,W11-2103,0,\N,Missing
2020.coling-main.444,W07-0718,0,0.238983,"y) as the two major characteristic of a translation. For each of those two criteria, the evaluators assigned overall quality scores on 9 point scales. The ARPA MT Initiative (White et al., 1994) defines “adequacy”, “fluency” and “comprehension” as a standard set of concepts for MT evaluation, and the evaluators are instructed to assign overall scores based on those three criteria. Years later, the first WMT (Workshop/Conference on Machine Translation) translation shared task in 20061 as well as the subsequent task in 2007 adopted adequacy and fluency as official metrics (Koehn and Monz, 2006; Callison-Burch et al., 2007). The participants in the shared task are asked to assign adequacy and fluency scores on 5 point scales to each submitted MT output. Later on, a binary ranking of two MT outputs was proposed (Vilar et al., 2007), and in a slightly changed form (comparing up to five outputs instead of two) became the official metric at WMT 2008 (Callison-Burch et al., 2008). It was reported that it required less effort and showed better inter-annotator agreement than adequacy and fluency scores, and it remained the official WMT metric until 2016. The quality critera for ranking were never explicitly defined, bu"
2020.coling-main.444,W08-0309,0,0.307751,"on those three criteria. Years later, the first WMT (Workshop/Conference on Machine Translation) translation shared task in 20061 as well as the subsequent task in 2007 adopted adequacy and fluency as official metrics (Koehn and Monz, 2006; Callison-Burch et al., 2007). The participants in the shared task are asked to assign adequacy and fluency scores on 5 point scales to each submitted MT output. Later on, a binary ranking of two MT outputs was proposed (Vilar et al., 2007), and in a slightly changed form (comparing up to five outputs instead of two) became the official metric at WMT 2008 (Callison-Burch et al., 2008). It was reported that it required less effort and showed better inter-annotator agreement than adequacy and fluency scores, and it remained the official WMT metric until 2016. The quality critera for ranking were never explicitly defined, but it was implicitly based on a combination of adequacy and fluency. Another method for assigning quality scores, continuous direct assessment (Graham et al., 2013), does not use discrete scales but assigns a continuous score between 0 and 100. Both adequacy and fluency were investigated as the guiding criterion, and it was concluded that the best option wa"
2020.coling-main.444,2020.lrec-1.461,1,0.878721,"Missing"
2020.coling-main.444,W18-6320,0,0.255068,"n any genre/domain and language pair, and it can be guided by various types of quality criteria. Also, it is not restricted to MT output, but can be used for other types of generated text. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2019), and all of them are essentially based on some of the following three quality criteria: adequacy (accuracy, fidelity), comprehensibility (intelligibility) and fluency (grammaticality). Adequacy measures how well the meaning of the original text is conveyed to the translated text. Comprehensibility reflects how well a reader is able to understand the translated text without access to the original text. Fluency describes the grammar of the target language in the translated text. The choice of criteria often depends on the task and on the purpose of the translation: for e"
2020.coling-main.444,W13-2305,0,0.769697,": it can be applied on any genre/domain and language pair, and it can be guided by various types of quality criteria. Also, it is not restricted to MT output, but can be used for other types of generated text. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2019), and all of them are essentially based on some of the following three quality criteria: adequacy (accuracy, fidelity), comprehensibility (intelligibility) and fluency (grammaticality). Adequacy measures how well the meaning of the original text is conveyed to the translated text. Comprehensibility reflects how well a reader is able to understand the translated text without access to the original text. Fluency describes the grammar of the target language in the translated text. The choice of criteria often depends on the task and on the purpose of"
2020.coling-main.444,W06-3114,0,0.527827,"tions. The proposed method is very general: it can be applied on any genre/domain and language pair, and it can be guided by various types of quality criteria. Also, it is not restricted to MT output, but can be used for other types of generated text. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2019), and all of them are essentially based on some of the following three quality criteria: adequacy (accuracy, fidelity), comprehensibility (intelligibility) and fluency (grammaticality). Adequacy measures how well the meaning of the original text is conveyed to the translated text. Comprehensibility reflects how well a reader is able to understand the translated text without access to the original text. Fluency describes the grammar of the target language in the translated text. The choice of criteria often d"
2020.coling-main.444,D18-1512,0,0.0598646,"Missing"
2020.coling-main.444,2014.eamt-1.38,1,0.90321,"Missing"
2020.coling-main.444,P11-1015,0,0.0348733,"Missing"
2020.coling-main.444,2020.conll-1.19,1,0.877797,"Missing"
2020.coling-main.444,I13-1166,0,0.0271035,"by adequacy and to use fluency only as auxiliary criterion for adequacy ties (Bojar et al., 2016). Continuous direct assessment eventually replaced ranking at the WMT shared tasks in 2017 (Bojar et al., 2017) and is still used as the official WMT metric (Barrault et al., 2019). Some publications dealt with evaluating user reviews: the annotators were asked to assign scores on 5 point scale for comprehensibility and scores on 2 point scale (“true” or “false”) for fidelity (Roturier and Bensadoun, 2011). Those annotations were later used for quality estimation of these two evaluation criteria (Rubino et al., 2013). Other publications propose alternative methods for assessing comprehensibility, namely question answering (Scarton and Specia, 2016) as well as filling gaps as its cheaper version (Forcada et al., 2018). Nevetheless, none of these methods provide information about actual parts of the translation which are problematic in terms of the given quality criterion. Our method overcomes this drawback by marking 1 http://statmt.org/wmt06/ 5060 actual issues. Furthermore, our method is much less demanding than error classification, where the evaluators are asked to assign error classes according to a p"
2020.coling-main.444,L16-1579,0,0.0190884,"eventually replaced ranking at the WMT shared tasks in 2017 (Bojar et al., 2017) and is still used as the official WMT metric (Barrault et al., 2019). Some publications dealt with evaluating user reviews: the annotators were asked to assign scores on 5 point scale for comprehensibility and scores on 2 point scale (“true” or “false”) for fidelity (Roturier and Bensadoun, 2011). Those annotations were later used for quality estimation of these two evaluation criteria (Rubino et al., 2013). Other publications propose alternative methods for assessing comprehensibility, namely question answering (Scarton and Specia, 2016) as well as filling gaps as its cheaper version (Forcada et al., 2018). Nevetheless, none of these methods provide information about actual parts of the translation which are problematic in terms of the given quality criterion. Our method overcomes this drawback by marking 1 http://statmt.org/wmt06/ 5060 actual issues. Furthermore, our method is much less demanding than error classification, where the evaluators are asked to assign error classes according to a predefined typology, for example (Vilar et al., 2006), the MQM scheme2 (Lommel et al., 2014; Klubiˇcka et al., 2018), etc. Although our"
2020.coling-main.444,vilar-etal-2006-error,0,0.377174,"Missing"
2020.coling-main.444,W07-0713,0,0.892898,"thod is very general: it can be applied on any genre/domain and language pair, and it can be guided by various types of quality criteria. Also, it is not restricted to MT output, but can be used for other types of generated text. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2019), and all of them are essentially based on some of the following three quality criteria: adequacy (accuracy, fidelity), comprehensibility (intelligibility) and fluency (grammaticality). Adequacy measures how well the meaning of the original text is conveyed to the translated text. Comprehensibility reflects how well a reader is able to understand the translated text without access to the original text. Fluency describes the grammar of the target language in the translated text. The choice of criteria often depends on the task a"
2020.coling-main.444,1994.amta-1.25,0,0.901793,"Missing"
2020.conll-1.19,W19-5301,0,0.0578258,"Missing"
2020.conll-1.19,W07-0718,0,0.428226,"ate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT quality by assigning overall scores to MT outputs, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Callison-Burch et al., 2007; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2019), and all of them rely on at least one of the three translation quality criteria: comprehensibility (comprehension, intelligibility), adequacy (fidelity, semantic accuracy), and fluency (grammaticality). Comprehensibility reflects the degree to which a translated text can be understood, adequacy reflects the degree to which the translation conveys the meaning of the original text in the source language, and fluency reflect the grammar of the translated text. The raters are usually asked to assign an overall score for th"
2020.conll-1.19,2010.eamt-1.12,0,0.0896488,"Missing"
2020.conll-1.19,D14-1172,0,0.854715,"translation conveys the meaning of the original text in the source language, and fluency reflect the grammar of the translated text. The raters are usually asked to assign an overall score for the given translation criterion. In order to get more details about translation performance, error classification and analysis emerged in the field of MT (Vilar et al., 2006; Lommel et al., 2014; Klubiˇcka et al., 2018; Van Brussel et al., 2018). However, there is less work dealing with human perception of MT quality and errors. For statistical phrase-based MT systems (SMT), Kirchhoff et al. (2014) and Federico et al. (2014) were identifying error types which are mostly disliked by readers. In the last five years, systems based on artificial neural networks (NMT) have become the new state of the art. Several evaluation studies, such as (Castilho et al., 2017; Klubiˇcka et al., 2018; Van Brussel et al., 2018) reported that these systems are able to produce more fluent and readable translations, but that they are still sufferring from adequacy issues. In addition, many participants mentioned that good fluency of NMT outputs makes it more difficult to spot adequacy errors such as omissions or mistranslations. Such “"
2020.conll-1.19,W13-2305,0,0.406067,"slations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT quality by assigning overall scores to MT outputs, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Callison-Burch et al., 2007; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2019), and all of them rely on at least one of the three translation quality criteria: comprehensibility (comprehension, intelligibility), adequacy (fidelity, semantic accuracy), and fluency (grammaticality). Comprehensibility reflects the degree to which a translated text can be understood, adequacy reflects the degree to which the translation conveys the meaning of the original text in the source language, and fluency reflect the grammar of the translated text. The raters are usually asked to assign an overall score for the given translation criterion. In order to get more"
2020.conll-1.19,W06-3114,0,0.236545,"comprehensible inadequate) and discarded correct (incomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT quality by assigning overall scores to MT outputs, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Callison-Burch et al., 2007; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2019), and all of them rely on at least one of the three translation quality criteria: comprehensibility (comprehension, intelligibility), adequacy (fidelity, semantic accuracy), and fluency (grammaticality). Comprehensibility reflects the degree to which a translated text can be understood, adequacy reflects the degree to which the translation conveys the meaning of the original text in the source language, and fluency reflect the grammar of the translated text. The raters are usually asked to a"
2020.conll-1.19,2014.eamt-1.38,1,0.925201,"Missing"
2020.conll-1.19,P11-1015,0,0.00906334,"if the reader has access to the source text or to a correct translation to find out that the meaning is wrong. This means that comprehensibility may have the same misleading effect making the reader accept an incorrect information. On the other hand, because comprehensibility is different than fluency (fluent sentences can be incomprehensible and vice versa), the effects might be different. 3 Data set Our analysis has been carried out on written usergenerated content, namely user reviews. Two types of publicly available user reviews written in English have been analysed: IMDb movie reviews1 (Maas et al., 2011) and Amazon product reviews2 (McAuley et al., 2015). A set of those user reviews was translated into Croatian and Serbian, two closely related mid-size less-resourced morphologically rich European languages. The reviews were translated3 by three on-line systems: Google Translate4 , Bing5 and Amazon translate6 . The analysed text consists of a mixture of MT outputs from the three systems including 222 translated reviews consisting of about 1500 sentences (segments) and 19837 untokenised words in total. This text was then given to the annotators to mark comprehensibility and adequacy issues, and"
2020.conll-1.19,W18-1803,0,0.324921,"te of the art. Several evaluation studies, such as (Castilho et al., 2017; Klubiˇcka et al., 2018; Van Brussel et al., 2018) reported that these systems are able to produce more fluent and readable translations, but that they are still sufferring from adequacy issues. In addition, many participants mentioned that good fluency of NMT outputs makes it more difficult to spot adequacy errors such as omissions or mistranslations. Such “fluently inadequate” errors may mislead readers into trusting the content based on fluency alone, especially when surrounded by fluent and adequate parts of a text (Martindale and Carpuat, 2018). Automatic identification of such errors for both SMT and NMT systems has been investigated in (Martindale et al., 2019) and it is confirmed that these errors appear much more often in NMT system. To the best of our knowlegde, comprehensibility, while being a very important translation quality factor, has not been investigated in depth yet. It 256 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 256–264 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 should be stressed that comprehensibility is"
2020.conll-1.19,W19-6623,0,0.0846036,"eported that these systems are able to produce more fluent and readable translations, but that they are still sufferring from adequacy issues. In addition, many participants mentioned that good fluency of NMT outputs makes it more difficult to spot adequacy errors such as omissions or mistranslations. Such “fluently inadequate” errors may mislead readers into trusting the content based on fluency alone, especially when surrounded by fluent and adequate parts of a text (Martindale and Carpuat, 2018). Automatic identification of such errors for both SMT and NMT systems has been investigated in (Martindale et al., 2019) and it is confirmed that these errors appear much more often in NMT system. To the best of our knowlegde, comprehensibility, while being a very important translation quality factor, has not been investigated in depth yet. It 256 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 256–264 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 should be stressed that comprehensibility is very different from fluency – a fluent text can be incomprehensible (for example “Colorless green ideas sleep furiously.”"
2020.conll-1.19,2020.coling-main.444,1,0.877797,"Missing"
2020.conll-1.19,2011.mtsummit-papers.27,0,0.592481,"ncomprehensible adequate) translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations. 1 Introduction While automatic evaluation metrics are very important and invaluable tools for rapid development of machine translation (MT) systems, they are only a substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT quality by assigning overall scores to MT outputs, such as (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Callison-Burch et al., 2007; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2019), and all of them rely on at least one of the three translation quality criteria: comprehensibility (comprehension, intelligibility), adequacy (fidelity, semantic accuracy), and fluency (grammaticality). Comprehensibility reflects the degree to which a translated text can be understood, adequacy reflects the degree to which the translation conveys the meaning of the original text in the source language, and fluency reflect the grammar of the translated text. The raters are usually asked to assign an overall score for the given translation criterion."
2020.conll-1.19,stymne-ahrenberg-2012-practice,0,0.0231675,"rmal written language) translated into Croatian and Serbian (as a case of mid-size less-resourced morphologically rich European languages). It is worth noting that the aim of this work is not to compare MT systems, nor to estimate their overall performance for the given language pairs and domain in order to potentially improve them. The aim of this work is to explore relations between two aspects of human perception of translation quality. 2 Related work Lot of research on MT evaluation deals with classification and analysis of MT errors, for example (Vilar et al., 2006; Farr´us et al., 2010; Stymne and Ahrenberg, 2012; Lommel et al., 2014; Klubiˇcka et al., 2018). Few papers deal with human perception of these errors, but neither of them defines precisely which criterion is the translation quality based on. A similar study on SMT outputs based on linear mixed-effects models is described in (Federico et al., 2014), aiming to estimate the impact of different translation errors to the overall translation quality. For each MT output, experts were asked to assign a score on a 5-point scale while other experts annotated the errors. The results confirmed that the frequency of errors of a given type does not corre"
2020.conll-1.19,L18-1600,0,0.0625288,"Missing"
2020.conll-1.19,vilar-etal-2006-error,0,0.627045,"Missing"
2020.conll-1.19,1994.amta-1.25,0,0.729097,"Missing"
2020.eamt-1.39,ahrenberg-2017-comparing,0,0.0858211,"certain POS categories, function words and collocations. Rabinovich et al. (2016) include analysis of nonnative texts, namely texts originally written in the given language but by non-native speakers. They found that these texts generally exhibit different features than native originals and HTs, thus representing yet another text category. On the other hand, their features are closer to those of HTs than to native originals, indicating the influence (“interference”) of the native language. In addition to analysis of HTs, more and more publications report analysis of machine translated texts. Ahrenberg (2017) compares MT outputs with HTs by means of automatically calculated text features as well as by manual analysis of divergences (shifts) from the source text. The main finding is that MT output is much more similar to the source text than HT. Another study of machine translated texts (Vanmassenhove et al., 2019) reports significantly lower lexical richness in MT outputs in comparison to originals and HTs. Post-editing (PE) of MT outputs has lead to yet another type of translated text which has been ˇ analysed extensively in the recent years (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 20"
2020.eamt-1.39,W19-5204,0,0.0297134,"Missing"
2020.eamt-1.39,P11-1132,0,0.459749,"tion universals”, represents a general set of features shared by all translations, independent of the characteristics of involved languages c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. (Baker et al., 1993). Another category, “interference”, reflects the impact of the source language, the “trace” which the source language leaves in the translation (Toury, 1979). Some studies investigate and demonstrate the existence of both categories, sometimes called “source universals” and “target universals” (Chesterman, 2004; Koppel and Ordan, 2011). Our research aims to find out whether differences between translators have any influence on the text features. We investigate impact of the translator’s expertise and native language. We present results of a computational analysis of a set of HTs originating from the news domain and involving three distinct language pairs, EnglishCroatian, German-French and English-Finnish. The analysis is guided by the following research questions: RQ1 Are there differences between HTs related to translator’s expertise? RQ2 Are there differences between HTs related to translator’s native language and transl"
2020.eamt-1.39,2009.mtsummit-papers.9,0,0.0838458,"(PE) of MT outputs has lead to yet another type of translated text which has been ˇ analysed extensively in the recent years (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018; Toral, 2019; Castilho et al., 2019). These studies demonstrated that PEs represent an additional text category with the features lying between those of HTs and of MT outputs. Relations between machine and human translation As machine translation (MT) technology improves, more and more work has been done on investigating relations between different aspects of MT and HT direction. First publications on this topic (Kurokawa et al., 2009; Lembersky et al., 2013) demonstrated that the direction of HT plays an important role for building a statistical MT system, and recommend training on parallel corpora which were translated in the same direction as the MT system (i.e. using originals as source and HTs as target). Recently, several publications (L¨aubli et al., 2018; Toral et al., 2018; Freitag et al., 2019; Zhang and Toral, 2019) demonstrated that the translation direction plays an important role both for human as well as for automatic evaluation of MT systems. Before these findings were published, this aspect has not been ta"
2020.eamt-1.39,D18-1512,0,0.0988613,"Missing"
2020.eamt-1.39,J13-4007,0,0.0259632,"lead to yet another type of translated text which has been ˇ analysed extensively in the recent years (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018; Toral, 2019; Castilho et al., 2019). These studies demonstrated that PEs represent an additional text category with the features lying between those of HTs and of MT outputs. Relations between machine and human translation As machine translation (MT) technology improves, more and more work has been done on investigating relations between different aspects of MT and HT direction. First publications on this topic (Kurokawa et al., 2009; Lembersky et al., 2013) demonstrated that the direction of HT plays an important role for building a statistical MT system, and recommend training on parallel corpora which were translated in the same direction as the MT system (i.e. using originals as source and HTs as target). Recently, several publications (L¨aubli et al., 2018; Toral et al., 2018; Freitag et al., 2019; Zhang and Toral, 2019) demonstrated that the translation direction plays an important role both for human as well as for automatic evaluation of MT systems. Before these findings were published, this aspect has not been taken into account at all i"
2020.eamt-1.39,W19-8703,0,0.0455291,"uts with HTs by means of automatically calculated text features as well as by manual analysis of divergences (shifts) from the source text. The main finding is that MT output is much more similar to the source text than HT. Another study of machine translated texts (Vanmassenhove et al., 2019) reports significantly lower lexical richness in MT outputs in comparison to originals and HTs. Post-editing (PE) of MT outputs has lead to yet another type of translated text which has been ˇ analysed extensively in the recent years (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018; Toral, 2019; Castilho et al., 2019). These studies demonstrated that PEs represent an additional text category with the features lying between those of HTs and of MT outputs. Relations between machine and human translation As machine translation (MT) technology improves, more and more work has been done on investigating relations between different aspects of MT and HT direction. First publications on this topic (Kurokawa et al., 2009; Lembersky et al., 2013) demonstrated that the direction of HT plays an important role for building a statistical MT system, and recommend training on parallel corpora which were translated in the"
2020.eamt-1.39,W19-6712,1,0.89007,"Missing"
2020.eamt-1.39,W16-3401,0,0.0262738,"analysis of machine translated texts. Ahrenberg (2017) compares MT outputs with HTs by means of automatically calculated text features as well as by manual analysis of divergences (shifts) from the source text. The main finding is that MT output is much more similar to the source text than HT. Another study of machine translated texts (Vanmassenhove et al., 2019) reports significantly lower lexical richness in MT outputs in comparison to originals and HTs. Post-editing (PE) of MT outputs has lead to yet another type of translated text which has been ˇ analysed extensively in the recent years (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018; Toral, 2019; Castilho et al., 2019). These studies demonstrated that PEs represent an additional text category with the features lying between those of HTs and of MT outputs. Relations between machine and human translation As machine translation (MT) technology improves, more and more work has been done on investigating relations between different aspects of MT and HT direction. First publications on this topic (Kurokawa et al., 2009; Lembersky et al., 2013) demonstrated that the direction of HT plays an important role for building a statistical MT system,"
2020.eamt-1.39,W15-3049,1,0.88513,"Missing"
2020.eamt-1.39,W18-6319,0,0.0221114,"istency. Here, it should be noted that a large lexical and/or grammatical variety as well as a large divergence from the source text are not necessarily positive. Effects on automatic MT evaluation Since the EnHr data set is the only one containing parallel (instead of comparable) HTs, it represents a perfect data set for testing the behaviour of automatic MT evaluation scores calculated on distinct reference translations. For this purpose, we translated the English source text by two online MT systems,11 Google Translate12 and Bing Translator.13 We then calculated the widely used BLEU score (Post, 2018) and two recently proposed characterbased metrics, F-score (Popovi´c, 2015) and edit distance (Wang et al., 2016). All scores are calculated by comparing MT output with each of the HTs. The resulting scores in Table 5 lead to different conclusions depending on the used reference HT. According to the professional HT, the Google MT output is substantially better than the Bing output in terms of all three evaluation metrics. If the first crowd HT is used as a reference, the differences between the two systems become small according to BLEU and chrF, whereas characTER even says that the Bing MT ou"
2020.eamt-1.39,Q15-1030,0,0.141511,"93) have emphasised the existence of “translation universals”, general features of translated texts, “simplification” and “explicitation” being the most wellknown. Other studies (Toury, 1979) have pointed out the influence of the source language, “interference”, whereas some (Chesterman, 2004) concentrate on both categories, called “S-universals” and “T-universals”. Since many text features can be measured quantitatively, a number of publications demonstrated that HTs can be automatically distinguished from originals (Baroni and Bernardini, 2006; Koppel and Ordan, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Rubino et al., 2016). The features used for the classifiers are partly motivated by the theoretical categories mentioned above, however many features are not directly related to a particular category, and many can belong to more than one category. The most common features are lexical variety (percentage of distinct words in a text), lexical density (sometimes called information density, percentage of content words in a text), sentence length, word length, as well as frequencies of certain POS categories, function words and collocations. Rabinovich et al. (2016) include analysis of nonnative"
2020.eamt-1.39,P16-1176,0,0.0183004,"11; Volansky et al., 2015; Rabinovich and Wintner, 2015; Rubino et al., 2016). The features used for the classifiers are partly motivated by the theoretical categories mentioned above, however many features are not directly related to a particular category, and many can belong to more than one category. The most common features are lexical variety (percentage of distinct words in a text), lexical density (sometimes called information density, percentage of content words in a text), sentence length, word length, as well as frequencies of certain POS categories, function words and collocations. Rabinovich et al. (2016) include analysis of nonnative texts, namely texts originally written in the given language but by non-native speakers. They found that these texts generally exhibit different features than native originals and HTs, thus representing yet another text category. On the other hand, their features are closer to those of HTs than to native originals, indicating the influence (“interference”) of the native language. In addition to analysis of HTs, more and more publications report analysis of machine translated texts. Ahrenberg (2017) compares MT outputs with HTs by means of automatically calculated"
2020.eamt-1.39,N16-1110,0,0.511451,"Missing"
2020.eamt-1.39,W18-6312,0,0.187861,"erences between HTs related to translator’s native language and translation direction? (from or into translator’s native language) The main contribution of this work is empirical, showing evidence of differences between text features of HTs produced by different translators. We expect our findings to motivate and drive future research in this direction in order to better understand these differences by identifying and analysing underlying linguistic phenomena. Moreover, differences between HTs may have practical impact on evaluation of machine translation (MT) systems. Several recent studies (Toral et al., 2018; L¨aubli et al., 2018; Zhang and Toral, 2019; Freitag et al., 2019) have shown that the translation direction has impact on the results of evaluation of MT outputs, so that it is important to specify whether originals or HTs were used as source texts for MT systems. Taking into account these studies and the findings reported in this work, potential effects of translators’ backgrounds on MT should be investigated too. 2 Related work Analysis of translated texts A lot of work has been done exploring differences between HTs and originals. Some studies (Baker et al., 1993) have emphasised the exi"
2020.eamt-1.39,W19-6627,0,0.338538,"anguage and translation direction? (from or into translator’s native language) The main contribution of this work is empirical, showing evidence of differences between text features of HTs produced by different translators. We expect our findings to motivate and drive future research in this direction in order to better understand these differences by identifying and analysing underlying linguistic phenomena. Moreover, differences between HTs may have practical impact on evaluation of machine translation (MT) systems. Several recent studies (Toral et al., 2018; L¨aubli et al., 2018; Zhang and Toral, 2019; Freitag et al., 2019) have shown that the translation direction has impact on the results of evaluation of MT outputs, so that it is important to specify whether originals or HTs were used as source texts for MT systems. Taking into account these studies and the findings reported in this work, potential effects of translators’ backgrounds on MT should be investigated too. 2 Related work Analysis of translated texts A lot of work has been done exploring differences between HTs and originals. Some studies (Baker et al., 1993) have emphasised the existence of “translation universals”, general f"
2020.eamt-1.39,W19-6622,0,0.0611624,"us representing yet another text category. On the other hand, their features are closer to those of HTs than to native originals, indicating the influence (“interference”) of the native language. In addition to analysis of HTs, more and more publications report analysis of machine translated texts. Ahrenberg (2017) compares MT outputs with HTs by means of automatically calculated text features as well as by manual analysis of divergences (shifts) from the source text. The main finding is that MT output is much more similar to the source text than HT. Another study of machine translated texts (Vanmassenhove et al., 2019) reports significantly lower lexical richness in MT outputs in comparison to originals and HTs. Post-editing (PE) of MT outputs has lead to yet another type of translated text which has been ˇ analysed extensively in the recent years (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018; Toral, 2019; Castilho et al., 2019). These studies demonstrated that PEs represent an additional text category with the features lying between those of HTs and of MT outputs. Relations between machine and human translation As machine translation (MT) technology improves, more and more work has been done on"
2020.eamt-1.39,N18-1136,0,0.0591873,"Missing"
2020.eamt-1.39,W16-2342,0,0.0119771,"gence from the source text are not necessarily positive. Effects on automatic MT evaluation Since the EnHr data set is the only one containing parallel (instead of comparable) HTs, it represents a perfect data set for testing the behaviour of automatic MT evaluation scores calculated on distinct reference translations. For this purpose, we translated the English source text by two online MT systems,11 Google Translate12 and Bing Translator.13 We then calculated the widely used BLEU score (Post, 2018) and two recently proposed characterbased metrics, F-score (Popovi´c, 2015) and edit distance (Wang et al., 2016). All scores are calculated by comparing MT output with each of the HTs. The resulting scores in Table 5 lead to different conclusions depending on the used reference HT. According to the professional HT, the Google MT output is substantially better than the Bing output in terms of all three evaluation metrics. If the first crowd HT is used as a reference, the differences between the two systems become small according to BLEU and chrF, whereas characTER even says that the Bing MT output is better. A similar tendency can be observed if the student HT is 6 10 7 11 https://www.abumatran.eu/ HT fr"
2020.eamt-1.39,W19-5208,0,0.190388,"s native language and translation direction? (from or into translator’s native language) The main contribution of this work is empirical, showing evidence of differences between text features of HTs produced by different translators. We expect our findings to motivate and drive future research in this direction in order to better understand these differences by identifying and analysing underlying linguistic phenomena. Moreover, differences between HTs may have practical impact on evaluation of machine translation (MT) systems. Several recent studies (Toral et al., 2018; L¨aubli et al., 2018; Zhang and Toral, 2019; Freitag et al., 2019) have shown that the translation direction has impact on the results of evaluation of MT outputs, so that it is important to specify whether originals or HTs were used as source texts for MT systems. Taking into account these studies and the findings reported in this work, potential effects of translators’ backgrounds on MT should be investigated too. 2 Related work Analysis of translated texts A lot of work has been done exploring differences between HTs and originals. Some studies (Baker et al., 1993) have emphasised the existence of “translation universals”, general f"
2020.eamt-1.52,W19-3715,1,0.878989,"Missing"
2020.eamt-1.52,2011.mtsummit-papers.27,0,0.0419979,"particular MT systems. We are currently analysing MT outputs3 of three on-line systems: Google Translate4 , Bing5 and Amazon translate6 . We are also developing our own system using publicly available data, which will be analysed in the later stages of the project. 1 https://ai.stanford.edu/˜amaas/data/ sentiment/ 2 http://jmcauley.ucsd.edu/data/amazon/ 3 generated at the end of January 2020 4 https://translate.google.com/ 5 https://www.bing.com/translator 6 https://aws.amazon.com/translate/ 4 Evaluation procedure Our evaluation procedure is based on comprehensibility and fidelity (adequacy) (Roturier and Bensadoun, 2011), and it is being carried out on the review level (not on the sentence level). It should be noted that comprehensibility is not fluency – a fluent text can be incomprehensible, and vice versa. The novelty of our procedure is asking the annotators to concentrate on problematic parts of the text and to mark them, without assigning any scores or classifying errors. The procedure can also be guided by other evaluation criteria, not only comprehensibility and adequacy. The annotators were computational linguistics students and researchers fluent in the source language and native speakers of the tar"
2020.lrec-1.461,N18-1118,0,0.0219273,"scribes in detail our methodology, followed by section 5. where we present the results and discuss our findings. In section 6., we present our general observations and general guidelines for MT evaluation context, with suggestions for future research. 2. Related Work Although the term “document level” has been used freely to refer to MT systems handling context beyond the sentence level, the definition of what exactly constitutes a documentlevel is not yet well defined. Work on document-level MT show that those systems mostly use a context span of sentence pairs (Tiedemann and Scherrer, 2017; Bawden et al., 2018; M¨uller et al., 2018) and very few have attempted to go beyond that span (Voita et al., 2019). For some of the developed context-aware MT models, test suites have been designed to better evaluate translation of the addressed discourse-level phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019). As for overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. To reassess claims of “human parity” in MT, Toral et al. (2018) used consecutive single sentences (opposed to randomised single sentences in Hassan et al. (2018)) to"
2020.lrec-1.461,D18-1512,0,0.304858,"Missing"
2020.lrec-1.461,W18-6307,0,0.050152,"Missing"
2020.lrec-1.461,W17-4811,0,0.0269221,"en in section 3. Section 4. describes in detail our methodology, followed by section 5. where we present the results and discuss our findings. In section 6., we present our general observations and general guidelines for MT evaluation context, with suggestions for future research. 2. Related Work Although the term “document level” has been used freely to refer to MT systems handling context beyond the sentence level, the definition of what exactly constitutes a documentlevel is not yet well defined. Work on document-level MT show that those systems mostly use a context span of sentence pairs (Tiedemann and Scherrer, 2017; Bawden et al., 2018; M¨uller et al., 2018) and very few have attempted to go beyond that span (Voita et al., 2019). For some of the developed context-aware MT models, test suites have been designed to better evaluate translation of the addressed discourse-level phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019). As for overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. To reassess claims of “human parity” in MT, Toral et al. (2018) used consecutive single sentences (opposed to randomised single sentences in Has"
2020.lrec-1.461,tiedemann-2012-parallel,0,0.151583,"Missing"
2020.lrec-1.461,W18-6312,1,0.850075,"aluation, human evaluation 1. Introduction One of the biggest challenges for MT is the ability to handle discourse dependencies and the wider context of a document. Although currently an active community is working on developing discourse-level MT systems, the improvements reported for those systems are still limited. One of the main reasons for this is that the evaluation of documentlevel systems (both automatic and human) has primarily been performed at the sentence level and is, therefore, unable to recognise the real improvements of document-level systems. Furthermore, two recent studies (Toral et al., 2018; L¨aubli et al., 2018) independently reassessed the bold claims of MT “achieving human parity” (Hassan et al., 2018) and found that the lack of extra-sentential context has a great effect on quality assessment. After these two papers were published, WMT19 considered their criticisms and attempted, for the first time, a document-level human evaluation for some language pairs. Despite the increased in the field, both for extending MT systems to operate on the document level, as well as for improving the evaluation methodology by expanding it to the document level, the definition of what exactly"
2020.lrec-1.461,P19-1116,0,0.0253011,"uss our findings. In section 6., we present our general observations and general guidelines for MT evaluation context, with suggestions for future research. 2. Related Work Although the term “document level” has been used freely to refer to MT systems handling context beyond the sentence level, the definition of what exactly constitutes a documentlevel is not yet well defined. Work on document-level MT show that those systems mostly use a context span of sentence pairs (Tiedemann and Scherrer, 2017; Bawden et al., 2018; M¨uller et al., 2018) and very few have attempted to go beyond that span (Voita et al., 2019). For some of the developed context-aware MT models, test suites have been designed to better evaluate translation of the addressed discourse-level phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019). As for overall MT evaluation, a few attempts have been made to perform human evaluation with document-level set-ups. To reassess claims of “human parity” in MT, Toral et al. (2018) used consecutive single sentences (opposed to randomised single sentences in Hassan et al. (2018)) to rank translations by two MT systems (Microsoft and Google) and a human reference from the WMT"
2020.vardial-1.10,N19-1388,0,0.024448,"al., 2018). The systems operate on sub-word units generated by byte-pair encoding (BPE) (Sennrich et al., 2016b). We set the number of BPE merge operations at 32000 both for the source and for the target language texts. We do not use shared vocabularies between the source (English, German) and the target (Serbian, Croatian) languages because they are distinct. For multilingual systems, on the other hand, we build a joint vocabulary for the two target languages (Serbian and Croatian) because they are very similar. These systems are built using the same technique as (Johnson et al., 2017) and (Aharoni et al., 2019), namely adding a target language label “SR” or “HR” to each source sentence. All the systems have Transformer architecture with 6 layers for both the encoder and decoder, model size of 512, feed forward size of 2048, and 8 attention heads. For training, we use Adam optimiser (Kingma and Ba, 2015), initial learning rate of 0.0002, and batch size of 4096 (sub)words. Validation perplexity is calculated after every 4000 batches (at so-called “checkpoints”), and if this perplexity does not improve after 20 checkpoints, the training stops. For set-ups with less than two million segments,8 following"
2020.vardial-1.10,W11-2131,0,0.0855674,"Missing"
2020.vardial-1.10,W17-4755,0,0.0134676,"ecall. The chrF score is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long tradition of using it for MT evaluation despite well-known faults, and the two character level scores because they are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018). Recently, the chrF score is recommended as a replacement for BLEU (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus, the percentage of particular target language data in this corpus, as well as the training time in terms of days. 4.1 Translation from English The results for translation from English are presented in Table 3. As expected, multilingual systems are better than bilingual for all set-ups, even for the unbalanced clean low-resourced corpus. However, the improvements are small"
2020.vardial-1.10,W18-6315,0,0.0183131,"e two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 102 Proceedings of the 7th VarD"
2020.vardial-1.10,W18-1820,0,0.0270192,"Croatian by an NMT system thus providing FT synthetic parallel corpora. In order to obtain a balanced corpus in terms of the two target languages, we translated half of the IMDb (about 300k segments) and half of the Amazon (about 500k segments) corpora into Croatian, and the other two halves into Serbian. More details about the NMT systems used for BT and FT can be found in the next section. Detailed statistics for all movie reviews can be seen in Table 2. 3 NMT systems All our systems are based on the Transformer architecture (Vaswani et al., 2017) and built using the Sockeye implementation (Hieber et al., 2018). The systems operate on sub-word units generated by byte-pair encoding (BPE) (Sennrich et al., 2016b). We set the number of BPE merge operations at 32000 both for the source and for the target language texts. We do not use shared vocabularies between the source (English, German) and the target (Serbian, Croatian) languages because they are distinct. For multilingual systems, on the other hand, we build a joint vocabulary for the two target languages (Serbian and Croatian) because they are very similar. These systems are built using the same technique as (Johnson et al., 2017) and (Aharoni et"
2020.vardial-1.10,2009.mtsummit-papers.9,0,0.0931716,"Missing"
2020.vardial-1.10,W18-6316,0,0.338426,"South Slavic languages are generally less supported and investigated in natural language processing, they have been explored in the field of machine translation (MT). Nevertheless, a large part of the work deals with the previous state-of-the-art approach, namely phrase-based statistical machine translation (PBSMT) (Popovi´c and Ljubeˇsi´c, 2014; Toral et al., 2014; Popovi´c and Arˇcan, 2015; Arˇcan et al., 2016; Popovi´c et al., 2016; S´anchez-Cartagena et al., 2016; Mauˇcec and Brest, 2017), while much less work can be found about the new state-of-the-art, neural machine translation (NMT) (Lakew et al., 2018; Lohar et al., 2019). In this work, we focus on NMT into Croatian and Serbian, two very closely related South Slavic languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating"
2020.vardial-1.10,W14-0405,0,0.0403373,"Missing"
2020.vardial-1.10,W19-3715,1,0.80412,"Missing"
2020.vardial-1.10,W18-6450,0,0.0120761,"e is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long tradition of using it for MT evaluation despite well-known faults, and the two character level scores because they are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018). Recently, the chrF score is recommended as a replacement for BLEU (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus, the percentage of particular target language data in this corpus, as well as the training time in terms of days. 4.1 Translation from English The results for translation from English are presented in Table 3. As expected, multilingual systems are better than bilingual for all set-ups, even for the unbalanced clean low-resourced corpus. However, the improvements are smaller for Croatian, t"
2020.vardial-1.10,2020.acl-main.448,0,0.075307,"Missing"
2020.vardial-1.10,J82-2005,0,0.475792,"Missing"
2020.vardial-1.10,W15-4913,1,0.899953,"Missing"
2020.vardial-1.10,W14-4210,1,0.868872,"Missing"
2020.vardial-1.10,W16-4813,1,0.874385,"Missing"
2020.vardial-1.10,W15-3049,1,0.871723,"Missing"
2020.vardial-1.10,W18-6319,0,0.0146645,"forward-translated IMDb data. Forward translation was performed by the system “+ SELECTED -BT”. +A MAZON -FT: on the data used for the system “+IMD B -FT” enriched with forward-translated Amazon data. Forward translation was performed by the system “+IMD B -FT”. In addition to these mid-resourced partially in-domain systems, we also translate the movie reviews test set by the three systems trained on full out-of-domain OPUS data (EN→HR FULL , EN→SR FULL , EN → HR + SR FULL CLEAN / ED ). 4 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is based on word ngram (n in range from 1 to 4) precision and brevity penalty which should replace recall. The chrF score is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long"
2020.vardial-1.10,W16-3421,0,0.0274555,"Missing"
2020.vardial-1.10,P19-1021,0,0.0192737,"” to each source sentence. All the systems have Transformer architecture with 6 layers for both the encoder and decoder, model size of 512, feed forward size of 2048, and 8 attention heads. For training, we use Adam optimiser (Kingma and Ba, 2015), initial learning rate of 0.0002, and batch size of 4096 (sub)words. Validation perplexity is calculated after every 4000 batches (at so-called “checkpoints”), and if this perplexity does not improve after 20 checkpoints, the training stops. For set-ups with less than two million segments,8 following the recommendations for low-resource settings in (Sennrich and Zhang, 2019), we changed the following parameters: training stops after 10 checkpoints instead of 20, initial learning rate is 0.0001 instead of 0.0002, and checkpoint interval is 100 batches instead of 4000. 3.1 Systems for translating from English The following set-ups were investigated for translating from English into Croatian and Serbian: Clean data only The bilingual (EN→HR CLEAN , EN → SR CLEAN ) and multilingual (EN→HR + SR CLEAN) low-resourced systems are trained only on News and Other data from Table 1. The segments in these texts are mainly properly aligned, so we refer to it as “clean”. It is"
2020.vardial-1.10,P16-1009,0,0.317856,"sely related South Slavic languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/lic"
2020.vardial-1.10,P16-1162,0,0.373808,"sely related South Slavic languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/lic"
2020.vardial-1.10,W18-6305,0,0.0610788,"Missing"
2020.vardial-1.10,tiedemann-2012-parallel,0,0.0765211,"ranslating English user reviews into Serbian has been addressed in (Lohar et al., 2019). They compared PBSMT and NMT systems trained on out-of-domain data, and then further investigated the NMT system with additional synthetic data. However, they explored only Serbian as a target language, and their baseline system was built on a very small News corpus of 200k segments. In this work, we also include Croatian, and we build the systems on more data, both out-of-domain as well as in-domain. 2 Data sets 2.1 OPUS parallel data For all our systems, we use the publicly available OPUS4 parallel data (Tiedemann, 2012). The vast majority of these resources for the desired language combinations consists of OpenSubtitles. For English and both target languages, we also used SETIMES News, Bible, Tilde, EU-bookshop, QED, and Tatoeba corpora. In addition, we used GlobalVoices for Serbian, and hrenWac, TED and Wikimedia for Croatian. For German, we only used OpenSubtitles because other corpora are rather sparse. However, including these corpora might be interesting for future work. The original parallel data were filtered in order to eliminate noisy parts: too long segments (more than 100 words), segment pairs wit"
2020.vardial-1.10,2014.eamt-1.45,1,0.816457,"Missing"
2020.vardial-1.10,N18-1136,0,0.0599846,"Missing"
2020.vardial-1.10,W16-2342,0,0.013707,"was performed by the system “+ SELECTED -BT”. +A MAZON -FT: on the data used for the system “+IMD B -FT” enriched with forward-translated Amazon data. Forward translation was performed by the system “+IMD B -FT”. In addition to these mid-resourced partially in-domain systems, we also translate the movie reviews test set by the three systems trained on full out-of-domain OPUS data (EN→HR FULL , EN→SR FULL , EN → HR + SR FULL CLEAN / ED ). 4 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is based on word ngram (n in range from 1 to 4) precision and brevity penalty which should replace recall. The chrF score is based on character n-gram matching (n in range from 1 to 6) instead of word n-gram matching. It is F-score which weights recall two times more than precision. The characTER score is based on edit distance which takes into account not only substitutions, insertions and deletions, but also word sequence reorderings and character sequences in unmatched words. We use the BLEU score because of the long tradition of using it for MT evaluation despite well-know"
2020.vardial-1.10,W19-5208,0,0.0482939,"Missing"
2020.vardial-1.10,D16-1160,0,0.0182344,"c languages. We explore two source languages, English and German. The main goals of our research are to explore two source languages, each of them with different sizes and types of training corpora, as well as to test our systems on translating English user reviews, a challenging domain/genre where no parallel training data are available. For these purposes, we train bilingual and multilingual NMT systems on different publicly available parallel training corpora. For translating English user reviews, we also explore different types of synthetic parallel in-domain data (Sennrich et al., 2016a; Zhang and Zong, 2016; Burlot and Yvon, 2018; Poncelas et al., 2018a), which is a widely used practice in NMT. We explored two types of synthetic data: back-translated (BT) and forward-translated (FT). BT data consist of in-domain target language data and their machine translations into English, whereas FT data consist of English data and their machine translations into Serbian and Croatian. All our experiments were carried out on publicly available data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 102 Proc"
2020.wmt-1.51,N19-1388,0,0.0219217,"an ngram each time it is selected. Therefore the score of a sentence s is computed as in Equation (1): score(s, Seed, Sel) = ngr∈{s 4 MT systems All our systems are built using the Sockeye implementation (Hieber et al., 2018) of the Transformer architecture (Vaswani et al., 2017). The systems operate on sub-word units generated by byte-pair encoding (BPE) (Sennrich et al., 2016b). We set the number of BPE merging operations at 32000. We use shared vocabularies between the languages because they are similar. Multilingual systems are built using the same technique as (Johnson et al., 2017) and (Aharoni et al., 2019), namely adding a target language label “SR” or “HR” to each source sentence. We investigated the following set-ups: 0.5CSel (ngr) P T lion best ranked Croatian sentences were translated into Slovenian. Translation into Slovenian: Slovenian is the target language for two translation directions, and we wanted to have equally relevant Slovenian sentences for both directions. Therefore, we did not take the first two million sentences for one source language and the second two million for the other, because the Slovenian sentences for the first source language would be more relevant than those for"
2020.wmt-1.51,W11-2131,0,0.053038,"Missing"
2020.wmt-1.51,W17-4755,0,0.0531506,"Missing"
2020.wmt-1.51,W18-6315,0,0.0180013,"ds, we investigate additional cleaning of remaining misaligned segments by using character n-gram matching scores (Popovi´c, 2015). The beauty of the method for similar languages is that it can be applied directly to the given training corpus providing matching scores for each pair of the source-target segments. For distant languages, translation of one side of the training corpus would be required. Finally, we make use of monolingual data in each of the three languages by creating additional synthetic parallel training sets via back-translation (Sennrich et al., 2016a; Poncelas et al., 2018; Burlot and Yvon, 2018). This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian–Slovenian and Serbian– Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synthetic parallel data thro"
2020.wmt-1.51,W18-1820,0,0.0271459,"atively selects sentences from an initial set S based on the number of n-grams which overlap with an in-domain text Seed and adds these sentences to a selected set Sel. In addition, in order to promote a diversity, after a sentence is selected, its n-grams suffer a penalisation so that they are less likely to be selected in the following iterations. The default FDA system halves the score of an ngram each time it is selected. Therefore the score of a sentence s is computed as in Equation (1): score(s, Seed, Sel) = ngr∈{s 4 MT systems All our systems are built using the Sockeye implementation (Hieber et al., 2018) of the Transformer architecture (Vaswani et al., 2017). The systems operate on sub-word units generated by byte-pair encoding (BPE) (Sennrich et al., 2016b). We set the number of BPE merging operations at 32000. We use shared vocabularies between the languages because they are similar. Multilingual systems are built using the same technique as (Johnson et al., 2017) and (Aharoni et al., 2019), namely adding a target language label “SR” or “HR” to each source sentence. We investigated the following set-ups: 0.5CSel (ngr) P T lion best ranked Croatian sentences were translated into Slovenian. T"
2020.wmt-1.51,W16-3422,0,0.0463491,"Missing"
2020.wmt-1.51,2009.mtsummit-papers.9,0,0.0292993,"scores are relatively low given the similarity of the languages. One reason is domain/genre discrepance between the training and the development/test sets. Another possible reason is the nature of the OpenSubtitles corpus. The majority of non-English texts in OpenSubtitles are namely human translations from English originals. Therefore, for translation from English, the source language is the original one and the target language is its human translation.2 On the other hand, for translation not involving English, both sides are human translations, which can have a strong impact on performance (Kurokawa et al., 2009; Vyas et al., 2018; Zhang and Toral, 2019). These effects should be investigated in future work. 2 And other way round for translation into English. Results on the test set Based on the results on the development set, we submitted the outputs of the systems with backtranslated data (HR + SR→SL CLEAN + BT, SL→ HR + SR CLEAN + BT) as primary submissions. The outputs of the systems trained on cleaned data (HR + SR→SL CLEAN , SL→HR + SR CLEAN) were submitted as first contrastive, and the outputs of the baseline multilingual systems (HR + SR→SL , SL → HR + SR ) as second contrastive submissions. T"
2020.wmt-1.51,W14-0405,0,0.0632125,"Missing"
2020.wmt-1.51,W18-6450,0,0.0286062,"Missing"
2020.wmt-1.51,2020.acl-main.448,0,0.0165728,". The training corpora consist of 14.8M segments. 5 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is used because of the long tradition. The two character level scores are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018), especially for morphologically rich languages. In addition, the chrF score is recommended as a replacement for BLEU in a recent detailed study encompassing a number of automatic MT metrics (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus and the training time. Table 2 shows the results both on the development and on the test set for each of the four translation directions. First of all, it can be seen that the automatic scores are relatively low given the similarity of the languages. One reason is domain/genre discrepance between the training and the development/test sets. Another possible reason is the nature of the OpenSubtitles corpus. The majority of non-English texts in OpenSubtitles are namely human tran"
2020.wmt-1.51,W18-2703,0,0.0313243,"ata is useful. Afterwards, we investigate additional cleaning of remaining misaligned segments by using character n-gram matching scores (Popovi´c, 2015). The beauty of the method for similar languages is that it can be applied directly to the given training corpus providing matching scores for each pair of the source-target segments. For distant languages, translation of one side of the training corpus would be required. Finally, we make use of monolingual data in each of the three languages by creating additional synthetic parallel training sets via back-translation (Sennrich et al., 2016a; Poncelas et al., 2018; Burlot and Yvon, 2018). This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian–Slovenian and Serbian– Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingual data by creating synt"
2020.wmt-1.51,W15-3049,1,0.873958,"Missing"
2020.wmt-1.51,W16-4806,1,0.898839,"Missing"
2020.wmt-1.51,W16-4813,1,0.904044,"Missing"
2020.wmt-1.51,W14-4210,1,0.867397,"Missing"
2020.wmt-1.51,W18-6319,0,0.0120456,"instead of 20.2M). 3. Systems trained on cleaned OpenSubtitles and synthetic back-translated parallel Wac data Two multilingual systems HR + SR → SL CLEAN + BT and SL → HR + SR CLEAN + BT are trained on joint cleaned OpenSubtitles corpora together with the corresponding synthetic back-translated data selected from hrWac, slWac and srWac. The monolingual data was back-translated by the corresponding systems trained on cleaned OpenSubtitles. The training corpora consist of 14.8M segments. 5 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is used because of the long tradition. The two character level scores are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018), especially for morphologically rich languages. In addition, the chrF score is recommended as a replacement for BLEU in a recent detailed study encompassing a number of automatic MT metrics (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus and the training time. Table 2"
2020.wmt-1.51,P16-1009,0,0.171093,"g Serbian and Croatian data is useful. Afterwards, we investigate additional cleaning of remaining misaligned segments by using character n-gram matching scores (Popovi´c, 2015). The beauty of the method for similar languages is that it can be applied directly to the given training corpus providing matching scores for each pair of the source-target segments. For distant languages, translation of one side of the training corpus would be required. Finally, we make use of monolingual data in each of the three languages by creating additional synthetic parallel training sets via back-translation (Sennrich et al., 2016a; Poncelas et al., 2018; Burlot and Yvon, 2018). This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian–Slovenian and Serbian– Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingu"
2020.wmt-1.51,P16-1162,0,0.368769,"g Serbian and Croatian data is useful. Afterwards, we investigate additional cleaning of remaining misaligned segments by using character n-gram matching scores (Popovi´c, 2015). The beauty of the method for similar languages is that it can be applied directly to the given training corpus providing matching scores for each pair of the source-target segments. For distant languages, translation of one side of the training corpus would be required. Finally, we make use of monolingual data in each of the three languages by creating additional synthetic parallel training sets via back-translation (Sennrich et al., 2016a; Poncelas et al., 2018; Burlot and Yvon, 2018). This paper describes the ADAPT-DCU machine translation systems built for the WMT 2020 shared task on Similar Language Translation. We explored several set-ups for NMT for Croatian–Slovenian and Serbian– Slovenian language pairs in both translation directions. Our experiments focus on different amounts and types of training data: we first apply basic filtering on the OpenSubtitles training corpora, then we perform additional cleaning of remaining misaligned segments based on character n-gram matching. Finally, we make use of additional monolingu"
2020.wmt-1.51,tiedemann-2012-parallel,0,0.0505064,"is also larger and includes local word order, verb mood and/or tense formation, question structure, usage of cases, structural properties for certain conjunctions, as well as some other structural differences. Another important difference is the Slovenian dual grammatical number which refers to two entities (apart from singular for one and plural for more than two). It requires additional set of pronouns, as well as additional sets for noun, adjective and verb inflexion rules not existing either in Croatian or in Serbian. 3 Data For training, we used publicly available OPUS1 parallel corpora (Tiedemann, 2012) indicated by the workshop organisers. OpenSubtitles is indicated for all translation directions. For Croatian– Slovenian, other corpora are indicated too, but they are either not sentence-aligned (JW300 ) or are extremely noisy (DGT, MultiParaCrawl ). Therefore, we decided to use only OpenSubtitles for all translation directions. It is worth noting that the organisers also indicated the SETIMES News parallel Croatian– Serbian corpus. Developing an additional Croatian– Serbian MT system for converting Serbian data into 1 http://opus.nlpl.eu/ sl-sr set train dev test train dev test domain Subti"
2020.wmt-1.51,N18-1136,0,0.0592363,"Missing"
2020.wmt-1.51,W16-2342,0,0.0169099,"penSubtitles and synthetic back-translated parallel Wac data Two multilingual systems HR + SR → SL CLEAN + BT and SL → HR + SR CLEAN + BT are trained on joint cleaned OpenSubtitles corpora together with the corresponding synthetic back-translated data selected from hrWac, slWac and srWac. The monolingual data was back-translated by the corresponding systems trained on cleaned OpenSubtitles. The training corpora consist of 14.8M segments. 5 Results We evaluate our systems using the following three automatic overall evaluation scores: sacreBLEU (Post, 2018), chrF (Popovi´c, 2015) and characTER (Wang et al., 2016). The BLEU score is used because of the long tradition. The two character level scores are shown to correlate much better with human assessments (Bojar et al., 2017; Ma et al., 2018), especially for morphologically rich languages. In addition, the chrF score is recommended as a replacement for BLEU in a recent detailed study encompassing a number of automatic MT metrics (Mathur et al., 2020). In addition to the automatic MT evaluation scores, for each of the systems we report the size of the training corpus and the training time. Table 2 shows the results both on the development and on the tes"
2020.wmt-1.51,W19-5208,0,0.0221833,"rity of the languages. One reason is domain/genre discrepance between the training and the development/test sets. Another possible reason is the nature of the OpenSubtitles corpus. The majority of non-English texts in OpenSubtitles are namely human translations from English originals. Therefore, for translation from English, the source language is the original one and the target language is its human translation.2 On the other hand, for translation not involving English, both sides are human translations, which can have a strong impact on performance (Kurokawa et al., 2009; Vyas et al., 2018; Zhang and Toral, 2019). These effects should be investigated in future work. 2 And other way round for translation into English. Results on the test set Based on the results on the development set, we submitted the outputs of the systems with backtranslated data (HR + SR→SL CLEAN + BT, SL→ HR + SR CLEAN + BT) as primary submissions. The outputs of the systems trained on cleaned data (HR + SR→SL CLEAN , SL→HR + SR CLEAN) were submitted as first contrastive, and the outputs of the baseline multilingual systems (HR + SR→SL , SL → HR + SR ) as second contrastive submissions. The test sets were not at all translated by"
2021.conll-1.18,W08-0309,0,0.0672758,"as for developing suitable automatic metrics. Human evaluation is typically provided in one of the following ways: assigning an overall quality score to each translated sentence, ranking two or more translations of the same source language sentence from best to worst, or annotating actual translation errors. The errors can be only highlighted (marked) or corrected (postedited), but can also be classified according to a 1 http://www.statmt.org/wmt20/ metrics-task.html given pre-defined scheme, such as MQM scheme.2 However, human judgments of translation quality show a high degree of variance (Callison-Burch et al., 2008; Denkowski and Lavie, 2010), especially for fine-grained error classification based on a detailed error scheme involving many error types (Lommel et al., 2014; Klubiˇcka et al., 2018). One of the reasons for the variance is the variety of possible solutions: there is no single objectively correct translation of a given text, but rather a range of possible translations from perfect over good to acceptable. Moreover, there is no single universal criterion for translation quality. Although all manual evaluations are essentially based on some of the following three criteria: adequacy (meaning pre"
2021.conll-1.18,2020.wmt-1.137,0,0.241333,"preservation; the most frequently used), fluency (grammar of the target language; frequently used) and comprehension (readability; rarely used), the precise definition of the criterion is not always given to the annotators. Very often, an unspecified mixture of adequacy and fluency is used. In addition, other factors like target audience, goal of translation, etc. can have influence on evaluator’s perception of quality. Most publications dealing with human evaluation of MT, such as (Vilar et al., 2007; CallisonBurch et al., 2008; Klubiˇcka et al., 2018; Kreutzer et al., 2020; Popovi´c, 2020; Castilho, 2020; Freitag et al., 2021), report an overall inter-annotator agreement (IAA) score such as percentage of equal labels, Koehn’s κ, Fleiss’ κ, Krippendorf’s α, or similar. However, less work has been done on analysing the actual disagreements. Ranking and assigning overall scores are the mostly used methods and a large amount of annotated data is publicly available (for example in WMT shared tasks3 ), but these methods do not provide enough information 2 http://www.qt21.eu/mqm-definition/ definition-2015-12-30.html 3 http://www.statmt.org/wmt20/index. html 234 Proceedings of the 25th Conference on"
2021.conll-1.18,2010.amta-papers.20,0,0.199804,"utomatic metrics. Human evaluation is typically provided in one of the following ways: assigning an overall quality score to each translated sentence, ranking two or more translations of the same source language sentence from best to worst, or annotating actual translation errors. The errors can be only highlighted (marked) or corrected (postedited), but can also be classified according to a 1 http://www.statmt.org/wmt20/ metrics-task.html given pre-defined scheme, such as MQM scheme.2 However, human judgments of translation quality show a high degree of variance (Callison-Burch et al., 2008; Denkowski and Lavie, 2010), especially for fine-grained error classification based on a detailed error scheme involving many error types (Lommel et al., 2014; Klubiˇcka et al., 2018). One of the reasons for the variance is the variety of possible solutions: there is no single objectively correct translation of a given text, but rather a range of possible translations from perfect over good to acceptable. Moreover, there is no single universal criterion for translation quality. Although all manual evaluations are essentially based on some of the following three criteria: adequacy (meaning preservation; the most frequent"
2021.conll-1.18,1983.tc-1.13,0,0.651263,"Missing"
2021.conll-1.18,W18-1820,0,0.047573,"Missing"
2021.conll-1.18,W19-8642,0,0.0282374,"ypes showing that the annotators agreed to a large extent on untranslated words as well as on number, gender or case errors, while most disagreements were coming from omissions and tense errors. Contrary to (Lommel et al., 2014), the agreement for order errors was high, however this analysis was carried out on the sentence level, not on the word level, therefore diminishing the word-level span problem. While publications usually report only an overall inter-annotator agreement, many researchers analysed the disagreements in natural language processing evaluation from different points of view. Amidei et al. (2019) argues that standard IAA coefficients should not represent the only criterion for checking the reliability of human evaluation of natural language generation due to natural variability of human language, and suggest that correlation coefficients should be used. Some researchers compared IAA for different evaluation methods, such as ranking vs assigning overall scores. Belz and Kow (2010) compared the two methods for evaluating natural language generation, and CallisonAnother publication dealing with different error Burch et al. (2008); Denkowski and Lavie (2010) types reports results for natu"
2021.conll-1.18,2020.eamt-1.15,0,0.144911,"owing three criteria: adequacy (meaning preservation; the most frequently used), fluency (grammar of the target language; frequently used) and comprehension (readability; rarely used), the precise definition of the criterion is not always given to the annotators. Very often, an unspecified mixture of adequacy and fluency is used. In addition, other factors like target audience, goal of translation, etc. can have influence on evaluator’s perception of quality. Most publications dealing with human evaluation of MT, such as (Vilar et al., 2007; CallisonBurch et al., 2008; Klubiˇcka et al., 2018; Kreutzer et al., 2020; Popovi´c, 2020; Castilho, 2020; Freitag et al., 2021), report an overall inter-annotator agreement (IAA) score such as percentage of equal labels, Koehn’s κ, Fleiss’ κ, Krippendorf’s α, or similar. However, less work has been done on analysing the actual disagreements. Ranking and assigning overall scores are the mostly used methods and a large amount of annotated data is publicly available (for example in WMT shared tasks3 ), but these methods do not provide enough information 2 http://www.qt21.eu/mqm-definition/ definition-2015-12-30.html 3 http://www.statmt.org/wmt20/index. html 234 Proce"
2021.conll-1.18,2021.humeval-1.15,0,0.0206821,"choose this type of error annotation because we believe that it can better reflect the differences in annotators’ perception of errors than error classification: it is not bound to any predefined error scheme which might be tailored for a specific task and/or language pair, so that the evaluators had freedom to mark any part of the text which they perceive as problematic. 2 Related work natural language evaluation is often expected due to ambiguity and variation of language. Therefore, a number of disagreements do not represent “errors&quot; or “noise&quot; but are fully legitimate. On the other hand, (Oortwijn et al., 2021) argue that inter-rater disagreements are not necessarily due to inherent properties of the language, but at least in part to the annotation task being underspecified. However, none of the publications analysed the actual disagreements and error perception of different annotators. One of the first publications in this direction deals with IAA for error classification for MT using the MQM error scheme (Lommel et al., 2014). They reported that different degree of agreement can be observed for different error types, not always for the same reasons. For example, disagreement is high for word oder"
2021.conll-1.18,Q19-1043,0,0.0595544,"Missing"
2021.conll-1.18,2020.coling-main.444,1,0.852566,"Missing"
2021.conll-1.18,2020.coling-main.422,0,0.0440614,"Missing"
2021.conll-1.18,2020.inlg-1.22,0,0.0343361,"rion for checking the reliability of human evaluation of natural language generation due to natural variability of human language, and suggest that correlation coefficients should be used. Some researchers compared IAA for different evaluation methods, such as ranking vs assigning overall scores. Belz and Kow (2010) compared the two methods for evaluating natural language generation, and CallisonAnother publication dealing with different error Burch et al. (2008); Denkowski and Lavie (2010) types reports results for natural language generfor machine translation. All reported that evaluaation (Thomson and Reiter, 2020). Similarly to tors generally agree more in ranking. (Lommel et al., 2014), they report that some error Castilho (2020) compares IAA for three evaluatypes are more difficult for annotators to agree on tion methods for machine translation, namely rankthan others, as well as the word span plays an iming, assigning scores and error classification, but portant role for disagreements. Also they say that only in the context of evaluating isolated sentences some of the observed disagreements could have vs evaluating larger amounts of text (paragraphs, been resolved by more detailed annotation instruc"
2021.conll-1.18,W07-0713,0,0.0562534,"lthough all manual evaluations are essentially based on some of the following three criteria: adequacy (meaning preservation; the most frequently used), fluency (grammar of the target language; frequently used) and comprehension (readability; rarely used), the precise definition of the criterion is not always given to the annotators. Very often, an unspecified mixture of adequacy and fluency is used. In addition, other factors like target audience, goal of translation, etc. can have influence on evaluator’s perception of quality. Most publications dealing with human evaluation of MT, such as (Vilar et al., 2007; CallisonBurch et al., 2008; Klubiˇcka et al., 2018; Kreutzer et al., 2020; Popovi´c, 2020; Castilho, 2020; Freitag et al., 2021), report an overall inter-annotator agreement (IAA) score such as percentage of equal labels, Koehn’s κ, Fleiss’ κ, Krippendorf’s α, or similar. However, less work has been done on analysing the actual disagreements. Ranking and assigning overall scores are the mostly used methods and a large amount of annotated data is publicly available (for example in WMT shared tasks3 ), but these methods do not provide enough information 2 http://www.qt21.eu/mqm-definition/ def"
2021.gebnlp-1.11,2020.acl-main.619,0,0.0923147,"Missing"
2021.gebnlp-1.11,2020.acl-main.485,0,0.0142554,"reating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation (NMT) system trained to ‘translate’ from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results for automatic generation of gender alternatives for conversational sentences in Spanish. 1 Introduction Recent studies have exposed challenging systematic issues related to bias that extend to a range of AI applications, including Natural Language Processing (NLP) technology (Costa-jussà, 2019; Blodgett et al., 2020). Observed bias problems range from copying biases already existing in data to claims that the training process can lead to an exacerbation or amplification of observed biases (Zhou and Schiebinger, 2018; Vanmassenhove et al., 2021). The algorithms learn to maximize the overall probability of an occurrence, leading to 93 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 93–102 August 5, 2021. ©2021 Association for Computational Linguistics there is agreement with the grammatical gender of an object referred to in the previous sentence, while in (b) there is a"
2021.gebnlp-1.11,W19-3822,0,0.238039,"each sentence with the missing gender variants, thus fostering inclusion in online conversations/NLP applications. Generating gender variants can and should also be used to create gender balanced conversational data that can be used to train less biased NLP models such as machine translation models, language models, chat bots, etc. Unlike previous studies, we did not want to limit ourselves to one specific gender phenomenon, such as gender markings on professions (Zmigrod et al., 2019)) (for which the gender can easily be swapped by using hand-crafted lists) or first person personal pronouns (Habash et al., 2019)). The objective of this research aims to include as many cases as possible of gender alternatives related not only to gender of persons but also to grammatical gender of the objects referred to. In Example 1, (a) illustrates an example of two alternatives for a sentence where Gender bias is a frequent occurrence in NLPbased applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of"
2021.gebnlp-1.11,W18-1820,0,0.0218077,"ied manually, thus it contained some noise. segments 2 193 657 1 018 3 066 5 648 15 892 Table 1: Statistics of data used for building the NMT rewriter. 5 Neural Rewriter Once we compiled a sufficient amount of genderparalell data, we were able to train our automatic rewriter. The automatic rewriter is a NMT system trained on the following parallel data: original sentences as the source language, and re-gendered sentence as the target language. For neutral sentences, the source and the target parts are identical. The NMT rewriter was built using the publicly available Sockeye10 implementation (Hieber et al., 2018) of the Transformer architecture (Vaswani et al., 2017). The system operates on subword units generated by byte-pair encoding (BPE)(Sennrich et al., 2016). We set the number of BPE merging operations to 32000. We have experimented with the following setups: • a Standard NMT system without any additional tags • an NMT system with neutrality/regenderability tags in the source part In addition to OpenSubtitles, we also obtained data from the industry partner consisting of around 8 000 sentences readily available with all possible alternative versions of the sentences provided. An additional 22 00"
2021.gebnlp-1.11,P16-1162,0,0.01095,"5 Neural Rewriter Once we compiled a sufficient amount of genderparalell data, we were able to train our automatic rewriter. The automatic rewriter is a NMT system trained on the following parallel data: original sentences as the source language, and re-gendered sentence as the target language. For neutral sentences, the source and the target parts are identical. The NMT rewriter was built using the publicly available Sockeye10 implementation (Hieber et al., 2018) of the Transformer architecture (Vaswani et al., 2017). The system operates on subword units generated by byte-pair encoding (BPE)(Sennrich et al., 2016). We set the number of BPE merging operations to 32000. We have experimented with the following setups: • a Standard NMT system without any additional tags • an NMT system with neutrality/regenderability tags in the source part In addition to OpenSubtitles, we also obtained data from the industry partner consisting of around 8 000 sentences readily available with all possible alternative versions of the sentences provided. An additional 22 000 sentences had to be revised manually in order to produce the correct gender variant for re-genderable sentences. This set was used as an additional test"
2021.gebnlp-1.11,tiedemann-2012-parallel,0,0.0148612,"Missing"
2021.gebnlp-1.11,2021.eacl-main.188,1,0.743414,"oth the automatic and manual analysis of the approach show promising results for automatic generation of gender alternatives for conversational sentences in Spanish. 1 Introduction Recent studies have exposed challenging systematic issues related to bias that extend to a range of AI applications, including Natural Language Processing (NLP) technology (Costa-jussà, 2019; Blodgett et al., 2020). Observed bias problems range from copying biases already existing in data to claims that the training process can lead to an exacerbation or amplification of observed biases (Zhou and Schiebinger, 2018; Vanmassenhove et al., 2021). The algorithms learn to maximize the overall probability of an occurrence, leading to 93 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 93–102 August 5, 2021. ©2021 Association for Computational Linguistics there is agreement with the grammatical gender of an object referred to in the previous sentence, while in (b) there is agreement with the gender of the speaker/writer (i.e. a person). other approaches which heavily rely on linguistic tools (Zmigrod et al., 2019) or on manually created gender-parallel data (Habash et al., 2019). 2 Example 1. (a) [MALE"
2021.gebnlp-1.11,D18-1521,0,0.0277006,"et (pre-processing step) or in the output (post-processing step). In the following paragraphs, we focus on the latter as it is most closely related to our approach. There have been attempts to artificially increase the variety in already existing data sets by creating alternatives to sentences in order to decrease the overall bias (in terms of gender).4 This approach has been referred to in the literature as ‘Counterfactual Data Augmentation’(CDA) (Lu et al., 2018). Their CDA approach consists of a simple bidirectional dictionary of gendered words such as he:she, her:him/his, queen:king, etc. Zhao et al. (2018) does not use the term CDA as this was introduced later, but what they describe can be interpreted as a rudimentary approach to CDA: they augmented the existing data set by adding additional sentences in which personal pronouns ‘he’ and ‘she’ had been swapped. Another CDA approach is described in Zmigrod et al. (2019). Similar to Lu et al. (2018), the approach relies on a bidirectional dictionary of animate nouns. Unlike Lu et al. (2018), pronouns are not handled and the languages worked on are Hebrew and Spanish, languages that have more gender markers than English. Since solely changing the"
2021.gebnlp-1.11,P19-1161,0,0.283352,"most probable option which is prone to bias. In our work, we aim to enable the generation of multiple gender variants by expanding each sentence with the missing gender variants, thus fostering inclusion in online conversations/NLP applications. Generating gender variants can and should also be used to create gender balanced conversational data that can be used to train less biased NLP models such as machine translation models, language models, chat bots, etc. Unlike previous studies, we did not want to limit ourselves to one specific gender phenomenon, such as gender markings on professions (Zmigrod et al., 2019)) (for which the gender can easily be swapped by using hand-crafted lists) or first person personal pronouns (Habash et al., 2019)). The objective of this research aims to include as many cases as possible of gender alternatives related not only to gender of persons but also to grammatical gender of the objects referred to. In Example 1, (a) illustrates an example of two alternatives for a sentence where Gender bias is a frequent occurrence in NLPbased applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns"
2021.inlg-1.31,2020.inlg-1.24,1,0.709963,"ded. The method can be applied to any language generation task, genre/domain and language (pair), and can be guided by diverse error types (quality criteria). 2.2 Quality Criteria and Error Rates The two quality criteria underlying error annotations were Comprehensibility and Adequacy, both commonly used in MT (ALPAC, 1966; White et al., 1994; Roturier and Bensadoun, 2011). Comprehensibility: The degree to which a text can be understood. When evaluating Comprehensbility of a translated text, the source language text is not shown to evaluators. In terms of the classification system proposed by Belz et al. (2020), Comprehensibility captures the goodness of both the form and content of a text in its own right, and is assessed here by a subjective, absolute, intrinsic evaluation measure. Adequacy (in MT): The degree to which a translation conveys the meaning of the original text in the source language. When evaluating adequacy of a translated text, the source language text is shown to evaluators. In terms of Belz et al.’s classification system, Adequacy captures the correctness of the content of a text relative to the input, and is assessed here also by a subjective, absolute, intrinsic evaluation measu"
2021.inlg-1.31,2020.eamt-1.15,0,0.0251609,"ificant at α = 0.01.) system-level correlation results: while All and Major error counts correlate reasonably well for both error types (although slightly better for Adequacy than for Comprehension), the coefficients for the Minor error types are notably lower. We will return to some of the above points in the discussion section (Section 5). 4 Inter-annotator Agreement The original study reported inter-annotator agreement (IAA) in terms of F-score and normalised edit distance (definitions below). In this paper we also report Krippendorff’s α for both original and reproduction study, following Kreutzer et al. (2020) who used it in a similar error marking study.8 F-score: To compute sentence-level F1-score, the starting point was the paired sequences ev1 and ev2 of word-level error labels (Major, Minor or None) assigned by the two annotators to a sentence. Precision was then computed as the labels from ev1 also present in ev2, and Recall as labels from ev2 also present in ev1. The F1-score was then calculated in the usual way, as the harmonic mean of Precision and Recall. Due to possible length differences in a pair of label sequences (due to insertion of X labels representing missing words), matches are"
2021.inlg-1.31,P11-1015,0,0.0180998,"Missing"
2021.inlg-1.31,2020.coling-main.444,1,0.856685,"Missing"
2021.inlg-1.31,2011.mtsummit-papers.27,0,0.0511433,"n for Computational Linguistics MT typically ask annotators either to assign overall per-sentence scores, or to rank two or more translations in terms of given quality criteria, i.e. information about any errors that motivate scores/rankings is not recorded. The method can be applied to any language generation task, genre/domain and language (pair), and can be guided by diverse error types (quality criteria). 2.2 Quality Criteria and Error Rates The two quality criteria underlying error annotations were Comprehensibility and Adequacy, both commonly used in MT (ALPAC, 1966; White et al., 1994; Roturier and Bensadoun, 2011). Comprehensibility: The degree to which a text can be understood. When evaluating Comprehensbility of a translated text, the source language text is not shown to evaluators. In terms of the classification system proposed by Belz et al. (2020), Comprehensibility captures the goodness of both the form and content of a text in its own right, and is assessed here by a subjective, absolute, intrinsic evaluation measure. Adequacy (in MT): The degree to which a translation conveys the meaning of the original text in the source language. When evaluating adequacy of a translated text, the source langu"
2021.inlg-1.31,1994.amta-1.25,0,0.347633,"Missing"
2021.mtsummit-research.14,2020.eamt-1.41,0,0.061126,"Missing"
2021.mtsummit-research.14,2020.wmt-1.1,0,0.0369872,"well as to serve as a gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from best to worst (Vilar et al., 2007; Callison-Bur"
2021.mtsummit-research.14,W15-3001,0,0.0312847,", 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from best to worst (Vilar et al., 2007; Callison-Burch et al., 2008; Bojar et al., 2015). For assessing comprehension, question answering (Scarton and Specia, 2016) and filling gaps (Forcada et al., 2018) were explored, too. Recently, evaluators have been asked to highlight the observed translation errors (Kreutzer et al., 2020; Popovi´c, 2020a). In order to get more details about the actual errors, error classification according to a predefined error scheme is often performed. The mostly applied schemes have been the one proposed Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 163 by Vilar et al. (20"
2021.mtsummit-research.14,W08-0309,0,0.0840523,"et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from best to worst (Vilar et al., 2007; Callison-Burch et al., 2008; Bojar et al., 2015). For assessing comprehension, question answering (Scarton and Specia, 2016) and filling gaps (Forcada et al., 2018) were explored, too. Recently, evaluators have been asked to highlight the observed translation errors (Kreutzer et al., 2020; Popovi´c, 2020a). In order to get more details about the actual errors, error classification according to a predefined error scheme is often performed. The mostly applied schemes have been the one proposed Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 16"
2021.mtsummit-research.14,D14-1172,0,0.0218482,"rs exclusively related to gender and/or case: there are more gender and case errors, but within other phenomena with larger spans: rephrasing, noun phrase, conjunction. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 169 4.1 Major vs minor errors As mentioned in Section 2, the evaluators of the DCU data set were asked to distinguish between major and minor errors. While some of the phenomena are found to be much more frequent than others, frequency of errors is not necessarily related to their importance/severity (Federico et al., 2014; Kirchhoff et al., 2014). Therefore, we further analysed all identified phenomena in order to determine whether they are more related to major or to minor errors. We have, however, to take into account that for the less frequent phenomena, the results of this analysis might not be sufficiently reliable. Perceptions of each adequacy comprehension phenomenon major minor correct major minor correct of the phenomena in the rephrasing 32.0 37.6 30.3 33.6 38.0 28.4 form of percentage are ambiguity 48.2 31.5 20.3 39.2 39.2 21.6 shown in Table 5. The noun phrase 35.5 34.2 30.2 33.1 35.6 31.3 numbers"
2021.mtsummit-research.14,W18-6320,0,0.0280612,"ality and progress, as well as to serve as a gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from best to worst (Vilar et"
2021.mtsummit-research.14,W13-2305,0,0.0244246,"k for measuring MT quality and progress, as well as to serve as a gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from be"
2021.mtsummit-research.14,W18-1820,0,0.020081,"le data sets with highlighted MT errors: one provided by Dublin City University (DCU)2 and one provided by Heidelberg University (HU).3 While both data sets contain MT outputs with highlighted translation errors, there are several important differences between them. DCU data set This data set was created for purposes of MT evaluation (Popovi´c, 2020a). The set consists of English user reviews translated into Croatian and Serbian. For each of the target languages, five different MT systems were used: three online systems (Amazon, Bing and Google) and two in-house systems based on the Sockeye4 (Hieber et al., 2018) implementation. In total, the data set contains outputs of ten different MT systems. Two quality criteria were used for highlighting errors: adequacy and comprehension. An important difference between the two (apart from the definition) which can lead to differences 1 http://www.qt21.eu/mqm-definition/definition-2015-12-30.html 2 https://github.com/m-popovic/QRev-annotations 3 https://www.cl.uni-heidelberg.de/statnlpgroup/humanmt/ 4 https://github.com/awslabs/sockeye Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page"
2021.mtsummit-research.14,D17-1263,0,0.0682727,"Missing"
2021.mtsummit-research.14,D19-3019,0,0.0140283,"largest part of the text is annotated by two evaluators, while a small part of the text (about 40 sentences) is annotated by three or four evaluators. Nothing is annotated by a single evaluator. Inter-annotator agreement in terms of Krippendorf’s α is 0.61 for adequacy errors and 0.51 for comprehension errors. HU data set This data set was not created for purposes of MT evaluation, but for improving an NMT system by giving it feedback about errors (Kreutzer et al., 2020). The set consists of English TED talks translated into German by one MT system, an in-house system based on the Joey NMT5 (Kreutzer et al., 2019) implementation. A very important difference in comparison to the DCU data set is that no specific quality criterion was used: the evaluators were only asked to “highlight the errors”. Usually, such “generic” criterion represents a mixture of adequacy and fluency. Also, they were not asked to distinguish between major and minor errors. Another very important fact is, since the data set is created in order to improve a system, and the used loss function did not support omissions and reordering errros, the evaluators are specifically asked not to highlight these two types of errors. As for conte"
2021.mtsummit-research.14,2020.eamt-1.15,0,0.305475,"gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from best to worst (Vilar et al., 2007; Callison-Burch et al., 2008; Bojar"
2021.mtsummit-research.14,2014.eamt-1.38,1,0.856048,"Missing"
2021.mtsummit-research.14,W19-5302,0,0.0228852,"1 Introduction and related work Machine translation (MT), like many other natural language generation tasks, is difficult to evaluate because there is no single correct output for a given input: for each source text, there is a large set of possible correct translations. Therefore, while costly both in time and resources, human evaluation is required to provide a reliable feedback for measuring MT quality and progress, as well as to serve as a gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved"
2021.mtsummit-research.14,W18-1803,0,0.0188158,"nnial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 170 text, POS ambiguity and hallucinations are mainly perceived as major errors. Repetitions and prepositions are mostly perceived as minor comprehension errors, but equally often as major and as minor adequacy errors. The presented results indicate not only that severity of errors is perceived differently for different phenomena, but also that perception of some phenomena depends on the quality criterion. Previous work has already shown that adequacy errors are often “masked” by good fluency (Martindale and Carpuat, 2018), and also by good comprehension (Popovi´c, 2020b). All that motivated us to investigate the differences between quality criteria for each of the identified phenomena. 4.2 Adequacy vs comprehension Table 6 presents discrepances between the two quality criteria: inadequate comprehensible words are the words which changed the meaning of the source text but are perceived as comprehensible when reading the translation. On the other hand, adequate incomprehensible words are the words which are perceived as incomprehensible although their meaning is preserved. The results are presented only for the"
2021.mtsummit-research.14,2020.wmt-1.77,0,0.0266084,"in human evaluations. 1 Introduction and related work Machine translation (MT), like many other natural language generation tasks, is difficult to evaluate because there is no single correct output for a given input: for each source text, there is a large set of possible correct translations. Therefore, while costly both in time and resources, human evaluation is required to provide a reliable feedback for measuring MT quality and progress, as well as to serve as a gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much me"
2021.mtsummit-research.14,W18-6307,0,0.0596618,"Missing"
2021.mtsummit-research.14,2020.coling-main.444,1,0.855822,"Missing"
2021.mtsummit-research.14,2020.conll-1.19,1,0.838496,"Missing"
2021.mtsummit-research.14,W19-5354,0,0.0193843,"translations) while oth- Table 4: Percentages of perceived errors related to the identified ers tend to diverge from the phenomena: adequacy errors in DCU corpus (left), comprehensource (generating incorrect sion errors in DCU corpus (middle), errors in HU corpus (right). rephrasings). These effects should be investigated further in more details, also by creating appropriate test suites. As for ambiguous source words, our analysis confirmed that they represent a challenge for modern NMT systems. Several test suites have already been developed (Rios Gonzales et al., 2018; M¨uller et al., 2018; Raganato et al., 2019), but creating more test suites covering different types of ambiguous words and various language pairs would be certainly beneficial. It should be noted that, while translation of ambiguous words can be improved by context-aware (“document-level”) NMT systems, incorporating external context often could be more helpful than extending context to more sentences. For example, if a source text is a product review, it can indicate that “I will get this part” most probably means “I will buy this part of some object”, while for a movie or book review “I don’t get this part” probably means “I don’t und"
2021.mtsummit-research.14,W18-6437,0,0.0637978,"Missing"
2021.mtsummit-research.14,2011.mtsummit-papers.27,0,0.0713042,"beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more competing outputs from best to worst (Vilar et al., 2007; Callison-Burch et al., 2008; Bojar et al., 2015). For assessing comprehension, question answering (Scarton and Specia, 2016) and filling gaps (Forcada et al., 2018) were explored, too. Recently, evaluators have been asked to highlight the observed translation errors (Kreutzer et al., 2020; Popovi´c, 2020a). In order to get more details about the actual errors, error classification according to a predefined error scheme is often performed. The mostly applied schemes hav"
2021.mtsummit-research.14,W18-6305,0,0.0824299,"Missing"
2021.mtsummit-research.14,W07-0713,0,0.0293972,"e a reliable feedback for measuring MT quality and progress, as well as to serve as a gold standard for development of automatic evaluation metrics. While better and better automatic metrics are constantly emerging (Mathur et al., 2020; Ma et al., 2019), many of them being based on semantic word representations (embeddings), all of them represent only an approximate substitution for human assessment of translation quality. Various methods have been proposed and used for the human evaluation of MT output from the beginning of MT until now (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Vilar et al., 2007; Graham et al., 2013; Forcada et al., 2018; Barrault et al., 2020; Kreutzer et al., 2020; Popovi´c, 2020a), and all of them are essentially based on some of the following three quality criteria: adequacy (how much meaning is preserved), comprehensibility (how comprehensible/readable the translation is) and fluency (grammar of the target language). The evaluators are usually asked to assign an overall quality score for the given MT output (ALPAC, 1966; White et al., 1994; Koehn and Monz, 2006; Roturier and Bensadoun, 2011; Graham et al., 2013; Barrault et al., 2020) or to rank two or more comp"
2021.mtsummit-research.14,vilar-etal-2006-error,0,0.232414,"Missing"
2021.mtsummit-research.14,P19-1116,0,0.0250986,"21). Another method to better understand particular strengths and weaknesses of MT systems is to identify nature and causes of the errors in form of linguistically motivated phenomena which, although related, often go beyond the usual error types. This type of analysis is being increasingly employed in the last years in order to better understand the ocurring errors (Popovi´c, 2018; Arnejˇsek and Unk, 2020) and also to create specialised test sets (“challenge test sets” or “test suites”) in order to perform more focussed evaluation procedures on identified phenomena (Isˇ stari´c et al., 2018; Voita et al., 2019). abelle et al., 2017; Soˇ This work goes in this direction, but in a slightly different way: we do not try to identify the phenomena from scratch, but from translation errors already observed and highlighted by several evaluators (Kreutzer et al., 2020; Popovi´c, 2020a). The error marking was not guided by any pre-defined error scheme, so that the evaluators had more freedom in annotating errors than in typical error classification tasks such as MQM. We analysed the nature of these errors by tagging them with possible causes and/or plausible explanations of their origin (referred to as “pheno"
2021.mtsummit-research.14,1994.amta-1.25,0,0.860777,"Missing"
avramidis-etal-2012-involving,eisele-chen-2010-multiun,0,\N,Missing
avramidis-etal-2012-involving,federmann-2010-appraise,1,\N,Missing
avramidis-etal-2012-involving,J11-4002,1,\N,Missing
avramidis-etal-2012-involving,P02-1040,0,\N,Missing
avramidis-etal-2012-involving,W10-1738,1,\N,Missing
avramidis-etal-2012-involving,P07-2045,0,\N,Missing
avramidis-etal-2012-involving,W11-2104,1,\N,Missing
avramidis-etal-2012-involving,W10-1703,0,\N,Missing
avramidis-etal-2012-involving,vilar-etal-2006-error,1,\N,Missing
avramidis-etal-2012-involving,W11-2100,0,\N,Missing
avramidis-etal-2012-involving,2011.eamt-1.36,1,\N,Missing
avramidis-etal-2012-involving,2010.amta-papers.27,0,\N,Missing
avramidis-etal-2014-taraxu,federmann-2010-appraise,0,\N,Missing
avramidis-etal-2014-taraxu,avramidis-etal-2012-involving,1,\N,Missing
avramidis-etal-2014-taraxu,W10-1738,1,\N,Missing
avramidis-etal-2014-taraxu,P07-2045,0,\N,Missing
avramidis-etal-2014-taraxu,2013.mtsummit-posters.5,1,\N,Missing
berka-etal-2012-automatic,niessen-etal-2000-evaluation,0,\N,Missing
berka-etal-2012-automatic,C08-1141,0,\N,Missing
berka-etal-2012-automatic,J03-1002,0,\N,Missing
berka-etal-2012-automatic,fishel-etal-2012-terra,1,\N,Missing
berka-etal-2012-automatic,vilar-etal-2006-error,0,\N,Missing
berka-etal-2012-automatic,W11-2107,0,\N,Missing
C04-1045,J93-2003,0,0.0132705,"ty compared to the, to our knowledge, best system are reported on the German-English Verbmobil corpus. 1 Introduction In statistical machine translation, a translation model P r(f1J |eI1 ) describes the correspondences between the words in the source language sentence f1J and the words in the target language sentence eI1 . Statistical alignment models are created by introducing a hidden variable aJ1 representing a mapping from the source word fj into the target word eaj . So far, most of the statistical machine translation systems are based on the single-word alignment models as described in (Brown et al., 1993) as well as the Hidden Markov alignment model (Vogel et al., 1996). The lexicon models used in these systems typically do not include any linguistic or contextual information which often results in inadequate alignments between the sentence pairs. In this work, we propose an approach to improve the quality of the statistical alignments by taking into account the interdependencies of different derivations of the words. We are getting use of the hierarchical representation of the statistical lexicon model as proposed in (Nießen and Ney, 2001) for the conventional EM training procedure. Experimen"
C04-1045,C02-1032,1,0.704972,"Missing"
C04-1045,2001.mtsummit-papers.45,1,0.949885,"ed on the single-word alignment models as described in (Brown et al., 1993) as well as the Hidden Markov alignment model (Vogel et al., 1996). The lexicon models used in these systems typically do not include any linguistic or contextual information which often results in inadequate alignments between the sentence pairs. In this work, we propose an approach to improve the quality of the statistical alignments by taking into account the interdependencies of different derivations of the words. We are getting use of the hierarchical representation of the statistical lexicon model as proposed in (Nießen and Ney, 2001) for the conventional EM training procedure. Experimental results are reported for the German-English Verbmobil corpus and the evaluation is done by comparing the obtained Viterbi alignments after the training of conventional models and models which are using morpho-syntactic information with a manually annotated reference alignment. 2 Related Work The popular IBM models for statistical machine translation are described in (Brown et al., 1993) and the HMM-based alignment model was introduced in (Vogel et al., 1996). A good overview of all these models is given in (Och and Ney, 2003) where the"
C04-1045,W01-1407,1,0.909123,"ed on the single-word alignment models as described in (Brown et al., 1993) as well as the Hidden Markov alignment model (Vogel et al., 1996). The lexicon models used in these systems typically do not include any linguistic or contextual information which often results in inadequate alignments between the sentence pairs. In this work, we propose an approach to improve the quality of the statistical alignments by taking into account the interdependencies of different derivations of the words. We are getting use of the hierarchical representation of the statistical lexicon model as proposed in (Nießen and Ney, 2001) for the conventional EM training procedure. Experimental results are reported for the German-English Verbmobil corpus and the evaluation is done by comparing the obtained Viterbi alignments after the training of conventional models and models which are using morpho-syntactic information with a manually annotated reference alignment. 2 Related Work The popular IBM models for statistical machine translation are described in (Brown et al., 1993) and the HMM-based alignment model was introduced in (Vogel et al., 1996). A good overview of all these models is given in (Och and Ney, 2003) where the"
C04-1045,P00-1056,1,0.739818,"e initial models. Train where f bt represents the base form of the word f with sequence of corresponding tags, e.g. “gehen-V-IND-PRES”; s and the M-step is then performed using hierarchical counts: δ(f bt, f btjs )δ(e, eis ) i,j C(f b, e) = For each full form, refined hierarchical counts are obtained in the following way: Test Sentences Words Vocabulary Singletons Sentences Words S relations P relations German English 34446 329625 343076 5936 3505 2600 1305 354 3233 3109 2559 4596 Table 1: Corpus statistics for Verbmobil task 6.1 Evaluation Method We use the evaluation criterion described in (Och and Ney, 2000). The obtained word alignment is compared to a reference alignment produced by human experts. The annotation scheme explicitly takes into account the ambiguity of the word alignment. The unambiguous alignments are annotated as sure alignments (S) and the ambiguous ones as possible alignments (P ). The set of possible alignments P is used especially for idiomatic expressions, free translations and missing function words. The set S is subset of the set P (S ⊆ P ). The quality of an alignment A is computed as appropriately redefined precision and recall measures. Additionally, we use the alignmen"
C04-1045,J03-1002,1,0.111339,"sed in (Nießen and Ney, 2001) for the conventional EM training procedure. Experimental results are reported for the German-English Verbmobil corpus and the evaluation is done by comparing the obtained Viterbi alignments after the training of conventional models and models which are using morpho-syntactic information with a manually annotated reference alignment. 2 Related Work The popular IBM models for statistical machine translation are described in (Brown et al., 1993) and the HMM-based alignment model was introduced in (Vogel et al., 1996). A good overview of all these models is given in (Och and Ney, 2003) where the model IBM-6 is also introduced as the log-linear interpolation of the other models. Context dependencies have been introduced into the training of alignments in (Varea et al., 2002), but they do not take any linguistic information into account. Some recent publications have proposed the use of morpho-syntactic knowledge for statistical machine translation, but mostly only for the preprocessing step whereas training procedure of the statistical models remains the same (e.g. (Nießen and Ney, 2001a)). Incorporation of the morpho-syntactic knowlegde into statistical models has been deal"
C04-1045,C96-2141,1,0.9073,"the German-English Verbmobil corpus. 1 Introduction In statistical machine translation, a translation model P r(f1J |eI1 ) describes the correspondences between the words in the source language sentence f1J and the words in the target language sentence eI1 . Statistical alignment models are created by introducing a hidden variable aJ1 representing a mapping from the source word fj into the target word eaj . So far, most of the statistical machine translation systems are based on the single-word alignment models as described in (Brown et al., 1993) as well as the Hidden Markov alignment model (Vogel et al., 1996). The lexicon models used in these systems typically do not include any linguistic or contextual information which often results in inadequate alignments between the sentence pairs. In this work, we propose an approach to improve the quality of the statistical alignments by taking into account the interdependencies of different derivations of the words. We are getting use of the hierarchical representation of the statistical lexicon model as proposed in (Nießen and Ney, 2001) for the conventional EM training procedure. Experimental results are reported for the German-English Verbmobil corpus a"
C04-1045,W02-1012,0,\N,Missing
fishel-etal-2012-terra,specia-etal-2010-dataset,0,\N,Missing
fishel-etal-2012-terra,W10-1755,0,\N,Missing
fishel-etal-2012-terra,J11-4002,1,\N,Missing
fishel-etal-2012-terra,P02-1040,0,\N,Missing
fishel-etal-2012-terra,W10-1738,0,\N,Missing
fishel-etal-2012-terra,P11-1103,0,\N,Missing
fishel-etal-2012-terra,W09-0401,0,\N,Missing
fishel-etal-2012-terra,P07-2045,1,\N,Missing
fishel-etal-2012-terra,P11-1022,0,\N,Missing
fishel-etal-2012-terra,C08-1141,0,\N,Missing
fishel-etal-2012-terra,P11-4010,0,\N,Missing
fishel-etal-2012-terra,2011.eamt-1.12,0,\N,Missing
fishel-etal-2012-terra,vilar-etal-2006-error,0,\N,Missing
fishel-etal-2012-terra,2010.eamt-1.12,0,\N,Missing
J11-4002,W05-0909,0,0.0279152,"nstraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times. Preprocessing and normalization methods for improving the evaluation using the standard measures WER, PER, BLEU , and NIST are investigated by Leusch et al. (2005). The same set of measures is examined by Matusov et al. (2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g., translation of speech recognition output). The METEOR metric (Banerjee and Lavie 2005) ﬁrst counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003). IQ (Gim´enez and Amigo´ 2006) is a framework for automatic evaluation in which evaluation metrics can be combined. Nevertheless, none of these measures or extensions takes into account any details about actual translation errors, for example, what the contribution of verbs is in the overall error rate, how many"
J11-4002,J07-2003,0,0.0194535,"each of the error categories, the basic POS classes are analyzed as well in order to estimate which POS classes of each category are reliable for the comparison of translation outputs. The investigations are carried out on six Spanish-to-English TC- STAR outputs generated by phrase-based systems (Vilar et al. 2005) and three German-to-English WMT 09 outputs produced in the framework of the fourth shared translation task (CallisonBurch et al. 2009). Two of the WMT 09 outputs are generated by standard phrase-based systems (Zens, Och, and Ney 2002) and one by a hierarchical phrase-based system (Chiang 2007). For the TC- STAR outputs two reference translations are available for the automatic error analysis, and for the WMT 09 outputs only a single reference is available. For all texts, the ﬂexible human error analysis is carried out. The following sections summarize all the results along with the Spearman and Pearson correlation coefﬁcients calculated across the different translation outputs. 4.3.1 Results on TC-STAR Corpora. The error analyses were carried out on six Spanishto-English outputs generated by phrase-based translation systems built on different sizes of training corpora in order to e"
J11-4002,gimenez-amigo-2006-iqmt,0,0.0514713,"Missing"
J11-4002,2007.mtsummit-papers.39,0,0.120279,"vie 2005) is presented together with a detailed analysis of the obtained results. Automatic error analysis is still a rather unexplored area. A method for automatic identiﬁcation of patterns in translation output using POS sequences is proposed by Lopez and Resnik (2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between WER and PER for three POS classes (nouns, adjectives, and verbs) is proposed by Popovi´c et al. (2006) for the estimation of inﬂectional and reordering errors. Semi-automatic error analysis (Kirchhoff et al. 2007) is carried out in order to identify problematic 1 G ALE — Global Autonomous Language Exploitation. http://www.arpa.mil/ipto/programs/gale/ index.htm. 2 TC- STAR — Technology and Corpora for Speech to Speech Translation. http://www.tc-star.org/. 3 EACL 09 Fourth Workshop on Statistical Machine Translation. http://www.statmt.org/wmt09/. 659 Computational Linguistics Volume 37, Number 4 characteristics of source documents such as genre, domain, language, and so on. Zhou et al. (2008) propose a diagnostic evaluation of linguistic check-points obtained automatically by aligning parsed source and t"
J11-4002,E06-1031,1,0.873192,"Missing"
J11-4002,W05-0903,1,0.917898,"e Translation Edit Rate (TER) (Snover et al. 2006) and the CD ER measure (Leusch, Uefﬁng, and Ney 2006) are based on the edit distance (WER) but allow reordering of blocks. TER uses an edit distance with additional costs for shifts of word sequences. The CD ER measure drops certain constraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times. Preprocessing and normalization methods for improving the evaluation using the standard measures WER, PER, BLEU , and NIST are investigated by Leusch et al. (2005). The same set of measures is examined by Matusov et al. (2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g., translation of speech recognition output). The METEOR metric (Banerjee and Lavie 2005) ﬁrst counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003). IQ (Gim´enez and Amigo´"
J11-4002,2005.eamt-1.13,0,0.0496164,"Missing"
J11-4002,H05-2007,0,0.0199519,"nslation errors, for example, what the contribution of verbs is in the overall error rate, how many full forms are wrong although their base forms are correct, or how many words are missing. A framework for human error analysis and error classiﬁcation ´ Carbonell, has been proposed by Vilar et al. (2006), where a classiﬁcation scheme (Llitjos, and Lavie 2005) is presented together with a detailed analysis of the obtained results. Automatic error analysis is still a rather unexplored area. A method for automatic identiﬁcation of patterns in translation output using POS sequences is proposed by Lopez and Resnik (2005) in order to see how well a translation system is capable of capturing systematic reordering patterns. Using relative differences between WER and PER for three POS classes (nouns, adjectives, and verbs) is proposed by Popovi´c et al. (2006) for the estimation of inﬂectional and reordering errors. Semi-automatic error analysis (Kirchhoff et al. 2007) is carried out in order to identify problematic 1 G ALE — Global Autonomous Language Exploitation. http://www.arpa.mil/ipto/programs/gale/ index.htm. 2 TC- STAR — Technology and Corpora for Speech to Speech Translation. http://www.tc-star.org/. 3 E"
J11-4002,2005.iwslt-1.19,1,0.739337,"ER measure (Leusch, Uefﬁng, and Ney 2006) are based on the edit distance (WER) but allow reordering of blocks. TER uses an edit distance with additional costs for shifts of word sequences. The CD ER measure drops certain constraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times. Preprocessing and normalization methods for improving the evaluation using the standard measures WER, PER, BLEU , and NIST are investigated by Leusch et al. (2005). The same set of measures is examined by Matusov et al. (2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g., translation of speech recognition output). The METEOR metric (Banerjee and Lavie 2005) ﬁrst counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003). IQ (Gim´enez and Amigo´ 2006) is a framework for automatic evaluation in which evaluati"
J11-4002,P02-1040,0,0.092082,"contribution of each error category in a particular translation output, and comparing different translation outputs using these categories. In addition, we show how the new error measures can be used to get more information about the differences between translation systems trained on different source and target languages, between different training set-ups for a same phrase-based translation system, as well as between different translation systems. 1.1 Related Work A number of automatic evaluation measures for machine translation output have been investigated in recent years. The BLEU metric (Papineni et al. 2002) and the closely related NIST metric (Doddington 2002), along with WER and PER, have been widely used by many machine translation researchers. The Translation Edit Rate (TER) (Snover et al. 2006) and the CD ER measure (Leusch, Uefﬁng, and Ney 2006) are based on the edit distance (WER) but allow reordering of blocks. TER uses an edit distance with additional costs for shifts of word sequences. The CD ER measure drops certain constraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times"
J11-4002,W06-3101,1,0.868667,"Missing"
J11-4002,popovic-ney-2006-pos,1,0.752492,"Missing"
J11-4002,2006.amta-papers.25,0,0.101284,"ed to get more information about the differences between translation systems trained on different source and target languages, between different training set-ups for a same phrase-based translation system, as well as between different translation systems. 1.1 Related Work A number of automatic evaluation measures for machine translation output have been investigated in recent years. The BLEU metric (Papineni et al. 2002) and the closely related NIST metric (Doddington 2002), along with WER and PER, have been widely used by many machine translation researchers. The Translation Edit Rate (TER) (Snover et al. 2006) and the CD ER measure (Leusch, Uefﬁng, and Ney 2006) are based on the edit distance (WER) but allow reordering of blocks. TER uses an edit distance with additional costs for shifts of word sequences. The CD ER measure drops certain constraints for the hypothesis: Only the words in the reference have to be covered exactly once, whereas those in the hypothesis can be covered zero, one, or multiple times. Preprocessing and normalization methods for improving the evaluation using the standard measures WER, PER, BLEU , and NIST are investigated by Leusch et al. (2005). The same set of measures is"
J11-4002,2003.mtsummit-papers.51,0,0.173097,"Missing"
J11-4002,2005.mtsummit-papers.34,1,0.667368,"FTE EsEn1 VT 0.800 0.800 0.800 0.935 0.552 0.978 1.000 1.000 1.000 0.996 0.983 0.991 EnEs1 FTE EnEs2 FTE EnEs1 VT 0.950 0.600 1.000 0.754 0.572 0.538 1.000 1.000 0.950 0.987 0.998 0.990 The experiments should also show how reliable each error category is for the comparison of translation outputs. For each of the error categories, the basic POS classes are analyzed as well in order to estimate which POS classes of each category are reliable for the comparison of translation outputs. The investigations are carried out on six Spanish-to-English TC- STAR outputs generated by phrase-based systems (Vilar et al. 2005) and three German-to-English WMT 09 outputs produced in the framework of the fourth shared translation task (CallisonBurch et al. 2009). Two of the WMT 09 outputs are generated by standard phrase-based systems (Zens, Och, and Ney 2002) and one by a hierarchical phrase-based system (Chiang 2007). For the TC- STAR outputs two reference translations are available for the automatic error analysis, and for the WMT 09 outputs only a single reference is available. For all texts, the ﬂexible human error analysis is carried out. The following sections summarize all the results along with the Spearman a"
J11-4002,vilar-etal-2006-error,1,0.535776,"Missing"
J11-4002,W09-0415,0,0.0195382,"Missing"
J11-4002,J10-3009,0,0.0103603,"on. http://www.tc-star.org/. 3 EACL 09 Fourth Workshop on Statistical Machine Translation. http://www.statmt.org/wmt09/. 659 Computational Linguistics Volume 37, Number 4 characteristics of source documents such as genre, domain, language, and so on. Zhou et al. (2008) propose a diagnostic evaluation of linguistic check-points obtained automatically by aligning parsed source and target sentences. For each check-point, the number of matched n-grams of the references is then calculated. Linguistically based reordering along with the syntax-based evaluation of reordering patterns is described in Xiong et al. (2010). In this work, we propose a novel framework for automatic error analysis of machine translation output based on WER and PER, and systematically investigate a set of possible methods to carry out an error analysis at the word level. 2. A Framework for Automatic Error Analysis The basic idea for automatic error analysis described in this work is to take into account details from the WER (edit distance) and PER algorithms, namely, to identify all erroneous words which are actually contributing to the error rate, and then to combine these words with different types of linguistic knowledge. The ge"
J11-4002,2002.tmi-tutorials.2,0,0.066437,"Missing"
J11-4002,C08-1141,0,0.1705,"opovi´c et al. (2006) for the estimation of inﬂectional and reordering errors. Semi-automatic error analysis (Kirchhoff et al. 2007) is carried out in order to identify problematic 1 G ALE — Global Autonomous Language Exploitation. http://www.arpa.mil/ipto/programs/gale/ index.htm. 2 TC- STAR — Technology and Corpora for Speech to Speech Translation. http://www.tc-star.org/. 3 EACL 09 Fourth Workshop on Statistical Machine Translation. http://www.statmt.org/wmt09/. 659 Computational Linguistics Volume 37, Number 4 characteristics of source documents such as genre, domain, language, and so on. Zhou et al. (2008) propose a diagnostic evaluation of linguistic check-points obtained automatically by aligning parsed source and target sentences. For each check-point, the number of matched n-grams of the references is then calculated. Linguistically based reordering along with the syntax-based evaluation of reordering patterns is described in Xiong et al. (2010). In this work, we propose a novel framework for automatic error analysis of machine translation output based on WER and PER, and systematically investigate a set of possible methods to carry out an error analysis at the word level. 2. A Framework fo"
J11-4002,W09-0401,0,\N,Missing
J11-4002,W07-0718,0,\N,Missing
L16-1005,W15-4913,1,0.865948,"Missing"
L16-1005,2011.eamt-1.36,1,0.841266,"Missing"
L16-1005,W15-4914,1,0.899844,"Missing"
L16-1005,tiedemann-2012-parallel,0,0.0816726,"Missing"
L16-1005,vilar-etal-2006-error,0,0.448329,"Missing"
L16-1005,2013.mtsummit-papers.15,0,0.166131,"ty will derive valuable knowledge about the annotation process and translation errors from the corpus we presented. 1.1. Related work Publicly available post-edited data have been used for a while for quality estimation and error prediction WMT tasks1 , also for automatic post-editing task (Bojar et al., 2015). Parts of the corpora are also error annotated, though only with basic edit distance operations (substitution, deletion, insertion and shift) or binary tags (“ok” for correct words and “bad” for erroneous ones). Another corpus containing same type of edit annotation is the TRACE corpus (Wisniewski et al., 2013) which consists of FrenchEnglish and English-French post-edited translation outputs annotated with basic edit distance error types. Detailed manual error analysis has been often used in the last decade to determine the most prominent errors for particular task/translation system (e.g. (Vilar et al., 2006) at 1 27 http://www.statmt.org/wmt15/quality-estimation-task.html the beginning, (Lommel et al., 2014) recently), as well as to investigate impacts of different error classes to various aspects of translation quality ((Kirchhoff et al., 2012; Federico et al., 2014). Nevertheless, none of these"
L16-1005,W15-3001,0,0.0276669,"-ofthe-art SMT systems for the described target languages as well as for a number of related ones. We also believe that our positive experience of joining postediting, automatic and manual error classification will be further used for more language pairs and by professional translators, and that the community will derive valuable knowledge about the annotation process and translation errors from the corpus we presented. 1.1. Related work Publicly available post-edited data have been used for a while for quality estimation and error prediction WMT tasks1 , also for automatic post-editing task (Bojar et al., 2015). Parts of the corpora are also error annotated, though only with basic edit distance operations (substitution, deletion, insertion and shift) or binary tags (“ok” for correct words and “bad” for erroneous ones). Another corpus containing same type of edit annotation is the TRACE corpus (Wisniewski et al., 2013) which consists of FrenchEnglish and English-French post-edited translation outputs annotated with basic edit distance error types. Detailed manual error analysis has been often used in the last decade to determine the most prominent errors for particular task/translation system (e.g. ("
L16-1005,D14-1172,0,0.0182603,"ation is the TRACE corpus (Wisniewski et al., 2013) which consists of FrenchEnglish and English-French post-edited translation outputs annotated with basic edit distance error types. Detailed manual error analysis has been often used in the last decade to determine the most prominent errors for particular task/translation system (e.g. (Vilar et al., 2006) at 1 27 http://www.statmt.org/wmt15/quality-estimation-task.html the beginning, (Lommel et al., 2014) recently), as well as to investigate impacts of different error classes to various aspects of translation quality ((Kirchhoff et al., 2012; Federico et al., 2014). Nevertheless, none of these error analyses have been carried out on post-edits, only on raw translation outputs. Manual error analysis of Finish post-edits has been carried out in (Koponen, 2013) in order to investigate discrepances between the estimated cognitive effort and actual technical effort, however, to the best of our knowledge, the corpus is not publicly available. ¨ corpus (Avramidis et al., The publicly available TARA X U 2014) contains both post-edited as well as error annotated translation outputs, whereby the two tasks were carried out completely separately, and even not on th"
L16-1005,fishel-etal-2012-terra,1,0.888744,"Missing"
L16-1005,2012.eamt-1.35,0,0.0185885,"same type of edit annotation is the TRACE corpus (Wisniewski et al., 2013) which consists of FrenchEnglish and English-French post-edited translation outputs annotated with basic edit distance error types. Detailed manual error analysis has been often used in the last decade to determine the most prominent errors for particular task/translation system (e.g. (Vilar et al., 2006) at 1 27 http://www.statmt.org/wmt15/quality-estimation-task.html the beginning, (Lommel et al., 2014) recently), as well as to investigate impacts of different error classes to various aspects of translation quality ((Kirchhoff et al., 2012; Federico et al., 2014). Nevertheless, none of these error analyses have been carried out on post-edits, only on raw translation outputs. Manual error analysis of Finish post-edits has been carried out in (Koponen, 2013) in order to investigate discrepances between the estimated cognitive effort and actual technical effort, however, to the best of our knowledge, the corpus is not publicly available. ¨ corpus (Avramidis et al., The publicly available TARA X U 2014) contains both post-edited as well as error annotated translation outputs, whereby the two tasks were carried out completely separa"
L16-1005,P07-2045,0,0.00424994,"e number of actually correct words and (ii) manual and automatic error classification were performed completely independently. 2. 2012). It should be noted that the OpenSubtitles corpus contains transcriptions and translations of spoken language thus being slightly peculiar for machine translation. From ¨ corpus, the post-edited part containing WMT the TARA X U News texts is used. Table 1 gives an overview of the total amount of sentences and words used for generation of the corpus PE2 rr for each domain and translation direction. All translations have been generated using phrase-based Moses (Koehn et al., 2007), where the word alignments were built with GIZA++ (Och and Ney, 2003). The 5-gram language model was built with the SRILM toolkit (Stolcke, 2002). For each translation output, a system was trained on the corresponding in-domain parallel data. 3. Post-editing and error annotation For the Serbian and Slovenian MT outputs, both tasks, i.e. post-editing and error annotation, were performed by MT researchers with some experience with human translation. The annotators are native speakers of the target languages ¨ data, and fluent in both source languages. For the TARA X U post-editing was performed"
L16-1005,2005.mtsummit-papers.11,0,0.017249,"as Slavic languages, have quite free word order and are highly inflected. The derivational morphology is also rich, multiple negation is used, and there are no articles, only determiners. Similarly, German is also a morphologically rich language and a very challenging language for machine translation. Spanish is generally less inflective than the Slavic languages and German, however the number of possible verb inflections is rather high. The following domains/genres were used: news texts from the enhanced version3 of the SETimes (Tyers and Alperen, 2010) corpus for English→Serbian, EuroParl (Koehn, 2005) for English/German→Slovenian and OpenSubtitles4 for all language pairs. All the corpora are downloaded from the OPUS web site5 (Tiedemann, source language text machine translation system (Moses) machine translation post-editing post-edited machine translation automatic error annotation (Hjerson) error (pre-)annotated original and post-edited translations identifying language related issues error correction and/or expansion error annotated original and post-edited translations issue annotations Figure 1: Procedure of generating the PE2 rr corpus. 2 http://terra.cl.uzh.ch/terra-corpus-collectio"
L16-1005,2013.mtsummit-wptp.1,0,0.0179714,"analysis has been often used in the last decade to determine the most prominent errors for particular task/translation system (e.g. (Vilar et al., 2006) at 1 27 http://www.statmt.org/wmt15/quality-estimation-task.html the beginning, (Lommel et al., 2014) recently), as well as to investigate impacts of different error classes to various aspects of translation quality ((Kirchhoff et al., 2012; Federico et al., 2014). Nevertheless, none of these error analyses have been carried out on post-edits, only on raw translation outputs. Manual error analysis of Finish post-edits has been carried out in (Koponen, 2013) in order to investigate discrepances between the estimated cognitive effort and actual technical effort, however, to the best of our knowledge, the corpus is not publicly available. ¨ corpus (Avramidis et al., The publicly available TARA X U 2014) contains both post-edited as well as error annotated translation outputs, whereby the two tasks were carried out completely separately, and even not on the same set of translation outputs. The Terra corpus (Fishel et al., 2012) is a publicly available collection of manually error annotated corpora2 which has been used for assessment of the automatic"
L16-1005,2014.eamt-1.38,1,0.906445,"Missing"
L16-1005,J03-1002,0,\N,Missing
L16-1296,W15-5705,1,0.838417,"Missing"
L16-1296,P02-1040,0,0.134922,"Missing"
L18-1073,abdelali-etal-2014-amara,0,0.0313475,"has the main benefit of access to people speaking the different languages. 3. Data Selection Our aim was to collect wikification annotations for 500 to 1,000 sentences from parallel educational texts for each language pair. We used three existing parallel text resources as the basis for the sentence selection so as to cover a broad range of online courses and to cover all eleven language pairs. In particular, we use parallel texts from course material of the Coursera MOOC platform, the Iversity MOOC platform, and the QCRI Educational Domain (QED) Corpus (formerly known as QCRI AMARA Corpus) (Abdelali et al., 2014). The Iversity data consists of (i) manually translated MOOC data, and (ii) MT output of English MOOC data produced by the first MT software prototype that was developed in the first year of the TraMOOC project. Both Coursera and QED material consist of MOOC subtitles that were translated using crowdsourcing in other projects unrelated to TraMOOC. The QED corpus consists of a large collection of files and each file contains a number of subtitles from MOOC video lectures in a particular language. The aligned files (parallel corpus) share the same first part of the file name. However, not all fi"
L18-1073,S16-1081,0,0.0258532,"We use the data set for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit"
L18-1073,S17-2001,0,0.03374,"for an in-depth analysis and evaluation of machine translation output. In addition to word-based evaluations (e.g., BLEU), a semantic evaluation can be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evalua"
L18-1073,W08-0336,0,0.024088,"of speech (such as “mmm”, [NOISE], or [MUSIC]). Also, particularly long paragraphs were not selected, so as to maintain the microtasking nature of the activity. The selected source and target sentences were automatically tokenized using the multilingual tokenizer Ucto1 (van Gompel et al., 2017). Ucto has language-specific rules for the tokenization of several languages including Dutch, English, German, Italian, Portuguese, and Russian. For the other languages, generic language-independent settings of Ucto were used except for the Chinese language where we applied the Stanford Word Segmenter (Chang et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016"
L18-1073,D13-1184,0,0.0441826,"Missing"
L18-1073,N16-1104,0,0.0316436,"ng et al., 2008). 4. Annotation via Crowdsourcing Crowdsourcing has been used extensively for annotating corpora due to being a cheap and fast means to collecting human intelligence input, compared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task"
L18-1073,S15-2049,0,0.0331012,"e also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data set is intended as both tuning material for the developed implicit machine translation system evaluation tool and for testing the final machine translation systems. The data set also gives us insights into the coverage of"
L18-1073,P11-1138,0,0.042925,"2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those created for the evaluation of wikification tools (Mihalcea and Csomai, 2007). The Illinois Wikifier was evaluated on English material annotated with Wikipedia links (Ratinov et al., 2011). This evaluation set consists of Wikipedia pages and news articles. In the context of automatic word sense disambiguation, there is another related multilingual data set from Semeval-2015 task 13 (Moro and Navigli, 2015) containing links to BabelNet (Navigli and Ponzetto, 2012) and Wikipedia pages with news articles in three languages (of which only Italian matches the languages targeted in the TraMOOC project). These data sets do not cover all the languages we are interested in and, additionally, do not target the educational domain. Therefore, a new data set needed to be created. This data"
L18-1073,sabou-etal-2014-corpus,0,0.0251481,"ared to requesting expertbased intervention (Wang et al., 2013). Crowdsourcing approaches vary from voluntary work and gaming to paid microtasks (Bougrine et al., 2017). Applications involve, but are not limited to, the annotation of speech corpora (Bougrine et al., 2017; Su et al., 2017), of named entities (Bontcheva et al., 2017), of domain-specific concepts (Good et al., 2014) of relation extraction (Liu et al., 2016). Researchers have shown particular interest in issues pertaining to ethical implications (Cohen et al., 2016), as well as best practices for obtaining optimal quality output (Sabou et al., 2014). Best practice guidelines are followed in the present work also, after experimentation with varying 1 Ucto is freely available at languagemachines.github.io/ucto/. 468 https:// Figure 1: CrowdFlower interface for manual annotation. parameterization schemata, and involve task decomposition into simple microtasks, the appropriate crowd choice, the appropriate crowd reward choice, data preparation, task design, task completion time, quality control, task monitoring and crowd evaluation. Given pairs of aligned texts, the entity annotation (wikification) task consists of identifying and annotating"
L18-1073,N16-1072,0,0.0225831,"an be performed, as the links to the Wikipedia pages in the various target languages provide an additional source of information. Such implicit MT evaluation aims to judge the MT quality between source and target language without using an explicit (manual) translation step. Alternatively, the data set is suited for other multilingual tasks that focus on semantic aspects such as the cross-lingual Semantic Textual Similarity task (Agirre et al., 2016; Cer et al., 2017). Furthermore, the data set can be used as multilingual training material for the development of novel wikification tools (e.g., Tsai and Roth (2016)), or tools that automatically detect and link topics in a text to their respective Wikipedia pages. In the remainder of this paper we discuss related work in Section 2., the creation of this data set via crowdsourcing (Section 3.), the difficulties that we encountered using crowdsourcing for such complex annotation task (Section 4.) and the outcomes of this process in Section 5.. We also briefly discuss the use case of implicit translation evaluation for which we created the data set in Section 6. and conclude in Section 7.. 2. Related Work Comparable and related types of data sets are those"
popovic-ney-2004-towards,J93-2003,0,\N,Missing
popovic-ney-2004-towards,E03-1076,0,\N,Missing
popovic-ney-2004-towards,W01-1407,1,\N,Missing
popovic-ney-2004-towards,P02-1040,0,\N,Missing
popovic-ney-2004-towards,J01-2001,0,\N,Missing
popovic-ney-2004-towards,W02-0603,0,\N,Missing
popovic-ney-2004-towards,P02-1038,1,\N,Missing
popovic-ney-2006-pos,P02-1040,0,\N,Missing
popovic-ney-2006-pos,W05-0820,0,\N,Missing
popovic-ney-2006-pos,W05-0831,1,\N,Missing
popovic-ney-2006-pos,P05-1066,0,\N,Missing
popovic-ney-2006-pos,2005.iwslt-1.20,1,\N,Missing
popovic-ney-2006-pos,2001.mtsummit-papers.45,1,\N,Missing
R19-1107,W05-0909,0,0.0591163,"ompare German-to-English translation hypotheses generated by systems trained (i) only on authentic data, (ii) only on synthetic data, and (iii) on authentic data enhanced with different types of BT data: SMT, NMT. We exploit two types of synthetic and authentic data combinations: (a) randomly selected half of target sentences backtranslated by SMT and another half by NMT system, and (b) joining all BT data (thus repeating each target segment). The translation hypotheses are compared in terms of four automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015). These metrics give an overall estimate of the quality of the translations with respect to the reference (human translation of the test set). In addition, the translation hypotheses are analyzed in terms of five error categories, lexical variety and syntactic variety. 1. Models trained with 1M auth + 2M synth sentences using the default settings, including 13 training epochs. 2. Models trained on 1M auth data only, trained either: (a) using the default settings, including 13 training epochs. (b) Trained for 39 epochs, to obtain a same amount of training effort as for"
R19-1107,P17-4012,0,0.0677521,"of base. • NMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an NMT model (with the same configuration as described in Section 5 but in the English to German direction) trained with the base set. • hybrNMTSMT: Synthetic parallel corpus combining NMTsynth and SMTsynth sets. It has been built by maintaining the same target side of auth, and as source side we alternate between NMTsynth and SMTsynth each 500K sentences. Experimental Settings For the experiments we have built German-toEnglish NMT models using the Pytorch port of OpenNMT (Klein et al., 2017). We use the default parameters: 2-layer LSTM with 500 hidden units. The models are trained for the same number of epochs. As the model trained with all authentic data converges after 13 epochs, we use that many iterations to train the models (we use the same amount of epochs). As optimizer we use stochas• fullhybrNMTSMT: Synthetic parallel corpus combining all segments from NMTsynth and SMTsynth sets (double size, each original target sentence repeated twice with both an NMT and SMT back-translation-generated translation). 924 7 Experiments The results show that adding synthetic data has a po"
R19-1107,W04-3250,0,0.494404,". As optimizer we use stochas• fullhybrNMTSMT: Synthetic parallel corpus combining all segments from NMTsynth and SMTsynth sets (double size, each original target sentence repeated twice with both an NMT and SMT back-translation-generated translation). 924 7 Experiments The results show that adding synthetic data has a positive impact on the performance of the models as all of them achieve improvements when compared to that built only with authentic data 1M base. These improvements are statistically significant at p=0.01 (computed with multeval (Clark et al., 2011) using Bootstrap Resampling (Koehn, 2004)). However, the increases of quality are different depending on the approach followed to create the BT data. In our experiments, we build models on different portions of the datasets described in Section 6. First, we train an initial NMT model using the base data set. Then, in order to investigate how much the models benefit from using synthetic data generated by different approaches, we build models with increasing sizes of data (from the data sets described in Section 6). The models explored are built with data that ranges from 1M sentences (built with only authentic data from base data set)"
R19-1107,W18-6315,0,0.14104,"s (Poncelas et al., 2019). Edunov et al. (2018) confirmed that synthetic data can sometimes match the performance of authentic data. In addition, a comprehensive analysis of different methods to generate synthetic source sentences was carried out. This analysis revealed that sampling from the model distribution or noising beam outputs out-performs pure beam search, which is typically used in NMT. Their analysis shows that synthetic data based on sampling and noised beam search provides a stronger training signal than synthetic data based on argmax inference. One of the experiments reported in Burlot and Yvon (2018) is comparing performance between models trained with NMT and SMT BT data. The best Moses system (Koehn et al., 2007) is almost as good as the NMT system trained with the same (authentic) data, and much faster to train. Improvements obtained with the Moses system trained with a small training corpus are much smaller; this system even decreases the performance for the out-of-domain test. The authors also investigated some properties of BT data and found out that the back-translated sources are on average shorter than authentic ones, syntactically simpler than authentic ones, and contain smaller"
R19-1107,L18-1004,0,0.0339369,"Missing"
R19-1107,P03-1021,0,0.173985,"n their translation hypotheses. 5 6 Data The parallel data used for the experiments has been obtained from WMT 2015 (Bojar et al., 2015). We build two parallel sets with these sentences: base (1M sentences) and auth (3M sentences). We use the target side of auth to create the following datasets: • SMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an SMT model trained with base set in the English to German direction. It has been built using the Moses toolkit with default settings, using GIZA++ for word alignment and tuned using MERT (Och, 2003)). The language model (of order 8) is built with the KenLM toolkit (Heafield, 2011) using the German side of base. • NMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an NMT model (with the same configuration as described in Section 5 but in the English to German direction) trained with the base set. • hybrNMTSMT: Synthetic parallel corpus combining NMTsynth and SMTsynth sets. It has been built by maintaining the same target side of auth, and as source side we alternate between NMTsynth and SMTsynth each 500K sentences. Experimental"
R19-1107,P11-2031,0,0.0665491,"in the models (we use the same amount of epochs). As optimizer we use stochas• fullhybrNMTSMT: Synthetic parallel corpus combining all segments from NMTsynth and SMTsynth sets (double size, each original target sentence repeated twice with both an NMT and SMT back-translation-generated translation). 924 7 Experiments The results show that adding synthetic data has a positive impact on the performance of the models as all of them achieve improvements when compared to that built only with authentic data 1M base. These improvements are statistically significant at p=0.01 (computed with multeval (Clark et al., 2011) using Bootstrap Resampling (Koehn, 2004)). However, the increases of quality are different depending on the approach followed to create the BT data. In our experiments, we build models on different portions of the datasets described in Section 6. First, we train an initial NMT model using the base data set. Then, in order to investigate how much the models benefit from using synthetic data generated by different approaches, we build models with increasing sizes of data (from the data sets described in Section 6). The models explored are built with data that ranges from 1M sentences (built wit"
R19-1107,P02-1040,0,0.104339,"of our knowledge, this has not been investigated yet. We compare German-to-English translation hypotheses generated by systems trained (i) only on authentic data, (ii) only on synthetic data, and (iii) on authentic data enhanced with different types of BT data: SMT, NMT. We exploit two types of synthetic and authentic data combinations: (a) randomly selected half of target sentences backtranslated by SMT and another half by NMT system, and (b) joining all BT data (thus repeating each target segment). The translation hypotheses are compared in terms of four automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015). These metrics give an overall estimate of the quality of the translations with respect to the reference (human translation of the test set). In addition, the translation hypotheses are analyzed in terms of five error categories, lexical variety and syntactic variety. 1. Models trained with 1M auth + 2M synth sentences using the default settings, including 13 training epochs. 2. Models trained on 1M auth data only, trained either: (a) using the default settings, including 13 training epochs. (b) Trained fo"
R19-1107,D18-1045,0,0.0216509,"oint onwards. 923 tic gradient descent (SGD), in combination with learning rate decay, halving the learning rate starting from the 8th epoch. In order to build the models, all data sets are tokenized and truecased and segmented with BytePair Encoding (BPE) (Sennrich et al., 2016b) built on the joint vocabulary using 89500 merge operations. For testing the models we use the test set provided in the WMT 2015 News Translation Task (Bojar et al., 2015). As development set, we use 5K randomly sampled sentences from development sets provided in previous years of WMT. models (Poncelas et al., 2019). Edunov et al. (2018) confirmed that synthetic data can sometimes match the performance of authentic data. In addition, a comprehensive analysis of different methods to generate synthetic source sentences was carried out. This analysis revealed that sampling from the model distribution or noising beam outputs out-performs pure beam search, which is typically used in NMT. Their analysis shows that synthetic data based on sampling and noised beam search provides a stronger training signal than synthetic data based on argmax inference. One of the experiments reported in Burlot and Yvon (2018) is comparing performance"
R19-1107,W18-2703,0,0.0607276,"anguage sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmented with back-translated data created by merging different MT approaches. 1 The work presented in Poncelas et al. (2018) draws an early-stage empirical roadmap to investigating the effects of BT data. In particular, it looks at how the amount of BT data impacts the performance of the final NMT system. In Sennrich et al. (2016a) and Poncelas et al. (2018), the systems used to generate the BT data are neural. However, it has been noted that often different paradigms can contribute differently to a given task. For example, it has been shown that applying an APE system based on NMT technology improves statistical machine translation (SMT) output, but has lower impact on NMT output (Bojar et al., 2017; Chatterjee et"
R19-1107,W11-2123,0,0.0532852,"ments has been obtained from WMT 2015 (Bojar et al., 2015). We build two parallel sets with these sentences: base (1M sentences) and auth (3M sentences). We use the target side of auth to create the following datasets: • SMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an SMT model trained with base set in the English to German direction. It has been built using the Moses toolkit with default settings, using GIZA++ for word alignment and tuned using MERT (Och, 2003)). The language model (of order 8) is built with the KenLM toolkit (Heafield, 2011) using the German side of base. • NMTsynth: Created by translating the targetside sentences of auth. The model used to generate the sentences is an NMT model (with the same configuration as described in Section 5 but in the English to German direction) trained with the base set. • hybrNMTSMT: Synthetic parallel corpus combining NMTsynth and SMTsynth sets. It has been built by maintaining the same target side of auth, and as source side we alternate between NMTsynth and SMTsynth each 500K sentences. Experimental Settings For the experiments we have built German-toEnglish NMT models using the Py"
R19-1107,W16-2378,0,0.038863,"Missing"
R19-1107,W15-3049,1,0.923108,"Missing"
R19-1107,2011.eamt-1.36,1,0.735645,"Missing"
R19-1107,P16-1009,0,0.670634,"APE) (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, the effects of various parameters for creating back-translated (BT) data have not been investigated enough as to indicate what are the optimal conditions in not only creating but also employing such data to train high-quality neural machine translation (NMT) systems. Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation (Sennrich et al., 2016a), which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmen"
R19-1107,P16-1162,0,0.838573,"APE) (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018). However, the effects of various parameters for creating back-translated (BT) data have not been investigated enough as to indicate what are the optimal conditions in not only creating but also employing such data to train high-quality neural machine translation (NMT) systems. Neural Machine Translation (NMT) models achieve their best performance when large sets of parallel data are used for training. Consequently, techniques for augmenting the training set have become popular recently. One of these methods is back-translation (Sennrich et al., 2016a), which consists on generating synthetic sentences by translating a set of monolingual, target-language sentences using a Machine Translation (MT) model. Generally, NMT models are used for backtranslation. In this work, we analyze the performance of models when the training data is extended with synthetic data using different MT approaches. In particular we investigate back-translated data generated not only by NMT but also by Statistical Machine Translation (SMT) models and combinations of both. The results reveal that the models achieve the best performances when the training set is augmen"
R19-1107,2006.amta-papers.25,0,0.0605592,"ot been investigated yet. We compare German-to-English translation hypotheses generated by systems trained (i) only on authentic data, (ii) only on synthetic data, and (iii) on authentic data enhanced with different types of BT data: SMT, NMT. We exploit two types of synthetic and authentic data combinations: (a) randomly selected half of target sentences backtranslated by SMT and another half by NMT system, and (b) joining all BT data (thus repeating each target segment). The translation hypotheses are compared in terms of four automatic evaluation metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005) and CHRF (Popovi´c, 2015). These metrics give an overall estimate of the quality of the translations with respect to the reference (human translation of the test set). In addition, the translation hypotheses are analyzed in terms of five error categories, lexical variety and syntactic variety. 1. Models trained with 1M auth + 2M synth sentences using the default settings, including 13 training epochs. 2. Models trained on 1M auth data only, trained either: (a) using the default settings, including 13 training epochs. (b) Trained for 39 epochs, to obtain a sa"
R19-1107,W17-4717,0,\N,Missing
R19-1111,W15-4913,1,0.902651,"Missing"
R19-1111,W17-4702,0,0.0189174,"analysis. To the best of our knowledge, our work represents the first experiments related to ambiguous conjunctions and machine translation. We report the results of an extensive evaluation showing that certain conjunction ambiguities pose a challenge to the state-of-the-art machine translation systems. Table 2: Distribution of sentences requiring each of the two target language variants of the source conjunction “but” in different publicly available parallel corpora. 2 Related Work Lexical ambiguity as a challenge for machine translation has received a lot of attention in recent years. Rios Gonzales et al. (2017) and Rios Gonzales et al. (2018) focus on ambiguous German nouns, while Guillou et al. (2018) and M¨uller et al. (2018) investigate ambiguous English pronouns. Broader linguistic evaluations presented in Burchardt et al. (2017) and Klubiˇcka et al. (2018) also include ambiguity, but conjunctions are not mentioned in any context. Syntactic ambiguity for rule-based Englishto-Bulgarian machine translation is investigated in Pericliev (1984), but these ambiguities are not related to conjunctions. Ambiguity of conjunctions “and” and “or” is investigated for requirement specifications in Sharma et a"
R19-1111,W18-6437,0,0.0182948,"owledge, our work represents the first experiments related to ambiguous conjunctions and machine translation. We report the results of an extensive evaluation showing that certain conjunction ambiguities pose a challenge to the state-of-the-art machine translation systems. Table 2: Distribution of sentences requiring each of the two target language variants of the source conjunction “but” in different publicly available parallel corpora. 2 Related Work Lexical ambiguity as a challenge for machine translation has received a lot of attention in recent years. Rios Gonzales et al. (2017) and Rios Gonzales et al. (2018) focus on ambiguous German nouns, while Guillou et al. (2018) and M¨uller et al. (2018) investigate ambiguous English pronouns. Broader linguistic evaluations presented in Burchardt et al. (2017) and Klubiˇcka et al. (2018) also include ambiguity, but conjunctions are not mentioned in any context. Syntactic ambiguity for rule-based Englishto-Bulgarian machine translation is investigated in Pericliev (1984), but these ambiguities are not related to conjunctions. Ambiguity of conjunctions “and” and “or” is investigated for requirement specifications in Sharma et al. (2014) and for legal texts in"
R19-1111,W18-6435,0,0.0258475,"ambiguous conjunctions and machine translation. We report the results of an extensive evaluation showing that certain conjunction ambiguities pose a challenge to the state-of-the-art machine translation systems. Table 2: Distribution of sentences requiring each of the two target language variants of the source conjunction “but” in different publicly available parallel corpora. 2 Related Work Lexical ambiguity as a challenge for machine translation has received a lot of attention in recent years. Rios Gonzales et al. (2017) and Rios Gonzales et al. (2018) focus on ambiguous German nouns, while Guillou et al. (2018) and M¨uller et al. (2018) investigate ambiguous English pronouns. Broader linguistic evaluations presented in Burchardt et al. (2017) and Klubiˇcka et al. (2018) also include ambiguity, but conjunctions are not mentioned in any context. Syntactic ambiguity for rule-based Englishto-Bulgarian machine translation is investigated in Pericliev (1984), but these ambiguities are not related to conjunctions. Ambiguity of conjunctions “and” and “or” is investigated for requirement specifications in Sharma et al. (2014) and for legal texts in Adams and Kaye (2006), however without any relation to (eith"
R19-1111,tiedemann-2012-parallel,0,0.0158253,"with the first variant, but1 , can be found more frequently in the data. Table 2 presents the distribution of the two types of sentences with the conjunction ”but” found in publicly available data for several language pairs in two domains: news and subtitles. 3.2 4 Experimental Set-Up 4.1 Test Sets In order to estimate a system’s capability to translate ambiguous conjunctions, evaluation is performed on specialised test sets specifically designed for the conjunctions “but” and “and” and their two variants. The test sets are created semi-automatically using the multilingual subtitles corpora2 (Tiedemann, 2012). Only short segments (up to 20 words) were included, all noise was removed, and rare named entities which could introduce additional effects were avoided or replaced. Thus, about 1000 source sentences in English and in French were prepared for the conjunction “but”, and 250 source sentences in English and in Portuguese for conjunction “and”. Detailed corpus statistics are presented in Table 5. It should be noted that although the test sets were created using a bilingual corpus, the resulting test sets do not contain any reference translations. The reason for this is twofold: on the one hand,"
R19-1111,E83-1013,0,0.688726,"to go to the hotel, but to the beach. Wir wollten nicht zum Hotel, sondern zum Strand. No quer´ıamos ir al hotel, sino a la playa. Nismo hteli da idemo u hotel nego na plaˇzu. She will not come but call. No va a venir, sino a llamar. Sie kommt nicht sondern ruft an. Ne´ce do´ci nego c´ e zvati. Table 1: Examples of difference between the two variants of the English conjunction “but”. domain News Subtitles lang. En-De En-Sr En-Hr En-De Fr-De En-Sr but1 65.2 79.7 78.3 97.2 96.6 97.1 but2 34.8 20.3 21.7 2.8 3.4 2.9 The first work dealing with conjunctions and machine translation is described in Huang (1983). It explores conjunction scope for English parser to be used in rule-based MT systems, but it does not address the ambiguity. Another work related to conjunctions and machine translation is the work of Xu et al. (2014), who proposes using conjunctions for Chinese sentence segmentation in order to achieve better translation quality. Some problems with translating conjunctions by phrase-based machine translation systems involving South Slavic languages are mentioned in Popovi´c and Arˇcan (2015), but without any systematic quantitative analysis. To the best of our knowledge, our work represents"
R19-1111,W17-3204,0,0.0359087,"Missing"
R19-1111,W18-6307,0,0.0518959,"Missing"
R19-1111,P84-1111,0,0.574361,"vailable parallel corpora. 2 Related Work Lexical ambiguity as a challenge for machine translation has received a lot of attention in recent years. Rios Gonzales et al. (2017) and Rios Gonzales et al. (2018) focus on ambiguous German nouns, while Guillou et al. (2018) and M¨uller et al. (2018) investigate ambiguous English pronouns. Broader linguistic evaluations presented in Burchardt et al. (2017) and Klubiˇcka et al. (2018) also include ambiguity, but conjunctions are not mentioned in any context. Syntactic ambiguity for rule-based Englishto-Bulgarian machine translation is investigated in Pericliev (1984), but these ambiguities are not related to conjunctions. Ambiguity of conjunctions “and” and “or” is investigated for requirement specifications in Sharma et al. (2014) and for legal texts in Adams and Kaye (2006), however without any relation to (either human or machine) translation. 3 3.1 Ambiguity of “But” and “And” Conjunction ”But” In some languages, there are two possible variants of the conjunction “but”. One variant, but1 , can be used after either a positive or a negative clause. The other variant, but2 , is used after a negative clause when expressing a contradiction. The first claus"
R19-1131,2013.mtsummit-posters.8,0,0.0195825,"the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article, etc.) in order to preserve grammaticality and the original meaning on the source"
R19-1131,W10-1607,0,0.0550848,"Missing"
R19-1131,P13-4015,0,0.0215202,"the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article, etc.) in order to preserve grammaticality and the original meaning on the source"
R19-1131,2010.eamt-1.31,0,0.0104101,", and with minimal manual correction of the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article, etc.) in order to preserve grammaticalit"
R19-1131,N06-1003,0,0.0867505,"ng for its grammaticality and meaning preservation, and with minimal manual correction of the output), the fluency of the translation can be improved. With regards to improving translation adequacy and post-editting effort, our experiments show that the type of the translation architecture (PBMT or NMT), and the strategy for using ATS system, both play a significant role. 2 Related Work For many language pairs (e.g. English-French, English-Spanish, English-Hindu), attempts were made at rewriting input sentences using paraphrasing or textual entailment to improve the performance of MT systems (Callison-Burch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015; Mirkin et al., 2013a,b). However, they all focus only on out-of-vocabulary words, or difficult to translate shorter n-grams. ˇ Stajner and Popovi´c (2016) went one step further, using lexico-syntactic automatic text simplification systems as a pre-processing step for English-to-Serbian machine translation. In this way, they covered both lexical and syntactic transformations on the source side. The ATS outputs were manually inspected by human editors who were also allowed to do minor revisions (correcting the tense, gender, article,"
R19-1131,C96-2183,0,0.808785,"s, a simpler variant is the one that requires a shorter reading time and leads to better text comprehension scores. In the case of text or sentence simplification used as a preprocessing step for a given natural language processing (NLP) task, e.g. machine translation (MT), information extraction (IE), summarization, and semantic role labeling (SRL), a simpler variant is the one that leads to better performances of that NLP system. Text simplification was originally proposed as a pre-processing step for machine translation (Chandrasekar, 1994) and later for information extraction and parsing (Chandrasekar et al., 1996). At those early stages, automated text simplification (ATS) was not mature enough to help improving performances of those systems. Instead, the idea was explored only hypothetically, using manual text simplification (Chandrasekar, 1994; Vickrey and Koller, 2008). Evans (2011) later showed that an automated simplification of coordinate structures can improve IE systems. Later, the focus of the ATS shifted towards text accessibility and better social inclusion, having the main goal of making texts easier to understand by 1141 Proceedings of Recent Advances in Natural Language Processing, pages"
R19-1131,P11-2117,0,0.177598,"n of the ATS output (see Section 3). We find that success of the ATS used as a pre-processing step heavily depends on the type of the MT system used (PBMT or NMT). 3 Experimental Setup We randomly selected 10 original articles from the 100 news articles automatically simplified by the state-of-the-art lexico-syntactic ATS system (Siddharthan and Angrosh, 2014) in the work of ˇ Stajner and Glavaˇs (2017). The ATS system that consists of a rule-based syntactic simplification module and a supervised lexical simplification module built upon the English Wikipedia - Simple English Wikipedia corpus (Coster and Kauchak, 2011). We further explored three possible scenarios in which ATS can be used as a pre-processing step for MT (Figure 1): 1142 • Scenario 1 (Corrected): Automatically simplified sentences are manually corrected before being used as the source sentences for MT, to ensure the preservation of the original meaning and the grammaticality of the MT input; • Scenario 2 (Filtered): Automatically simplioriginal texts STEP 1 automated text simplification automatically simplified sentences Scenario 1 STEP 2.1 Scenario 2 correcting (post editing) sentences with low G and M STEP 3 STEP 2.2 Scenario 3 filter sent"
R19-1131,W15-4913,1,0.861372,"Missing"
R19-1131,E14-1076,0,0.0262015,"rbian translation, we also use the current stateof-the-art neural MT system for that language pair (see Section 3.1). We explore three different scenarios for using ATS as the pre-processing step, in search for fully automatic use of ATS in MT, without human correction of the ATS output (see Section 3). We find that success of the ATS used as a pre-processing step heavily depends on the type of the MT system used (PBMT or NMT). 3 Experimental Setup We randomly selected 10 original articles from the 100 news articles automatically simplified by the state-of-the-art lexico-syntactic ATS system (Siddharthan and Angrosh, 2014) in the work of ˇ Stajner and Glavaˇs (2017). The ATS system that consists of a rule-based syntactic simplification module and a supervised lexical simplification module built upon the English Wikipedia - Simple English Wikipedia corpus (Coster and Kauchak, 2011). We further explored three possible scenarios in which ATS can be used as a pre-processing step for MT (Figure 1): 1142 • Scenario 1 (Corrected): Automatically simplified sentences are manually corrected before being used as the source sentences for MT, to ensure the preservation of the original meaning and the grammaticality of the M"
R19-1131,P08-1040,0,0.0367483,"tion (MT), information extraction (IE), summarization, and semantic role labeling (SRL), a simpler variant is the one that leads to better performances of that NLP system. Text simplification was originally proposed as a pre-processing step for machine translation (Chandrasekar, 1994) and later for information extraction and parsing (Chandrasekar et al., 1996). At those early stages, automated text simplification (ATS) was not mature enough to help improving performances of those systems. Instead, the idea was explored only hypothetically, using manual text simplification (Chandrasekar, 1994; Vickrey and Koller, 2008). Evans (2011) later showed that an automated simplification of coordinate structures can improve IE systems. Later, the focus of the ATS shifted towards text accessibility and better social inclusion, having the main goal of making texts easier to understand by 1141 Proceedings of Recent Advances in Natural Language Processing, pages 1141–1150, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_131 various target readers, e.g. people with low literacy levels (Alu´ısio and Gasperin, 2010), or people with some kind of reading or cognitive impairments, such as aphasia (De"
R19-1131,W16-3411,1,0.913909,"Missing"
W04-3235,E85-1023,0,0.118461,"Other prominent examples are speech recognition and machine translation. The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules. From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and"
W04-3235,J96-1002,0,0.0539595,"ed probability distribution p(g1N |w1N ): P r(g1N |w1N ) → p(g1N |w1N ) and apply the chain rule: p(g1N |w1N ) = = N Y n=1 N Y p(gn |g1n−1 , w1N ) n−1 n+2 p(gn |gn−2 , wn−2 ) n=1 As for the generative model, we have made specific assumptions: There is a second-order dependence for the tags g1n , and the dependence on the words n+2 w1N is limited to a window wn−2 around position n. The resulting model is still rather complex and requires further specifications. The typical procedure is to resort to log-linear modelling, which is also referred to as maximum entropy modelling (Ratnaparkhi, 1996; Berger et al., 1996). 3.2.1 String Error For the minimum string error, we obtain the decision rule: w1N → gˆ1N n o = arg max p(g1N |w1N ) g1N Since this is still a second-order model, we can use dynamic programming to compute the most likely POS string. 3.2.2 Symbol Error For the minimum symbol error, the marginal (and posterior) probability pm (g|w1N ) has to be computed: pm (g|w1N ) = X P r(g1N |w1N ) X Y 5 Experimental Results g1N : gm =g = n−1 n+2 p(gn |gn−2 , wn−2 ) g1N : gm =g n which, due to the specific structure of the model n−1 n+2 p(gn |gn−2 , wn−2 ), can be calculated efficiently using only a forward"
W04-3235,J94-2001,0,0.330196,"ate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistical approach are not addressed. Part of this work goes back to (Bahl et al., 1974) who considered a problem in coding theory. (Goel and Byrne, 2003): The error measure considered by the authors is the word error rate in speech recognition, i.e. the edit distance. Due to the mathematical complexity of this error measure, th"
W04-3235,W96-0213,0,0.431369,"1N ) by a model-based probability distribution p(g1N |w1N ): P r(g1N |w1N ) → p(g1N |w1N ) and apply the chain rule: p(g1N |w1N ) = = N Y n=1 N Y p(gn |g1n−1 , w1N ) n−1 n+2 p(gn |gn−2 , wn−2 ) n=1 As for the generative model, we have made specific assumptions: There is a second-order dependence for the tags g1n , and the dependence on the words n+2 w1N is limited to a window wn−2 around position n. The resulting model is still rather complex and requires further specifications. The typical procedure is to resort to log-linear modelling, which is also referred to as maximum entropy modelling (Ratnaparkhi, 1996; Berger et al., 1996). 3.2.1 String Error For the minimum string error, we obtain the decision rule: w1N → gˆ1N n o = arg max p(g1N |w1N ) g1N Since this is still a second-order model, we can use dynamic programming to compute the most likely POS string. 3.2.2 Symbol Error For the minimum symbol error, the marginal (and posterior) probability pm (g|w1N ) has to be computed: pm (g|w1N ) = X P r(g1N |w1N ) X Y 5 Experimental Results g1N : gm =g = n−1 n+2 p(gn |gn−2 , wn−2 ) g1N : gm =g n which, due to the specific structure of the model n−1 n+2 p(gn |gn−2 , wn−2 ), can be calculated efficiently"
W04-3235,J88-1003,0,\N,Missing
W05-0806,J93-2003,0,0.0124716,"Missing"
W05-0806,W01-1407,1,0.882366,"Missing"
W05-0806,J04-2003,1,0.83514,"t language. Usually, the performance of a translation system strongly depends on the size of the available training corpus. However, acquisition of a large high-quality bilingual parallel text for the desired domain and language pair requires lot of time and effort, and, for many language pairs, is even not possible. Besides, small corpora have certain advantages - the acquisition does not require too much effort and also manual creation and correction are possible. Therefore there is an increasing number of publications dealing with limited amounts of bilingual data (Al-Onaizan et al., 2000; Nießen and Ney, 2004). In this work, we examine the quality of several statistical machine translation systems constructed on a small amount of parallel Serbian-English text. The main bilingual parallel corpus consists of about 3k sentences and 20k running words from an unrestricted domain. The translation systems are built on the full corpus as well as on a reduced corpus containing only 200 parallel sentences. A small set of about 350 short phrases from the web is used as additional bilingual knowledge. In addition, we investigate the use of monolingual morpho-syntactic knowledge i.e. base forms and POS tags. 1"
W05-0806,J03-1002,1,0.0207503,"is to translate a source language sequence f1 , . . . , fJ into a target language sequence e1 , . . . , eI by maximising the conditional probability P r(eI1 |f1J ). This probability can be factorised into the translation model probability P (f1J |eI1 ) which describes the correspondence between the words in the source and the target sequence, and the language model probability P (eJ1 ) which describes well-formedness of the produced target sequence. These two probabilities can be modelled independently of each other. For detailed descriptions of SMT models see for example (Brown et al., 1993; Och and Ney, 2003). Translation probabilities are learnt from a bilingual parallel text corpus and language model probabilities are learnt from a monolingual text in the tarFor the Serbian language, as a rather minor and not widely studied language, there are not many language resources available, especially not parallel texts. On the other side, investigations on this language may be quite useful since the majority of principles can be extended to the wider group of Slavic languages (e.g. Czech, Polish, Russian, etc.). In this work, we exploit small Serbian-English parallel texts as a bilingual knowledge sourc"
W05-0806,P02-1040,0,0.078321,"timise the scaling factors, results obtained for this set do not differ from those for the test set. Therefore only the joint error rates (Development+Test) are reported. As for the external test set, results for this text are reported only for the full corpus systems, since for the reduced corpus the error rates are higher but the effects of using phrases and morpho-syntactic information are basically the same. 4.2 Translation Results The evaluation metrics used in our experiments are WER (Word Error Rate), PER (Positionindependent word Error Rate) and BLEU (BiLingual Evaluation Understudy) (Papineni et al., 2002). Since BLEU is an accuracy measure, we use 1BLEU as an error measure. 45 4.2.1 Translation from Serbian into English Error rates for the translation from Serbian into English are shown in Table 3 and some examples are shown in Table 6. It can be seen that there is a significant decrease in all error rates when the full forms are replaced with their base forms. Since the redundant information contained in the inflection is removed, the system can better capture the relevant information and is capable of producing correct or approximatively correct translations even for unseen full forms of the"
W05-0806,popovic-ney-2004-towards,1,0.895064,"Missing"
W05-0806,J82-2005,0,0.563994,"Missing"
W05-0806,W96-0213,0,0.0102235,". The morpho-syntactic annotation of the English part of the corpus has been done by the constraint grammar parser ENGCG for morphological and syntactic analysis of English language. For each word, this tool provides its base form and sequence of morpho-syntactic tags. For the Serbian corpus, to our knowlegde there is no available tool for automatic annotation of this language. Therefore, the base forms have been introduced manually and the POS tags have been provided partly manually and partly automatically using a statistical maximum-entropy based POS tagger similar to the one described in (Ratnaparkhi, 1996). First, the 200 sentences of the reduced training corpus have been annotated completely manually. Then the first 500 sentences of the rest of the training corpus have been tagged automatically and the errors have been manually corrected. Afterwards, the POS tagger has been trained on the extended corpus (700 sentences), the next 500 sentences of the rest are annotated, and the procedure has been repeated until the annotation has been finished for the complete corpus. Table 1: Statistics of the Serbian-English Assimil corpus Serbian English Training: original base forms original no article ful"
W06-3101,P04-1079,0,0.0149782,"sibilities for improvements. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al."
W06-3101,W05-0909,0,0.0366343,"lation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nie"
W06-3101,N04-4015,0,0.00900173,"has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the tran"
W06-3101,2005.iwslt-1.19,1,0.729357,"s. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed"
W06-3101,C00-2162,1,0.81769,"Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of"
W06-3101,2001.mtsummit-papers.45,1,0.834325,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,W01-1407,1,0.840883,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,niessen-etal-2000-evaluation,1,0.397729,"nt issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our kno"
W06-3101,P02-1040,0,0.115841,"t`ecnica de Catalunya (UPC), Barcelona, Spain ⊥ ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy {popovic,ney}@informatik.rwth-aachen.de {gupta,federico}@itc.it {agispert,canton}@gps.tsc.upc.es {lambert,banchs}@gps.tsc.upc.es Abstract A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for au"
W06-3101,popovic-ney-2004-towards,1,0.340303,"Missing"
W06-3101,popovic-ney-2006-pos,1,0.738548,"Missing"
W06-3101,2005.mtsummit-papers.34,1,0.721067,"173 0.15 0.09 2.7 1.7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full base"
W06-3101,vilar-etal-2006-error,1,0.542481,"Missing"
W06-3101,2005.iwslt-1.20,1,0.479674,"7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full baseline reorder 13k bas"
W06-3101,H05-1085,0,\N,Missing
W07-0707,P04-1079,0,0.0242721,"ork Automatic evaluation measures for machine translation output are receiving more and more attention in the last years. The B LEU metric (Papineni et al., 2002) and the closely related N IST metric (Doddington, 2002) along with W ER and P ER 48 Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, c Prague, June 2007. 2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of B LEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures W ER, P ER, B LEU and N IST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric M ETEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a se"
W07-0707,W05-0909,0,0.0510415,"n extended version of B LEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures W ER, P ER, B LEU and N IST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric M ETEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The T ER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference. The CD ER measure (Leusch et al., 2006) is based on edit distance, such as the well-known W ER, but allows reordering of blocks. Nevertheless, none of these measures or extensions takes into account linguist"
W07-0707,carreras-etal-2004-freeling,0,0.0157882,"Missing"
W07-0707,W05-0903,1,0.913645,"sures for machine translation output are receiving more and more attention in the last years. The B LEU metric (Papineni et al., 2002) and the closely related N IST metric (Doddington, 2002) along with W ER and P ER 48 Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, c Prague, June 2007. 2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of B LEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures W ER, P ER, B LEU and N IST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric M ETEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched wo"
W07-0707,E06-1031,1,0.699498,"n in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric M ETEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The T ER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference. The CD ER measure (Leusch et al., 2006) is based on edit distance, such as the well-known W ER, but allows reordering of blocks. Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. However, human error analysis, like any human evaluation, is a time consuming task. Whereas"
W07-0707,2005.iwslt-1.19,1,0.644817,"h W ER and P ER 48 Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, c Prague, June 2007. 2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of B LEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures W ER, P ER, B LEU and N IST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric M ETEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The T ER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it"
W07-0707,C00-2162,1,0.772826,"ual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. However, human error analysis, like any human evaluation, is a time consuming task. Whereas the use of linguistic knowledge for improving the performance of a statistical machine translation system is investigated in many publications for various language pairs (like for example (Nießen and Ney, 2000), (Goldwater and McClosky, 2005)), its use for the analysis of translation errors is still a rather unexplored area. Some automatic methods for error analysis using base forms and P OS tags are proposed in (Popovi´c et al., 2006; Popovi´c and Ney, 2006). These measures are based on differences between W ER and P ER which are calculated separately for each P OS class using subsets extracted from the original texts. Standard overall W ER and P ER of the original texts are not at all 49 taken into account. In this work, the standard W ER and P ER are decomposed and analysed. 3 Decomposition of W"
W07-0707,P02-1040,0,0.079304,"is. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system. 1 Introduction Evaluation of machine translation output is a very important but difficult task. Human evaluation is expensive and time consuming. Therefore a variety of automatic evaluation measures have been studied over the last years. The most widely used are Word Error Rate (W ER), Position independent word Error Rate (P ER), the B LEU score (Papineni et al., 2002) and the N IST score (Doddington, 2002). These measures have shown to be valuable tools for comparing Hermann Ney Lehrstuhl f¨ur Informatik 6 RWTH Aachen University Aachen, Germany ney@cs.rwth-aachen.de different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some more detailed analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. A framework for human error analysis has been proposed in (Vilar et al., 2006), but as"
W07-0707,W06-3101,1,0.773222,"Missing"
W07-0707,2006.amta-papers.25,0,0.0313844,"ation using the standard measures W ER, P ER, B LEU and N IST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric M ETEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The T ER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference. The CD ER measure (Leusch et al., 2006) is based on edit distance, such as the well-known W ER, but allows reordering of blocks. Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar"
W07-0707,2005.mtsummit-papers.34,1,0.692141,"od can be easily extended to other types of linguistic information. In addition, two methods for error analysis using the W ER and P ER decompositons together with base forms are proposed: estimation of inflectional errors and distribution of missing words over P OS classes. The translation corpus used for our error analysis is built in the framework of the T C -S TAR project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (E PPS) in Spanish and English. The translation system used is the phrase-based statistical machine translation system described in (Vilar et al., 2005; Matusov et al., 2006). 2 Related Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years. The B LEU metric (Papineni et al., 2002) and the closely related N IST metric (Doddington, 2002) along with W ER and P ER 48 Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, c Prague, June 2007. 2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of B LEU which uses n-grams weighted according to their frequency estimated from a"
W07-0707,vilar-etal-2006-error,1,0.707062,"Missing"
W07-0707,H05-1085,0,\N,Missing
W09-0402,W07-0718,0,0.0707219,"Missing"
W09-0402,W08-0309,0,0.0494742,"METEOR scores. 2008 BLEU Experiments on 2008 test data M BLEU For the official shared evaluation task in 2008, the human evaluation scores were different – the adequacy and fluency scores were abandoned being rather time consuming and often inconsistent, and the sentence ranking was proposed as one of the human evaluation scores: the manual evaluators were asked to rank translated sentences relative to each other. RWTH participated in this shared task with the two most promising metrics according to the previous experiments, i.e. POS B LEU and POS F, and the detailed results can be found in (Callison-Burch et al., 2008). It was shown that these metrics also correlate very well with the sentence ranking on the document level. However, on the sentence level the performance was much weaker: a percentage of sentence pairs for which the human comparison yields the same result as the comparison using particular automatic metric was not very high. We believe that the main reason for this is the fact that the metrics based only on the POS tags can assign high scores to translations without correct semantic meaning, because they are taking into account only syntactic structure without taking into account the actual w"
W09-0402,carreras-etal-2004-freeling,0,0.0153625,"Missing"
W09-0402,W06-3114,0,0.0935888,"Missing"
W09-0402,P02-1040,0,0.0933923,"Missing"
W09-0402,A00-1031,0,\N,Missing
W09-0410,E06-1005,1,0.803237,"st participles, but there are many cases when other verb forms also occur at the clause end. For the translation from German into English, following verb types were moved towards the beginning of a clause: infinitives, infinitives+zu, finite verbs, past participles and negative particles. For the translation from English to German, infinitives and past participles were moved to the end of a clause, where punctuation marks, subordinate conjunctions and finite verbs are considered as the beginning of the next clause. 4 System combination For system combination we used the approach described in (Matusov et al., 2006). The method is based on the generation of a consensus translation out of the output of different translation systems. The core of the method consists in building a confusion network for each sentence by aligning and combining the (single-best) translation hypothesis from one MT system with the translations produced by the other MT systems (and the other translations from the same system, if n-best lists are used in combination). For each sentence, each MT system is selected once as “primary” system, and the other hypotheses are aligned to this hypothesis. The resulting confusion networks are"
W09-0410,popovic-ney-2006-pos,1,0.894671,"Missing"
W09-0410,N07-1029,0,0.0191149,"s. The core of the method consists in building a confusion network for each sentence by aligning and combining the (single-best) translation hypothesis from one MT system with the translations produced by the other MT systems (and the other translations from the same system, if n-best lists are used in combination). For each sentence, each MT system is selected once as “primary” system, and the other hypotheses are aligned to this hypothesis. The resulting confusion networks are combined into a signle word graph, which is then weighted with system-specific factors, similar to the approach of (Rosti et al., 2007), and a trigram LM trained on the MT hypotheses. The translation with the best total score within this word graph is selected as consensus translation. The scaling factors of these models are optimized using the Condor toolkit (Berghen and Bersini, 2005) to achieve optimal B LEU score on the dev set. 5 5.1 Experimental results Experimental settings For all translation directions, we used the provided EuroParl and News parallel corpora to train the translation models and the News monolingual corpora to train the language models. All systems were optimised for the B LEU score on the development"
W09-0410,2008.iwslt-papers.7,1,0.894714,"Missing"
W09-0410,A00-1031,0,0.0616954,"Missing"
W09-0410,carreras-etal-2004-freeling,0,0.0349781,"Missing"
W09-0410,2005.iwslt-1.18,1,\N,Missing
W09-0410,W99-0604,1,\N,Missing
W09-0410,C04-1006,1,\N,Missing
W09-0410,E03-1076,0,\N,Missing
W09-0410,W07-0813,0,\N,Missing
W09-0410,C08-1128,1,\N,Missing
W09-0410,J90-2002,0,\N,Missing
W09-0410,P05-1071,0,\N,Missing
W09-0410,P02-1040,0,\N,Missing
W09-0410,W03-1709,0,\N,Missing
W09-0410,W06-3111,1,\N,Missing
W09-0410,J04-2004,0,\N,Missing
W09-0410,W07-0401,1,\N,Missing
W09-0410,J06-4004,0,\N,Missing
W09-0410,W05-0831,1,\N,Missing
W09-0410,J03-1002,1,\N,Missing
W09-0410,P06-1001,0,\N,Missing
W09-0410,takezawa-etal-2002-toward,0,\N,Missing
W09-0410,2005.eamt-1.37,1,\N,Missing
W09-0410,2006.iwslt-papers.1,1,\N,Missing
W09-0410,2007.iwslt-1.25,1,\N,Missing
W09-0410,2004.iwslt-evaluation.13,1,\N,Missing
W09-0410,2007.iwslt-1.3,1,\N,Missing
W09-0410,J07-2003,0,\N,Missing
W09-0410,2006.iwslt-evaluation.15,1,\N,Missing
W09-0410,P03-1021,0,\N,Missing
W09-0410,2007.iwslt-1.10,0,\N,Missing
W11-2104,C04-1046,0,0.80923,"of every given sentence. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training"
W11-2104,W08-0309,0,0.121698,"ch metrics have been known as Confidence Estimation metrics and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that perfo"
W11-2104,W10-1703,0,0.363914,"s and quite a few projects have suggested solutions on this direction. With our submission to the Shared Task, we allow such a metric to be systematically compared with the state-of-the-art reference-aware MT metrics. Our approach suggests building a Confidence Estimation metric using already existing human judgments. This has been motivated by the existence of human-annotated data containing comparisons of the outputs of several systems, as a result of the evaluation tasks run by the Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010). This amount of data, which has been freely available for further research, gives an opportunity for applying machine learning techniques to model the human annotators’ choices. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al. (2010)). Our proposition is similar, but works without reference translations. We develop a solution of applying machine learning in order to build a statistical classifier that performs similar to the human ranking: it is trained to rank sev"
W11-2104,W09-3712,0,0.0156559,"ng, the classifiers were used to perform ranking on a test set of 184 sentences which had been kept apart from the 2010 data, with the criterion that they do not contain contradictions among human judgments. In order to allow further comparison with other evaluation metrics, we performed an extended experiment: we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to per"
W11-2104,N07-1051,0,0.0080242,": we trained the classifiers over the WMT 2008 and 2009 data and let them perform automatic ranking on the full WMT 2010 test set, this time without any restriction on human evaluation agreement. In both experiments, tokenization was performed with the PUNKT tokenizer (Kiss et al., 2006; Garrette and Klein, 2009), while n-gram features were generated with the SRILM toolkit (Stolcke, 2002). The language model was relatively big and had been built upon all lowercased monolingual training sets for the WMT 2011 Shared Task, interpolated on the 2007 test set. As a PCFG parser, the Berkeley Parser (Petrov and Klein, 2007) was preferred, due 1 data acquired from http://www.statmt.org/wmt11 67 Feature selection Although the automatic NLP tools provided a lot of features (section 2.3), the classification methods we used (and particularly naïve Bayes were the development was focused on) would be expected to perform better given a smaller group of statistically independent features. Since exhaustive training/testing of all possible feature subsets was not possible, we performed feature selection based on the Relieff method (Kononenko, 1994; Kira and Rendell, 1992). Automatic ranking was performed based on the most"
W11-2104,P06-1055,0,0.00745077,"e pairwise comparison of system outputs ti and tj with respective ranks ri and rj , determined as:  1 ri &lt; rj c(ri , rj ) = −1 ri &gt; rj At testing time, after the classifier has made all the pairwise decisions, those need to be converted back to ranks. System entries are ordered, according to how many times each of them won in the pairwise comparison, leading to rank lists similar to the ones provided by human annotators. Note that this kind of decomposition allows for ties when there are equal times of winnings. 66 Acquiring features • Parsing: Processing features acquired from PCFG parsing (Petrov et al., 2006) for both source and target side include: – – – – parse log likelihood, number of n-best trees, confidence for the best parse, average confidence of all trees. Ratios of the above target features to their respective source features were included. • Shallow grammatical match: The number of occurences of particular node tags on both the source and the target was counted on the PCFG parses. In particular, NPs, VPs, PPs, NNs and punctuation occurences were counted. Then the ratio of the occurences of each tag in the target sentence by its occurences on the source sentence was also calculated. 2.4"
W11-2104,2009.eamt-1.15,0,0.0842697,"Missing"
W11-2104,N07-1029,0,0.0599919,"Missing"
W11-2104,2009.mtsummit-papers.16,0,0.0218978,"teria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating against this data. Evalu"
W11-2104,H05-1096,0,0.181409,"nce. As qualitative criteria, we use statistical features indicating the quality and the grammaticality of the output. 2 2.1 Automatic ranking method From Confidence Estimation to ranking Confidence estimation has been seen from the Natural Language Processing (NLP) perspective as a problem of binary classification in order to assess the correctness of a NLP system output. Previous work focusing on Machine Translation includes statistical methods for estimating correctness scores or correctness probabilities, following a rich search over the spectrum of possible features (Blatz et al., 2004a; Ueffing and Ney, 2005; Specia et al., 2009; Raybaud and Caroline Lavecchia, 2009; Rosti et al., 65 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 65–70, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2007). In this work we slightly transform the binary classification practice to fit the standard WMT human evaluation process. As human annotators have provided their evaluation in the form of ranking of five system outputs at a sentence level, we build our evaluation mechanism with similar functionality, aiming to training from and evaluating aga"
W11-2104,W09-0401,0,\N,Missing
W11-2104,J06-4003,0,\N,Missing
W11-2109,W05-0909,0,0.0830542,"xplored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of ev"
W11-2109,J93-2003,0,0.0279455,"st be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using reference translation, the problem of evaluating translation quality (e.g., completeness, ordering, domain fit, etc.) is transformed into a kind of paraphrase evaluation in the target language, which is a very difficult problem itself. In addition, the set of selected references always represents only a small subset of all good translations. To remedy these drawbacks, we propose a truly automatic evaluation metric which is based on the IBM 1 lexicon scores (Brown et al., 1993). The inclusion of IBM 1 scores in translation systems has shown experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated"
W11-2109,W08-0309,0,0.0789113,"n experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P"
W11-2109,W10-1703,0,0.0538159,"have been used for confidence estimation for machine translation (Blatz et al., 2003). To the best of our knowledge, these scores have not yet been used as an evaluation metric. We carry out a systematic comparison between several variants of IBM 1 scores. The Spearman’s rank correlation coefficients on the document (system) level between the IBM 1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (CallisonBurch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 IBM 1 scores The IBM 1 model is a bag-of-word translation model which gives the sum of all possible alignment probabilities between the words in the source sentence and the words in the target sentence. Brown et al. (1993) defined the IBM 1 probability score for a translation 99 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 99–103, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics pair f1J and eI1 in the following way: P (f1J |eI1 ) = 3 Experiments on WMT 2008, WMT 2009 and WMT 2010 test data I J"
W11-2109,P02-1040,0,0.0908633,"BM 1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the IBM 1 scores are competitive with the classic evaluation metrics, the most promising being IBM 1 scores calculated on morphemes and POS-4grams. 1 Introduction Currently used evaluation metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), etc. are based on the comparison between human reference translations and the automatically generated hypotheses in the target language to be evaluated. While this scenario helps in the design of machine translation systems, it has two major drawbacks. The first one is the practical criticism that using reference translations is inefficient and expensive: in real-life situations, the quality of machine translation must be evaluated without having to pay humans for producing reference translations first. The second criticism is methodological: in using refer"
W11-2109,E09-1087,0,0.0855911,"Missing"
W11-2109,W09-0401,0,\N,Missing
W11-2109,C04-1046,0,\N,Missing
W11-2110,W08-0309,0,0.160081,"combinations. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Machine translation outputs in five different European languages are used: English, Spanish, French, German and Czech. The results show that the F scores which take into account morphemes and POS tags are the most promising metrics. 1 Introduction Recent investigations have shown that the n-gram based evaluation metrics calculated on Part-ofSpeech (POS) sequences correlate very well with human judgments (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Popovi´c and Ney, 2009) clearly outperforming the widely used metrics BLEU and TER. The BLEU score measured on morphemes is shown to be useful for evaluation of morphologically rich languages (Luong et al., 2010). We propose the use of morphemes for a set of n-gram based automatic evaluation metrics and investigate the correlation of the novel metrics with human judgments. We carry out a systematic comparison between the F and BLEU based metrics calculated on various combinations of words, morphemes and POS tags. The focus of this work is not a comparison of the"
W11-2110,W10-1703,0,0.0282078,"ags alone, the metrics calculated on pairs, i.e. words and POS tags, words and morphemes as well as morphemes and POS tags, and the metrics which take everything into account – lexical, morphological and syntactic information, i.e. words, morphemes and POS tags. Spearman’s rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al., 2008), fourth (Callison-Burch et al., 2009) and fifth (Callison-Burch et al., 2010) shared translation tasks. 2 Evaluation metrics We carried out a systematic comparison between the following metrics: • single unit (word/morpheme/POS) metrics: – 1 WORD F Standard F score: takes into account all word n-grams which have a counterpart Apart from the standard lated. BLEU score which is tightly re104 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 104–107, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics both in the corresponding reference and in the hypothesis. – MORPH F Morpheme F score: takes into account all"
W11-2110,D10-1015,0,0.0206217,"uages are used: English, Spanish, French, German and Czech. The results show that the F scores which take into account morphemes and POS tags are the most promising metrics. 1 Introduction Recent investigations have shown that the n-gram based evaluation metrics calculated on Part-ofSpeech (POS) sequences correlate very well with human judgments (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Popovi´c and Ney, 2009) clearly outperforming the widely used metrics BLEU and TER. The BLEU score measured on morphemes is shown to be useful for evaluation of morphologically rich languages (Luong et al., 2010). We propose the use of morphemes for a set of n-gram based automatic evaluation metrics and investigate the correlation of the novel metrics with human judgments. We carry out a systematic comparison between the F and BLEU based metrics calculated on various combinations of words, morphemes and POS tags. The focus of this work is not a comparison of the morpheme and POS based metrics with the standard evaluation metrics1 as in (Popovi´c and Ney, 2009), but rather a comparison within the proposed set of metrics in order to decide which score(s) should be submitted to the WMT 2011 evaluation ta"
W11-2110,P02-1040,0,0.0907427,"lated. BLEU score which is tightly re104 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 104–107, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics both in the corresponding reference and in the hypothesis. – MORPH F Morpheme F score: takes into account all morpheme n-grams which have a counterpart both in the corresponding reference and in the hypothesis. – POS F POS F score: takes into account all POS n-grams which have a counterpart both in the corresponding reference and in the hypothesis. – BLEU The standard BLEU score (Papineni et al., 2002). – POS B LEU The standard BLEU score calculated on POS tags. – MORPH B LEU The standard BLEU score calculated on morphemes. • pairwise metrics: – – – – – – WP F F score of word and POS n-grams. WM F F score of word and morpheme n-grams. MP F F score of morpheme and POS n-grams. WP B LEU Arithmetic mean of BLEU and POS B LEU scores. WM B LEU Arithmetic mean of BLEU and MOR PH B LEU scores. MP B LEU Arithmetic mean of MORPH B LEU and POS B LEU scores. • metrics taking everything into account: – WMP F F score on word, morpheme and POS ngrams. – WMP B LEU Arithmetic mean of BLEU, MORPH B LEU and"
W11-2110,W09-0402,1,0.89352,"Missing"
W11-2110,E09-1087,0,0.0971745,"Missing"
W11-2110,W09-0401,0,\N,Missing
W12-3105,W10-1703,0,0.0513929,"Missing"
W12-3105,W11-2106,0,0.0901512,"Missing"
W12-3105,fishel-etal-2012-terra,1,0.86613,"Missing"
W12-3105,N06-1014,0,0.0173292,"Zeman et al., 2011) and Hjerson (Popovi´c and Ney, 2011) use different methods for automatic error analysis. Addicter explicitly aligns the hypothesis and reference translations and induces error categories based on the alignment coverage while Hjerson compares words encompassed in the WER (word error rate) and PER (position-independent word error rate) scores to the same end. Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al., 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. Both tools use word lemmas for their analysis; we used TreeTagger (Schmid, 1995) for analyzing English, Spanish, German and French and Morˇce (Spoustov´a et al., 2007) to analyze Czech. The same tools are used for PoS-tagging in some experiments. 2.2 HYP -1 Binary Classification Pairwise comparison of sentence pairs is achieved with a binary SVM classifier, trained via sequential minimal optimization (Platt, 1998), implemented in Weka (Hall et al., 2009). The input feature vectors are comp"
W12-3105,P02-1040,0,0.0842937,"Missing"
W12-3105,W11-2111,0,0.0174685,"e scores remain to be checked against the human 69 judgments from WMT’12. The introduced TerrorCat metric has certain dependencies. For one thing, in order to apply it to new languages, a training set of manual rankings is required – although this can be viewed as an advantage, since it enables the user to tune the metric to his/her own preference. Additionally, the metric depends on lemmatization and PoS-tagging. There is a number of directions to explore in the future. For one, both Addicter and Hjerson report MT errors related more to adequacy than fluency, although it was shown last year (Parton et al., 2011) that fluency is an important component in rating translation quality. It is also important to test how well the metric performs if lemmatization and PoStagging are not available. For this year’s competition, training data was taken separately for every language pair; it remains to be tested whether combining human judgements with the same target language and different source languages leads to better or worse performance. To conclude, we have described TerrorCat, one of the submissions to the metrics shared task of WMT’12. TerrorCat is rather demanding to apply on one hand, having more requir"
W12-3105,J11-4002,1,0.872172,"Missing"
W12-3105,W07-1709,0,0.0315439,"Missing"
W12-3106,W08-0309,0,0.0802097,"Missing"
W12-3106,J11-4002,1,0.876025,"Missing"
W12-3106,W07-1709,0,0.0712781,"Missing"
W12-3106,W09-0401,0,\N,Missing
W12-3106,W10-1703,0,\N,Missing
W12-3116,J93-2003,0,0.0706504,"trics. The usual approach is to use a set of features which are used to train a classifier in order to assign a prediction score to each sentence. In this work, we propose a set of features based on the morphological and syntactic properties of involved languages thus abstracting away from word surface particularities (such as vocabulary and domain). This approach is shown to be very useful for evaluation task (Popovi´c, 2011; Popovi´c et al., 2011; Callison-Burch et al., 2011). The features investigated in this work are based on the language model (LM) scores and on the IBM 1 lexicon scores (Brown et al., 1993). The inclusion of IBM 1 scores in translation systems has shown experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). The IBM 1 scores calculated on morphemes and POS-4grams are shown to be competitive with the classic evaluation metrics based on comparison with given reference translations (Popovi´c et al., 2011; Callison-Burch et al., 2011). To the best of our knowledge, these scores have not yet been used for translation quality estimation. The LM scores of words and POS tags are us"
W12-3116,W11-2110,1,0.859725,"Missing"
W12-3116,2009.mtsummit-papers.16,0,0.0169102,"on systems has shown experimentally to improve translation quality (Och et al., 2003). They also have been used for confidence estimation for machine translation (Blatz et al., 2003). The IBM 1 scores calculated on morphemes and POS-4grams are shown to be competitive with the classic evaluation metrics based on comparison with given reference translations (Popovi´c et al., 2011; Callison-Burch et al., 2011). To the best of our knowledge, these scores have not yet been used for translation quality estimation. The LM scores of words and POS tags are used for quality estimation in previous work (Specia et al., 2009), and in our work we investigate the scores calculated on morphemes and POS tags. At this point, only preliminary experiments have been carried out in order to determine if the proposed features are promising at all. We did not use any classifier, we used the obtained scores to rank the sentences of a given translation output from the best to the worst. The Spearman’s rank correlation coefficients between our ranking and the ranking obtained using human scores are then computed on the provided manually annotated data sets. 2 Morpheme- and POS-based features A number of features for quality est"
W12-3116,specia-etal-2010-dataset,0,0.0146743,"ation. Once a morpheme segmentation has been learnt from some text, it can be used for segmenting new texts. In our experiments, the splitting are learnt from the training corpus used for the IBM 1 lexicon probabilities. The obtained segmentation is then used for splitting the corresponding source texts and hypotheses. Detailed corpus statistics are shown in Table 1. Using the obtained probabilities, the scores described in Section 2 are calculated for the provided annotated data: the English-Spanish data from WMT 2008 consisting of four translation outputs produced by four different systems (Specia et al., 2010), the French-English and English-Spanish data from WMT 2010 (Specia, 2011), as well as for an additional WMT 2011 German-English and EnglishGerman annotated data. The human quality scores for the first two data sets range from 1 to 4, and for the third data set from 1 to 3. The interpretation of human scores is: 1. requires complete retranslation (bad) 2. post-editing quicker than retranslation (edit− ); this class was omitted for the third data set 3. little post-editing needed (edit+ ) • rank> percentage of translation outputs where the particular feature has better correlation than the othe"
W12-3116,2011.eamt-1.12,0,0.0119033,"d for segmenting new texts. In our experiments, the splitting are learnt from the training corpus used for the IBM 1 lexicon probabilities. The obtained segmentation is then used for splitting the corresponding source texts and hypotheses. Detailed corpus statistics are shown in Table 1. Using the obtained probabilities, the scores described in Section 2 are calculated for the provided annotated data: the English-Spanish data from WMT 2008 consisting of four translation outputs produced by four different systems (Specia et al., 2010), the French-English and English-Spanish data from WMT 2010 (Specia, 2011), as well as for an additional WMT 2011 German-English and EnglishGerman annotated data. The human quality scores for the first two data sets range from 1 to 4, and for the third data set from 1 to 3. The interpretation of human scores is: 1. requires complete retranslation (bad) 2. post-editing quicker than retranslation (edit− ); this class was omitted for the third data set 3. little post-editing needed (edit+ ) • rank> percentage of translation outputs where the particular feature has better correlation than the other investigated features. 4 Results 4.1 Arithmetic means The preliminary ex"
W12-3116,E09-1087,0,0.0364499,"Missing"
W12-3116,C04-1046,0,\N,Missing
W12-3116,W11-2109,1,\N,Missing
W13-2240,C12-1008,1,0.893937,"features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul´e et al., 2008). The open source language tool1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Demˇsar et al., 2004). 2.3.2 3.2 For the sub-task on sentence-ranking we used pairwise classification, so that we can take advantage of several powerful binary classification methods (Avramidis, 2012). We used logistic regression, which optimizes a logistic function to predict values in the range between zero and one (Cameron, 1998), given a feature set X: P (X) = 1 1+ e−1(a+bX) (1) Regression The sentence-ranking sub-task has provided training data for two language pairs, German-English and English-Spanish. For both sentence pairs, we train the systems using the provided annotated data sets WMT2010, WMT2011 and WMT2012, while the data set WMT2009 is used for the evaluation during the development phase. Data sets are analyzed with black-box feature generation. For each language pair, the t"
W13-2240,2005.mtsummit-papers.11,0,0.023686,"orithm to iteratively minimize the least squares error computed from training data (Miller, 2002). Experiments are repeated with two variations of Logistic Regression concerning internal features treatment: Stepwise Feature Set Selection (Hosmer, 1989) and L2-Regularization (Lin et al., 2007). Relieff is implemented for k=5 nearest neighbours sampling m=100 reference instances. Information gain is calculated after discretizing features into n=100 values N-gram features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). PCFG parsing features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul´e et al., 2008). The open source language tool1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Demˇsar et al., 2004). 2.3.2 3.2 For the sub-task on sentence-ranking we used pairwise classification, so that we can ta"
W13-2240,W11-2104,1,0.623947,"on of the position of the unknown words. Black-box features Features of this type are generated as a result of automatic analysis of both the source sentence and the MT output (when applicable), whereas many of them are already part of the baseline infrastructure. For all features we also calculate the ratios of the source to the target sentence. These features include: PCFG Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of VPs, NPs etc.), the parse log-likelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). Log probability (pC) and future cost estimate (c) of the phrases chosen as part of the best translation: minimum and maximum values and their position in the sentence averaged to the number of sentences, and also their average, variance, standard deviation; count of the phrases whose probability or future cost estimate is lower and higher than their standard deviation; the ratio of these phrases to the total number of phrases. Rule-based language correction is a result of hand-written controlled language rules, that indicate mistakes on several pre-defined error categories (Naber, 2003). We"
W13-2240,W07-0734,0,0.0259301,"1 Information gain Information gain (Hunt et al., 1966) estimates the difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007), using the other translations as references (Soricut et al., 2012). 2.1.2 Feature selection Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. 2.2.2 ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif330 ranking tasks: Mean Reciprocal Rank - MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain - NDGC (J¨arvelin and Kek¨al¨ainen, 2002), which give better scores to models when higher ranks (i.e. better transl"
W13-2240,W13-2202,0,0.0864643,"Missing"
W13-2240,P02-1040,0,0.0945526,"rch et al., 2012). 2.2.1 Information gain Information gain (Hunt et al., 1966) estimates the difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007), using the other translations as references (Soricut et al., 2012). 2.1.2 Feature selection Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. 2.2.2 ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif330 ranking tasks: Mean Reciprocal Rank - MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain - NDGC (J¨arvelin and Kek¨al¨ainen, 2002), which give better scores to models when higher"
W13-2240,P06-1055,0,0.0492042,"unknown to the phrase table, average number of unknown words first/last position of an unknown word in the sentence normalized to the number of tokens, variance and deviation of the position of the unknown words. Black-box features Features of this type are generated as a result of automatic analysis of both the source sentence and the MT output (when applicable), whereas many of them are already part of the baseline infrastructure. For all features we also calculate the ratios of the source to the target sentence. These features include: PCFG Features: We parse the text with a PCFG grammar (Petrov et al., 2006) and we derive the counts of all node labels (e.g. count of VPs, NPs etc.), the parse log-likelihood and the number of the n-best parse trees generated (Avramidis et al., 2011). Log probability (pC) and future cost estimate (c) of the phrases chosen as part of the best translation: minimum and maximum values and their position in the sentence averaged to the number of sentences, and also their average, variance, standard deviation; count of the phrases whose probability or future cost estimate is lower and higher than their standard deviation; the ratio of these phrases to the total number of"
W13-2240,N07-1051,0,0.0145822,"ations of Logistic Regression concerning internal features treatment: Stepwise Feature Set Selection (Hosmer, 1989) and L2-Regularization (Lin et al., 2007). Relieff is implemented for k=5 nearest neighbours sampling m=100 reference instances. Information gain is calculated after discretizing features into n=100 values N-gram features are computed with the SRILM toolkit (Stolcke, 2002) with an order of 5, based on monolingual training material from Europarl (Koehn, 2005) and News Commentary (CallisonBurch et al., 2011). PCFG parsing features are generated on the output of the Berkeley Parser (Petrov and Klein, 2007) trained over an English, a German and a Spanish treebank (Taul´e et al., 2008). The open source language tool1 is used to annotate source and target sentences with language suggestions. The annotation process is organised with the Ruffus library (Goodstadt, 2010) and the learning algorithms are executed using the Orange toolkit (Demˇsar et al., 2004). 2.3.2 3.2 For the sub-task on sentence-ranking we used pairwise classification, so that we can take advantage of several powerful binary classification methods (Avramidis, 2012). We used logistic regression, which optimizes a logistic function t"
W13-2240,W12-3118,0,0.172404,"difference between the prior entropy of the classes and the posterior entropy given the attribute values. It is useful for estimating the quality of each attribute but it works under the assumption that features are independent, so it is not suitable when strong feature inter-correlation exists. Information gain is only used for the sentence ranking task after discretization of the feature values. Contrastive evaluation scores: For the ranking task, each translation is scored with an automatic metric (Papineni et al., 2002; Lavie and Agarwal, 2007), using the other translations as references (Soricut et al., 2012). 2.1.2 Feature selection Glass-box features Glass-box features are available only for the timeprediction task, as a result of analyzing the verbose output of the Minimum Bayes Risk decoding process. 2.2.2 ReliefF ReliefF assesses the ability of each feature to distinguish between very similar instances from dif330 ranking tasks: Mean Reciprocal Rank - MRR (Voorhees, 1999) and Normalized Discounted Cumulative Gain - NDGC (J¨arvelin and Kek¨al¨ainen, 2002), which give better scores to models when higher ranks (i.e. better translations) are ordered correctly, as these are more important than low"
W13-2240,taule-etal-2008-ancora,0,0.0896671,"Missing"
W13-2240,W12-3102,0,\N,Missing
W14-4210,espla-gomis-etal-2014-comparing,1,0.887321,"Missing"
W14-4210,W14-0405,1,0.855262,"Missing"
W14-4210,W13-2408,1,0.889943,"Missing"
W14-4210,P07-2045,0,0.00751512,"Missing"
W14-4210,P02-1040,0,0.0913714,"d on the content published on the SETimes.com news portal which publishes “news and views from Southeast Europe” in ten languages: Bulgarian, Bosnian, Greek, English, Croatian, Macedonian, Romanian, Albanian and Serbian. We used the parallel trilingual Croatian-English-Serbian part of the corpus. The detailed corpus statistic is shown in Table 2. The Croatian language is further referred to as hr, Serbian as sr and English as en. The translation system used is the phrase-based Moses system (Koehn et al., 2007). The evaluation metrics used for assessment of the translations are the BLEU score (Papineni et al., 2002) and the F-score, which also takes recall into account and generally better correlates with human rankings which has been shown in (Melamed et al., 2003) and confirmed in (Popovi´c, 2011). For Language adaptation methods The following methods were investigated for adaptation of the test set in the other language: • lexical conversion of the most frequent words (conv); The most frequent6 different words together with simple morphological variations are replaced by the words in the corresponding language. This method is simple and fast, however it is very basic and also requires knowledge of the"
W14-4210,N03-2021,0,0.0269268,"ek, English, Croatian, Macedonian, Romanian, Albanian and Serbian. We used the parallel trilingual Croatian-English-Serbian part of the corpus. The detailed corpus statistic is shown in Table 2. The Croatian language is further referred to as hr, Serbian as sr and English as en. The translation system used is the phrase-based Moses system (Koehn et al., 2007). The evaluation metrics used for assessment of the translations are the BLEU score (Papineni et al., 2002) and the F-score, which also takes recall into account and generally better correlates with human rankings which has been shown in (Melamed et al., 2003) and confirmed in (Popovi´c, 2011). For Language adaptation methods The following methods were investigated for adaptation of the test set in the other language: • lexical conversion of the most frequent words (conv); The most frequent6 different words together with simple morphological variations are replaced by the words in the corresponding language. This method is simple and fast, however it is very basic and also requires knowledge of the involved languages to be set up. It can be seen as a very first step towards the use of a rule-based Croatian-Serbian system. • Croatian-Serbian transla"
W14-4210,W11-2110,1,0.877682,"Missing"
W14-4210,E12-1015,0,0.0780458,"LT4CloseLang), pages 76–84, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2.2 BLEU ) over the Google baseline in the tourism domain (Toral et al., 2014). A rule-based Apertium system (Peradin et al., 2014) has been recently developed for translation from and into Slovenian (also closely related language, but more distant). Techniques simpler than general SMT such as character-level translation have been investigated for translation between various close language pairs, where for the South Slavic group the Bulgarian-Macedonian pair has been explored (Nakov and Tiedemann, 2012). Characterbased translation has also been used for translating between Bosnian and Macedonian in order to build pivot translation systems from and into English (Tiedemann, 2012). Developing POS taggers and lemmatizers for Croatian and Serbian and using Croatian models on Serbian data has been explored in (Agi´c et al., 2013). To the best of our knowledge, a systematic investigation of cross-language translation systems involving Croatian and Serbian, thereby exploiting benefits from the language closeness and analyzing problems induced by language differences has not been carried out yet. 2 2"
W14-4210,P12-2059,0,\N,Missing
W14-5104,avramidis-etal-2012-richly,1,0.867792,"Missing"
W14-5104,W14-3339,0,0.0137678,"x features in order to predict numerical indications of translation quality, such as postediting effort (Rubino et al., 2013; Hildebrand and Vogel, 2013) or post-editing time (Avramidis et al., 2013). Contrary to these works, we only predict specific error types, with the focus on understanding the contribution of the features. Prediction of specific error types was included in the shared tasks of the 8th and 9th Workshop on Statistical Machine Translation (Bojar et al., 2013; Bojar et al., 2014). Several participants contributed systems that predict error types (Besacier and Lecouteux, 2013; Bicici and Way, 2014; de Souza et al., 2014). In that case, prediction was done on the word level and contrary to our experiments, no glass-box features were used, therefore there was no connection of the ML with the decoding process. Guzm´an and Vogel (2012), in the work that is most related to ours, aim to identify the contribution of the features. Similar to several previously mentioned works, a multivariate linear regression model is trained in order to predict continuous quality values of complex metrics. Although the aim of this work is similar to ours, we work in a more fine-grained way: instead of modelli"
W14-5104,C04-1046,0,0.0295306,"utput (Section 4.1). D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 20–29, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 Related Work Error detection for MT and Quality Estimation is an important component of post-editing approaches. Our work focuses solely on features derived from the decoding process. The first experiments on “Confidence Estimation” make use of a small number of Statistical Machine Translation (SMT) features in order to train a supervised model for predicting the quality of the Translation (Blatz et al., 2004). Later work, identified as “Quality Estimation”, defines such features as “glass-box” features (Specia et al., 2009). 54 glass-box features are shown to be very informative, when fitted in a regression model, along with other black-box features. Avramidis (2011) uses decoding features in a sentence-level pairwise classification approach for Hybrid MT in order to select the best translations out of outputs produced by statistical and rulebased systems, whereas a corpus of machine translation outputs with internal meta-data was released at that time (Avramidis et al., 2012). Later works use gla"
W14-5104,W07-0718,0,0.0157585,"e pair lang de-en en-de de-fr fr-de de-es es-de all sentences total p.e 1811 1139 1101 315 982 198 1051 122 543 543 931 931 6419 3248 reordering err. total p.e 1043 474 891 232 819 157 851 88 288 288 345 345 4237 1584 missing words total p.e 1079 570 671 151 597 80 691 76 322 322 333 333 3693 1532 extra words total p.e 869 454 722 208 630 147 621 66 186 186 339 339 3367 1400 Table 3: The size of the corpus per error category and language pair. p.e. indicates the number of sentences that were minimally post-edited by professional translators with MERT using the news corpus test set from WMT07 (Callison-Burch et al., 2007). The decoding features are extracted from Moses’ verbose output of level 2. Our target language model with an order of 5 is trained with SRILM toolkit (Stolcke, 2002), based on the respective monolingual training material. The Orange toolkit (Demˇsar et al., 2004) is used for processing and running the Logistic Regression algorithms. The Hjerson tool (Popovi´c, 2011b) was used in order to detect errors on the translation. 6 6.1 Results Model performance A necessary step is to check how well each model fits the data, since a well-fit model is required for drawing conclusions. For this purpose"
W14-5104,W14-3340,0,0.0335952,"Missing"
W14-5104,C12-1063,0,0.0330443,"Missing"
W14-5104,W13-2246,0,0.0308525,"features are shown to be very informative, when fitted in a regression model, along with other black-box features. Avramidis (2011) uses decoding features in a sentence-level pairwise classification approach for Hybrid MT in order to select the best translations out of outputs produced by statistical and rulebased systems, whereas a corpus of machine translation outputs with internal meta-data was released at that time (Avramidis et al., 2012). Later works use glass-box features in order to predict numerical indications of translation quality, such as postediting effort (Rubino et al., 2013; Hildebrand and Vogel, 2013) or post-editing time (Avramidis et al., 2013). Contrary to these works, we only predict specific error types, with the focus on understanding the contribution of the features. Prediction of specific error types was included in the shared tasks of the 8th and 9th Workshop on Statistical Machine Translation (Bojar et al., 2013; Bojar et al., 2014). Several participants contributed systems that predict error types (Besacier and Lecouteux, 2013; Bicici and Way, 2014; de Souza et al., 2014). In that case, prediction was done on the word level and contrary to our experiments, no glass-box features"
W14-5104,P07-1019,0,0.0351913,"aining a statistical model on these error categories.1 . In order to detect the errors on the translation output, we follow the automatic method by Popovi´c and Ney (2011), which has shown to correlate well with human error annotation. This method automatically detects errors based on the edit distance of the produced translation against a reference human translation. An example of how errors are detected can be seen in Figure 1. 4.2 Phrase-based SMT search graph The glass-box features are extracted from the decoding process of a phrase-based SMT system (Koehn et al., 2003) with cube-pruning (Huang and Chiang, 2007). The decoding process performs a search in various dimensions, calculating scores for many phrases and hypothesis expansions. Most scores are difficult to be interpreted as glass-box features in their initial form. The amount of scores calculated per sentence is not fixed, whereas the basic requirement for each feature is to have only one value that is valid on a sentence level, so that it can be used in the sentence error prediction model. For this purpose, we process the verbose output of the decoder and derive scores, counts and other statistics that can have this sentence-level interpreta"
W14-5104,N03-1017,0,0.00998221,"our data give sufficient amounts for training a statistical model on these error categories.1 . In order to detect the errors on the translation output, we follow the automatic method by Popovi´c and Ney (2011), which has shown to correlate well with human error annotation. This method automatically detects errors based on the edit distance of the produced translation against a reference human translation. An example of how errors are detected can be seen in Figure 1. 4.2 Phrase-based SMT search graph The glass-box features are extracted from the decoding process of a phrase-based SMT system (Koehn et al., 2003) with cube-pruning (Huang and Chiang, 2007). The decoding process performs a search in various dimensions, calculating scores for many phrases and hypothesis expansions. Most scores are difficult to be interpreted as glass-box features in their initial form. The amount of scores calculated per sentence is not fixed, whereas the basic requirement for each feature is to have only one value that is valid on a sentence level, so that it can be used in the sentence error prediction model. For this purpose, we process the verbose output of the decoder and derive scores, counts and other statistics t"
W14-5104,2005.mtsummit-papers.11,0,0.158089,"Missing"
W14-5104,W07-0734,0,0.0286154,"lly significant coefficients of the logistic function are used to analyze parts of the decoding process that are related to the particular errors. 1 Introduction Evaluating the output of Machine Translation (MT) has been in the focus since the first developments of the field. There have been several efforts to measure the translation performance, or to identify errors by defining manual and automatic metrics. Advanced automatic metrics and Quality Estimation methods have introduced machine learning (ML) techniques in order to predict indications about the quality of the produced translations (Lavie and Agarwal, 2007; Stanojevic and Sima’an, 2014). When compared to traditional automatic metrics, ML techniques allow acquiring knowledge about the quality of the translation out of a big amount of features. Such features are typically black-box features, generated by automatic analysis over the text of the source or the translations, or less often glass-box features, derived from the internal functioning of the translation mechanism. In this work, we focus on the glass-box fea20 tures. However, instead of focusing on the performance of a quality assessment mechanism, we look backwards into what happened durin"
W14-5104,P02-1040,0,0.091003,"a weight vector estimated by ML to minimize the error of the function ◦, given samples of X and Y . This vector contains coefficients for each one of the features. Given a well-fit model and a relevant statistical function, these coefficients can indicate the importance of each feature. Our aim is to use the β coefficients in order to explain several behaviours of the decoding process, relevant to the errors. The exact formulation of the statistical function ◦ is given in Section 4.3. Our intention is to not train the model using as a dependent variable a complex quality metric such as BLEU (Papineni et al., 2002) or WER, since this would increase complexity by capturing many issues in just one number. Instead, we choose a more fine-grained approach, by focusing onto specific type of errors that occur often in machine translation output (Section 4.1). D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 20–29, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 Related Work Error detection for MT and Quality Estimation is an important component of post-editing approaches. Our work focuses solely on features derived from the decodi"
W14-5104,J11-4002,1,0.884579,"Missing"
W14-5104,2011.eamt-1.36,1,0.897659,"Missing"
W14-5104,2009.eamt-1.5,0,0.0157733,"essing, pages 20–29, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) 3 Related Work Error detection for MT and Quality Estimation is an important component of post-editing approaches. Our work focuses solely on features derived from the decoding process. The first experiments on “Confidence Estimation” make use of a small number of Statistical Machine Translation (SMT) features in order to train a supervised model for predicting the quality of the Translation (Blatz et al., 2004). Later work, identified as “Quality Estimation”, defines such features as “glass-box” features (Specia et al., 2009). 54 glass-box features are shown to be very informative, when fitted in a regression model, along with other black-box features. Avramidis (2011) uses decoding features in a sentence-level pairwise classification approach for Hybrid MT in order to select the best translations out of outputs produced by statistical and rulebased systems, whereas a corpus of machine translation outputs with internal meta-data was released at that time (Avramidis et al., 2012). Later works use glass-box features in order to predict numerical indications of translation quality, such as postediting effort (Rubino"
W14-5104,W14-3354,0,0.0276059,"Missing"
W14-5104,vilar-etal-2006-error,0,0.0885356,"Missing"
W14-5104,P07-2045,0,\N,Missing
W14-5104,W13-2248,0,\N,Missing
W14-5104,W10-1703,0,\N,Missing
W15-3004,W07-0726,0,0.0343284,"he QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17"
W15-3004,eisele-chen-2010-multiun,0,0.0119801,"t consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT sy"
W15-3004,W11-2104,1,0.900755,"Missing"
W15-3004,W12-3138,0,0.0126825,"t is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computat"
W15-3004,W10-1708,0,0.0233189,"1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-based Lucy system (Alonso and Thurmair, 2003) includes the results of long linguistic efforts over the last decades and that has been used in previous projects including E URO M ATRIX, E URO M ATRIX + and QTL AUNCH PAD, while relevant hybrid systems have been submitted to WMT (Chen et al., 2007; Federmann et al., 2010; Hunsicker et al., 2012). The transferbased approach has shown good results that compete with pure statistical systems, whereas it focuses on translating according to linguistic struc• A statistical Moses system, • the commercial transfer-based system Lucy, • their serial combination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015"
W15-3004,W12-0115,0,0.0126604,"bination (”LucyMoses”), and • an informed selection mechanism (”ranker”). The components of this hybrid system will be detailed in the sections below. 1 Translation systems http://qtleap.eu/ 66 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 66–73, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. 2.1.1 Empirical machine learning classifier (primary submission) The machine learning (ML) selection mechanism is based on encouraging results of previous projects including E URO M ATRIX + (Federmann and Hunsicker, 2011), META-NET (Federmann, 2012), QTL AUNCH PAD (Avramidis, 2013; Shah et al., 2013). It has been extended to include several features that can only be generated on a sentence level and would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In the architecture at hand, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source wit"
W15-3004,P02-1040,0,0.0935117,"Missing"
W15-3004,N07-1051,0,0.0411992,"l system combination produces a perfect translation. In this particular case, the machine translation is even better than the human reference (“W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus.”) as the latter is introducing a determiner for “table”, which is not justified by the source. 2.1 Feature sets We experimented with feature sets that performed well in previous experiments. In particular: • Basic syntax-based feature set: unknown words, count of tokens, count of alternative parse trees, count of verb phrases, PCFG parse log likelihood. The parsing was performed with the Berkeley Parser (Petrov and Klein, 2007) and features were extracted from both source and target. This feature set has performed well as a metric in WMT-11 metrics task (Avramidis et al., 2011). Sentence level selection • Basic feature set + 17 QuEst baseline features: this feature set combines the basic syntax-based feature set described above We present two methods for performing sentence level selection, one with pairwise classifier and one based on POS 4-gram IBM 1 models. 67 icality. Further analysis on this aspect may be required. with the baseline feature set of the QuEst toolkit (Specia et al., 2013) as per WMT-13 (Bojar et"
W15-3004,W11-2123,0,0.0411543,"o methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this paper is the first of a series of MT system prototypes developed in the project. Figure 1 shows the overall architecture that includes: Lucy The transfer-"
W15-3004,W11-2110,1,0.867236,"Missing"
W15-3004,N07-1064,0,0.190898,"analysis phase, where the sourcelanguage text is parsed and a tree of the source language is constructed • the transfer phase, where the analysis tree is used for the transfer phase, where canonical forms and categories of the source are transferred into similar representations of the target language • the generation phase, where the target sentence is formed out of the transfered representations by employing inflection and agreement rules. LucyMoses As an alternative way of automatic post-editing of the transfer-based system, a serial transfer+SMT system combination is used, as described in (Simard et al., 2007). For building it, the first stage is translation of the source language part of the training corpus by the transfer-based system. In the second stage, an SMT system is trained using the transfer-based translation output as a source language and the target language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. In previous experiments, however, the method on its own could not outperform Moses trained on a large parallel corpus. The example in Figure 1 (taken from the QTL EAP corpus u"
W15-3004,P13-1135,0,0.0238802,"by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM 1 models based on POS 4-grams (contrastive submission). 1 Figure 1: Architecture of System Combination. 2 Moses Our statistical machine translation system was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the corpora Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013) and MultiUN (Eisele and Chen, 2010). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar (Popovi´c and Ney, 2006). As a tuning set we used the news-test 2013. Introduction The system architecture we will describe has been developed within the QTL EAP project.1 The goal of the project is to explore different combinations of shallow and deep processing for improving MT quality. The system presented in this pap"
W15-3004,P13-4014,0,0.0588687,"Missing"
W15-3004,J93-2003,0,\N,Missing
W15-3004,W05-0909,0,\N,Missing
W15-3004,W13-2201,0,\N,Missing
W15-3004,W11-2141,0,\N,Missing
W15-3049,P02-1040,0,0.11697,"Missing"
W15-3049,W11-2111,0,0.0189943,"as well as segment-level correlation for 6gram F1 (CHR F) and F3-scores (CHR F3) on WMT 14 data for all available target languages. The results are very promising, especially for the CHR F3 score – for translation from English, this variant showed the highest segment-level correlations outperforming even the best metrics on the WMT 14 shared evaluation task. 1 2 CHR F score The general formula for the CHR F score is: Introduction Recent investigations have shown that character level n-grams play an important role for automatic evaluation as a part of more complex metrics such as MT E R ATER (Parton et al., 2011) and BEER (Stanojevi´c and Sima’an, 2014a; Stanojevi´c and Sima’an, 2014b). However, they have not been investigated as an individual metric so far. On the other hand, the n-gram based F-scores, especially the linguistically motivated ones based on Part-of-Speech tags and morphemes (Popovi´c, 2011), are shown to correlate very well with human judgments clearly outperforming the widely used metrics such as BLEU and TER. In this work, we propose the use of the Fscore based on character n-grams, i.e. the CHR F score. We believe that this score has a potential as a stand-alone metric because it is"
W15-3049,W11-2110,1,0.878259,"Missing"
W15-3049,2006.amta-papers.25,0,0.69421,"Missing"
W15-3049,W14-3354,0,0.0203433,"Missing"
W15-3049,W05-0909,0,0.696191,"Missing"
W15-3049,D14-1025,0,0.0265536,"Missing"
W15-3049,W12-3102,0,\N,Missing
W15-3049,W13-2201,0,\N,Missing
W15-4913,2013.iwslt-evaluation.1,0,0.0658852,"Missing"
W15-4913,2005.mtsummit-papers.11,0,0.0267067,"M 200k sr-de / / / 1.8M / Avg. Sent. Length DGT Europarl EMEA OpenSubtitles SEtimes sl 16.0 23.4 12.7 7.7 / sr / / / 7.6 22.4 en 17.3 27.0 12.3 9.2 23.8 de 16.6 25.4 11.8 8.9 / Table 1: Corpora characteristics. mains were used for all language pairs due to unavailability. It should be noted that according to the META-NET White Papers, both languages have minimal support, with only fragmentary text and speech resources. For the SlovenianEnglish and Slovenian-German language pairs, four domains were investigated: DGT translation memories provided by the JRC (Steinberger et al., 2012), Europarl (Koehn, 2005), European Medicines Agency corpus (EMEA) in the pharmaceutical domain, as well as the OpenSubtitles7 corpus. All the corpora are downloaded from the OPUS web site8 (Tiedemann, 2012). For the Serbian language, only two domains were available: the enhanced version of the SEtimes corpus9 (Tyers and Alperen, 2010) containing “news and views from South-East Europe” for Serbian-English, and OpenSubtitles for the Serbian-English and Serbian-German language pairs. It should be noted that all the corpora contain written texts except OpenSubtitles, which contains transcriptions and translations of spok"
W15-4913,J03-1002,0,0.00878391,"er than in other texts, the tuning and test sets for this domain contain 3000 sentences whereas all other sets contain 1000 sentences. Another remark regarding the OpenSubtitles corpus is that we trained our systems only on those sentence pairs, which were available in En7 http://www.opensubtitles.org/ http://opus.lingfil.uu.se/ 9 http://nlp.ffzg.hr/resources/corpora/ setimes/ 8 99 glish as well as in German in order to have a completely same condition for all systems. All systems have been trained using phrasebased Moses (Koehn et al., 2007), where the word alignments were build with GIZA++ (Och and Ney, 2003). The 5-gram language model was build with the SRILM toolkit (Stolcke, 2002). 5 Evaluation and error analysis The evaluation has been carried out in three steps: ﬁrst, the BLEU scores were calculated for each of the systems. Then, the automatic error classiﬁcation has been applied in order to estimate actual translation errors. After that, manual inspection of language related phenomena leading to particular errors is carried out in order to deﬁne the most important issues which should be addressed for building better systems and/or develop better models. 5.1 BLEU scores As a ﬁrst evaluation s"
W15-4913,P02-1040,0,0.0914288,"model was build with the SRILM toolkit (Stolcke, 2002). 5 Evaluation and error analysis The evaluation has been carried out in three steps: ﬁrst, the BLEU scores were calculated for each of the systems. Then, the automatic error classiﬁcation has been applied in order to estimate actual translation errors. After that, manual inspection of language related phenomena leading to particular errors is carried out in order to deﬁne the most important issues which should be addressed for building better systems and/or develop better models. 5.1 BLEU scores As a ﬁrst evaluation step, the BLEU scores (Papineni et al., 2002) have been calculated for each of the translation outputs in order to get a rough idea about the performance for different domains and translation directions. The scores are presented in Table 2: • the highest scores are obtained for translations into English; • the scores for translations into German are similar to those for translations into Slovenian and Serbian; • the scores for Serbian and Slovenian are better when translated from English than when translated from German; • the best scores are obtained for DGT (which contains a large number of repetitions), followed by EMEA (which is very"
W15-4913,steinberger-etal-2012-dgt,0,0.0394633,"Missing"
W15-4913,tiedemann-2012-parallel,0,0.0573637,"able 1: Corpora characteristics. mains were used for all language pairs due to unavailability. It should be noted that according to the META-NET White Papers, both languages have minimal support, with only fragmentary text and speech resources. For the SlovenianEnglish and Slovenian-German language pairs, four domains were investigated: DGT translation memories provided by the JRC (Steinberger et al., 2012), Europarl (Koehn, 2005), European Medicines Agency corpus (EMEA) in the pharmaceutical domain, as well as the OpenSubtitles7 corpus. All the corpora are downloaded from the OPUS web site8 (Tiedemann, 2012). For the Serbian language, only two domains were available: the enhanced version of the SEtimes corpus9 (Tyers and Alperen, 2010) containing “news and views from South-East Europe” for Serbian-English, and OpenSubtitles for the Serbian-English and Serbian-German language pairs. It should be noted that all the corpora contain written texts except OpenSubtitles, which contains transcriptions and translations of spoken language thus being slightly peculiar for machine translation. On the other hand, this is the only corpus containing all language pairs of interest. Table 1 shows the amount of pa"
W15-4913,2014.eamt-1.45,0,0.278394,"Missing"
W15-4913,W14-4210,1,0.812988,"Missing"
W15-4913,P07-2045,0,\N,Missing
W15-4913,etchegoyhen-etal-2014-machine,0,\N,Missing
W15-4914,W05-0814,0,0.0377031,"lish sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classiﬁer are: • to estimate the error distribution within a translation output • ﬁrst four letters of the word (4let) The simplest way for word reduction is to use only its ﬁrst n letters. The choice of ﬁrst four letters has been shown to be successful for improvement of word alignments (Fraser and Marcu, 2005), therefore we decided to set n to four. • ﬁrst two thirds of the word length (2thirds) In order to take the word length into account, the words are reduced to 2/3 of their original length (rounded down). • word stem (stem) A more reﬁned method which splits words into stems and sufﬁxes based on harmonic mean of their frequencies is used, similar to the compound splitting method described Experiments and results • to compare different translation outputs in terms of error categories Therefore we tested the described methods for both these aspects by comparing the results with those obtained whe"
W15-4914,E03-1076,0,0.0295937,"for lemmas, it would not be possible to detect any inﬂectional error thus setting the inﬂectional error rate to zero, and noise would be introduced in omission, addition and mistranslation error rates. Therefore, a simple use of the full forms instead of lemmas is not advisable, especially for the highly inﬂective languages. The goal of this work is to examine possible methods for processing of the full words in a more or less simple way in order to yield a reasonable error classiﬁcation results by using them as a replacement for lemmas. Following methods for word reduction are explored: in (Koehn and Knight, 2003). The sufﬁx of each word is removed and only the stem is preserved. For calculation of stem and sufﬁx frequencies, both the translation output and its corresponding reference translation are used. Examples of two English sentences processed by each of the methods is shown in Table 1. The methods are tested on various distinct target languages and domains, some of the languages being very morphologically rich. Detailed description of the texts can be found in the next section. 3 The two main objectives of automatic error classiﬁer are: • to estimate the error distribution within a translation o"
W15-4914,2005.mtsummit-papers.11,0,0.0288445,"” error rates. The best way for the assessment would be, of course, a comparison with human error classiﬁcation. Nevertheless, this has not been done for two reasons: ﬁrst, the original method using lemmas is already thoroughly tested in previous work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable"
W15-4914,J11-4002,1,0.893667,"Missing"
W15-4914,2011.eamt-1.12,0,0.0230765,"revious work (Popovi´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-speciﬁc corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using T"
W15-4914,E09-1087,0,0.0605874,"Missing"
W15-4914,tiedemann-2012-parallel,0,0.0606606,"i´c and Ney, 2011) and is shown to correlate well with human judgements. Second, human evaluation is resource and timeconsuming. The explored target languages in this work are English, Spanish, German, Slovenian and Czech 106 originating from news, technical texts, client data of Language Service Providers, pharmaceutical domain, Europarl (Koehn, 2005), as well as the OpenSubtitles1 spoken language corpus. In addition, one Basque translation output from technical domain has been available as well. The publicly available texts are described in (Callison-Burch et al., 2011), (Specia, 2011) and (Tiedemann, 2012). The majority of translation outputs has been created by statistical systems but a number of translations has been produced by rule-based systems. It should be noted that not all target languages were available for all domains, however the total amount of texts and the diversity of languages and domains are sufﬁcient to obtain reliable results – about 36000 sentences with average number of words ranging from 8 (subtitles) through 15 (domain-speciﬁc corpora) up to 25 (Europarl and news) have been analysed. Lemmas for English, Spanish and German texts are generated using TreeTagger,2 Slovenian"
W15-4914,W11-2103,0,\N,Missing
W15-5702,2003.mtsummit-systems.1,0,0.0185567,"(6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation occurs in three phases, namely analysis, transfer, and generation. All three phases consist of hand-coded linguistic rules which have shown to perform well for capturing the structural and semantic differences between German and other language"
W15-5702,W14-3302,0,0.0706052,"Missing"
W15-5702,eisele-chen-2010-multiun,0,0.0162457,"gure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we als"
W15-5702,W11-2141,0,0.122729,"dent target language structures. A RestAPI allows the different processing steps and/or intermediate results to be influenced. Deep features for empirical enhancement Although deep techniques indicate good coverage of a number of linguistic phenomena, each of the three phases may frequently encounter serious robustness issues and/or the inability to fully process a given sentence. Erroneous analysis from early phases may aggregate along the pipeline and cause further sub-optimal choices in later phases, thus severely deteriorating the quality of the produced translation. Preliminary analysis (Federmann and Hunsicker, 2011) has shown that such is the case for source sentences that are ungrammatical in the first place or that have a very shallow syntax with many specialized lexical entries. To tackle these issues, we combine the transfer-based component with our supportive SMT engine in the following two ways: (a) train a statistical machine translation to automatically post-edit the output of the transfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that perf"
W15-5702,W12-0115,0,0.0146068,"combination produces a perfect translation. In this particular case, the machine translation (W¨ahlen Sie im Einf¨ugen Men¨u Tabelle aus) is even better than the human reference (W¨ahlen Sie im Einf¨ugen Men¨u die Tabelle aus) as the latter introduces a determiner for “table” that is not justified by the source. English Transfer-‐ based MT German* SMT German Figure 2: Serial System Combination en→de. 2.4 Parallel System Combination: Selection Mechanism The selection mechanism is based on encouraging results of previous projects including Euromatrix Plus (Federmann and Hunsicker, 2011), T4ME (Federmann, 2012), QTLaunchPad (Avramidis, 2013; Shah et al., 2013). It has been extended to include several deep features that can only be generated on a sentence level and that would otherwise blatantly increase the complexity of the transfer or decoding algorithm. In System 1, automatic syntactic and dependency analysis is employed on a sentence level, in order to choose the sentence that fulfills the basic quality aspects of the translation: (a) assert the fluency of the generated sentence, by analyzing the quality of its syntax (b) ensure its adequacy, by comparing the structures of the source with the st"
W15-5702,W11-2123,0,0.00929833,"l., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has sh"
W15-5702,popovic-ney-2006-pos,1,0.760166,"e pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield, 2011). For German to English, we also experimented with the method of pre-ordering the source side based on the target-side grammar 13 (Popovic and Ney, 2006). As a tuning set we used the news-test 2013. In our architecture, this system on its own also serves as baseline. 2.2 Transfer-based MT system: Lucy The transfer-based core of System 1 is based on the Lucy system (Alonso and Thurmair, 2003) that includes the results of long linguistic efforts over the last decades and that has successfully been used in previous projects including Euromatrix+ and QTLaunchPad. The transfer-based approach has shown good results that compete with pure statistical systems, although its focus is on translating according to linguistic structures sets. Translation oc"
W15-5702,N07-1064,0,0.0303755,"ransfer-based system (“serial combination”) (b) use the post-edited or the SMT output in cases where the transfer-based system exhibits lower performance. This is done through an empirical selection mechanism that performs real-time analysis of the produced translations and automatically selects the output that is predicted to be of a better quality (Avramidis, 2011). Figure 1 shows the overall architecture of System 1 for en→de. 2.3 Serial System Combination: Lucy+Moses For automatic post-editing of the transfer-based system, a serial Transfer+SMT system combination is used, as described in (Simard et al., 2007) The first stage is translation of the source-language part of the training corpus by the transfer-based system. The second stage is training an SMT system with the transfer-based translation output as a source language and the target-language part as a target language. Later, the test set is first translated by the transfer-based system, and the obtained translation is translated by the SMT system. Figure 2 illustrates the architecture for translation direction en→de. Note that the notion of “German*” in the figure is meant to distinguish the input and output of the SMT system. “German*” is t"
W15-5702,P13-1135,0,0.0293718,"arts from each employed method. Figure 1 shows the overall hybrid architecture that includes: • A statistical Moses system, • the commercial transfer-based system Lucy, • their serial system combination, and • an informed selection mechanism (“ranker”). The components of this hybrid system will be detailed in the sections below. 2.1 Statistical MT system: Moses Our statistical machine translation component was based on a vanilla phrase-based system built with Moses (Koehn et al., 2007) trained on the following corpora: Europarl ver. 7, News Commentary ver. 9 (Bojar et al., 2014), Commoncrawl (Smith et al., 2013), and MultiUN (Eisele and Chen, 2010) as well as on the following domain corpora: the Document Foundation (Libreoffice Help – 47K sentence pairs, Libreoffice User Interface – 35K parallel entries), the Document Foundation Terminology (690 translated terms), the Document Foundation Website (226 sentence pairs), Chromium browser (6,3K parallel entries), Ubuntu Documentation (6,3K sentence pairs), Ubuntu Saucy (183K parallel entries), and Drupal web-content management (5K parallel entries). Language models of order 5 have been built and interpolated with SRILM (Stolcke, 2002) and KenLM (Heafield,"
W16-2341,W15-3001,0,0.0631714,"Missing"
W16-2341,W11-2110,1,0.861598,"Missing"
W16-2341,W15-3049,1,0.908092,"Missing"
W16-2341,specia-etal-2010-dataset,0,0.0134853,"n-grams: (a) average τ for individual n-grams (b) τ on WMT14 (left) and WMT15 (right) documents for different n-gram weight distributions. τ coefficients for comparing four systems using direct human scores the proposed distributions outperform the uniform one for some of the texts, especially for translation out of English, none of them is unquestionably better than the uniform distribution of weights. Therefore, the uniform n-gram weights were used for the WMT16 metrics task. 4 The starting point was testing τ coefficients for CHR F2 and WORD F2 on the English→Spanish data set described in (Specia et al., 2010) and the motivation was simply to explore the correlations obtained on direct human scores instead of relative rankings. The data set contains 4000 source segments and their reference translations, machine translation outputs of four SMT systems, as well as human estimations of required post-editing effort in the interval from 1 (requires complete retranslation) to 4 (fit for purpose). The distribution of segments with each of the four human ratings for each of the systems is shown in Table 5a and it can be seen that the fourth system is significantly worse than the other three, which are rath"
W16-2341,W15-3031,0,0.120286,"Missing"
W16-3410,P07-1038,0,0.0618344,"Missing"
W16-3410,W08-0330,0,0.023172,"isniewski et al., 2013], the PEs are used as references for automatic estimation of performed edit operations, namely substitutions, deletions, insertions and shifts. [Denkowski et al., 2014] report the improvements of the BLEU scores calculated on independent references as well as on PEs in order to emphasise the suitability of their methods for the post-editing task. A number of publications deals with the usage of multiple references for automatic MT evaluation. Using pseudo-references, i.e. raw translation outputs from different MT systems has been investigated in [Albrecht and Hwa, 2007, Albrecht and Hwa, 2008] and it is shown that, even though these are not correct human translations, it is beneficiary to add pseudo-references instead of using one single reference. Adding automatically generated paraphrases together to a set of standard human references for tuning has been investigated in [Madnani et al., 2008], and it is shown that the paraphrases are improving automatic scores BLEU and TER when the number of multiple human references is less than four. Recently, multiple references have been explored in [Qin and Specia, 2015] in terms of using recurring information in these references in order t"
W16-3410,avramidis-etal-2014-taraxu,1,0.69556,"Missing"
W16-3410,W05-0909,0,0.130374,"translation (MT) output is an important and difficult task. The fastest way is to use an automatic evaluation metric, which compares the obtained output with a human translation of the same source text and calculates a numerical score related to their similarity. Despite all disadvantages and criticisms, such metrics are still irreplaceable for many tasks (such as rapid development of a new system, tuning of a statistical MT system, etc.) and are considered as at least baseline metrics for MT quality evaluation. All these metrics (n-gram based such as BLEU [Papineni et al., 2002] and METEOR [Banerjee and Lavie, 2005], edit-distance based such as TER [Snover et al., 2006], etc.) are reference-based, i.e. a human reference translation is needed as a gold standard. Since there is usually not only one single best translation of a text, the best way of evaluating an MT output would be to compare it with many references Post-edits as References 219 – nevertheless, creating each reference translation is a time consuming and expensive process. Therefore, automatic MT evaluation is usually carried out using only a single reference. On the other hand, MT has considerably improved in the recent years so that the us"
W16-3410,2013.mtsummit-papers.5,0,0.021967,"he effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In [Wisniewski et al., 2013], the PEs are used as references for automatic estimation of performed edit operations, namely substitutions, deletions, insertions and shifts. [Denk"
W16-3410,2011.mtsummit-papers.17,0,0.0301016,"rios: comparing four distinct MT systems using PEs originating from these systems, as well as comparing translations from two different source languages using PEs originating from these source languages. In addition, the effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items o"
W16-3410,E14-1042,0,0.0139424,"tiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In [Wisniewski et al., 2013], the PEs are used as references for automatic estimation of performed edit operations, namely substitutions, deletions, insertions and shifts. [Denkowski et al., 2014] repo"
W16-3410,W12-3123,0,0.0165944,"s, as well as comparing translations from two different source languages using PEs originating from these source languages. In addition, the effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In [Wisniewski et al., 2013], t"
W16-3410,2009.mtsummit-papers.9,0,0.100609,"Missing"
W16-3410,J13-4007,0,0.0358047,"Missing"
W16-3410,2008.amta-papers.13,0,0.0471425,"se the suitability of their methods for the post-editing task. A number of publications deals with the usage of multiple references for automatic MT evaluation. Using pseudo-references, i.e. raw translation outputs from different MT systems has been investigated in [Albrecht and Hwa, 2007, Albrecht and Hwa, 2008] and it is shown that, even though these are not correct human translations, it is beneficiary to add pseudo-references instead of using one single reference. Adding automatically generated paraphrases together to a set of standard human references for tuning has been investigated in [Madnani et al., 2008], and it is shown that the paraphrases are improving automatic scores BLEU and TER when the number of multiple human references is less than four. Recently, multiple references have been explored in [Qin and Specia, 2015] in terms of using recurring information in these references in order to generate better version of BLEU and NIST [Doddington, 2002] metrics by better n-gram weighting. 220 Popovi´c et al. To the best of our knowledge, no systematic investigation regarding the use of postedited translation outputs as reference translations has been carried out yet. 2 Research questions Althou"
W16-3410,2014.amta-researchers.12,0,0.0451975,"Missing"
W16-3410,P03-1021,0,0.0453732,"multiple references; – standard deviations are • dropping with increasing number of multiple references • close to zero only for more than 10 references • smaller for the MT systems of lower performance These tendencies can be equally observed for all data sets, no matter how many PEs (more similar to MT outputs) and how many independent references (less similar to MT outputs) are used. 4.4 Tuning A preliminary experiment regarding tuning on PEs originating from different source languages has been carried out using the O PEN S UBTITLES data set: (i) the translation system was tuned with MERT [Och, 2003] on BLEU using (i) the independent reference (standard method), (ii) using the PE originating from the corresponding language and (iii) using the PE originating from the other language. The results for another test set (not the one used for tuning) containing 2000 sentences6 are presented in Table 8 showing that tuning on the post-edit from the corresponding source language produces best BLEU and METEOR scores. This confirms the effect of the source language bias and indicates a potential of using PEs of a MT system for tuning and development of this system. 5 6 also available at https://gith"
W16-3410,W15-3049,1,0.88329,"Missing"
W16-3410,L16-1005,1,0.592254,"Missing"
W16-3410,W15-4915,0,0.0188542,"ifferent MT systems has been investigated in [Albrecht and Hwa, 2007, Albrecht and Hwa, 2008] and it is shown that, even though these are not correct human translations, it is beneficiary to add pseudo-references instead of using one single reference. Adding automatically generated paraphrases together to a set of standard human references for tuning has been investigated in [Madnani et al., 2008], and it is shown that the paraphrases are improving automatic scores BLEU and TER when the number of multiple human references is less than four. Recently, multiple references have been explored in [Qin and Specia, 2015] in terms of using recurring information in these references in order to generate better version of BLEU and NIST [Doddington, 2002] metrics by better n-gram weighting. 220 Popovi´c et al. To the best of our knowledge, no systematic investigation regarding the use of postedited translation outputs as reference translations has been carried out yet. 2 Research questions Although the PEs are intuitively better suitable for MT evaluation than standard human references because they are closer to the MT output structure, there are several important questions which have to be taken into account: 1."
W16-3410,2006.amta-papers.25,0,0.0576935,"The fastest way is to use an automatic evaluation metric, which compares the obtained output with a human translation of the same source text and calculates a numerical score related to their similarity. Despite all disadvantages and criticisms, such metrics are still irreplaceable for many tasks (such as rapid development of a new system, tuning of a statistical MT system, etc.) and are considered as at least baseline metrics for MT quality evaluation. All these metrics (n-gram based such as BLEU [Papineni et al., 2002] and METEOR [Banerjee and Lavie, 2005], edit-distance based such as TER [Snover et al., 2006], etc.) are reference-based, i.e. a human reference translation is needed as a gold standard. Since there is usually not only one single best translation of a text, the best way of evaluating an MT output would be to compare it with many references Post-edits as References 219 – nevertheless, creating each reference translation is a time consuming and expensive process. Therefore, automatic MT evaluation is usually carried out using only a single reference. On the other hand, MT has considerably improved in the recent years so that the use of MT outputs as a starting point for human translati"
W16-3410,2011.eamt-1.12,0,0.0263364,"as reference translations has been scarcely investigated so far. This work explores two scenarios: comparing four distinct MT systems using PEs originating from these systems, as well as comparing translations from two different source languages using PEs originating from these source languages. In addition, the effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to"
W16-3410,W15-3031,0,0.0636673,"Missing"
W16-3410,2010.jec-1.6,0,0.0494613,"This work explores two scenarios: comparing four distinct MT systems using PEs originating from these systems, as well as comparing translations from two different source languages using PEs originating from these source languages. In addition, the effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated"
W16-3410,2013.mtsummit-papers.15,0,0.038773,"omparing translations from two different source languages using PEs originating from these source languages. In addition, the effects of using multiple references are reported in terms of variations and standard deviations of automatic scores for different number of references. 1.1 Related work Post-edited translations have been used for many applications, such as automatic prediction of translation quality [Specia, 2011], analysing various aspects of post-editing effort [Tatsumi and Roturier, 2010, Blain et al., 2011], human and automatic analysis of performed edit operations [Koponen, 2012, Wisniewski et al., 2013], as well as improving translation and language model of an SMT system by learning from postedits [Bertoldi et al., 2013, Denkowski et al., 2014, Mathur et al., 2014]. The cachebased approach, introduced in [Bertoldi et al., 2013], makes it possible to periodically add knowledge from PEs into an SMT system in real-time, without the need to stop it. The main idea behind the cache-based models is to mix a large global (static) model with a small local (dynamic) model estimated from recent items observed in the history of the input stream. In [Wisniewski et al., 2013], the PEs are used as refere"
W16-3411,E14-1076,0,0.186775,") could lead to improvements in the quality of machine translation (Chandrasekar, 1994). Since then, a great number of ATS systems has been proposed not only for English, but also for other languages, e.g. Basque (Aranzabe et al., 2013), Portuguese (Specia, 2010), Spanish (Saggion et al., 2015), French (Brauwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or only syntactic (Siddharthan, 2011) simplification, to those combining lexical and syntactic simplification (Angrosh and Siddharthan, 2014). Recently, several ATS systems have been proposed which do not only simplify given text/sentences but also reduce the amount of information contained by removing highlevel details, such as appositions, adverbial phrases, or purely descriptive sentences ˇ ((Glavaˇs and Stajner, 2013), (Siddharthan et al., 2014), (Narayan and Gardent, 2014)). However, in these twenty years, the motivation for building ATS systems has shifted from improving text processing systems to making texts more accessible to wider audiences (e.g. children, non-native speakers, people with low literacy levels, and people w"
W16-3411,W14-1206,0,0.0740635,"Missing"
W16-3411,C96-2183,0,0.776808,"s the results of our experiments, while Section 5 summarises the main findings and presents ideas for future research. 2 Related Work Automatic text simplification (ATS) systems aim to transform original texts into their lexically and syntactically simpler variants. In theory, they could also simplify texts on the discourse level, but most of the systems still operate only on the sentence level. The motivation for building the first ATS systems was to improve the performance of machine translation systems and other text processing tasks, e.g. parsing, information retrieval, and summarisation (Chandrasekar et al., 1996). It was argued that simplified sentences (which have simpler sentential structures and reduced ambiguity) could lead to improvements in the quality of machine translation (Chandrasekar, 1994). Since then, a great number of ATS systems has been proposed not only for English, but also for other languages, e.g. Basque (Aranzabe et al., 2013), Portuguese (Specia, 2010), Spanish (Saggion et al., 2015), French (Brauwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or onl"
W16-3411,W14-3348,0,0.0158963,"uments and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (retaining the same proportions of the sentences from the three corpora as in the training dataset). The English-to-Serbian part of the ASISTENT system (Arˇcan et al., 2016) was tested on 2,000 sentences from the three corpora used for training and tuning (the 2,000 sentences which were not used for training and tuning) and achieved a 38.88 BLEU score (Papineni et al., 2002), a 31.18 METEOR score (Denkowski and Lavie, 2014), and a 61.62 chrF3 score (Popovi´c, 2015). 3.3 Evaluation Procedure From the initial set of 100 news articles, we randomly selected 65 original sentences and evaluated all translation outputs (from original sentences, and TS-A and TS-C systems, which led to a total of 195 target sentences) with respect to the following aspects: 1. adequacy, i.e. meaning preservation 2. fluency, i.e. grammaticality 3. technical post-editing effort, i.e. amount of necessary edit operations Each of the tasks has been carried out separately, i.e. the evaluation of adequacy and fluency were carried out in two sepa"
W16-3411,R13-2011,1,0.876726,"Missing"
W16-3411,P15-2011,1,0.866013,"Missing"
W16-3411,N03-1017,0,0.0232797,"ar-old man was arrested on April 30 on suspicion. A 21-year-old man was released on jail until May 29. TS-A (PE) A 21-year-old man was arrested on April 30 on suspicion of murder. A 21-year-old man was released on bail until May 29. This subset of sentences was later used for MT experiments and human evaluation and postediting. Can Text Simplification help Machine Translation? 3.2 235 Statistical Machine Translation System For the machine translation from English to Serbian, we used the ASISTENT system.6 It is a freely available SMT system, based on the widely used phrase-based SMT framework (Koehn et al., 2003) and it supports translations from English to Slovene, Croatian and Serbian and vice versa. Additionally, translations between those three Slavic languages are also possible. The system was trained using the Moses toolkit (Koehn et al., 2007). The word alignments were built with GIZA++ (Och and Ney, 2003), and the 5-gram language model was built using the SRILM toolkit (Stolcke, 2002) The training dataset originates from the OPUS website7 (Tiedemann, 2012) where three domains were available for the Serbian-English language pair: the enhanced version of the SEtimes corpus8 (Tyers and Alperen, 2"
W16-3411,P14-1041,0,0.0732302,"lacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or only syntactic (Siddharthan, 2011) simplification, to those combining lexical and syntactic simplification (Angrosh and Siddharthan, 2014). Recently, several ATS systems have been proposed which do not only simplify given text/sentences but also reduce the amount of information contained by removing highlevel details, such as appositions, adverbial phrases, or purely descriptive sentences ˇ ((Glavaˇs and Stajner, 2013), (Siddharthan et al., 2014), (Narayan and Gardent, 2014)). However, in these twenty years, the motivation for building ATS systems has shifted from improving text processing systems to making texts more accessible to wider audiences (e.g. children, non-native speakers, people with low literacy levels, and people with various language or learning disabilities). Therefore, ATS systems have only been evaluated for the quality of the generated output, its readability levels, and usefulness in making texts more accessible to target populations (reducing reading speed and improving comprehension). To the best of our knowledge, there has been no evaluatio"
W16-3411,J03-1002,0,0.00520082,"ation and postediting. Can Text Simplification help Machine Translation? 3.2 235 Statistical Machine Translation System For the machine translation from English to Serbian, we used the ASISTENT system.6 It is a freely available SMT system, based on the widely used phrase-based SMT framework (Koehn et al., 2003) and it supports translations from English to Slovene, Croatian and Serbian and vice versa. Additionally, translations between those three Slavic languages are also possible. The system was trained using the Moses toolkit (Koehn et al., 2007). The word alignments were built with GIZA++ (Och and Ney, 2003), and the 5-gram language model was built using the SRILM toolkit (Stolcke, 2002) The training dataset originates from the OPUS website7 (Tiedemann, 2012) where three domains were available for the Serbian-English language pair: the enhanced version of the SEtimes corpus8 (Tyers and Alperen, 2010) containing “news and views from South-East Europe”, OpenSubtitles9 , and the KDE localisation documents and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (reta"
W16-3411,P02-1040,0,0.103535,"OpenSubtitles9 , and the KDE localisation documents and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (retaining the same proportions of the sentences from the three corpora as in the training dataset). The English-to-Serbian part of the ASISTENT system (Arˇcan et al., 2016) was tested on 2,000 sentences from the three corpora used for training and tuning (the 2,000 sentences which were not used for training and tuning) and achieved a 38.88 BLEU score (Papineni et al., 2002), a 31.18 METEOR score (Denkowski and Lavie, 2014), and a 61.62 chrF3 score (Popovi´c, 2015). 3.3 Evaluation Procedure From the initial set of 100 news articles, we randomly selected 65 original sentences and evaluated all translation outputs (from original sentences, and TS-A and TS-C systems, which led to a total of 195 target sentences) with respect to the following aspects: 1. adequacy, i.e. meaning preservation 2. fluency, i.e. grammaticality 3. technical post-editing effort, i.e. amount of necessary edit operations Each of the tasks has been carried out separately, i.e. the evaluation of"
W16-3411,W15-3049,1,0.898515,"Missing"
W16-3411,W15-4913,1,0.914144,"Missing"
W16-3411,W11-2802,0,0.130393,"ed that simplified sentences (which have simpler sentential structures and reduced ambiguity) could lead to improvements in the quality of machine translation (Chandrasekar, 1994). Since then, a great number of ATS systems has been proposed not only for English, but also for other languages, e.g. Basque (Aranzabe et al., 2013), Portuguese (Specia, 2010), Spanish (Saggion et al., 2015), French (Brauwers et al., 2014), and Italian (Barlacchi and Tonelli, 2013). For English, the state-of-the-art ATS systems range from those performing only lexˇ ical (Glavaˇs and Stajner, 2015) or only syntactic (Siddharthan, 2011) simplification, to those combining lexical and syntactic simplification (Angrosh and Siddharthan, 2014). Recently, several ATS systems have been proposed which do not only simplify given text/sentences but also reduce the amount of information contained by removing highlevel details, such as appositions, adverbial phrases, or purely descriptive sentences ˇ ((Glavaˇs and Stajner, 2013), (Siddharthan et al., 2014), (Narayan and Gardent, 2014)). However, in these twenty years, the motivation for building ATS systems has shifted from improving text processing systems to making texts more accessib"
W16-3411,tiedemann-2012-parallel,0,0.0118122,"lish to Serbian, we used the ASISTENT system.6 It is a freely available SMT system, based on the widely used phrase-based SMT framework (Koehn et al., 2003) and it supports translations from English to Slovene, Croatian and Serbian and vice versa. Additionally, translations between those three Slavic languages are also possible. The system was trained using the Moses toolkit (Koehn et al., 2007). The word alignments were built with GIZA++ (Och and Ney, 2003), and the 5-gram language model was built using the SRILM toolkit (Stolcke, 2002) The training dataset originates from the OPUS website7 (Tiedemann, 2012) where three domains were available for the Serbian-English language pair: the enhanced version of the SEtimes corpus8 (Tyers and Alperen, 2010) containing “news and views from South-East Europe”, OpenSubtitles9 , and the KDE localisation documents and manuals, i.e. technical domain. Approximately 20.7M sentences, in total, were used for training (20.5M subtitles, 200,000 news, 30,000 technical), and 2,000 sentences were used for tuning (retaining the same proportions of the sentences from the three corpora as in the training dataset). The English-to-Serbian part of the ASISTENT system (Arˇcan"
W16-4806,2007.mtsummit-papers.5,0,0.059558,"other without switching to the foreign language. Furthermore, many documents are distributed in their original language, even in the neighbouring countries. Another fact is that MT between related languages is less problematic than between distant languages (Kolovratn´ık et al., 2009). Still, there is a need for translation even between very closely related language pairs such as Serbian and Croatian, for example, for the sake of producing standard official documents which exixst in one language but not the other. Another application of such systems is the two-stage (also called “pivot”) MT (Babych et al., 2007): for example, if an adequate English-Croatian system is available whereas an English-Serbian system is not, or is of poor quality, English source sentences can first be translated into Croatian, and then the obtained output is further translated into Serbian by a Croatian-Serbian MT system. A similar application can also include enriching parallel training corpora by producing “synthetic” data in the less resourced related language (Bertoldi and Federico, 2009). This work examines MT systems between three closely related South Slavic languages, namely Croatian, Serbian and Slovenian. Therefor"
W16-4806,W09-0432,0,0.0262873,"cial documents which exixst in one language but not the other. Another application of such systems is the two-stage (also called “pivot”) MT (Babych et al., 2007): for example, if an adequate English-Croatian system is available whereas an English-Serbian system is not, or is of poor quality, English source sentences can first be translated into Croatian, and then the obtained output is further translated into Serbian by a Croatian-Serbian MT system. A similar application can also include enriching parallel training corpora by producing “synthetic” data in the less resourced related language (Bertoldi and Federico, 2009). This work examines MT systems between three closely related South Slavic languages, namely Croatian, Serbian and Slovenian. Therefore we used the Asistent1 phrase-based translation system (Arˇcan et al., 2016), which was developed to translate text between English and the morphological complex south Slavic languages: Slovene, Serbian and Croatian. Additionally, an RBMT system2 (Klubiˇcka et al., 2016) is analysed for translation between Croatian and Serbian in both directions in order to explore advantages and disadvantages of both approaches for very close language pairs. This work is licen"
W16-4806,W11-2123,0,0.0261644,"sidered as a false friend. Another important difference is the Slovenian dual grammatical number which refers to two entities (apart from singular for one and plural for more than two). It requires additional sets for noun, adjective and verb inflexion rules not existing either in Croatian or in Serbian. 3 Experimental set-up 3.1 Machine translation systems The statistical phrase-based systems (Koehn, 2004) were trained using the Moses toolkit (Koehn et al., 2007) with MERT tuning. The word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with kenLM (Heafield, 2011). The parallel texts used to train the SMT systems were mostly obtained from the OPUS5 web site (Tiedemann, 2009), which contains various corpora of different sizes and domains. Although corpora in distinct domains, e.g., legal, medical, financial, IT, exist for many language pairs including some of the South Slavic languages and English, parallel data between South Slavic languages pairs consist mostly of the OpenSubtitles6 corpus and a little portion of the technical domain. For Serbian-Croatian language pair, the SEtimes corpus from the news domain (Tyers and Alperen, 2010) is also availabl"
W16-4806,W16-3422,1,0.816705,"Missing"
W16-4806,koen-2004-pharaoh,0,0.103908,"the construction tudi ne is used, whereas in Croatian and Serbian a negation conjunction ni is used. Slovenian conjunction pa also has different usage and structural requirements, and it can also be considered as a false friend. Another important difference is the Slovenian dual grammatical number which refers to two entities (apart from singular for one and plural for more than two). It requires additional sets for noun, adjective and verb inflexion rules not existing either in Croatian or in Serbian. 3 Experimental set-up 3.1 Machine translation systems The statistical phrase-based systems (Koehn, 2004) were trained using the Moses toolkit (Koehn et al., 2007) with MERT tuning. The word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with kenLM (Heafield, 2011). The parallel texts used to train the SMT systems were mostly obtained from the OPUS5 web site (Tiedemann, 2009), which contains various corpora of different sizes and domains. Although corpora in distinct domains, e.g., legal, medical, financial, IT, exist for many language pairs including some of the South Slavic languages and English, parallel data between South Slavic languages pairs con"
W16-4806,W14-4212,0,0.0520977,"Missing"
W16-4806,J03-1002,0,0.0086579,"s different usage and structural requirements, and it can also be considered as a false friend. Another important difference is the Slovenian dual grammatical number which refers to two entities (apart from singular for one and plural for more than two). It requires additional sets for noun, adjective and verb inflexion rules not existing either in Croatian or in Serbian. 3 Experimental set-up 3.1 Machine translation systems The statistical phrase-based systems (Koehn, 2004) were trained using the Moses toolkit (Koehn et al., 2007) with MERT tuning. The word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with kenLM (Heafield, 2011). The parallel texts used to train the SMT systems were mostly obtained from the OPUS5 web site (Tiedemann, 2009), which contains various corpora of different sizes and domains. Although corpora in distinct domains, e.g., legal, medical, financial, IT, exist for many language pairs including some of the South Slavic languages and English, parallel data between South Slavic languages pairs consist mostly of the OpenSubtitles6 corpus and a little portion of the technical domain. For Serbian-Croatian language pair, the SEtimes corp"
W16-4806,P02-1040,0,0.0948478,"nces was post-edited as well. The test sets were post-edited for two reasons: 1. post-edited data are generally more convenient for analysis and identifying prominent errors and issues; 2. the OpenSubtitles contain translations from English the as original source so that the obtained translations are often too different and do not fully reflect the language closeness. Although it was not the motivation for post-editing, it should be noted that there were no available reference translations for Croatian-Serbian additional test sets. 3.3 Evaluation For all test sets and MT systems, BLEU scores (Papineni et al., 2002) and character n-gram F-scores CHR F3 (Popovi´c, 2015) are reported. BLEU is a well-known and widely used metric, and CHR F3 is shown to correlate very well with human judgments for morphologically rich languages (Stanojevi´c et al., 2015). Besides, it seems convenient for closely related languages since a large portion of differences is on the character level. In order to better understand the overall evaluation scores and differences between the MT systems, five error classes, produced by the automatic error analysis tool Hjerson (Popovi´c, 2011), are reported. Finally, in order to determine"
W16-4806,W15-4913,1,0.890673,"Missing"
W16-4806,W14-4210,1,0.88454,"Missing"
W16-4806,W15-3049,1,0.895301,"Missing"
W16-4806,W15-3031,0,0.0636686,"Missing"
W16-4806,W16-3423,0,0.0483922,"Missing"
W16-4813,2013.iwslt-evaluation.1,0,0.0459683,"Missing"
W16-4813,W16-3422,1,0.61955,"Missing"
W16-4813,J03-1002,0,0.00529792,"12k parallel segments were extracted, and for English-Serbian about 50k. An interesting observation is that although Croatian is generally better supported in terms of publicly available parallel data,6 Serbian is currently better supported for educational parallel texts. As for the out-of-domain corpus, we used the SETimes news corpus (Tyers and Alperen, 2010) since it is relatively large (200k parallel sentences) and clean. Moses set-ups We trained the statistical phrase-based systems using the Moses toolkit (Koehn et al., 2007) with MERT tuning. The word alignments were built with GIZA++ (Och and Ney, 2003) and a 5-gram language model was built with SRILM (Stolcke, 2002). The investigated bilingual training set-ups are: 1. en-hr SEtimes (relatively large clean out-of-domain corpus) 2. en-hr Coursera (small in-domain corpus) 3. en-hr Coursera (small in-domain corpus) + en-sr Coursera (larger in-domain corpus) 4. en-hr Coursera + en-hr’ Coursera 5. en-hr SEtimes + en-hr Coursera + en-hr’ Coursera 6 http://opus.lingfil.uu.se/ 100 sentences Training Dev Test 1) setimes 2) coursera 3) 2+coursera en-sr 4) 2+coursera en-hr’ 5) 1+4 coursera coursera 206k 12k 62k 62k 268k 2935 2091 running words en hr 4."
W16-4813,P02-1040,0,0.0956083,"Missing"
W16-4813,W14-4210,1,0.479533,"Missing"
W16-4813,W15-3049,1,0.871794,"Missing"
W16-4813,W16-3421,1,0.40739,"Missing"
W16-4813,2014.eamt-1.45,0,0.669565,"Missing"
W16-4813,W16-3423,0,0.0385996,"Missing"
W17-4770,W15-3049,1,0.331548,"Missing"
W17-4770,W16-2341,1,0.852907,"Missing"
W17-4770,W16-2322,0,0.0419573,"Missing"
W17-4770,W15-3031,0,0.059424,"Missing"
W18-0541,W17-4755,0,0.0249842,"Missing"
W18-0541,P15-2011,0,0.0628113,"Missing"
W18-0541,I17-4027,0,0.0595004,"Missing"
W18-0541,S16-1152,0,0.0665202,"Missing"
W18-0541,P15-4015,0,0.171906,"Missing"
W18-0541,W15-3049,1,0.905072,"Missing"
W18-0541,P13-3015,0,0.164103,"ex and three simple English words with their 2-grams, 3-grams and 4-grams and corresponding frequencies. Under the (very) naive assumption of conditional independence between individual n-grams, these frequencies are then used for estimating the classcontidition probabilities of the Naive Bayes multinomial model: Although the relation between character ngrams and word complexity intuitively depends on the language, we still decided to investigate crosslingual CWI and to participate in this track. 1.1 Related work Several different techniques for identifying complex words were investigated by (Shardlow, 2013) which include word frequency, word length and syllable counts among others, but no character sequences. The first CWI shared task (Paetzold and Specia, 2016) featured 42 systems based on different techniques and using different features such as semantic, morphological, lexical, as well as word frequencies which are reported to be a very important factor for CWI. One of the submitted systems (Mukherjee et al., 2016) used Naive Bayes classifier with morphological, semantic and lexical features, however no character n-grams were investigated. Another system (Zampieri et al., 2016) used probabili"
W18-0541,S16-1155,0,0.216291,"Missing"
W18-0541,W14-3354,0,0.0489213,"Missing"
W18-0541,W16-2342,0,0.0416551,"Missing"
W18-0541,W18-0507,0,0.101978,"Missing"
W18-0541,I17-2068,0,0.343916,"cter n-grams were investigated. Another system (Zampieri et al., 2016) used probabilities of word character trigrams and sentence character trigrams together with word length and sentence length to measure orthographic difficulty. These features together with the word frequency features are used for three classifiers: Random Forest, Nearest neighbour and SVM. Nevertheless, no results regarding the contribution of character trigram features were reported. Number of vowels, number of syllables and number of characters (word length) together with word frequencies in corpora were investigated in (Yimam et al., 2017b), but no experiments with character n-grams were conducted. 2 Nngr cˆ = arg max P (c) c Y i=1 P (ngri |c) (1) where P (ngri |c) is the conditional probability that the n-gram ngri occurs in a word with the class value c, and Nngr is the total number of distinct n-grams, i.e. the dimension of the feature vector. P (c) is the prior probability that a word has class label c. For the multinomial model, these two probabilities can be estimated as relative frequencies in the following way: count(ngri , c) Pˆ (ngri |c) = PN gr i=1 count(ngri , c) (2) where the numerator represents the number of occ"
W18-0541,yimam-etal-2017-multilingual,0,0.151918,"cter n-grams were investigated. Another system (Zampieri et al., 2016) used probabilities of word character trigrams and sentence character trigrams together with word length and sentence length to measure orthographic difficulty. These features together with the word frequency features are used for three classifiers: Random Forest, Nearest neighbour and SVM. Nevertheless, no results regarding the contribution of character trigram features were reported. Number of vowels, number of syllables and number of characters (word length) together with word frequencies in corpora were investigated in (Yimam et al., 2017b), but no experiments with character n-grams were conducted. 2 Nngr cˆ = arg max P (c) c Y i=1 P (ngri |c) (1) where P (ngri |c) is the conditional probability that the n-gram ngri occurs in a word with the class value c, and Nngr is the total number of distinct n-grams, i.e. the dimension of the feature vector. P (c) is the prior probability that a word has class label c. For the multinomial model, these two probabilities can be estimated as relative frequencies in the following way: count(ngri , c) Pˆ (ngri |c) = PN gr i=1 count(ngri , c) (2) where the numerator represents the number of occ"
W18-7006,de-marneffe-etal-2006-generating,0,0.243939,"Missing"
W18-7006,R13-2011,1,0.893703,"Missing"
W18-7006,P15-2011,1,0.923176,"Missing"
W18-7006,2010.eamt-1.31,0,0.0117803,"ssary. We also explore in which way simplification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification"
W18-7006,P09-1089,0,0.0346317,"ifications where necessary. We also explore in which way simplification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic"
W18-7006,N15-1156,0,0.0452024,"Missing"
W18-7006,2013.mtsummit-posters.8,0,0.221298,"lification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification systems are usually divided into lexical"
W18-7006,P13-4015,0,0.751716,"lification of relative clauses can improve the quality of translations, and which types of English relative clauses pose problems to machine translation into Serbian and German. We focus on English-to-Serbian and English-toGerman machine translation, as both target languages are morphologically rich and structurally different from English. 2 2.1 2.2 ATS for Improving MT Many works have so far proposed to rewrite input sentences using paraphrasing or textual entailment to improve machine translation, e.g. (CallisonBurch et al., 2006; Mirkin et al., 2009; Aziz et al., 2010; Tyagi et al., 2015). Mirkin et al. (2013a,b) go one step further, proposing an interactive tool which identifies sentences which are most likely to be translated poorly, offers possible rewritings for the human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification systems are usually divided into lexical"
W18-7006,N15-2002,0,0.0442498,"Missing"
W18-7006,W14-1206,0,0.0442268,"Missing"
W18-7006,W15-3049,1,0.89239,"Missing"
W18-7006,N06-1003,0,0.169772,"Missing"
W18-7006,C96-2183,0,0.579378,"post-edit operations) to obtain correct translation. We find that larger improvements can be achieved for more complex target languages, as well as for MT systems with lower overall performance. The improvements mainly originate from correctly simplified sentences with relatively complex structure, while simpler structures are already translated sufficiently well using the original source sentences. 1 Introduction Text simplification (TS) was initially proposed in the late nineties as a pre-processing step that would improve machine translation (MT), information extraction (IE), and parsing (Chandrasekar et al., 1996). At that time, text simplification was done manually and focused mainly on syntactic transformations. In the last 20 years, many automatic text simplification (ATS) systems were proposed for various languages. Most of them were done with the goal of making texts more understandable to humans. The most mature systems are those proposed for English language. The initial goal of using automatic syntactic simplification for improving MT systems has been forgotten, with the only exception being the recent work ˇ of Stajner and Popovi´c (2016), where two lexicosyntactic ATS systems were used for tr"
W18-7006,W11-2802,0,0.299523,"human editor, and then performs translation. Although such approach requires some human post-editing effort, the effort is just monolingual (at the source side only). All these approaches, although being proposed and tested on different language pairs (English-French, EnglishSpanish, English-Hindu), only focus on out-ofRelated work Automatic Text Simplification Automatic text simplification systems are usually divided into lexical simplification (LS) systems ˇ (e.g. (Baeza-Yates et al., 2015; Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016)), syntactic simplification (SS) systems (e.g. (Siddharthan, 2011; ˇ Aranzabe et al., 2012; Glavaˇs and Stajner, 2013; Brouwers et al., 2014)), and lexico-syntactic simplification (LSS) systems (e.g. (Siddharthan and 40 score 5 vocabulary words, or difficult to translate shorter n-grams. ˇ The recent work of Stajner and Popovi´c (2016), investigated the impact of lexico-syntactic automatic text simplification systems on Englishto-Serbian machine translation. They used two lexico-simplification systems: the EvLex system ˇ (Stajner and Glavaˇs, 2017) which performs sentence splitting, lexical substitution, and content reduction, and a “classical” lexico-synta"
W18-7006,E14-1076,0,0.570372,"tate-of-the-art lexical simplificaˇ tion systems are unsupervised (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016), and although they have a decent coverage (better than the supervised LS systems) they often lead to ungrammatiˇ cal output or change of original meaning (Stajner and Glavaˇs, 2017). The changes in meaning are not subtle, but rather essential, and as such, those systems are suitable as a preprocessing step in machine translation only with a manual correction of ˇ their output (Stajner and Popovi´c, 2016). The state-of-the-art syntactic simplification systems are rule-based (Siddharthan and Angrosh, 2014; Saggion et al., 2015), and as such, provide more grammatical output, at the cost of being too conservative and often not making any changes at all. Out of all syntactic simplification operations, simplification of the relative clauses is the most studied and the most reliable one, especially for English. Therefore, in this study, we focus only on this type of transformations hoping to minimize the necessity for manually correcting simplification output. Cameron’s submitted text reads in part like a plot summary of the Lorax film provided on the Internet Movie Database website, which begins:"
W18-7006,E17-1100,0,0.0452159,"Missing"
W18-7006,W16-3411,1,0.906631,"Missing"
W19-5353,E83-1013,0,0.470436,"Missing"
W19-5353,W18-6307,0,0.133501,"Missing"
W19-5353,W17-4702,0,0.0504458,"Missing"
W19-5353,W18-6437,0,0.0326164,"Missing"
W19-5353,tiedemann-2012-parallel,0,0.071749,"Missing"
W19-6609,D16-1025,0,0.0128627,"(Popovi´c and Ney, 2011; Zeman et al., 2011), as a way to identify weaknesses of the systems and define priorities for their improvement, has received a fair amount of attention in the MT community. Although automatic error classification still cannot deal with fine-grained error taxonomies, it represents a valuable tool for fast and large scale translation error analysis. With the emergence of neural MT systems, first insights about the differences between the neural approach and the then stateof-the-art statistical phrase-based approach were obtained by using automatic error classification. Bentivogli et al. (2016) analyzed four MT systems for English into German by comparing different TER (Snover et al., 2006) scores and sub-scores, and Toral and S´anchez-Cartagena (2017) applied the WER-based approach proposed by Popovi´c and Ney (2011) for a multilingual and multifaceted evaluation of eighteen MT systems for nine translation directions including six languages from four different families. So far, automatic error classification is based on hard decisions about the error class for a given word. Addicter (Zeman et al., 2011) uses a first-order Markov model for aligning reference words with hypothesis wo"
W19-6609,2010.eamt-1.12,0,0.0582698,"Missing"
W19-6609,C18-2019,0,0.0204232,"labels to each word. This work presents first results of a new error classification method, which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well a"
W19-6609,fishel-etal-2012-terra,1,0.870352,"Missing"
W19-6609,P02-1040,0,0.108956,"rst results of a new error classification method, which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well as automatic (Popovi´c and Ney, 2011; Zeman et a"
W19-6609,L16-1005,1,0.882883,"Missing"
W19-6609,2011.eamt-1.36,1,0.549007,"Missing"
W19-6609,J11-4002,1,0.846879,"Missing"
W19-6609,W15-3049,1,0.893383,"Missing"
W19-6609,2006.amta-papers.25,0,0.51403,"which assigns multiple error labels to each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well as automatic (Popovi´c and Ney, 2011; Zeman et al., 2011), as a way to identify weaknesses of th"
W19-6609,E17-1100,0,0.0246038,"Missing"
W19-6609,vilar-etal-2006-error,1,0.779809,"Missing"
W19-6609,W16-2342,0,0.0129748,"o each word. We assign fractional counts for each label, which can be interpreted as a confidence for the label. Our method generates sensible multi-error suggestions, and improves the correlation between manual and automatic error distributions. 1 Introduction Translations produced by machine translation (MT) systems have been evaluated mostly in terms of overall performance scores, either by manual evaluations (ALPAC, 1966; White et al., 1994; Graham et al., 2017; Federmann, 2018) or by automatic metrics (Papineni et al., 2002; Lavie and Denkowski, 2009; Snover et al., 2006; Popovi´c, 2015; Wang et al., 2016). All these overall scores give an indication of the general performance of a given system, but they do not provide any additional information. Translation error analysis, both manual (Vilar et al., 2006; Farr´us et al., 2010; Lommel et al., c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 David Vilar Amazon Germany dvilar@amazon.com 2014b) as well as automatic (Popovi´c and Ney, 2011; Zeman et al., 2011), as a way to identify weaknesses of the systems and define priorities for"
W19-6712,ahrenberg-2017-comparing,0,0.117809,"human reference translations is largely beneficial for metrics’ performance (Albrecht and Hwa, 2008; Popovi´c et al., 2016b). In practice, however, only one reference HT is available and its characteristics can strongly affect the results of automatic evaluation. A lot of work has been done exploring differences between different types of texts, such as between texts originally written in a given language and texts translated into a given language (“translationese”) (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Wintner, 2016; Daems et al., 2017), human and machine translations (Ahrenberg, 2017), as well as post-edited MT Dublin, Aug. 19-23, 2019 |p. 80 ˇ outputs (PEs) as a special case of HT (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018). Overall, the main findings are that HTs differ from original language texts because the source language seems to always “leave a trace” in the translation (“shine through”); similarly, PE, although generally capable of reaching the same quality as HT, always carries a “trace” of the used MT system. Less work can be found about relations between these facts and the evaluation of machine translation. Popovi´c et al. (2016b) compared the u"
W19-6712,avramidis-etal-2014-taraxu,1,0.896647,"Missing"
W19-6712,W18-6401,0,0.0259343,") and Fomicheva et al. (2015). Ahrenberg (2006) proposes a method to identify “simpler” HTs which have certain desirable properties and are not too complex for MT systems. Fomicheva et al. (2015) investigate rulebased paraphrasing methods to reduce shifts in HTs and generate additional reference translations. Nevertheless, there is still a lot of room for systematic and extensive experiments dealing with different HTs and their relation to the MT evaluation. For example, even though a large number of HTs intended for MT evaluation have been generated in the framework of the WMT shared tasks1 (Bojar et al., 2018; Bojar et al., 2017) running since 2006 until the present, no information can be found about the translators, such as how many segments did each individual translate, what are their qualifications, what translation experience or credentials do they have, how are they linked to the domain(s) of the data sets, whether they were aware of the purpose of their work and whether it had any influence. This work reports the first qualitative feedback related to this topic. The work is based on the author’s translation experience related to the evaluation of MT output compared to the experience unrelat"
W19-6712,W16-3401,0,0.293413,", 2016b). In practice, however, only one reference HT is available and its characteristics can strongly affect the results of automatic evaluation. A lot of work has been done exploring differences between different types of texts, such as between texts originally written in a given language and texts translated into a given language (“translationese”) (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Wintner, 2016; Daems et al., 2017), human and machine translations (Ahrenberg, 2017), as well as post-edited MT Dublin, Aug. 19-23, 2019 |p. 80 ˇ outputs (PEs) as a special case of HT (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018). Overall, the main findings are that HTs differ from original language texts because the source language seems to always “leave a trace” in the translation (“shine through”); similarly, PE, although generally capable of reaching the same quality as HT, always carries a “trace” of the used MT system. Less work can be found about relations between these facts and the evaluation of machine translation. Popovi´c et al. (2016b) compared the use of PEs and HTs, and suggest that PEs should be used carefully for MT evaluation due to the bias of each PE towards its"
W19-6712,L16-1005,1,0.900772,"Missing"
W19-6712,2014.eamt-1.42,0,0.0904979,"Missing"
W19-6712,W16-4806,1,0.898164,"Missing"
W19-6712,cyrus-2006-building,0,0.0566389,"ons in this direction, fewer translation shifts were performed than when translating for other purposes. This finding will hopefully initialise further systematic research both from the aspect of MT as well as from the aspect of translation studies (TS) and bring translation theory and MT closer together. 1 Introduction and Motivation The notion of translation shifts (Catford, 1965; van Leuven-Zwart, 1989; van Leuven-Zwart, 1990; c 2019 The authors. This article is licensed under a Creative Commons 4.0 licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 2 Curys, 2006) is an important concept in translation theory. A shift has ocurred if the translation procedure has been “oblique” instead of “direct/literal” (Vinay and Darbelnet, 1958) so that there are “departures from formal correspondence” (Catford, 1965). A translated text in a target language can differ from the original text in the source language in many aspects and levels (such as lexical, structural, discourse, etc.) and still be perfectly acceptable. Apart from the transformations necessary for the sake of grammatical wellformedness, it is common practice to introduce optional changes to adapt th"
W19-6712,W16-3410,1,0.744366,"Missing"
W19-6712,W13-2713,0,0.0245667,"lation in order to maximally preserve the original linguistic effect, whereas “domesticating” should be implemented in technical translation in order to ensure immediate intelligibility. With the emergence of MT technologies, the (positive and negative aspects of) literality might be revisited including the additional MT point of view. Several general strategies for approaching MT from the perspectives of TS were proposed ˇ by Culo (2014), although connecting translation procedures and MT has not been mentioned. A Proceedings of MT Summit XVII, volume 2 step in this direction is described in (Jones and Irvine, 2013), where the authors investigate potentials and limits of statistical MT to perform literal vs. oblique translation. Exploring the new state-of-the-art MT approach, namely neural machine translation (NMT), in this sense would be a very interesting line of research, especially taking into account the general ability of NMT systems to produce fluent translations. It can be supposed that some of the free/flexible/oblique translations would be easier for an (N)MT system to generate than some others. A systematic analysis of translation shifts, possibly including different types of text (for example"
W19-6712,W19-3715,1,0.819679,"Missing"
W19-6712,Q15-1030,0,0.0792227,"ation metrics is based on similarity between MT and HT, and availability of a heterogeneous set of human reference translations is largely beneficial for metrics’ performance (Albrecht and Hwa, 2008; Popovi´c et al., 2016b). In practice, however, only one reference HT is available and its characteristics can strongly affect the results of automatic evaluation. A lot of work has been done exploring differences between different types of texts, such as between texts originally written in a given language and texts translated into a given language (“translationese”) (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Wintner, 2016; Daems et al., 2017), human and machine translations (Ahrenberg, 2017), as well as post-edited MT Dublin, Aug. 19-23, 2019 |p. 80 ˇ outputs (PEs) as a special case of HT (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018). Overall, the main findings are that HTs differ from original language texts because the source language seems to always “leave a trace” in the translation (“shine through”); similarly, PE, although generally capable of reaching the same quality as HT, always carries a “trace” of the used MT system. Less work can be found about relations between these f"
W19-6712,W18-6312,0,0.0871749,"s through the use of automatic evaluation metrics (Lohar et al., 2019). During this translation process, the author noticed that a number of translations could feel much more natural if they diverged from the close (literal) translations, however she abstained from introducing these shifts knowing that the final goal of the translation was evaluating an MT system. An important fact is that none of the MT evaluations included comparison between MT quality and HT quality in order to estimate the remaining gap (Toral and Way, 2018), or to claim “human parity” (Hassan et al., 2018) (reassessed by Toral et al. (2018)) “cracking NMT”2 or similar. 3 Observations on translation shifts The main observation about both translation and post-editing processes is a tendency towards a balance between two antagonised aspects: maximal similarity between source and translated texts and naturalness of the generated text in the target language. On the one hand, paraphrasing the close version and shifting away from the source is nor2 https://www.sdl.com/blog/sdl-cracks-russian-neuralmachine-translation.html Proceedings of MT Summit XVII, volume 2 mal and natural, and the most common in practice. On the other hand, keepin"
W19-6712,C16-3005,0,0.0177682,"larity between MT and HT, and availability of a heterogeneous set of human reference translations is largely beneficial for metrics’ performance (Albrecht and Hwa, 2008; Popovi´c et al., 2016b). In practice, however, only one reference HT is available and its characteristics can strongly affect the results of automatic evaluation. A lot of work has been done exploring differences between different types of texts, such as between texts originally written in a given language and texts translated into a given language (“translationese”) (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Wintner, 2016; Daems et al., 2017), human and machine translations (Ahrenberg, 2017), as well as post-edited MT Dublin, Aug. 19-23, 2019 |p. 80 ˇ outputs (PEs) as a special case of HT (Culo and Nitzke, 2016; Daems et al., 2017; Farrell, 2018). Overall, the main findings are that HTs differ from original language texts because the source language seems to always “leave a trace” in the translation (“shine through”); similarly, PE, although generally capable of reaching the same quality as HT, always carries a “trace” of the used MT system. Less work can be found about relations between these facts and the ev"
