2020.cl-2.7,C16-1332,0,0.16846,"Missing"
2020.cl-2.7,N16-2002,0,0.0315692,"Missing"
2020.cl-2.7,W19-3621,0,0.0671714,"Missing"
2020.cl-2.7,D19-1530,0,0.0230379,"rse) to not biased at all (doctors), illustrates how important it is to be aware of the influence of choices concerning implementation and parameter values. 5. Final Remarks If analogies might not be the most appropriate tool to capture certain relations, surely matters have been made worse by the way that consciously or not they have been used (Gonen and Goldberg [2019] have rightly dubbed them sensational “party tricks”). This is harmful for at least two reasons. One is that they get easily propagated both in science itself (Jha and Mamidi 2017; Gebru et al. 2018; Mohammad et al. 2018; Hall Maudslay et al. 2019), also outside NLP and artificial intelligence (McQuillan 2018) and in popularized articles (Zou and Schiebinger 2018), where readers are usually in no position to verify the reliability or significance of such examples. The other is that they might mislead the search for bias and the application of debiasing strategies. And although it is debatable whether we should aim at debiasing or rather at transparency and awareness (Caliskan, Bryson, and Narayanan 2017; Gonen and Goldberg 2019), it is crucial that we are clear and transparent about what analogies can and cannot do as a diagnostic for e"
2020.cl-2.7,P16-2096,0,0.0335857,"played by human biases in choosing which analogies to search for, and which results to report. We also show that even when subjective 1 https://www.microsoft.com/en-us/research/uploads/prod/2019/08/ACL-MingZhou-50min-ming. v9-5d5104dcbe73c.pdf, slide 29. 2 This work does not mean at all to downplay the presence and danger of human biases in word embeddings. On the contrary: Embeddings do encode human biases (Caliskan, Bryson, and Narayanan 2017; Garg et al. 2018; Kozlowski, Taddy, and Evans 2019; Gonen and Goldberg 2019), and we agree that this issue deserves the full attention of the field (Hovy and Spruit 2016). 488 Nissim, van Noord, and van der Goot Fair Is Better than Sensational choices are minimized in input (as in Bolukbasi et al. 2016), parameter tuning might have consequences on the results, which should not go unnoticed or underestimated. 2. What Counts as Analogy? In linguistics, analogies of the form A : B :: C : D can be conceived on two main levels of analysis (Fischer 2019). The first one is morphological (so-called strict proportional analogies), and they account for systematic language regularities. The second one is more at the lexico-semantic level, and similarities can get looser"
2020.cl-2.7,W17-2902,0,0.119777,"ty of answers, ranging from what can be considered to be biased (nurse) to not biased at all (doctors), illustrates how important it is to be aware of the influence of choices concerning implementation and parameter values. 5. Final Remarks If analogies might not be the most appropriate tool to capture certain relations, surely matters have been made worse by the way that consciously or not they have been used (Gonen and Goldberg [2019] have rightly dubbed them sensational “party tricks”). This is harmful for at least two reasons. One is that they get easily propagated both in science itself (Jha and Mamidi 2017; Gebru et al. 2018; Mohammad et al. 2018; Hall Maudslay et al. 2019), also outside NLP and artificial intelligence (McQuillan 2018) and in popularized articles (Zou and Schiebinger 2018), where readers are usually in no position to verify the reliability or significance of such examples. The other is that they might mislead the search for bias and the application of debiasing strategies. And although it is debatable whether we should aim at debiasing or rather at transparency and awareness (Caliskan, Bryson, and Narayanan 2017; Gonen and Goldberg 2019), it is crucial that we are clear and tra"
2020.cl-2.7,W14-1618,0,0.27862,"A : B :: C : D are distinct (Mikolov et al. 2013), that is, the model is forced to return a different concept than any of the input ones. Given an analogy of the form A : B :: C : D, the code explicitly prevents yielding any term D such that D == B, D == A, or D == C. Although this constraint is helpful when all terms are expected to be different, it becomes a problem, and even a dangerous artifact, when the terms could or should be the same. Second, we discuss different analogy detection strategies/measures that have been proposed, namely, the original 3 COSADD measure, the 3 COSMUL measure (Levy and Goldberg 2014), and the Bolukbasi et al. (2016) formula, which introduces a different take on the analogy construction, reducing the impact of subjective choices (Section 4.3). Third, we highlight the role played by human biases in choosing which analogies to search for, and which results to report. We also show that even when subjective 1 https://www.microsoft.com/en-us/research/uploads/prod/2019/08/ACL-MingZhou-50min-ming. v9-5d5104dcbe73c.pdf, slide 29. 2 This work does not mean at all to downplay the presence and danger of human biases in word embeddings. On the contrary: Embeddings do encode human bias"
2020.cl-2.7,W16-2503,0,0.277012,"en extensively used to show that embeddings carry worrying biases present in our society and thus encoded in language. This bias is often demonstrated by using the analogy task to find stereotypical relations, such as the classic man is to doctor as woman is to nurse or man is to computer programmer as woman is to homemaker. The potential of the analogy task has been recently questioned, though. It has been argued that what is observed through the analogy task might be mainly due to irrelevant neighborhood structure rather than to the vector offset that supposedly captures the analogy itself (Linzen 2016; Rogers, Drozd, and Li 2017). Also, Drozd, Gladkova, and Matsuoka (2016) have shown that the original and classically used 3 COSADD method (Mikolov et al. 2013) is not able to capture all linguistic regularities present in the embeddings. With the recently proposed contextual embeddings (Peters et al. 2018; Devlin et al. 2019), it is non-trivial to evaluate on the analogy task, and is thus not commonly used. Recent research has shown that analogies are also not an accurate diagnostic to detect bias in word embeddings (Gonen and Goldberg 2019). Nevertheless, analogies are not only still widely"
2020.cl-2.7,N19-1062,0,0.137304,"19), it is non-trivial to evaluate on the analogy task, and is thus not commonly used. Recent research has shown that analogies are also not an accurate diagnostic to detect bias in word embeddings (Gonen and Goldberg 2019). Nevertheless, analogies are not only still widely used, but have also left a strong footprint, with some by-nowclassic examples often brought up as proof of human bias in language models. A case in point is the opening speech by the ACL President at ACL 2019 in Florence, Italy, where the issue of bias in embeddings is brought up showing biased analogies from a 2019 paper (Manzini et al. 2019b).1 This contribution thus aims at providing some clarifications over the past use of analogies to hopefully raise further and broader awareness of their potential and their limitations, and put well-known and possibly new analogies in the right perspective.2 First, we take a closer look at the concept of analogy together with requirements and expectations. We look at how the original analogy structure was used to query embeddings, and some misconceptions that a simple implementation choice has caused. In the original proportional analogy implementation, all terms of the equation A : B :: C :"
2020.cl-2.7,S18-1001,0,0.0232058,"considered to be biased (nurse) to not biased at all (doctors), illustrates how important it is to be aware of the influence of choices concerning implementation and parameter values. 5. Final Remarks If analogies might not be the most appropriate tool to capture certain relations, surely matters have been made worse by the way that consciously or not they have been used (Gonen and Goldberg [2019] have rightly dubbed them sensational “party tricks”). This is harmful for at least two reasons. One is that they get easily propagated both in science itself (Jha and Mamidi 2017; Gebru et al. 2018; Mohammad et al. 2018; Hall Maudslay et al. 2019), also outside NLP and artificial intelligence (McQuillan 2018) and in popularized articles (Zou and Schiebinger 2018), where readers are usually in no position to verify the reliability or significance of such examples. The other is that they might mislead the search for bias and the application of debiasing strategies. And although it is debatable whether we should aim at debiasing or rather at transparency and awareness (Caliskan, Bryson, and Narayanan 2017; Gonen and Goldberg 2019), it is crucial that we are clear and transparent about what analogies can and can"
2020.cl-2.7,N18-1202,0,0.0638757,"as woman is to homemaker. The potential of the analogy task has been recently questioned, though. It has been argued that what is observed through the analogy task might be mainly due to irrelevant neighborhood structure rather than to the vector offset that supposedly captures the analogy itself (Linzen 2016; Rogers, Drozd, and Li 2017). Also, Drozd, Gladkova, and Matsuoka (2016) have shown that the original and classically used 3 COSADD method (Mikolov et al. 2013) is not able to capture all linguistic regularities present in the embeddings. With the recently proposed contextual embeddings (Peters et al. 2018; Devlin et al. 2019), it is non-trivial to evaluate on the analogy task, and is thus not commonly used. Recent research has shown that analogies are also not an accurate diagnostic to detect bias in word embeddings (Gonen and Goldberg 2019). Nevertheless, analogies are not only still widely used, but have also left a strong footprint, with some by-nowclassic examples often brought up as proof of human bias in language models. A case in point is the opening speech by the ACL President at ACL 2019 in Florence, Italy, where the issue of bias in embeddings is brought up showing biased analogies f"
2020.cl-2.7,S17-1017,0,0.19566,"Missing"
2020.cl-2.7,N18-2039,0,0.305339,"lly ignore one or more input vectors. Most likely, this is because the traditional definition of analogies expects all terms to be different (see Section 2), and the original analogy test set reflects this. Without this constraint, 3 COSADD for example would return B in absence of close neighbors. However, we have seen that this is a strong constraint, both in morphosyntactic and semantic analogies. Moreover, even though this constraint is mentioned in the original paper (Mikolov et al. 2013) and in follow-up work (Linzen 2016; Bolukbasi et al. 2016; Rogers, Drozd, and Li 2017; Goldberg 2017; Schluter 2018), we believe this is not common knowledge in the field (analogy examples are still widely used), and even more so outside the field.4 4. Is the Bias in the Models, in the Implementation, or in the Queries? In addition to preventing input vectors from being returned, other types of implementation choices (such as punctuation, capitalization, or word frequency cutoffs), and subjective decisions play a substantial role. So, what is the actual influence of such choices on obtaining biased responses? In what follows, unless otherwise specified, we run all queries on the standard GoogleNews embeddin"
2020.evalnlgeval-1.5,W05-0909,0,0.0610457,"ed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evaluate the success of a model in regenerating a headline from one newspaper, in the style of the other. We add two further consistency checks, both of which aim at content"
2020.evalnlgeval-1.5,E09-1014,0,0.0607084,"Missing"
2020.evalnlgeval-1.5,2020.lrec-1.828,1,0.810698,"Missing"
2020.evalnlgeval-1.5,E14-1074,0,0.0600624,"Missing"
2020.evalnlgeval-1.5,P17-4012,0,0.0122863,"ifferent frameworks with different takes on the same problem: (a) as a true translation task, where given a headline in one style, the model learns to generate a new headline in the target style; (b) as a summarisation task, where headlines are viewed as an extreme case of summarisation and generated from the article. We exploit article-headline generators trained on opposite sources to do the transfer. This approach does not in principle require parallel data for training. For the translation approach (S2S), we train a supervised BiLSTM sequence-to-sequence model with attention from OpenNMT (Klein et al., 2017) 1 Note that all sets also always contain the headlines’ respective full articles, though these are not necessarily used. to map the headline from left-wing to right-wing, and viceversa. Since the model needs parallel data, we exploit the aligned headlines for training. We experiment with three differently composed training sets, varying not only in size, but also in the strength of the alignment, as shown in Figure 1b. For the summarisation approach (SUM), we use two pointer-generator networks (See et al., 2017), which include a pointing mechanism able to copy words from the source as well as"
2020.evalnlgeval-1.5,W19-8643,1,0.892211,"Missing"
2020.evalnlgeval-1.5,W04-1013,0,0.0256706,", 2019; Luo et al., 38 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 38–43, Online (Dublin, Ireland), December 2020. 2019), can shed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evalua"
2020.evalnlgeval-1.5,N19-1049,0,0.0483085,"Missing"
2020.evalnlgeval-1.5,N18-2012,0,0.0472247,"Missing"
2020.evalnlgeval-1.5,D17-1238,0,0.0311446,"Missing"
2020.evalnlgeval-1.5,P02-1040,0,0.114378,"nsfer (Fu et al., 2018; Mir et al., 2019; Luo et al., 38 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 38–43, Online (Dublin, Ireland), December 2020. 2019), can shed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’"
2020.evalnlgeval-1.5,N18-1012,0,0.0198065,"summarisation system SUM does better at content preservation (HH and AH) than S2S1. However, its scores on the main classifier are worse in both transfer directions, as well as on average. The average compliancy score is higher for S2S1. In summary, for data which is not strongly aligned, our methods suggest that style transfer is better when conceived as a translation task. BLEU is higher for SUM, but the overall extremely low scores across the board suggest that it might not be a very informative metric for this setup, although commonly used to assess content preservation in style transfer (Rao and Tetreault, 2018). Our HH and AH classifiers appear more indicative in this respect, and ROUGE scores seem to correlate a bit more with them, when compared to BLEU. It remains to be investigated whether BLEU, ROUGE, and our content-checking classifiers do in fact measure something similar or not. With better-aligned data (bottom panel), the picture is more nuanced. Here, the main comparison is between two systems trained on strongly aligned data, one of which (S2S2) has additional, weakly aligned data. The overall compliancy score suggests that this improves style transfer (and this system is also the top perf"
2020.evalnlgeval-1.5,J18-3002,0,0.0235647,"Missing"
2020.evalnlgeval-1.5,J09-4008,0,0.0932183,"Missing"
2020.evalnlgeval-1.5,W02-2113,0,0.189512,"Missing"
2020.evalnlgeval-1.5,P17-1099,0,0.0166246,"ain a supervised BiLSTM sequence-to-sequence model with attention from OpenNMT (Klein et al., 2017) 1 Note that all sets also always contain the headlines’ respective full articles, though these are not necessarily used. to map the headline from left-wing to right-wing, and viceversa. Since the model needs parallel data, we exploit the aligned headlines for training. We experiment with three differently composed training sets, varying not only in size, but also in the strength of the alignment, as shown in Figure 1b. For the summarisation approach (SUM), we use two pointer-generator networks (See et al., 2017), which include a pointing mechanism able to copy words from the source as well as pick them from a fixed vocabulary, thereby allowing better handling of out-of-vocabulary words. ability to reproduce novel words. One model is trained on the la Repubblica portion of the training set, the other on Il Giornale. In a style transfer setting we use these models as follows: Given a headline from Il Giornale, for example, the model trained on la Repubblica can be run over the corresponding article from Il Giornale to generate a headline in the style of la Repubblica, and vice versa. To train the model"
2020.evalnlgeval-1.5,2020.acl-main.704,0,0.0162999,". We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evaluate the success of a model in regenerating a headline from one newspaper, in the style of the other. We add two further consistency checks, both of which aim at content assessment, and are carried out"
2020.findings-emnlp.389,2020.tacl-1.54,0,0.0255378,"rmation is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance. 1 Introduction and Background Natural Language Processing is now dominated by transformer-based models (Vaswani et al., 2017), like BERT (Devlin et al., 2019), a model trained on predicting masked tokens and relations between sentences. BERT’s impact is so strong that we already talk about ‘BERTology’ (Rogers et al., 2020). In addition to using BERT in NLP tasks and end applications, research has also been done on BERT, especially to reveal what linguistic information is available in different parts of the model. This is done, e.g., investigating what BERT’s attention heads might be attending to (Clark et al., 2019), or looking at its internal vector representations using so-called probing (or diagnostic) classifiers (Tenney et al., 2019a). It has been noted that BERT progressively acquires linguistic information roughly in the same the order of the classic language processing pipeline (Tenney et al., 2019b,a):"
2020.findings-emnlp.389,P19-1452,0,0.341333,"ike BERT (Devlin et al., 2019), a model trained on predicting masked tokens and relations between sentences. BERT’s impact is so strong that we already talk about ‘BERTology’ (Rogers et al., 2020). In addition to using BERT in NLP tasks and end applications, research has also been done on BERT, especially to reveal what linguistic information is available in different parts of the model. This is done, e.g., investigating what BERT’s attention heads might be attending to (Clark et al., 2019), or looking at its internal vector representations using so-called probing (or diagnostic) classifiers (Tenney et al., 2019a). It has been noted that BERT progressively acquires linguistic information roughly in the same the order of the classic language processing pipeline (Tenney et al., 2019b,a): surface features are expressed in lower layers, syntactic features more in middle layers and semantic ones in higher layers (Jawahar et al., 2019). So, for example, information on part-of-speech appears to be acquired earlier than on coreference. Most work dedicated to understanding the inner workings of BERT has focused on English, though non-English BERT models do exist, in two forms. One is a multilingual model (Dev"
2020.findings-emnlp.389,W02-2024,0,0.190615,"Missing"
2020.findings-emnlp.389,P19-1356,0,\N,Missing
2020.findings-emnlp.389,N19-1423,0,\N,Missing
2020.gebnlp-1.1,S18-2005,0,0.0293931,"th mitigating bias in English ELMo (Zhao et al., 2019) and in embeddings of morphologically rich languages (Zmigrod et al., 2019). 3 Data In line with previous research (Kurita et al., 2019; Zhao et al., 2019; Basta et al., 2019), we measure gender bias in BERT using sentence templates. For this purpose we create the Bias Evaluation Corpus with Professions (BEC-Pro), containing English and German sentences built from templates (Section 3.2). We also use two previously existing corpora, which are described in Section 3.1. 3.1 Existing Corpora The Equity Evaluation Corpus (EEC) was developed by Kiritchenko and Mohammad (2018) as a benchmark corpus for testing gender and racial bias in NLP systems in connection with emotions. It contains 8,640 sentences constructed using 11 sentence templates with the variables <person>, which is instantiated by a male- or female-denoting NP; and <emotion word>, whose values can be one of the basic emotions. We use this corpus for preliminary bias assessment.2 This corpus also inspired the structure of the BEC-Pro, and we borrow from it the person words used in our templates. The GAP corpus (Webster et al., 2018) was developed as a benchmark for measuring gender bias in coreference"
2020.gebnlp-1.1,W19-3823,0,0.128092,"e), December 13, 2020. Contributions This work makes the following contributions: (i) We present and release the Bias Evaluation Corpus with Professions (BEC-Pro), a template-based corpus in English and German, which we created to measure gender bias with respect to different profession groups. We make the dataset and code for all experiments publicly available at https://github.com/marionbartl/gender-bias-BERT. (ii) Through a more diverse sentence context in our corpus than in previous research, we confirm that the method of querying BERT’s underlying MLM (Masked Language Model), proposed by Kurita et al. (2019), can be used for bias detection in contextualized word embeddings. (iii) We test our bias analysis on BERT against actual U.S. workforce statistics, which helps us to observe that the BERT language model does not only encode biases that reflect real-world data, but also those that are based on stereotypes. For bias mitigation, (iv) we show the success of a technique on BERT, which was previously applied on ELMo (Peters et al., 2018; Zhao et al., 2019). Finally, (v) we attempt the cross-lingual transfer of a bias measuring method proposed for English, and show how this method is impaired by th"
2020.lrec-1.35,E17-2039,1,0.891138,"Missing"
2020.lrec-1.35,J09-1005,0,0.0228919,"tically used PIEs per million tokens. the highest idiom frequencies aligns nicely with previous work on idiom frequencies. For example, Moon (1998) notes that the frequency of idioms in spoken language has been overestimated relative to written language. She suggest this may be caused by the high frequency of idioms in scripted speech, such as in fiction, film, and television, a category which also covers W news script. As for W newsp other (and W newsp brdsht, which has the third-highest fIdiom), it has been noted that journalistic writing is a particularly rich source of idioms (Moon, 1998; Fazly et al., 2009; Gr´egoire, 2009). More generally, we see that academic texts (W ac) have the lowest frequency of PIEs, followed by non-academic non-fiction (W nonAc), i.e. texts whose main purpose is instruction, information, and education. PIEs are most frequent in news, prose fiction, conversations, and popular magazines (pop lore), i.e. texts whose main purpose is entertainment. However, spoken conversations (S conv) do not fit this category neatly, even if they have a similar PIE frequency. We have no clear explanation for its high PIE frequency, but we do note that it stands out of the group of news, p"
2020.lrec-1.35,N06-2015,0,0.166345,"form, use a large amount of almost 2,000 PIE types with no restriction on syntactic pattern, automatic pre-extraction, and five sense labels. As far as we know, crowdsourcing has not been utilised for creating an idiom corpus before. However, there is closely related work by Kato et al. (2018), who create a corpus of verbal multiword expressions, a group which includes idioms as well, but is a lot broader, incorporating particle verbs, collocations, and other types of set phrases. Kato et al. extract all instances of a set of MWE types taken from Wiktionary from part of the OntoNotes corpus (Hovy et al., 2006). Since simple extraction based on words can yield a lot of noise, i.e. non-instances, they refine those extractions based on the gold-standard part-of-speech tags and parse trees that are present in the OntoNotes corpus. Most novel, however, is their use of crowdsourcing for distinguishing between literal equivalents of MWE phrases like get up in ‘He gets up early’ and actual MWE instances like in ‘He gets up a hill’. They frame the task as a sense annotation task, asking crowdworkers to label instances as either literal, non-literal, unclear, or ‘none of the above’. Using this procedure, the"
2020.lrec-1.35,L18-1396,0,0.0281549,"s done on the PIE corpora, there is significant variation, with Cook et al. (2008) using only three tags (idiomatic, literal, unclear), whereas Sporleder et al. (2010) use six (idiomatic, literal, both, unclear, embedded, meta-linguistic). As for our approach, we allow a large amount of deviation from the PIE’s dictionary form, use a large amount of almost 2,000 PIE types with no restriction on syntactic pattern, automatic pre-extraction, and five sense labels. As far as we know, crowdsourcing has not been utilised for creating an idiom corpus before. However, there is closely related work by Kato et al. (2018), who create a corpus of verbal multiword expressions, a group which includes idioms as well, but is a lot broader, incorporating particle verbs, collocations, and other types of set phrases. Kato et al. extract all instances of a set of MWE types taken from Wiktionary from part of the OntoNotes corpus (Hovy et al., 2006). Since simple extraction based on words can yield a lot of noise, i.e. non-instances, they refine those extractions based on the gold-standard part-of-speech tags and parse trees that are present in the OntoNotes corpus. Most novel, however, is their use of crowdsourcing for"
2020.lrec-1.35,E09-1086,0,0.0250721,"e.g. idiomatic, literal, and unclear), and the ‘syntactic type’ of the expressions covered. Syntactic type means that, in some cases, only idiom types following a certain syntactic pattern were included, e.g. only verb-(determiner)-noun combinations such as hold your fire and see stars. In general, there is large variation in corpus creation methods, regarding PIE definition, extraction method, annotation schemes, base corpus, and PIE type inventory. Depending on the goal of the corpus, the amount of deviation that is allowed from the PIE’s dictionary form to the instances can be very little (Sporleder and Li, 2009), to quite a lot (Sporleder et al., 2010). The number of PIE types covered by each corpus is limited, ranging from 17 to 65 types, often limited to one or more syntactic patterns. The extraction of PIE instances is usually done in a semiautomatic manner, by manually defining patterns in a text or parse tree, and doing some manual filtering afterwards. This works well, but an extension to a large number of PIE types (e.g. several hundreds) would also require a large increase in the amount of manual effort involved. Considering the sense annotations done on the PIE corpora, there is significant"
2020.lrec-1.35,sporleder-etal-2010-idioms,0,0.0301652,"d the ‘syntactic type’ of the expressions covered. Syntactic type means that, in some cases, only idiom types following a certain syntactic pattern were included, e.g. only verb-(determiner)-noun combinations such as hold your fire and see stars. In general, there is large variation in corpus creation methods, regarding PIE definition, extraction method, annotation schemes, base corpus, and PIE type inventory. Depending on the goal of the corpus, the amount of deviation that is allowed from the PIE’s dictionary form to the instances can be very little (Sporleder and Li, 2009), to quite a lot (Sporleder et al., 2010). The number of PIE types covered by each corpus is limited, ranging from 17 to 65 types, often limited to one or more syntactic patterns. The extraction of PIE instances is usually done in a semiautomatic manner, by manually defining patterns in a text or parse tree, and doing some manual filtering afterwards. This works well, but an extension to a large number of PIE types (e.g. several hundreds) would also require a large increase in the amount of manual effort involved. Considering the sense annotations done on the PIE corpora, there is significant variation, with Cook et al. (2008) using"
2020.lrec-1.828,W10-4201,0,0.222329,"2018). Similarly, humans failed to outperform automatic systems in recognising the native language of nonEnglish speakers writing in English (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017; Hu et al., 2017; Keskar et al., 2019, e.g.), and on attempts to define best practices for human and automatic evaluation (van der Lee et al., 2019), reliable and shared evaluation metrics and strategies concerning style-aware generation are still lacking (Fu et al., 2018). As a contribution to this aspect, we develop style-aware headline generation models, and discuss an evaluation strategy based on text classification, which is particularly useful g"
2020.lrec-1.828,E09-1014,0,0.238704,"mans failed to outperform automatic systems in recognising the native language of nonEnglish speakers writing in English (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017; Hu et al., 2017; Keskar et al., 2019, e.g.), and on attempts to define best practices for human and automatic evaluation (van der Lee et al., 2019), reliable and shared evaluation metrics and strategies concerning style-aware generation are still lacking (Fu et al., 2018). As a contribution to this aspect, we develop style-aware headline generation models, and discuss an evaluation strategy based on text classification, which is particularly useful given that human judgemen"
2020.lrec-1.828,E14-1074,0,0.528122,"Missing"
2020.lrec-1.828,W17-4912,0,0.0154511,"ish (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017; Hu et al., 2017; Keskar et al., 2019, e.g.), and on attempts to define best practices for human and automatic evaluation (van der Lee et al., 2019), reliable and shared evaluation metrics and strategies concerning style-aware generation are still lacking (Fu et al., 2018). As a contribution to this aspect, we develop style-aware headline generation models, and discuss an evaluation strategy based on text classification, which is particularly useful given that human judgement for this task is found to be unreliable. While the strategy of using classification as evaluation is in itself not new"
2020.lrec-1.828,P16-2051,0,0.0413101,"Missing"
2020.lrec-1.828,N18-1169,0,0.0245404,"judgement. Besides the works mentioned in the Introduction to frame the problem, we will not discuss further related work on style modelling or summarisation. Rather, we concentrate on discussing previous works that make use of automatic classification for the evaluation of NLG systems, also to show in what sense our approach differ from existing ones. Using a classifier to assess the goodness of generated texts in connection to a broad definition of style-aware generation has been used in several previous works (Hu et al., 2017; Tian et al., 2018; Prabhumoye et al., 2018; John et al., 2018; Li et al., 2018, e.g.). However, these works tend to focus on sentiment aspects (transforming a positive review into a negative one, for example), which are usually mostly associated to a lexical problem (only a small part of style). Indeed, the problem of style transfer is usually addressed within the Variational Autoencoder framework and/or trough lexical substitution. Lexical substitution was also the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation was used. In addition, Li et al. (20"
2020.lrec-1.828,D16-1230,0,0.0169047,", which are a prime tool to capture attention and make clear statements about the newspaper’s position over a certain event. Can this newspaper-specific style be distinguished? And is it preserved in automatically generated headlines? To answer such questions, we train newspaper-specific headline generation models, and evaluate how style-compliant the generated headline is for a given newspaper. How such evaluation can be performed though is yet another research question of its own. Evaluating generated text just using standard metrics based on lexical overlap is normally not accurate enough (Liu et al., 2016). In machine translation, for example, the decisive, final system evaluation is typically human-based, as the lexically-based BLEU score is not exhaustive. Automatic evaluation strategies are still used because human evaluation is expensive, not always available, and complex to include in highly iterative developments. However, human evaluation is not always a decisive and accurate strategy, since there might be aspects of text that for people are not so easy to grasp. For example, in profiling, where differently from the assessment of the goodness of translated text, evaluation can be perform"
2020.lrec-1.828,W15-0620,0,0.0118382,"t always a decisive and accurate strategy, since there might be aspects of text that for people are not so easy to grasp. For example, in profiling, where differently from the assessment of the goodness of translated text, evaluation can be performed against discrete gold labels, several studies found that humans are definitely not better than machines in identifying the gender of a writer (Koppel et al., 2002; Flekova et al., 2016; van der Goot et al., 2018). Similarly, humans failed to outperform automatic systems in recognising the native language of nonEnglish speakers writing in English (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017;"
2020.lrec-1.828,D15-1221,0,0.0467534,"Missing"
2020.lrec-1.828,P18-1080,0,0.0137694,"overcome the limitation of unreliable human judgement. Besides the works mentioned in the Introduction to frame the problem, we will not discuss further related work on style modelling or summarisation. Rather, we concentrate on discussing previous works that make use of automatic classification for the evaluation of NLG systems, also to show in what sense our approach differ from existing ones. Using a classifier to assess the goodness of generated texts in connection to a broad definition of style-aware generation has been used in several previous works (Hu et al., 2017; Tian et al., 2018; Prabhumoye et al., 2018; John et al., 2018; Li et al., 2018, e.g.). However, these works tend to focus on sentiment aspects (transforming a positive review into a negative one, for example), which are usually mostly associated to a lexical problem (only a small part of style). Indeed, the problem of style transfer is usually addressed within the Variational Autoencoder framework and/or trough lexical substitution. Lexical substitution was also the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation"
2020.lrec-1.828,N18-1012,0,0.0666406,"the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation was used. In addition, Li et al. (2018) compared the automatic classification-based evaluation with human evaluation. They find a high correlation between human and automatic evaluation in two out of their three data-sets, showing the validity of the automatic approach. However, the task of sentiment analysis, though subjective, is not too hard for humans, who are usually able to perceive sentiment encapsulated in text. Rao and Tetreault (2018) also exploited human and automatic classification as benchmarks for a machine translation system that translates formal texts into informal texts and vice-versa. Also in this case, usually text register is something that humans are quite able to grasp. Our work differs from the above in at least two respects. One is that we want to evaluate the capabilities of an NLG system to learn (different) stylistics aspects from (different) training data sets, rather than evaluating the capabilities of style transfer systems mostly based on lexical substituFigure 1: Red: generation task. Blue: classific"
2020.lrec-1.828,W16-5603,0,0.0161894,"rks (Hu et al., 2017; Tian et al., 2018; Prabhumoye et al., 2018; John et al., 2018; Li et al., 2018, e.g.). However, these works tend to focus on sentiment aspects (transforming a positive review into a negative one, for example), which are usually mostly associated to a lexical problem (only a small part of style). Indeed, the problem of style transfer is usually addressed within the Variational Autoencoder framework and/or trough lexical substitution. Lexical substitution was also the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation was used. In addition, Li et al. (2018) compared the automatic classification-based evaluation with human evaluation. They find a high correlation between human and automatic evaluation in two out of their three data-sets, showing the validity of the automatic approach. However, the task of sentiment analysis, though subjective, is not too hard for humans, who are usually able to perceive sentiment encapsulated in text. Rao and Tetreault (2018) also exploited human and automatic classification as benchmarks for a machine translation system that translat"
2020.lrec-1.828,D15-1044,0,0.0129556,"er-specific style. Importantly, we also observe that humans aren’t reliable judges for this task, since although familiar with the newspapers, they are not able to discern their specific styles even in the original human-written headlines. The utility of automatic evaluation goes therefore beyond saving the costs and hurdles of manual annotation, and deserves particular care in its design. Keywords: Natural Language Generation, Stylistic variations, Evaluation 1. Introduction Automatic headline generation is conceptually a simple task which can be conceived as a form of extreme summarisation (Rush et al., 2015): given an article, or a portion of it, generate its headline. Generation of headlines though is not just a matter of summarising the content. Different newspapers report the news in different ways, depending on their policies and strategies. For example, they might exhibit some topic-biases, such as writing more about gossip vs more about politics. But even when reporting on the same topics, they might exhibit specific stylistic features related to word choices, word order, punctuation usage, etc. This might be even more evident when newspapers are positioned at opposite ends of the political"
2020.lrec-1.828,P17-1099,0,0.0664172,"eadlines and verify whether it is able to correctly classify their source. Figure 1 shows an overview of the approach. 6710 3.1. Generation Models classifier human As the focus of this contribution is not on making the best model for headline generation, rather on evaluation strategies, we leverage existing implementations of sequence-tosequence networks. More specifically, we experiment with the following three models: • Sequence-to-Sequence with Attention (S2S) We used a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al., 2014) with the configuration used by See et al. (2017) but we used a bidirectional instead of a unidirectional layer. This choice applies to all the models we used. The final configuration is 1 bidirectional encoder-decoder layer with 256 LSTM cells each, no dropout and shared embeddings with size 128; the model is optimised with Adagrad with learning rate 0.15 and gradient clipped (Mikolov, 2012) to a maximum magnitude of 2. • Pointer Generator Network (PN) The basic architecture is a sequence-to-sequence model, but the hybrid pointer-generator network uses a pointing mechanism (See et al., 2017) that lets it copy words from the source text, and"
2020.lrec-1.828,P18-2061,1,0.868035,"Missing"
2020.lrec-1.828,W19-8643,0,0.0947261,"Missing"
2020.peoples-1.2,W17-4407,0,0.012787,"ince both T WI S TY and Personal-ITY implement the MBTI model, analyses and experiments over personality detection can be carried out also in a cross-domain setting. 3.2 Corpus Creation and Description The fact that users often self-disclose information about themselves on social media makes it possible to adopt Distant Supervision (DS) for the acquisition of training data. DS is a semi-supervised method that has been abundantly and successfully used in affective computing and profiling to assign silver labels to data on the basis of indicative proxies (Go et al., 2009; Pool and Nissim, 2016; Emmery et al., 2017). We observed that some YouTube Italian users were used to leave comments to videos on the MBTI theory, in which they were stating their own personality type (e.g. Sono ENTJ...chi altro? [en: “I’m ENTJ...anyone else?”]; INTP, primo test che effettivamente ha ragione [en: ”INTP, the first test that is actually right”]). We exploited such comments to create Personal-ITY. The methodology, explained in detail in (Bassignana et al., 2020), consisted in creating automatically a list of YouTube users annotated with MBTI personality labels starting from the comments cited above. In the second macro-st"
2020.peoples-1.2,2020.acl-main.560,0,0.0140739,"a weighted random baseline (WRB) and a majority baseline (MAJ). Let us notice that the model proposed in (Verhoeven et al., 2016) for the Italian language is the only one not reaching any baseline (for all the other languages the model proposed reach at least the weighted random baseline). This also prompted us to work on the Italian language, where there is still ample room for improvement on the development of resources and models for the personality detection task. More in general, our choice has to be seen as an intention of improving the state of the art for languages other than English (Joshi et al., 2020). Recent computational approaches on personality prediction from texts investigated the use of deep learning (Majumder et al., 2017) and regression models (Akrami et al., 2019). 13 3 Data To run experiments on personality detection, we have created a dedicated corpus with MBTI labels, exploiting distant supervision: Personal-ITY (Bassignana et al., 2020). Here, we summarise the choices that we made regarding the source of the data and the theoretical trait model, the procedure followed to construct the corpus, and provide a description of the resulting dataset. In addition, we also partly use"
2020.peoples-1.2,D14-1162,0,0.0860708,"near SVM 15 (LinearSVM), with standard parameters, and tested three types of features: lexical-, stylistic-, and embeddings-based. We used four placeholders for hashtags, urls, usernames and emojis. At the lexical level, we experimented with word (1-2) and character (3-4) n-grams, both as raw counts as well as tf-idf weighted. Character n-grams were tested also with a word-boundary option. Considering stylistic features, we investigated the use of emojis, hashtags, pronouns, punctuation and capitalisation. Lastly, we also experimented with embeddings-based representations, using more generic (Pennington et al., 2014) and YouTube-specific (Nieuwenhuis and Nissim, 2019) pre-trained models. We created one representation per user by averaging the vectors for all words written by that user. We used 10-fold cross-validation, and assessed the models using macro f-score. We deem this way of averaging over f-scores per class appropriate, since the dataset is quite unbalanced, but we want good performance for each class. For comparison, we calculated a majority baseline (MAJ). Table 3 shows the results of our experiments with the different feature types described above. Regarding n-grams and embeddings representati"
2020.peoples-1.2,W15-2913,0,0.431571,"aceBook comments (700 millions words) written by 136.000 users who shared their status updates. Interesting correlations were observed between word usage and personality traits. For the 2015 PAN Author Profiling Shared Task (Pardo et al., 2015), personality was added to gender and age in their standard profiling task, with tweets in English, Spanish, Italian and Dutch annotated according to the Big Five model assigning a score in a range [-0.5; +0.5] for each trait. If looking at data labelled with the MBTI traits, we find a corpus of 1.2M English tweets annotated with personality and gender (Plank and Hovy, 2015), and the multilingual dataset T WI S TY (Verhoeven et al., 2016). The latter is a corpus of data collected from Twitter using a Distant Supervision approach. It is annotated with MBTI personality labels and gender for six languages (Dutch, German, French, Italian, Portuguese and Spanish), and includes a total of 18,168 authors. As we concentrate on Italian, we report in Table 1 an overview of the available Italian corpora labelled with personality traits. We include information on our own Personal-ITY corpus, which is described in Section 3. For T WI S TY, we only report information for the I"
2020.peoples-1.2,W16-4304,1,0.836214,"istence of T WI S TY. Since both T WI S TY and Personal-ITY implement the MBTI model, analyses and experiments over personality detection can be carried out also in a cross-domain setting. 3.2 Corpus Creation and Description The fact that users often self-disclose information about themselves on social media makes it possible to adopt Distant Supervision (DS) for the acquisition of training data. DS is a semi-supervised method that has been abundantly and successfully used in affective computing and profiling to assign silver labels to data on the basis of indicative proxies (Go et al., 2009; Pool and Nissim, 2016; Emmery et al., 2017). We observed that some YouTube Italian users were used to leave comments to videos on the MBTI theory, in which they were stating their own personality type (e.g. Sono ENTJ...chi altro? [en: “I’m ENTJ...anyone else?”]; INTP, primo test che effettivamente ha ragione [en: ”INTP, the first test that is actually right”]). We exploited such comments to create Personal-ITY. The methodology, explained in detail in (Bassignana et al., 2020), consisted in creating automatically a list of YouTube users annotated with MBTI personality labels starting from the comments cited above."
2020.peoples-1.2,L16-1258,0,0.318122,"ect prevalence of traits include human judgements regarding semantic similarity and relations between adjectives that people use to describe themselves and others. This is because language is believed to be a prime carrier of personality traits (Schwartz et al., 2013). This aspect, together with the progressive increase of available user-generated data from social media, has prompted the task of Personality Detection, i.e., the automatic prediction of personality from written texts (Whelan and Davies, 2006; Argamon et al., 2009; Celli et al., 2013; Youyou et al., 2015; Litvinova et al., 2016; Verhoeven et al., 2016). Personality detection can be useful in predicting life outcomes such as substance use, political attitudes and physical health. Other fields of application are marketing, politics, psychological and social assessment and, in the computational domain, dialogue systems (Ma et al., 2020) and chatbots (Qian et al., 2018). As a contribution to personality detection in languages other than English, we have developed Personal-ITY, a novel corpus of YouTube comments in Italian, which are annotated with MBTI personality traits. The corpus creation methodology, described in detail in (Bassignana et al"
2020.restup-1.4,S19-2007,0,0.0206298,"ng the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an attempt to promote both the development of working systems and a better understanding of the phenomenon. Vidgen et al. (2019) and Jurgens et al. (2019) identify a set of pending issues that require attention and care by people in NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types of abusive/hateful behaviors to recognize. For instance, definitions of hate speech • use of communities (Tulkens et al., 2016; Merenda et al., 2018): potentially hateful or abusive messages are extracted by collecting data from on-l"
2020.restup-1.4,P19-1357,0,0.0228063,"sages before finding, very sparse, hateful cases. To circumvent this obstacle, three main strategies have been adopted so far: The automatic detection of abusive and offensive messages in on-line communities has become a pressing issue. The promise of Social Media to create a more open and connected world is challenged by the growth of abusive behaviors, among which cyberbullying, trolling, and hate speech are some of the most known. It has also been shown that awareness of being a victim of some kind of abusive behavior is less widespread than what one actually reports as having experienced (Jurgens et al., 2019). The body of work conducted in the areas of abusive language, hate speech, and offensive language has rapidly grown in the last years, leaving the field with a variety of definitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and d"
2020.restup-1.4,L18-1443,0,0.122229,"Missing"
2020.restup-1.4,W19-3509,0,0.0166277,"rld is challenged by the growth of abusive behaviors, among which cyberbullying, trolling, and hate speech are some of the most known. It has also been shown that awareness of being a victim of some kind of abusive behavior is less widespread than what one actually reports as having experienced (Jurgens et al., 2019). The body of work conducted in the areas of abusive language, hate speech, and offensive language has rapidly grown in the last years, leaving the field with a variety of definitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an a"
2020.restup-1.4,N16-2013,0,0.0872087,"NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types of abusive/hateful behaviors to recognize. For instance, definitions of hate speech • use of communities (Tulkens et al., 2016; Merenda et al., 2018): potentially hateful or abusive messages are extracted by collecting data from on-line communities that are known either to promote or tolerate such types of messages; • use of keywords (Waseem and Hovy, 2016; Basile et al., 2019; Zampieri et al., 2019a): specific keywords which are not hateful or abusive per se but that may be the target of hateful or abusive messages, like for instance the word “migrants”, are selected to collect random messages from Social Media outlets; • use of users (Wiegand et al., 2018; Ribeiro et al., 2018): seed users that have been identified via some heuristics to regularly post abusive or hateful materials are selected and their messages collected. In a variation of this approach, additional potential “hateful” users are identified by applying network analysis to the"
2020.restup-1.4,W17-3012,0,0.0977559,"open and connected world is challenged by the growth of abusive behaviors, among which cyberbullying, trolling, and hate speech are some of the most known. It has also been shown that awareness of being a victim of some kind of abusive behavior is less widespread than what one actually reports as having experienced (Jurgens et al., 2019). The body of work conducted in the areas of abusive language, hate speech, and offensive language has rapidly grown in the last years, leaving the field with a variety of definitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini"
2020.restup-1.4,N18-1095,0,0.15358,"r instance, definitions of hate speech • use of communities (Tulkens et al., 2016; Merenda et al., 2018): potentially hateful or abusive messages are extracted by collecting data from on-line communities that are known either to promote or tolerate such types of messages; • use of keywords (Waseem and Hovy, 2016; Basile et al., 2019; Zampieri et al., 2019a): specific keywords which are not hateful or abusive per se but that may be the target of hateful or abusive messages, like for instance the word “migrants”, are selected to collect random messages from Social Media outlets; • use of users (Wiegand et al., 2018; Ribeiro et al., 2018): seed users that have been identified via some heuristics to regularly post abusive or hateful materials are selected and their messages collected. In a variation of this approach, additional potential “hateful” users are identified by applying network analysis to the seed users. 1 For a more detailed overview of available datasets in different languages please consult https://github.com/leondz/ hatespeechdata. 14 Common advantages of these approaches mainly lie in the reduction of annotation time and a higher density of positive instances, i.e. hateful messages in our"
2020.restup-1.4,N19-1060,0,0.107468,"Missing"
2020.restup-1.4,N19-1144,0,0.0344569,"efinitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an attempt to promote both the development of working systems and a better understanding of the phenomenon. Vidgen et al. (2019) and Jurgens et al. (2019) identify a set of pending issues that require attention and care by people in NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types o"
2020.restup-1.4,S19-2010,0,0.0291805,"efinitions and a lack of reflection on the intersection among such different phenomena (Waseem et al., 2017; Vidgen et al., 2019). As a direct consequence, there has been a flood of annotated datasets in different languages, 1 all somehow addressing the same phenomena (e.g. offensive language, or hate speech) but applying slightly different definitions, different annotation approaches (e.g. experts vs. crowdsourcing), and different reference domains (e.g., Twitter, Facebook, Reddit). Hate speech, in particular, has been the target of the latest major evaluation campaigns such as SemEval 2019 (Zampieri et al., 2019b; Basile et al., 2019), EVALITA 2018 (Bosco et al., 2018), and IberEVAL 2018 (Fersini et al., 2018) in an attempt to promote both the development of working systems and a better understanding of the phenomenon. Vidgen et al. (2019) and Jurgens et al. (2019) identify a set of pending issues that require attention and care by people in NLP working on this topic. One of them concerns a revision of what actually constitutes abuse. The perspective that has been adopted so far in the definition of abusive language, and most importantly of hate speech, has been limited to specific and narrow types o"
2021.acl-short.62,2020.acl-main.703,0,0.0905078,"Missing"
2021.acl-short.62,P18-1080,0,0.0353935,"Missing"
2021.acl-short.62,N18-1012,0,0.399325,"re popular. These include disentangling style and content by learning a distinct representation for each (Shen et al., 2017; Fu et al., 2018; John et al., 2019), and back translation (Zhang et al., 2018; Lample et al., 2019; Luo et al., 2019; Prabhumoye et al., 2018). A common strategy to enhance style accuracy is to introduce a reward in the form of a style classifier (Lample et al., 2019; Gong et al., 2019; Luo et al., 2019; Wu et al., 2019; Sancheti et al., 2020). As a result, unsupervised models achieve good accuracy in style strength. Content preservation is however usually unsuccessful (Rao and Tetreault, 2018). Parallel data can help to preserve content, but is limited. Niu et al. (2018) combine the train sets of two different domains and incorporate machine translation to train their models with a multi-task learning schema, plus model ensembles. Sancheti et al. (2020) use it to train a supervised sequence-tosequence model, and in addition to the commonly used style strength reward, they include a reward based on BLEU (Papineni et al., 2002) to enhance content preservation. Shang et al. (2019) propose a semi-supervised model combining parallel data with large amounts of non-parallel data. Pre-trai"
2021.acl-short.62,2020.acl-main.704,0,0.0572608,"Missing"
2021.acl-short.62,D19-1499,0,0.0347138,"Missing"
2021.acl-short.62,D19-1365,0,0.0357564,"Missing"
2021.acl-short.62,2020.coling-main.203,0,0.0854563,"Missing"
2021.acl-short.62,P19-1482,0,0.0449996,"Missing"
2021.emnlp-main.349,W18-2703,0,0.023279,"nce on Empirical Methods in Natural Language Processing, pages 4241–4254 c November 7–11, 2021. 2021 Association for Computational Linguistics In practice, we propose a framework that adopts a multi-step procedure which builds upon a general-purpose pre-trained sequence-to-sequence (seq2seq) model. First, we strengthen the model’s ability to rewrite by conducting a second phase of pre-training on natural pairs derived from an existing collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative backtranslation (Hoang et al., 2018) approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs on-the-fly. Lastly, we use our best resulting model to generate static synthetic pairs, which are then used offline as parallel training data. be used as pairs to train the model of the opposite transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting"
2021.emnlp-main.349,K19-1005,0,0.0612565,"Missing"
2021.emnlp-main.349,P19-1041,0,0.0217944,"e transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting, inserting and combining to create the pair’s target. Back-transferring is generally used to reconstruct the source sentence for training, so that pairs are also made on-the-fly. Lample et al. (2019) provide evidence that disentangling style and content to learn distinct representations (Shen et al., 2017; Fu et al., 2018; John et al., 2019; Yi et al., 2020) is not necessary. Reconstructing the source, instead, appears beneficial: Contributions Using a large pre-trained seq2seq it is used by Dai et al. (2019) who pre-train a model model (1) we achieve state-of-the-art results for the two most popular style transfer tasks without task- on style transfer data with the Transformer architecture (Vaswani et al., 2017); and by Zhou et al. specific parallel data. We show that (2) generic (2020), who use an attentional seq2seq model that resources can be leveraged to derive parallel data pre-trains the model to reconstruct the source se"
2021.emnlp-main.349,D14-1181,0,0.00370936,"this dataset in its entirety or filtered (models M1.1 and M1.2 in Table 3). In the first case, the whole of the paraphrase pairs from PARABANK 2 are used to further pretrain the model. In the second case, we follow the rationale that not all pairs are equally relevant for our tasks, and selecting task-specific ones could be beneficial. For instance, while both PARABANK 2 pairs in Table 1 are good examples of rewriting, the one on the right is more meaningful in terms of formality transfer. Therefore, we train two binary style classifiers, one for formality and one for polarity, using TextCNN (Kim, 2014) on the training sets of GYAFC and YELP. These classifiers are then used to automatically select more strongly style-opposed pairs. The resulting filtered paraphrase subset Dp is such a set of pairs: Dp = {(x, y)|(p(s1 |x) + p(s2 |y))/2 > σ} (2) σ = 0.85 in our experiments. Synthetic Pairs for Polarity Swap Due to the nature of polarity swap, we expect that even filtered paraphrases might not benefit polarity swap as much as formality. We therefore add another strategy to enhance polarity swap rewriting and create pairs for further pre-training exploiting a general-purpose lexical resource (mo"
2021.emnlp-main.349,2020.inlg-1.25,0,0.0170189,"s, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative backtranslation (Hoang et al., 2018) approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs on-the-fly. Lastly, we use our best resulting model to generate static synthetic pairs, which are then used offline as parallel training data. be used as pairs to train the model of the opposite transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting, inserting and combining to create the pair’s target. Back-transferring is generally used to reconstruct the source sentence for training, so that pairs are also made on-the-fly. Lample et al. (2019) provide evidence that disentangling style and content to learn distinct representations (Shen et al., 2017; Fu et al., 2018; John et al., 2019; Yi et al., 2020) is not necessary. Reconstructing the source, instead, appears beneficial: Contributions Using a larg"
2021.emnlp-main.349,2020.acl-main.703,0,0.24799,"NLP tasks, large pre-trained models have been shown to provide an excellent base for fine- ident which strategy works best for creating partuning in a supervised setting (Chawla and Yang, allel data, whether offline or on-the-fly, and the simultaneous advantage of both strategies has not 2020; Lai et al., 2021). been fully explored. Lastly, Chawla and Yang Since parallel data for fine-tuning such large (2020) develop a semi-supervised model based on models for style transfer is scarce, a substantial amount of work has gone into methods for creat- sequence-to-sequence pre-trained model (BART, Lewis et al. (2020)) using parallel training data and ing artificial sentence pairs so that models can be large amounts of non-parallel data, which achieves trained in a supervised regime. One way to do this is to artificially generate par- a significant performance. In previous work, we have also shown that a sequence-to-sequence preallel data via back-translation, so that training pairs trained model (BART) outperforms a language are created on-the-fly during the training process itself (Zhang et al., 2018; Lample et al., 2019; Prab- model (GPT-2) in content preservation and overall performance when task-speci"
2021.emnlp-main.349,P02-1040,0,0.112176,"tly to modelling and evaluation metrics. 4243 PARABANK 2 Step 1: Further pre-training Aligned X–? "" modified using WordNet / X–Y from Paraphrase data Step 3: Final Training Step 2: IBT training Model A Model A X→? "" Model A Model B Model B Y→X "" Model B Figure 1: General overview of our pipeline. 3.2 Task Evaluation The performance of text style transfer is commonly assessed on style strength and content preservation. For style strength, using a pre-trained style classifier is the most popular automatic evaluation strategy. For content preservation, n-gram-based matching metrics such as BLEU (Papineni et al., 2002) are most commonly used. However, these metrics usually fail to recognise information beyond the lexical level. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correla"
2021.emnlp-main.349,D14-1162,0,0.0850711,": IBT training Model A Model A X→? "" Model A Model B Model B Y→X "" Model B Figure 1: General overview of our pipeline. 3.2 Task Evaluation The performance of text style transfer is commonly assessed on style strength and content preservation. For style strength, using a pre-trained style classifier is the most popular automatic evaluation strategy. For content preservation, n-gram-based matching metrics such as BLEU (Papineni et al., 2002) are most commonly used. However, these metrics usually fail to recognise information beyond the lexical level. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correlation with human judgments. These metrics, with the prime examples of BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), have recently shown promising results in machine tran"
2021.emnlp-main.349,2020.emnlp-main.213,0,0.0181968,"l. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correlation with human judgments. These metrics, with the prime examples of BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), have recently shown promising results in machine translation evaluation. To the best of our knowledge, only our previous work used BLEURT in the evaluation of formality style transfer models (Lai et al., 2021); we are now proposing to use it also for the evaluation of polarity swap, and to add COMET to the pool of evaluation metrics to be systematically adopted in the evaluation of text style transfer tasks. Therefore, in addition to BLEU, which allows us to compare to previous work, we also use BLEURT and COMET. Let us bear in mind that “content preservation” does not mean exactly the same"
2021.emnlp-main.349,2020.acl-main.704,0,0.0248722,"ormation beyond the lexical level. Since word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have become the prime alternative to n-gram-based matching to capture similarity, embeddings-based metrics have also been developed (Fu et al., 2018). However, embedding-based metrics like cosine similarity still work at the token-level, and might fail to capture the overall semantics of a sentence. To overcome such limitations, recent work has developed learnable metrics, which attempt to directly optimize correlation with human judgments. These metrics, with the prime examples of BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), have recently shown promising results in machine translation evaluation. To the best of our knowledge, only our previous work used BLEURT in the evaluation of formality style transfer models (Lai et al., 2021); we are now proposing to use it also for the evaluation of polarity swap, and to add COMET to the pool of evaluation metrics to be systematically adopted in the evaluation of text style transfer tasks. Therefore, in addition to BLEU, which allows us to compare to previous work, we also use BLEURT and COMET. Let us bear in mind that “content preservation” do"
2021.emnlp-main.349,2020.emnlp-demos.6,0,0.0884038,"Missing"
2021.emnlp-main.349,P19-1482,0,0.0840865,"sfer literature, Text style transfer is, broadly put, the task con- since they do not use manually labelled data. verting a text of one style into another while preWe explore how parallel data can best be derived serving its content. In its recent tradition within and integrated in a general style transfer frameNatural Language Generation (NLG), two tasks work. To do so, we create pairs in a variety of and their corresponding datasets have been com- ways and use them in different stages of our framemonly used (Zhang et al., 2018; Luo et al., 2019; work. A core aspect of our approach is leveragWu et al., 2019; Yi et al., 2020; Zhou et al., 2020). ing generic resources to derive training pairs, both One dataset was specifically created for formality natural and synthetic. On the natural front, we transfer and contains parallel data (GYAFC (Rao use abundant data from a generic rewriting task: and Tetreault, 2018)), while the other one contains paraphrasing. As for synthetic data, we leverage a large amount of non-parallel sentiment labelled a general-purpose computational lexicon using its texts (YELP (Li et al., 2018)), with parallel pairs antonymy relation to generate polarity pairs. 4241 Proceedi"
2021.emnlp-main.349,P18-1090,0,0.021037,"g collection of generic paraphrases, as well as on synthetic pairs created using a general-purpose lexical resource. Second, through an iterative backtranslation (Hoang et al., 2018) approach, we train two models, each in a transfer direction, so that they can provide each other with synthetically generated pairs on-the-fly. Lastly, we use our best resulting model to generate static synthetic pairs, which are then used offline as parallel training data. be used as pairs to train the model of the opposite transfer direction. Another common strategy is to use style-wordediting (Li et al., 2018; Xu et al., 2018; Wu et al., 2019; Lee, 2020) to explicitly separate content and style. These approaches first detect relevant words in the source and then do operations like deleting, inserting and combining to create the pair’s target. Back-transferring is generally used to reconstruct the source sentence for training, so that pairs are also made on-the-fly. Lample et al. (2019) provide evidence that disentangling style and content to learn distinct representations (Shen et al., 2017; Fu et al., 2018; John et al., 2019; Yi et al., 2020) is not necessary. Reconstructing the source, instead, appears beneficia"
2021.emnlp-main.349,2020.acl-main.639,0,0.327674,"er is, broadly put, the task con- since they do not use manually labelled data. verting a text of one style into another while preWe explore how parallel data can best be derived serving its content. In its recent tradition within and integrated in a general style transfer frameNatural Language Generation (NLG), two tasks work. To do so, we create pairs in a variety of and their corresponding datasets have been com- ways and use them in different stages of our framemonly used (Zhang et al., 2018; Luo et al., 2019; work. A core aspect of our approach is leveragWu et al., 2019; Yi et al., 2020; Zhou et al., 2020). ing generic resources to derive training pairs, both One dataset was specifically created for formality natural and synthetic. On the natural front, we transfer and contains parallel data (GYAFC (Rao use abundant data from a generic rewriting task: and Tetreault, 2018)), while the other one contains paraphrasing. As for synthetic data, we leverage a large amount of non-parallel sentiment labelled a general-purpose computational lexicon using its texts (YELP (Li et al., 2018)), with parallel pairs antonymy relation to generate polarity pairs. 4241 Proceedings of the 2021 Conference on Empiric"
2021.findings-acl.433,2020.acl-main.421,0,0.0167176,"elds∗, Malvina Nissim and Martijn Wieling University of Groningen The Netherlands {wietse.de.vries, m.bartelds, m.nissim, m.b.wieling}@rug.nl Abstract languages that are not included in mBERT pretraining usually show poor performance (Nozza et al., 2020; Wu and Dredze, 2020). An alternative to multilingual transfer learning is the adaptation of existing monolingual models to other languages. Zoph et al. (2016) introduce a method for transferring a pre-trained machine translation model to lower-resource languages by only fine-tuning the lexical layer. This method has also been applied to BERT (Artetxe et al., 2020) and GPT-2 (de Vries and Nissim, 2020). Artetxe et al. (2020) also show that BERT models with retrained lexical layers perform well in downstream tasks, but comparatively high performance has only been demonstrated for languages for which at least 400MB of data is available. To test if this procedure is also effective for lowto zero-resource languages, we consider two regional language varieties spoken in the North of the Netherlands, namely Gronings (Low Saxon language variant) and West Frisian. For many (minority) languages, the resources needed to train large models are not available. We in"
2021.findings-acl.433,N19-1423,0,0.0281391,"Missing"
2021.findings-acl.433,2020.acl-main.156,0,0.0647324,"Missing"
2021.findings-acl.433,D16-1163,0,0.0787935,"Missing"
2021.findings-acl.74,D16-1250,0,0.167256,"kolov et al. (2013). They observe that words and their translations in other languages show similar constellations of related words after such a transformation. An alternative method that is generally considered to be an improvement (Ruder et al., 2019) is the orthogonal procrustes solution. This method adds the constraint that the transformation matrix must be orthogonal. In practice this means that the transfor837 mation only contains rotations and reflections and no scaling and translation. This constraint enables length normalisation (Xing et al., 2015) and ensures monolingual invariance (Artetxe et al., 2016). Mapping-based approaches rely on isomorphism, which means that a one-to-one token mapping between source and target lexical embedding spaces should be possible. This assumption is used for bilingual lexicon induction after alignment (Conneau et al., 2018). However, the isomorphism assumption highly depends on language similarity and (amount of) training data (Søgaard et al., 2018). Some more complex alignment methods like RCLS (Joulin et al., 2018) optimise for dictionary translation performance, which assumes isomorphism, but simpler methods like the orthogonal procrustes solution are more"
2021.findings-acl.74,2020.acl-main.421,0,0.043077,"with little parallel data (Zoph et al., 2016) as well as other classic NLP tasks (Lin et al., 2019). In machine translation a model can be adapted by initially training it for a high-resource language pair after which the model should be partially retrained for a low-resource language (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Retraining a randomly initialised lexical layer while freezing the rest of the model is an effective method to adapt a model to a new language, and dictionary based initialisation is not required to get the best performance (Zoph et al., 2016). Artetxe et al. (2020) show that a monolin1 https://huggingface.co/GroNLP https://github.com/wietsedv/ gpt2-recycle 2 gual BERT model can be adapted from a source language to a different target language by retraining the lexical layer for the target language while freezing the Transformer layers in the model. Zero shot adaptation for downstream tasks is possible by finetuning the original source model with source language data and swapping lexical layers afterwards. Lexical layer retraining approaches may be effective despite the presence of source and target language dissimilarities if a downstream task does not r"
2021.findings-acl.74,2020.acl-main.747,0,0.0789231,"Missing"
2021.findings-acl.74,2020.findings-emnlp.292,0,0.0259667,"nglish and Italian tend to have the same word order (SVO), while Dutch is SVO in main clauses, but SOV in subordinate ones; at noun phrase level, English and Dutch share constituent order (for example adjective-noun) while Italian is different (mostly noun-adjective). A GPT-2 based model has previously been trained from scratch for Italian (De Mattei et al., 2020). We can thus compare sentences generated by this model with sentences generated by our adapted model. For Dutch, no other GPT-2 based models exist, but similar BERT-based models have been trained from scratch (de Vries et al., 2019; Delobelle et al., 2020). Procedure Overview and Contributions When training a new language model, weights of an existing pre-trained model for another language can be used for initialisation. The first step in our training procedure is to only retrain the lexical embeddings of the GPT-2 small model, without touching the Transformer layers. We show that retrained lexical embeddings are well aligned with the English vo836 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 836–846 August 1–6, 2021. ©2021 Association for Computational Linguistics cabulary and that GPT-2 is capable of gener"
2021.findings-acl.74,P19-1070,0,0.041643,"Missing"
2021.findings-acl.74,D18-1330,0,0.0225603,"Missing"
2021.findings-acl.74,E17-2068,0,0.108471,"Missing"
2021.findings-acl.74,P19-1120,0,0.0152748,"ing approaches may be effective despite the presence of source and target language dissimilarities if a downstream task does not require perfect data. However, these methods have not been applied yet to generative language models where dissimilarities can cause clear syntactic and lexical errors. Language similarity plays a role in the effectiveness of transfer learning for language models. For instance, in machine translation French is a better parent model for Spanish than German (Zoph et al., 2016). Word order differences between languages can negatively influence transfer performance, and Kim et al. (2019) show that randomly swapping words in the source language, which forces the model to rely less on consistent word order, can improve performance in the target language. Overall, genetic similarity between source and target languages can play a role, but Lin et al. (2019) have shown that in practice the geographic distances between countries of origin, syntactic similarity and subword overlap are better predictors of transfer performance for machine learning, part-of-speech tagging, dependency parsing and entity linking. 2.2 Aligning word embeddings Alignment of lexical embeddings, for example"
2021.findings-acl.74,W18-6325,0,0.0156396,"arning can be an effective strategy to adapt models to lower-resource languages by initially training a model for a source language and then further training (parts of) the model for a target language. It has been successfully used to create machine translation models with little parallel data (Zoph et al., 2016) as well as other classic NLP tasks (Lin et al., 2019). In machine translation a model can be adapted by initially training it for a high-resource language pair after which the model should be partially retrained for a low-resource language (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Retraining a randomly initialised lexical layer while freezing the rest of the model is an effective method to adapt a model to a new language, and dictionary based initialisation is not required to get the best performance (Zoph et al., 2016). Artetxe et al. (2020) show that a monolin1 https://huggingface.co/GroNLP https://github.com/wietsedv/ gpt2-recycle 2 gual BERT model can be adapted from a source language to a different target language by retraining the lexical layer for the target language while freezing the Transformer layers in the model. Zero shot adaptation for downstream tasks i"
2021.findings-acl.74,2020.emnlp-main.215,0,0.0155073,"ping between source and target lexical embedding spaces should be possible. This assumption is used for bilingual lexicon induction after alignment (Conneau et al., 2018). However, the isomorphism assumption highly depends on language similarity and (amount of) training data (Søgaard et al., 2018). Some more complex alignment methods like RCLS (Joulin et al., 2018) optimise for dictionary translation performance, which assumes isomorphism, but simpler methods like the orthogonal procrustes solution are more effective for downstream tasks like natural language inference (Glavaˇs et al., 2019). Mohiuddin et al. (2020) propose a solution to the isomorphism problem by learning a new shared embedding space with an auto-encoding neural model instead of trying to fit the embeddings of one language in the space of another language. 3 Resources Models The models that we train are based on the pre-trained GPT-2 language models (Radford et al., 2019). GPT-2 is an auto-regressive Transformerdecoder based language model for English and comes in four sizes: small (12 layers), medium (24 layers), large (36 layers) and extra large (48 layers). Our experiments use the small (sml) and medium (med) model sizes. Pre-trainin"
2021.findings-acl.74,I17-2050,0,0.0135221,"uage transfer Transfer learning can be an effective strategy to adapt models to lower-resource languages by initially training a model for a source language and then further training (parts of) the model for a target language. It has been successfully used to create machine translation models with little parallel data (Zoph et al., 2016) as well as other classic NLP tasks (Lin et al., 2019). In machine translation a model can be adapted by initially training it for a high-resource language pair after which the model should be partially retrained for a low-resource language (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Retraining a randomly initialised lexical layer while freezing the rest of the model is an effective method to adapt a model to a new language, and dictionary based initialisation is not required to get the best performance (Zoph et al., 2016). Artetxe et al. (2020) show that a monolin1 https://huggingface.co/GroNLP https://github.com/wietsedv/ gpt2-recycle 2 gual BERT model can be adapted from a source language to a different target language by retraining the lexical layer for the target language while freezing the Transformer layers in the model. Zero shot adaptatio"
2021.findings-acl.74,N18-2012,0,0.0591519,"Missing"
2021.findings-acl.74,P19-1355,0,0.0177936,"embedding space. This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2. English GPT-2 models with relearned lexical embeddings can generate realistic sentences in Italian and Dutch. Though on average these sentences are still identifiable as artificial by humans, they are assessed on par with sentences generated by a GPT-2 model fully trained from scratch. 1 Introduction Large pre-trained language models have brought unprecedented progress in NLP, but also concerns regarding the excessive computing power needed to train them (Strubell et al., 2019). Limited access to large amounts of computational resources, as well as environmental considerations, curb possibilities for less-resourced and less-researched languages. Additionally, models like GPT-2 (Radford et al., 2019) are trained on amounts of data that are not available for most languages. As a result of these limitations, language models are commonly trained for English, whereas reproductions in other languages may underperform or not exist. That language models can benefit from information in other languages has been demonstrated by the effectiveness of multilingual BERT (mBERT) an"
2021.findings-acl.74,P18-1072,0,0.0306773,"Missing"
2021.findings-acl.74,2020.findings-emnlp.389,1,0.861673,"Missing"
2021.findings-acl.74,N15-1104,0,0.10118,"is the least-squares linear transformation method by Mikolov et al. (2013). They observe that words and their translations in other languages show similar constellations of related words after such a transformation. An alternative method that is generally considered to be an improvement (Ruder et al., 2019) is the orthogonal procrustes solution. This method adds the constraint that the transformation matrix must be orthogonal. In practice this means that the transfor837 mation only contains rotations and reflections and no scaling and translation. This constraint enables length normalisation (Xing et al., 2015) and ensures monolingual invariance (Artetxe et al., 2016). Mapping-based approaches rely on isomorphism, which means that a one-to-one token mapping between source and target lexical embedding spaces should be possible. This assumption is used for bilingual lexicon induction after alignment (Conneau et al., 2018). However, the isomorphism assumption highly depends on language similarity and (amount of) training data (Søgaard et al., 2018). Some more complex alignment methods like RCLS (Joulin et al., 2018) optimise for dictionary translation performance, which assumes isomorphism, but simpler"
2021.findings-acl.74,D16-1163,0,0.139722,"search relevant for the present work is found in the more general field of transfer learning, with a specific focus on language transfer. We also discuss how our approach of translating lexical layers in different model sizes relates to work on aligning word embeddings. 2.1 Language transfer Transfer learning can be an effective strategy to adapt models to lower-resource languages by initially training a model for a source language and then further training (parts of) the model for a target language. It has been successfully used to create machine translation models with little parallel data (Zoph et al., 2016) as well as other classic NLP tasks (Lin et al., 2019). In machine translation a model can be adapted by initially training it for a high-resource language pair after which the model should be partially retrained for a low-resource language (Zoph et al., 2016; Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Retraining a randomly initialised lexical layer while freezing the rest of the model is an effective method to adapt a model to a new language, and dictionary based initialisation is not required to get the best performance (Zoph et al., 2016). Artetxe et al. (2020) show that a monolin1 ht"
2021.gem-1.2,2021.ccl-1.108,0,0.0250209,"Missing"
2021.gem-1.2,2020.lrec-1.883,1,0.75383,"Missing"
2021.gem-1.2,N19-1423,0,0.00849659,"by the classifierbased reward used in style transfer tasks (Lample et al., 2019; Gong et al., 2019; Luo et al., 2019; Sancheti et al., 2020), we reward the model to push its classification confidence. We evaluate the new perception-enhanced models in comparison with the original GePpeTto by running both an automatic as well as a human evaluation on output generated by the various models. Lastly, we conduct a linguistic analysis to highlight which linguistic characteristics are more commonly found in human- and machine-perceived text. Introduction Pre-trained language models, such as the BERT (Devlin et al., 2019) and the GPT (Radford et al., 2018, 2019) families, are nowadays the core component of NLP systems. These models, based on the Transformer (Vaswani et al., 2017) and trained using huge amounts of crawl data (which can contain substantial noise), have been shown to produce high quality text, more often than not judged as human-written (Radford et al., 2019; De Mattei et al., 2020; Brown et al., 2020). Existing evaluations of GPT-2 models (Ippolito et al., 2020; De Mattei et al., 2020) have shown that while generated sentences were ranked lower in human perception than gold sentences, many gold"
2021.gem-1.2,N19-1320,0,0.0282807,"applications, as for example human-machine interaction in dialogues; the other is that it opens the opportunity to investigate what linguistic aspects make a text more humanly-perceived. We run our experiments on Italian, using GePpeTto (De Mattei et al., 2020) as pre-trained model. First, we collect human judgements on gold texts and texts generated by GePpeTto in terms of how they are perceived (human or automatically produced). We then fine-tune GePpeTto with this perceptionlabelled data. In addition, inspired by the classifierbased reward used in style transfer tasks (Lample et al., 2019; Gong et al., 2019; Luo et al., 2019; Sancheti et al., 2020), we reward the model to push its classification confidence. We evaluate the new perception-enhanced models in comparison with the original GePpeTto by running both an automatic as well as a human evaluation on output generated by the various models. Lastly, we conduct a linguistic analysis to highlight which linguistic characteristics are more commonly found in human- and machine-perceived text. Introduction Pre-trained language models, such as the BERT (Devlin et al., 2019) and the GPT (Radford et al., 2018, 2019) families, are nowadays the core comp"
2021.gem-1.2,2020.emnlp-demos.6,0,0.0193228,"Missing"
2021.gem-1.2,N19-1169,0,0.0282901,"GePpeTto is able to produce text which is much closer to human quality rather than to the text generated by other baseline models. Still, real human-produced text is recognised as such more often than GePpeTto’s output. k where φ are the parameters of GePpeTto, and Rk is the reward of the kth sequence y s sampled from the distribution of model’s outputs at each time step in decoding. The framework can be trained endto-end by combining the policy gradient with the cross entropy loss of the base model. 4 We run both a human and an automatic evaluation, in line with Ippolito et al. (2020)’s and Hashimoto et al. (2019)’s suggestions in terms of evaluation’s diversity and quality. For the automatic evaluation, we train a regressor on the perception-labelled data (with the original 1–5 values) adding a dropout (Srivastava et al., 2014) and a dense layer on the top of UmBERTo. We use Adam (Kingma and Ba, 2015) with initial learning rate is 1e-5, and set the batch size to 16. We calculate the correlation of the regressor’s scores with human judgements over each single data point in the test set (N=1400), and observe good scores (Pearson=0.54 (p &lt; 10−4 ) and RMSE=0.75). For the human evaluation, we assign to eac"
2021.teachingnlp-1.26,W13-3402,0,0.0188431,"oaching the problem with no previous knowledge helps raising some crucial questions, such as: what are the building blocks of our linguistic ability that allow us to perform such a task?, how much knowledge can we extract from text alone?, what does linguistic knowledge look like? Since the workshop here presented is the first activity of this kind in the Italian context, we took inspiration from games and problems such as those outlined in Radev and Pustejovsky (2013) and used for the North American Computational Linguistics Olympiads, similar to the ones described in Van Halteren (2002) and Iomdin et al. (2013). Particularly, we were inspired by the institution of (Computational) Linguistic Olympiads in making our workshop a problem-solving game with dif2 Genesis and Goals ferent activities, each related to a different aspect of computational language processing. Linguistic We set to develop an activity whose main aim Olympiads are now an established annual event in would be to provide a broad overview of language many parts of the world since they first took place modeling, and, most importantly, to highlight the in Moscow in 1965. In these competitions students open challenges in language understa"
2021.teachingnlp-1.26,W13-3403,0,0.0333036,"e understanding and (generally of high-school age) are faced with lingeneration. guistic problems of varying nature, that require Without any ambition to present and explain the participants to use problem-solving abilities to unactual NLP techniques to students, we rather focover underlying patterns or rules in the data. For cused on showing how language, which is usually an in-depth discussion of the history and diffusion conceptualized by the layperson as a simple and of Linguistic Olympiads in the world, see Derzhanmonolithic object, is instead a complex stratificaski and Payne (2010) and Littell et al. (2013). tion of interconnected layers that need to be disenIn the choice of algorithms to include in our tangled in order to provide a suitable formalization. dissemination activity, we decided to leave aside In developing our activity, we took inspiration neural networks and instead focus on traditional from the word salad Linguistic Puzzle, as pubstatistical approaches, both for historical reasons lished in Radev and Pustejovsky (2013): and for the fact that these convey more clearly the Charlie and Jane had been passing notes in distinction between different layers of linguistic inclass, when sud"
2021.teachingnlp-1.26,W02-0101,0,0.364574,"Missing"
2021.teachingnlp-1.7,2021.teachingnlp-1.26,1,0.597779,"depending on the participants’ background and metalinguistic awareness. Our interactive workshops took the form of modular games where participants, guided by trained tutors, acted as if they were computers that had to recognize speech and text, as well as to generate written sentences in a mysterious language they knew nothing about. The present contribution only describes the teaching materials and provide a general outline of the activities composing the workshop. For a detailed discussion and reflection on the workshop genesis and goals and on how it was received by the participants see (Pannitto et al., 2021). The teaching support consist in an interactive presentation plus hands-on material, either in hardcopy or digital form. We share a sample presentation3 and an open-access repository4 containing both printable materials to download and scripts to reproduce them on different input data. We describe and make available the gamebased material developed for a laboratory run at several Italian science festivals to popularize NLP among young students. 1 Malvina Nissim University of Groningen m.nissim@rug.nl Introduction The present paper aims at describing in detail the teaching materials developed"
2021.woah-1.6,S19-2007,0,0.0931745,"y create denser datasets, but at the same time risks of developing biased data are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 integrates different bottom-up approaches to collect data providing a first crossfertlization attempt across two social media platforms and paying attention to minimize the introduction of biases. Vidgen and Derczynski (2021) provides a comprehensive survey covering 63 datasets all targeting a specific abusive phenomenon/behavior. The majority of them (25 datasets) is for English, with a long tail of other languages mostly belonging to the Indo-European family, although limited in their diversity. The l"
2021.woah-1.6,2020.lrec-1.760,1,0.813205,"Missing"
2021.woah-1.6,P19-1271,0,0.0554259,"Missing"
2021.woah-1.6,2021.eacl-main.114,0,0.0185085,"ons exist and they mainly concentrate along three dimensions: (i) definitions; (ii) data sources and collection methods; and (iii) language diversity. The development of automatic methods for detecting forms of abusive language has been rapid and has seen a boom of definitions, labels, and phenomena being investigated, including racism (Waseem and Hovy, 2016a; Davidson et al., 2017, 2019), hate speech (Alfina et al., 2017; Founta et al., 2018; Mishra et al., 2018; Basile et al., 2019), toxicity3 and verbal aggression (Kumar et al., 2018), misogyny (Frenda et al., 2018; Pamungkas et al., 2020; Guest et al., 2021), and offensive language (Wiegand et al., 2018; Zampieri et al., 2019a; Rosenthal et al., 2020). Variations in definitions and in annotation guidelines have given rise to isolated datasets, limiting the portability of trained systems and reuse of resources (Swamy et al., 2019; Fortuna et al., 2021). Comprehensive frameworks that integrate and harmonize the variety of definitions and investigate the interactions across the annotated phenomena are still at early stages (Poletto et al., 2020). DALC v1.0 is compatible with existing definitions of abusive language and promotes a multi-layered annot"
2021.woah-1.6,2020.restup-1.4,1,0.724282,"Missing"
2021.woah-1.6,W18-4401,0,0.0281548,"Missing"
2021.woah-1.6,L18-1443,0,0.021205,"focuses on messages from social media platforms, with Twitter being the most used Vidgen and Derczynski (2021). Unlike other language phenomena, e.g., named entities, abusive language is less widespread and cannot be easily captured by means of random sampling. Schematically, we identify three major methods to collect data: namely: (i) use of communities (Tulkens et al., 2016; Del Vigna et al., 2017; Merenda et al., 2018; Kennedy et al., 2018) which targets online communities known to be more likely to have abusive behaviors; (ii) use of keywords (Waseem and Hovy, 2016b; Alfina et al., 2017; Sanguinetti et al., 2018; ElSherief et al., 2018; Founta et al., 2018), where manually compiled lists of words corresponding either to potential targets (e.g, “women”, “migrants”, a.o.) or profanities are employed; (iii) use of seed 3 Data Collection DALC v1.0 is based on a sample of a large ongoing collection of Twitter messages in Dutch at the University of Groningen (Tjong Kim Sang, 2011). For its construction, rather than focusing individually on any of the mentioned approaches, 3 The Toxic Comment Clas- sification Challenge https: //bit.ly/2QuHKD6 55 2018; Gerstenfeld, 2017).4 We use data from the Dutch Centraal"
2021.woah-1.6,P19-1163,0,0.0596307,"Missing"
2021.woah-1.6,C18-1093,0,0.135299,"lies on language-specific resources to train tools to distinguish the “good” messages from the harmful ones. As a contribution in this direction, we have developed the Dutch Abusive Language Corpus, or DALC v1.0, a manually annotated corpus of tweets for abusive language detection in Dutch.2 The resource is unique in the Dutch-speaking panorama because of the approach used to collect the data, the annotation guidelines, and the final data curation. DALC is compatible with previous work on abusive language in other languages (Waseem and Hovy, 2016a; Papegnies et al., 2017; Founta et al., 2018; Mishra et al., 2018; Davidson et al., 2019; Poletto et al., 2020) but presents innovations both with respect to the application of the label “abusive” to messages and the adoption of a multi-layered annotation to distinguish the explicitness of the abusive message and its target (Waseem et al., 2017). Our contributions can be summarized as follows: As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for a"
2021.woah-1.6,K19-1088,0,0.0314753,"Missing"
2021.woah-1.6,W17-3012,0,0.107455,"in Dutch.2 The resource is unique in the Dutch-speaking panorama because of the approach used to collect the data, the annotation guidelines, and the final data curation. DALC is compatible with previous work on abusive language in other languages (Waseem and Hovy, 2016a; Papegnies et al., 2017; Founta et al., 2018; Mishra et al., 2018; Davidson et al., 2019; Poletto et al., 2020) but presents innovations both with respect to the application of the label “abusive” to messages and the adoption of a multi-layered annotation to distinguish the explicitness of the abusive message and its target (Waseem et al., 2017). Our contributions can be summarized as follows: As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually annotated for abusive language. The resource address a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of"
2021.woah-1.6,N16-2013,0,0.126696,"nt.rug.nl gerbentimmerman@protonmail.com Abstract guage. Such step relies on language-specific resources to train tools to distinguish the “good” messages from the harmful ones. As a contribution in this direction, we have developed the Dutch Abusive Language Corpus, or DALC v1.0, a manually annotated corpus of tweets for abusive language detection in Dutch.2 The resource is unique in the Dutch-speaking panorama because of the approach used to collect the data, the annotation guidelines, and the final data curation. DALC is compatible with previous work on abusive language in other languages (Waseem and Hovy, 2016a; Papegnies et al., 2017; Founta et al., 2018; Mishra et al., 2018; Davidson et al., 2019; Poletto et al., 2020) but presents innovations both with respect to the application of the label “abusive” to messages and the adoption of a multi-layered annotation to distinguish the explicitness of the abusive message and its target (Waseem et al., 2017). Our contributions can be summarized as follows: As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Cor"
2021.woah-1.6,N19-1060,0,0.0588333,"available at https://github.com/ tommasoc80/DALC 1 https://what-europe-does-for-me.eu/ en/portal/2/H19 54 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 54–66 August 6, 2021. ©2021 Association for Computational Linguistics 2 Related Work users (Wiegand et al., 2018; Ribeiro et al., 2018), which collects messages from users that have been identified to post abusive texts via some heuristics. Each of these methods has advantages and disadvantages. For instance, the use of keywords may create denser datasets, but at the same time risks of developing biased data are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 int"
2021.woah-1.6,N18-1095,0,0.111781,"ated corpus for abusive language detection in Dutch, DALC v1.0; • a series of baseline experiments using different architectures (i.e., a dictionary based approach, a Linear SVM, a Dutch transformerbased language model) showing the complexity of the task. 2 The corpus, the annotation guidelines, and the baselines models are publicly available at https://github.com/ tommasoc80/DALC 1 https://what-europe-does-for-me.eu/ en/portal/2/H19 54 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 54–66 August 6, 2021. ©2021 Association for Computational Linguistics 2 Related Work users (Wiegand et al., 2018; Ribeiro et al., 2018), which collects messages from users that have been identified to post abusive texts via some heuristics. Each of these methods has advantages and disadvantages. For instance, the use of keywords may create denser datasets, but at the same time risks of developing biased data are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each ap"
2021.woah-1.6,N19-1144,0,0.192267,"a are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 integrates different bottom-up approaches to collect data providing a first crossfertlization attempt across two social media platforms and paying attention to minimize the introduction of biases. Vidgen and Derczynski (2021) provides a comprehensive survey covering 63 datasets all targeting a specific abusive phenomenon/behavior. The majority of them (25 datasets) is for English, with a long tail of other languages mostly belonging to the Indo-European family, although limited in their diversity. The lack of publicly available datasets for any Sino-Tibetan, Niger-Congo, or Afro-"
2021.woah-1.6,S19-2010,0,0.159841,"a are very high (Wiegand et al., 2019). Furthermore, according to the specific platform used, some of the methods cannot be reliably applied. For instance, in a platform like Twitter targeting online communities is not trivial. Recently, refinements have been proposed to address limitations of each approach. In some cases controversial posts, videos or keywords are used as proxies for communities (Hammer, 2016; Graumans et al., 2019), in other cases hybrid approaches are proposed by combining keywords and seed users (Basile et al., 2019), others exploit platform pre-filtering functionalities (Zampieri et al., 2019a). DALC v1.0 integrates different bottom-up approaches to collect data providing a first crossfertlization attempt across two social media platforms and paying attention to minimize the introduction of biases. Vidgen and Derczynski (2021) provides a comprehensive survey covering 63 datasets all targeting a specific abusive phenomenon/behavior. The majority of them (25 datasets) is for English, with a long tail of other languages mostly belonging to the Indo-European family, although limited in their diversity. The lack of publicly available datasets for any Sino-Tibetan, Niger-Congo, or Afro-"
alex-etal-2006-impact,W05-1306,0,\N,Missing
alex-etal-2006-impact,W03-0424,0,\N,Missing
alex-etal-2006-impact,W05-0304,0,\N,Missing
alex-etal-2006-impact,E03-1071,0,\N,Missing
alex-etal-2006-impact,grover-etal-2000-lt,1,\N,Missing
carletta-etal-2004-using,nissim-etal-2004-annotation,1,\N,Missing
J05-3004,P99-1008,0,0.355508,"pendent knowledge; different ways of encoding information; and sense ambiguity. 2 In this article, we restrict the notion of definite NPs to NPs modified by the article ‘the.’ 3 These systems also use surface-level features (such as string matching), recency, and grammatical constraints. In this article, we concentrate on the lexical and semantic knowledge employed. 368 Markert and Nissim Knowledge Sources for Anaphora Resolution In Section 3, we discuss an alternative to the manual construction of knowledge bases, which we call the corpus-based approach. A number of researchers (Hearst 1992; Berland and Charniak 1999, among others) have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (Poesio et al. 2002; Meyer and Dale 2002). Building on our previous work (Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two ways. First, we suggest using the Web for anaphora resolution instead of the smallersize, but less noisy and more balanced, corpora used previously, making available a huge additional source of knowledge.4 Second, we do n"
J05-3004,P01-1009,0,0.0263244,"Missing"
J05-3004,T75-2034,0,0.779722,"Missing"
J05-3004,W03-0424,0,0.0238403,"Missing"
J05-3004,W03-2410,0,0.0992571,"Missing"
J05-3004,C96-1084,1,0.837701,"Missing"
J05-3004,N01-1008,0,0.736438,"Missing"
J05-3004,C92-2082,0,0.646129,"nd context-dependent knowledge; different ways of encoding information; and sense ambiguity. 2 In this article, we restrict the notion of definite NPs to NPs modified by the article ‘the.’ 3 These systems also use surface-level features (such as string matching), recency, and grammatical constraints. In this article, we concentrate on the lexical and semantic knowledge employed. 368 Markert and Nissim Knowledge Sources for Anaphora Resolution In Section 3, we discuss an alternative to the manual construction of knowledge bases, which we call the corpus-based approach. A number of researchers (Hearst 1992; Berland and Charniak 1999, among others) have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (Poesio et al. 2002; Meyer and Dale 2002). Building on our previous work (Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two ways. First, we suggest using the Web for anaphora resolution instead of the smallersize, but less noisy and more balanced, corpora used previously, making available a huge additional source of"
J05-3004,W97-1307,0,0.0561876,"er the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other NLP tasks. 5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and antecedent are identical. 369 Computational Linguistics Volume 31, Number 3 2. The Knowledge Gap and Other Problems for Lexico-semantic Resources A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002; Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexical and world knowledge for the resolution of full NP anaphora and the lack of such knowledge in existing ontologies (Section 2.1). In addition to this knowledge gap, we summarize other, methodological problems with the use of ontologies in anaphora resolution (Section 2.2). 2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical Heads In the following, we discuss previous studies on the automatic resolution of cor"
J05-3004,J03-3005,0,0.0274785,"xical knowledge sources in isolation. It remains to be seen how the results carry forward when the knowledge sources interact with other features (for example, grammatical preferences). A similar issue concerns the integration of the methods into anaphoricity determination in addition to antecedent selection. Additionally, future work should explore the contribution of different knowledge sources for yet other anaphora types. 4 There is a growing body of research that uses the Web for NLP. As we concentrate on anaphora resolution in this article, we refer the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other NLP tasks. 5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and antecedent are identical. 369 Computational Linguistics Volume 31, Number 3 2. The Knowledge Gap and Other Problems for Lexico-semantic Resources A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio 2000; Harabagiu, Bunescu, and Ma"
J05-3004,C96-1021,0,0.101073,"Missing"
J05-3004,W03-2606,1,0.863702,"Missing"
J05-3004,W99-0108,0,0.061313,"Missing"
J05-3004,P98-2143,0,0.27725,"Missing"
J05-3004,W03-1023,1,0.797908,"Missing"
J05-3004,P04-1020,0,0.0119498,"hms. Second, the anaphoricity of the definite NPs in Case Study II has de facto been manually determined, as we restrict our study to antecedent selection for the NPs that are marked in the MUC corpus as coreferent. One of the reasons why pronoun resolution has been more successful than definite NP resolution is that whereas pronouns are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been argued by several researchers that an anaphora resolution algorithm should proceed to antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng 2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a twostage process which we also follow in this article. Although recent work on automatic anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our algorithms will perform worse when building on non-manually determined anaphors. Future work will explore the extent of such a decrease in performance. 44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors. 397 Computational Linguistics Volume 31, Number 3 6.2 Directions for Improvement All algor"
J05-3004,C02-1139,0,0.058851,"NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before. A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads. For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontologies; domain-specific and context-dependent k"
J05-3004,P02-1014,0,0.313949,"NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before. A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads. For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontologies; domain-specific and context-dependent k"
J05-3004,poesio-etal-2002-acquiring,0,0.153059,"Missing"
J05-3004,P04-1019,0,0.502075,"ey invoke and that invoked by the antecedent and are also easily used to accommodate subjective viewpoints. They should therefore benefit especially from not relying wholly on standard taxonomic links. Different patterns can be developed for anaphora types that build on nonhyponymy relations. For example, bridging exploits meronymy and/or causal relations (among others). Therefore, patterns that express “part-of” links, for example, such as X of Y and genitives, would be appropriate. Indeed, these patterns have been recently used in Web search for antecedent selection for bridging anaphora by Poesio et al. (2004). They compare accuracy in antecedent selection for a method that integrates Web hits and focusing techniques with a method that uses WordNet and focusing, achieving comparable results for both methods. This strenghtens our hypothesis that antecedent selection for full NP anaphora without hand-modeled lexical knowledge has become feasible. 7. Conclusions We have explored two different ways of exploiting lexical knowledge for antecedent selection in other-anaphora and definite NP coreference. Specifically, we have compared a hand-crafted and -structured source of information such as WordNet and"
J05-3004,W97-1301,0,0.337964,"Missing"
J05-3004,preiss-etal-2004-anaphoric,0,0.0435016,"Missing"
J05-3004,N04-1010,0,0.0204219,"errors. 397 Computational Linguistics Volume 31, Number 3 6.2 Directions for Improvement All algorithms we have described can be considered blueprints for more complex versions. Specifically, the WordNet-based algorithms could be improved by exploiting information encoded in WordNet beyond explicitly encoded links (glosses could be mined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]). The Webbased algorithms could similarly benefit from the exploration of different patterns and their combination, as well as from using non-pattern-based approaches for hyponymy detection (Shinzato and Torisawa 2004). In addition, we have evaluated the contribution of lexical resources in isolation rather than within a more sophisticated system that integrates additional non-lexical features. It is unclear whether integrating such knowledge sources in a full-resolution system might even out the differences between the Web-based and the WordNet-based algorithms or exacerbate them. Modjeska, Markert, and Nissim (2003) included a feature based on Web scores in a naive Bayes model for other-anaphora resolution that also used grammatical features and showed that the addition of the Web feature yielded an 11.4-"
J05-3004,W02-1040,0,0.0560673,"Missing"
J05-3004,P03-2012,0,0.041812,"nd, the anaphoricity of the definite NPs in Case Study II has de facto been manually determined, as we restrict our study to antecedent selection for the NPs that are marked in the MUC corpus as coreferent. One of the reasons why pronoun resolution has been more successful than definite NP resolution is that whereas pronouns are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been argued by several researchers that an anaphora resolution algorithm should proceed to antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng 2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a twostage process which we also follow in this article. Although recent work on automatic anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our algorithms will perform worse when building on non-manually determined anaphors. Future work will explore the extent of such a decrease in performance. 44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors. 397 Computational Linguistics Volume 31, Number 3 6.2 Directions for Improvement All algorithms we have d"
J05-3004,J00-4005,0,0.0531786,"Missing"
J05-3004,J00-4003,0,0.396229,"ssions than (increasing) costs. Similarly, in Example (4), the NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before. A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads. For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and"
J05-3004,J03-4002,0,0.0157584,", far-reaching repercussions. (WSJ) (4) The ordinance, in Moon Township, prohibits locating a group home for the handicapped within a mile of another such facility. (WSJ) In Example (1), the definite noun phrase (NP) the periodical corefers with the magazine.2 In Example (2), the definite NP the hood can be felicitously used because a related entity has already been introduced by the NP the jacket, and a part-of relation between the two entities can be established. Examples (3)–(4) are instances of other-anaphora. Other-anaphora are a subclass of comparative anaphora (Halliday and Hasan 1976; Webber et al. 2003) in which the anaphoric NP is introduced by a lexical modifier (such as other, such, and comparative adjectives) that specifies the relationship (such as set-complement, similarity and comparison) between the entities invoked by anaphor and antecedent. For other-anaphora, the modifiers other or another provide a set-complement to an entity already evoked in the discourse model. In Example (3), the NP other, far-reaching repercussions refers to a set of repercussions excluding increasing costs and can be paraphrased as other (far-reaching) repercussions than (increasing) costs. Similarly, in Ex"
J05-3004,P03-1023,0,0.453898,"Missing"
J05-3004,S01-1035,0,\N,Missing
J05-3004,P99-1016,0,\N,Missing
J05-3004,M95-1017,0,\N,Missing
J05-3004,J01-4004,0,\N,Missing
J05-3004,C98-2138,0,\N,Missing
J17-4007,W13-2212,0,0.0736247,"Missing"
J17-4007,P13-1166,0,0.125253,"is not clear what is learned; and (b) when algorithms are reimplemented for replicability purposes. Regarding (a), we think that differences in used parameter settings are not actually a bad thing; we learn from this that we overfit on the previous task, or that we need to adapt our systems to another data set or domain. Regarding (b), this is a real problem because starting from scratch to reimplement existing systems is unnecessarily time-consuming. In addition, it would always be desirable to be able to directly reproduce the same results of the same model for the same task (Pedersen 2008; Fokkens et al. 2013). Withdrawal from Competition. Participants may withdraw from a shared task if their ranking in the competition can negatively affect their reputation and/or future funding. For example, Parra Escart´ın et al. (2017) suggest that companies might prefer to withdraw from the competition if they are not highly ranked, to avoid blemishing their reputation. This is something that we could not quantify in our survey, as in case of withdrawal there would be no evidence of participation in reports. There are two aspects, though, that we can quantify. The first aspect is the number of teams that do not"
J17-4007,W13-3601,0,0.0214099,"o build on others’ work to try out new variations of a method, without having to reimplement things from scratch. To ensure this, it is desirable that everything needed to reproduce experimental results is publicly and freely available, including code, data, pre-trained models, and so on. Interestingly, at the CoNLL-2013 shared task a similar step was taken, but only in terms of pre-condition: “While all teams in the shared task use the NUCLE corpus, they are also allowed to use additional external resources (both corpora and tools) so long as they are publicly available and not proprietary” (Ng et al. 2013). We would like to take this a step further, by enforcing the sharing of whatever resource teams might choose to use, so as to favor the injection of new resources in the field. Applying this principle to shared tasks in practice, we propose making the primary competition a “public track,” where participants can use any code, data, and pre-trained models they want, as long as others can then freely obtain them. In other words: All resources used to participate in the shared task should be subsequently shared with the community. Although this does not ensure equal access to resources for the cu"
J17-4007,W17-1608,0,0.103254,"Missing"
J17-4007,J08-3010,0,\N,Missing
J17-4007,W14-1701,0,\N,Missing
L16-1449,W07-1604,0,0.0135527,"le prepositions. As outlined in Table 5, this can occur in two situations: because of a substitution error or because of a deletion error. Note that before feeding the vector with a preposition to this selection model, it is still unknown whether this preposition is correct or incorrect. Attesting this is done by analysing the output from the model. Table 6 illustrates the different outcomes of the selection model. For this task, we selected the 15 most frequent prepositions in Dutch (see Table 1), and trained an SVM model on vectors of native data, using the features in Table 7, inspired by (Chodorow et al., 2007). The selected preposition is predicted as a single-label classification task. 5. Results The detection and selection models were trained on 2M and 20M feature vectors, respectively. The models were subsequently tested on 268,895 feature vectors of unseen native data and on 1,499 items of the L2 data. 5.1. Baselines In order to have a lower bound to compare our system to, we devised a few baselines for each model, with increasing predictive power. Description Token bigram left of preposition Token bigram right of preposition Token trigram left of preposition Token trigram left of preposition V"
L16-1449,W12-2006,0,0.0704142,"Missing"
L16-1449,I08-1059,0,0.0519066,"Missing"
L16-1449,han-etal-2010-using,0,0.0215753,"One simple first step could just be using a spell corrector to improve the quality of L2 data and overall preprocessing. Linguistically-informed features on learners’ use of prepositions should also be investigated, though the variety of mother tongues in the L2 corpus doesn’t make this straightforward. Another interesting possibility to be investigated in this context would be to add artificially generated errors to the native training corpora (as suggested in (Rozovskaya and Roth, 2010)), so as to reduce the differences between native and learner language. Specifically, Rozovskaya and Roth (2010) show that introducing artificial errors in article usage in English in the training data increases results on 2823 learner test data. They make sure that the distribution of these errors is similar to the distribution of errors in ESL (English as a Second Language) data. This could prove very useful as it teaches the system what kind of errors can be expected and which errors are rare, and might be especially interesting for the case of prepositions as they are a closed class but with many possible different confusions. As far as the corpus itself is concerned, the LCN corpus proved to be use"
L16-1449,W13-3601,0,0.0522344,"Missing"
L16-1449,W14-1701,0,0.0384015,"Missing"
L16-1449,N10-1018,0,0.026485,"d embeddings (Mikolov et al., 2013). However, a prime direction for improvement is to make the model more familiar with L2 language. One simple first step could just be using a spell corrector to improve the quality of L2 data and overall preprocessing. Linguistically-informed features on learners’ use of prepositions should also be investigated, though the variety of mother tongues in the L2 corpus doesn’t make this straightforward. Another interesting possibility to be investigated in this context would be to add artificially generated errors to the native training corpora (as suggested in (Rozovskaya and Roth, 2010)), so as to reduce the differences between native and learner language. Specifically, Rozovskaya and Roth (2010) show that introducing artificial errors in article usage in English in the training data increases results on 2823 learner test data. They make sure that the distribution of these errors is similar to the distribution of errors in ESL (English as a Second Language) data. This could prove very useful as it teaches the system what kind of errors can be expected and which errors are rare, and might be especially interesting for the case of prepositions as they are a closed class but wi"
L16-1449,C08-1109,0,0.0662615,"Missing"
L16-1449,2006.jeptalnrecital-invite.2,0,0.0302997,"Missing"
markert-nissim-2002-towards,W98-0720,0,\N,Missing
markert-nissim-2002-towards,kilgarriff-rosenzweig-2000-english,0,\N,Missing
markert-nissim-2002-towards,N01-1009,0,\N,Missing
markert-nissim-2002-towards,C00-2128,0,\N,Missing
markert-nissim-2002-towards,P93-1012,0,\N,Missing
markert-nissim-2002-towards,J96-2004,0,\N,Missing
markert-nissim-2002-towards,P96-1006,0,\N,Missing
nissim-etal-2004-annotation,P98-1013,0,\N,Missing
nissim-etal-2004-annotation,C98-1013,0,\N,Missing
nissim-etal-2004-annotation,J96-2004,1,\N,Missing
nissim-etal-2004-annotation,carletta-etal-2004-using,1,\N,Missing
nissim-etal-2004-annotation,salmon-alt-vieira-2002-nominal,0,\N,Missing
nissim-perboni-2008-italian,baroni-etal-2004-introducing,0,\N,Missing
nissim-perboni-2008-italian,E93-1037,0,\N,Missing
nissim-perboni-2008-italian,J97-4002,0,\N,Missing
P03-1008,briscoe-carroll-2002-robust,0,0.0321221,"Missing"
P03-1008,J96-2004,0,0.132438,"Missing"
P03-1008,P92-1047,0,0.358394,"m generalises over two levels of contextual similarity. Resulting inferences exceed the complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 1 Introduction Metonymy is a figure of speech, in which one expression is used to refer to the standard referent of a related one (Lakoff and Johnson, 1980). In (1),1 “seat 19” refers to the person occupying seat 19. (1) Ask seat 19 whether he wants to swap The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., machine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993) and anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002). 1 (1) was actually uttered by a flight attendant on a plane. Katja Markert ICCS, School of Informatics University of Edinburgh and School of Computing University of Leeds markert@inf.ed.ac.uk In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be questioned, people occupy seats, people can be questioned). Metonymic readings are also potentially open-ended (Nunberg, 1978), so that developing a machine l"
P03-1008,J98-1002,0,0.0878372,"dividual word to treat regular sense distinctions.13 By exploiting additional similarity levels and integrating a thesaurus we further generalise the kind of inferences we can make and limit the size of annotated training data: as our sampling frame contains 553 different names, an annotated data set of 925 samples is quite small. These generalisations over context and collocates are also applicable to standard WSD and can supplement those achieved e.g., by subcategorisation frames (Martinez et al., 2002). Our approach to word similarity to overcome data sparseness is perhaps most similar to (Karov and Edelman, 1998). However, they mainly focus on the computation of similarity measures from the training data. We instead use an off-the-shelf resource without adding much computational complexity and achieve a considerable improvement in our results. 8 Conclusions We presented a supervised classification algorithm for metonymy recognition, which exploits the similarity between examples of conventional metonymy, operates on semantic classes and thereby enables complex inferences from training to test examples. We showed that syntactic head-modifier relations are a high precision feature for metonymy recogniti"
P03-1008,W02-1027,1,0.795724,"Missing"
P03-1008,markert-nissim-2002-towards,1,0.752782,"ctually quite regular (Lakoff and Johnson, 1980; Nunberg, 1995).2 In (2), “Pakistan”, the name of a location, refers to one of its national sports teams.3 (2) Pakistan had won the World Cup Similar examples can be regularly found for many other location names (see (3) and (4)). (3) England won the World Cup (4) Scotland lost in the semi-final In contrast to (1), the regularity of these examples can be exploited by a supervised machine learning algorithm, although this method is not pursued in standard approaches to regular polysemy and metonymy (with the exception of our own previous work in (Markert and Nissim, 2002a)). Such an algorithm needs to infer from examples like (2) (when labelled as a metonymy) that “England” and “Scotland” in (3) and (4) are also metonymic. In order to 2 Due to its regularity, conventional metonymy is also known as regular polysemy (Copestake and Briscoe, 1995). We use the term “metonymy” to encompass both conventional and unconventional readings. 3 All following examples are from the British National Corpus (BNC, http://info.ox.ac.uk/bnc). Pakistan had won the World Cup Scotland lost in the semi-final annotated corpus of occurrences of country names. context reduction Pakista"
P03-1008,W00-1326,0,0.011849,"rds of one semantic class and assign literal readings and metonymic patterns to new test instances of possibly different words of the same semantic class. This class-based approach enables one to, for example, infer the reading of (3) from that of (2). We use a decision list (DL) classifier. All features encountered in the training data are ranked in the DL (best evidence first) according to the following loglikelihood ratio (Yarowsky, 1995):  Log P r(readingi|f eaturek ) P r(readingj |f eaturek ) j6=i  P We estimated probabilities via maximum likelihood, adopting a simple smoothing method (Martinez and Agirre, 2000): 0.1 is added to both the denominator and numerator. The target readings to be distinguished are literal, place-for-people, place-forevent, place-for-product, othermet and mixed. All our algorithms are tested on our annotated corpus, employing 10-fold cross-validation. We evaluate accuracy and coverage: Acc = # correct decisions made # decisions made Cov = # decisions made # test data We also use a backing-off strategy to the most frequent reading (literal) for the cases where no decision can be made. We report the results as accuracy backoff (Accb ); coverage backoff is always 1. We are also"
P03-1008,C02-1112,0,0.0169953,"rare words that undergo regular sense alternations and do not have to annotate and train separately for every individual word to treat regular sense distinctions.13 By exploiting additional similarity levels and integrating a thesaurus we further generalise the kind of inferences we can make and limit the size of annotated training data: as our sampling frame contains 553 different names, an annotated data set of 925 samples is quite small. These generalisations over context and collocates are also applicable to standard WSD and can supplement those achieved e.g., by subcategorisation frames (Martinez et al., 2002). Our approach to word similarity to overcome data sparseness is perhaps most similar to (Karov and Edelman, 1998). However, they mainly focus on the computation of similarity measures from the training data. We instead use an off-the-shelf resource without adding much computational complexity and achieve a considerable improvement in our results. 8 Conclusions We presented a supervised classification algorithm for metonymy recognition, which exploits the similarity between examples of conventional metonymy, operates on semantic classes and thereby enables complex inferences from training to t"
P03-1008,P93-1012,0,0.655354,"similarity. Resulting inferences exceed the complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 1 Introduction Metonymy is a figure of speech, in which one expression is used to refer to the standard referent of a related one (Lakoff and Johnson, 1980). In (1),1 “seat 19” refers to the person occupying seat 19. (1) Ask seat 19 whether he wants to swap The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., machine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993) and anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002). 1 (1) was actually uttered by a flight attendant on a plane. Katja Markert ICCS, School of Informatics University of Edinburgh and School of Computing University of Leeds markert@inf.ed.ac.uk In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be questioned, people occupy seats, people can be questioned). Metonymic readings are also potentially open-ended (Nunberg, 1978), so that developing a machine learning algorithm based on previous e"
P03-1008,P02-1031,0,0.02341,"Missing"
P03-1008,W98-0720,0,0.693276,"he complexity of inferences undertaken in word sense disambiguation. We also compare automatic and manual methods for syntactic feature extraction. 1 Introduction Metonymy is a figure of speech, in which one expression is used to refer to the standard referent of a related one (Lakoff and Johnson, 1980). In (1),1 “seat 19” refers to the person occupying seat 19. (1) Ask seat 19 whether he wants to swap The importance of resolving metonymies has been shown for a variety of NLP tasks, e.g., machine translation (Kamei and Wakao, 1992), question answering (Stallard, 1993) and anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002). 1 (1) was actually uttered by a flight attendant on a plane. Katja Markert ICCS, School of Informatics University of Edinburgh and School of Computing University of Leeds markert@inf.ed.ac.uk In order to recognise and interpret the metonymy in (1), a large amount of knowledge and contextual inference is necessary (e.g. seats cannot be questioned, people occupy seats, people can be questioned). Metonymic readings are also potentially open-ended (Nunberg, 1978), so that developing a machine learning algorithm based on previous examples does not seem feasible. However,"
P03-1008,P95-1026,0,0.128787,"cular word and assigns word senses to new test instances of the same word, (supervised) metonymy recognition can be trained on a set of labelled instances of different words of one semantic class and assign literal readings and metonymic patterns to new test instances of possibly different words of the same semantic class. This class-based approach enables one to, for example, infer the reading of (3) from that of (2). We use a decision list (DL) classifier. All features encountered in the training data are ranked in the DL (best evidence first) according to the following loglikelihood ratio (Yarowsky, 1995):  Log P r(readingi|f eaturek ) P r(readingj |f eaturek ) j6=i  P We estimated probabilities via maximum likelihood, adopting a simple smoothing method (Martinez and Agirre, 2000): 0.1 is added to both the denominator and numerator. The target readings to be distinguished are literal, place-for-people, place-forevent, place-for-product, othermet and mixed. All our algorithms are tested on our annotated corpus, employing 10-fold cross-validation. We evaluate accuracy and coverage: Acc = # correct decisions made # decisions made Cov = # decisions made # test data We also use a backing-off stra"
P03-1008,J99-4002,0,\N,Missing
P15-1146,P05-1074,1,0.5637,"DB, annotated on MTurk as described in Section 5 1514 Lexical Distributional Paraphrase Translation Path WordNet We use the lemmas, POS tags, and phrase lengths of p1 and p2 , the substrings shared by p1 and p2 , and the Levenstein, Jaccard, and Hamming distances between p1 and p2 . Given a dependency context vectors for p1 and p2 , we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and CallisonBurch (2014) for a complete description of all of the features included with PPDB. We include the number of foreign language “pivots” (translations) shared by p1 and p2 for each of 24 languages used in the construction of PPDB, as a fraction of the total number of translations observed for each of p1 and p2 . We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which are observed between p1 and p2 in the Annotated Gigaword corpus (Napoles et al., 2012). We include binary features indicating whether Word"
P15-1146,D07-1017,0,0.0891599,"country/patriotic drive/vehicle family/home basketball/court playing/toy islamic/jihad delay/time Unrelated girl/play found/party profit/year man/talk car/family holiday/series green/tennis sunday/tour city/south back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves"
P15-1146,S14-2114,1,0.90105,"Missing"
P15-1146,W08-2222,1,0.588578,"Missing"
P15-1146,S14-2141,0,0.0747714,"dependent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphr"
P15-1146,W04-3205,0,0.0861134,"tempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, ma"
P15-1146,P11-1062,0,0.0709412,"Missing"
P15-1146,S13-2045,0,0.00626621,"ringboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference usin"
P15-1146,W09-0215,0,0.0129149,"ve of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). W"
P15-1146,C04-1051,0,0.466255,"sk of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extr"
P15-1146,ganitkevitch-callison-burch-2014-multilingual,1,0.922495,"Missing"
P15-1146,S14-2055,0,0.0262925,"trieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase tabl"
P15-1146,P98-2127,0,0.382648,"⇠ Cosine Similarity shades/the shade yard/backyard each other/man picture/drawing practice/target Monolingual (symmetric) ¬ large/small ⌘ few/several ¬ different/same ¬ other/same ¬ put/take Monolingual (asymmetric) A boy/little boy A man/two men A child/three children ⌘ is playing/play A side/both sides ⌘ A ⌘ ⌘ ⌘ Bilingual dad/father some kid/child a lot of/many female/woman male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. in X and in Y separate X from Y to X and/or to Y from X to Y more/less X than Y ate levels of agreement (Fleiss’s  = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification Table 4: Top paths associated with the ¬ class. We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and e"
P15-1146,W07-1431,0,0.153459,"wing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this t"
P15-1146,J10-3003,0,0.048805,"Missing"
P15-1146,N13-1092,1,0.191264,"Missing"
P15-1146,S14-2001,0,0.0556365,"Missing"
P15-1146,W07-1401,0,0.0157195,"RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a represent"
P15-1146,W12-3018,1,0.892473,"Missing"
P15-1146,D09-1122,0,0.082205,"Missing"
P15-1146,C92-2082,0,0.478737,"ources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 15"
P15-1146,P06-1101,0,0.0656433,"4), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly"
P15-1146,C08-1107,0,0.0134612,"of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-"
P15-1146,W04-3206,0,0.0162431,"Missing"
P15-1146,C04-1146,0,0.0100585,"es examples of some of the paths most indicative of the ¬ relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as describe"
P15-1146,Q14-1034,1,0.68632,"Missing"
P15-1146,S14-2044,0,0.0221701,"tion, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dola"
P15-1146,W07-1409,0,\N,Missing
P15-1146,C98-2122,0,\N,Missing
P15-1146,S14-2004,0,\N,Missing
P15-1146,J13-3001,0,\N,Missing
P18-2061,D11-1120,0,0.0396552,"ive: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approach"
P18-2061,W17-4407,0,0.0934559,"rkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 2017). Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017). Relying heavily on the lexicon though has its limitations, as it results in models with limited portability. Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011). Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubeši´c et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this"
P18-2061,C14-1184,0,0.0612336,"Missing"
P18-2061,D12-1139,0,0.0579617,"Missing"
P18-2061,P16-1080,0,0.100081,"Missing"
P18-2061,N07-1051,0,0.0192212,"ence of additional user-specific metadata. Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowl"
P18-2061,W16-3920,0,0.0222583,"aking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and"
P18-2061,W17-2901,1,0.855355,"Missing"
P18-2061,D15-1130,0,0.0306333,"ans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 20"
P18-2061,I17-4024,1,0.880203,"Missing"
P18-2061,W15-2913,1,0.891191,"eatures. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an auth"
P18-2061,P16-2067,1,0.838314,"ach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can pr"
P18-2061,P15-1169,0,0.0610355,"Missing"
P18-2061,W11-0310,0,0.018254,"yle beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 2017). Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017). Relying heavily on the lexicon though has its limitations, as it results in models with limited portability. Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011). Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubeši´c et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this information is not always available. We propose a novel approach where the actual text is still used, but bleached out and transformed into more abstract, and potentially better transferable features. One could view this as a method in between the open vocabulary strategy and the stylometric approach. It has the advantage of fading"
P18-2061,Q14-1002,0,0.020286,"specific metadata. Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a g"
P18-2061,L16-1258,1,0.881658,"rediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine. If humans can predict gender cross-lingually, they are likely to rely on aspects beyond lexical information. Data We obtain data from the T WI S TY corpus (Verhoeven et al., 2016), a multi-lingual collection of Twitter users, for the languages with 500+ users, namely Dutch, French, Portuguese, and Spanish. We complement them with English, using data from a predecessor of T WI S TY (Plank and Hovy, 2015). All datasets contain manually annotated gender information. To simplify interpretation for the cross-language experiments, we balance gender in all datasets by downsampling to the minority class. The datasets’ final sizes are given in Table 2. We use 200 tweets per user, as done by previous work (Verhoeven et al., 2016). We leave the data untokenized to exclude any lan"
P19-1246,K16-1018,0,0.0141984,"determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics"
P19-1246,P16-2051,0,0.0387176,"Missing"
P19-1246,P18-2061,1,0.832182,"Missing"
P19-1246,D15-1162,0,0.0809055,"hese non-lexical feature representations, we use a Convolutional Neural Network (CNN) classifier (LeCun et al., 1995), rather than rely on sparse models as we did for our lexical baseline.4 The model consists of a single convolutional layer coupled with a sum-pooling operation; a Multi-Layer Perceptron on top improves discrimination performance between classes. We use the Adam optimizer (Kingma and Ba, 2015) with a fixed learning rate (0.001) and L2 regularization (Ng, 2004); a dropout layer (0.2) (Srivastava et al., 2014) helps to prevent overfitting. For the implementation we rely on spaCy (Honnibal and Johnson, 2015). 6.1 Bleached representation Recently, van der Goot et al. (2018) introduced a language-independent representation termed bleaching for capturing gender differences in writing style, while abstracting away from lexical information. Bleaching preserves surface information while obfuscating lexical content. This allows a focus on lexical variation as a function of personal style, while reducing the possible influence of topic as a determining factor. We experiment with this idea under the assumption that authors belonging to different groups will show a difference in the formality of their writ"
P19-1246,P15-2079,0,0.0240695,"butes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics can be found in Nguyen et al. (2016). By contrast, there has been relatively little work on socio-economic status. Flekova et al. (2016) show that textual features can predict income, demonstrating a relationship between this and age. Lampos et al. (2016) also report good results on inferring the socio-economic status of social media users from text. Like the present work, they use distant supervision, exploiting occupation information in Twitter p"
P19-1246,K15-1011,0,0.128815,"price/prestige range, under the assumption that customers (and salespersons) of these establishments would belong to different social strata. ""[Labov’s study] was designed to test two ideas [. . . ]: first, that the variable (r) is a social differentiator in all levels of New York City speech; and second, that casual and anonymous speech events could be used as the basis for a systematic study of language."" (Labov, 2006, p. 40. Italics ours.) Inspired by Labov’s work and the recent surge of interest in computational social science (CioffiRevilla, 2016) and computational sociolinguistics (e.g. Johannsen et al., 2015), we set out to investigate whether and to what extent variations in writing style, analysed in terms of several linguistic variables, are influenced by socio-economic status (RQ1; see below). To do so, we use user-generated restaurant reviews on social media. User-generated content bears important similarities to Labov’s ""casual and anonymous speech events"" on at least two fronts: 1) anonymity is here still preserved since we are not including personal information about the authors; furthermore 2) social media are now recognised in the literature as a source of naturally (i.e. casual) occurri"
P19-1246,P12-3005,0,0.0415768,"ains more than 5 million documents, from over 1 million authors, with a Zipfian distribution: a small number of authors publish most of the reviews, while most of the authors only leave one review. Grouping reviews per author and filtering out authors with only one review reduces the final dataset to fewer than a thousand authors, though this set of reviews is large and allows us to infer demographic information about the reviewers (see also Hovy et al., 2015). Language The Yelp! dataset contains reviews written in multiple languages, though the vast majority are in English. We use langid.py (Lui and Baldwin, 2012) to automatically detect and filter out non-English instances. The need for both good parsing performance and large quantity of text limits us from working with data from other languages. rant as a proxy for socio-economic status. The average price of a meal in a restaurant is encoded by four labels: $, $$, $$$, $$$$. As a first, coarse step, we accept this representation and divide our population into four groups. We group all of the reviews per author and represent each author as a vector, where each element is the price range of a restaurant reviewed by the user. We compute the mode of this"
P19-1246,W17-5007,0,0.0279096,"computational sociolinguistics, we developed accurate neural models to predict socioeconomic status from text. While lexical information is highly predictive, it is restricted to topic. In contrast, syntactic information is almost as predictive and is a much better signal for stylistic varia2590 tion. From a methodological point of view, we can draw two conclusions from this work. First, as has been noted (Plank et al., 2016), neural networks can perform well with relatively small datasets, in this case proving competitive with the sparse models that are usually favoured in author profiling (Malmasi et al., 2017; Basile et al., 2018). Second, distant supervision with proxy labels for socio-economic status yields useful insights and is validated externally via readability scores. This is encouraging for further studies in computational social science in ecologically valid and relatively labour-free settings. Nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (Askalidis and Malthouse, 2016) — that we will take into account in future work. First, we wish to investigate the role of additional variables (such as age and"
P19-1246,petrov-etal-2012-universal,0,0.0194458,"Missing"
P19-1246,C14-1168,0,0.0281915,"ic status influences language use and is a determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general"
P19-1246,P16-2067,0,0.015995,"2014) finds an interesting correlation between the price range of a restaurant and the lengths of food names on its menu. 9 Conclusion Inspired by Labov and encouraged by recent interest in computational sociolinguistics, we developed accurate neural models to predict socioeconomic status from text. While lexical information is highly predictive, it is restricted to topic. In contrast, syntactic information is almost as predictive and is a much better signal for stylistic varia2590 tion. From a methodological point of view, we can draw two conclusions from this work. First, as has been noted (Plank et al., 2016), neural networks can perform well with relatively small datasets, in this case proving competitive with the sparse models that are usually favoured in author profiling (Malmasi et al., 2017; Basile et al., 2018). Second, distant supervision with proxy labels for socio-economic status yields useful insights and is validated externally via readability scores. This is encouraging for further studies in computational social science in ecologically valid and relatively labour-free settings. Nevertheless, there are limitations of distant labelling and social media data — with issues related specifi"
P19-1246,W16-4304,1,0.859234,"language use and is a determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computatio"
P19-1246,W14-5307,0,0.0243786,"Missing"
P19-1246,P05-2008,0,0.100302,"el). 8 Related Work The idea that socio-economic status influences language use and is a determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Ver"
P19-1246,D17-1035,0,0.0487601,"Missing"
P19-1246,W11-0310,0,0.0280438,"kes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics can be found in Nguyen et al. (2016). By contrast, there has been relatively little work on socio-economic status. Flekova et al. (2016) show that textual features can predict income, demonstrating a relationship between this and age. Lampos et al. (2016) also report good results on inferring the socio-economic status of social media users from text. Like the present work, they use distant supervisio"
P19-1246,L16-1258,0,0.025161,"05), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics can be found in Nguyen et al. (2016). By contrast, there has been relatively little work on socio-economic status. Flekova et al. (2016) show that textual features can predict income, demonstrating a relationship between this and age. Lampos et al. (2016) also report good results on inferring the socio-economic status of social media users from text. Like the present work, they use distant supervision, exploiting occupation information in Twitter profiles. Our work differs from these precedents in that we investigate"
P19-1246,N16-1174,0,0.0138096,"th labels which denote the social class of the authors we adopt the paradigm of distant supervision. We take the price range of the restau(c) Balance: the raw data is highly skewed towards class $$ (Figure 2), but for our experiments we want equally represented classes to avoid any size-related effects. 1 The repository contains all code and models which can be run by acquiring the freely available Yelp dataset. 2 This data is released within the context of the Yelp! Challenge, a multi-domain shared task which has attracted attention in NLP primarily for benchmarking text classification (e.g. Yang et al., 2016)). We use the dataset released for Round 11. Y $ of reviewed restaurant In order to address (a), we employ an entropybased strategy to filter out noisier data points. This 2584 Table 2 shows the final label and token distribution, after filtering and downsampling. In Figure 3, we show two sample reviews, one from class $ and one from class $$$$. $$ $ $$$ $$$$ Figure 2: Author distribution before filtering. While users belonging to class $$$$ might visit cheaper places, the same is not true in the opposite direction: this explains the small size of class $$$$. is described below. For the size-"
P19-1246,J16-3007,0,\N,Missing
P19-1246,W13-1706,0,\N,Missing
P19-1246,W13-1102,0,\N,Missing
S07-1007,E03-1067,0,0.0617854,"attlesnake tastes like chicken.2 Again, the meat read1 This example was taken from the Berkely Master Metaphor list (Lakoff and Johnson, 1980) . 2 From now on, all examples in this paper are taken from the British National Corpus (BNC) (Burnard, 1995), but Ex. 23. Malvina Nissim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, co"
S07-1007,E06-1042,0,0.014413,"inguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1) Sex, drugs, and Vietnam have haunted Bi"
S07-1007,J96-2004,0,0.0214098,", respectively. Before metonymy annotation, samples that were not understood by the annotators because of insufficient context were removed from the datsets. In addition, a sample was also removed if the name extracted was a homonym not in the desired semantic class (for example Mr. Greenland when annotating locations).4 For those names that do have the semantic class location or organisation, metonymy annotation was performed, using the categories described in Section 2. All training set annotation was carried out independently by both organisers. Annotation was highly reliable with a kappa (Carletta, 1996) of 3 Apart from class-specific metonymic readings, some patterns seem to apply across classes to all names. In the SemEval dataset, we annotated two of them. Chevrolet is feminine because of its sound (it’s a longer word than Ford, has an open vowel at the end, connotes Frenchness). https://www.cia.gov/cia/publications/ factbook/index.html 4 Given that the task is not about standard Named Entity Recognition, we assume that the general semantic class of the name is already known. larity: coarse, medium, or fine, with an increasing number and specification of target classification categories, a"
S07-1007,W98-0720,0,0.475797,"Missing"
S07-1007,P92-1047,0,0.881369,"Missing"
S07-1007,W07-0103,0,0.0175944,"tudies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1) Sex, drugs, and Vietnam have haunted Bill Clinton’s campaign. In Ex. 2"
S07-1007,W02-1027,1,0.847667,"es 3 and 4 report accuracy for all systems. 6 Table 5 provides a summary of the results with lowest, highest, and average accuracy and f-scores for each subtask and granularity level.7 The task seemed extremely difficult, with 2 of the 5 systems (up13,FUH) participating in the location task not beating the baseline. These two systems relied mainly on shallow features with limited or no use of external resources, thus suggesting that these features might only be of limited use for identifying metonymic shifts. The organisers themselves have come to similar conclusions in their own experiments (Markert and Nissim, 2002). The systems using syntactic/grammatical features (GYDER, UTD-HLT-CG, XRCE-M) could improve over the baseline whether using manual annotation or parsing. These systems also made heavy use of feature generalisation. Classification granularity had only a small effect on system performance. Only few of the fine-grained categories could be distinguished with reasonable success (see the fscores in Table 5). These include literal readings, and place-for-people, org-for-members, and org-forproduct metonymies, which are the most frequent categories (see Tables 1 and 2). Rarer metonymic targets were e"
S07-1007,J04-1002,0,0.0186404,"23. Malvina Nissim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1"
S07-1007,P03-1008,1,0.870149,"(Burnard, 1995), but Ex. 23. Malvina Nissim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happe"
S07-1007,E06-3009,0,0.0761844,"issim Dept. of Linguistics and Oriental Studies University of Bologna, Italy malvina.nissim@unibo.it ing of rattlesnake is not listed in WordNet whereas the meat reading for chicken is. As there is no common framework or corpus for figurative language resolution, previous computational works (Fass, 1997; Hobbs et al., 1993; Barnden et al., 2003, among others) carry out only smallscale evaluations. In recent years, there has been growing interest in metaphor and metonymy resolution that is either corpus-based or evaluated on larger datasets (Martin, 1994; Nissim and Markert, 2003; Mason, 2004; Peirsman, 2006; Birke and Sarkaar, 2006; Krishnakamuran and Zhu, 2007). Still, apart from (Nissim and Markert, 2003; Peirsman, 2006) who evaluate their work on the same dataset, results are hardly comparable as they all operate within different frameworks. This situation motivated us to organise the first shared task for figurative language, concentrating on metonymy. In metonymy one expression is used to refer to the referent of a related one, like the use of an animal name for its meat. Similarly, in Ex. 1, Vietnam, the name of a location, refers to an event (a war) that happened there. (1) Sex, drugs, an"
S07-1007,P93-1012,0,0.777408,"Missing"
S14-2114,S14-2001,0,0.127342,"t and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) and statistical semantics. A promising way to provide insight into these questions was brought forward as Shared Task 1 in the SemEval-2014 campaign for semantic evaluation (Marelli et al., 2014). In this task, a system is given a set of sentence pairs, and has to predict for each pair whether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations u"
S14-2114,W08-2222,1,0.883349,"hether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1 To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&C tools. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings"
S14-2114,P07-2009,1,0.612665,"s is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations used for textual entailment are 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1 To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&C tools. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org"
S14-2114,N13-1092,0,0.217137,"works as follows: (i) produce a formal semantic representation for each sentence for a given sentence pair; (ii) translate these semantic representations into first-order logic; (iii) use off-theshelf theorem provers and model builders to check whether the first sentence entails the second, or whether the sentences are contradictory. This is essentially an improved version of the framework introduced by Bos & Markert (2006). To generate background knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma & Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) a"
S18-1167,L18-1320,1,0.768875,"rly given that our system is a minimally supervised system that requires no training and minimal parameter optimisation. In addition to describing the submitted system, and discussing the implications of the relative success of such a system on this task, we also report on other, more complex models we experimented with. 1 2 Introduction and Background Traditional evaluation tasks for semantic models have aimed to evaluate semantic relatedness (Bruni et al., 2014; Agirre et al., 2009) or similarity (Hill et al., 2014). More recently, analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016; Abdou et al., 2018) have emerged in order to assess a model’s ability to correctly answer questions in the form of “a is to b as c is to ?” using vector arithmetic. However, these approaches are not sufficient in evaluating the semantic competence of any given model: there are numerous flaws with similarity and analogy-based evaluation, the most pressing being the lack of correlation with downstream performance in real-world tasks (Schnabel et al., 2015). Furthermore, though analogy questions can assess how well certain semantic relations are modeled (country : capital, country: language), this is arguably more"
S18-1167,N09-1003,0,0.17152,"Missing"
S18-1167,D15-1075,0,0.0330277,"Missing"
S18-1167,D17-1070,0,0.0299604,"Missing"
S18-1167,N16-2002,0,0.0132348,"core of 0.75, particularly given that our system is a minimally supervised system that requires no training and minimal parameter optimisation. In addition to describing the submitted system, and discussing the implications of the relative success of such a system on this task, we also report on other, more complex models we experimented with. 1 2 Introduction and Background Traditional evaluation tasks for semantic models have aimed to evaluate semantic relatedness (Bruni et al., 2014; Agirre et al., 2009) or similarity (Hill et al., 2014). More recently, analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016; Abdou et al., 2018) have emerged in order to assess a model’s ability to correctly answer questions in the form of “a is to b as c is to ?” using vector arithmetic. However, these approaches are not sufficient in evaluating the semantic competence of any given model: there are numerous flaws with similarity and analogy-based evaluation, the most pressing being the lack of correlation with downstream performance in real-world tasks (Schnabel et al., 2015). Furthermore, though analogy questions can assess how well certain semantic relations are modeled (country : capital, country: language), t"
S18-1167,D14-1162,0,0.0867001,"Y(w) 2: s12 ← S IM(w1 , w2 ) 3: s13 ← S IM(w1 , w3 ) 4: s23 ← S IM(w2 , w3 ) 5: if s13 − s23 > 0.015 & s12 > 0.30 & s13 > 0.1 & s23 < 0.54 then 6: return 1 7: else 8: return 0 These thresholds were obtained via grid-search over both the training data and validation data, per VSM. The range of evaluated thresholds was between 0 and 0.50, with strides of 0.02. Besides choice of VSM, these thresholds were the only variable parameters in our model. The VSM used in our final submission consisted of an average of three sets of embeddings: GloVe word embeddings trained on Common Crawl (840B tokens) (Pennington et al., 2014), the same GloVe embeddings post counter-fitting (Mrkˇsi´c et al., 2016) using data from the training and valida1009 tion sets, and Paragramsl999 embeddings provided by Wieting et al. (2015), also post counter-fitting. Counter-fitting is detailed in Section 4.2. The VSM obtained by averaging these three models (AvgVSM) outperformed each individual VSM, and was therefore submitted as our official system. 4 Model Variations 4.1 Distributional Vector-Space models In the course of our investigation we tested a large number of vector-space models which were generated using different methods. All VS"
S18-1167,P16-2068,0,0.0175613,"1 Distributional Vector-Space models In the course of our investigation we tested a large number of vector-space models which were generated using different methods. All VSMs were evaluated with each of our models and with our final system in order to determine the model which best encodes the discriminative information required for this task. Below is a description of all VSMs we tried: (a) Skipgram embeddings trained on the Google News Corpus (Mikolov et al., 2013). (b) Glove embeddings trained on Common Crawl (6B and 840B tokens). (c) LexVec embeddings trained on Common Crawl (58B tokens) (Salle et al., 2016). (d) Paragram embeddings trained on English Wikipedia1 and tuned on SimLex-999. (e) Items (b) (840B), (c), and (d) counter-fitted using the training and validation sets. (f) AvgVSM: Average of three models cf. Section 3.3. 4.2 Counter-fitting Counter-fitting is a method of post-processing VSMs to adapt them to certain linguistic constraints such as information from semantic lexicons or ontologies. Mrkˇsi´c et al. (2016) for instance, successfully used counter-fitting with semantic lexicons to achieve a new state of the art on the SimLex-999 similarity judgment dataset. We employed counter-fit"
S18-1167,D15-1036,0,0.0147321,"relatedness (Bruni et al., 2014; Agirre et al., 2009) or similarity (Hill et al., 2014). More recently, analogy tasks (Mikolov et al., 2013; Gladkova et al., 2016; Abdou et al., 2018) have emerged in order to assess a model’s ability to correctly answer questions in the form of “a is to b as c is to ?” using vector arithmetic. However, these approaches are not sufficient in evaluating the semantic competence of any given model: there are numerous flaws with similarity and analogy-based evaluation, the most pressing being the lack of correlation with downstream performance in real-world tasks (Schnabel et al., 2015). Furthermore, though analogy questions can assess how well certain semantic relations are modeled (country : capital, country: language), this is arguably more of a measure of context co-occurrence than it is a testament to any semantic understanding. Capturing Discriminative Attributes is a novel semantic evaluation task which aims to assess the extent to which semantic models can capture semantic differences between words. Particularly, Data The data provided for this task was split between training and validation sections, with 17,500 and 2,721 samples comprising the former and latter, res"
S18-1167,J15-4004,0,0.0551838,"Missing"
S18-1167,S18-1117,0,0.131316,"ponding label ∈ {0, 1}. A sample was deemed as discriminative (label 1) if the feature served as an attribute that distinguished the pivot from the comparison. Otherwise, the sample was labeled as non-discriminative (0). Examples from the data are shown in Table 2. Please note that directionality is meaningful, i.e., swapping the pivot and comparison columns for the first two examples can change the label. For instance, pink is considered as discriminative for pig with regard to sheep (label is 1), but not the other way round (label would be 0). For a detailed description of the dataset, see (Krebs et al., 2018). pivot comparison feature sandwiches pig banana uncle breakfast sheep raisin father lunch pink round male label 1 1 0 0 Table 1: Samples of provided data. In the first two samples, the feature is discriminative (sandwiches are eaten at lunch; pigs are pink). In the last two, the feature is not discriminative (neither bananas nor raisins are round; an uncle and a father are both male). 1008 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1008–1012 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics It is important"
W02-1027,W98-0720,0,\N,Missing
W02-1027,markert-nissim-2002-towards,1,\N,Missing
W02-1027,N01-1009,0,\N,Missing
W02-1027,N01-1011,0,\N,Missing
W02-1027,J99-4002,0,\N,Missing
W02-1027,P95-1026,0,\N,Missing
W02-1027,W00-1326,0,\N,Missing
W02-1027,P92-1047,0,\N,Missing
W02-1027,P93-1012,0,\N,Missing
W02-1027,J96-2004,0,\N,Missing
W02-1027,P96-1006,0,\N,Missing
W02-1027,A00-2009,0,\N,Missing
W02-1027,W99-0613,0,\N,Missing
W03-1023,P95-1017,0,0.141436,"Missing"
W03-1023,P99-1008,0,0.0363221,"Missing"
W03-1023,P01-1009,0,0.101889,"Missing"
W03-1023,W03-2607,0,0.0573518,"in our corpus). 6 Related Work and Discussion Modjeska (2002) presented two hand-crafted algorithms, SAL and LEX, which resolve the anaphoric references of other-NPs on the basis of grammatical salience and lexical information from WordNet, respectively. In our own previous work (Markert et 15 If several antecedents have the highest M Iant they all get value “webfirst”. al., 2003) we presented a preliminary symbolic approach that uses Web counts and a recency-based tie-breaker for resolution of other-anaphora and bridging descriptions. (For another Web-based symbolic approach to bridging see (Bunescu, 2003).) The approach described in this paper is the first machine learning approach to other-anaphora. It is not directly comparable to the symbolic approaches above for two reasons. First, the approaches differ in the data and the evaluation metrics they used. Second, our algorithm does not yet constitute a full resolution procedure. As the classifier operates on the whole set of antecedent-anaphor pairs, more than one potential antecedent for each anaphor can be classified as “antecedent=yes”. This can be amended by e.g. incremental processing. Also, the classifier does not know that each other-N"
W03-1023,W99-0501,0,0.0612947,"Missing"
W03-1023,C92-2082,0,0.138269,"ture set is encouraging. However, the semantic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that"
W03-1023,W02-1030,0,0.0176733,"mpetition learning approach (Connolly et al., 1997). Finally, the full resolution procedure will have to take into account other factors, e.g., syntactic constraints on antecedent realization. Our approach is the first ML approach to any kind of anaphora that integrates the Web. Using the Web as a knowledge source has considerable advantages. First, the size of the Web almost eliminates the problem of data sparseness for our task. For this reason, using the Web has proved successful in several other fields of NLP, e.g., machine translation (Grefenstette, 1999) and bigram frequency estimation (Keller et al., 2002). In particular, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we obtain semantic information that is not and perhaps should not be encoded in an ontology (redescriptions, vague relations, etc.). Finally, these local dependencies al"
W03-1023,W03-2606,1,0.561576,"antecedent is not the flashy sports, but rather flashy sport shows, and thus an important piece of information is omitted. Alternatively, the antecedent is a content-for-container metonymy. Overall, our approach misclassifies antecedents whose relation to the other-anaphor is based on similarity, property-sharing, causality, or is constrained to a specific domain. These relation types are not — and perhaps should not be — encoded in WordNet. 5 Naive Bayes with the Web With its approximately 3033M pages8 the Web is the largest corpus available to the NLP community. Building on our approach in (Markert et al., 2003), we suggest using the Web as a knowledge source for anaphora resolution. In this paper, we show how to integrate Web counts for lexico-syntactic patterns specific to other-anaphora into our ML approach. 5.1 Basic Idea In the examples we consider, the relation between anaphor and antecedent is implicitly expressed, i.e., anaphor and antecedent do not stand in a structural relationship. However, they are linked by a strong semantic relation that is likely to be structurally explicitly expressed in other texts. We exploit this insight by adopting the following procedure: 1. In other-anaphora, a"
W03-1023,P02-1014,0,0.376737,"Missing"
W03-1023,poesio-etal-2002-acquiring,0,0.220534,"ntic knowledge the algorithm relies on is not sufficient for many cases of other-anaphors (Section 4.2). Many expressions, word senses and lexical relations are missing from WordNet. Whereas it includes Moscow as a hyponym of city, so that the relation between anaphor and antecedent in (1) can be retrieved, it does not include the sense of school as university, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore require extensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that the Web can be used as a huge additional source of"
W03-1023,J01-4004,0,0.319187,"Missing"
W03-1023,W02-1040,0,0.0934807,"Missing"
W03-1023,S01-1035,0,\N,Missing
W04-1217,A00-1031,0,0.00462315,"escribing the immediate content and context of each word, including the word itself, the previous and next words, word prefixes and suffix of up to a length of 6 characters, word shapes, and features describing the named entity tags assigned to the previous words. Word shapes refer to a mapping of each word onto equivalence classes that encodes attributes such as length, capitalization, numerals, greek letters, and so on. For instance, “Varicella-zoster” would become Xxxxx, “mRNA” would become xXXX, and “CPA1” would become XXXd. We also incorporated part-ofspeech tagging, using the TnT tagger(Brants, 2000) retrained on the GENIA corpus gold standard partof-speech tagging. We also used various interaction terms (conjunctions) of these base-level features in various ways. The full set of local features is outlined in Table 1. 2.2 External Resources We made use of a number of external resources, including gazetteers, web-querying, use of the surrounding abstract, and frequency counts from the British National Corpus. System Description Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the 88 Word Features TnT POS Prefix/suffix Abbreviations Word Shape P"
W04-1217,W03-0424,0,0.0107123,"ations of each pattern to the web, using the Google API, and obtained the number of hits. The pattern that returned the highest number of hits determined the feature value (e.g., “web-protein”, or “web-RNA”). If no hits were returned by any pattern, a value “O-web” was assigned. This value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the tagger’s performance). 2.2.4 Abstracts A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al., 1999)). A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. Table 1: Local Features (+ indicates conjunction) 2.2.1 Frequency Many entries in gazetteers are ambiguous words, occasionally used in the sense that the gazetteer seeks to represent, but at l"
W04-1217,P03-1054,1,0.0225709,"words are a common source of error and their classification is more likely to benefit from the use of external resources. We assigned each word in the training and testing data a frequency category corresponding to its frequency in the British National Corpus, a 100 million word balanced corpus, and used conjunctions of this category and certain other features. 2.3 Deeper Syntactic Features While the local features discussed earlier are all fairly surface level, our system also makes use of deeper syntactic features. We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging – we believe that the unlexicalized nature of this parser makes it a particularly suitable statistical parser to use when there is a large domain mismatch between the training material (Wall Street Journal text) and the target domain, but have not yet carefully evaluated this. Then, for each word in the sentence which is inside a noun phrase, the head and governor of the noun phrase are extracted. These features are not very useful when identifying only two classes (such as GENE and OTHER in the BioCreative task), but they were quite useful for this"
W04-1217,W03-0428,1,0.423703,"Missing"
W04-1217,M98-1021,0,0.0342742,"Missing"
W04-1217,E99-1001,0,0.0346474,"web, using the Google API, and obtained the number of hits. The pattern that returned the highest number of hits determined the feature value (e.g., “web-protein”, or “web-RNA”). If no hits were returned by any pattern, a value “O-web” was assigned. This value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the tagger’s performance). 2.2.4 Abstracts A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al., 1999)). A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. Table 1: Local Features (+ indicates conjunction) 2.2.1 Frequency Many entries in gazetteers are ambiguous words, occasionally used in the sense that the gazetteer seeks to represent, but at least as frequently not. So"
W04-1217,M98-1004,0,\N,Missing
W04-1217,M98-1012,0,\N,Missing
W04-1217,M98-1014,0,\N,Missing
W05-0307,P98-1013,0,0.023939,"ve and are linked back to their head. The guidelines contain a decision tree the annotators use to establish priority in case more than one class is appropriate for a given entity. For example, if a mediated/general entity is also old/identity the latter is to be preferred to the former. Similar precedence relations hold among subtypes. To provide more robust and reliable clues in annotating bridging types (e.g. for distinguishing between poss and part), we provided replacement tests and referred to relations encoded in knowledge bases such as WordNet (Fellbaum, 1998) (for part) and FrameNet (Baker et al., 1998) (for situation). 3.2 Validation of the Scheme Three Switchboard dialogues (for a total of 1738 markables) were marked up by two different annotators for assessing the validity of the scheme. We evaluated annotation reliability by using the Kappa statistic (Carletta, 1996). Good quality annotation of discourse phenomena normally yields a kappa ( ) of about .80. We assessed the validity of the scheme on the four-way classification into the three main categories (old, mediated and new) and the nonapplicable category. We also evaluated the annotation including the subtypes. All cases where at lea"
W05-0307,W04-2707,0,0.0598777,"Missing"
W05-0307,carletta-etal-2004-using,1,0.897851,"versations (average six minutes), between speakers of American English, for three million words. The corpus is distributed as stereo speech signals with an orthographic transcription per channel time-stamped at the word level. A third of this is syntactically parsed as part of the Penn Treebank (Marcus et al., 1993) and has dialog act annotation (Shriberg et al., 1998). We used a subset of this. In adherence with current standards, we converted all the existing annotations, and are producing the new discourse annotations in a coherent multi-layered XML-conformant schema, using NXT technology (Carletta et al., 2004). 1 This allows us to search over and integrate information from the many layers of annotation, including the 1 Beside the NXT tools, we also used the TIGER Switchboard filter (Mengel and Lezius, 2000) for the XMLconversion. Using existing markup we automatically selected and filtered NPs to be annotated, excluding locative, directional, and adverbial NPs and disfluencies, and adding possessive pronouns. See (Nissim et al., 2004) for technical details. 45 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45–52, c Ann Arbor, June 2005. 2005 Association for"
W05-0307,J93-2004,0,0.0243998,"Missing"
W05-0307,mengel-lezius-2000-xml,0,0.0399157,"-stamped at the word level. A third of this is syntactically parsed as part of the Penn Treebank (Marcus et al., 1993) and has dialog act annotation (Shriberg et al., 1998). We used a subset of this. In adherence with current standards, we converted all the existing annotations, and are producing the new discourse annotations in a coherent multi-layered XML-conformant schema, using NXT technology (Carletta et al., 2004). 1 This allows us to search over and integrate information from the many layers of annotation, including the 1 Beside the NXT tools, we also used the TIGER Switchboard filter (Mengel and Lezius, 2000) for the XMLconversion. Using existing markup we automatically selected and filtered NPs to be annotated, excluding locative, directional, and adverbial NPs and disfluencies, and adding possessive pronouns. See (Nissim et al., 2004) for technical details. 45 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45–52, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics sound files. NXT tools can be easily customised to accommodate different layers of annotation users want to add, including data sets that have low-level annotations time-stamp"
W05-0307,nissim-etal-2004-annotation,1,0.715633,"we converted all the existing annotations, and are producing the new discourse annotations in a coherent multi-layered XML-conformant schema, using NXT technology (Carletta et al., 2004). 1 This allows us to search over and integrate information from the many layers of annotation, including the 1 Beside the NXT tools, we also used the TIGER Switchboard filter (Mengel and Lezius, 2000) for the XMLconversion. Using existing markup we automatically selected and filtered NPs to be annotated, excluding locative, directional, and adverbial NPs and disfluencies, and adding possessive pronouns. See (Nissim et al., 2004) for technical details. 45 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 45–52, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics sound files. NXT tools can be easily customised to accommodate different layers of annotation users want to add, including data sets that have low-level annotations time-stamped against a set of synchronized signals, multiple, crossing tree structures, and connection to external corpus resources such as gesture ontologies and lexicons (Carletta et al., 2004). 3 Information Status Information Status descr"
W05-0307,C98-1013,0,\N,Missing
W05-0307,J96-2004,0,\N,Missing
W05-0307,W99-0309,0,\N,Missing
W06-1602,C04-1180,1,0.892698,"Missing"
W06-1602,W02-2024,0,0.0664823,"Missing"
W06-1602,A00-1031,0,0.051983,"er”, and then with the possessive construction, yielding the wrong semantic interpretation. To deal with this, we analyse possessives that interact with the superlative as follows: Rome NP ’s ((NP/N)/(N/N)NP (NP/N)/(N/N) oldest N/N 5.1 church N This analysis yields the correct comparison set for superlative that follow a possessive noun phrase, given the following lexical semantics for the genitive: u ;S(λx.(p(x);n(λy. of(y,x) Superlative Detection Baseline system For superlative detection we generated a baseline that solely relies on part-ofspeech information. The data was tagged using TnT (Brants, 2000), using a model trained on the Wall Street Journal. In the WSJ tagset, superlatives can be marked in two different ways, depending on whether the adjective is inflected or modified by most/least. So, “largest”, for instance, is tagged as JJS, whereas “most beautiful” is a sequence of RBS (most) and JJ (beautiful). We also checked that they are followed by a common or proper noun (NN.*), allowing one word to occur in between. To cover more complex cases, we also considered pre-modification by adjectives (JJ), and cardinals (CD). In summary, we matched on sequences found by the following pattern"
W06-1602,J96-2004,0,0.0125046,"Missing"
W06-1602,P04-1014,0,0.0239456,"Missing"
W06-1602,W05-0619,0,0.0209342,"Missing"
W06-1602,P02-1043,0,0.0128512,"a working system. 4.1 1993)). We follow (Bos et al., 2004; Bos, 2005) in automatically building semantic representation on the basis of CCG derivations in a compositional fashion. We briefly summarise the approach here. The semantic representation for a word is determined by its CCG category, POS-tag, and lemma. Consider the following lexical entries: Combinatory Categorial Grammar (CCG) CCG is a lexicalised theory of grammar (Steedman, 2001). We used Clark & Curran’s widecoverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). In CCG-bank, the majority of superlative adjective of cases are analysed as follows: the tallest woman NP/N N/N N the: λp.λq.( tallest: λp.λx.( man: λx. ;p(x);q(x)) ( y y6=x ;p(y))⇒ ;p(x)) taller(x,y) man(x) These lexical entries are combined in a compositional fashion following the CCG derivation, using the λ-calculus as a glue language: N NP most devastating droughts (N/N)/(N/N) N/N N tallest man: λx. N/N man(x) y ⇒ y6=x taller(x,y) man(y) N third largest bank N/N (N/N)(N/N) N/N N N x man(x) y the tallest man: λq.( ⇒ y6=x taller(x,y) man(y) Clark & Curran’s parser outputs besides a CCG de"
W06-1612,carletta-etal-2004-using,1,0.855275,"cation of information status in English. 2 Data For our experiments we annotated a portion of the transcribed Switchboard corpus (Godfrey et al., 1992), consisting of 147 dialogues (Nissim et al., 2004).1 In the following section we provide a brief description of the annotation categories. 2.1 Annotation Our annotation of information status mainly builds on (Prince, 1992), and employs a distinction into old, mediated, and new entities similar to the work of (Strube, 1998; Eckert and Strube, 2001). All noun phrases (NPs) were extracted as markable entities using pre-existing parse information (Carletta et al., 2004). An entity was annotated as new if it has not been previously referred to and is yet unknown to the hearer. The tag mediated was instead used whenever an entity that is newly mentioned in the dialogue can be inferred by the hearer thanks to prior or general context.2 Typical examples of mediated entities are generally known objects (such as “the sun”, or “the Pope” (L¨obner, 1985)), and bridging anaphors (Clark, 1975; Vieira and Poesio, 2000), where an entity is related to a previously introduced one. Whenever an entity was neither new nor mediated, it was considered as old. 2.2 Setup We spli"
W06-1612,J96-2004,0,0.239392,"ed in future work. In addition to the main categories, we used two more annotation classes: a tag non-applicable, used for entities that were wrongly extracted in the automatic selection of markables (e.g. “course” in “of course”), for idiomatic occurrences, and expletive uses of “it”; and a tag not-understood to be applied whenever an annotator did not fully understand the text. Instances annotated with these two tags, as well as all traces, which were left unannotated, were excluded from all our experiments. Inter-annotator agreement was measured using the kappa (K) statistics (Cohen, 1960; Carletta, 1996) on 1,502 instances (three Switchboard dialogues) marked by two annotators who followed specific written guidelines. Given that the task involves a fair amount of subjective judgement, agreement was remarkably high. Over the three dialogues, the annotation yielded K = .845 for the old/med/new classification (K = .788 when including the finer-grained subtype distinction). Specifically, “old” proved to be the easiest to distinguish, with K = .902; for “med” and “new” agreement was measured at K = .800 and K = .794, respectively. A value of K > .76 is usually considered good agreement. Further de"
W06-1612,J95-2003,0,0.118356,"this information in the current experiments. Moreover, we did not want our model to rely too heavily on a feature that is not easy to obtain automatically. Figure 3: Top of C.5, full training set, three classes Figure 3 shows the top of C4.5 (trained on the full training set for the three-way classification), which looks remarkably different from the rules in Figure 1. We had based our decision of emphasising the importance of the NP type on the linguistic evidence that different syntactic realisations reflect different degrees of availability of discourse entities (Giv´on, 1983; Ariel, 1990; Grosz et al., 1995). In the learnt model, however, knowledge about NP type is only used as subordinate to other features. This is indeed mirrored in the fact that removing NP type information from the feature set causes accuracy to drop, but a classifier building on NP type alone performs poorly (see Table 8).3 Interestingly, though, more informative knowledge about syntactic form seems to be derived from the determiner type, which helps distinguish degrees of oldness among common nouns. 3 The NPtype-only classifier assigns old to pronouns and med to all other types; it never assigns new. 100 7 Conclusions and F"
W06-1612,P98-2204,0,0.126668,"for English. Exploiting an existing annotated corpus, in this paper we report experiments on learning a model for the automatic identification of information status in English. 2 Data For our experiments we annotated a portion of the transcribed Switchboard corpus (Godfrey et al., 1992), consisting of 147 dialogues (Nissim et al., 2004).1 In the following section we provide a brief description of the annotation categories. 2.1 Annotation Our annotation of information status mainly builds on (Prince, 1992), and employs a distinction into old, mediated, and new entities similar to the work of (Strube, 1998; Eckert and Strube, 2001). All noun phrases (NPs) were extracted as markable entities using pre-existing parse information (Carletta et al., 2004). An entity was annotated as new if it has not been previously referred to and is yet unknown to the hearer. The tag mediated was instead used whenever an entity that is newly mentioned in the dialogue can be inferred by the hearer thanks to prior or general context.2 Typical examples of mediated entities are generally known objects (such as “the sun”, or “the Pope” (L¨obner, 1985)), and bridging anaphors (Clark, 1975; Vieira and Poesio, 2000), wher"
W06-1612,P96-1038,0,0.102089,"Missing"
W06-1612,P03-2012,0,0.094577,"old/med/new classification (K = .788 when including the finer-grained subtype distinction). Specifically, “old” proved to be the easiest to distinguish, with K = .902; for “med” and “new” agreement was measured at K = .800 and K = .794, respectively. A value of K > .76 is usually considered good agreement. Further details on the annotation process and corpus description are provided in (Nissim et al., 2004) anaphoric definite noun phrases, thus severely affecting performance. There has been recent interest in determining anaphoricity before performing anaphora resolution (Ng and Cardie, 2002; Uryupina, 2003), but results have not been entirely satisfactory. Given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination. Postolache et al. (2005) have recently shown that learning information structure with high accuracy is feasible for Czech. However, there are yet no studies that explore such a task for English. Exploiting an existing annotated corpus, in this paper we report experiments on learning a model for the automatic identification of information status in English. 2 Data For our experiments"
W06-1612,J00-4003,0,0.340138,"r to the work of (Strube, 1998; Eckert and Strube, 2001). All noun phrases (NPs) were extracted as markable entities using pre-existing parse information (Carletta et al., 2004). An entity was annotated as new if it has not been previously referred to and is yet unknown to the hearer. The tag mediated was instead used whenever an entity that is newly mentioned in the dialogue can be inferred by the hearer thanks to prior or general context.2 Typical examples of mediated entities are generally known objects (such as “the sun”, or “the Pope” (L¨obner, 1985)), and bridging anaphors (Clark, 1975; Vieira and Poesio, 2000), where an entity is related to a previously introduced one. Whenever an entity was neither new nor mediated, it was considered as old. 2.2 Setup We split the 147 dialogues into a training, a development and an evaluation set. The training set contains 40,865 NPs distributed over 94 dialogues, the development set consists of 23 dialogues for a total of 10,565 NPs, and the evaluation set comprises 30 dialogues with 12,624 NPs. Instances were randomised, so that occurrences of NPs from the same dialogue were possibly split across the different sets. 1 Switchboard is a collection of spontaneous p"
W06-1612,J93-2004,0,0.0299547,"Missing"
W06-1612,C02-1139,0,0.0683162,"ded K = .845 for the old/med/new classification (K = .788 when including the finer-grained subtype distinction). Specifically, “old” proved to be the easiest to distinguish, with K = .902; for “med” and “new” agreement was measured at K = .800 and K = .794, respectively. A value of K > .76 is usually considered good agreement. Further details on the annotation process and corpus description are provided in (Nissim et al., 2004) anaphoric definite noun phrases, thus severely affecting performance. There has been recent interest in determining anaphoricity before performing anaphora resolution (Ng and Cardie, 2002; Uryupina, 2003), but results have not been entirely satisfactory. Given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination. Postolache et al. (2005) have recently shown that learning information structure with high accuracy is feasible for Czech. However, there are yet no studies that explore such a task for English. Exploiting an existing annotated corpus, in this paper we report experiments on learning a model for the automatic identification of information status in English. 2 Data Fo"
W06-1612,nissim-etal-2004-annotation,1,0.861132,"rs who followed specific written guidelines. Given that the task involves a fair amount of subjective judgement, agreement was remarkably high. Over the three dialogues, the annotation yielded K = .845 for the old/med/new classification (K = .788 when including the finer-grained subtype distinction). Specifically, “old” proved to be the easiest to distinguish, with K = .902; for “med” and “new” agreement was measured at K = .800 and K = .794, respectively. A value of K > .76 is usually considered good agreement. Further details on the annotation process and corpus description are provided in (Nissim et al., 2004) anaphoric definite noun phrases, thus severely affecting performance. There has been recent interest in determining anaphoricity before performing anaphora resolution (Ng and Cardie, 2002; Uryupina, 2003), but results have not been entirely satisfactory. Given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination. Postolache et al. (2005) have recently shown that learning information structure with high accuracy is feasible for Czech. However, there are yet no studies that explore such a tas"
W06-1612,H05-1002,0,0.0500934,".794, respectively. A value of K > .76 is usually considered good agreement. Further details on the annotation process and corpus description are provided in (Nissim et al., 2004) anaphoric definite noun phrases, thus severely affecting performance. There has been recent interest in determining anaphoricity before performing anaphora resolution (Ng and Cardie, 2002; Uryupina, 2003), but results have not been entirely satisfactory. Given that old entities are more likely to be referred to by anaphors, for instance, identification of information status could improve anaphoricity determination. Postolache et al. (2005) have recently shown that learning information structure with high accuracy is feasible for Czech. However, there are yet no studies that explore such a task for English. Exploiting an existing annotated corpus, in this paper we report experiments on learning a model for the automatic identification of information status in English. 2 Data For our experiments we annotated a portion of the transcribed Switchboard corpus (Godfrey et al., 1992), consisting of 147 dialogues (Nissim et al., 2004).1 In the following section we provide a brief description of the annotation categories. 2.1 Annotation"
W06-1612,P98-1013,0,\N,Missing
W06-1612,C98-1013,0,\N,Missing
W06-1612,C98-2199,0,\N,Missing
W09-3707,W06-1670,0,0.0831106,"Missing"
W09-3707,W05-1004,0,0.02445,"ing information included in qualia structures (Pustejovsky, 1995) for deriving the compound’s interpretation. The use of qualia structures for this task is appropriate and semantically sound but absolutely not straightforward to implement, since there does not exist an electronic repository of qualias, so that the structures 47 would need to be constructed by hand, thereby involving a large amount of manual work. Recent work has shown that the automatic acquisition of qualias can be performed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico-syntactic patterns can be used successfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness prob"
W09-3707,P07-1072,0,0.343899,"is well known that whereas English composes CNs of the type N+N, Romance languages must glue the two nouns by means of a preposition, thus yielding CNs of the form N+P+N, thereby partially making explicit the underlying semantic relation (Busa and Johnston, 1996). So, in Ex. 4, the “purpose” relation between dessert and fork is (partially) made explicit by the preposition “da”. In contrast, the “property” relation binding plastic and fork (a fork made of plastic) is expressed using “di” (Ex. 5). (4) forchetta da dessert (en: dessert fork) (5) forchetta di plastica (en: plastic fork) Recently, Girju (2007) has exploited this observation including cross-language information in a system for the automatic interpretation of NN compounds in English. However, whereas it is true that the overt preposition restricts the set of possible relations, it is also true that prepositions are still semantically ambiguous, since there is no one-to-one correspondence between prepositions and relations. So, “di”, used in a “property” relation above, can also express a “part-whole” (Ex. 6), a “theme” (Ex. 7), and several other relations. (6) dorso della mano (the back of the hand) (7) suonatore di chitarra (guitar"
W09-3707,W96-0309,0,0.42362,"or the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. 1 In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of seman"
W09-3707,P95-1007,0,0.157302,"c relations for the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. 1 In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a"
W09-3707,H05-1112,0,0.0549636,"Missing"
W09-3707,P08-1052,0,0.0484194,"tructed by hand, thereby involving a large amount of manual work. Recent work has shown that the automatic acquisition of qualias can be performed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico-syntactic patterns can be used successfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous ap"
W09-3707,P06-1015,0,0.029371,"ould directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the annotation phase, we have not included corresponding features yet in the statistical model we use, but we plan to do so in future extensions. 3 Annotation Framework and Data For developing an annotation framework, we built on Italian grammars, existing classifications (see Section 2), and a preliminary study of corpus d"
W09-3707,picca-etal-2008-supersense,0,0.0488669,"Missing"
W09-3707,W01-0511,0,0.0408823,"ian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. 1 In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of semantic relations. Nastase and"
W09-3707,P06-1040,0,0.057895,"uccessfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the annotation phase, we have not included corresponding features yet in the statistical model we use, but we plan to do so in future extensions. 3 Annotation Framework and Data For developing an annotation framework, we built on Italian grammars, existing classifica"
W09-3707,J02-3004,0,\N,Missing
W09-3707,S07-1003,0,\N,Missing
W13-0501,J12-2006,0,0.298551,"uistic annotation of modality: a data-driven hierarchical model Malvina Nissim University of Bologna Paola Pietrandrea Lattice-CNRS, France Andrea Sans`o University of Insubria Abstract Since the first step towards developing systems which deal with modality automatically is the creation of appropriate, annotated resources, the last few years have witnessed the development of annotation schemes and annotated corpora for different aspects of modality in different languages (McShane et al. (2004); Wiebe et al. (2005); Szarvas et al. (2008); Sauri and Pustejovsky (2009); Hendrickx et al. (2012); Baker et al. (2012)). While important contributions, these remain mainly separate efforts. And while there have been efforts towards finding a common avenue for modality annotation, such as the CoNLL-2010 Shared Task, ACL thematic workshops and a special issue of Computational Linguistics (Morante and Sporleder (2012)), the computational linguistics community is still far from having developed working, shared standards for converting modalityrelated issues into annotation categories. Linguistic theory, and especially linguistic typology, has already gone a long way in the study of modality across languages. Howe"
W13-0501,J12-2002,0,0.0648665,"ttitude markers, such as hypothetical subordinating conjunctions (3a), or alternative constructions (3b): (3) Sometimes I think that surely, eventually, there will come a time in my veterinary career that I don’t get quite so ridiculous when confronted with a puppy. a. if he misses me, I am happy b. either he misses me or he doesn’t love me The association of a given attitude marker within a given factuality value is not entirely predictable. Sometimes, even the well-established identification of a certainty attitude with a factual value, which is posited as an axiom, for example in FactBank (Sauri and Pustejovsky, 2012), has to be reconsidered. Let us examine Example 4, where the non factual predicate “I think” and the certainty adverb “surely” impose respectively a non factual and a certainty value 9 (5) Someone’s knocking at the door. That will be John. 5 Implementation Similarly, past forms may be used as non-factual (specifically, counterfactual) markers (Fleischman (1995)) not only when they are under the scope of conditional markers (6a) but also when they are used in independent clauses (6b): The annotation model we are currently developing for both factuality and speaker’s attitude is modular, langua"
W13-0501,E12-2021,0,0.0572224,"Missing"
W13-0501,W08-0606,0,0.129124,"Missing"
W13-0501,hendrickx-etal-2012-modality,0,0.219353,"ry in extension from a single word to an entire text Masini and Pietrandrea (2010). One major problem is that in a few projects the annotators are not asked to select the annotation units but only to assign modality values to preselected markables, thus turning annotation into a classification task. In their annotation guidelines, Baker et al. (2010) assume that the units to be marked up are already highlighted and do not exceed the clause limit (i.e. the maximum extension is a phrase) and revolve around a verb, but it isn’t clearly specified how such units are selected, nor why. Differently, Hendrickx et al. (2012) let the annotators choose the unit and its extension, allowing also for cross-sentential markables to be selected. However, they pre-select data to be annotated by matching a finite set of modality trigging verbs, thereby also imposing some degree of constraint. While pre-selecting annotation units maximises homogeinity and reduces disagreement among annotators, it is not clear exactly which units are to be marked up and whether it is at all an appropriate procedure in all cases. Building on insights coming from linguistic typology, we will take a stand on these issues and claim that a cross-"
W13-0501,2005.mtsummit-papers.11,0,0.00454366,"idering existing collaborative platforms to perform distributed annotation over the web, so as to optimise the contribution of native speakers. Content-wise, we plan to enrich our model in at least two ways: (1) by providing a coherent model for the annotation of the strength of modality values (certain, probable, impossible; necessary, prohibited, impossible, etc.); (2) by specifying for each modal attitude, the source of the attitude. Interannotator agreement will also be calculated to assess the validity of the scheme. Concerning data, we are currently using the Europarl’s parallel corpus (Koehn (2005)), but we also aim at including other comparable corpora to maximise linguistic diversity (languages outside Europe will be included) and register variation (mainly through the inclusion of spoken corpora). the nature of the markables, we can provide an annotation for each relevant element of our corpus, ranging from the entire text, to an embedded single word. Each markable is linked to its own trigger, regardless of the level of embedding of the trigger itself. Technically, this is done via layers of standoff annotation for factuality and attitude, which point to markables and triggers via t"
W13-0501,J12-2001,0,0.302523,"appropriate, annotated resources, the last few years have witnessed the development of annotation schemes and annotated corpora for different aspects of modality in different languages (McShane et al. (2004); Wiebe et al. (2005); Szarvas et al. (2008); Sauri and Pustejovsky (2009); Hendrickx et al. (2012); Baker et al. (2012)). While important contributions, these remain mainly separate efforts. And while there have been efforts towards finding a common avenue for modality annotation, such as the CoNLL-2010 Shared Task, ACL thematic workshops and a special issue of Computational Linguistics (Morante and Sporleder (2012)), the computational linguistics community is still far from having developed working, shared standards for converting modalityrelated issues into annotation categories. Linguistic theory, and especially linguistic typology, has already gone a long way in the study of modality across languages. However, this very aspect of cross-linguality has been overlooked in devising annotation schemes. Instead, we believe that working in a multilingual environment could ease the annotation, and at the same time make it more semantically meaningful, by keeping the layer of functional categories distinct fr"
W13-1015,baroni-etal-2004-introducing,0,0.0341169,"Missing"
W13-1015,calzolari-etal-2002-towards,0,0.0528903,"Andrea Zaninello Zanichelli Editore, Humanities Department andrea.zaninello@gmail.com Introduction and Background One of the crucial issues in the analysis and processing of MWEs is their internal variability. Indeed, the feature that mostly characterises MWEs is their fixedness at some level of linguistic analysis, be it morphology, syntax, or semantics. The morphological aspect is not trivial in languages which exhibit a rich morphology, such as Romance languages. The issue is relevant in at least three aspects of MWE representation and processing: lexicons, identification, and extraction (Calzolari et al., 2002). At the lexicon level, MWEs are usual stored as one form only, the so-called quotation form (or citation form). However, some variations of the quotation form might also be valid instances of MWEs (Bond et al., 2005) — some but not all, as some of them might actually be plain compositional phrases. This becomes relevant for automatic identification and extraction. If a lexicon stores the quotation form only, identification on a corpus done via matching lexicon strings as such would miss valid variations of a given MWE. Identification could be done exploiting lemmas rather than quotation forms"
W13-1015,ramisch-etal-2010-mwetoolkit,0,0.0129079,"a selection strategy for all of the matched instances. One could just use frequency directly from the corpus where the identification is done, but this might not always be possible due to corpus size. This is why using an external repository of patterns evaluated against a large reference corpus for a given language might be useful. In extraction tasks, patterns can be used as filters, either as a post-processing phase after matching lemmas for given POS sequences, or directly extracting only allowed configurations which could be specified for instance in extraction tools such as mwetoolkit (Ramisch et al., 2010). In previous work we have shown that patterns can be derived comparing found instances against their lemmatised form, making this a realistic setting even in extraction where quotation forms are not known (Nissim and Zaninello, 2013). 3 Ranking For ranking variation patterns we take into account the following figures: • the total number of different variation patterns per POS sequence As a global ranking score (GRSvp ), the resulting average share is combined with the spread, namely the ratio of instances over variations (219/13 for f lex f ix f ix), a pattern-internal measure indicating the"
W13-1015,weller-heid-2010-extraction,0,0.0311829,"ion form of a given MWE would help in increasing recall while keeping precision high. However, specifying such variations for each MWE would be too costly and wouldn’t help in extraction, as no specifications could be done a priori on yet unknown MWEs. Optimally, one would need to find more general variation patterns that could be applied to classes of MWEs. Indeed, the main idea behind this work is that MWEs can be handled through more general patterns. This is also claimed, for instance, by Masini (2007) whose analysis on Italian MWEs takes a constructionist perspective (Goldberg, 2003), by Weller and Heid (2010), who treat verbal expressions in German, and also by Gr´egoire (2010), who bases his work on the Equivalence Class Method (ECM, (Odijk, 2004)) assuming that MWEs may be clustered according to their syntactic pattern and treated homogeneously. We suggest that variation patterns can be found and defined over POS sequences. Working on Italian, in this paper we report the results of ongoing research and show how such patterns can be derived, we then propose a way to encode them in a repository, which can be combined with existing lexicons of MWEs. For the moment, we restrict our study to contiguo"
W13-1015,zaninello-nissim-2010-creation,1,0.873644,"Missing"
W13-1614,W11-0705,0,0.0128613,"Missing"
W13-1614,baccianella-etal-2010-sentiwordnet,0,0.0142533,"5; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006; Taboada et al., 2011, e.g.). To our knowledge, there isn’t such a resource already available for Italian. Besides hand-crafting, there have been proposals for creating resources for new languages in a semi-automatic fashion, using manually annotated sets of seeds (Pitel and Grefenstette, 2008), or exploiting twitter emoticons directly (Pak and Paroubek, 2011). Rather than creating a new polarity lexicon from scratch, we exploit three existing resources, namely MultiWordNet (Pianta et al., 2002), SentiWordNet (Esuli and Sebastiani, 2006; Baccianella et al., 2010), and WordNet itself (Fellbaum, 1998) to obtain an annotated lexicon of senses for Italian. Basically, we port the SentiWordNet annotation to the Italian portion of MultiWordNet, and we do so in a completely automatic fashion. Our starting point is SentiWordNet, a version of WordNet where the independent values positive, negative, and objective are associated to 117,660 synsets, each value in the zero-one interval. MultiWordNet is a resource which aligns Italian and English synsets and can thus be used to transfer polarity information associated to English synsets in SentiWordNet to Italian sy"
W13-1614,P00-1064,0,0.0607421,"ositive, negative, and objective are associated to 117,660 synsets, each value in the zero-one interval. MultiWordNet is a resource which aligns Italian and English synsets and can thus be used to transfer polarity information associated to English synsets in SentiWordNet to Italian synsets. One obstacle is that while SentiWordNet refers to WordNet 3.0, MultiWordNet’s alignment holds for WordNet 1.6, and synset reference indexes are not plainly carried over from one version to the next. We filled this gap using an automatically produced mapping between synsets of Wordnet versions 1.6 and 3.0 (Daud et al., 2000), making it possible to obtain SentiWordNet annotation for the Italian synsets of MultiWordNet. The coverage of our resource is however rather low compared to the English version, and this is due to the alignment procedure which must exploit an earlier version of the resource. The number of synsets is less than one third of that of SentiWordNet. average across senses, as we also do — although we also add the polypathy-aware strategy. We cannot use the first-sense strategy because through the alignment procedure senses are not ranked according to frequency anymore. 3.2 3.3 Polarity assignment G"
W13-1614,C10-2028,0,0.0196983,"omatic system as it must discern what is said about the topic itself and what is said more generally or about another entity mentioned in the text. Indeed, this contribution can be seen as a first step towards polarity detection in Italian tweets. The information we obtain from SentiWordNet and the ways we combine it could obviously be used as feature in a learning setting. Other sources of information, to be used in combination with our polarity scores or integrated in a statistical model, are the socalled noisy labels, namely strings (such as emoticons or specific hashtags (Go et al., 2009; Davidov et al., 2010)) that can be taken as positive or negative polarity indicators as such. Speriosu et al. (2011) have shown that training a maximum entropy classier using noisy labels as class predictors in the training set yields an improvement of about three percentage points over a lexicon-based prediction. Another important issue to deal with is figurative language. During manual annotation we have encountered many cases of irony or sarcasm, which is a phenomenon that must be obviously tackled. There have been attempts at identifying it automatically in the context of tweets (Gonz´alez-Ib´an˜ ez et al., 20"
W13-1614,esuli-sebastiani-2006-sentiwordnet,0,0.0230035,"tionally, existing tools for the syntactic analysis of Italian, such as the DeSR parser (Attardi et al., 2009), might not be robust enough for processing such texts. Exploiting information coming from a polarity 4 5 http://ilk.uvt.nl/ucto/ www.cs.york.ac.uk/semeval-2013/task2/. 102 Polarity lexicon for Italian Most polarity detection systems make use, in some way, of an affection lexicon, i.e. a language-specific resource which assigns a negative or positive prior polarity to terms. Such resources have been built by hand or derived automatically (Wilson et al., 2005; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006; Taboada et al., 2011, e.g.). To our knowledge, there isn’t such a resource already available for Italian. Besides hand-crafting, there have been proposals for creating resources for new languages in a semi-automatic fashion, using manually annotated sets of seeds (Pitel and Grefenstette, 2008), or exploiting twitter emoticons directly (Pak and Paroubek, 2011). Rather than creating a new polarity lexicon from scratch, we exploit three existing resources, namely MultiWordNet (Pianta et al., 2002), SentiWordNet (Esuli and Sebastiani, 2006; Baccianella et al., 2010), and WordNet itself (Fellbaum"
W13-1614,P07-1054,0,0.0102583,"st sense of a term (the most frequent in WordNet) or taking the 103 Gold standard For evaluating the system performance we created two gold standard sets, both annotated by three independent native-speakers, who were given very simple and basic instructions and performed the annotation via a web-based interface. The value to be assigned to each tweet is one out of positive, neutral, or negative. As mentioned, the neutral value includes both objective statements as well as subjective statements where the twitter’s position is neutral or equally positive and negative at the same time (see also (Esuli and Sebastiani, 2007)). All data selected for annotation comes from TWITA. The first dataset consists of 1,000 randomly selected tweets. The second dataset is topicoriented, i.e. we randomly extracted 1,000 tweets from all those containing a given topic. Topicoriented, or target-dependent (Jiang et al., 2011), classification involves detecting opinions about a specific target rather than detecting the more general opinion expressed in a given tweet. We identify a topic through a given hashtag, and in this experiment we chose the tag “Grillo”, the leader of an Italian political movement. While in the first set the"
W13-1614,P11-2102,0,0.00943364,"et al., 2010)) that can be taken as positive or negative polarity indicators as such. Speriosu et al. (2011) have shown that training a maximum entropy classier using noisy labels as class predictors in the training set yields an improvement of about three percentage points over a lexicon-based prediction. Another important issue to deal with is figurative language. During manual annotation we have encountered many cases of irony or sarcasm, which is a phenomenon that must be obviously tackled. There have been attempts at identifying it automatically in the context of tweets (Gonz´alez-Ib´an˜ ez et al., 2011), and we plan to explore this issue in future work. Finally, the co-presence of meta and linguistic information allows for a wide range of linguistic queries and statistical analyses on the whole of the corpus, also independently of sentiment information, of course. For example, correlations between parts-of-speech and polarity have been found (Pak and Paroubek, 2010), and one could expect also correlations with sentiment and time of the day, or month of the year, and so on. Acknowledgments We would like to thank Manuela, Marcella e Silvia for their help with annotation, and the reviewers for"
W13-1614,P11-1016,0,0.00597195,"based interface. The value to be assigned to each tweet is one out of positive, neutral, or negative. As mentioned, the neutral value includes both objective statements as well as subjective statements where the twitter’s position is neutral or equally positive and negative at the same time (see also (Esuli and Sebastiani, 2007)). All data selected for annotation comes from TWITA. The first dataset consists of 1,000 randomly selected tweets. The second dataset is topicoriented, i.e. we randomly extracted 1,000 tweets from all those containing a given topic. Topicoriented, or target-dependent (Jiang et al., 2011), classification involves detecting opinions about a specific target rather than detecting the more general opinion expressed in a given tweet. We identify a topic through a given hashtag, and in this experiment we chose the tag “Grillo”, the leader of an Italian political movement. While in the first set the annotators were asked to assign a polarity value to the message of the tweet as a whole, in the second set the value was to be assigned to the author’s opinion concerning the hashtag, in this case Beppe Grillo. This is a relevant distinction, since it can happen that the tweet is, say, ve"
W13-1614,P12-3005,0,0.0165724,"Missing"
W13-1614,pak-paroubek-2010-twitter,0,0.0233556,"Missing"
W13-1614,pitel-grefenstette-2008-semi,0,0.0156227,"exicon for Italian Most polarity detection systems make use, in some way, of an affection lexicon, i.e. a language-specific resource which assigns a negative or positive prior polarity to terms. Such resources have been built by hand or derived automatically (Wilson et al., 2005; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006; Taboada et al., 2011, e.g.). To our knowledge, there isn’t such a resource already available for Italian. Besides hand-crafting, there have been proposals for creating resources for new languages in a semi-automatic fashion, using manually annotated sets of seeds (Pitel and Grefenstette, 2008), or exploiting twitter emoticons directly (Pak and Paroubek, 2011). Rather than creating a new polarity lexicon from scratch, we exploit three existing resources, namely MultiWordNet (Pianta et al., 2002), SentiWordNet (Esuli and Sebastiani, 2006; Baccianella et al., 2010), and WordNet itself (Fellbaum, 1998) to obtain an annotated lexicon of senses for Italian. Basically, we port the SentiWordNet annotation to the Italian portion of MultiWordNet, and we do so in a completely automatic fashion. Our starting point is SentiWordNet, a version of WordNet where the independent values positive, neg"
W13-1614,W11-2207,0,0.00856422,"erally or about another entity mentioned in the text. Indeed, this contribution can be seen as a first step towards polarity detection in Italian tweets. The information we obtain from SentiWordNet and the ways we combine it could obviously be used as feature in a learning setting. Other sources of information, to be used in combination with our polarity scores or integrated in a statistical model, are the socalled noisy labels, namely strings (such as emoticons or specific hashtags (Go et al., 2009; Davidov et al., 2010)) that can be taken as positive or negative polarity indicators as such. Speriosu et al. (2011) have shown that training a maximum entropy classier using noisy labels as class predictors in the training set yields an improvement of about three percentage points over a lexicon-based prediction. Another important issue to deal with is figurative language. During manual annotation we have encountered many cases of irony or sarcasm, which is a phenomenon that must be obviously tackled. There have been attempts at identifying it automatically in the context of tweets (Gonz´alez-Ib´an˜ ez et al., 2011), and we plan to explore this issue in future work. Finally, the co-presence of meta and lin"
W13-1614,W08-1207,0,0.01459,"0.296 (vr) 0.698 (r) Table 3: Best results on the topic-specific set. In brackets POS combination: (n)oun, (v)erb, (a)djective, adve(r)b. Inter-annotator agreement was measured via Fleiss’ Kappa across three annotators. On the generic set, we found an agreement of Kappa = 0.321, while on the topic-specific set we found Kappa = 0.397. This confirms our expectation that annotating topicspecific tweets is actually an easier task. We might also consider using more sophisticated and finegrained sentiment annotation schemes which have proved to be highly reliable in the annotation of English data (Su and Markert, 2008a). 3.4 Table 2: Best results on the generic set. In brackets POS combination: (n)oun, (v)erb, (a)djective, adve(r)b. Evaluation We ran our system on both datasets described in Section 3.3, using all possible variations of two parameters, namely all combinations of part-of-speech tags and the application of the threshold scheme, as discussed in Section 3.2. We measure overall accuracy as well as precision, recall, and f-score per polarity value. In Tables 2 and 3, we report best scores, and indicate in brackets the associated POS combination. For instance, in Table 2, we can read that the reca"
W13-1614,C08-1104,0,0.0112696,"0.296 (vr) 0.698 (r) Table 3: Best results on the topic-specific set. In brackets POS combination: (n)oun, (v)erb, (a)djective, adve(r)b. Inter-annotator agreement was measured via Fleiss’ Kappa across three annotators. On the generic set, we found an agreement of Kappa = 0.321, while on the topic-specific set we found Kappa = 0.397. This confirms our expectation that annotating topicspecific tweets is actually an easier task. We might also consider using more sophisticated and finegrained sentiment annotation schemes which have proved to be highly reliable in the annotation of English data (Su and Markert, 2008a). 3.4 Table 2: Best results on the generic set. In brackets POS combination: (n)oun, (v)erb, (a)djective, adve(r)b. Evaluation We ran our system on both datasets described in Section 3.3, using all possible variations of two parameters, namely all combinations of part-of-speech tags and the application of the threshold scheme, as discussed in Section 3.2. We measure overall accuracy as well as precision, recall, and f-score per polarity value. In Tables 2 and 3, we report best scores, and indicate in brackets the associated POS combination. For instance, in Table 2, we can read that the reca"
W13-1614,J11-2001,0,0.331196,"the syntactic analysis of Italian, such as the DeSR parser (Attardi et al., 2009), might not be robust enough for processing such texts. Exploiting information coming from a polarity 4 5 http://ilk.uvt.nl/ucto/ www.cs.york.ac.uk/semeval-2013/task2/. 102 Polarity lexicon for Italian Most polarity detection systems make use, in some way, of an affection lexicon, i.e. a language-specific resource which assigns a negative or positive prior polarity to terms. Such resources have been built by hand or derived automatically (Wilson et al., 2005; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006; Taboada et al., 2011, e.g.). To our knowledge, there isn’t such a resource already available for Italian. Besides hand-crafting, there have been proposals for creating resources for new languages in a semi-automatic fashion, using manually annotated sets of seeds (Pitel and Grefenstette, 2008), or exploiting twitter emoticons directly (Pak and Paroubek, 2011). Rather than creating a new polarity lexicon from scratch, we exploit three existing resources, namely MultiWordNet (Pianta et al., 2002), SentiWordNet (Esuli and Sebastiani, 2006; Baccianella et al., 2010), and WordNet itself (Fellbaum, 1998) to obtain an a"
W13-1614,W12-0607,0,0.0981562,"Missing"
W13-1614,P06-1134,0,0.00991642,"actically ill-formed. Additionally, existing tools for the syntactic analysis of Italian, such as the DeSR parser (Attardi et al., 2009), might not be robust enough for processing such texts. Exploiting information coming from a polarity 4 5 http://ilk.uvt.nl/ucto/ www.cs.york.ac.uk/semeval-2013/task2/. 102 Polarity lexicon for Italian Most polarity detection systems make use, in some way, of an affection lexicon, i.e. a language-specific resource which assigns a negative or positive prior polarity to terms. Such resources have been built by hand or derived automatically (Wilson et al., 2005; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006; Taboada et al., 2011, e.g.). To our knowledge, there isn’t such a resource already available for Italian. Besides hand-crafting, there have been proposals for creating resources for new languages in a semi-automatic fashion, using manually annotated sets of seeds (Pitel and Grefenstette, 2008), or exploiting twitter emoticons directly (Pak and Paroubek, 2011). Rather than creating a new polarity lexicon from scratch, we exploit three existing resources, namely MultiWordNet (Pianta et al., 2002), SentiWordNet (Esuli and Sebastiani, 2006; Baccianella et al., 2010),"
W13-1614,H05-1044,0,0.026242,"sions, and often syntactically ill-formed. Additionally, existing tools for the syntactic analysis of Italian, such as the DeSR parser (Attardi et al., 2009), might not be robust enough for processing such texts. Exploiting information coming from a polarity 4 5 http://ilk.uvt.nl/ucto/ www.cs.york.ac.uk/semeval-2013/task2/. 102 Polarity lexicon for Italian Most polarity detection systems make use, in some way, of an affection lexicon, i.e. a language-specific resource which assigns a negative or positive prior polarity to terms. Such resources have been built by hand or derived automatically (Wilson et al., 2005; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006; Taboada et al., 2011, e.g.). To our knowledge, there isn’t such a resource already available for Italian. Besides hand-crafting, there have been proposals for creating resources for new languages in a semi-automatic fashion, using manually annotated sets of seeds (Pitel and Grefenstette, 2008), or exploiting twitter emoticons directly (Pak and Paroubek, 2011). Rather than creating a new polarity lexicon from scratch, we exploit three existing resources, namely MultiWordNet (Pianta et al., 2002), SentiWordNet (Esuli and Sebastiani, 2006; B"
W15-1832,E12-2019,1,0.932691,"t it can be carried out in (i) a completely data-driven fashion and (ii) using judgments by multiple speakers without linguistic training, thus making it extremely inexpensive and light, yet useful. To comply with (i), we make sure that prepositions are not derived from a fixed precompiled list, but rather acquired automatically, case by case, exploiting Google’s n-grams to generate candidates. The compounds themselves are Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 251 taken from an existing semantically annotated corpus, the Groningen Meaning Bank (Basile et al., 2012). Regarding (ii), we exploit crowd-sourcing and develop a game-with-a-purpose setting to collect data. The acquired data can then be analysed to investigate more closely the use of prepositions for interpreting noun noun compounds and the extent to which different people agree. Moreover, the data can be used to collect descriptive statistics on preposition use in this context that might give new insight into this approach. 2 Method In this section we describe how we selected nounnoun compounds from a corpus (Step 1), generated potential prepositional relations for each compound (Step 2), and t"
W15-1832,dima-etal-2014-tell,0,0.0275558,"eloped an inventory comprising eight different prepositions: of, for, with, in, on, at, about, and from. The third set of attempts views compound interpretation as a paraphrasing task (Nakov, 2007). This would yield interpretations such as “a crime committed during a war” for our earlier example war crime. None of the three approaches show clear advantages. On the one side of the spectrum, the fixedvocabulary-approach faces the problem of being too strict. On the other end of it, paraphrasing is hard to control. Attempts at combining more than one approach for English (Girju, 2009) or German (Dima et al., 2014) still rely heavily on pre-constructed sets of relations/prepositions, the latter advocating a hybrid approach combining a semantic-relation and preposition-based method. Given that the preposition-approach lies somewhere between these other two approaches, and can be taken in such a way that is entirely data driven, this is the approach that we will consider and use in this paper. While we are aware of its expressive limitations (prepositions might not be sufficient, and they might preserve some ambiguity of the compound), we still think it is interesting to test to what extent it can be carr"
W15-1832,J09-2003,0,0.0245038,"pounds (Levi, 1978), developed an inventory comprising eight different prepositions: of, for, with, in, on, at, about, and from. The third set of attempts views compound interpretation as a paraphrasing task (Nakov, 2007). This would yield interpretations such as “a crime committed during a war” for our earlier example war crime. None of the three approaches show clear advantages. On the one side of the spectrum, the fixedvocabulary-approach faces the problem of being too strict. On the other end of it, paraphrasing is hard to control. Attempts at combining more than one approach for English (Girju, 2009) or German (Dima et al., 2014) still rely heavily on pre-constructed sets of relations/prepositions, the latter advocating a hybrid approach combining a semantic-relation and preposition-based method. Given that the preposition-approach lies somewhere between these other two approaches, and can be taken in such a way that is entirely data driven, this is the approach that we will consider and use in this paper. While we are aware of its expressive limitations (prepositions might not be sufficient, and they might preserve some ambiguity of the compound), we still think it is interesting to test"
W15-1832,P95-1007,0,0.0426598,"are mainly three different approaches that deal with this problem. The first family of approaches take a (usually small) fixed inventory of relations and use it to describe compounds based on well-established ontologies. The second line Malvina Nissim Center for Language and Cognition Oude Kijk in ’t Jatstraat 26 University of Groningen m.nissim@rug.nl of research takes a set of English prepositions to describe compounds (in a way similar as we did above). This makes sense, as prepositions naturally describe a relation between two entities. The seminal work following this tradition is Lauer (Lauer, 1995), who, inspired by Levi’s work on fixing a set of possible predicates for interpreting noun-noun compounds (Levi, 1978), developed an inventory comprising eight different prepositions: of, for, with, in, on, at, about, and from. The third set of attempts views compound interpretation as a paraphrasing task (Nakov, 2007). This would yield interpretations such as “a crime committed during a war” for our earlier example war crime. None of the three approaches show clear advantages. On the one side of the spectrum, the fixedvocabulary-approach faces the problem of being too strict. On the other en"
W15-1832,W13-0215,1,0.852943,"expansion plan of the expansion plans of a expansion ... plans for an expansions plans for the expansions In case the number of resulting instances was lower than five, the empty places were filled up with the most frequently used prepositions overall computed for all compounds extracted from the Google n-gram corpus. These were: of, from, on, for and by. The total for a preposition given a compound is the sum of all frequencies obtained for each single query. The third step is using the data generated in Step 2 in a GWAP, a game with a purpose, in order to collect human judgements. Wordrobe (Venhuizen et al., 2013), an existing internet-based GWAP architecture was used to launch a nounnoun compound annotation task in the shape of a game named burgers at www.wordrobe.org. Players of this game, not necessarily knowing anything about linguistic annotation, received a snippet of a text with the relevant noun-noun compound marked up in bold face, and were asked to select the most appropriate prepositions of the five candidates generated in Step 2. They were awarded points relative to the agreement of other players’ choices for the same question (using add1 smoothing initially). Players were instructed to hit"
W16-4304,N15-1184,0,0.0767073,"Missing"
W16-4304,P15-1010,0,0.0985402,"Facebook pages for a total of ˇ uˇrek and Sojka, 2010), we trained the embed20,000 sentences. Using the gensim library (Reh˚ dings with the following parameters: window size of 5, learning rate of 0.01 and dimensionality of 100. We filtered out words with frequency lower than 2 occurrences. • Retrofitted embeddings: Retrofitting (Faruqui et al., 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it’s done for example to create senseaware (Iacobacci et al., 2015) or sentiment-aware (Tang et al., 2014) embeddings.4 In this work, we retrofit general embeddings to include information about emotions, so that emotion-similar words can get closer in space. Both the Google as well as our Facebook embeddings were retrofitted with 4 Training emotion-aware embeddings is a strategy that we plan to explore in future work. 34 lexical information obtained from the NRC10 Lexicon mentioned above, which provides emotionsimilarity for each token. Note that differently from the previous two types of embeddings, the retrofitted ones do rely on handcrafted information in"
W16-4304,W10-0208,0,0.110799,"inson, 2016), but interest in people’s opinions and how they feel isn’t limited to commercial reasons, as it invests social monitoring, too, including health care and education (Mohammad, 2016). However, emotions and opinions are not always expressed this explicitly, so that there is high interest in developing systems towards their automatic detection. Creating manually annotated datasets large enough to train supervised models is not only costly, but also—especially in the case of opinions and emotions—difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010). Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.e. using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emot"
W16-4304,P09-1113,0,0.0698321,"s is not only costly, but also—especially in the case of opinions and emotions—difficult, due to the intrinsic subjectivity of the task (Strapparava and Mihalcea, 2008; Kim et al., 2010). Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.e. using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica. Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon. Emoticons are used as proxies also by Hallsmar and Palm (2016), who use distributed vector representations to find which words are interchangeable with emoticons but also which emoticons are used in a similar context. We take advantage of"
W16-4304,L16-1623,0,0.0415302,"t posts that have a certain length, ignore posts that are only quotes or captions to images, or expand posts by including content from linked html pages, which might provide larger and better contexts (Plank et al., 2014). Additionally, and most importantly, one could use an entropy-based measure to select only posts that have a strong emotion rather than just considering the majority emotion as training label. For the former, namely the choice of Facebook pages, which we believe deserves the most investigation, one could explore several avenues, especially in relation to stance-based issues (Mohammad et al., 2016). In our dataset, for example, a post about Chile beating Colombia in a football match during the Copa America had very contradictory reactions, depending on which side readers would cheer for. Similarly, the very same political event, for example, would get very different reactions from readers if it was posted on Fox News or The Late Night Show, as the target audience is likely to feel very differently about the same issue. This also brings up theoretical issues related more generally to the definition of the emotion detection task, as it’s strongly dependent on personal traits of the audien"
W16-4304,N12-1071,0,0.106651,"ween the system scores and the gold standard; and a coarsegrained method where each emotion score was converted to a binary label, and precision, recall, and f-score were computed to assess performance. As it is done in most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), we also treat this as a classification problem (coarse-grained). This dataset has been extensively used for the evaluation of various unsupervised methods (Strapparava and Mihalcea, 2008), but also for testing different supervised learning techniques and feature portability (Mohammad, 2012). 3.2 Fairy Tales dataset This is a dataset collected by Alm (2008), where about 1,000 sentences from fairy tales (by B. Potter, H.C. Andersen and Grimm) were annotated with the same six emotions of the Affective Text dataset, though with different names: Angry, Disgusted, Fearful, Happy, Sad, and Surprised. In most works that use this dataset (Kim et al., 2010; Chaffar and Inkpen, 2011; Calvo and Mac Kim, 2013), only sentences where all annotators agreed are used, and the labels angry and disgusted are merged. We adopt the same choices. 3.3 ISEAR The ISEAR (International Survey on Emotion Ant"
W16-4304,C14-1168,0,0.0406572,"on a larger corpus might also be successful, but would rely on an external lexicon. The largest room for yielding not only better results but also interesting insights on extensions of this approach lies in the choice of training instances, both in terms of Facebook pages to get posts from, as well as in which posts to select from the given pages. For the latter, one could for example only select posts that have a certain length, ignore posts that are only quotes or captions to images, or expand posts by including content from linked html pages, which might provide larger and better contexts (Plank et al., 2014). Additionally, and most importantly, one could use an entropy-based measure to select only posts that have a strong emotion rather than just considering the majority emotion as training label. For the former, namely the choice of Facebook pages, which we believe deserves the most investigation, one could explore several avenues, especially in relation to stance-based issues (Mohammad et al., 2016). In our dataset, for example, a post about Chile beating Colombia in a football match during the Copa America had very contradictory reactions, depending on which side readers would cheer for. Simil"
W16-4304,E12-1049,0,0.0941755,"f the task (Strapparava and Mihalcea, 2008; Kim et al., 2010). Therefore, research has focused on unsupervised methods enriched with information derived from lexica, which are manually created (Kim et al., 2010; Chaffar and Inkpen, 2011). Since Go et al. (2009) have shown that happy and sad emoticons can be successfully used as signals for sentiment labels, distant supervision, i.e. using some reasonably safe signals as proxies for automatically labelling training data (Mintz et al., 2009), has been used also for emotion recognition, for example exploiting both emoticons and Twitter hashtags (Purver and Battersby, 2012), but mainly towards creating emotion lexica. Mohammad and Kiritchenko (2015) use hashtags, experimenting also with highly fine-grained emotion sets (up to almost 600 emotion labels), to create the large Hashtag Emotion Lexicon. Emoticons are used as proxies also by Hallsmar and Palm (2016), who use distributed vector representations to find which words are interchangeable with emoticons but also which emoticons are used in a similar context. We take advantage of distant supervision by using Facebook reactions as proxies for emotion labels, which to the best of our knowledge hasn’t been done y"
W16-4304,S07-1013,0,0.509556,"to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section 3.4. A summary is provided in Table 1, which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section 4), all three have been used as benchmarks for our evaluation. 3.1 Affective Text dataset Task 14 at SemEval 2007 (Strapparava and Mihalcea, 2007) was concerned with the classification of emotions and valence in news headlines. The headlines where collected from several news websites including Google news, The New York Times, BBC News and CNN. The used emotion labels were Anger, Disgust, Fear, Joy, Sadness, Surprise, in line with the six basic emotions of Ekman’s standard model (Ekman, 1992). Valence was to be determined as positive or negative. Classification of emotion and valence were treated as separate tasks. Emotion labels were not considered as mututally exclusive, and each emotion was assigned a score from 0 to 100. Training/dev"
W16-4304,strapparava-valitutti-2004-wordnet,0,0.218721,"Missing"
W16-4304,P14-1146,0,0.378718,"ojka, 2010), we trained the embed20,000 sentences. Using the gensim library (Reh˚ dings with the following parameters: window size of 5, learning rate of 0.01 and dimensionality of 100. We filtered out words with frequency lower than 2 occurrences. • Retrofitted embeddings: Retrofitting (Faruqui et al., 2015) has been shown as a simple but efficient way of informing trained embeddings with additional information derived from some lexical resource, rather than including it directly at the training stage, as it’s done for example to create senseaware (Iacobacci et al., 2015) or sentiment-aware (Tang et al., 2014) embeddings.4 In this work, we retrofit general embeddings to include information about emotions, so that emotion-similar words can get closer in space. Both the Google as well as our Facebook embeddings were retrofitted with 4 Training emotion-aware embeddings is a strategy that we plan to explore in future work. 34 lexical information obtained from the NRC10 Lexicon mentioned above, which provides emotionsimilarity for each token. Note that differently from the previous two types of embeddings, the retrofitted ones do rely on handcrafted information in the form of a lexical resource. 4.3 Res"
W17-4404,N13-1037,0,0.176312,"stead, its normalized form is analyzed correctly, as shown in Figure 1. While being a promising direction, we see at least two issues with the assessment of normalization as a successful step in POS tagging noncanonical text. Firstly, normalization experiments are usually carried out assuming that the tokens to be normalized are already detected (gold error detection). Thus little is known on how normalization impacts tagging accuracy in a realworld scenario (not assuming gold error detection). Secondly, normalization is one way to go about processing non-canonical data, but not the only one (Eisenstein, 2013; Plank, 2016). Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form. For instance, Introduction Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (Gimpel et al., 2011; Hovy et al., 2014), parsing (McClosky, 2010; Foster et al., 2011) and named entity recognition (Ritter et al., 2011). In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data. One obvious way to tackle the probl"
W17-4404,I11-1100,0,0.0249902,"Missing"
W17-4404,P11-2008,0,0.146694,"Missing"
W17-4404,W15-4319,0,0.125726,"Missing"
W17-4404,P11-1038,0,0.775345,"latter approach yields a tagging model that is competitive with a Twitter state-of-the-art tagger. 1 NN pix NNS pictures NN comming VBG coming NNS tomoroe NN tomorrow Figure 1: Example tweet from the test data, raw and normalized form, tagged with Stanford NLP. different times, typically a new specifically dedicated tool needs to be created. The alternative route is to take a general purpose state-of-the-art POS tagger and adapt it to successfully tag non-canonical data. In the case of Twitter, one way to go about this is lexical normalization. It is the task of detecting “ill-formed” words (Han and Baldwin, 2011) and replacing them with their canonical counterpart. To illustrate why this might help, consider the following tweet: “new pix comming tomoroe”. An off-the-shelf system such as the Stanford NLP suite1 makes several mistakes on the raw input, e.g., the verb ‘comming’ as well as the plural noun ‘pix’ are tagged as singular noun. Instead, its normalized form is analyzed correctly, as shown in Figure 1. While being a promising direction, we see at least two issues with the assessment of normalization as a successful step in POS tagging noncanonical text. Firstly, normalization experiments are usu"
W17-4404,hovy-etal-2014-pos,1,0.851806,"Missing"
W17-4404,L16-1012,0,0.0197501,"orming POS tagging. Finally, normalization for POS tagging is certainly not limited to non-canonical data stemming from social media. Indeed, another stream of related work is focused on historical data, usually originating from the 15th till the 18th century. The motivation behind this is that in order to apply current language processing tools, the texts need to be normalized first, as spelling has changed through time. Experiments on POS tagging historical data that was previously normalized have been investigated for English (Yang and Eisenstein, 2016), German (Bollmann, 2013), and Dutch (Hupkes and Bod, 2016; Tjong Kim Sang, 2016). In this latter work, different methods of ‘translating’ historical Dutch texts to modern Dutch are explored, and a vocabulary lookup-based approach appears to work best.8 In this paper we focused on normalization and POS tagging for Twitter data only. gers obtain the highest performance. The ARK tagger has difficulties with prepositions (P), which are mistagged as numerals ($). These are almost all cases of ‘2’ and ‘4’, which represent Twitter slang for ‘to’ and ‘for’, respectively. Our system performs a lot better on these, due to the normalization model as already ob"
W17-4404,P16-2067,1,0.840559,"e-of-art performance on the erroneous tokens (using gold error detection) on the LexNorm dataset (Han and Baldwin, 2011) as well as state-of-art on another corpus which is usually benchmarked without assuming gold error detection (Baldwin et al., 2015). We refer the reader to the paper (van der Goot and van Noord, 2017) for further details. To obtain a more detailed view of the effect of normalization on POS tagging, we investigate four experimental setups: We use B ILTY, an off-the-shelf bi-directional Long Short-Term Memory (bi-LSTM) tagger which utilizes both word and character embeddings (Plank et al., 2016). The tagger is trained on 1,576 training tweets (Section 2.1). We tune the parameters of the POS tagger on the development set to derive the following hyperparameter setup, which we use throughout the rest of the experiments: 10 epochs, 1 bi-LSTM layer, 100 input dimensions for words, 256 for characters, σ=0.2, constant embeddings initializer, Adam trainer, and updating embeddings during backpropagation.4 3 • normalizing only unknown words; To Normalize • considering all words: the model decides whether a word should be normalized or not; First we evaluate the impact of normalization on the P"
W17-4404,P14-3012,0,0.0196513,"ed as training in a semi-supervised setting (Section 4). For all experiments we use existing datasets as well as newly created resources, cf. Section 2.1. The POS model used is described in Section 2.2. 2 Some tags are rare, like M and Y. In fact, M occurs only once in T EST L; Y never occurs in D EV and only once in T EST L and three times in T EST O. Therefore our confusion matrices (over D EV and T EST O, respectively) have different number of labels on the axes. 3 https://en.wikipedia.org/wiki/Most_ common_words_in_English 32 2.2 Model We train the normalization model on 2,577 tweets from Li and Liu (2014). Our model (van der Goot and van Noord, 2017) achieves state-of-art performance on the erroneous tokens (using gold error detection) on the LexNorm dataset (Han and Baldwin, 2011) as well as state-of-art on another corpus which is usually benchmarked without assuming gold error detection (Baldwin et al., 2015). We refer the reader to the paper (van der Goot and van Noord, 2017) for further details. To obtain a more detailed view of the effect of normalization on POS tagging, we investigate four experimental setups: We use B ILTY, an off-the-shelf bi-directional Long Short-Term Memory (bi-LSTM"
W17-4404,D11-1141,0,0.471888,"alworld scenario (not assuming gold error detection). Secondly, normalization is one way to go about processing non-canonical data, but not the only one (Eisenstein, 2013; Plank, 2016). Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form. For instance, Introduction Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (Gimpel et al., 2011; Hovy et al., 2014), parsing (McClosky, 2010; Foster et al., 2011) and named entity recognition (Ritter et al., 2011). In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data. One obvious way to tackle the problem of processing non-canonical data is to build taggers that are specifically tailored to such text. A prime example is the ARK POS tagger, designed especially to process English Twitter data (Gimpel et al., 2011; Owoputi et al., 2013), on which it achieves state-of-the-art results. One drawback of this approach is that non-canonical data is not all of the same kind, so that for non-canonical non-Twitter data or even collections of Twitter sample"
W17-4404,N15-1142,0,0.0124374,"tion is to leave the text as is, and exploit very large amounts of raw data via semi-supervised learning. The rationale behind this is the following: provided the size of the data is sufficient, a model can be trained to naturally learn the POS tags of noisy data. 4.1 Effect of Word Embeddings An easy and effective use of word embeddings in neural network approaches is to use them to initialize the word lookup parameters. We train a skip-gram word embeddings model using word2vec (Mikolov et al., 2013) on 760M tweets (as described in Section 3.1). We also experiment with structured skip-grams (Ling et al., 2015), an adaptation of word2vec which takes word order into account. It has been shown to be beneficial for syntactically oriented tasks, like POS tagging. Therefore we want to evaluate structured skip-grams as well. The normalization model uses word embeddings with a window size of 1; we compare this Effect of Self-training WINDOW SIZE SKIPG . STRUCT. SKIPG . 1 5 88.14 (±.30) 88.51 (±.24) 87.56 (±.08) 88.11 (±.49) Table 2: Accuracy on raw D EV: various pretrained skip-gram embeddings for initialization. 34 training data where only tweets that do not contain named entities are selected. Hence, we"
W17-4404,N16-1157,0,0.0576384,"ting systems incorporated any normalization strategy before performing POS tagging. Finally, normalization for POS tagging is certainly not limited to non-canonical data stemming from social media. Indeed, another stream of related work is focused on historical data, usually originating from the 15th till the 18th century. The motivation behind this is that in order to apply current language processing tools, the texts need to be normalized first, as spelling has changed through time. Experiments on POS tagging historical data that was previously normalized have been investigated for English (Yang and Eisenstein, 2016), German (Bollmann, 2013), and Dutch (Hupkes and Bod, 2016; Tjong Kim Sang, 2016). In this latter work, different methods of ‘translating’ historical Dutch texts to modern Dutch are explored, and a vocabulary lookup-based approach appears to work best.8 In this paper we focused on normalization and POS tagging for Twitter data only. gers obtain the highest performance. The ARK tagger has difficulties with prepositions (P), which are mistagged as numerals ($). These are almost all cases of ‘2’ and ‘4’, which represent Twitter slang for ‘to’ and ‘for’, respectively. Our system performs a lot bet"
W17-4404,W17-1410,0,0.0368357,"Missing"
W17-4404,N10-1004,0,0.01616,"ttle is known on how normalization impacts tagging accuracy in a realworld scenario (not assuming gold error detection). Secondly, normalization is one way to go about processing non-canonical data, but not the only one (Eisenstein, 2013; Plank, 2016). Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form. For instance, Introduction Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (Gimpel et al., 2011; Hovy et al., 2014), parsing (McClosky, 2010; Foster et al., 2011) and named entity recognition (Ritter et al., 2011). In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data. One obvious way to tackle the problem of processing non-canonical data is to build taggers that are specifically tailored to such text. A prime example is the ARK POS tagger, designed especially to process English Twitter data (Gimpel et al., 2011; Owoputi et al., 2013), on which it achieves state-of-the-art results. One drawback of this approach is that non-canonical data is not all of the same kind, so that"
W17-4404,N13-1039,0,0.0770857,"Missing"
W17-5043,W17-5007,0,0.156163,"n explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this paper, we explore the performance of a linear SVM trained on languageindependent character features for the NLI Shared Task 2017. Our basic system (G RONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution. 1 Introduction Native Language I"
W17-5043,C16-1333,1,0.848832,"dropout, 20 epochs, trained with the adam optimization algorithm (Kingma and Ba, 2014) for 20 iterations with a batch size of 50. 3.3.2 Deep Residual Networks Deep residual networks (resnets) are a class of convolutional neural networks (CNNs), which consist of several convolutional blocks with skip connections in between (He et al., 2016). Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks. Resnets have been shown to be useful for NLP tasks, such as text classification (Conneau et al., 2016), and sequence labelling (Bjerva et al., 2016). We applied resnets with four residual blocks in our en385 semble experiments, each containing two successive one-dimensional convolutions. Each such block is followed by an average pooling layer and dropout (p = 0.5, Srivastava et al. (2014)). The resnets were applied to several input representations: word unigrams, and character 4-6-grams. The outputs of each resnet are concatenated before passing through two fully connected layers. We trained the resnet over 50 epochs with adam, using the model with the lowest validation loss. In addition to dropout, we used weight decay for regularization"
W17-5043,C12-1025,0,0.0743422,"ect their L2 writing. At a large scale, this could be extended to enhance existing teaching pedagogies and tailor them towards students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with e"
W17-5043,P16-2067,1,0.895832,"Missing"
W17-5043,W13-1706,0,0.128826,"students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this pape"
W17-5043,D14-1142,0,0.431151,"Missing"
W17-5043,W13-1714,0,0.414695,"Missing"
W18-4919,E17-4011,0,0.0222994,"and for any language for which an idiom dictionary exists. Although we use manually created definitions, these can also be acquired and refined automatically, as done by Liu and Hwa (2016). A side benefit of considering both figurative and literal senses is that separate scores can be assigned for both senses. This could be used for detecting difficult cases like dual meanings or puns, since those cases would get high scores for both senses. In future work, our aim is to further improve these cohesion graph-based classifiers by exploring different similarity measures, such as those tested by Ehren (2017) for German. Another promising avenue is to use more compositional representations of contexts and literalisations (see also Gharbieh et al., 2016). This would also allow us to use the information from verbs and modifiers more effectively, as in its current form our method relies on word-to-word comparisons and only nouns contribute to performance. Finally, we find that, for evaluation, using both micro- and macro-averaged metrics is an important way of ensuring balanced performance on both infrequent and frequent PIE types, in addition to using a wide range of corpora. Acknowledgements This w"
W18-4919,J09-1005,0,0.0199713,"ver PIE types, in addition to the harmonic mean of the two. 4 Comparison to Related Work Ideally, we would be able to compare approaches from different papers directly, but this is often impossible. The lack of an established evaluation framework means that reported results for PIE disambiguation are often on different (splits of) datasets, obtained in different ways (cross-validation, leave-oneout) using a range of different metrics (micro- and macro-averaged accuracy and F1-score). For example, Sporleder and Li (2009) report micro-accuracy and micro-F1 scores on the Gigaword corpus, whereas Fazly et al. (2009) report macro-accuracy scores on the VNC-Tokens dataset. A potential solution to this problem was provided by SemEval-2013 Task 5b on PIE disambiguation (Korkontzelos et al., 2013), as results from different participants could be directly compared. However, this dataset does not seem to have been used by the community since. Nevertheless, we compare to two other unsupervised approaches, the canonical form classifier (CForm) by Fazly et al. (2009), and the k-means clustering approach (KMeans) of Gharbieh et al. (2016). The CForm classifier is based on the assumption that idiomatic PIEs show les"
W18-4919,W16-1817,0,0.0736863,"(2009) report micro-accuracy and micro-F1 scores on the Gigaword corpus, whereas Fazly et al. (2009) report macro-accuracy scores on the VNC-Tokens dataset. A potential solution to this problem was provided by SemEval-2013 Task 5b on PIE disambiguation (Korkontzelos et al., 2013), as results from different participants could be directly compared. However, this dataset does not seem to have been used by the community since. Nevertheless, we compare to two other unsupervised approaches, the canonical form classifier (CForm) by Fazly et al. (2009), and the k-means clustering approach (KMeans) of Gharbieh et al. (2016). The CForm classifier is based on the assumption that idiomatic PIEs show less variability than literal ones. It uses a set of canonical forms for each idiom (e.g. make a mark, make one’s mark), and labels all PIEs occurring in a canonical form as idiomatic, and literal otherwise. The KMeans classifier builds vector representations of both the PIE and its immediate context based on word embeddings, and clusters those using the k-means algorithm. It then uses the CForm classifier to label the clusters, and propagates the majority label for each cluster to all PIEs in that cluster. Both are eva"
W18-4919,D17-1263,0,0.0152657,"successful enough to manage, but too successful for help. (British National Corpus (BNC; Burnard, 2007) - doc. ACP - sent. 1209) (2) There was still a dark blob, where it might have hit the wall. (BNC - doc. B2E - sent. 1531) Distinguishing literal and figurative uses is a crucial step towards being able to automatically interpret the meaning of a text containing idiomatic expressions. It has been shown that idiomatic expressions pose a challenge for various NLP applications (Sag et al., 2002), including sentiment analysis (Williams et al., 2015) and machine translation (Salton et al., 2014a; Isabelle et al., 2017). For the latter, it has also been shown that being able to interpret idioms indeed improves performance (Salton et al., 2014b). In this work, we use a method for unsupervised disambiguation that exploits semantic cohesion between the PIE and its context, based on the lexical cohesion approach pioneered by Sporleder and Li (2009). We extend this method and evaluate it on English data in a comprehensive evaluation framework, in order to answer the following research question: Do contexts enriched with literalisations of idioms provide a useful new signal for disambiguation? 2 Approach The disam"
W18-4919,S13-2007,0,0.191292,"ion asks whether literalisations of figurative senses are a useful source of information for improved disambiguation of PIEs. To provide an answer, we test our lexical cohesion graph with and without literalisation on a collection of existing datasets (Section 3.1), and evaluate performance using both micro- and macro-accuracy (Section 3.2). 3.1 Data In order to provide a comprehensive evaluation dataset, we make use of four sizeable corpora containing sense-annotated PIEs:4 the VNC-Tokens Dataset (Cook et al., 2008), the IDIX Corpus (Sporleder et al., 2010), the SemEval-2013 Task 5b dataset (Korkontzelos et al., 2013), and the PIE Corpus.5 An overview of these datasets is provided in Table 1. # Types # Instances # Sense labels Source Corpus VNC-Tokens IDIX SemEval-2013 Task 5b PIE Corpus 53 52 65 278 2,984 4,022 4,350 1,050 3 6 4 3 BNC BNC ukWaC BNC Combined (development) Combined (test) 299 146 8,235 3,073 2 2 BNC & ukWaC BNC & ukWaC Table 1: Overview of existing corpora of sense-annotated PIEs. The source corpus indicates the corpora from which the PIE instances were selected, either the British National Corpus (Burnard, 2007) or ukWaC (Ferraresi et al., 2008). Each corpus has slightly different benefits"
W18-4919,N16-1040,0,0.0132815,"is potential for combining the two types of classification to achieve better performance. Using average similarity differences as confidence values to pick one classifier over the other for a particular instance proved ineffective, but a more advanced setup combining the features of the two classifiers could yield a more effective combination. Moreover, literalisations are cheap to acquire and are available for many PIE types and for any language for which an idiom dictionary exists. Although we use manually created definitions, these can also be acquired and refined automatically, as done by Liu and Hwa (2016). A side benefit of considering both figurative and literal senses is that separate scores can be assigned for both senses. This could be used for detecting difficult cases like dual meanings or puns, since those cases would get high scores for both senses. In future work, our aim is to further improve these cohesion graph-based classifiers by exploring different similarity measures, such as those tested by Ehren (2017) for German. Another promising avenue is to use more compositional representations of contexts and literalisations (see also Gharbieh et al., 2016). This would also allow us to"
W18-4919,D14-1162,0,0.0812033,"Figure 1. In the original approach, though, it is only tested whether the literal sense fits or not, by comparing the full and pruned graph. However, this does not measure whether the figurative sense fits. Ideally, we would like to compare the fit of the literal and figurative senses directly. We do this by introducing and using idiom literalisations (Section 2.2). 2.1 Basic Lexical Cohesion Graph We reimplement the original lexical cohesion graph method with one major modification: instead of Normalized Google Distance we use cosine similarity between 300-dimensional GloVe word embeddings (Pennington et al., 2014). Furthermore, we adapt specifics of the classifier to optimise performance on the development set. We use only nouns to build the contexts, where the part-of-speech of words is determined automatically using the spaCy PoS-tagger3 , instead of both nouns and verbs. As a context window, we use two sentences of additional context on either side of the sentence containing the PIE. We also remove edges between two PIE component words, since those are the same for all instances of the same type and thus uninformative. Finally, PIEs are only classified as literal if average similarity of the pruned"
W18-4919,W14-1007,0,0.0175452,"to British youth: not successful enough to manage, but too successful for help. (British National Corpus (BNC; Burnard, 2007) - doc. ACP - sent. 1209) (2) There was still a dark blob, where it might have hit the wall. (BNC - doc. B2E - sent. 1531) Distinguishing literal and figurative uses is a crucial step towards being able to automatically interpret the meaning of a text containing idiomatic expressions. It has been shown that idiomatic expressions pose a challenge for various NLP applications (Sag et al., 2002), including sentiment analysis (Williams et al., 2015) and machine translation (Salton et al., 2014a; Isabelle et al., 2017). For the latter, it has also been shown that being able to interpret idioms indeed improves performance (Salton et al., 2014b). In this work, we use a method for unsupervised disambiguation that exploits semantic cohesion between the PIE and its context, based on the lexical cohesion approach pioneered by Sporleder and Li (2009). We extend this method and evaluate it on English data in a comprehensive evaluation framework, in order to answer the following research question: Do contexts enriched with literalisations of idioms provide a useful new signal for disambiguat"
W18-4919,W14-0806,0,0.0172588,"to British youth: not successful enough to manage, but too successful for help. (British National Corpus (BNC; Burnard, 2007) - doc. ACP - sent. 1209) (2) There was still a dark blob, where it might have hit the wall. (BNC - doc. B2E - sent. 1531) Distinguishing literal and figurative uses is a crucial step towards being able to automatically interpret the meaning of a text containing idiomatic expressions. It has been shown that idiomatic expressions pose a challenge for various NLP applications (Sag et al., 2002), including sentiment analysis (Williams et al., 2015) and machine translation (Salton et al., 2014a; Isabelle et al., 2017). For the latter, it has also been shown that being able to interpret idioms indeed improves performance (Salton et al., 2014b). In this work, we use a method for unsupervised disambiguation that exploits semantic cohesion between the PIE and its context, based on the lexical cohesion approach pioneered by Sporleder and Li (2009). We extend this method and evaluate it on English data in a comprehensive evaluation framework, in order to answer the following research question: Do contexts enriched with literalisations of idioms provide a useful new signal for disambiguat"
W18-4919,E09-1086,0,0.389799,"iguation of potentially idiomatic expressions involves determining the sense of a potentially idiomatic expression in a given context, e.g. determining that make hay in ‘Investment banks made hay while takeovers shone.’ is used in a figurative sense. This enables automatic interpretation of idiomatic expressions, which is important for applications like machine translation and sentiment analysis. In this work, we present an unsupervised approach for English that makes use of literalisations of idiom senses to improve disambiguation, which is based on the lexical cohesion graph-based method by Sporleder and Li (2009). Experimental results show that, while literalisation carries novel information, its performance falls short of that of state-of-the-art unsupervised methods. 1 Introduction Interpreting potentially idiomatic expressions (PIEs, for short) is the task of determining the meaning of PIEs in context.1 In its most basic form, it consists of distinguishing between the figurative and literal usage of a given expression, as illustrated by hit the wall in Examples (1) and (2), respectively. (1) Melanie hit the wall so familiar to British youth: not successful enough to manage, but too successful for h"
W18-4919,sporleder-etal-2010-idioms,0,0.0243996,"the idiom more concisely. 3 Experiments Our research question asks whether literalisations of figurative senses are a useful source of information for improved disambiguation of PIEs. To provide an answer, we test our lexical cohesion graph with and without literalisation on a collection of existing datasets (Section 3.1), and evaluate performance using both micro- and macro-accuracy (Section 3.2). 3.1 Data In order to provide a comprehensive evaluation dataset, we make use of four sizeable corpora containing sense-annotated PIEs:4 the VNC-Tokens Dataset (Cook et al., 2008), the IDIX Corpus (Sporleder et al., 2010), the SemEval-2013 Task 5b dataset (Korkontzelos et al., 2013), and the PIE Corpus.5 An overview of these datasets is provided in Table 1. # Types # Instances # Sense labels Source Corpus VNC-Tokens IDIX SemEval-2013 Task 5b PIE Corpus 53 52 65 278 2,984 4,022 4,350 1,050 3 6 4 3 BNC BNC ukWaC BNC Combined (development) Combined (test) 299 146 8,235 3,073 2 2 BNC & ukWaC BNC & ukWaC Table 1: Overview of existing corpora of sense-annotated PIEs. The source corpus indicates the corpora from which the PIE instances were selected, either the British National Corpus (Burnard, 2007) or ukWaC (Ferrar"
zaninello-nissim-2010-creation,baroni-etal-2004-introducing,0,\N,Missing
zaninello-nissim-2010-creation,copestake-etal-2002-multiword,0,\N,Missing
zaninello-nissim-2010-creation,W06-2403,0,\N,Missing
zaninello-nissim-2010-creation,W04-0411,0,\N,Missing
zaninello-nissim-2010-creation,calzolari-etal-2002-towards,0,\N,Missing
