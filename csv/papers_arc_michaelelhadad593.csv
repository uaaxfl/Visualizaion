2021.unimplicit-1.3,Evaluation Guidelines to Deal with Implicit Phenomena to Assess Factuality in Data-to-Text Generation,2021,-1,-1,2,0,658,roy eisenstadt,Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language,0,"Data-to-text generation systems are trained on large datasets, such as WebNLG, Ro-toWire, E2E or DART. Beyond traditional token-overlap evaluation metrics (BLEU or METEOR), a key concern faced by recent generators is to control the factuality of the generated text with respect to the input data specification. We report on our experience when developing an automatic factuality evaluation system for data-to-text generation that we are testing on WebNLG and E2E data. We aim to prepare gold data annotated manually to identify cases where the text communicates more information than is warranted based on the in-put data (extra) or fails to communicate data that is part of the input (missing). While analyzing reference (data, text) samples, we encountered a range of systematic uncertainties that are related to cases on implicit phenomena in text, and the nature of non-linguistic knowledge we expect to be involved when assessing factuality. We derive from our experience a set of evaluation guidelines to reach high inter-annotator agreement on such cases."
2021.naacl-main.9,Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of {GQA},2021,-1,-1,4,0,3240,yonatan bitton,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models{'} performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA{'}s compositionality and carefully balanced label distribution, two high-performing models drop 13-17{\%} in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models."
2021.findings-emnlp.259,Data Efficient Masked Language Modeling for Vision and Language,2021,-1,-1,2,0,3240,yonatan bitton,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Masked language modeling (MLM) is one of the key sub-tasks in vision-language pretraining. In the cross-modal setting, tokens in the sentence are masked at random, and the model predicts the masked tokens given the image and the text. In this paper, we observe several key disadvantages of MLM in this setting. First, as captions tend to be short, in a third of the sentences no token is sampled. Second, the majority of masked tokens are stop-words and punctuation, leading to under-utilization of the image. We investigate a range of alternative masking strategies specific to the cross-modal setting that address these shortcomings, aiming for better fusion of text and image in the learned representation. When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings. Further, our pre-training approach substantially outperforms the baseline model on a prompt-based probing task designed to elicit image objects. These results and our analysis indicate that our method allows for better utilization of the training data."
2020.lrec-1.727,Building a {H}ebrew Semantic Role Labeling Lexical Resource from Parallel Movie Subtitles,2020,35,0,2,0,18077,ben eyal,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a semantic role labeling resource for Hebrew built semi-automatically through annotation projection from English. This corpus is derived from the multilingual OpenSubtitles dataset and includes short informal sentences, for which reliable linguistic annotations have been computed. We provide a fully annotated version of the data including morphological analysis, dependency syntax and semantic role labeling in both FrameNet and ProbBank styles. Sentences are aligned between English and Hebrew, both sides include full annotations and the explicit mapping from the English arguments to the Hebrew ones. We train a neural SRL model on this Hebrew resource exploiting the pre-trained multilingual BERT transformer model, and provide the first available baseline model for Hebrew SRL as a reference point. The code we provide is generic and can be adapted to other languages to bootstrap SRL resources."
2020.dt4tp-1.2,Neural Micro-Planning for Data to Text Generation Produces more Cohesive Text,2020,-1,-1,2,0,658,roy eisenstadt,Proceedings of the Workshop on Discourse Theories for Text Planning,0,None
N19-1395,Question Answering as an Automatic Evaluation Metric for News Article Summarization,2019,0,9,3,0,10707,matan eyal,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Recent work in the field of automatic summarization and headline generation focuses on maximizing ROUGE scores for various news datasets. We present an alternative, extrinsic, evaluation metric for this task, Answering Performance for Evaluation of Summaries. APES utilizes recent progress in the field of reading-comprehension to quantify the ability of a summary to answer a set of manually created questions regarding central entities in the source article. We first analyze the strength of this metric by comparing it to known manual evaluation metrics. We then present an end-to-end neural abstractive model that maximizes APES, while increasing ROUGE scores to competitive results."
W16-2526,Sentence Embedding Evaluation Using Pyramid Annotation,2016,16,1,3,1,26280,tal baumel,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,"Word embedding vectors are used as input for a variety of tasks. Choosing the right model and features for producing such vectors is not a trivial task and different embedding methods can greatly affect results. In this paper we repurpose the Pyramid Method annotations used for evaluating automatic summarization to create a benchmark for comparing embedding models when identifying paraphrases of text snippets containing a single clause. We present a method of converting pyramid annotation files into two distinct sentence embedding tests. We show that our method can produce a good amount of testing data, analyze the quality of the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance."
L16-1688,The {H}ebrew {F}rame{N}et Project,2016,0,1,2,0,35396,avi hayoun,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present the Hebrew FrameNet project, describe the development and annotation processes and enumerate the challenges we faced along the way. We have developed semi-automatic tools to help speed the annotation and data collection process. The resource currently covers 167 frames, 3,000 lexical units and about 500 fully annotated sentences. We have started training and testing automatic SRL tools on the seed data."
P14-1086,Query-Chain Focused Summarization,2014,18,11,3,1,26280,tal baumel,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Update summarization is a form of multidocument summarization where a document set must be summarized in the context of other documents assumed to be known. Efficient update summarization must focus on identifying new information and avoiding repetition of known information. In Query-focused summarization, the task is to produce a summary as an answer to a given query. We introduce a new task, Query-Chain Summarization, which combines aspects of the two previous tasks: starting from a given document set, increasingly specific queries are considered, and a new summary is produced at each step. This process models exploratory search: a user explores a new topic by submitting a sequence of queries, inspecting a summary of the result set and phrasing a new query at each step. We present a novel dataset comprising 22 querychains sessions of length up to 3 with 3 matching human summaries each in the consumerhealth domain. Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries. We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant. Evaluation indicates the algorithm improves on strong baselines."
W13-3102,"Multi-document multilingual summarization corpus preparation, Part 2: {C}zech, {H}ebrew and {S}panish",2013,6,8,1,1,659,michael elhadad,Proceedings of the {M}ulti{L}ing 2013 Workshop on Multilingual Multi-document Summarization,0,"This document overviews the strategy, effort and aftermath of the MultiLing 2013 multilingual summarization data collection. We describe how the Data Contributors of MultiLing collected and generated a multilingual multi-document summarization corpus on 10 different languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian and Spanish. We discuss the rationale behind the main decisions of the collection, the methodology used to generate the multilingual corpus, as well as challenges and problems faced per language. This paper overviews the work on Czech, Hebrew and Spanish languages."
W13-1915,Effect of Out Of Vocabulary Terms on Inferring Eligibility Criteria for a Retrospective Study in {H}ebrew {EHR},2013,9,2,2,1,915,raphael cohen,Proceedings of the 2013 Workshop on Biomedical Natural Language Processing,0,"The Electronic Health Record (EHR) contains information useful for clinical, epidemiological and genetic studies. This information of patient symptoms, history, medication and treatment is not completely captured in the structured part of the EHR but is often found in the form of freetext narrative. A major obstacle for clinical studies is finding patients that fit the eligibility criteria of the study. Using EHR in order to automatically identify relevant cohorts can help speed up both clinical trials and retrospective studies (Restificar, Korkontzelos et al. 2013). While the clinical criteria for inclusion and exclusion from the study are explicitly stated in most studies, automating the process using the EHR database of the hospital is often impossible as the structured part of the database (age, gender, ICD9/10 medical codes, etc.xe2x80x99) rarely covers all of the criteria. Many resources such as UMLS (Bodenreider 2004), cTakes (Savova, Masanz et al. 2010), MetaMap (Aronson and Lang 2010) and recently richly annotated corpora and treebanks (Albright, Lanfranchi et al. 2013) are available for processing and representing medical texts in English. Resource poor languages, however, suffer from lack in NLP tools and medical resources. Dictionaries exhaustively mapping medical terms to the UMLS medical meta-thesaurus are only available in a limited number of languages besides English. NLP annotation tools, when they exist for resource poor languages, suffer from heavy loss of accuracy when used outside the domain on which they were trained, as is well documented for English (Tsuruoka, Tateishi et al. 2005; Tateisi, Tsuruoka et al. 2006). In this work we focus on the problem of classifying patient eligibility for inclusion in retrospective study of the epidemiology of epilepsy in Southern Israel. Israel has a centralized structure of medical services which include advanced EHR systems. However, the free text sections of these EHR are written in Hebrew, a resource poor language in both NLP tools and handcrafted medical vocabularies. Epilepsy is a common chronic neurologic disorder characterized by seizures. These seizures are transient signs and/or symptoms of abnormal, excessive, or hyper synchronous neuronal activity in the brain. Epilepsy is one of the most common of the serious neurological disorders (Hirtz, Thurman et al. 2007)."
J13-1007,"Word Segmentation, Unknown-word Resolution, and Morphological Agreement in a {H}ebrew Parsing System",2013,53,10,2,0.970804,3457,yoav goldberg,Computational Linguistics,0,"We present a constituency parsing system for Modern Hebrew. The system is based on the PCFG-LA parsing method of Petrov et al. 2006, which is extended in various ways in order to accommodate the specificities of Hebrew as a morphologically rich language with a small treebank. We show that parsing performance can be enhanced by utilizing a language resource external to the treebank, specifically, a lexicon-based morphological analyzer. We present a computational model of interfacing the external lexicon and a treebank-based parser, also in the common case where the lexicon and the treebank follow different annotation schemes. We show that Hebrew word-segmentation and constituency-parsing can be performed jointly using CKY lattice parsing. Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model. We suggest modeling grammatical agreement in a constituency-based parser as a filter mechanism that is orthogonal to the grammar, and present a concrete implementation of the method. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make.n n These contributions extend outside of the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsing methodology is useful in any case where the input is uncertain. Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with a small treebank."
W12-3308,Domain Adaptation of a Dependency Parser with a Class-Class Selectional Preference Model,2012,26,7,3,1,915,raphael cohen,Proceedings of {ACL} 2012 Student Research Workshop,0,"When porting parsers to a new domain, many of the errors are related to wrong attachment of out-of-vocabulary words. Since there is no available annotated data to learn the attachment preferences of the target domain words, we attack this problem using a model of selectional preferences based on domain-specific word classes. Our method uses Latent Dirichlet Allocations (LDA) to learn a domain-specific Selectional Preference model in the target domain using un-annotated data. The model provides features that model the affinities among pairs of words in the domain. To incorporate these new features in the parsing model, we adopt the co-training approach and retrain the parser with the selectional preferences features. We apply this method for adapting Easy First, a fast non-directional parser trained on WSJ, to the biomedical domain (Genia Treebank). The Selectional Preference features reduce error by 4.5% over the co-training baseline."
P11-2124,Joint {H}ebrew Segmentation and Parsing using a {PCFGLA} Lattice Parser,2011,20,21,2,1,3457,yoav goldberg,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs."
W10-2927,Inspecting the Structural Biases of Dependency Parsing Algorithms,2010,18,8,2,1,3457,yoav goldberg,Proceedings of the Fourteenth Conference on Computational Natural Language Learning,0,"We propose the notion of a structural bias inherent in a parsing system with respect to the language it is aiming to parse. This structural bias characterizes the behaviour of a parsing system in terms of structures it tends to under- and over- produce. We propose a Boosting-based method for uncovering some of the structural bias inherent in parsing systems. We then apply our method to four English dependency parsers (an Arc-Eager and Arc-Standard transition-based parsers, and first- and second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses."
W10-1412,Easy-First Dependency Parsing of {M}odern {H}ebrew,2010,7,17,2,1,3457,yoav goldberg,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"We investigate the performance of an easy-first, non-directional dependency parser on the Hebrew Dependency treebank. We show that with a basic feature set the greedy parser's accuracy is on a par with that of a first-order globally optimized MST parser. The addition of morphological-agreement feature improves the parsing accuracy, making it on-par with a second-order globally optimized MST parser. The improvement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used."
N10-1115,An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing,2010,25,143,2,1,3457,yoav goldberg,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models."
J10-4009,"Book Review: Natural Language Processing with Python by Steven Bird, Ewan {K}lein, and Edward Loper",2010,2,12,1,1,659,michael elhadad,Computational Linguistics,0,None
W09-3819,{H}ebrew Dependency Parsing: Initial Results,2009,12,17,2,1,3457,yoav goldberg,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We describe a newly available Hebrew Dependency Treebank, which is extracted from the Hebrew (constituency) Tree-bank. We establish some baseline unlabeled dependency parsing performance on Hebrew, based on two state-of-the-art parsers, MST-parser and MaltParser. The evaluation is performed both in an artificial setting, in which the data is assumed to be properly morphologically segmented and POS-tagged, and in a real-world setting, in which the parsing is performed on automatically segmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than Malt-Parser, and (b) both parsers do not make good use of morphological information when parsing Hebrew."
W09-2005,{G}aiku : Generating Haiku with Word Associations Norms,2009,19,59,4,1,46977,yael netzer,Proceedings of the Workshop on Computational Approaches to Linguistic Creativity,0,"Word associations are an important element of linguistic creativity. Traditional lexical knowledge bases such as WordNet formalize a limited set of systematic relations among words, such as synonymy, polysemy and hypernymy. Such relations maintain their systematicity when composed into lexical chains. We claim that such relations cannot explain the type of lexical associations common in poetic text. We explore in this paper the usage of Word Association Norms (WANs) as an alternative lexical knowledge source to analyze linguistic computational creativity. We specifically investigate the Haiku poetic genre, which is characterized by heavy reliance on lexical associations. We first compare the density of WAN-based word associations in a corpus of English Haiku poems to that of WordNet-based associations as well as in other non-poetic genres. These experiments confirm our hypothesis that the non-systematic lexical associations captured in WANs play an important role in poetic text. We then present Gaiku, a system to automatically generate Haikus from a seed word and using WAN-associations. Human evaluation indicate that generated Haikus are of lesser quality than human Haikus, but a high proportion of generated Haikus can confuse human readers, and a few of them trigger intriguing reactions."
E09-1038,"Enhancing Unlexicalized Parsing Performance Using a Wide Coverage Lexicon, Fuzzy Tag-Set Mapping, and {EM}-{HMM}-Based Lexical Probabilities",2009,16,24,4,1,3457,yoav goldberg,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank. This is achieved by defining a stochastic mapping layer between the two resources. Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora. We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens."
D09-1119,On the Role of Lexical Features in Sequence Labeling,2009,28,12,2,1,3457,yoav goldberg,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking. While standard models require as many as 100K distinct features, we derive models with as little as 1K features that perform as well or better on different domains. These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood. Contrastive error analysis (with and without lexical features) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities -- but we find this contribution does not generalize outside the training corpus. As a general strategy, we believe lexical features should not be directly derived from a training corpus but instead, carefully inferred and selected from other sources."
P08-2060,"split{SVM}: Fast, Space-Efficient, non-Heuristic, Polynomial Kernel Computation for {NLP} Applications",2008,5,43,2,1,3457,yoav goldberg,"Proceedings of ACL-08: HLT, Short Papers",0,"We present a fast, space efficient and non-heuristic method for calculating the decision function of polynomial kernel classifiers for NLP applications. We apply the method to the MaltParser system, resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy (a 30 time speedup over existing methods). The method implementation is available as the open-source splitSVM Java library."
P08-1083,Unsupervised Lexicon-Based Resolution of Unknown Words for Full Morphological Analysis,2008,9,23,4,1,32102,meni adler,Proceedings of ACL-08: HLT,1,"Morphological disambiguation proceeds in 2 stages: (1) an analyzer provides all possible analyses for a given token and (2) a stochastic disambiguation module picks the most likely analysis in context. When the analyzer does not recognize a given token, we hit the problem of unknowns. In large scale corpora, unknowns appear at a rate of 5 to 10% (depending on the genre and the maturity of the lexi"
P08-1085,{EM} Can Find Pretty Good {HMM} {POS}-Taggers (When Given a Good Start),2008,20,56,3,1,3457,yoav goldberg,Proceedings of ACL-08: HLT,1,"We address the task of unsupervised POS tagging. We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries. We present a family of algorithms to compute effective initial estimations p(t|w). We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline. We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods."
adler-etal-2008-tagging,Tagging a {H}ebrew Corpus: the Case of Participles,2008,10,7,5,1,32102,meni adler,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We report on an effort to build a corpus of Modern Hebrew tagged with part-of-speech and morphology. We designed a tagset specific to Hebrew while focusing on four aspects: the tagset should be consistent with common linguistic knowledge; there should be maximal agreement among taggers as to the tags assigned to maintain consistency; the tagset should be useful for machine taggers and learning algorithms; and the tagset should be effective for applications relying on the tags as input features. In this paper, we illustrate these issues by explaining our decision to introduce a tag for beinoni forms in Hebrew. We explain how this tag is defined, and how it helped us improve manual tagging accuracy to a high-level, while improving automatic tagging and helping in the task of syntactic chunking."
W07-0808,Can You Tag the Modal? You Should.,2007,7,5,4,1,46977,yael netzer,Proceedings of the 2007 Workshop on Computational Approaches to {S}emitic Languages: Common Issues and Resources,0,"Computational linguistics methods are typically first developed and tested in English. When applied to other languages, assumptions from English data are often applied to the target language. One of the most common such assumptions is that a standard part-of-speech (POS) tagset can be used across languages with only slight variations. We discuss in this paper a specific issue related to the definition of a POS tagset for Modern Hebrew, as an example to clarify the method through which such variations can be defined. It is widely assumed that Hebrew has no syntactic category of modals. There is, however, an identified class of words which are modal-like in their semantics, and can be characterized through distinct syntactic and morphologic criteria. We have found wide disagreement among traditional dictionaries on the POS tag attributed to such words. We describe three main approaches when deciding how to tag such words in Hebrew. We illustrate the impact of selecting each of these approaches on agreement among human taggers, and on the accuracy of automatic POS taggers induced for each method. We finally recommend the use of a modal tag in Hebrew and provide detailed guidelines for this tag. Our overall conclusion is that tagset definition is a complex task which deserves appropriate methodology."
P07-1029,{SVM} Model Tampering and Anchored Learning: A Case Study in {H}ebrew {NP} Chunking,2007,11,8,2,1,3457,yoav goldberg,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We study the issue of porting a known NLP method to a language with little existing NLP resources, specifically Hebrew SVM-based chunking. We introduce two SVM-based methods xe2x80x90 Model Tampering and Anchored Learning. These allow fine grained analysis of the learned SVM models, which provides guidance to identify errors in the training corpus, distinguish the role and interaction of lexical features and eventually construct a model with xe2x88xbc10% error reduction. The resulting chunker is shown to be robust in the presence of noise in the training corpus, relies on less lexical features than was previously understood and achieves an F-measure performance of 92.2 on automatically PoS-tagged text. The SVM analysis methods also provide general insight on SVM-based chunking."
P06-1084,An Unsupervised Morpheme-Based {HMM} for {H}ebrew Morphological Disambiguation,2006,14,49,2,1,32102,meni adler,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text. When the word is ambiguous (there are several possible analyses for the word), a disambiguation procedure based on the word context must be applied. This paper deals with morphological disambiguation of the Hebrew language, which combines morphemes into a word in both agglutinative and fusional ways. We present an un-supervised stochastic model - the only resource we use is a morphological analyzer-which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language.We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules (which are quite restricted in Hebrew) helps in the disambiguation. We adapt HMM algorithms for learning and searching this text representation, in such a way that segmentation and tagging can be learned in parallel in one step. Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets. Our method is applicable to other languages with affix morphology."
P06-1087,Noun Phrase Chunking in {H}ebrew: Influence of Lexical and Morphological Features,2006,16,11,3,1,3457,yoav goldberg,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We present a method for Noun Phrase chunking in Hebrew. We show that the traditional definition of base-NPs as non-recursive noun phrases does not apply in Hebrew, and propose an alternative definition of Simple NPs. We review syntactic properties of Hebrew related to noun phrases, which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English. As a confirmation, we apply methods known to work well for English to Hebrew data. These methods give low results (F from 76 to 86) in Hebrew. We then discuss our method, which applies SVM induction over lexical and morphological features. Morphological features improve the average precision by ~0.5%, recall by ~1%, and F-measure by ~0.75, resulting in a system with average performance of 93% precision, 93.4% recall and 93.2 F-measure."
N06-2027,Using Semantic Authoring for {B}lissymbols Communication Boards,2006,8,7,2,1,46977,yael netzer,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"Natural language generation (NLG) refers to the process of producing text in a spoken language, starting from an internal knowledge representation structure. Augmentative and Alternative Communication (AAC) deals with the development of devices and tools to enable basic conversation for language-impaired people. We present an applied prototype of an AAC-NLG system generating written output in English and Hebrew from a sequence of Bliss symbols. The system does not translate the symbols sequence, but instead, it dynamically changes the communication board as the choice of symbols proceeds according to the syntactic and semantic content of selected symbols, generating utterances in natural language through a process of semantic authoring."
W05-1602,Interactive Authoring of Logical Forms for Multilingual Generation,2005,24,7,2,0,50773,ofer biller,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"We present an authoring system for logical forms encoded as conceptual graphs (CG). The system belongs to the family of WYSIWYM (What You See Is What You Mean) text generation systems: logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages. The system maintains a model of the discourse context corresponding to the authored documents. The system helps users author documents formulated in the CG format. In a first stage, a domainspecific ontology is acquired by learning from example texts in the domain. The ontology acquisition module builds a typed hierarchy of concepts and relations derived from the WordNet and Verbnet. The user can then edit a specific document, by entering utterances in sequence, and maintaining a representation of the context. While the user enters data, the system performs the standard steps of text generation on the basis of the authored logical forms: reference planning, aggregation, lexical choice and syntactic realization xe2x80x93 in several languages (we have implemented English and Hebrew and are exploring an implementation using the Bliss graphical language). The feedback in natural language is produced in real-time for every single modification performed by the author. We perform a cost-benefit analysis of the application of NLG techniques in the context of authoring cooking recipes in English and Hebrew. By combining existing large-scale knowledge resources (WordNet, Verbnet, the SURGE and HUGG realization grammars) and techniques from modern integrated software development environment (such as the Eclipse IDE), we obtain an efficient tool for the generation of logical forms, in domains where content is not available in the form of databases. xe2x88x97Research supported by the Israel Ministry of Science Knowledge Center for Hebrew Computational Linguistics and by the Frankel Fund"
W00-1428,"Integrating a Large-Scale, Reusable Lexicon with a Natural Language Generator",2000,13,7,3,0,49187,hongyan jing,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"This paper presents the integration of a large-scale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer. The lexicon was combined from multiple existing resources in a semi-automatic process. The integration is a multi-step unification process. This integration allows the reuse of lexical, syntactic, and semantic knowledge encoded in the lexicon in the development of lexical chooser module in a generation system. The lexicon also brings other benefits to a generation system: for example, the ability to generate many lexical and syntactic paraphrases and the ability to avoid non-grammatical output."
P99-1019,Bilingual {H}ebrew-{E}nglish Generation of Possessives and Partitives: Raising the Input Abstraction Level,1999,15,2,2,1,46977,yael netzer,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"We present a contrastive analysis of the syntactic realizations of possessives and partitives in Hebrew and English and conclude by presenting an input specification for complex NPs which is slightly more abstract than the one used in SURGE. We define two main features - possessor and ref-set and discuss how the grammar handles complex syntactic co-occurrence phenomena based on this input. We conclude by evaluating how the resulting input specification language is appropriate for both languages.n n Syntactic realization grammars have traditionally attempted to accept inputs with the highest possible level of abstraction, in order to facilitate the work of the components (sentence planner) preparing the input. Recently, the search for higher abstraction has been, however, challenged (Elhadad and Robin, 1996) (Lavoie and Rambow, 1997) (Busemann and Horacek, 1998). In this paper, we contribute to the issue of selecting the ideal abstraction level in the input to syntactic realization grammar by considering the case of partitives and possessives in a bilingual Hebrew-English generation grammar. In the case of bilingual generation, the ultimate goal is to provide a single input structure, where only the openclass lexical entries are specific to the language. In that case, the minimal abstraction required must cover the different syntactic constraints of the two languages."
P99-1071,Information Fusion in the Context of Multi-Document Summarization,1999,17,308,3,1,847,regina barzilay,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary.
W98-1418,Generation of Noun Compounds in {H}ebrew: Can Syntactic Knowledge Be Fully Encapsulated?,1998,7,4,2,1,46977,yael netzer,Natural Language Generation,0,None
W98-1012,Generating Determiners and Quantifiers in {H}ebrew,1998,5,4,2,1,46977,yael netzer,Computational Approaches to {S}emitic Languages,0,"This paper presents a specific part of HUGG, a generation grammar for Hebrew. This part deals with determiners and quantifiers. Our main goal is to determine which set of features must be present in the input to the generation grammar to control the generation of complex determiners and quantifiers.n n Hebrew determiners are characterized by two properties: (1) definiteness is marked in several places in the NP and it interacts with compound constructs; (2) the order of appearance of determiners and quantifiers within a complex NP is flexible but still restricted.n n HUGG is developed with the goal to design an input specification language for syntactic realization as close as possible for English and Hebrew, to allow easy development of bilingual generation applications. We show in this paper how the original set of features controlling the generation of determiners in SURGE, a large generation grammar for English, has been enriched to account for the specificities of Hebrew determiners and how, in the process, SURGE has been modified and improved.n n We present a set of functional features -- organized around the categories of amount, partitive and identity -- and show how these features determine a variety of syntactic constructs."
W97-0703,Using Lexical Chains for Text Summarization,1997,24,810,2,1,847,regina barzilay,Intelligent Scalable Text Summarization,0,"We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains. We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources: the WordNet thesaurus, a part-of-speech tagger, shallow parser for the identification of nominal groups, and a segmentation algorithm. Summarization proceeds in four steps: the original text is segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted. We present in this paper empirical results on the identification of strong chains and of significant sentences. Preliminary results indicate that quality indicative summaries are produced. Pending problems are identified. Plans to address these short-comings are briefly presented."
J97-2001,Floating Constraints in Lexical Choice,1997,70,77,1,1,659,michael elhadad,Computational Linguistics,0,"Lexical choice is a computationally complex task, requiring a generation system to consider a potentially large number of mappings between concepts and words. Constraints that aid in determining which word is best come from a wide variety of sources, including syntax, semantics, pragmatics, the lexicon, and the underlying domain. Furthermore, in some situations, different constraints come into play early on, while in others, they apply much later. This makes it difficult to determine a systematic ordering in which to apply constraints. In this paper, we present a general approach to lexical choice that can handle multiple, interacting constraints. We focus on the problem of floating constraints, semantic or pragmatic constraints that float, appearing at a variety of different syntactic ranks, often merged with other semantic constraints. This means that multiple content units can be realized by a single surface element, and conversely, that a single content unit can be realized by a variety of surface elements. Our approach uses the Functional Unification Formalism (FUF) to represent a generation lexicon, allowing for declarative and compositional representation of individual constraints."
W96-0501,An Overview of {SURGE}: a Reusable Comprehensive Syntactic Realization Component,1996,3,126,1,1,659,michael elhadad,Eighth International Natural Language Generation Workshop (Posters and Demonstrations),0,None
C92-2096,Generating Coherent Argumentative Paragraphs,1992,20,10,1,1,659,michael elhadad,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We address the problem of generating a coherent paragraph presenting arguments for a conclusion in a text generation system. Existing text planning techniques are not appropriate for this task for two main reasons: they do not explain how arguments can be linked together in a linear presentation order and they do not explain how the rhetorical function of a proposition affects its wording.We present a mechanism to generate argumentative paragraphs where argumentative relations constrain not only the rhetorical structure of the paragraph, but also the surface form of each proposition. In our approach, a text planner relies on a set of specific argumentative relations to extract information from the knowledge base, to map it to scalar and context dependent evaluations and to organize it into chains of arguments. The same information used for planning is also used by the surface realization component to perform lexical choice at all the levels of the clause (connectives, main verb, adverbial adjuncts, adjectives and determiners). The mechanism is implemented in the ADVISOR II system using FUF, an extended functional unification formalism."
P90-1020,Types in Functional Unification Grammars,1990,24,25,1,1,659,michael elhadad,28th Annual Meeting of the Association for Computational Linguistics,1,"Functional Unification Grammars (FUGs) are popular for natural language applications because the formalism uses very few primitives and is uniform and expressive. In our work on text generation, we have found that it also has annoying limitations: it is not suited for the expression of simple, yet very common, taxonomic relations and it does not allow the specification of completeness conditions. We have implemented an extension of traditional functional unification. This extension addresses these limitations while preserving the desirable properties of FUGs. It is based on the notions of typed features and typed constituents. We show the advantages of this extension in the context of a grammar used for text generation."
C90-3018,Generating Connectives,1990,24,50,1,1,659,michael elhadad,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"We present an implemented procedure to select an appropriate connective to link two propositions, which is part of a large text generation system. Each connective is defined as a set of constraints between features of the propositions it connects. Our focus has been to identify pragmatic features that can be produced by a deep generator to provide a simple representation of connectives. Using these features, we can account for a variety of connective usages, and we can distinguish between similar connectives. We describe how a surface generator can produce complex sentences when given these features in input. The selection procedure is implemented as part of a large functional unification grammar."
