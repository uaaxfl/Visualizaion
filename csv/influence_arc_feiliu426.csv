2020.aacl-main.52,W13-2322,0,0.0127586,"The tags are transformed to a “highlight-on” embedding for each token if it is chosen by the content selector, and a “highlight-off ” embedding for each token not chosen. The highlight-on/off embeddings are added to token embeddings in an element-wise manner; both highlight and token embeddings are learned. An illustration is shown in Figure 1. Highlights provide a valuable intermediate representation suitable for shallow abstraction. Our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053"
2020.aacl-main.52,N19-1348,0,0.0283632,"ate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are fusible—there exists cohesive devices that tie the two sentences together into a coherent text—to avoid generating nonsensical outputs (Geva et al., 2019; Lebanoff et al., 2020). Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence. The contributions of this work are summarized as follows. 529 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 529–535 c December 4 - 7, 2020. 2020 Association for Computational Linguistics Sentence Selection Sent Pred Fine-Grained Content Selection NonHi"
2020.aacl-main.52,N18-1150,0,0.0202951,"t is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summ"
2020.aacl-main.52,N19-1355,0,0.0195614,"nces are selected to generate a fusion sentence, it is desirable to identify segments of text from these sentences that are potentially compatible with each other. The coarse-tofine method allows us to examine the intermediate results and compare them with ground-truth. Concretely, we add a classification layer to the final layer representation hL i for each token wi (Eq. (2)). The per-target-word loss is then interpolated with instance prediction (one or two sentences) loss using a coefficient λ. Such a multi-task learning objective has been shown to improve performance on a number of tasks (Guo et al., 2019). phighlight (wi ) = σ(v> hL i ) (2) where v is a vector of weights and σ is the sigmoid function. The model predicts phighlight for each token – whether the token should be included in the output fusion, calculated based on the given token’s representation. Information Fusion Given one or two sentences taken from a document and their fine-grained highlights, we proceed by describing a fusion process that generates a summary sentence from the selected content. Our model employs an encoderdecoder architecture based on pointer-generator 531 networks that has shown strong performance on its own a"
2020.aacl-main.52,P18-1013,0,0.0165201,"ions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhan"
2020.aacl-main.52,P19-1657,0,0.0201954,"2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarizat"
2020.aacl-main.52,A00-2024,0,0.428793,"Missing"
2020.aacl-main.52,2020.coling-main.499,1,0.701122,"from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the sour"
2020.aacl-main.52,D19-5406,0,0.0238845,"Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cas"
2020.aacl-main.52,D19-1051,0,0.0327316,"Missing"
2020.aacl-main.52,D18-1207,0,0.0406238,"Missing"
2020.aacl-main.52,D19-5413,1,0.896174,"ative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053,993 instances; every instance contains one or two candidate sentences. It is a positive instance if a groundtruth summary sentence can be formed by compressing or merging sentences of the instance, negative otherwise. For positive instances, we highlight all lemmatized unigrams appearing in the summary, excluding punctuation. We further add smoothing to the labels by highlighting single words that conSystem R-1 R-2 R-L SumBasic (Vanderwende et al., 2007) LexRank (Erkan"
2020.aacl-main.52,2020.acl-srw.26,1,0.704652,"of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sentences from the source document, then highlights their summary-worthy segments and uses those as a basis for composing a summary sentence. When a pair of sentences are selected, it is important to ensure that they are fusible—there exists cohesive devices that tie the two sentences together into a coherent text—to avoid generating nonsensical outputs (Geva et al., 2019; Lebanoff et al., 2020). Highlighting sentence segments allows us to perform fine-grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence. The contributions of this work are summarized as follows. 529 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 529–535 c December 4 - 7, 2020. 2020 Association for Computational Linguistics Sentence Selection Sent Pred Fine-Grained Content Selection NonHighlight Highlight Highli"
2020.aacl-main.52,P19-1209,1,0.933037,"ative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053,993 instances; every instance contains one or two candidate sentences. It is a positive instance if a groundtruth summary sentence can be formed by compressing or merging sentences of the instance, negative otherwise. For positive instances, we highlight all lemmatized unigrams appearing in the summary, excluding punctuation. We further add smoothing to the labels by highlighting single words that conSystem R-1 R-2 R-L SumBasic (Vanderwende et al., 2007) LexRank (Erkan"
2020.aacl-main.52,D18-1446,1,0.84409,"Missing"
2020.aacl-main.52,2020.acl-main.703,0,0.0462508,"nd systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford"
2020.aacl-main.52,P19-1210,0,0.0631557,"mportant segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture chooses one or two sen"
2020.aacl-main.52,W04-1013,0,0.0391224,"s may be placed on accurate content selection. nect two highlighted phrases and by dehighlighting isolated stopwords. At test time, four highestscored instances are selected per document; their important segments are highlighted by content selector then passed to the fusion step to produce a summary sentence each. The hyperparameter λ for weighing the per-target-word loss is set to 0.2 and highlighting threshold value is 0.15. The model hyperparameters are tuned on the validation split. Summarization Results We show experimental results on the standard test set and evaluated by ROUGE metrics (Lin, 2004) in Table 1. The performance of our cascade approaches, Cascade-Fusion and Cascade-Tag, is comparable to or outranks a number of extractive and abstractive baselines. Particularly, Cascade-Tag does not use a fusion step (§2) and is the output of fine-grained content selection. Cascade-Fusion provides a direct comparison against BERT-Abs (Lebanoff et al., 2019b) that uses sentence selection and fusion but lacks a fine-grained content selector. Our results suggest that a coarse-to-fine content selection strategy remains necessary to guide the fusion model to produce informative sentences. We obs"
2020.aacl-main.52,N15-1114,1,0.83228,"d to a “highlight-on” embedding for each token if it is chosen by the content selector, and a “highlight-off ” embedding for each token not chosen. The highlight-on/off embeddings are added to token embeddings in an element-wise manner; both highlight and token embeddings are learned. An illustration is shown in Figure 1. Highlights provide a valuable intermediate representation suitable for shallow abstraction. Our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic/semantic graphs (Filippova and Strube, 2008; Banarescu et al., 2013; Liu et al., 2015). It is more straightforward to incorporate highlights into an encoder-decoder fusion model, and obtaining highlights through sequence tagging can be potentially adapted to new domains. 3 Experimental Results Data and Annotation To enable direct comparison with end-to-end systems, we conduct experiments on the widely used CNN/DM dataset (See et al., 2017) to report results of our cascade approach. We use the procedure described in Lebanoff et al. (2019b) to create training instances for the sentence selector and fine-grained content selector. Our training data contains 1,053,993 instances; eve"
2020.aacl-main.52,P19-1500,0,0.0403015,"icit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and"
2020.aacl-main.52,W13-2117,0,0.0180037,"ly the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an abstract is created one sentence at a time through a cascaded pipeline. Our pipeline architecture choos"
2020.aacl-main.52,P17-1099,0,0.633251,"nto a coherent text is comparable to or outranks that of end-to-end systems, whereas a pipeline architecture allows for flexible content selection. We finally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research. 1 We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module, rather than associating it with text generation. Existing neural abstractive systems can perform content selection implicitly using end-to-end models (See et al., 2017; Celikyilmaz et al., 2018; Raffel et al., 2019; Lewis et al., 2020), or more explicitly, with an external module to select important sentences or words to aid generation (Tan et al., 2017; Gehrmann et al., 2018; Chen and Bansal, 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a"
2020.aacl-main.52,P17-1108,0,0.0244664,"Missing"
2020.aacl-main.52,2020.acl-main.458,0,0.0330706,"2018; Lebanoff et al., 2018, 2019b; Liu and Lapata, 2019). However, content selection concerns not only the selection of important segments from a document, but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary. Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufficient to meet the requirement of end-to-end neural abstractive summarization. Examples range from summarizing radiology reports (Jing et al., 2019; Zhang et al., 2020) to congressional bills (Kornilova and Eidelman, 2019) and meeting conversations (Mehdad et al., 2013; Li et al., 2019; Koay et al., 2020). The lack of annotated resources suggests that end-toend systems may not be a “one-size-fits-all” solution to neural text summarization. There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general-purpose neural text generators In this paper, we aim to investigate the feasibility of a cascade approach to neural text summarization. We explore a constrained summarization task, where an ab"
2020.acl-srw.26,J05-3002,0,0.148665,"improper use of points of correspondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best kno"
2020.acl-srw.26,P99-1071,0,0.883194,"://github.com/ucfnlp/ points-of-correspondence Table 1: Unfaithful summary sentences generated by neural abstractive summarizers, in-house and PG (See et al., 2017). They attempt to merge two sentences into one sentence with improper use of points of correspondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing ["
2020.acl-srw.26,N18-1150,0,0.0704995,"Missing"
2020.acl-srw.26,P18-1063,0,0.0365296,"it was actually Richards. Had the models explicitly recognized the points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of"
2020.acl-srw.26,2020.acl-main.454,0,0.0297295,"hanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences. We envision sentence correspondence to be related to text cohesion and coherence, which help establish correspondences between two pieces of text. Halliday and Hasan (1976) describe text cohesion as cohesive devices that tie two textual elements together. They identify five categories of cohesion: 195 human annotations of points of correspondence between sentences. The dataset fills a notabl"
2020.acl-srw.26,W11-1607,0,0.91516,"public memory in the West these days, but it is being used by the Islamic State in Iraq and Syria (ISIS) to support its sectarian narrative. In its propaganda, ISIS has been using Abu Ghraib and other cases of Western abuse to legitimize its current actions [...] [Summary] In its propaganda, ISIS is being used by the Islamic State in Iraq and Syria. Introduction Stitching portions of text together into a sentence is a crucial first step in abstractive summarization. It involves choosing which sentences to fuse, what content from each of them to retain and how best to present that information (Elsner and Santhanam, 2011). A major challenge in fusing sentences is to establish correspondence between sentences. If there exists no correspondence, it would be difficult, if not impossible, to fuse sentences. In Table 1, we present example source and fusion sentences, where the summarizer attempts to merge two sentences into a summary sentence with improper use of points of correspondence. In this paper, we seek to uncover hidden correspondences between sen1 https://github.com/ucfnlp/ points-of-correspondence Table 1: Unfaithful summary sentences generated by neural abstractive summarizers, in-house and PG (See et a"
2020.acl-srw.26,D08-1019,0,0.808411,"rrespondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best known for the 1988 party anthem"
2020.acl-srw.26,D18-1443,0,0.0637824,"Missing"
2020.acl-srw.26,P19-1209,1,0.865209,"able 2. If multiple PoC co-exist in an example, an annotator is expected to label them all; a separate PoC type will be assigned to each PoC occurrence. We are particularly interested in annotating inter-sentence PoC. If entity mentions (“John” and “he”) are found in the same sentence, we do not explicitly label them but assume such intra-sentence referencing can be captured by an existing coreference resolver. Instances of source sentences and summary sentences are obtained from the test and validation splits of the CNN/DailyMail corpus (See et al., 2017) following the procedure described by Lebanoff et al. (2019a). We take a human summary sentence as an anchor point to find two document sentences that are most similar to it based on ROUGE. It becomes an instance containing a pair of source sentences and their summary. The method allows us to identify a large quantity of candidate fusion instances. Annotations are performed in two stages. Stage one removes all spurious pairs that are generated by the heuristic, i.e. a summary sentence that is not a valid fusion of the corresponding two source sentences. Human annotators are given a pair of sentences and a summary sentence and are asked Common-Noun Rep"
2020.acl-srw.26,D18-1446,1,0.878761,"xplicitly recognized the points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondenc"
2020.acl-srw.26,W04-1013,0,0.141756,"Missing"
2020.acl-srw.26,P19-1500,0,0.0479364,"e points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goe"
2020.acl-srw.26,N19-1348,0,0.0567723,"een sentences. The dataset fills a notable gap of coreference resolution and summarization research. Our findings shed light on the importance of modeling points of correspondence, suggesting important future directions for sentence fusion. (McKeown et al., 2010) [S1] Palin actually turned against the bridge project only after it became a national symbol of wasteful spending. [S2] Ms. Palin supported the bridge project while running for governor, and abandoned it after it became a national scandal. [Fusion] Palin turned against the bridge project after it became a national scandal. DiscoFuse (Geva et al., 2019) [S1] Melvyn Douglas originally was signed to play Sam Bailey. [S2] The role ultimately went to Walter Pidgeon. [Fusion] Melvyn Douglas originally was signed to play Sam Bailey, but the role ultimately went to Walter Pidgeon. Acknowledgments We are grateful to the anonymous reviewers for their helpful comments and suggestions. This research was supported in part by the National Science Foundation grant IIS-1909603. Points of Correspondence Dataset (Our Work) [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist"
2020.acl-srw.26,P19-1330,0,0.0358079,"Missing"
2020.acl-srw.26,P14-5010,0,0.00319992,"ask of identifying points of correspondence. Thus, a natural question we ask is how well state-of-theart coreference resolvers can be adapted to this task. If coreference resolvers can perform reasonably well on PoC identification, then these resolvers can be used to extract PoC annotations to potentially enhance sentence fusion. If they perform poorly, coreference performance results can indicate areas of improvement for future work on detecting points of correspondence. In this paper, we compare three coreference resolvers on our dataset, provided by open-source libraries: Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and AllenNLP (Gardner et al., 2017). We base our evaluation on the standard metric used for coreference resolution, B-CUBED algorithm (Bagga and Baldwin, 1998), with some modifications. Each resolver is run on an input pair of sentences to obtain multiple clusters, each representing an entity (e.g., Johnny Kemp) containing multiple mentions (e.g., Johnny Kemp; he; the singer) of that entity. More than one cluster can be detected by the coreference resolver, as additional entities may exist in the given sentence pair (e.g., Johnny Kemp and the police). Simi"
2020.acl-srw.26,W05-1612,0,0.632817,"it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requi"
2020.acl-srw.26,P18-1013,0,0.0336924,"used to leave when it was actually Richards. Had the models explicitly recognized the points of correspondence in the sentences—that the journalist is a separate entity from Robert Downey Jr. and that Richards is separate from police officer—then a more accurate summary could have been generated. Related Work Uncovering hidden correspondences between sentences is essential for producing proper summary sentences. A number of recent efforts select important words and sentences from a given document, then let the summarizer attend to selected content to generate a summary (Gehrmann et al., 2018; Hsu et al., 2018; Chen and Bansal, 2018; Putra et al., 2018; Lebanoff et al., 2018; Liu and Lapata, 2019). These systems are largely agnostic to sentence correspondences, which can have two undesirable consequences. If only a single sentence is selected, it can be impossible for the summarizer to produce a fusion sentence from it. Moreover, if non-fusible textual units are selected, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to ga"
2020.acl-srw.26,N10-1044,0,0.346869,"ed, the summarizer is forced to fuse them into a summary sentence, yielding output summaries that often fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zha"
2020.acl-srw.26,D19-1051,0,0.0583459,"Missing"
2020.acl-srw.26,D19-5413,1,0.825439,"able 2. If multiple PoC co-exist in an example, an annotator is expected to label them all; a separate PoC type will be assigned to each PoC occurrence. We are particularly interested in annotating inter-sentence PoC. If entity mentions (“John” and “he”) are found in the same sentence, we do not explicitly label them but assume such intra-sentence referencing can be captured by an existing coreference resolver. Instances of source sentences and summary sentences are obtained from the test and validation splits of the CNN/DailyMail corpus (See et al., 2017) following the procedure described by Lebanoff et al. (2019a). We take a human summary sentence as an anchor point to find two document sentences that are most similar to it based on ROUGE. It becomes an instance containing a pair of source sentences and their summary. The method allows us to identify a large quantity of candidate fusion instances. Annotations are performed in two stages. Stage one removes all spurious pairs that are generated by the heuristic, i.e. a summary sentence that is not a valid fusion of the corresponding two source sentences. Human annotators are given a pair of sentences and a summary sentence and are asked Common-Noun Rep"
2020.acl-srw.26,N19-1236,0,0.0283394,"an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences"
2020.acl-srw.26,D18-1206,0,0.0634743,"Missing"
2020.acl-srw.26,C18-1102,0,0.0172749,"ften fail to keep the original meaning intact. Therefore, in this paper we had investigated the correspondences between sentences to gain an understanding of sentence fusion. Establishing correspondence between sentences goes beyond finding common words. Humans can fuse sentences sharing few or no common words if they can find other types of correspondence. Fusing such disparate sentences poses a serious challenge for automated fusion systems (Marsi and Krahmer, 2005; Filippova and Strube, 2008; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotat"
2020.acl-srw.26,P17-1099,0,0.479974,"m, 2011). A major challenge in fusing sentences is to establish correspondence between sentences. If there exists no correspondence, it would be difficult, if not impossible, to fuse sentences. In Table 1, we present example source and fusion sentences, where the summarizer attempts to merge two sentences into a summary sentence with improper use of points of correspondence. In this paper, we seek to uncover hidden correspondences between sen1 https://github.com/ucfnlp/ points-of-correspondence Table 1: Unfaithful summary sentences generated by neural abstractive summarizers, in-house and PG (See et al., 2017). They attempt to merge two sentences into one sentence with improper use of points of correspondence between sentences, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming"
2020.acl-srw.26,W13-3508,0,0.187253,"es, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best known for the 1988 party anthem “Just Got Paid,” died this"
2020.acl-srw.26,I13-1198,0,0.554407,"es, yielding nonsensical output. Summaries are manually re-cased for readability. tences, which has a great potential for improving content selection and deep sentence fusion. Sentence fusion (or multi-sentence compression) plays a prominent role in automated summarization and its importance has long been recognized (Barzilay et al., 1999). Early attempts to fuse sentences build a dependency graph from sentences, then decode a tree from the graph using integer linear programming, finally linearize the tree to generate a summary sentence (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Thadani and McKeown, 2013a). Despite valuable insights gained from 191 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 191–198 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics PoC Type Source Sentences Summary Sentence Pronominal Referencing [S1] The bodies showed signs of torture. [S2] They were left on the side of a highway in Chilpancingo, about an hour north of the tourist resort of Acapulco in the state of Guerrero. [S1] Bahamian R&B singer Johnny Kemp , best known for the 1988 party anthem “Just Got Paid,” died this"
2020.acl-srw.26,2020.acl-main.450,0,0.0398594,"and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences. We envision sentence correspondence to be related to text cohesion and coherence, which help establish correspondences between two pieces of text. Halliday and Hasan (1976) describe text cohesion as cohesive devices that tie two textual elements together. They identify five categories of cohesion: 195 human annotations of points of correspondence between sentences. The dataset fills a notable gap of coreference"
2020.acl-srw.26,D19-1053,1,0.846561,"; McKeown et al., 2010; Elsner and Santhanam, 2011; Thadani and McKeown, 2013b; Mehdad et al., 2013; Nayeem et al., 2018). These systems rely on common words to derive a connected graph from input sentences or subject-verb-object triples (Moryossef et al., 2019). When there are no common words in sentences, systems tend to break apart. There has been a lack of annotated datasets and guidelines for sentence fusion. Few studies have investigated the types of correspondence between sentences such as entity and event coreference. Evaluating sentence fusion systems requires not only novel metrics (Zhao et al., 2019; Zhang et al., 2020; Durmus et al., 2020; Wang et al., 2020) but also high-quality ground-truth annotations. It is therefore necessary to conduct a first study to look into cues humans use to establish correspondence between disparate sentences. We envision sentence correspondence to be related to text cohesion and coherence, which help establish correspondences between two pieces of text. Halliday and Hasan (1976) describe text cohesion as cohesive devices that tie two textual elements together. They identify five categories of cohesion: 195 human annotations of points of correspondence betw"
2020.bionlp-1.11,W19-1909,0,0.0696569,"Missing"
2020.bionlp-1.11,S17-2001,0,0.0674623,") have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) (with 5.7k and 3.6k training instances, resp.). Compared to general-domain STS benchmarks, labeled clinical STS data is more scarce, which tends to cause overfitting during fine-tuning. Moreover, further model scaling is a challenge due to GPU/TPU memory limitations and longer training time (Lan et al., 2019). This motivates us to search for model configurations which strike a balance between model flexibility and overfitting. In this paper, we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the i"
2020.bionlp-1.11,W05-1203,0,0.151145,"ely different fine-tuning and pooling strategies. We observe that the impact of domain-specific fine-tuning on clinical STS is much less than that in the general domain, likely due to the concept richness of the domain. Based on this, we propose two data augmentation techniques. Experimental results on N2C2-STS1 demonstrate substantial improvements, validating the utility of the proposed methods. 1 Introduction Semantic Textual Similarity (STS) is a language understanding task, involving assessing the degree of semantic equivalence between two pieces of text based on a graded numerical score (Corley and Mihalcea, 2005). It has application in tasks such as information retrieval (Hliaoutakis et al., 2006), question answering (Hoogeveen et al., 2018), and summarization (AL-Khassawneh et al., 2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regr"
2020.bionlp-1.11,N19-1423,0,0.214256,"referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (C"
2020.bionlp-1.11,I05-5002,0,0.0220521,"from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), performing below a multi-task biLSTM model. Similarly, while the large-scale pre-training of BERT has led to impressive improvements on a range of tasks, only very modest improvements have been achieved on STS tasks such as STSB (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) (with 5.7k and 3.6k training instances, resp.). Compared to general-domain STS benchmarks, labeled clinical STS data is more scarce, which tends to cause overfitting during fine-tuning. Moreover, further model scaling is a challenge due to GPU/TPU memory limitations and longer training time (Lan et al., 2019). This motivates us to search for model configurations which strike a balance between model flexibility and overfitting. In this paper, we study the impact of a number of model design choices. First, following Reimers and Gurevych (2019), we study the impact of various pooling methods on"
2020.bionlp-1.11,D15-1181,0,0.0230962,"ion retrieval (Hliaoutakis et al., 2006), question answering (Hoogeveen et al., 2018), and summarization (AL-Khassawneh et al., 2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across f"
2020.bionlp-1.11,D17-1082,0,0.0158979,"ng the capability and generalizability of LMs, while adapting a single fully-connected layer to capture task features. Sentence-BERT (Reimers and Gurevych, 2019) makes use of task-specific structures to optimize STS, concentrating on computational and time efficiency, and is evaluated on relatively larger datasets in the general domain. For evaluating the impact of number of layers transferred to the supervised target task from the pre-trained language model, GPT-2 has been analyzed on two datasets. However, they are both large: MultiNLI (Williams et al., 2018) with &gt;390k instances, and RACE (Lai et al., 2017) with &gt;97k instances. These tasks also both involve reasoning-related classification, as opposed to the nuanced regression task of STS. 2.2 Data Augmentation Synonym replacement is one of the most commonly used data augmentation methods to simulate linguistic diversity, but it introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to affected role and discharge to home to spark to home"
2020.bionlp-1.11,2021.ccl-1.108,0,0.124035,"Missing"
2020.bionlp-1.11,D19-1410,0,0.243996,"2016). In this paper, we focus on STS in the clinical domain, in the context of a recent task within the framework of N2C2 (the National NLP Clinical Challenges)1 , which makes use of the extended MedSTS data set (Wang et al., 2018), referring to N2C2-STS, with limited annotated sentences pairs (1.6K) that are rich in domain terms. Neural STS models typically consist of encoders to generate text representations, and a regression layer to measure the similarity score (He et al., 2015; Mueller and Thyagarajan, 2016; He and Lin, 1 https://portal.dbmi.hms.harvard.edu/ projects/n2c2-2019-t1/ 2016; Reimers and Gurevych, 2019). These architectures require a large amount of training data, an unrealistic requirement in low resource settings. Recently, pre-trained language models (LMs) such as GPT-2 (Radford et al., 2018) and BERT (Devlin et al., 2019) have been shown to benefit from pre-training over large corpora followed by fine tuning over specific tasks. However, for small-scale datasets, only limited finetuning can be done. For example, GPT-2 achieved strong results across four large natural language inference (NLI) datasets, but was less successful over the small-scale RTE corpus (Bentivogli et al., 2009), perf"
2020.bionlp-1.11,P16-1009,0,0.0325008,"n. Random insertion, deletion, and swapping of words have been demonstrated to be effective on five text classification tasks (Wei and Zou, 2019). But those experiments targeted topic prediction, in contrast to semantic reasoning such as STS and MultiNLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improvements for tasks demanding for adequate semantic understanding, such as low-resource machine translation (Xia et al., 2019) and question answering (Yu et al., 2019). This motivates our application of BT on low-resource clinical STS, to bridge linguistic variation between two sentences. This work represents the first exploration of applying BT for STS. 3 STS Model Configurations In this section, we study the impact of a number of model design choices on BERT for STS, using"
2020.bionlp-1.11,N16-1108,0,0.0388325,"Missing"
2020.bionlp-1.11,D19-1670,0,0.122152,"introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to affected role and discharge to home to spark to home). By contrast, replacing a complete mention of the concept can increase error propagation due to the prerequisite concept extraction and normalization. Random insertion, deletion, and swapping of words have been demonstrated to be effective on five text classification tasks (Wei and Zou, 2019). But those experiments targeted topic prediction, in contrast to semantic reasoning such as STS and MultiNLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improv"
2020.bionlp-1.11,N18-1101,0,0.0136766,"ed at improving downstream tasks indirectly by optimizing the capability and generalizability of LMs, while adapting a single fully-connected layer to capture task features. Sentence-BERT (Reimers and Gurevych, 2019) makes use of task-specific structures to optimize STS, concentrating on computational and time efficiency, and is evaluated on relatively larger datasets in the general domain. For evaluating the impact of number of layers transferred to the supervised target task from the pre-trained language model, GPT-2 has been analyzed on two datasets. However, they are both large: MultiNLI (Williams et al., 2018) with &gt;390k instances, and RACE (Lai et al., 2017) with &gt;97k instances. These tasks also both involve reasoning-related classification, as opposed to the nuanced regression task of STS. 2.2 Data Augmentation Synonym replacement is one of the most commonly used data augmentation methods to simulate linguistic diversity, but it introduces ambiguity if accurate context-dependent disambiguation is not performed. Moreover, random selection and replacement of a single word used in general texts is not plausible for term-rich clinical text, resulting in too much semantic divergence (e.g patient to af"
2020.bionlp-1.11,P19-1579,0,0.0156264,"NLI. Intuitively, they do not change the overall topic of a text, but can skew the meaning of a sentence, undermining the STS task. Swapping an entire semantic segment may mitigate the risk of introducing label noise to the STS task. Compared to semantic and syntactic distortion potentially caused by aforementioned methods, back translation (BT) (Sennrich et al., 2016) — translating to a target language then back to the original language — presents fluent augmented data and reliable improvements for tasks demanding for adequate semantic understanding, such as low-resource machine translation (Xia et al., 2019) and question answering (Yu et al., 2019). This motivates our application of BT on low-resource clinical STS, to bridge linguistic variation between two sentences. This work represents the first exploration of applying BT for STS. 3 STS Model Configurations In this section, we study the impact of a number of model design choices on BERT for STS, using a 12-layer base model initialized with pretrained weights. 3.1 Hierarchical Convolution (HConv) The resource-poor and concept-rich nature of clinical STS makes it difficult to train a large model endto-end on sentence pairs. To address this, most"
2020.coling-main.499,P06-4013,0,0.0804588,"pus; they are known as jargon terms. We then analyze the performance of a meeting summarization system with and without jargon terms. Our findings reveal that domain terminology can have a substantial impact on summarization performance. We publicly release all domain terminology to advance research in meeting summarization.1 1 Introduction A vast number of meetings are being held and recorded everyday, far more than can ever be comprehended. With this explosion of meetings comes a pressing need to develop summarization techniques to assist in browsing meeting archives (Carletta et al., 2006; Ailomaa et al., 2006). A meeting summarization system takes a meeting recording and its transcript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neura"
2020.coling-main.499,P18-1063,0,0.0303582,"nd produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain te"
2020.coling-main.499,N19-1423,0,0.00940168,"Missing"
2020.coling-main.499,W06-1643,0,0.0935746,"004) and BERTScore (Zhang et al., 2020) for summaries.4 We find that across all lengths and evaluation metrics, summarizing with jargon can lead to a performance boost for meeting summarization. While this work has primarily experimented with the ICSI corpus, the results are sufficiently substantial that we expect them to hold over similar meeting corpora. 5 Related Work Generating meeting summaries is a challenging problem with a great application potential. A significant number of techniques have been attempted in the past, including extraction of utterances and keyphrases from transcripts (Galley, 2006; Murray and Carenini, 2008; Liu et al., 2009; Gillick et al., 2009) and taking advantage of prosodic and speaker-related features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012). As spoken utterances are verbose with low information density, some methods further compress and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest i"
2020.coling-main.499,D19-5409,0,0.014439,"and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest in summarizing meeting transcripts (Shang et al., 2018; Zhu et al., 2020; Tardy et al., 2020) and other types of online and transcribed conversations (Goo and Chen, 2018; 4 The hash code for BERTScore is xlnet-base-cased_L12_no-idf_version=0.3.4(hug_trans=2.5.1)-rescaled 5692 Yuan and Yu, 2020; Gliwa et al., 2019). In particular, Tardy et al. (2020) create a corpus containing 22 public meetings including their automatic transcriptions from audio recordings and meeting reports written by a professional. Li et al. (2019) develop a multi-modal hierarchical attention mechanism for abstractive summarization, where attention is applied to topics, utterances and words to narrow the focus to salient content; their experiments were performed the AMI corpus, thus results are not directly comparable. Our work excludes prosodic and speaker-related features to focus solely on domain terminology. It provides a new b"
2020.coling-main.499,D19-1051,0,0.0585688,"t progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization performance, especially in the context of neural extractive summarization. Jargon is the specialized terminology associated with a particular domain (Meyers et al., 2014). It is employed in a communicative context and may not be well understood outside that context. Because meetings are usually held among professionals, jargon is ubiquitous in meeting discussions. In Table 1, we provide an example of jargon terms identified by human experts. Without a thorough study of technical jargon in the meeting domain, i"
2020.coling-main.499,2020.acl-main.460,0,0.0149878,"ssion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization performance, especially in the context of neural extractive summari"
2020.coling-main.499,D18-1446,1,0.841509,"which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization perfo"
2020.coling-main.499,2020.acl-main.703,0,0.0683034,"Missing"
2020.coling-main.499,P19-1210,0,0.229871,"research in meeting summarization.1 1 Introduction A vast number of meetings are being held and recorded everyday, far more than can ever be comprehended. With this explosion of meetings comes a pressing need to develop summarization techniques to assist in browsing meeting archives (Carletta et al., 2006; Ailomaa et al., 2006). A meeting summarization system takes a meeting recording and its transcript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent ye"
2020.coling-main.499,W04-1013,0,0.0444616,"Missing"
2020.coling-main.499,P19-1500,0,0.0233753,"t of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization performance, especially in the context of neur"
2020.coling-main.499,P09-2066,1,0.862823,"lease all domain terminology to advance research in meeting summarization.1 1 Introduction A vast number of meetings are being held and recorded everyday, far more than can ever be comprehended. With this explosion of meetings comes a pressing need to develop summarization techniques to assist in browsing meeting archives (Carletta et al., 2006; Ailomaa et al., 2006). A meeting summarization system takes a meeting recording and its transcript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on"
2020.coling-main.499,N09-1070,1,0.839658,"or summaries.4 We find that across all lengths and evaluation metrics, summarizing with jargon can lead to a performance boost for meeting summarization. While this work has primarily experimented with the ICSI corpus, the results are sufficiently substantial that we expect them to hold over similar meeting corpora. 5 Related Work Generating meeting summaries is a challenging problem with a great application potential. A significant number of techniques have been attempted in the past, including extraction of utterances and keyphrases from transcripts (Galley, 2006; Murray and Carenini, 2008; Liu et al., 2009; Gillick et al., 2009) and taking advantage of prosodic and speaker-related features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012). As spoken utterances are verbose with low information density, some methods further compress and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest in summarizing meeting transcripts (Shang et a"
2020.coling-main.499,W13-2117,0,0.512452,"rk Generating meeting summaries is a challenging problem with a great application potential. A significant number of techniques have been attempted in the past, including extraction of utterances and keyphrases from transcripts (Galley, 2006; Murray and Carenini, 2008; Liu et al., 2009; Gillick et al., 2009) and taking advantage of prosodic and speaker-related features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012). As spoken utterances are verbose with low information density, some methods further compress and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest in summarizing meeting transcripts (Shang et al., 2018; Zhu et al., 2020; Tardy et al., 2020) and other types of online and transcribed conversations (Goo and Chen, 2018; 4 The hash code for BERTScore is xlnet-base-cased_L12_no-idf_version=0.3.4(hug_trans=2.5.1)-rescaled 5692 Yuan and Yu, 2020; Gliwa et al., 2019). In particular, Tardy et al. (2020) create a corpus containing 2"
2020.coling-main.499,W14-6002,0,0.0262671,"a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization performance, especially in the context of neural extractive summarization. Jargon is the specialized terminology associated with a particular domain (Meyers et al., 2014). It is employed in a communicative context and may not be well understood outside that context. Because meetings are usually held among professionals, jargon is ubiquitous in meeting discussions. In Table 1, we provide an example of jargon terms identified by human experts. Without a thorough study of technical jargon in the meeting domain, it is unclear how best to optimize a meeting summarizer to incorporate domain knowledge. We present an assessment of the meeting summarization performance by comparing models trained with and without jargon. A collection of jargon terms are meticulously co"
2020.coling-main.499,D08-1081,0,0.33188,"performance. We publicly release all domain terminology to advance research in meeting summarization.1 1 Introduction A vast number of meetings are being held and recorded everyday, far more than can ever be comprehended. With this explosion of meetings comes a pressing need to develop summarization techniques to assist in browsing meeting archives (Carletta et al., 2006; Ailomaa et al., 2006). A meeting summarization system takes a meeting recording and its transcript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most pr"
2020.coling-main.499,D18-1206,0,0.0320172,"ext summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meet"
2020.coling-main.499,P17-1099,0,0.0792062,"ording and its transcript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In"
2020.coling-main.499,P16-1162,0,0.0139116,"Missing"
2020.coling-main.499,P18-1062,0,0.391255,"rminology to advance research in meeting summarization.1 1 Introduction A vast number of meetings are being held and recorded everyday, far more than can ever be comprehended. With this explosion of meetings comes a pressing need to develop summarization techniques to assist in browsing meeting archives (Carletta et al., 2006; Ailomaa et al., 2006). A meeting summarization system takes a meeting recording and its transcript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In c"
2020.coling-main.499,P17-1108,0,0.0286407,"nscript as input and produces a concise text summary as output, which preserves the most important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we in"
2020.coling-main.499,2020.lrec-1.829,0,0.172362,"f meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization performance, especially in the context of neural extractive summarization. Jargon is the specialized terminology associated with a particular domain (Meyers et al., 2014). It is employed in a communicative context and"
2020.coling-main.499,P13-1137,0,0.345724,"g corpora. 5 Related Work Generating meeting summaries is a challenging problem with a great application potential. A significant number of techniques have been attempted in the past, including extraction of utterances and keyphrases from transcripts (Galley, 2006; Murray and Carenini, 2008; Liu et al., 2009; Gillick et al., 2009) and taking advantage of prosodic and speaker-related features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012). As spoken utterances are verbose with low information density, some methods further compress and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest in summarizing meeting transcripts (Shang et al., 2018; Zhu et al., 2020; Tardy et al., 2020) and other types of online and transcribed conversations (Goo and Chen, 2018; 4 The hash code for BERTScore is xlnet-base-cased_L12_no-idf_version=0.3.4(hug_trans=2.5.1)-rescaled 5692 Yuan and Yu, 2020; Gliwa et al., 2019). In particular, Tardy et al. (2020) create"
2020.coling-main.499,D19-1389,0,0.0140103,"st important content of the meeting discussion (Murray and Carenini, 2008; Liu and Liu, 2009; Shang et al., 2018; Li et al., 2019). The techniques hold great potential to make large archives of meetings substantially more efficient to browse, search and facilitate information sharing. We envision an automated summarizer that is capable of generating meeting minutes by identifying salient utterances from transcribed meeting recordings. Neural text summarization has seen significant progress (See et al., 2017; Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Lebanoff et al., 2018; West et al., 2019; Liu and Lapata, 2019; Laban et al., 2020), but most prior work focused on written texts. In contrast, recent years have seen a growing interest in summarizing spoken texts (Tardy et al., 2020). Particularly, the characteristics of meetings, domain terminology and limited annotated data pose novel challenges to neural summarization models. We favor extractive over abstractive models as the latter are prone to hallucinate content that is unfaithful to the input (Kryscinski et al., 2019). In this paper, we investigate how domain terminology impacts meeting summarization performance, especially"
2020.coling-main.499,N10-1006,0,0.0891567,"Missing"
2020.coling-main.499,P09-1062,0,0.343859,"eeting summarization. While this work has primarily experimented with the ICSI corpus, the results are sufficiently substantial that we expect them to hold over similar meeting corpora. 5 Related Work Generating meeting summaries is a challenging problem with a great application potential. A significant number of techniques have been attempted in the past, including extraction of utterances and keyphrases from transcripts (Galley, 2006; Murray and Carenini, 2008; Liu et al., 2009; Gillick et al., 2009) and taking advantage of prosodic and speaker-related features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012). As spoken utterances are verbose with low information density, some methods further compress and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest in summarizing meeting transcripts (Shang et al., 2018; Zhu et al., 2020; Tardy et al., 2020) and other types of online and transcribed conversations (Goo and Chen, 2018; 4 The h"
2020.coling-main.499,2020.findings-emnlp.19,0,0.408561,"et al., 2009) and taking advantage of prosodic and speaker-related features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012). As spoken utterances are verbose with low information density, some methods further compress and merge utterances (Liu and Liu, 2013; Wang and Cardie, 2013; Mehdad et al., 2013). Despite these valuable contributions, a closer investigation remains necessary to develop an understanding of how domain terminology affects meeting summarization performance. Recent years have seen a renewed interest in summarizing meeting transcripts (Shang et al., 2018; Zhu et al., 2020; Tardy et al., 2020) and other types of online and transcribed conversations (Goo and Chen, 2018; 4 The hash code for BERTScore is xlnet-base-cased_L12_no-idf_version=0.3.4(hug_trans=2.5.1)-rescaled 5692 Yuan and Yu, 2020; Gliwa et al., 2019). In particular, Tardy et al. (2020) create a corpus containing 22 public meetings including their automatic transcriptions from audio recordings and meeting reports written by a professional. Li et al. (2019) develop a multi-modal hierarchical attention mechanism for abstractive summarization, where attention is applied to topics, utterances and words to"
2020.emnlp-main.338,P99-1071,0,0.588256,"Missing"
2020.emnlp-main.338,P18-1063,0,0.042947,"Missing"
2020.emnlp-main.338,W19-4828,0,0.126475,"ake effective use of these expressions to establish correspondence between sentences, often leading to ungrammatical and nonsensical outputs. 2.1 Transformer with Linking It is advantageous for a Transformer model to make use of PoC information for sentence fusion. While Transformer-based pretrained models have had considerable success (Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2020), they primarily feature pairwise relationships between tokens, but not PoC mentions, which are are text chunks of varying size. Only to a limited extent do these models embed knowledge of coreference (Clark et al., 2019), and there is a growing need for incorporating PoC linkages explicitly in a Transformer model to enhance its ability to perform sentence fusion. We propose to enrich Transformer’s source sequence with markups that indicate PoC linkages. Here PoC information is assumed to be available for any fusion instance (details in §3). We introduce special tokens ([Sk ] and [Ek ]) to mark the start and 1 end of each PoC mention; all mentions pertaining to the k-th PoC share the same start/end tokens. An example is illustrated in Figure 1, where Allan Donald and The 48-year-old former Test paceman are enr"
2020.emnlp-main.338,W04-1016,0,0.229077,"Missing"
2020.emnlp-main.338,N19-1423,0,0.0292351,"h Africa bowling coach vs. part of the coaching team are two PoCs. The use of alternative expressions for conveying the same meanings is standard practice in writing, as it increases lexical variety and reduces redundancy. However, existing summarizers cannot make effective use of these expressions to establish correspondence between sentences, often leading to ungrammatical and nonsensical outputs. 2.1 Transformer with Linking It is advantageous for a Transformer model to make use of PoC information for sentence fusion. While Transformer-based pretrained models have had considerable success (Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2020), they primarily feature pairwise relationships between tokens, but not PoC mentions, which are are text chunks of varying size. Only to a limited extent do these models embed knowledge of coreference (Clark et al., 2019), and there is a growing need for incorporating PoC linkages explicitly in a Transformer model to enhance its ability to perform sentence fusion. We propose to enrich Transformer’s source sequence with markups that indicate PoC linkages. Here PoC information is assumed to be available for any fusion instance (details in §3). We introduce"
2020.emnlp-main.338,2020.acl-main.454,0,0.0104413,"en compared to fusions that reuse the source text. Our results call for a System Pointer-Generator Transformer Trans-S HARE R EPR Reference Extractiveness Truthful. 1-gram 2-gram 3-gram 63.6 71.7 70.9 67.2 97.5 91.9 92.0 72.0 83.1 68.6 70.1 34.9 72.8 54.2 56.4 20.9 Table 3: Fusion sentences are evaluated by their level of truthfulness and extractivenss. Our system fusions attain a high level of truthfulness with moderate extractivenss. reexamination of sentence fusion using better evaluation metrics including semantics and questionanswering-based metrics (Zhao et al., 2019; Wang et al., 2020; Durmus et al., 2020). 4 Conclusion We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard setting of document summarization. Perfo"
2020.emnlp-main.338,W11-1607,0,0.44119,"Missing"
2020.emnlp-main.338,P19-1213,0,0.0319973,"Missing"
2020.emnlp-main.338,D08-1019,0,0.305742,"Missing"
2020.emnlp-main.338,D18-1443,0,0.0752772,"Missing"
2020.emnlp-main.338,N19-1348,0,0.122116,"Missing"
2020.emnlp-main.338,J95-2003,0,0.0428999,"e shifting distance, allowing the model attention to shift from “John” to the tokens “[E]” then to “loves” for predicting the next summary word. 2.2 Transformer with Shared Representation We explore an alternative method to allow mentions of the same PoC to be connected with each other. Particularly, we direct one attention head to focus on tokens belonging to the same PoC, allowing these tokens to share semantic representations, similar to Strubell et al. (2018). Sharing representation is meaningful as these mentions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Let z={z1 , . . . , z|z |} be a sequence containing PoC information, where zi ∈ {0, . . . , K} indicates the index of PoC to which the token xi belongs. zi =0 indicates xi is not associated with any PoC. Our T RANS -S HARE R EPR model selects an attention head h from the l-th layer of the Transformer model. The attention head h governs tokens that belong to PoCs (zi 6= 0). Its hidden representation hli is computed by modeling only pairwise relationships between token i and any token j of the same PoC (zi = zj ; Eq. (3)), while other tokens 21.0 ROUGE-2 We fine-tune the model on a sentence fu"
2020.emnlp-main.338,2020.acl-main.453,0,0.0280677,"Missing"
2020.emnlp-main.338,D19-5413,1,0.908158,"Missing"
2020.emnlp-main.338,2020.acl-srw.26,1,0.833527,"Missing"
2020.emnlp-main.338,2020.acl-main.703,0,0.0296695,"oaching team are two PoCs. The use of alternative expressions for conveying the same meanings is standard practice in writing, as it increases lexical variety and reduces redundancy. However, existing summarizers cannot make effective use of these expressions to establish correspondence between sentences, often leading to ungrammatical and nonsensical outputs. 2.1 Transformer with Linking It is advantageous for a Transformer model to make use of PoC information for sentence fusion. While Transformer-based pretrained models have had considerable success (Devlin et al., 2019; Dong et al., 2019; Lewis et al., 2020), they primarily feature pairwise relationships between tokens, but not PoC mentions, which are are text chunks of varying size. Only to a limited extent do these models embed knowledge of coreference (Clark et al., 2019), and there is a growing need for incorporating PoC linkages explicitly in a Transformer model to enhance its ability to perform sentence fusion. We propose to enrich Transformer’s source sequence with markups that indicate PoC linkages. Here PoC information is assumed to be available for any fusion instance (details in §3). We introduce special tokens ([Sk ] and [Ek ]) to mar"
2020.emnlp-main.338,W04-1013,0,0.052362,"stigation into the situation in Palestinian territories. Reference: Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis. Table 2: Example output of sentence fusion systems. PG only performs sentence shortening rather than fusion. Transformer fails to retain the original meaning and Transformer-S HARE R EPR performs best. Reference demonstrates a high level of abstraction. Sentences are manually de-tokenized for readability. We compare system outputs and references using a number of automatic evaluation metrics including ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2020). Results are presented in Table 1. We observe that all Transformer models outperform PG, suggesting that these models can benefit substantially from unsupervised pretraining on a large corpus of text. On the heuristic test set where training and testing conditions match (they both use automatically identified PoC), T RANS -L INKING performs better than T RANS -S HARE R EPR, and vice versa on the final test set. We conjecture that this is because the linking model has a stronger requirement on PoC boundaries and the training/test"
2020.emnlp-main.338,P19-1500,0,0.036562,"Missing"
2020.emnlp-main.338,W05-1612,0,0.579999,"Missing"
2020.emnlp-main.338,2020.acl-main.450,0,0.0116104,"hful, especially when compared to fusions that reuse the source text. Our results call for a System Pointer-Generator Transformer Trans-S HARE R EPR Reference Extractiveness Truthful. 1-gram 2-gram 3-gram 63.6 71.7 70.9 67.2 97.5 91.9 92.0 72.0 83.1 68.6 70.1 34.9 72.8 54.2 56.4 20.9 Table 3: Fusion sentences are evaluated by their level of truthfulness and extractivenss. Our system fusions attain a high level of truthfulness with moderate extractivenss. reexamination of sentence fusion using better evaluation metrics including semantics and questionanswering-based metrics (Zhao et al., 2019; Wang et al., 2020; Durmus et al., 2020). 4 Conclusion We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard setting of documen"
2020.emnlp-main.338,W13-2117,0,0.268015,"Missing"
2020.emnlp-main.338,D18-1206,0,0.0800122,"Missing"
2020.emnlp-main.338,P02-1040,0,0.107704,"situation in Palestinian territories. Reference: Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis. Table 2: Example output of sentence fusion systems. PG only performs sentence shortening rather than fusion. Transformer fails to retain the original meaning and Transformer-S HARE R EPR performs best. Reference demonstrates a high level of abstraction. Sentences are manually de-tokenized for readability. We compare system outputs and references using a number of automatic evaluation metrics including ROUGE (Lin, 2004), BLEU (Papineni et al., 2002) and BERTScore (Zhang et al., 2020). Results are presented in Table 1. We observe that all Transformer models outperform PG, suggesting that these models can benefit substantially from unsupervised pretraining on a large corpus of text. On the heuristic test set where training and testing conditions match (they both use automatically identified PoC), T RANS -L INKING performs better than T RANS -S HARE R EPR, and vice versa on the final test set. We conjecture that this is because the linking model has a stronger requirement on PoC boundaries and the training/testing conditions must match for"
2020.emnlp-main.338,D19-1053,1,0.839589,"e that as less truthful, especially when compared to fusions that reuse the source text. Our results call for a System Pointer-Generator Transformer Trans-S HARE R EPR Reference Extractiveness Truthful. 1-gram 2-gram 3-gram 63.6 71.7 70.9 67.2 97.5 91.9 92.0 72.0 83.1 68.6 70.1 34.9 72.8 54.2 56.4 20.9 Table 3: Fusion sentences are evaluated by their level of truthfulness and extractivenss. Our system fusions attain a high level of truthfulness with moderate extractivenss. reexamination of sentence fusion using better evaluation metrics including semantics and questionanswering-based metrics (Zhao et al., 2019; Wang et al., 2020; Durmus et al., 2020). 4 Conclusion We address the challenge of information fusion in the context of neural abstractive summarization by making crucial use of points of correspondence between sentences. We enrich Transformers with PoC information and report model performance on a new test bed for information fusion. Our findings suggest that modeling points of correspondence is crucial for effective sentence fusion, and sentence fusion remains a challenging direction of research. Future work may explore the use of points of correspondence and sentence fusion in the standard"
2020.emnlp-main.338,P17-1099,0,0.0383353,"-stopword tokens from each sentence that do not already exist in the other sentence. to find two document sentences that are most similar to it, which forms a fusion instance containing a pair of source sentences and their summary. PoCs have been annotated based on Halliday and Hasan’s theory of cohesion (1976) for 1,494 fusion instances, taken from 1,174 documents in the test and valid splits of CNN/DM with a moderate to high inter-annotator agreement (0.58). Automatic Evaluation We proceed by investigating the effectiveness of various sentence fusion models, including (a) Pointer-Generator (See et al., 2017) that employs an encoder-decoder architecture to condense input sentences to a vector representation, then decode it into a fusion sentence. (b) Transformer, our baseline Transformer architecture w/o PoC information. It is a strong baseline that resembles the UniLM model described in (Dong et al., 2019). (c) T RANS -L INKING uses special tokens to mark the boundaries of PoC mentions (§2.1). (d) T RANS -S HARE R EPR allows tokens of the same PoC to share representations (§2.2). All Transformer models are initialized with BERT-BASE parameters and are fine-tuned using UniLM’s sequenceto-sequence"
2020.emnlp-main.338,D18-1548,0,0.0524907,"Missing"
2020.emnlp-main.338,I13-1198,0,0.287394,"Missing"
2020.emnlp-main.509,2020.acl-main.175,0,0.0198387,"he original meaning can hinder the deployment of summarization techniques in real-world scenarios, as inaccurate and untruthful summaries can lead the readers to false conclusions (Cao et al., 2018; Falke et al., 2019; Lebanoff et al., 2019). We aim to produce summary highlights in this paper, which will be overlaid on source documents to allow summaries to be interpreted in context. Generation of summary highlights is of crucial importance to tasks such as producing informative snippets from search outputs (Kaisser et al., 2008), summarizing viewpoints in opinionated text (Paul et al., 2010; Amplayo and Lapata, 2020), and annotating website privacy policies to assist users in answering important questions (Sadeh et al., 2013). Determining the most appropriate textual unit for highlighting, however, has been an understudied problem. Extractive summarization selects whole sentences from documents; a sentence can contain 20 to 30 words on average (Kamigaito et al., 2018). Keyphrases containing two to three words are much less informative (Hasan and Ng, 2014). Neither are ideal solutions. There is a rising need for other forms of highlighting, and we explore subsentence highlights that strike a balance betwee"
2020.emnlp-main.509,P18-1063,0,0.0319353,"d as winter storms hit during one of the year’s busiest travel weeks. Self-Contained Segments • Some interstates are closed • hundreds of flights have been canceled as winter storms hit • flights have been canceled as winter storms hit • winter storms hit during one of the year’s busiest travel weeks Non-Self-Contained Segments • Some interstates are • closed and hundreds of flights have been • been canceled as winter storms hit during one of • hit during one of the year’s Table 2: Examples of self-contained and non-self-contained segments extracted from a document sentence. Tan et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Laban et al., 2020). With greater flexibility comes increased risk. Failing to accurately convey the original meaning can hinder the deployment of summarization techniques in real-world scenarios, as inaccurate and untruthful summaries can lead the readers to false conclusions (Cao et al., 2018; Falke et al., 2019; Lebanoff et al., 2019). We aim to produce summary highlights in this paper, which will be overlaid on source documents to allow summaries to be interpreted in context. Generation of summary highlights is of crucial"
2020.emnlp-main.509,P16-1046,0,0.0775281,"Missing"
2020.emnlp-main.509,P19-1098,1,0.890225,"s process produces a collection of self-contained and partially-overlapping segments from a set of documents. Next, we assess the informativeness of the segments and leverage DPP to identify a subset to form the summary highlights. 3.2 semi-definite matrix and Lij indicates the correlation between segments i and j; LY is a submatrix of L containing only entries indexed by elements in Y ; I is the identity matrix. This definition suggests that the probability of a summary P(Y ; L) is proportional to the determinant of LY . Segment Selection with DPP We employ the modeling framework proposed by Cho et al. (2019a) for modeling determinantal point processes. DPP (Kulesza and Taskar, 2012) defines a probability measure P over all subsets (2|Y |) of a ground set containing a collection of N segments Y = {1, 2, · · · , N}. The probability of an extractive summary, containing a subset of the segments Y ⊆ Y, is defined by Eq. (1), where det(·) is the determinant of a matrix; L ∈ RN×N is a positive A decomposition exists for the L-ensemble matrix: Lij = qi · Sij · qj where qi ∈ R+ is a quality score of the i-th segment and Sij is a pairwise similarity score between segments i and j. If q and S are available"
2020.emnlp-main.509,D19-5412,1,0.914146,"s process produces a collection of self-contained and partially-overlapping segments from a set of documents. Next, we assess the informativeness of the segments and leverage DPP to identify a subset to form the summary highlights. 3.2 semi-definite matrix and Lij indicates the correlation between segments i and j; LY is a submatrix of L containing only entries indexed by elements in Y ; I is the identity matrix. This definition suggests that the probability of a summary P(Y ; L) is proportional to the determinant of LY . Segment Selection with DPP We employ the modeling framework proposed by Cho et al. (2019a) for modeling determinantal point processes. DPP (Kulesza and Taskar, 2012) defines a probability measure P over all subsets (2|Y |) of a ground set containing a collection of N segments Y = {1, 2, · · · , N}. The probability of an extractive summary, containing a subset of the segments Y ⊆ Y, is defined by Eq. (1), where det(·) is the determinant of a matrix; L ∈ RN×N is a positive A decomposition exists for the L-ensemble matrix: Lij = qi · Sij · qj where qi ∈ R+ is a quality score of the i-th segment and Sij is a pairwise similarity score between segments i and j. If q and S are available"
2020.emnlp-main.509,P19-1102,0,0.197446,"onto the positive semi-definite (PSD) cone to ensure that it satisfies the PSD property (§3.2). This is accomplished in two steps, where L0 is the new L-ensemble. P L = ni=0 λi vi vi> (Eigenvalue decomposition) P L0 = ni=0 max{λi , 0}vi vi> (PSD projection) R-1 R-2 R-SU4 DPP-BERT (Cho et al., 2019b) DPP (Kulesza and Taskar, 2012) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) Centroid (Hong et al., 2014) ICSISumm (Gillick and Favre, 2009) Opinosis (Ganesan et al., 2010) Pointer-Gen (See et al., 2017) CopyTrans (Gehrmann et al., 2018) Hi-MAP (Fabbri et al., 2019) 39.05 38.10 29.48 31.04 34.44 35.49 37.31 27.07 31.43 28.54 35.78 10.23 9.14 4.25 6.03 7.11 7.80 9.36 5.03 6.03 6.38 8.90 14.35 13.40 8.64 10.23 11.19 12.02 13.12 8.63 10.01 7.22 11.43 HL-TreeSegs (Our work) HL-XLNetSegs (Our work) 39.18 39.26 10.30 10.70 14.37 14.47 Table 3: Results on DUC-04 dataset evaluated by ROUGE. 4 Experiments 4.1 Data Sets Our data comes from NIST. We use them to investigate the feasibility of the proposed multi-document summarization method. Particularly, we use DUC03/04 (Over and Yen, 2004) and TAC-08/09/10/11 datasets (Dang and Owczarzak, 2008), which contain 60/5"
2020.emnlp-main.509,P19-1213,0,0.0364517,"Missing"
2020.emnlp-main.509,C10-1039,0,0.213358,"t summarization data by maximizing log-likelihood. At each iteration, we project the L-ensemble onto the positive semi-definite (PSD) cone to ensure that it satisfies the PSD property (§3.2). This is accomplished in two steps, where L0 is the new L-ensemble. P L = ni=0 λi vi vi> (Eigenvalue decomposition) P L0 = ni=0 max{λi , 0}vi vi> (PSD projection) R-1 R-2 R-SU4 DPP-BERT (Cho et al., 2019b) DPP (Kulesza and Taskar, 2012) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) Centroid (Hong et al., 2014) ICSISumm (Gillick and Favre, 2009) Opinosis (Ganesan et al., 2010) Pointer-Gen (See et al., 2017) CopyTrans (Gehrmann et al., 2018) Hi-MAP (Fabbri et al., 2019) 39.05 38.10 29.48 31.04 34.44 35.49 37.31 27.07 31.43 28.54 35.78 10.23 9.14 4.25 6.03 7.11 7.80 9.36 5.03 6.03 6.38 8.90 14.35 13.40 8.64 10.23 11.19 12.02 13.12 8.63 10.01 7.22 11.43 HL-TreeSegs (Our work) HL-XLNetSegs (Our work) 39.18 39.26 10.30 10.70 14.37 14.47 Table 3: Results on DUC-04 dataset evaluated by ROUGE. 4 Experiments 4.1 Data Sets Our data comes from NIST. We use them to investigate the feasibility of the proposed multi-document summarization method. Particularly, we use DUC03/04 (O"
2020.emnlp-main.509,A83-1023,0,0.238369,"2019). We next discuss our method in greater detail. 3 Our Method We present a new method to identify self-contained segments, then select important and non-redundant segments to form a summary, as text fragments containing incomplete and disorganized information are hardly successful summary highlights. 3.1 Self-Contained Segments A self-contained segment is, in a sense, a miniature sentence. Any text segment containing incomplete or ungrammatical constructions is incomprehensible to humans. Table 2 presents examples of selfcontained and non-self-contained segments. Since its very inception (Vladutz, 1983), the concept of “semantically self-contained segment” has not been sufficiently examined in the literature and lacks an universal definition. We assume in this paper that a self-contained segment shall conform to certain syntactic validity constraints and there exists only weak dependencies between words that belong to the segment and those do not. The automatic identification of self-contained segments requires more than segmentation or parsing sentences into tree structures (Dozat and Manning, 2018). Self-contained segments do not necessarily correspond to constituents of the tree and furth"
2020.emnlp-main.509,P10-1058,0,0.0162456,"on average (Kamigaito et al., 2018). Keyphrases containing two to three words are much less informative (Hasan and Ng, 2014). Neither are ideal solutions. There is a rising need for other forms of highlighting, and we explore subsentence highlights that strike a balance between the amount and quality of emphasized content. It is best for highlighted segments to remain selfcontained. In fact, multiple partially-overlapping and self-contained segments can exist in a sentence, as illustrated in Table 2. Identifying self-contained segments has not been thoroughly investigated in previous studies. Woodsend and Lapata (2010) propose to generate story highlights by selecting and combining phrases. Li et al. (2016) explore elemen6283 tary discourse units generated using an RST parser as selection units. Spala et al. (2018) present a crowdsourcing method for workers to highlight sentences and compare systems. Arumae et al. (2019) propose to align human abstracts and source articles to create ground-truth highlight annotations. Importantly, and distinguishing our work from earlier literature, we make a first attempt to generate self-contained highlights, drawing on the successes of deep contextualized representations"
2021.adaptnlp-1.25,P14-1133,0,0.0138199,"ight 1st Intent Match: “survivor-in-sight” Canonical Utterance Reduction Spotted survivor at 9 o clock Hierarchical Projection Canonical Utterance 2nd Intent Match: “direction” 9 o clock Logical Form: {“intents”: [{“intent”: “survivor-in-sight”}, {“intent”: “direction”, “num”: 9}] Figure 1: Example generated semantic parse. A parser must (1) understand paraphrases of canonical utterances and (2) parse multiple intents in one utterance. Introduction Semantic parsing to map a natural language utterance to its logical form is regarded as a challenging task partly due to a lack of annotated data (Berant and Liang, 2014; Yin et al., 2018; Gardner et al., 2018). A promising avenue of research is to generate a set of candidate logical forms paired with their canonical realizations in natural language. Then, the canonical utterance that best matches the input is identified by a model, and its logical form is used as output (Berant and Liang, 2014). A paraphrase/sequence-to-sequence model may additionally be used to translate a canonical utterance to a logical form (Wang et al., 2015; Herzig and Berant, 2019; Cao et al., 2020; Marzoev et al., 2020). While the results are promising, most existing works do not han"
2021.adaptnlp-1.25,2020.acl-main.608,0,0.0241173,"al form is regarded as a challenging task partly due to a lack of annotated data (Berant and Liang, 2014; Yin et al., 2018; Gardner et al., 2018). A promising avenue of research is to generate a set of candidate logical forms paired with their canonical realizations in natural language. Then, the canonical utterance that best matches the input is identified by a model, and its logical form is used as output (Berant and Liang, 2014). A paraphrase/sequence-to-sequence model may additionally be used to translate a canonical utterance to a logical form (Wang et al., 2015; Herzig and Berant, 2019; Cao et al., 2020; Marzoev et al., 2020). While the results are promising, most existing works do not handle natural language utterances with multiple intents. We refer to an intent as a goal intended by a user’s utterance. Multi-intent utterances allow people to communicate core aspects of a situation in a consistent and timely manner, as illustrated in Figure 1. Multi-intent semantic parsing is especially suitable for military domains where emphasis is placed on communication skills, terminology, and brevity (Weinstein, 1990). While communication protocols are often published, variations are allowed given th"
2021.adaptnlp-1.25,P18-1068,0,0.0216505,"wed given the current situation. An area of interest is Intelligence, Surveillance, and Reconnaissance (ISR) domains where contact reports (e.g., “Arriving at home base and ready to descend”) often contain multiple intents, and a system must determine the number of intents, interpret the natural language and predict the exact logical forms for every intent, which can be highly challenging. We investigate new methods for semantic parsing of utterances with multiple intents. Importantly, and distinguishing our work from earlier literature (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Dong and Lapata, 2018; Zeng et al., 2020), our domain areas have no supervised training data, nor can pseudo-language utterances be created through crowdsourcing due to their sensi255 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 255–262 April 20, 2021. ©2021 Association for Computational Linguistics Natural Language Utterance tive nature and requirement of expert knowledge. We thus operate in a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a comprehensive collection of natural utterances for military applica"
2021.adaptnlp-1.25,P18-5006,0,0.0170448,"Canonical Utterance Reduction Spotted survivor at 9 o clock Hierarchical Projection Canonical Utterance 2nd Intent Match: “direction” 9 o clock Logical Form: {“intents”: [{“intent”: “survivor-in-sight”}, {“intent”: “direction”, “num”: 9}] Figure 1: Example generated semantic parse. A parser must (1) understand paraphrases of canonical utterances and (2) parse multiple intents in one utterance. Introduction Semantic parsing to map a natural language utterance to its logical form is regarded as a challenging task partly due to a lack of annotated data (Berant and Liang, 2014; Yin et al., 2018; Gardner et al., 2018). A promising avenue of research is to generate a set of candidate logical forms paired with their canonical realizations in natural language. Then, the canonical utterance that best matches the input is identified by a model, and its logical form is used as output (Berant and Liang, 2014). A paraphrase/sequence-to-sequence model may additionally be used to translate a canonical utterance to a logical form (Wang et al., 2015; Herzig and Berant, 2019; Cao et al., 2020; Marzoev et al., 2020). While the results are promising, most existing works do not handle natural language utterances with mult"
2021.adaptnlp-1.25,D18-1300,0,0.132586,"e and requirement of expert knowledge. We thus operate in a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a comprehensive collection of natural utterances for military applications is difficult; it can be easier to create a grammar that generates canonical utterances for the application. In addition, there are scenarios where there is insufficient time or funding to obtain supervised data, e.g. quickly building a virtual assistant for a new mobile app. Our goal is distinct from related efforts in dialog systems (Gupta et al., 2018; Vanzo et al., 2019; Lee et al., 2019; Ham et al., 2020); the parser does not have additional context or interaction but focuses on modeling complex compositional intents. We build on methods that project natural utterances to the canonical space (Marzoev et al., 2020) and investigate novel adaptations for handling multiintent utterances. Our contributions are as follows. copy acoustic to tact active one enabled $ROOT 1 $OP tact $SENSOR Partial Realizations aco tact $SENSOR 2 acoustic tact $SENSOR 4 acoustic tact active $NUM on 5 acoustic tact active one on Canonical Utterance acoustic tact a"
2021.adaptnlp-1.25,2020.acl-main.54,0,0.0132419,"a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a comprehensive collection of natural utterances for military applications is difficult; it can be easier to create a grammar that generates canonical utterances for the application. In addition, there are scenarios where there is insufficient time or funding to obtain supervised data, e.g. quickly building a virtual assistant for a new mobile app. Our goal is distinct from related efforts in dialog systems (Gupta et al., 2018; Vanzo et al., 2019; Lee et al., 2019; Ham et al., 2020); the parser does not have additional context or interaction but focuses on modeling complex compositional intents. We build on methods that project natural utterances to the canonical space (Marzoev et al., 2020) and investigate novel adaptations for handling multiintent utterances. Our contributions are as follows. copy acoustic to tact active one enabled $ROOT 1 $OP tact $SENSOR Partial Realizations aco tact $SENSOR 2 acoustic tact $SENSOR 4 acoustic tact active $NUM on 5 acoustic tact active one on Canonical Utterance acoustic tact active two on … alpha tact $SENSOR $STANDBY active Partial"
2021.adaptnlp-1.25,D19-1394,0,0.0161085,"ge utterance to its logical form is regarded as a challenging task partly due to a lack of annotated data (Berant and Liang, 2014; Yin et al., 2018; Gardner et al., 2018). A promising avenue of research is to generate a set of candidate logical forms paired with their canonical realizations in natural language. Then, the canonical utterance that best matches the input is identified by a model, and its logical form is used as output (Berant and Liang, 2014). A paraphrase/sequence-to-sequence model may additionally be used to translate a canonical utterance to a logical form (Wang et al., 2015; Herzig and Berant, 2019; Cao et al., 2020; Marzoev et al., 2020). While the results are promising, most existing works do not handle natural language utterances with multiple intents. We refer to an intent as a goal intended by a user’s utterance. Multi-intent utterances allow people to communicate core aspects of a situation in a consistent and timely manner, as illustrated in Figure 1. Multi-intent semantic parsing is especially suitable for military domains where emphasis is placed on communication skills, terminology, and brevity (Weinstein, 1990). While communication protocols are often published, variations ar"
2021.adaptnlp-1.25,P17-1089,0,0.0153883,"ation protocols are often published, variations are allowed given the current situation. An area of interest is Intelligence, Surveillance, and Reconnaissance (ISR) domains where contact reports (e.g., “Arriving at home base and ready to descend”) often contain multiple intents, and a system must determine the number of intents, interpret the natural language and predict the exact logical forms for every intent, which can be highly challenging. We investigate new methods for semantic parsing of utterances with multiple intents. Importantly, and distinguishing our work from earlier literature (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Dong and Lapata, 2018; Zeng et al., 2020), our domain areas have no supervised training data, nor can pseudo-language utterances be created through crowdsourcing due to their sensi255 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 255–262 April 20, 2021. ©2021 Association for Computational Linguistics Natural Language Utterance tive nature and requirement of expert knowledge. We thus operate in a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a compreh"
2021.adaptnlp-1.25,2020.acl-main.703,0,0.279011,"cularly challenging. The grammar contains 37 rules, 36 non-terminals, and approximately 60 terminals. H ELI Short commands were collected from helicopter communications and consolidated into a similar grammar to ISR. The grammar has a shallow structure and does not contain many nested intents, but each utterance is short (1–5 tokens), which has its own challenges. The grammar contains 48 rules, 47 non-terminals, and approximately 60 terminals. Example in Fig. 1. Natural language utterances in both datasets are wholly defined by its grammar. For evaluation, 257 ISR H ELI Single Intent seq2seq (Lewis et al., 2020) proj (Marzoev et al., 2020) NT-Avg 34.4 82.5 85.9 58.2 74.1 74.1 Multi Intent seq2seq (Lewis et al., 2020) NT-Avg + MetaGrammar NT-Avg + Reduction 48.1 16.3 49.1 45.5 25.2 38.8 Table 2: Logical form accuracies for internal ISR and H ELI datasets we expand to a set of paraphrased canonical utterances using an English-to-X → X-to-English procedure similar to those used for augmentation in paraphrase datasets (Wieting and Gimpel, 2018; Hu et al., 2019). OVERNIGHT (Wang et al., 2015) is a semantic parsing dataset over eight domains, including sports, restaurants, and social media. Each domain con"
2021.adaptnlp-1.25,W19-5931,0,0.0272439,"expert knowledge. We thus operate in a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a comprehensive collection of natural utterances for military applications is difficult; it can be easier to create a grammar that generates canonical utterances for the application. In addition, there are scenarios where there is insufficient time or funding to obtain supervised data, e.g. quickly building a virtual assistant for a new mobile app. Our goal is distinct from related efforts in dialog systems (Gupta et al., 2018; Vanzo et al., 2019; Lee et al., 2019; Ham et al., 2020); the parser does not have additional context or interaction but focuses on modeling complex compositional intents. We build on methods that project natural utterances to the canonical space (Marzoev et al., 2020) and investigate novel adaptations for handling multiintent utterances. Our contributions are as follows. copy acoustic to tact active one enabled $ROOT 1 $OP tact $SENSOR Partial Realizations aco tact $SENSOR 2 acoustic tact $SENSOR 4 acoustic tact active $NUM on 5 acoustic tact active one on Canonical Utterance acoustic tact active two on … alpha"
2021.adaptnlp-1.25,P15-1129,0,0.0551669,"Missing"
2021.adaptnlp-1.25,H90-1103,0,0.584271,"canonical utterance to a logical form (Wang et al., 2015; Herzig and Berant, 2019; Cao et al., 2020; Marzoev et al., 2020). While the results are promising, most existing works do not handle natural language utterances with multiple intents. We refer to an intent as a goal intended by a user’s utterance. Multi-intent utterances allow people to communicate core aspects of a situation in a consistent and timely manner, as illustrated in Figure 1. Multi-intent semantic parsing is especially suitable for military domains where emphasis is placed on communication skills, terminology, and brevity (Weinstein, 1990). While communication protocols are often published, variations are allowed given the current situation. An area of interest is Intelligence, Surveillance, and Reconnaissance (ISR) domains where contact reports (e.g., “Arriving at home base and ready to descend”) often contain multiple intents, and a system must determine the number of intents, interpret the natural language and predict the exact logical forms for every intent, which can be highly challenging. We investigate new methods for semantic parsing of utterances with multiple intents. Importantly, and distinguishing our work from earl"
2021.adaptnlp-1.25,P18-1042,0,0.0124916,"ly 60 terminals. Example in Fig. 1. Natural language utterances in both datasets are wholly defined by its grammar. For evaluation, 257 ISR H ELI Single Intent seq2seq (Lewis et al., 2020) proj (Marzoev et al., 2020) NT-Avg 34.4 82.5 85.9 58.2 74.1 74.1 Multi Intent seq2seq (Lewis et al., 2020) NT-Avg + MetaGrammar NT-Avg + Reduction 48.1 16.3 49.1 45.5 25.2 38.8 Table 2: Logical form accuracies for internal ISR and H ELI datasets we expand to a set of paraphrased canonical utterances using an English-to-X → X-to-English procedure similar to those used for augmentation in paraphrase datasets (Wieting and Gimpel, 2018; Hu et al., 2019). OVERNIGHT (Wang et al., 2015) is a semantic parsing dataset over eight domains, including sports, restaurants, and social media. Each domain contains a grammar to generate canonical utterances and LFs, as well as natural language paraphrases. As we are interested in weakly-supervised parsing, we ignore natural language utterances in training and only use those in the test set for evaluation. The datasets we use contain grammars and natural language data for utterances with a single intent, but they lack multi-intent data. We create simulated multi-intent utterances by conca"
2021.adaptnlp-1.25,P18-1070,0,0.0237225,"survivor-in-sight” Canonical Utterance Reduction Spotted survivor at 9 o clock Hierarchical Projection Canonical Utterance 2nd Intent Match: “direction” 9 o clock Logical Form: {“intents”: [{“intent”: “survivor-in-sight”}, {“intent”: “direction”, “num”: 9}] Figure 1: Example generated semantic parse. A parser must (1) understand paraphrases of canonical utterances and (2) parse multiple intents in one utterance. Introduction Semantic parsing to map a natural language utterance to its logical form is regarded as a challenging task partly due to a lack of annotated data (Berant and Liang, 2014; Yin et al., 2018; Gardner et al., 2018). A promising avenue of research is to generate a set of candidate logical forms paired with their canonical realizations in natural language. Then, the canonical utterance that best matches the input is identified by a model, and its logical form is used as output (Berant and Liang, 2014). A paraphrase/sequence-to-sequence model may additionally be used to translate a canonical utterance to a logical form (Wang et al., 2015; Herzig and Berant, 2019; Cao et al., 2020; Marzoev et al., 2020). While the results are promising, most existing works do not handle natural langua"
2021.adaptnlp-1.25,D18-1425,0,0.016627,"riations are allowed given the current situation. An area of interest is Intelligence, Surveillance, and Reconnaissance (ISR) domains where contact reports (e.g., “Arriving at home base and ready to descend”) often contain multiple intents, and a system must determine the number of intents, interpret the natural language and predict the exact logical forms for every intent, which can be highly challenging. We investigate new methods for semantic parsing of utterances with multiple intents. Importantly, and distinguishing our work from earlier literature (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Dong and Lapata, 2018; Zeng et al., 2020), our domain areas have no supervised training data, nor can pseudo-language utterances be created through crowdsourcing due to their sensi255 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 255–262 April 20, 2021. ©2021 Association for Computational Linguistics Natural Language Utterance tive nature and requirement of expert knowledge. We thus operate in a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a comprehensive collection of natural utteranc"
2021.adaptnlp-1.25,2020.acl-demos.24,0,0.0164475,"ituation. An area of interest is Intelligence, Surveillance, and Reconnaissance (ISR) domains where contact reports (e.g., “Arriving at home base and ready to descend”) often contain multiple intents, and a system must determine the number of intents, interpret the natural language and predict the exact logical forms for every intent, which can be highly challenging. We investigate new methods for semantic parsing of utterances with multiple intents. Importantly, and distinguishing our work from earlier literature (Iyer et al., 2017; Zhong et al., 2017; Yu et al., 2018; Dong and Lapata, 2018; Zeng et al., 2020), our domain areas have no supervised training data, nor can pseudo-language utterances be created through crowdsourcing due to their sensi255 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 255–262 April 20, 2021. ©2021 Association for Computational Linguistics Natural Language Utterance tive nature and requirement of expert knowledge. We thus operate in a weakly-supervised setting by assuming only access to a grammar that generates canonical utterances and logical forms. Obtaining a comprehensive collection of natural utterances for military applications is difficult;"
2021.emnlp-main.520,J08-4004,0,0.14758,"ange of what is normally found in annotating speech transcripts for extractive summaries (0.1∼0.3; Marge 4 We use 10-second intervals rather than utterances as measuring units as the duration of utterances vary. If annotators all selected some content, or no content at all, from a 10-second interval, they are in agreement. et al., 2010), as annotating spoken text is a highly challenging task. We find that annotators tend to perceive the same region as salient but they may disagree as to which utterances should be included in the summary due to verbosity of spoken text. We refer the reader to (Artstein and Poesio, 2008) for interpretations and improvements to IAA. 4 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ⊂ X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQVAE; van den Oord et al., 2017) to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization (Angelidis et al., 2021) and machine translation (Prato et al., 2020). We are interested in u"
2021.emnlp-main.520,P18-1063,0,0.0232141,"ng baselines on the dataset and shed light on the task of livestream transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with t"
2021.emnlp-main.520,N18-2097,1,0.84334,"transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to f"
2021.emnlp-main.520,D18-1409,0,0.0460202,"Missing"
2021.emnlp-main.520,P19-1102,0,0.0217495,"s are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate extractive rather tha"
2021.emnlp-main.520,D18-1443,0,0.0204063,"the task of livestream transcript summarization. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestr"
2021.emnlp-main.520,D19-1620,0,0.0182944,"eamer talks about Digital Painting with Maddy Bellwoar to create fairytale themed images. The annotators are asked to write a concise summary of this clip using their own words (Task A) and identify summary utterances (Task B). indicates the number of minutes since the beginning of the recording. When a user hovers over the thumbnail or scrolls past a video, we expect a textual summary to give a glimpse of the verbal content. This view of summarization leads us to annotate salient content across the video in an equally detailed manner. It naturally avoids lead bias that is ubiquitous in news (Grenander et al., 2019). We segment a video into 5-minute clips and annotate each clip for summary-worthy content. A clip contains an average of 51 utterances and 460 words. Due to time and budget constraints, we select 370 streamed video for summary annotation.3 Table 1 provides a detailed comparison of our annotated corpus with previous datasets, including Switchboard (Godfrey et al., 1992), ICSI (Janin et al., 2003) and AMI (Carletta et al., 2006) that contain both transcripts and human-annotated ex6459 3 Details of video selection are provided in Supplementary. Dataset Switchboard ICSI AMI StreamHover Type Telep"
2021.emnlp-main.520,2020.acl-main.437,0,0.0372448,"multiple as the latter are prone to generate hallucinated condimensions and discuss its strengths and weak- tent that does not exist in the source text (Cao nesses. Empirical results show that our method et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 1 outperforms strong summarization baselines. 2019; Maynez et al., 2020). The problem could be exacerbated by ungrammatical spoken utterances and transcription errors. Instead, we consider VQ2 Related Work VAE, an unsupervised representation learning techClosed captions are often provided onscreen, turn- nique (van den Oord et al., 2017; Jin et al., 2020; ing streaming videos into text on an unprecedented Angelidis et al., 2021) for content extraction. Unsuscale (Besik, 2020). However, there are very few pervised training of the VQ-VAE model and its insummarization studies that attempt to generate text ference could potentially be performed at the same previews of streaming videos to help users browse time, allowing important utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning pr"
2021.emnlp-main.520,L18-1016,0,0.0280773,"017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality resea"
2021.emnlp-main.520,D18-1208,0,0.0382294,"Missing"
2021.emnlp-main.520,D19-5413,1,0.902295,"Missing"
2021.emnlp-main.520,2020.acl-main.703,0,0.0764347,"o training, validation and test splits: • 3,884 clips (320 videos / 323 hours) in training, • 728 clips (25 videos / 61 hours) in validation, • 809 clips (25 videos / 67 hours) in test split. lead bias in news writing. Our setting is challenging as not only are there few utterances deemed to be summary-worthy but such utterances can occur anywhere in a video clip. Baselines. We compare StreamHover with stateof-the-art extractive and abstractive summarizers. The abstractive summarizers generate an abstract from the transcript of a clip without tuning.6 These include BART-large, BART-large-cnn (Lewis et al., 2020) and T5 (Raffel et al., 2020), which are some of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based models that extract relevant sentences based on eigenvector centrality. SumBasic (Vanderwende et al., 2007) assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization m"
2021.emnlp-main.520,D19-1370,0,0.154554,"t al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus"
2021.emnlp-main.520,P19-1210,0,0.100629,"t al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus"
2021.emnlp-main.520,2020.emnlp-main.752,0,0.0312864,"), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. Instead, we focus on generating short descriptions from transcripts and leave for future work crossmodality research. We describe our data annotation process in the following section. 3 Our Dataset We aim to create a large and representative corpus containing transcripts and summaries of streamed videos. We explore a leading social media platform (Behance.net) supported by Adobe Creative Cloud that features livestreams of creative work by artists and designers. The website boasts over 10 million users, who wa"
2021.emnlp-main.520,W04-1013,0,0.109384,"Missing"
2021.emnlp-main.520,P08-2051,0,0.0604866,"4, 2048}. The coefficient β used for commitment loss is set 7 The recent automatic metrics (Zhang et al., 2020; Sellam to 0.25 (Eq. (6)). These hyperparameters are tuned et al., 2020) have not been tested on speech transcripts. Spoken text contains filled pauses (um, uh, well), disfluencies (go-goon the validation set. We keep only utterances that go away), repetitions and verbal interruptions. ROUGE is the contain >5 words in consideration. The final train- only metric that has been validated to attain good correlation ing set contains 168,111 utterances. with human judgments on transcripts (Liu and Liu, 2008). 6463 FluCovRank Utterances • top left bottom / cloud studies today / find links to their original posts / hey jennifer saw the images / love the top left and bottom / info tab and i uploaded / colors are beautiful but im partial through colorful sky scenes / pretty large about 4000 by 4000 pixels / photo studies of today / moment 0 Hello good morning everybody welcome high foster highly art. 1 Hi Lisa, welcome everyone. 2 I hope you guys are having a good day so far. 3 Good to see you were going to be doing cloud studies today. 4 So if anybody is interested in joining in, if you want to work"
2021.emnlp-main.520,D19-1387,0,0.0905612,"tion. Our contributions are as follows. • We create a detailed annotation interface and new benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate"
2021.emnlp-main.520,W10-0716,0,0.0167421,"Missing"
2021.emnlp-main.520,2020.acl-main.174,0,0.0249024,"is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Pala"
2021.emnlp-main.520,P08-1054,0,0.137101,"Missing"
2021.emnlp-main.520,2020.findings-emnlp.1,0,0.0164094,"r the reader to (Artstein and Poesio, 2008) for interpretations and improvements to IAA. 4 Summarization Let X denote a sequence of spoken utterances from a segment of the transcript. Our summarizer aims to extract a subset of utterances Y ⊂ X that convey the essential content of the input. We experiment with an unsupervised summarizer that leverages vector-quantized variational autoencoders (VQVAE; van den Oord et al., 2017) to learn utterance representations and identifies summary utterances. The method was explored for opinion summarization (Angelidis et al., 2021) and machine translation (Prato et al., 2020). We are interested in using the method to account for domain characteristics of livestreams, which showcase new and creative work of artists and designers on their use of Photoshop, Illustrator, and other tools.5 VQ-VAE is a powerful framework for learning latent variable models using deep neural networks. It learns discrete vector representations for an utterance, which is then used to categorize the utterance along various dimensions. E.g., “Good morning Hi Everybody” suggests a greeting and opens up a dialogue; “I had probably 3 or 4 different customers on YouTube and ... on Facebook asked"
2021.emnlp-main.520,P17-1099,0,0.0301439,"w pervised training of the VQ-VAE model and its insummarization studies that attempt to generate text ference could potentially be performed at the same previews of streaming videos to help users browse time, allowing important utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning process. It is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al.,"
2021.emnlp-main.520,2020.acl-main.704,0,0.0382223,"Missing"
2021.emnlp-main.520,P18-1062,0,0.125413,"ome of the strongest performing neural abstractive summarizers that are pre-trained on language modeling and summarization tasks. The unsupervised extractive summarizers extract salient utterances from a clip. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based models that extract relevant sentences based on eigenvector centrality. SumBasic (Vanderwende et al., 2007) assigns higher scores to sentences containing frequently occurring content words. We further compare to a novel unsupervised graph-based summarization method for speech transcripts: FluCovRank (Shang et al., 2018) groups utterances into clusters, generates an abstractive sentence from each cluster, then selects the best elements from abstractive sentences under a budget constraint. Finally, we compare our approach with the Quantized Transformer (Angelidis et al., 2021), which uses a clustering interpretation of the quantized space and two-step sampling algorithm to extract summary sentences from reviews. Settings. We use pretrained BERT-BASE as our embedder Embedθ (·). The model has 12 layers, 12 heads per layer and a hidden size (H) of 768. A 6layer Transformer decoder is used as the generator Generat"
2021.emnlp-main.520,D19-1298,0,0.0170271,"t, livestreaming platforms may not of general-domain models. We refrain from using fully meet the needs of their customers. sequential methods for utterance selection. First, 6457 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6457–6474 c November 7–11, 2021. 2021 Association for Computational Linguistics it is difficult to scale up sequential prediction to process transcripts that exceed the maximum allowed length, even with models that handle long text (Beltagy et al., 2020; Zhao et al., 2020). Second, sequential methods (Narayan et al., 2018b; Xiao and Carenini, 2019) may not give enough flexibility to select salient utterances on-the-fly when content is being streamed live, thus they are unsuitable for our case. There has been a shortage of annotated datasets that are necessary for livestream transcript summarization. We build a browser-based user interface for summary annotation that provides to the annotators a clip of the livestream recording alongside a synchronized display of the transcript. The interface allows annotators to conveniently label summary utterances and write an abstractive summary using their own words (Figure 3). With a total of 500 h"
2021.emnlp-main.520,2021.naacl-main.110,1,0.73255,"ew benchmark dataset for automatic summarization of livestream transcripts. An informative preview of streamed content is of crucial importance to users when considering whether to hit play. Figure 2: A transcript snippet from “How to Create an Audio CD in Adobe Audition.” Most utterances are offtopic, except for the one marked in blue, suggesting the information density of livestream transcripts is low. et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Gehrmann et al., 2018; Cohan et al., 2018; Liu and Lapata, 2019; Fabbri et al., 2019; Bražinskas et al., 2020; Ladhak et al., 2020; Song et al., 2021). Despite their success, it remains unclear as to if and how the summarizers can be extended to spoken text, whose utterances may have very low information density. It is crucial to identify salient content from transcripts where a substantial number of utterances • We present StreamHover, a unsupervised model are devoted to informal chit-chats in an attempt to based on VQ-VAE to identify salient utterances connect with the audience (Figure 2). We investifrom livestream transcripts to form preview sum- gate extractive rather than abstractive approaches maries. We evaluate the method across mul"
2021.emnlp-main.520,2020.acl-main.553,0,0.0121756,"ortant utterances to be extracted or refind information that has been watched before. from a transcript segment on-the-fly during streamNeural text summarizers have focused primarily on ing, without interrupting the learning process. It is written text, including news articles, reviews, scien- also easier to tailor the model to specific domains tific papers and book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, nove"
2021.emnlp-main.520,P13-1137,0,0.030033,"nd book chapters (See et al., 2017; Tan compared to contemporary extractive methods (Ya1 sunaga et al., 2017; Dong et al., 2018; Xu and Our annotations and source code are available at https: //github.com/ucfnlp/streamhover Durrett, 2019; Wang et al., 2020). 6458 Our work contributes to a refined understanding of transcript summarization, which is understudied relative to its importance and potential. The transcripts may be obtained from channels such as movies and TVs (Papalampidi et al., 2020; Chen et al., 2021), interviews (Zhu et al., 2021), multiparty meetings (Murray and Carenini, 2008; Wang and Cardie, 2013; Li et al., 2019b; Koay et al., 2020, 2021; Zhong et al., 2021), telephone speech (Kafle and Huenerfauth, 2018) and more. The main thrust distinguishing our work with others is the combination of a benchmark summarization dataset, novel summarization methods and a challenging new domain where salient content is scattered throughout the transcript and mixed with substantial chit-chats. We do not make use of video event detection or multi-modal fusion (Zhu et al., 2018; Palaskar et al., 2019; Li et al., 2020) as little information could be gleaned from videos that mirror the artists’ desktop. I"
2021.naacl-main.110,E06-1040,0,0.0903084,"rotesters target French research ship 8 French police arrest five anti-nuclear protesters 8 Police arrest five anti-nuclear protesters in Antarctica 8 Police arrest five anti-nuclear protesters at French Antarctic Table 1: Example of alternative summaries generated from the source text. Admissible summaries are marked by 4. System summaries that fail to preserve the meaning of the source input are marked by 8. In this paper, we propose a new approach to overgenerate and select admissible summaries, which allows a summarizer to juggle multiple objectives and strike a good balance between them (Belz and Reiter, 2006). Our approach consists of two stages. Given a source text, a generator explores the space of all possible lengths to produce multiple variants of the target summary that contain diverse content. We then devise selectors to validate the quality of alternative summaries to predict whether they are admissible. Our selection mechanism can be cus1 Introduction tomized to suit particular needs without changing The learning objective of a modern abstractive sum- the generation space. Both stages can be effectively trained, optimized and evaluated. marizer is to produce system outputs that resemble r"
2021.naacl-main.110,2020.emnlp-main.337,0,0.0191521,"f these prop- similar vein, our generator produces a summary by erties are exhibited by system abstracts as a natural “filling-in-the-blanks” with appropriate words. The outcome of a learned summarizer (See et al., 2017; most confident words are generated first, less vital Takase et al., 2016; Tan et al., 2017; Chen and ones later. With confidence-driven generation, our Bansal, 2018; Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning of the left-to-right generation. Our method dramatically original document or fail to convey the main con- enhances the capability of the generator, making it cepts (Kryscinski et al., 2020). possible to explore summaries of varying lengths. 1392 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1392–14"
2021.naacl-main.110,P18-1015,0,0.0456291,"Missing"
2021.naacl-main.110,N18-1150,0,0.024098,"ng the most impor- right order. Beginning writers and language learntant information, being faithful to the original text, ers do not write in a strict sequential manner. In a grammatical and fluent, though some of these prop- similar vein, our generator produces a summary by erties are exhibited by system abstracts as a natural “filling-in-the-blanks” with appropriate words. The outcome of a learned summarizer (See et al., 2017; most confident words are generated first, less vital Takase et al., 2016; Tan et al., 2017; Chen and ones later. With confidence-driven generation, our Bansal, 2018; Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning of the left-to-right generation. Our method dramatically original document or fail to convey the main con- enhances the capability of the generator, making it cepts (Kryscinski et al., 2020)."
2021.naacl-main.110,P18-1063,0,0.16458,"Missing"
2021.naacl-main.110,K18-1054,0,0.0151914,"le given the source text. To distill information from the source text and the summary, we encode them into hidden vectors using RoBERTa (Liu et al., 2019). These are denoted by hx and hy , respectively. We create a vector for the pair, h = hx ⊕ hy ⊕ |hx − hy |⊕ (hx ∗ hy ), consisting of a concatenation of the two hidden vectors, their absolute difference |hx − hy |and their element-wise product (hx ∗ hy ). ⊕ is a concatenation of vectors. The output vector h is expected to capture the gist of the source text and the summary, and a similar approach is being used for natural language inference (Chen et al., 2018). The vector h is fed to a feed-forward layer to predict whether the summary is admissible given the source text. We have chosen to design the selector as a classifier rather than a ranking model because there can exist multiple, equally valid summaries for any source input. The classifier allows us to identify admissible summaries that are not only true-to-original but has the best overall quality. 4.2 Finding a suitable length for the summary is one of the most important open problems in automatic summarization (Shapira et al., 2018; Sun et al., 2019). A summary should be shorter than the or"
2021.naacl-main.110,W19-4828,0,0.0314131,"Missing"
2021.naacl-main.110,N19-1423,0,0.029847,"ce input according to users’ needs. On the contrary, post-editing of system summaries through a set of basic operations such as insertion and deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from these is our confidence-driven generation that goes beyond left-to-right order. It uses a denoising objective during training and is conveniently transformed into a semi-autoregressive generator at test time. We introduce a customized beam search algorithm to promote the generation of diverse outputs. In the following section, we describe in detail our two-step strategy. 3 A Confidence-Driven Generator endpoint. This is achieved by"
2021.naacl-main.110,P19-1331,0,0.413681,"ost probable next token in an autoregressive setting. We obtain the token that has the highest probability, and use it to replace the [ MASK ] token of that position. Next, the model makes new predictions for all remaining positions, conditioned on the source text and all summary tokens seen thus far. Our generator produces a summary having the exact given length and with a proper endpoint. fied for any source input according to users’ needs. On the contrary, post-editing of system summaries through a set of basic operations such as insertion and deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from these is our confidenc"
2021.naacl-main.110,2020.emnlp-main.749,0,0.785427,"Missing"
2021.naacl-main.110,2020.acl-main.454,0,0.0714326,"fail to capture the main concepts, and this kind of incomplete or partial information can lead to false assumptions about the original content. Moreover, summaries of moderate lengths may still contain hallucinated content that is nonexistent in the source text (Maynez et al., 2020). We present two summary selectors to combat these issues. Our first selector aims to predict what summary length is most suitable for a source text, whereas a second selector puts special emphasis on the overall quality of the system summary, in particular its faithfulness to the original text (Falke et al., 2019; Durmus et al., 2020). A novel dataset has been introduced in this work where we associate a source text with multiple summaries, and admissible ones are manually labelled by human annotators. Not only can the dataset be used to judge the effectiveness of summary selectors, but it provides a new testbed for future summarizers to compare their outputs against multiple reference summaries, which is key to improve the reliability of evaluation results (Louis and Nenkova, 2013). We have focused on generating abstractive summaries from single source sentences, but the insights gained from this study could inform the de"
2021.naacl-main.110,P19-1102,0,0.0475068,"fluent, though some of these prop- similar vein, our generator produces a summary by erties are exhibited by system abstracts as a natural “filling-in-the-blanks” with appropriate words. The outcome of a learned summarizer (See et al., 2017; most confident words are generated first, less vital Takase et al., 2016; Tan et al., 2017; Chen and ones later. With confidence-driven generation, our Bansal, 2018; Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning of the left-to-right generation. Our method dramatically original document or fail to convey the main con- enhances the capability of the generator, making it cepts (Kryscinski et al., 2020). possible to explore summaries of varying lengths. 1392 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language T"
2021.naacl-main.110,2020.emnlp-main.751,0,0.0705668,"Missing"
2021.naacl-main.110,P19-1213,0,0.174289,"Missing"
2021.naacl-main.110,W18-2706,0,0.0183577,"et al., 2020; Dong et al., 2020; Zhang et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al., 2017; Fan et al., 2018; Keskar et al., 2019; Makino et al., 2019; Song et al., 2020). In contrast, our generator focuses on producing multiple variants of the target summary that have diverse content and varying lengths. It offers precise control over the length of the summary, which has an important implication for fair comparison between different summarization systems (Napoles et al., 2011; Shapira et al., 2018). Our methodology allows for greater flexibility in designing summary selectors. The selectors may allow multiple admissible summaries to be identi• Our generator controls over the length of the summary,"
2021.naacl-main.110,D18-1443,0,0.0810204,"Missing"
2021.naacl-main.110,D19-1633,0,0.0161954,"deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from these is our confidence-driven generation that goes beyond left-to-right order. It uses a denoising objective during training and is conveniently transformed into a semi-autoregressive generator at test time. We introduce a customized beam search algorithm to promote the generation of diverse outputs. In the following section, we describe in detail our two-step strategy. 3 A Confidence-Driven Generator endpoint. This is achieved by shifting away from left-to-right generation but building a summary using a confidence-driven approach. Our generator is illustrated in Figure 1. To"
2021.naacl-main.110,N18-1065,0,0.0430912,"Missing"
2021.naacl-main.110,P18-1064,0,0.0481217,"Missing"
2021.naacl-main.110,D17-1227,0,0.0617457,"Missing"
2021.naacl-main.110,P19-1365,0,0.0256792,"(y|x) is factorized into a product of conditional probabilities Pθ (yoj |yo&lt;j , x) (Eq. (1)), where θ are model parameters to be optimized during training. Our learning objective is to minimize the negative data log-likelihood (Eq. (2)) to predict missing tokens yo∗j conditioned on the source text x and the summary tokens seen thus far yo&lt;j . We seek to produce a highly diverse set of alternative summaries from any source input, but standard M Y neural language generators with beam search only Pθ (y|x; o) = Pθ (yoj |yo&lt;j , x) (1) produce high-likelihood sequences rather than dij=1 verse ones (Ippolito et al., 2019). To address this M X limitation, we devise a new generator that is capaL(θ) = − log Pθ (yo∗j |yo&lt;j , x) (2) ble of producing summaries of varying lengths. A j=1 long summary can cover more important information of the source text, whereas a short summary Our generator is trained with a denoising objecis easy-to-read. Moreover, it produces a summary tive. It consists of a decoder-only architecture with having the exact given length and with a proper 12 Transformer blocks (Dong et al., 2019a). Given 1394 Input L=6 The Bank of Japan appealed to financial markets to remain calm Friday following t"
2021.naacl-main.110,2020.acl-main.703,0,0.091727,"Missing"
2021.naacl-main.110,2020.tacl-1.5,0,0.0463293,"Missing"
2021.naacl-main.110,D17-1222,0,0.0437632,"Missing"
2021.naacl-main.110,D16-1140,0,0.0229781,"al., 2019; Lebanoff et al., 2019a; Wang et al., 2020; Dong et al., 2020; Zhang et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al., 2017; Fan et al., 2018; Keskar et al., 2019; Makino et al., 2019; Song et al., 2020). In contrast, our generator focuses on producing multiple variants of the target summary that have diverse content and varying lengths. It offers precise control over the length of the summary, which has an important implication for fair comparison between different summarization systems (Napoles et al., 2011; Shapira et al., 2018). Our methodology allows for greater flexibility in designing summary selectors. The selectors may allow multiple admissible summaries to be identi• Our generator contro"
2021.naacl-main.110,D19-1051,0,0.110902,"Missing"
2021.naacl-main.110,2020.emnlp-main.750,0,0.1719,"Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning of the left-to-right generation. Our method dramatically original document or fail to convey the main con- enhances the capability of the generator, making it cepts (Kryscinski et al., 2020). possible to explore summaries of varying lengths. 1392 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1392–1404 June 6–11, 2021. ©2021 Association for Computational Linguistics Source Text: A court here Thursday sentenced a 24-year-old man to 10 years in jail after he admitted pummelling his baby son to death to silence him while watching television. Left to Right Generation (1 Summary) Man who Confidence Driven Generation (4 Summaries) Man gets 10 years Man who killed Man who kill the baby"
2021.naacl-main.110,D19-5413,1,0.873673,"arget summary that have varying lengths, then score and select the best summaries according to our needs. phasis on faithfulness to the original text. • Our experiments on benchmark summarization datasets suggest that this paradigm can surpass results of previous studies or rival state-of-the-art. We conclude with a discussion of our key findings, which has implications for the development of robust abstractive summarizers.1 2 Related Work It is important for neural abstractive summarizers to produce summaries that are faithful to the original texts (Cao et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 2019a; Wang et al., 2020; Dong et al., 2020; Zhang et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al.,"
2021.naacl-main.110,P19-1209,1,0.846629,"arget summary that have varying lengths, then score and select the best summaries according to our needs. phasis on faithfulness to the original text. • Our experiments on benchmark summarization datasets suggest that this paradigm can surpass results of previous studies or rival state-of-the-art. We conclude with a discussion of our key findings, which has implications for the development of robust abstractive summarizers.1 2 Related Work It is important for neural abstractive summarizers to produce summaries that are faithful to the original texts (Cao et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 2019a; Wang et al., 2020; Dong et al., 2020; Zhang et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al.,"
2021.naacl-main.110,2020.acl-main.24,0,0.117696,"iting of system summaries through a set of basic operations such as insertion and deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from these is our confidence-driven generation that goes beyond left-to-right order. It uses a denoising objective during training and is conveniently transformed into a semi-autoregressive generator at test time. We introduce a customized beam search algorithm to promote the generation of diverse outputs. In the following section, we describe in detail our two-step strategy. 3 A Confidence-Driven Generator endpoint. This is achieved by shifting away from left-to-right generation but building a"
2021.naacl-main.110,W04-1013,0,0.390095,"= log pθ (y|x), by its length |y|, with an exponent p (Eq. (3)). It is used by some neural abstractive summarizers (See et al., 2017; Lewis et al., 2020). However, the method does not consider the density of information in the source text and it may still generate ultra-short summaries. (3) R-1 32.67 36.15 36.27 34.19 37.04 35.98 38.45 38.73 38.90 39.12 35.51 36.71 39.27 R-2 15.59 17.54 17.57 16.92 19.03 17.76 19.53 19.71 20.05 19.86 16.33 17.27 20.40 R-L 30.64 33.63 33.62 31.81 34.46 33.63 36.04 35.96 36.00 36.24 32.75 33.63 36.76 Table 5: Results on the Gigaword test set evaluated by ROUGE (Lin, 2004).2 reaches the predicted length (|y |≤ Lpred ). Beyond that, increasing the length of the summary does not lead to additional rewards. We obtain the predicted length Lpred using a baseline abstractive summarizer, which takes the source text as input and greedily decodes a summary in a left-to-right manner until an end-of-sequence symbol is predicted; Lpred is the length of the decoding sequence. r is a coefficient to scale the reward and it is tuned on the validation data. Finally, the reward-augmented log-likelihood Srwd (x, y) is used as a scoring function to rank all summary hypotheses of v"
2021.naacl-main.110,P19-1500,0,0.0153651,"hful to the original text, ers do not write in a strict sequential manner. In a grammatical and fluent, though some of these prop- similar vein, our generator produces a summary by erties are exhibited by system abstracts as a natural “filling-in-the-blanks” with appropriate words. The outcome of a learned summarizer (See et al., 2017; most confident words are generated first, less vital Takase et al., 2016; Tan et al., 2017; Chen and ones later. With confidence-driven generation, our Bansal, 2018; Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning of the left-to-right generation. Our method dramatically original document or fail to convey the main con- enhances the capability of the generator, making it cepts (Kryscinski et al., 2020). possible to explore summaries of varying lengths. 1392 Proceedings of the 2021 Conference o"
2021.naacl-main.110,J13-2002,0,0.160938,"ector puts special emphasis on the overall quality of the system summary, in particular its faithfulness to the original text (Falke et al., 2019; Durmus et al., 2020). A novel dataset has been introduced in this work where we associate a source text with multiple summaries, and admissible ones are manually labelled by human annotators. Not only can the dataset be used to judge the effectiveness of summary selectors, but it provides a new testbed for future summarizers to compare their outputs against multiple reference summaries, which is key to improve the reliability of evaluation results (Louis and Nenkova, 2013). We have focused on generating abstractive summaries from single source sentences, but the insights gained from this study could inform the design of summarizers of all forms. Our method also has a great potential to incorporate human-in-theloop to teach the model to select the best summary. The main contributions of this paper are: • We propose a new approach to generate multiple variants of the target summary that have varying lengths, then score and select the best summaries according to our needs. phasis on faithfulness to the original text. • Our experiments on benchmark summarization da"
2021.naacl-main.110,P19-1099,0,0.0164606,"et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al., 2017; Fan et al., 2018; Keskar et al., 2019; Makino et al., 2019; Song et al., 2020). In contrast, our generator focuses on producing multiple variants of the target summary that have diverse content and varying lengths. It offers precise control over the length of the summary, which has an important implication for fair comparison between different summarization systems (Napoles et al., 2011; Shapira et al., 2018). Our methodology allows for greater flexibility in designing summary selectors. The selectors may allow multiple admissible summaries to be identi• Our generator controls over the length of the summary, which is especially well-suited when space"
2021.naacl-main.110,D19-1510,0,0.0155121,"redicting only the most probable next token in an autoregressive setting. We obtain the token that has the highest probability, and use it to replace the [ MASK ] token of that position. Next, the model makes new predictions for all remaining positions, conditioned on the source text and all summary tokens seen thus far. Our generator produces a summary having the exact given length and with a proper endpoint. fied for any source input according to users’ needs. On the contrary, post-editing of system summaries through a set of basic operations such as insertion and deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from the"
2021.naacl-main.110,2020.acl-main.173,0,0.0454532,"tal ones later. Our generator learns to dynamically add or remove content given a target length to produce summaries of varying lengths—short, medium and long. The output is a diverse set of alternative summaries. Identifying admissible summaries with desired properties is critical for a summarizer. Summaries of very short lengths may fail to capture the main concepts, and this kind of incomplete or partial information can lead to false assumptions about the original content. Moreover, summaries of moderate lengths may still contain hallucinated content that is nonexistent in the source text (Maynez et al., 2020). We present two summary selectors to combat these issues. Our first selector aims to predict what summary length is most suitable for a source text, whereas a second selector puts special emphasis on the overall quality of the system summary, in particular its faithfulness to the original text (Falke et al., 2019; Durmus et al., 2020). A novel dataset has been introduced in this work where we associate a source text with multiple summaries, and admissible ones are manually labelled by human annotators. Not only can the dataset be used to judge the effectiveness of summary selectors, but it pr"
2021.naacl-main.110,2020.emnlp-main.170,0,0.464511,"Missing"
2021.naacl-main.110,K16-1028,0,0.0661863,"Missing"
2021.naacl-main.110,W11-1611,0,0.0975345,"Missing"
2021.naacl-main.110,N18-1202,0,0.00792267,"d with a proper endpoint. fied for any source input according to users’ needs. On the contrary, post-editing of system summaries through a set of basic operations such as insertion and deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from these is our confidence-driven generation that goes beyond left-to-right order. It uses a denoising objective during training and is conveniently transformed into a semi-autoregressive generator at test time. We introduce a customized beam search algorithm to promote the generation of diverse outputs. In the following section, we describe in detail our two-step strategy. 3 A Confidence-Dri"
2021.naacl-main.110,D15-1044,0,0.13156,"Missing"
2021.naacl-main.110,P17-1099,0,0.826666,"It Crucially, we take a confidence-driven approach does not promote outputs that possess multiple de- to summary generation rather than using a left-tosirable properties, i.e., capturing the most impor- right order. Beginning writers and language learntant information, being faithful to the original text, ers do not write in a strict sequential manner. In a grammatical and fluent, though some of these prop- similar vein, our generator produces a summary by erties are exhibited by system abstracts as a natural “filling-in-the-blanks” with appropriate words. The outcome of a learned summarizer (See et al., 2017; most confident words are generated first, less vital Takase et al., 2016; Tan et al., 2017; Chen and ones later. With confidence-driven generation, our Bansal, 2018; Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning"
2021.naacl-main.110,P16-1162,0,0.00638956,"ed by ROUGE (Lin, 2004). We report R-1, R-2 and R-L F1 -scores that respectively measure the overlap of unigrams, bigrams, and longest common subsequences between system and reference summaries. For each summarization instance, our generator produces multiple alternative summaries, Experimental Setup Our generator is initialized ranging from L=7 to 16 tokens. E.g., “Daiwa Bank.” with RoBERTa-BASE (Liu et al., 2019) due to its corresponds to four tokens, ‘Dai’, ‘wa’, ‘Bank’ plus high performance on generation-related tasks. We an ending period. Our BEST- QUALITY and BESTuse Byte Pair Encoding (Sennrich et al., 2016) with LENGTH selectors each identifies a single best suma vocabulary of 50,265 tokens. The model con- mary from the set of alternative summaries for each tains 12 Transformer blocks (Vaswani et al., 2017), summarization instance. with a hidden size of 768 and 12 attention heads, We observe that the BEST- LENGTH selector has for a total of 110M parameters. We fine-tune the achieved the highest scores. It performs better than model on the train split of Gigaword and News- using any single target length for all summaries. room, respectively, before applying it to the test Among summaries of diffe"
2021.naacl-main.110,D18-1087,0,0.109061,"or faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al., 2017; Fan et al., 2018; Keskar et al., 2019; Makino et al., 2019; Song et al., 2020). In contrast, our generator focuses on producing multiple variants of the target summary that have diverse content and varying lengths. It offers precise control over the length of the summary, which has an important implication for fair comparison between different summarization systems (Napoles et al., 2011; Shapira et al., 2018). Our methodology allows for greater flexibility in designing summary selectors. The selectors may allow multiple admissible summaries to be identi• Our generator controls over the length of the summary, which is especially well-suited when space 1 is limited. Our selectors are designed to predict Our code and annotated data are made available on Github the optimal summary length and put special em- at https://github.com/ucfnlp/varying-length-summ 1393 Source Summary [CLS] … Step 1 [SEP] [CLS] … [SEP] [MASK] 0.51 0.22 0.27 [MASK] [MASK] 0.02 0.91 0.07 dog [MASK] 0.12 0.80 0.08 [MASK] [SEP] Voc"
2021.naacl-main.110,W19-2303,0,0.014072,"ng used for natural language inference (Chen et al., 2018). The vector h is fed to a feed-forward layer to predict whether the summary is admissible given the source text. We have chosen to design the selector as a classifier rather than a ranking model because there can exist multiple, equally valid summaries for any source input. The classifier allows us to identify admissible summaries that are not only true-to-original but has the best overall quality. 4.2 Finding a suitable length for the summary is one of the most important open problems in automatic summarization (Shapira et al., 2018; Sun et al., 2019). A summary should be shorter than the original, but long enough to include the most important information. Length normalization seeks to rescale the log-likelihood score of a summary, denoted by S(x, y) = log pθ (y|x), by its length |y|, with an exponent p (Eq. (3)). It is used by some neural abstractive summarizers (See et al., 2017; Lewis et al., 2020). However, the method does not consider the density of information in the source text and it may still generate ultra-short summaries. (3) R-1 32.67 36.15 36.27 34.19 37.04 35.98 38.45 38.73 38.90 39.12 35.51 36.71 39.27 R-2 15.59 17.54 17.57"
2021.naacl-main.110,D16-1112,0,0.0243372,"puts that possess multiple de- to summary generation rather than using a left-tosirable properties, i.e., capturing the most impor- right order. Beginning writers and language learntant information, being faithful to the original text, ers do not write in a strict sequential manner. In a grammatical and fluent, though some of these prop- similar vein, our generator produces a summary by erties are exhibited by system abstracts as a natural “filling-in-the-blanks” with appropriate words. The outcome of a learned summarizer (See et al., 2017; most confident words are generated first, less vital Takase et al., 2016; Tan et al., 2017; Chen and ones later. With confidence-driven generation, our Bansal, 2018; Celikyilmaz et al., 2018; Gehrmann summarizer learns to dynamically add or remove et al., 2018; Liu and Lapata, 2019; Lebanoff et al., content, and even paraphrase to produce a summary 2019b; Fabbri et al., 2019; Bražinskas et al., 2020). of a given length. In Table 2, we show an example Without direct optimization of desired properties, illustrating the difference between our method and system abstracts often change the meaning of the left-to-right generation. Our method dramatically original documen"
2021.naacl-main.110,P17-1108,0,0.0610425,"Missing"
2021.naacl-main.110,2020.acl-main.450,0,0.0432422,"varying lengths, then score and select the best summaries according to our needs. phasis on faithfulness to the original text. • Our experiments on benchmark summarization datasets suggest that this paradigm can surpass results of previous studies or rival state-of-the-art. We conclude with a discussion of our key findings, which has implications for the development of robust abstractive summarizers.1 2 Related Work It is important for neural abstractive summarizers to produce summaries that are faithful to the original texts (Cao et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 2019a; Wang et al., 2020; Dong et al., 2020; Zhang et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al., 2017; Fan et al., 2"
2021.naacl-main.110,P19-1207,0,0.0386256,"Missing"
2021.naacl-main.110,2020.findings-emnlp.217,0,0.0209792,"users’ needs. On the contrary, post-editing of system summaries through a set of basic operations such as insertion and deletion (Gu et al., 2019; Malmi et al., 2019; Dong et al., 2019b; Correia and Martins, 2019) may have intrinsic limitations by learning from single reference summaries to produce single outputs. In this paper, we provide a new dataset where each source text is associated with multiple admissible summaries to encourage diverse outputs. Our generator is inspired by unsupervised pretraining of deep neural models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yan et al., 2020; Zhang et al., 2020a; Lewis et al., 2020) and non-autoregressive machine translation (Gu et al., 2018; Ghazvininejad et al., 2019). Distinct from these is our confidence-driven generation that goes beyond left-to-right order. It uses a denoising objective during training and is conveniently transformed into a semi-autoregressive generator at test time. We introduce a customized beam search algorithm to promote the generation of diverse outputs. In the following section, we describe in detail our two-step strategy. 3 A Confidence-Driven Generator endpoint. This is achieved by shifting away fro"
2021.naacl-main.110,2020.acl-main.458,0,0.708646,"the best summaries according to our needs. phasis on faithfulness to the original text. • Our experiments on benchmark summarization datasets suggest that this paradigm can surpass results of previous studies or rival state-of-the-art. We conclude with a discussion of our key findings, which has implications for the development of robust abstractive summarizers.1 2 Related Work It is important for neural abstractive summarizers to produce summaries that are faithful to the original texts (Cao et al., 2017; Kryscinski et al., 2019; Lebanoff et al., 2019a; Wang et al., 2020; Dong et al., 2020; Zhang et al., 2020b). However, it remains questionable as to whether a summarizer must acquire that ability by learning from human reference summaries, or possibly through external resources such as textual entailment predictions (Falke et al., 2019). In this paper, we present a two-stage strategy to over-generate, then score system summaries externally for faithfulness and overall quality. Previous work has sought to control various aspects of the generated summary, including the style, length and amount of reused text (Kikuchi et al., 2016; Hu et al., 2017; Fan et al., 2018; Keskar et al., 2019; Makino et al."
2021.naacl-main.110,P17-1101,0,0.0402954,"Missing"
2021.naacl-srw.10,P18-1063,0,0.015735,"utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it"
2021.naacl-srw.10,D18-1443,0,0.0217647,"he continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summ"
2021.naacl-srw.10,D19-1620,0,0.290456,"ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summarizer may only process a relatively short transcript – about 5 minutes in duration. In this paper, we instead study an extractive meeting summarizer to identify salient utterances from the transcripts. It leverages a sliding window to navigate through a transcript of any length and a neural abstractive summarizer to find salient local conte"
2021.naacl-srw.10,N09-1041,0,0.078874,"AMI ASR team (Hain et al., 2006). We are able to generate a new version of automatic transcripts by using Google’s Speech-to-Text API as an off-the-shelf system.4 Comparing results on different versions of transcripts allows us to better assess the generality of our findings. Our baselines include both general-purpose extractive summarizers and meeting-specific summarizers. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based extractive methods. SumBasic (Vanderwende et al., 2007) selects sentences if they contain frequently occurring content words. KL-Sum (Haghighi and Vanderwende, 2009) adds sentences to the summary to minimize KL divergence. We additionally experiment with two meeting summarizers. Shang et al. (2018) group utterances into clusters, generate an abstractive sentence from each cluster using sentence compression, then select best elements from these sentences under a budget constraint. Koay et al. (2020) develop a supervised BERT summarizer to identify summary utterances. We report test set results in Table 1, where system summaries are compared with gold-standard extractive summaries using ROUGE metrics (Lin, 2004). The summary length is computed as the percen"
2021.naacl-srw.10,D19-5413,1,0.838254,"19; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summarizer may only process a relatively short transcript – about 5 minutes in duration. In this paper, we instead study an extractive meeting summarizer to identify salient"
2021.naacl-srw.10,2020.acl-main.703,0,0.0151333,"bly to competitive baselines given both automatic and human evaluations. We discuss how and to what extent the summarizer succeeds at capturing salient content.2 identify salient content on spoken text and, how it is to reduce lead bias that is not as frequent in spoken text as in news writing (Grenander et al., 2019). Secondly, a transcript can far exceed the maximum input length of the model, which is restricted by the GPU memory size. This is the case even for recent variants such as Reformer (Kitaev et al., 2020) and Longformer (Beltagy et al., 2020). Background: The BART Summarizer BART (Lewis et al., 2020) has demonstrated strong performance on neural abstractive summarization. It consists of a bidirectional encoder and a left-toright autoregressive decoder, each contains multiple layers of Transformers (Vaswani et al., 2017). The model is pretrained using a denoising objective that, given a corrupted input text, the encoder strives to learn meaningful representations and the decoder reconstructs the original text using the representations. In this study, we use BART-large-cnn as a base summarizer. It contains 12 layers in each of the encoder and decoder and uses a hidden size of 1024. The mode"
2021.naacl-srw.10,P19-1210,0,0.0189186,"i,xd.zangyiwu}@knights.ucf.edu feiliu@cs.ucf.edu Abstract Murray and Carenini, 2008; Gillick et al., 2009; Liu et al., 2009), detect meeting decisions (Hsueh and Moore, 2008), compress or merge utterances to generate abstracts (Liu and Liu, 2009; Wang and Cardie, 2013; Mehdad et al., 2013) and make use of acoustic-prosodic and speaker features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012) for utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Le"
2021.naacl-srw.10,W04-1013,0,0.0512549,"g content words. KL-Sum (Haghighi and Vanderwende, 2009) adds sentences to the summary to minimize KL divergence. We additionally experiment with two meeting summarizers. Shang et al. (2018) group utterances into clusters, generate an abstractive sentence from each cluster using sentence compression, then select best elements from these sentences under a budget constraint. Koay et al. (2020) develop a supervised BERT summarizer to identify summary utterances. We report test set results in Table 1, where system summaries are compared with gold-standard extractive summaries using ROUGE metrics (Lin, 2004). The summary length is computed as the percentage of selected utterances over all utterances of the meetings and average number of words per test summary. This information is reported wherever 3 The thresholds were determined heuristically on the training set by observing the resulting alignment. 4 Due to lack of documentation, we are unable to report the word error rates of Google and AMI speech recognizers. Consolidation. BART abstracts generated from local windows cannot be simply concatenated to form meeting-level summaries as they contain redundancy. When local windows are partially over"
2021.naacl-srw.10,D18-1208,0,0.0229723,"he time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summarizer may only process a relatively short transcript – about 5 minutes in duration. In this paper, we instead study an extractive meeting summarizer to identify salient utterances from the transcripts. It leverages a sliding window to navigate through a transcript of any length and a neural abstractive summarizer to"
2021.naacl-srw.10,P09-2066,1,0.796758,"Missing"
2021.naacl-srw.10,N09-1070,1,0.772088,"Missing"
2021.naacl-srw.10,2020.coling-main.499,1,0.817366,"ights.ucf.edu feiliu@cs.ucf.edu Abstract Murray and Carenini, 2008; Gillick et al., 2009; Liu et al., 2009), detect meeting decisions (Hsueh and Moore, 2008), compress or merge utterances to generate abstracts (Liu and Liu, 2009; Wang and Cardie, 2013; Mehdad et al., 2013) and make use of acoustic-prosodic and speaker features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012) for utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019"
2021.naacl-srw.10,D19-1051,0,0.0368448,"Missing"
2021.naacl-srw.10,2020.acl-main.173,0,0.0200507,"Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summarizer may only process a relatively short transcript – about 5 minutes in duration. In this paper, we instead study an extractive meeting summarizer to identify salient utterances from the tr"
2021.naacl-srw.10,W13-2117,0,0.0516042,"Missing"
2021.naacl-srw.10,W04-3252,0,0.0883317,"ated extractive summaries, which we use as gold-standard summaries. The original corpus include human transcripts and automatic speech recognition (ASR) output generated by the AMI ASR team (Hain et al., 2006). We are able to generate a new version of automatic transcripts by using Google’s Speech-to-Text API as an off-the-shelf system.4 Comparing results on different versions of transcripts allows us to better assess the generality of our findings. Our baselines include both general-purpose extractive summarizers and meeting-specific summarizers. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are graph-based extractive methods. SumBasic (Vanderwende et al., 2007) selects sentences if they contain frequently occurring content words. KL-Sum (Haghighi and Vanderwende, 2009) adds sentences to the summary to minimize KL divergence. We additionally experiment with two meeting summarizers. Shang et al. (2018) group utterances into clusters, generate an abstractive sentence from each cluster using sentence compression, then select best elements from these sentences under a budget constraint. Koay et al. (2020) develop a supervised BERT summarizer to identify summary utterances. We report"
2021.naacl-srw.10,D08-1081,0,0.150163,"Missing"
2021.naacl-srw.10,P13-1137,0,0.0597417,"Missing"
2021.naacl-srw.10,P19-1504,0,0.0200928,"pply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summarizer may only process a relatively short transcript – about 5 minutes in duration. In this paper, we instead study an extractive meeting summarizer to identify salient utterances from the transcripts. It leverages a sliding window to navigate through a transcript of any length and a neural abstractive summarizer to find salient local content. In particular, we aim to address three key questions: (1) what are suitable window and stride"
2021.naacl-srw.10,H91-1071,0,0.760351,"2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polifroni et al., 1991), it indicates that the summarizer may only process a relatively short transcript – about 5 minutes in duration. In this paper, we instead study an extractive meeting summarizer to identify salient utterances from the transcripts. It leverages a sliding window to navigate through a transcript of any length and a neural abstractive summarizer to find salient local content. In particular, we aim to address three key questions: (1) what are suitable window and stride sizes? (2) Meeting minutes record any subject matters discussed, decisions reached and actions taken at meetings. The importance of"
2021.naacl-srw.10,2020.findings-emnlp.19,0,0.0189151,"ct Murray and Carenini, 2008; Gillick et al., 2009; Liu et al., 2009), detect meeting decisions (Hsueh and Moore, 2008), compress or merge utterances to generate abstracts (Liu and Liu, 2009; Wang and Cardie, 2013; Mehdad et al., 2013) and make use of acoustic-prosodic and speaker features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012) for utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the m"
2021.naacl-srw.10,P17-1099,0,0.0223007,"Metze, 2012) for utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 2020) but also the models’ strong positional bias that occurs as a consequence of fine-tuning on news articles (Kedzie et al., 2018; Grenander et al., 2019). Neural summarizers also assume a maximum sequence length, e.g., Perez-Beltrachini et al. (2019) use the first 800 tokens of the document as input. With an estimated speaking rate of 122 words per minute (Polif"
2021.naacl-srw.10,P09-1062,0,0.101214,"Missing"
2021.naacl-srw.10,P18-1062,0,0.103711,"6 {jjkoay,alexroustai,xd.zangyiwu}@knights.ucf.edu feiliu@cs.ucf.edu Abstract Murray and Carenini, 2008; Gillick et al., 2009; Liu et al., 2009), detect meeting decisions (Hsueh and Moore, 2008), compress or merge utterances to generate abstracts (Liu and Liu, 2009; Wang and Cardie, 2013; Mehdad et al., 2013) and make use of acoustic-prosodic and speaker features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012) for utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski"
2021.naacl-srw.10,2020.coling-main.63,0,0.0227532,"u@cs.ucf.edu Abstract Murray and Carenini, 2008; Gillick et al., 2009; Liu et al., 2009), detect meeting decisions (Hsueh and Moore, 2008), compress or merge utterances to generate abstracts (Liu and Liu, 2009; Wang and Cardie, 2013; Mehdad et al., 2013) and make use of acoustic-prosodic and speaker features (Maskey and Hirschberg, 2005; Zhu et al., 2009; Chen and Metze, 2012) for utterance extraction. The continued development of automatic transcription and its easy accessibility have sparked a renewed interest in meeting summarization (Shang et al., 2018; Li et al., 2019; Koay et al., 2020; Song et al., 2020; Zhu et al., 2020; Zhong et al., 2021), where neural representations are explored for this task. We believe the time is therefore ripe for a reconsideration of the approach to automatic minuting. It may be tempting to apply neural abstractive summarization to meetings given its remarkable recent success on summarization benchmarks, e.g., CNN/DM (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Laban et al., 2020). However, the challenge lies not only in handling hallucinations that are seen in abstractive models (Kryscinski et al., 2019; Lebanoff et al., 2019; Maynez et al., 20"
2021.newsum-1.13,P11-1049,0,0.0413047,"sed content into an abstractive summary; and • through extensive experiments on multiple benchmark summarization datasets, we demonstrate the effectiveness of the endorsement method over state-of-the-art baselines. We make our code and models publicly available.1 2 Related Work Redundancy is essential in multi-document summarization. Without repetition and redundancy, even humans cannot agree on what information is salient and should be included in the summary (Daume III and Marcu, 2004). Optimizing summaries for frequency-based saliency has attained success prior to the era of deep learning (Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2012; Boudin et al., 2015). These extractive systems strive to include the most frequently occurring concepts in the summary. However, when it comes to abstractive summarization systems, the frequency of concepts is not fully utilized by modern neural models. 1 https://github.com/ucfnlp/endorser-summ Recent studies on MuDAS implicitly estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019)"
2021.newsum-1.13,P15-1153,0,0.0255265,"ource documents, and reiteration implies X increased salience. Any information that is present {s, e} = arg max (S(xk ) − δ) (13) {i,j}∈m k=i among multiple sources is likely to be important. Thus, our method identifies salient segments con- Soft vs. Hard Alignment A hard alignment besidering both within- and cross-document saliency. tween the synopsis and document can be obtained Our approach is in spirit similar to those of build- from string matching. A document token receives ing semantic concept graphs for multi-document a score of 1 if it finds a match in the synopsis. Simsummarization (Bing et al., 2015; Handler and ilar to above, we offset the scores by δ to obtain O’Connor, 2018; Falke and Gurevych, 2019) in segments of endorsed text. Hard alignment is senthat frequently reiterated concepts are likely to be sitive to entities and quantities; yet it can miss out captured. However, we do not explicitly construct on paraphrases. We compare the effectiveness of semantic concept graphs, but focus on modeling these alignment methods in the results section. synopsis-document endorsement and incorporating 4.2 Synopses as Endorsers it into summary generation, which distinguishes our work from these"
2021.newsum-1.13,D15-1220,0,0.0201205,"tensive experiments on multiple benchmark summarization datasets, we demonstrate the effectiveness of the endorsement method over state-of-the-art baselines. We make our code and models publicly available.1 2 Related Work Redundancy is essential in multi-document summarization. Without repetition and redundancy, even humans cannot agree on what information is salient and should be included in the summary (Daume III and Marcu, 2004). Optimizing summaries for frequency-based saliency has attained success prior to the era of deep learning (Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2012; Boudin et al., 2015). These extractive systems strive to include the most frequently occurring concepts in the summary. However, when it comes to abstractive summarization systems, the frequency of concepts is not fully utilized by modern neural models. 1 https://github.com/ucfnlp/endorser-summ Recent studies on MuDAS implicitly estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et"
2021.newsum-1.13,2020.emnlp-main.337,0,0.0434666,"Missing"
2021.newsum-1.13,P18-1063,0,0.0192822,"oader challenges of this task using a case study. Document Abstract Document loganlebanoff@knights.ucf.edu {bingqing.wang,zhe.feng2}@us.bosch.com feiliu@cs.ucf.edu Doc C Endorse Generate Figure 1: An example of synopsis-document relationships. Synopsis-document endorsements are leveraged to identify important text segments from a source document (e.g., Doc A). Strongly endorsed segments of all documents are consolidated into an abstractive summary. Multi-document Abstractive Summarization, i.e. MuDAS, remains a challenging problem compared to its single-document counterpart (See et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Raffel et al., 2020; Lewis et al., 2020). The task poses a substantial challenge to modern neural models: when the set of source documents is concatenated into a flat sequence, it may exceed the maximum length allowed by the GPU memory. There are also fewer datasets available to train MuDAS models in an end-to-end fashion. Recent work tackles this problem by selecting representative sentences from the source documents to reduce the task to single1 Introduction document summarization (Lebanoff et al., 2018; “Repeat a lie often enough and it becomes the truth.” Coavoux et"
2021.newsum-1.13,P19-1098,1,0.853391,"y estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et al. (2020) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salie"
2021.newsum-1.13,D19-5412,1,0.852669,"y estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et al. (2020) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salie"
2021.newsum-1.13,W19-4828,0,0.0231811,"on weights as a normalized dot product between query and key vectors. The output of the head is a weighted sum of value vectors. 3.2 Companion Heads We introduce a set of companion heads for each original head. All companion heads of z share the parameters {WzQ , WzK , WzV }, but a companion headz,τ j with an endorsement level of τ attends only to source tokens that are endorsed τ times or more. This is achieved with a special binary mask Miτ (Eqs. (9-10)). The original heads are believed to copy over source tokens that are deemed relevant to summary tokens according to the dependency syntax (Clark et al., 2019). The companion heads serve a similar purpose but have a narrower focus on endorsed source tokens—frequently endorsed tokens are more likely to be copied over by companion heads. The method thus improves head diversity similar to that of sparse Transformers (Correia et al., 2019; Huang et al., 2021). The hyperparameter τ controls the level of endorsement. Finally, all (l) heads are pooled into a hidden vector gbj (Eq. (11)) to be passed to the feedforward layer. (l−1) j ∈ [n] (6) (L) i ∈ [m] (7) qjz = WzQ gej kiz = WzK hi (L) viz = WzV hi i ∈ [m] (8) m z> z X exp(qj ki ) Pm headz,τ Miτ viz (9)"
2021.newsum-1.13,D19-5405,0,0.103999,"nsal, 2018; Narayan et al., 2018; Raffel et al., 2020; Lewis et al., 2020). The task poses a substantial challenge to modern neural models: when the set of source documents is concatenated into a flat sequence, it may exceed the maximum length allowed by the GPU memory. There are also fewer datasets available to train MuDAS models in an end-to-end fashion. Recent work tackles this problem by selecting representative sentences from the source documents to reduce the task to single1 Introduction document summarization (Lebanoff et al., 2018; “Repeat a lie often enough and it becomes the truth.” Coavoux et al., 2019; Fabbri et al., 2019). This proverb stresses the importance of repetition Nevertheless, there could be substantial informaand frequency in human comprehension. It causes tion loss if only representative sentences are used an endorsement effect that increases the salience for MuDAS. It becomes unclear what information of repeated information. In this paper, we lever- is reiterated and salient, resulting in unimportant age the endorsement effect to summarize multiple sentence parts being included in the summary. E.g., documents that discuss a particular event or topic when the sentence “World l"
2021.newsum-1.13,D19-1223,0,0.0530215,"Missing"
2021.newsum-1.13,W04-1016,0,0.179765,"Missing"
2021.newsum-1.13,P19-1102,0,0.0672904,"al., 2018; Raffel et al., 2020; Lewis et al., 2020). The task poses a substantial challenge to modern neural models: when the set of source documents is concatenated into a flat sequence, it may exceed the maximum length allowed by the GPU memory. There are also fewer datasets available to train MuDAS models in an end-to-end fashion. Recent work tackles this problem by selecting representative sentences from the source documents to reduce the task to single1 Introduction document summarization (Lebanoff et al., 2018; “Repeat a lie often enough and it becomes the truth.” Coavoux et al., 2019; Fabbri et al., 2019). This proverb stresses the importance of repetition Nevertheless, there could be substantial informaand frequency in human comprehension. It causes tion loss if only representative sentences are used an endorsement effect that increases the salience for MuDAS. It becomes unclear what information of repeated information. In this paper, we lever- is reiterated and salient, resulting in unimportant age the endorsement effect to summarize multiple sentence parts being included in the summary. E.g., documents that discuss a particular event or topic when the sentence “World leaders join to pledge"
2021.newsum-1.13,N19-1074,0,0.0124032,"e} = arg max (S(xk ) − δ) (13) {i,j}∈m k=i among multiple sources is likely to be important. Thus, our method identifies salient segments con- Soft vs. Hard Alignment A hard alignment besidering both within- and cross-document saliency. tween the synopsis and document can be obtained Our approach is in spirit similar to those of build- from string matching. A document token receives ing semantic concept graphs for multi-document a score of 1 if it finds a match in the synopsis. Simsummarization (Bing et al., 2015; Handler and ilar to above, we offset the scores by δ to obtain O’Connor, 2018; Falke and Gurevych, 2019) in segments of endorsed text. Hard alignment is senthat frequently reiterated concepts are likely to be sitive to entities and quantities; yet it can miss out captured. However, we do not explicitly construct on paraphrases. We compare the effectiveness of semantic concept graphs, but focus on modeling these alignment methods in the results section. synopsis-document endorsement and incorporating 4.2 Synopses as Endorsers it into summary generation, which distinguishes our work from these studies. We investigate two A synopsis contains the main points of the source variants to compute segment"
2021.newsum-1.13,hong-etal-2014-repository,0,0.022176,". WCEP has a single reference summary per cluster written by editors. The target summary length is 100 words for DUC/TAC and 40 words for WCEP, following the convention of previously published results. Endorsement-related statistics for these datasets are presented in Table 1. 6 Experiments Baseline Systems. We compare our endorsement method to strong multi-document summarization baselines. The extractive summarization systems include (i) TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are graph-based approaches that perform strongly on this task. (ii) Centroid (Hong et al., 2014) computes the importance of a source sentence based on its cosine similarity with the document centroid. (iii) Submodular (Lin and Bilmes, 2011) treats multidocument summarization as a submodular max5 Data imization problem. (iv) KL-Sum (Haghighi and We experiment with a large-scale multi-document Vanderwende, 2009) is a greedy approach that adds summarization dataset (Gholipour Ghalandari et al., sentences to the summary to minimize KL diver2020) whose data are gathered from the Wikipedia gence. (v) TSR and BertReg (Gholipour Ghalandari Current Events Portal (WCEP).3 The dataset con- et al.,"
2021.newsum-1.13,2021.naacl-main.112,0,0.0265912,"j with an endorsement level of τ attends only to source tokens that are endorsed τ times or more. This is achieved with a special binary mask Miτ (Eqs. (9-10)). The original heads are believed to copy over source tokens that are deemed relevant to summary tokens according to the dependency syntax (Clark et al., 2019). The companion heads serve a similar purpose but have a narrower focus on endorsed source tokens—frequently endorsed tokens are more likely to be copied over by companion heads. The method thus improves head diversity similar to that of sparse Transformers (Correia et al., 2019; Huang et al., 2021). The hyperparameter τ controls the level of endorsement. Finally, all (l) heads are pooled into a hidden vector gbj (Eq. (11)) to be passed to the feedforward layer. (l−1) j ∈ [n] (6) (L) i ∈ [m] (7) qjz = WzQ gej kiz = WzK hi (L) viz = WzV hi i ∈ [m] (8) m z> z X exp(qj ki ) Pm headz,τ Miτ viz (9) j = z> z r=0 exp(qj kr ) i=0  1 if Endorse(xi ) ≥ τ Miτ = (10) 0 otherwise n τmax head X X (l) τ gbj = headz,τ (11) j Wz z=1 τ =0 When τmax is set to 0, the model reduces to its initial form using the original heads, i.e., headz,0 j . τ τ Further, we initialize Wz = λ Wz , where Wz ∈ hhead ×hmodel"
2021.newsum-1.13,2020.findings-emnlp.231,0,0.0332556,"ity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias (Grenander et al., 2"
2021.newsum-1.13,D19-1051,0,0.0459031,"Missing"
2021.newsum-1.13,D19-1620,0,0.0177769,"; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias (Grenander et al., 2019; Xing et al., 2021), which hinders the development of multiple-document summarization. Our salience estimates are obtained from a frequency-driven endorsement model. Below we present details of the proposed method. 3 Summarization with Endorsement We approach the MuDAS problem in two stages. First, we obtain fine-grained segment-level endorsement for any candidate document. By excluding unendorsed sentences from consideration, we reduce the set of documents to a single input document. We next present an enhanced abstractive summarization model to consolidate the document into a succinct abstr"
2021.newsum-1.13,N09-1041,0,0.175103,"Missing"
2021.newsum-1.13,2020.acl-main.460,0,0.0282003,"raphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias (Grenander et al., 2019; Xing et al., 20"
2021.newsum-1.13,N18-1159,0,0.0232396,"Missing"
2021.newsum-1.13,D18-1446,1,0.87469,"Missing"
2021.newsum-1.13,2020.acl-main.703,0,0.311554,"t Document loganlebanoff@knights.ucf.edu {bingqing.wang,zhe.feng2}@us.bosch.com feiliu@cs.ucf.edu Doc C Endorse Generate Figure 1: An example of synopsis-document relationships. Synopsis-document endorsements are leveraged to identify important text segments from a source document (e.g., Doc A). Strongly endorsed segments of all documents are consolidated into an abstractive summary. Multi-document Abstractive Summarization, i.e. MuDAS, remains a challenging problem compared to its single-document counterpart (See et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Raffel et al., 2020; Lewis et al., 2020). The task poses a substantial challenge to modern neural models: when the set of source documents is concatenated into a flat sequence, it may exceed the maximum length allowed by the GPU memory. There are also fewer datasets available to train MuDAS models in an end-to-end fashion. Recent work tackles this problem by selecting representative sentences from the source documents to reduce the task to single1 Introduction document summarization (Lebanoff et al., 2018; “Repeat a lie often enough and it becomes the truth.” Coavoux et al., 2019; Fabbri et al., 2019). This proverb stresses the impo"
2021.newsum-1.13,2020.webnlg-1.12,0,0.0187078,"2015). These extractive systems strive to include the most frequently occurring concepts in the summary. However, when it comes to abstractive summarization systems, the frequency of concepts is not fully utilized by modern neural models. 1 https://github.com/ucfnlp/endorser-summ Recent studies on MuDAS implicitly estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et al. (2020) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zha"
2021.newsum-1.13,W04-1013,0,0.030362,"from many synopses. worthy content from multiple documents is beneficial for an abstractive model. Moreover, removing the “abstractive model”— meaning summaries are created extractively by selecting the highest-endorsed sentences—results in a large decrease in scores. It indicates that contentselection by endorsement cannot be done alone without an abstractor to create a more concise summary. This is especially the case for WCEP, where human reference summaries are relatively short. We additionally report BERTScore (Zhang et al., 2020b) to evaluate summaries, in addition to the ROUGE metric (Lin, 2004). BERTScore uses cosine similarity between BERT contextual embeddings of words to detect word overlap between two texts, thus overcoming the problem of lexical variation in summarization. On DUC-04, the F1 scores are 29.89 and 30.14, respectively for our sequential and reciprocal model. The score for human reference summary is 35.08. They show very similar trends to those in Table 3, suggesting that our method when tested in out-of-domain scenarios can achieve competitive results. where semantic content units (SCUs) are identified from the reference summaries and are matched to phrases in the"
2021.newsum-1.13,P11-1052,0,0.0382218,"P, following the convention of previously published results. Endorsement-related statistics for these datasets are presented in Table 1. 6 Experiments Baseline Systems. We compare our endorsement method to strong multi-document summarization baselines. The extractive summarization systems include (i) TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are graph-based approaches that perform strongly on this task. (ii) Centroid (Hong et al., 2014) computes the importance of a source sentence based on its cosine similarity with the document centroid. (iii) Submodular (Lin and Bilmes, 2011) treats multidocument summarization as a submodular max5 Data imization problem. (iv) KL-Sum (Haghighi and We experiment with a large-scale multi-document Vanderwende, 2009) is a greedy approach that adds summarization dataset (Gholipour Ghalandari et al., sentences to the summary to minimize KL diver2020) whose data are gathered from the Wikipedia gence. (v) TSR and BertReg (Gholipour Ghalandari Current Events Portal (WCEP).3 The dataset con- et al., 2020) are regression-based sentence ranking tains an archive of important news events happen- methods using averaged word embeddings (TSR) ing a"
2021.newsum-1.13,P19-1500,0,0.0190254,"ed in the summary (Daume III and Marcu, 2004). Optimizing summaries for frequency-based saliency has attained success prior to the era of deep learning (Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2012; Boudin et al., 2015). These extractive systems strive to include the most frequently occurring concepts in the summary. However, when it comes to abstractive summarization systems, the frequency of concepts is not fully utilized by modern neural models. 1 https://github.com/ucfnlp/endorser-summ Recent studies on MuDAS implicitly estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et al. (2020) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequ"
2021.newsum-1.13,2020.emnlp-main.136,0,0.0425482,"using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et al. (2020) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are par"
2021.newsum-1.13,W04-3252,0,0.146385,"of the model’s generality in out-of-domain scenarios. DUC and TAC datasets contain four reference summaries per cluster created by NIST evaluators. WCEP has a single reference summary per cluster written by editors. The target summary length is 100 words for DUC/TAC and 40 words for WCEP, following the convention of previously published results. Endorsement-related statistics for these datasets are presented in Table 1. 6 Experiments Baseline Systems. We compare our endorsement method to strong multi-document summarization baselines. The extractive summarization systems include (i) TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are graph-based approaches that perform strongly on this task. (ii) Centroid (Hong et al., 2014) computes the importance of a source sentence based on its cosine similarity with the document centroid. (iii) Submodular (Lin and Bilmes, 2011) treats multidocument summarization as a submodular max5 Data imization problem. (iv) KL-Sum (Haghighi and We experiment with a large-scale multi-document Vanderwende, 2009) is a greedy approach that adds summarization dataset (Gholipour Ghalandari et al., sentences to the summary to minimize KL diver2020) whose da"
2021.newsum-1.13,D18-1206,0,0.0205275,"s task using a case study. Document Abstract Document loganlebanoff@knights.ucf.edu {bingqing.wang,zhe.feng2}@us.bosch.com feiliu@cs.ucf.edu Doc C Endorse Generate Figure 1: An example of synopsis-document relationships. Synopsis-document endorsements are leveraged to identify important text segments from a source document (e.g., Doc A). Strongly endorsed segments of all documents are consolidated into an abstractive summary. Multi-document Abstractive Summarization, i.e. MuDAS, remains a challenging problem compared to its single-document counterpart (See et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Raffel et al., 2020; Lewis et al., 2020). The task poses a substantial challenge to modern neural models: when the set of source documents is concatenated into a flat sequence, it may exceed the maximum length allowed by the GPU memory. There are also fewer datasets available to train MuDAS models in an end-to-end fashion. Recent work tackles this problem by selecting representative sentences from the source documents to reduce the task to single1 Introduction document summarization (Lebanoff et al., 2018; “Repeat a lie often enough and it becomes the truth.” Coavoux et al., 2019; Fabbri et"
2021.newsum-1.13,C18-1102,0,0.0166737,"ization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias (Grenander et al., 2019; Xing et al., 2021), which hinders the development of multiple-document summarization. Our salience estimates are obtained from a frequency-driven endorsement model. Below we present details of the proposed method. 3 Summarization with Endorsement We approach the MuDAS problem in two stages. First, we obtain fine-grained segment-level endorsement for any candidate document. By excluding unendorsed sentences from consideration, we reduce the set of"
2021.newsum-1.13,N04-1019,0,0.257238,",” are more readily endorsed 6.3 A Case Study than other phrases. These entities are frequently repeated verbatim in all of the documents, thereby We present an in-depth analysis of our fine-grained increasing their likelihood of being endorsed. endorsement in Table 6. Soft alignment is used to endorse a candidate document from synopses of We envision future neural document summarizathe cluster. We compare the resulting endorsements tion systems to produce better synopses than BART. to the text segments chosen by a human using the They can lead to more accurate estimates for enPyramid method (Nenkova and Passonneau, 2004), dorsed segments, hence improving the overall per126 formance of our multi-document summarizer. The endorsement mechanism at its core is simple and robust—looking for shared content between a document and a synopsis. It provides great flexibility allowing the summarizer to potentially operate on document clusters containing a varying number of documents, which is a desirable characteristic. 7 Conclusion We presented a novel framework to model asynchronous endorsement between synopses and documents for multi-document abstractive summarization. We introduced an endorsement method to enrich the"
2021.newsum-1.13,P19-1504,0,0.018137,"g (Berg-Kirkpatrick et al., 2011; Kulesza and Taskar, 2012; Boudin et al., 2015). These extractive systems strive to include the most frequently occurring concepts in the summary. However, when it comes to abstractive summarization systems, the frequency of concepts is not fully utilized by modern neural models. 1 https://github.com/ucfnlp/endorser-summ Recent studies on MuDAS implicitly estimate frequency using hierarchical encoders / decoders. Liu and Lapata (2019) encode the documents using hierarchical Transformers where cross-document relationships are characterized by attention weights. Perez-Beltrachini et al. (2019) explore structured convolutional decoders. Li et al. (2020) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain"
2021.newsum-1.13,2020.tacl-1.54,0,0.0403915,"Missing"
2021.newsum-1.13,P17-1099,0,0.326648,"ons and discuss broader challenges of this task using a case study. Document Abstract Document loganlebanoff@knights.ucf.edu {bingqing.wang,zhe.feng2}@us.bosch.com feiliu@cs.ucf.edu Doc C Endorse Generate Figure 1: An example of synopsis-document relationships. Synopsis-document endorsements are leveraged to identify important text segments from a source document (e.g., Doc A). Strongly endorsed segments of all documents are consolidated into an abstractive summary. Multi-document Abstractive Summarization, i.e. MuDAS, remains a challenging problem compared to its single-document counterpart (See et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018; Raffel et al., 2020; Lewis et al., 2020). The task poses a substantial challenge to modern neural models: when the set of source documents is concatenated into a flat sequence, it may exceed the maximum length allowed by the GPU memory. There are also fewer datasets available to train MuDAS models in an end-to-end fashion. Recent work tackles this problem by selecting representative sentences from the source documents to reduce the task to single1 Introduction document summarization (Lebanoff et al., 2018; “Repeat a lie often enough and it becomes"
2021.newsum-1.13,2021.acl-short.119,0,0.0341838,"an et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias (Grenander et al., 2019; Xing et al., 2021), which hinders the development of multiple-document summarization. Our salience estimates are obtained from a frequency-driven endorsement model. Below we present details of the proposed method. 3 Summarization with Endorsement We approach the MuDAS problem in two stages. First, we obtain fine-grained segment-level endorsement for any candidate document. By excluding unendorsed sentences from consideration, we reduce the set of documents to a single input document. We next present an enhanced abstractive summarization model to consolidate the document into a succinct abstract, analogously to"
2021.newsum-1.13,2021.acl-long.539,0,0.0332429,"levance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias (Grenander et al., 2019; Xing et al., 2021), which hinders the development of multiple-document summarization. Our salience estimates are obtained from a frequency-driven endorsement model."
2021.newsum-1.13,P19-1499,0,0.0211862,"20) leverage similarity and discourse graphs to alter the attention mechanism of encoder-decoder models. Researchers have also attempted optimization algorithms such as maximal margin relevance and determinantal point processes combined with contextualized representations and reinforcement learning (Cho et al., 2019a,b; Mao et al., 2020). Despite promising progress, modeling frequency for multidocument summarization remains an open problem, in part because neural summarization models are often pretrained on single documents that contain little or no redundant content (Kryscinski et al., 2019; Zhang et al., 2019; Jin and Wan, 2020; Laban et al., 2020; Zhang et al., 2020a). Named entities and quantities that represent salient information details are not properly accounted for (Xu and Durrett, 2021). If we do not explicitly model frequency, abstractive summarizers may fail to adequately recognize such salient details. We are particularly interested in reducing multiple input documents to a single document, then consolidate the content into a succinct abstract (Nayeem et al., 2018; Coavoux et al., 2019). Our method enhances the single document with fine-grained segment salience to offset the lead bias ("
2021.newsum-1.13,D19-1053,1,0.845146,"al is to estimate whether a token xi of the document is endorsed by the synopsis. A soft alignment between the synopsis and document is attainable by utilizing text evaluation metrics such as BERTScore (Zhang et al., 2020b), where we build contextualized embeddings for tokens of the document and synopsis, compute the cosine similarity of embeddings, and find a most similar synopsis token for each token of the document to obtain the endorsement score S(xi ) (Eq. (12)). Albeit a greedy alignment, the method can produce competitive results comparing to methods such as the earth mover’s distance (Zhao et al., 2019). In this section, we present the first stage in our S(xi ) = max Sim(xi , yj ) (12) yj ∈S approach – modelling endorsement – whose outContiguous Segments It is important to endorse puts are passed to the abstractive summarization segments of text rather than isolated tokens, as segmodel in the second stage. Modelling endorsement ments such as “$8 million” is either included in the serves two main purposes. It allows us to identify abstract in its entirety, or not at all. We transform salient segments of text using a frequency-driven token-level endorsement scores into binary deciendorsement m"
C14-1084,P11-1054,0,0.0123665,"orty iterations. After estimating parameters, we decode using the Viterbi algorithm. 4 Experiments Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph and the section level: • A greedy divising clustering algorithm, as implemented in CLUTO.12 The algorithm performs a sequence of bisections until the desired number of clusters is reached. In each step, a cluster is selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et al., 2011). • The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten runs, with random initialization. • The Viterbi state assignment after VB inference, using the mean field parameters. We report averaged results over ten runs, with random initialization. Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph pairs annotated “yes.” 4.1 Results In Figure 1, we present performance of different algorithms using a range of hidden state values K ∈ {1, 2, . . . , 20}. The top row shows precision, recall and F -scores on se"
C14-1084,D08-1035,0,0.0113779,"amm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve agreement on par with the lower half of crowdworkers, with about half of the difference from the median due to the bag of words representation and half due to the inherent greediness of the methods. Acknowledgments The authors g"
C14-1084,D08-1036,0,0.0676509,"VB We consider two estimation methods, neither novel. Both are greedy hillclimbing methods that locally optimize functions based on likelihood under the HMM. The first method is EM, adapted for the multiple emission case; the equations for the E-step (forwardbackward algorithm and subsequent posterior calculations) and the M-step are shown in Table 5. We also consider Bayesian inference, which seeks to marginalize out the parameter values, since we are really only interested in the assignment of sections to hidden states. Further, Bayesian inference has been found favorable on small datasets (Gao and Johnson, 2008). We assume symmetric Dirichlet priors on the transition and emission distributions, parameterized respectively by λ = 1 and λ0 = 0.1. We apply mean-field variational approximate inference as described by Beal (2003), which amounts to an EM-like procedure. The E-step is identical to EM, and the M-step involves a transformation of the expected counts, shown in Table 5. (We also explored Gibbs sampling; performance was less stable but generally similar; for clarity we do not report the results here.) 889 3.3 Implementation Details In modeling, the vocabulary excludes 429 stopwords,11 words whose"
C14-1084,P07-1094,0,0.0111223,"ial information, sharing with third parties, and deletion of data. This expectation is supported by recommendation by privacy experts (Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector, the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering and HMM-based alignment methods. Our results show that these algor"
C14-1084,D07-1031,0,0.0163184,"hird parties, and deletion of data. This expectation is supported by recommendation by privacy experts (Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector, the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve a"
C14-1084,P11-1052,0,0.0328184,"ently happens within forty iterations. After estimating parameters, we decode using the Viterbi algorithm. 4 Experiments Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph and the section level: • A greedy divising clustering algorithm, as implemented in CLUTO.12 The algorithm performs a sequence of bisections until the desired number of clusters is reached. In each step, a cluster is selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et al., 2011). • The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten runs, with random initialization. • The Viterbi state assignment after VB inference, using the mean field parameters. We report averaged results over ten runs, with random initialization. Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph pairs annotated “yes.” 4.1 Results In Figure 1, we present performance of different algorithms using a range of hidden state values K ∈ {1, 2, . . . , 20}. The top row shows precision, recall"
C14-1084,J94-2001,0,0.00885974,"on of a user’s contact, location, health, and financial information, sharing with third parties, and deletion of data. This expectation is supported by recommendation by privacy experts (Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector, the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering"
C14-1084,P14-2099,1,0.300019,"nce of the mobile Web and the Internet of Things, with early efforts in this area including the use of static analysis to identify sensitive data flows in mobile apps (Lin et al., 2012) and the development of mobile app privacy profiles (Liu et al., 2014). Increased automation for such efforts motivates our interest in privacy policies as a text genre for NLP, with the general goal of supporting both user-oriented tools that interpret policies and studies of the contents of policies by legal scholars. In this paper, we start with a corpus of 1,010 policies collected from widely-used websites (Ramanath et al., 2014),1 and seek to automatically align segments of policies. We believe this is a worthwhile first This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.usableprivacy.org/data 884 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 884–894, Dublin, Ireland, August 23-29 2014. Amazon.com Privacy Notice ... What About Cookies? Cookies are unique identifiers that we tra"
C16-1006,W16-3605,0,0.0518934,"Missing"
C16-1006,Q13-1032,0,0.179007,"e. We have only demonstrated the highlights of Human Summary 1 to avoid overlaying of two sets of colors on student responses. The superscripts of the phrase highlights are imposed by the authors of this paper to differentiate colors when printed in grayscale (y: yellow , g: green , r: red , b: blue , and m: magenta ). setting. Lastly, a greedy clustering algorithm K-medoids (Kaufman and Rousseeuw, 1987) was previously used to group candidate phrases. It ignores global information and may suffer from a “collapsing” effect, which leads to the generation of a large cluster with unrelated items (Basu et al., 2013). The goal of this work is to explore a phrase-based highlighting scheme, which is new to the summarization task. We aim to improve the phrase summarization framework by exploiting new capabilities that are enabled by the highlighting scheme. In the new scheme, human annotators are instructed to 1) create summary phrases from the student responses, 2) associate a number with each summary phrase which indicates the number of students who raise the issue (henceforth student supporters), and 3) highlight the corresponding phrases in both the human summary and student responses. Table 1 illustrate"
C16-1006,P11-1049,0,0.0163738,"ty detection to group phrases into clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 200"
C16-1006,W14-5909,0,0.0249645,"Missing"
C16-1006,stefanescu-etal-2014-latent,0,0.0350288,"Missing"
C16-1006,P16-1188,0,0.0153082,"xt quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a"
C16-1006,P14-1119,0,0.0137364,"i et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent words/phrases. A summarization system is expec"
C16-1006,W11-1104,0,0.0233764,"Missing"
C16-1006,W15-3601,0,0.0244842,"document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent words/phrases. A summarization system is expected to produ"
C16-1006,P10-1052,0,0.0276573,"Missing"
C16-1006,D13-1047,1,0.854627,"nto clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2"
C16-1006,W04-1013,0,0.190731,"utomatic summarization, a departure from prior work. It highlights the phrases in the human summary and also the semantically similar phrases in student responses. We create a new dataset annotated with this highlighting scheme1 . • We push the boundary of a phrase-based summarization framework by using our highlighting scheme to enable identification of candidate phrases as well as estimation of phrase similarities with supervision, and by using community detection to group phrases into clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2"
C16-1006,D09-1027,0,0.0351284,"h, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent wor"
C16-1006,N15-1114,1,0.809757,"aluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that"
C16-1006,loza-etal-2014-building,0,0.0247957,"om student responses, however there lacks a comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alle"
C16-1006,D15-1227,1,0.744514,"ack submitted by students after each lecture in response to two reflective prompts (Boud et al., 2013): 1) “Describe what you found most interesting in today’s class” and 2) “Describe what was confusing or needed more detail.” Education researchers have demonstrated that asking students to respond to reflection prompts can improve both teaching and learning (Van den Boom et al., 2004; Menekse et al., 2011). However, summarizing these responses for large classes (e.g., introductory STEM, MOOCs) remains costly, time-consuming, and an onerous task for humans (Mosteller, 1989). In our prior work, Luo and Litman (2015) (henceforth L&L) introduced the task of automatic summarization of student responses. The challenges of this task include 1) high lexical variety, because students tend to use different word expressions to communicate the same or similar meanings (e.g., “bike elements” vs. “bicycle parts”), and 2) high length variety, as the student responses range from a single word to multiple sentences. To tackle the challenges, L&L proposed a phrase summarization framework consisting of three stages: phrase extraction, phrase clustering, and phrase ranking. The approach extracts noun phrases from student"
C16-1006,N15-3004,1,0.644177,"o sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside"
C16-1006,N16-1010,1,0.836379,"d accurate estimates of the number of student supporters for each phrase. Luo and Litman (2015) focus on extracting noun phrases from student responses, however there lacks a comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches s"
C16-1006,W09-1801,0,0.0249462,"ion, and by using community detection to group phrases into clusters. • We conduct comprehensive evaluations in terms of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et"
C16-1006,D09-1137,0,0.0269337,"patrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source text. The downside is that they do not take lexical variety into account or considering semantically-equivalent words/phrases. A summariza"
C16-1006,W13-2117,0,0.0366518,"Missing"
C16-1006,K16-1028,0,0.0402006,"y ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicati"
C16-1006,N04-1019,0,0.0605178,"proaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alleviate the challenge. The new highlighting scheme described in this work holds promise for establishing direct links between the phrases in student responses and those in the human summary, allowing us to develop a new evaluation metric based on color matching. 3 New Data and Annotation When reviewing the student feedback, we observe that not all issues are equally important. Some teaching problems are more prominent than others. Summary phrases should naturally reflect the number of students who raise the iss"
C16-1006,P02-1040,0,0.0956881,"(S¸tef˘anescu et al., 2014). One drawback of this approach is that the similarity of phrases that do not appear in a background corpus cannot be captured. In this work we develop an ensemble of similarity metrics by feeding them into a supervised classification framework. We use the phrase highlights as supervision, where phrases of the same color are positive examples and those of different colors are negative examples. We experiment with a range of metrics for measuring lexical similarity, including lexical overlap (Rus et al., 2013), cosine similarity, LIN similarity (Miller, 1995), BLEU (Papineni et al., 2002), SimSum (Lin, 2004), Word Embedding (Goldberg and Levy, 2014), and LSA (Deerwester et al., 1990). LIN similarity is based on WordNet definitions. Lexical overlap, cosine similarity, BLEU, and SimSum are related to how many words the two phrases have in common, while Word Embedding and LSA both capture the phrase similarity in a low dimensional semantic space. Therefore, we use an ensemble of the above similarity metrics by feeding them as features in a SVM classification model, assuming it will be better suited for this task than the LSA alone. Table 5 presents the intrinsic evaluation result"
C16-1006,P11-1110,0,0.0221478,"Missing"
C16-1006,P13-4028,0,0.0141991,"ntial role in phrase-based summarization. Better similarity learning helps produce better phrase clusters, which in turn leads to more accurate estimation of the number of student supporters for each summary phrase. While a human annotator 4 We use the implementation of Wapiti (Lavergne et al., 2010) with default parameters. 57 could distinguish the semantic similarity or dissimilarity of the phrase highlights, it remains unclear if a single similarity metric could fulfill this goal or if we may need an ensemble of different metrics. L&L calculate the pairwise phrase similarity using SEMILAR (Rus et al., 2013) with the latent semantic analysis (LSA) trained on the Touchstone corpus (S¸tef˘anescu et al., 2014). One drawback of this approach is that the similarity of phrases that do not appear in a background corpus cannot be captured. In this work we develop an ensemble of similarity metrics by feeding them into a supervised classification framework. We use the phrase highlights as supervision, where phrases of the same color are positive examples and those of different colors are negative examples. We experiment with a range of metrics for measuring lexical similarity, including lexical overlap (Ru"
C16-1006,D15-1044,0,0.0204712,"of both summary text quality, measured by ROUGE (Lin, 2004), and how well phrase summaries capture the most pressing student needs, measured by a new evaluation metric based on color matching. 1 This data set is publicly available at http://www.coursemirror.com/download/dataset2 54 2 Related Work Work on automatic text summarization involves multiple granularities, ranging from keywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase"
C16-1006,C00-2127,0,0.0986794,"eywords, phrases, to sentences. Traditional approaches have largely focused on sentence extraction (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2013) and document abstraction (Liu et al., 2015; Rush et al., 2015; Durrett et al., 2016; Nallapati et al., 2016). In both cases, the produced summary is expected to be cohesive and coherent. We deviate from this path and seek to directly generate a set of bullet points as a summary. Phrases are easy to search and browse like words but more meaningful, and fit better on the small screen of a mobile device compared to sentences (Ueda et al., 2000; Luo et al., 2015). Our task setting differs from those of keyphrase extraction (Wu et al., 2005; Liu et al., 2009; Medelyan et al., 2009; Hasan and Ng, 2014; Kan, 2015). Of key importance is that each summary phrase is associated with a numerical value, indicating the number of students who raise the issue. This information is critical to course instructors for making informed choices. Intuitively our task setting bears similarity to word/phrase cloud (Yatani et al., 2011; Brooks et al., 2014), where the cloud gives greater prominence to words or phrases that appear frequently in the source"
C16-1006,N16-1007,0,0.0130269,"comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alleviate the challenge. The new highlighting sche"
C16-1006,C14-1187,1,0.831097,"s, however there lacks a comprehensive evaluation of the results, taking the number of student supporters into account. Other related work on student responses includes collecting student responses using a mobile application named CourseMIRROR (Luo et al., 2015; Fan et al., 2015), determining the quality of a student reflective response and providing feedback (Luo and Litman, 2016), and extracting informative sentences from the student feedback (Luo et al., 2016). Traditional approaches to summary annotation have been based on either sentence extracts or document abstracts (Loza et al., 2014; Xiong and Litman, 2014; Wang and Ling, 2016). An effective linkage between the document content and human summary on the micro level have been largely absent. Barker et al.(2016) partially address this challenge by linking a summary back to a group of sentences that support the summary. However, this linkage is weak since it tells only that there is one sentence or more supporting the summary within the group, without explicitly telling which one(s). Approaches such as Pyramid (Nenkova and Passonneau, 2004) have exploited creating Summary Content Units (SCUs) to establish such links and alleviate the challenge. The"
C18-1101,D17-1130,0,0.0145116,"tural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the sou"
C18-1101,W13-2322,0,0.385348,"mples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semantic formalism based on propositional logic and the neo-Davidsonian event representation (Parsons, 1990; Schein, 1993). It represents the meaning of a sentence using a rooted, directed, and acyclic graph, where nodes are concepts and edges are semantic relations. Figure 1 shows an example AMR graph. A concept node can be a PropBank frameset (“state-01”), an English word (“warhead”), a special keyword (“date-entity”), or a string literal (“Japan”). A relation can be either a core argument (“ARG0,” “ARG1”) or a modification relationship (“mod,”"
C18-1101,P99-1071,0,0.434216,"are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural language generation (NLG, Reiter and Dale, 2000; Gatt and Krahmer, 2018) is a promising line of work. The approaches often identify salient text units from source documents, arrange them in a compact form, such as domain-specific templates, and subsequently synthesize them into natural language texts (Barzilay et al., 1999; Genest and Lapalme, 2011; Oya et al., 2014; Gerani et al., 2014; Fabbrizio et al., 2014). A challenge faced by these approaches is that there 1179 multiple sets of similar sentences covering different aspects of the source Content Planning Source Sentence Selection a cluster of source documents discussing the same topic … … Content Planning Surface Realization … an abstractive summary containing multiple summary sentences … Surface Realization a set of summary graphs serving as content representation of the source Figure 2: Our system framework, consisting of three major components. lacks a"
C18-1101,P15-1153,0,0.0387043,"n (Paulus et al., 2017; See et al., 2017). These approaches are limited by the availability of training data, and large datasets for multi-document summarization can be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semanti"
C18-1101,P17-1112,0,0.0182108,"ary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured"
C18-1101,P13-2131,0,0.103986,"rence sentence, we further extract a set of source sentences. They are judged similar to the reference sentence via a similarity metric. The summary AMR graphs and sets of source sentences thus form the training data for content planning. We gauge how best to select source sentences by exploring different similarity metrics. In particular, (i) LCS calculates the longest common subsequence between a candidate source sentence and the reference sentence; (ii) VSM represents sentences using the vector space model and calculates the cosine similarity between the two sentence vectors; (iii) Smatch (Cai and Knight, 2013) calculates the F-score of AMR concepts between the candidate and reference sentences; (iv) Concept Coverage selects source sentences to maximize the coverage of AMR concepts of the reference sentence. We experiment with these source sentence selection strategies and compare their effectiveness in Section §5.1. 4 Datasets and Baselines We perform experiments on standard multi-document summarization datasets2 , prepared by the NIST researchers for DUC/TAC competitions and later exploited by various summarization studies (Nenkova and McKeown, 2011; Hong et al., 2014; Yogatama et al., 2015). A su"
C18-1101,D14-1085,0,0.0245326,"y of training data, and large datasets for multi-document summarization can be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semantic formalism based on propositional logic and the neo-Davidsonian event representation (Parson"
C18-1101,E17-1051,0,0.0305965,"-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface r"
C18-1101,W14-4408,0,0.100922,"Missing"
C18-1101,P14-1134,0,0.311201,"ensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR grap"
C18-1101,N16-1087,0,0.305445,"nstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (§3.2) converts a summary graph to its PENMAN representation (Banaresc"
C18-1101,C10-1039,0,0.399961,"hese approaches are limited by the availability of training data, and large datasets for multi-document summarization can be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semantic formalism based on propositional logic and th"
C18-1101,W11-1608,0,0.123714,"ngle-document summarization (Paulus et al., 2017; See et al., 2017). These approaches are limited by the availability of training data, and large datasets for multi-document summarization can be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representati"
C18-1101,D14-1168,0,0.2057,"large datasets for multi-document summarization can be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semantic formalism based on propositional logic and the neo-Davidsonian event representation (Parsons, 1990; Schein, 1993)"
C18-1101,N09-1041,0,0.079293,"on framework with a number of extractive (ext-∗) and abstractive (abs-∗) summarization systems, including the most recent neural encoder-decoder architecture (See et al., 2017). They are described as follows. • ext-LexRank (Erkan and Radev, 2004) is a graph-based approach that computes sentence importance based on the concept of eigenvector centrality in a graph representation of source sentences; • ext-SumBasic (Vanderwende et al., 2007) is an extractive approach that assumes words occurring frequently in a document cluster have a higher chance of being included in the summary; • ext-KL-Sum (Haghighi and Vanderwende, 2009) describes a method that greedily adds sentences to the summary so long as it decreases the KL divergence; • abs-Opinosis (Ganesan et al., 2010) generates abstractive summaries by searching for salient paths on a word co-occurrence graph created from source documents; 1 We use N =M =5 in our experiments. This setting fuses 5 source sentences to a summary sentence. It then produces 5 summary sentences for each topic, corresponding to the average number of sentences in human summaries. 2 https://duc.nist.gov/data.html https://tac.nist.gov/data/index.html 1183 Approach LCS Smatch Concept Cov. VSM"
C18-1101,hong-etal-2014-repository,0,0.0322354,"ce vectors; (iii) Smatch (Cai and Knight, 2013) calculates the F-score of AMR concepts between the candidate and reference sentences; (iv) Concept Coverage selects source sentences to maximize the coverage of AMR concepts of the reference sentence. We experiment with these source sentence selection strategies and compare their effectiveness in Section §5.1. 4 Datasets and Baselines We perform experiments on standard multi-document summarization datasets2 , prepared by the NIST researchers for DUC/TAC competitions and later exploited by various summarization studies (Nenkova and McKeown, 2011; Hong et al., 2014; Yogatama et al., 2015). A summarization instance includes generating a text summary containing 100 words or less from a cluster of 10 source documents discussing a single topic. 4 human reference summaries are provided for each cluster of documents; they are created by NIST assessors. We use the datasets from DUC-03, DUC-04, TAC-09, TAC-10, and TAC-11 in this study, containing 30/50/44/46/44 clusters of documents respectively. We compare our AMR summarization framework with a number of extractive (ext-∗) and abstractive (abs-∗) summarization systems, including the most recent neural encoder-"
C18-1101,D17-1227,0,0.0276979,"Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural language generation (NLG, Reiter and Dale, 2000; Gatt and Krahmer, 2018) is a promising line of work. The approaches often identify salient te"
C18-1101,N15-2019,0,0.0270002,"resentation. This paper studies the feasibility of using AMR, a semantic formalism grounded in linguistic theory, for content representation. Within this framework, condensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system c"
C18-1101,D16-1140,0,0.0278647,"ly describe opportunities and challenges for advancing this line of research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural language generation (NLG, Reiter and Dale, 2000; Gatt an"
C18-1101,P17-1014,0,0.0968012,"lation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (§3.2) converts a summary graph to its PENMAN representation (Banarescu et al., 2013) and generates a natural language sentence from it. Source sentence extra"
C18-1101,W04-1013,0,0.0957077,"Missing"
C18-1101,N15-1114,1,0.923426,"ntent planning consumes a set of similar sentences and derives a summary graph from them; surface realization transforms a summary graph to a natural language sentence. This framework allows each component (source sentence selection, content planning, surface realization) to be individually optimized using small-scale, in-domain training data, reducing the need for large-scale parallel training data. Our research contributions are summarized as follows: • we investigate AMR, a linguistically-grounded semantic formalism, as a new form of content representation for multi-document summarization. Liu et al. (2015) conducted a pilot study using AMR for single-document summarization. This paper exploits the structured prediction framework but presents a full pipeline for generating abstractive summaries from multiple source documents; • we study to what extent the AMR parser and generator, used for mapping text to and from AMR, can impact the summarization performance. We also compare multiple source sentence selection strategies to group source sentences into clusters covering various aspects of the topic; • we conduct extensive experiments on benchmark summarization datasets, and contrast our work with"
C18-1101,S17-2090,0,0.030147,"identifies concepts from the sentence and then determines the relations between them by searching for the maximum spanning connected subgraph (MSCG) from a complete graph representing all possible relations between the identified concepts. CAMR (Wang et al., 2015b) approaches AMR parsing from a different perspective. It describes a transition-based AMR parsing algorithm that transforms from a dependency parse tree to an AMR graph. We choose JAMR and CAMR because these parsers have been made open-source and both of them reported encouraging results in the recent SemEval evaluations (May, 2016; May and Priyadarshi, 2017). Source Graph Construction. Given a set of source sentences and their AMR graphs, source graph construction attempts to consolidate all sentence AMR graphs to a connected source graph. This is accomplished by performing concept merging. Graph nodes representing the same concept, determined by the surface word form, are merged to a single node in the source graph. Importantly, we perform 1180 coreference resolution on the source documents to identify clusters of mentions of the same entity or event. Graph nodes representing these mentions are also merged. A special treatment to date entity (se"
C18-1101,S16-1166,0,0.0175498,"that first identifies concepts from the sentence and then determines the relations between them by searching for the maximum spanning connected subgraph (MSCG) from a complete graph representing all possible relations between the identified concepts. CAMR (Wang et al., 2015b) approaches AMR parsing from a different perspective. It describes a transition-based AMR parsing algorithm that transforms from a dependency parse tree to an AMR graph. We choose JAMR and CAMR because these parsers have been made open-source and both of them reported encouraging results in the recent SemEval evaluations (May, 2016; May and Priyadarshi, 2017). Source Graph Construction. Given a set of source sentences and their AMR graphs, source graph construction attempts to consolidate all sentence AMR graphs to a connected source graph. This is accomplished by performing concept merging. Graph nodes representing the same concept, determined by the surface word form, are merged to a single node in the source graph. Importantly, we perform 1180 coreference resolution on the source documents to identify clusters of mentions of the same entity or event. Graph nodes representing these mentions are also merged. A special"
C18-1101,K16-1028,0,0.0320091,"rk with state-of-the-art baselines, including the pointer-generator networks (See et al., 2017). Results show that leveraging the AMR representation for summarization is promising. Our framework is flexible, allowing different components to be optimized independently using small-scale, in-domain datasets. We finally describe opportunities and challenges for advancing this line of research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news artic"
C18-1101,W14-4407,0,0.0313079,"titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural language generation (NLG, Reiter and Dale, 2000; Gatt and Krahmer, 2018) is a promising line of work. The approaches often identify salient text units from source documents, arrange them in a compact form, such as domain-specific templates, and subsequently synthesize them into natural language texts (Barzilay et al., 1999; Genest and Lapalme, 2011; Oya et al., 2014; Gerani et al., 2014; Fabbrizio et al., 2014). A challenge faced by these approaches is that there 1179 multiple sets of similar sentences covering different aspects of the source Content Planning Source Sentence Selection a cluster of source documents discussing the same topic … … Content Planning Surface Realization … an abstractive summary containing multiple summary sentences … Surface Realization a set of summary graphs serving as content representation of the source Figure 2: Our system framework, consisting of three major components. lacks a principled means of content representation."
C18-1101,N15-1119,0,0.0305877,"ystem framework, consisting of three major components. lacks a principled means of content representation. This paper studies the feasibility of using AMR, a semantic formalism grounded in linguistic theory, for content representation. Within this framework, condensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further res"
C18-1101,P14-1084,0,0.0674847,"be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graphs (Ganesan et al., 2010), syntactic parse trees (Cheung and Penn, 2014; Gerani et al., 2014), and domain-specific templates (Pighin et al., 2014). Nonetheless, generating summary text from these heuristic forms of representation can be difficult. There is an increasing need to exploit a semantic formalism so that condensing source documents to this form and generating summary sentences from it can both be carried out in a principled way. This paper explores Abstract Meaning Representation (AMR, Banarescu et al., 2013) as a form of content representation. AMR is a semantic formalism based on propositional logic and the neo-Davidsonian event representation (Parsons, 1990; Schein, 1993). It represents the meaning of a sentence using a roo"
C18-1101,W16-6603,0,0.123598,"wnstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (§3.2) converts a summary graph to its PENMAN representation (Banarescu et al., 2013) and generates a natural langua"
C18-1101,D15-1044,0,0.055754,"and contrast our work with state-of-the-art baselines, including the pointer-generator networks (See et al., 2017). Results show that leveraging the AMR representation for summarization is promising. Our framework is flexible, allowing different components to be optimized independently using small-scale, in-domain datasets. We finally describe opportunities and challenges for advancing this line of research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acqui"
C18-1101,P16-2079,0,0.0206245,"sing AMR, a semantic formalism grounded in linguistic theory, for content representation. Within this framework, condensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content p"
C18-1101,N15-4003,0,0.0685433,"Missing"
C18-1101,P17-1099,0,0.681697,"k summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research. 1 Introduction Abstractive summarization seeks to generate concise and grammatical summaries that preserve the meaning of the original; further, they shall abstract away from the source syntactic forms. The task often involves high-level text transformations such as sentence fusion, generalization, and paraphrasing (Jing and McKeown, 1999). Recent neural abstractive summarization studies focus primarily on single-document summarization (Paulus et al., 2017; See et al., 2017). These approaches are limited by the availability of training data, and large datasets for multi-document summarization can be costly to obtain. Generating abstractive summaries for sets of source documents thus remains a challenging task. Traditional approaches to abstractive summarization often condense the source documents to a set of “semantic units,” then reconstruct abstractive summaries from these semantic units. Previous work has investigated various forms of content representation. Examples include noun/verb phrases (Genest and Lapalme, 2011; Bing et al., 2015), word-occurrence graph"
C18-1101,D16-1224,0,0.0187207,"l on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (§3.2) converts a summary graph to its PENMAN representation (Banarescu et al., 2013) and"
C18-1101,P17-2002,0,0.0329583,"uding machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (§3.2) converts a summary graph to its PENMAN representation (Banarescu et al., 2013) and generates a natural language sentence from it"
C18-1101,C18-1146,1,0.842505,"ocus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural language generation (NLG, Reiter and Dale, 2000; Gatt and Krahmer, 2018) is a promising line of work. The approaches often identify salient text units from source documents, arrange them in a compact form, such as domain-specific templates,"
C18-1101,E17-2047,0,0.0174253,"romising. Our framework is flexible, allowing different components to be optimized independently using small-scale, in-domain datasets. We finally describe opportunities and challenges for advancing this line of research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alte"
C18-1101,N18-1106,0,0.0181134,"ically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected source graph, then extracts a summary graph from the source graph via structured prediction. Surface realization (§3.2) conve"
C18-1101,D16-1112,0,0.0514646,"s. lacks a principled means of content representation. This paper studies the feasibility of using AMR, a semantic formalism grounded in linguistic theory, for content representation. Within this framework, condensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization."
C18-1101,W15-3504,0,0.020738,"esentation of the source Figure 2: Our system framework, consisting of three major components. lacks a principled means of content representation. This paper studies the feasibility of using AMR, a semantic formalism grounded in linguistic theory, for content representation. Within this framework, condensing source documents to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These s"
C18-1101,P17-1108,0,0.0334671,"using small-scale, in-domain datasets. We finally describe opportunities and challenges for advancing this line of research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarization via natural langu"
C18-1101,P15-2141,0,0.107454,"to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected s"
C18-1101,N15-1040,0,0.430763,"to summary AMR graphs and generating natural language sentences from summary graphs are both data-driven and not specifically designed for any domain. The AMR formalism has demonstrated great potential on a number of downstream applications, including machine translation (Tamchyna et al., 2015), entity linking (Pan et al., 2015), summarization (Liu et al., 2015; Takase et al., 2016), question answering (Jurczyk and Choi, 2015), and machine comprehension (Sachan and Xing, 2016). Moreover, significant research efforts are dedicated to map English sentences to AMR graphs (Flanigan et al., 2014; Wang et al., 2015a; Wang et al., 2015b; Ballesteros and Al-Onaizan, 2017; Buys and Blunsom, 2017; Damonte et al., 2017; Szubert et al., 2018), and generating sentences from AMR (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). These studies pave the way for further research exploiting AMR for multi-document summarization. 3 Our Approach We describe our major system components in this section. In particular, content planning (§3.1) takes as input a set of similar sentences. It maps each sentence to an AMR graph, merges all AMR graphs to a connected s"
C18-1101,D09-1091,0,0.0372213,"0 − 1)∗“)”::EOS else output += (k − k 0 )∗“)”::EOS end if end for 3.3 Source Sentence Selection We seek to generate an abstractive summary containing multiple sentences from a cluster of documents discussing a single topic (e.g., health and safety). Each summary sentence will cover a topic aspect; it is generated by fusing a set of relevant source sentences. We thus perform clustering on all source sentences to find salient topic aspects and their corresponding sets of similar sentences. Spectral clustering has been shown to perform strongly on different clustering problems (Ng et al., 2002; Yogatama and Tanaka-Ishii, 2009). The approach constructs an affinity matrix by applying a pairwise similarity function to all source sentences. It then calculates the eigenvalues of the matrix and performs clustering in the low-dimensional space spanned by the largest eigenvectors. A large cluster indicates a salient topic aspect. We focus on the M largest clusters and extract N sentences from each cluster.1 These sentences have the highest similarity scores with other sentences in the cluster. The selected sets of relevant sentences are later fed to the content planning component to generate summary AMR graphs. Training th"
C18-1101,D15-1228,1,0.86387,"match (Cai and Knight, 2013) calculates the F-score of AMR concepts between the candidate and reference sentences; (iv) Concept Coverage selects source sentences to maximize the coverage of AMR concepts of the reference sentence. We experiment with these source sentence selection strategies and compare their effectiveness in Section §5.1. 4 Datasets and Baselines We perform experiments on standard multi-document summarization datasets2 , prepared by the NIST researchers for DUC/TAC competitions and later exploited by various summarization studies (Nenkova and McKeown, 2011; Hong et al., 2014; Yogatama et al., 2015). A summarization instance includes generating a text summary containing 100 words or less from a cluster of 10 source documents discussing a single topic. 4 human reference summaries are provided for each cluster of documents; they are created by NIST assessors. We use the datasets from DUC-03, DUC-04, TAC-09, TAC-10, and TAC-11 in this study, containing 30/50/44/46/44 clusters of documents respectively. We compare our AMR summarization framework with a number of extractive (ext-∗) and abstractive (abs-∗) summarization systems, including the most recent neural encoder-decoder architecture (Se"
C18-1101,P17-1101,0,0.0358949,"ized independently using small-scale, in-domain datasets. We finally describe opportunities and challenges for advancing this line of research. 2 Related Work Neural abstractive summarization has sparked great interest in recent years. These approaches focus primarily on short text summarization and single-document summarization (Rush et al., 2015; Nallapati et al., 2016). Variants of the neural encoder-decoder architecture have been exploited to reduce out-ofvocabulary tokens and word repetitions (See et al., 2017; Suzuki and Nagata, 2017), improve the attention mechanism (Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017), control the summary length (Kikuchi et al., 2016), improve the learning objective and search (Ranzato et al., 2016; Huang et al., 2017), and generate summaries that are true to the original inputs (Cao et al., 2018; Song et al., 2018). Training neural models requires large amounts of data; they are often acquired by pairing news articles with titles or human-written highlights. Nonetheless, obtaining parallel data for multi-document summarization is often costly. There is thus a need to investigate alternative approaches that are less data-thirsty. Abstractive summarizatio"
C18-1101,W01-0100,0,\N,Missing
C18-1146,P13-1020,0,0.0580136,"Missing"
C18-1146,W08-1106,0,0.106118,"systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above a"
C18-1146,D14-1082,0,0.0356151,"ween the system summary and the source text. S(w)=logP (w)+η 4 B({y<t ,w},x)−B(y<t ,x) S (18) Experiments We evaluate the proposed structure-infused copy mechanisms for summarization in this section. We describe the dataset, experimental settings, baselines, and finally, evaluation results and analysis. 4.1 Data Sets We evaluate our proposed models on the Gigaword summarization dataset (Parker, 2011; Rush et al., 2015). The task is to reduce the first sentence of an article to a title-like summary. We obtain dependency parse trees for source sentences using the Stanford neural network parser (Chen and Manning, 2014). We also use the standard train/valid/test data splits. Following (Rush et al., 2015), the train and valid splits are pruned2 to improve the data quality. Spurious pairs that are repetitive, overly long/short, and pairs whose source and summary sequences have little word overlap are removed. No pruning is performed for instances in the test set. The processed corpus contains 4,018K training instances. We construct two (non-overlapped) validation sets: “valid-4096” contains 4,096 randomly sampled instances from the valid split; it is used for hyperparameter tuning and early stopping. “valid-20"
C18-1146,P17-1177,0,0.0297056,". 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize important source words and relations during training, while striving to preserve them in the summaries at test time to aid reproduction of factual details. Our intent of incorporating source syntax in summarization is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017), in part because NMT does not handle the information loss from source to target. In contrast, a summarization system must selectively preserve source content to render concise and grammatical summaries. We specifically focus on sentence summarization, where the goal is to reduce the first sentence of an article to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved. 3 Our Approach We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and preserves the meaning of the source sentence. A source word is repla"
C18-1146,P16-1046,0,0.0551738,"uld be supplied as input to the encoder (left) or be exempted from encoding and directly concatenated with the encoder hidden states (right). The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to be selectively copied to the target sequence. It expands the search space for summary words to include both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (pgen ) vs. copying it from the source text (1 − pgen ). The basic model is similar to that of the pointer-generator networks (See et al., 2017). The switch is a feedforward layer with sigmoid activation (Eq. (8)). At time step t, its input is a concatenation of the decoder hidden state hdt , context vector ct , and the embedding of the previously generated word yt−1 . For predicting the next word, we combine the generation and copy probabilities, shown in Eq. (9). If"
C18-1146,N16-1012,0,0.0423721,"he “Struct+2Way+Word” architecture that respectively models the semantic and syntactic importance of source words achieves the highest scores. It outperforms its counterpart of “Struct+2Way+Relation,” which seeks to preserve source dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated 3 w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000 1724 System Gigaword Test-1951 R-1 R-2 R-L ABS and ABS+ (Rush et al., 2015) are the first work introducing an encoder-decoder architecture for summarization. ABS (Rush et al., 2015) ABS+ (Rush et al., 2015) Luong-NMT (Chopra et al., 2016) RAS-LSTM (Chopra et al., 2016) RAS-Elman (Chopra et al., 2016) ASC+FSC1 (Miao and Blunsom, 2016) lvt2k-1sent (Nallapati et al., 2016) lvt5k-1sent (Nallapati et al., 2016) Multi-Task (Pasunuru et al., 2017) DRGD (Li et al., 2017b) 29.55 29.76 33.10 32.55 33.78 34.17 32.67 35.30 32.75 36.27 11.32 11.88 14.45 14.70 15.97 15.94 15.59 16.64 15.35 17.57 26.42 26.96 30.71 30.03 31.15 31.92 30.64 32.62 30.82 33.62 Luong-NMT (Chopra et al., 2016) is a re-implementation of the attentive stacked LSTM encoder-decoder of Luong et al. (2015a). Baseline (this paper) Struct+Input (this paper) Struct+2Way+Rel"
C18-1146,P02-1057,0,0.397477,"Missing"
C18-1146,P16-1188,0,0.040411,"et al., 2017); • through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective. Our approach surpasses state-of-the-art published systems on the benchmark dataset.1 2 Related Work Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et"
C18-1146,W14-4408,0,0.15379,"Missing"
C18-1146,D15-1042,0,0.0700408,"tion is effective. Our approach surpasses state-of-the-art published systems on the benchmark dataset.1 2 Related Work Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme"
C18-1146,N10-1131,0,0.166457,".1 2 Related Work Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent lang"
C18-1146,D14-1168,0,0.113498,"tch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper"
C18-1146,P16-1154,0,0.0494078,"ddings (sei ) should be supplied as input to the encoder (left) or be exempted from encoding and directly concatenated with the encoder hidden states (right). The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to be selectively copied to the target sequence. It expands the search space for summary words to include both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (pgen ) vs. copying it from the source text (1 − pgen ). The basic model is similar to that of the pointer-generator networks (See et al., 2017). The switch is a feedforward layer with sigmoid activation (Eq. (8)). At time step t, its input is a concatenation of the decoder hidden state hdt , context vector ct , and the embedding of the previously generated word yt−1 . For predicting the next word, we combine the generation and copy probabiliti"
C18-1146,P16-1014,0,0.0525111,"(Eq. (7)). et,i = v> tanh(We [hdt ||hei ] + be ) exp(et,i ) αt,i = PS i0 =1 exp(et,i0 ) PS ct = i=1 αt,i hei e d = tanh(Wh [hd ||ct ] + bh ) h t Pvocab (w) = t ed softmax(Wy h t 1719 y +b ) (3) (4) (5) (6) (7) ct ct ↵t,i ↵t,i hei hdt [xi ||sei ] [hei ||sei ] hdt xi Figure 2: System architectures for ‘Struct+Input’ (left) and ‘Struct+Hidden’ (right). A critical question we seek to answer is whether the structural embeddings (sei ) should be supplied as input to the encoder (left) or be exempted from encoding and directly concatenated with the encoder hidden states (right). The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to be selectively copied to the target sequence. It expands the search space for summary words to include both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (pgen ) vs. copying it from the s"
C18-1146,D16-1140,0,0.0316296,"al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too"
C18-1146,D13-1047,1,0.865286,"ence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a m"
C18-1146,D14-1076,1,0.909188,"ructure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure"
C18-1146,P17-1064,0,0.31922,"lness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize important source words and relations during training, while striving to preserve them in the summaries at test time to aid reproduction of factual details. Our intent of incorporating source syntax in summarization is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017), in part because NMT does not handle the information loss from source to target. In contrast, a summarization system must selectively preserve source content to render concise and grammatical summaries. We specifically focus on sentence summarization, where the goal is to reduce the first sentence of an article to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved. 3 Our Approach We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and preserves the meaning of the source sentence. A"
C18-1146,D17-1222,0,0.472522,"lness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize important source words and relations during training, while striving to preserve them in the summaries at test time to aid reproduction of factual details. Our intent of incorporating source syntax in summarization is different from that of neural machine translation (NMT) (Li et al., 2017a; Chen et al., 2017), in part because NMT does not handle the information loss from source to target. In contrast, a summarization system must selectively preserve source content to render concise and grammatical summaries. We specifically focus on sentence summarization, where the goal is to reduce the first sentence of an article to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved. 3 Our Approach We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and preserves the meaning of the source sentence. A"
C18-1146,W04-1013,0,0.279482,"Missing"
C18-1146,N15-1114,1,0.889614,"vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper seeks to directly incorporate source-side"
C18-1146,D15-1166,0,0.0565821,"seek to answer is whether the structural embeddings (sei ) should be supplied as input to the encoder (left) or be exempted from encoding and directly concatenated with the encoder hidden states (right). The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to be selectively copied to the target sequence. It expands the search space for summary words to include both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (pgen ) vs. copying it from the source text (1 − pgen ). The basic model is similar to that of the pointer-generator networks (See et al., 2017). The switch is a feedforward layer with sigmoid activation (Eq. (8)). At time step t, its input is a concatenation of the decoder hidden state hdt , context vector ct , and the embedding of the previously generated word yt−1 . For predicting the next word, we c"
C18-1146,P15-1002,0,0.0162863,"seek to answer is whether the structural embeddings (sei ) should be supplied as input to the encoder (left) or be exempted from encoding and directly concatenated with the encoder hidden states (right). The copy mechanism (Gulcehre et al., 2016; See et al., 2017) allows words in the source sequence to be selectively copied to the target sequence. It expands the search space for summary words to include both the output vocabulary and the source text. The copy mechanism can effectively reduce out-ofvocabulary tokens in the generated text, potentially aiding a number of applications such as MT (Luong et al., 2015b) and text summarization (Gu et al., 2016; Cheng and Lapata, 2016; Zeng et al., 2017). Our copy mechanism employs a ‘switch’ to estimate the likelihood of generating a word from the vocabulary (pgen ) vs. copying it from the source text (1 − pgen ). The basic model is similar to that of the pointer-generator networks (See et al., 2017). The switch is a feedforward layer with sigmoid activation (Eq. (8)). At time step t, its input is a concatenation of the decoder hidden state hdt , context vector ct , and the embedding of the previously generated word yt−1 . For predicting the next word, we c"
C18-1146,E06-1038,0,0.0840299,"information in neural sentence summarization is effective. Our approach surpasses state-of-the-art published systems on the benchmark dataset.1 2 Related Work Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al.,"
C18-1146,D16-1031,0,0.0326457,"nce of source words achieves the highest scores. It outperforms its counterpart of “Struct+2Way+Relation,” which seeks to preserve source dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated 3 w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000 1724 System Gigaword Test-1951 R-1 R-2 R-L ABS and ABS+ (Rush et al., 2015) are the first work introducing an encoder-decoder architecture for summarization. ABS (Rush et al., 2015) ABS+ (Rush et al., 2015) Luong-NMT (Chopra et al., 2016) RAS-LSTM (Chopra et al., 2016) RAS-Elman (Chopra et al., 2016) ASC+FSC1 (Miao and Blunsom, 2016) lvt2k-1sent (Nallapati et al., 2016) lvt5k-1sent (Nallapati et al., 2016) Multi-Task (Pasunuru et al., 2017) DRGD (Li et al., 2017b) 29.55 29.76 33.10 32.55 33.78 34.17 32.67 35.30 32.75 36.27 11.32 11.88 14.45 14.70 15.97 15.94 15.59 16.64 15.35 17.57 26.42 26.96 30.71 30.03 31.15 31.92 30.64 32.62 30.82 33.62 Luong-NMT (Chopra et al., 2016) is a re-implementation of the attentive stacked LSTM encoder-decoder of Luong et al. (2015a). Baseline (this paper) Struct+Input (this paper) Struct+2Way+Relation (this paper) Struct+Hidden (this paper) Struct+2Way+Word (this paper) 35.43 35.32 35.46 35."
C18-1146,K16-1028,0,0.311354,"ambican man arrested for murder Src Ref Sys An Alaska father who was too drunk to drive had his 11-year-old son take the wheel, authorities said. Drunk Alaska dad has 11 year old drive home alaska father who was too drunk to drive Src Table 1: Example source sentences, reference and system summaries produced by a neural attentive seq-to-seq model. System summaries fail to preserve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance. The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the results are impressive, individual system summaries can appear unreliable and fail to preserve the meaning of the source texts. Table 1 presents two examples. In these cases, the syntactic structure of source sentences is relatively rare but perfectly normal. The first sentence contains two appositional phrases (“suspect of murdering Jorge Microsse,” “director of Maputo central prison”) and the second sentence has a relative clause (“who was too drunk to drive”), both located between the subject and the main verb. The system, however, fails t"
C18-1146,W17-4504,0,0.0140401,"ch seeks to preserve source dependency relations in summaries. We conjecture that the imperfect dependency parse trees generated 3 w/ ROUGE options: -n 2 -m -w 1.2 -c 95 -r 1000 1724 System Gigaword Test-1951 R-1 R-2 R-L ABS and ABS+ (Rush et al., 2015) are the first work introducing an encoder-decoder architecture for summarization. ABS (Rush et al., 2015) ABS+ (Rush et al., 2015) Luong-NMT (Chopra et al., 2016) RAS-LSTM (Chopra et al., 2016) RAS-Elman (Chopra et al., 2016) ASC+FSC1 (Miao and Blunsom, 2016) lvt2k-1sent (Nallapati et al., 2016) lvt5k-1sent (Nallapati et al., 2016) Multi-Task (Pasunuru et al., 2017) DRGD (Li et al., 2017b) 29.55 29.76 33.10 32.55 33.78 34.17 32.67 35.30 32.75 36.27 11.32 11.88 14.45 14.70 15.97 15.94 15.59 16.64 15.35 17.57 26.42 26.96 30.71 30.03 31.15 31.92 30.64 32.62 30.82 33.62 Luong-NMT (Chopra et al., 2016) is a re-implementation of the attentive stacked LSTM encoder-decoder of Luong et al. (2015a). Baseline (this paper) Struct+Input (this paper) Struct+2Way+Relation (this paper) Struct+Hidden (this paper) Struct+2Way+Word (this paper) 35.43 35.32 35.46 35.49 35.47 17.49 17.50 17.51 17.61 17.66 33.39 33.25 33.28 33.33 33.52 RAS-LSTM and RAS-Elman (Chopra et al., 2"
C18-1146,D14-1162,0,0.0866956,"ot handle the information loss from source to target. In contrast, a summarization system must selectively preserve source content to render concise and grammatical summaries. We specifically focus on sentence summarization, where the goal is to reduce the first sentence of an article to a title-like summary. We believe even for this reasonably simple task there remains issues unsolved. 3 Our Approach We seek to transform a source sentence x to a summary sentence y that is concise, grammatical, and preserves the meaning of the source sentence. A source word is replaced by its Glove embedding (Pennington et al., 2014) before it is fed to the system; the vector is denoted by xi (i ∈ [S]; ‘S’ for source). Similarly, a summary word is denoted by yt (t ∈ [T ]; ‘T’ for target). If a word does not appear in the input vocabulary, it is replaced by a special ‘hunki’ token. We begin this section by describing the basic summarization framework, followed by our new copy mechanisms used to encourage source words and dependency relations to be preserved in the summary. 3.1 The Basic Framework We build an encoder-decoder architecture for this work. An encoder condenses the entire source text to a continuous vector; it a"
C18-1146,P14-1084,0,0.103993,"ulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure in the copy mechanism of an abstractive sentence summarization system. It learns to recognize important source words and relations during training, while striving to preserve them in the summaries at test time to ai"
C18-1146,D15-1044,0,0.678295,"irector escapes mozambican man arrested for murder Src Ref Sys An Alaska father who was too drunk to drive had his 11-year-old son take the wheel, authorities said. Drunk Alaska dad has 11 year old drive home alaska father who was too drunk to drive Src Table 1: Example source sentences, reference and system summaries produced by a neural attentive seq-to-seq model. System summaries fail to preserve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance. The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the results are impressive, individual system summaries can appear unreliable and fail to preserve the meaning of the source texts. Table 1 presents two examples. In these cases, the syntactic structure of source sentences is relatively rare but perfectly normal. The first sentence contains two appositional phrases (“suspect of murdering Jorge Microsse,” “director of Maputo central prison”) and the second sentence has a relative clause (“who was too drunk to drive”), both located between the subject and the main verb. The"
C18-1146,P17-1099,0,0.681019,"murder Src Ref Sys An Alaska father who was too drunk to drive had his 11-year-old son take the wheel, authorities said. Drunk Alaska dad has 11 year old drive home alaska father who was too drunk to drive Src Table 1: Example source sentences, reference and system summaries produced by a neural attentive seq-to-seq model. System summaries fail to preserve summary-worthy content of the source (e.g., main verbs) despite their syntactic importance. The sequence-to-sequence learning paradigm has achieved remarkable success on abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). While the results are impressive, individual system summaries can appear unreliable and fail to preserve the meaning of the source texts. Table 1 presents two examples. In these cases, the syntactic structure of source sentences is relatively rare but perfectly normal. The first sentence contains two appositional phrases (“suspect of murdering Jorge Microsse,” “director of Maputo central prison”) and the second sentence has a relative clause (“who was too drunk to drive”), both located between the subject and the main verb. The system, however, fails to identify the mai"
C18-1146,D16-1112,0,0.0389603,"source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language generation (NLG)-based abstractive summarization (Carenini and Cheung, 2008; Gerani et al., 2014; Fabbrizio et al., 2014; Liu et al., 2015; Takase et al., 2016) also makes extensive use of structural information, including syntactic/semantic parse trees, discourse structures, and domainspecific templates built using a text planner or an OpenIE system (Pighin et al., 2014). In particular, Cao et al. (2018) leverage OpenIE and dependency parsing to extract fact tuples from the source text and use those to improve the faithfulness of summaries. 1 We made our system publicly available at: https://github.com/KaiQiangSong/struct_infused_summ 1718 Different from the above approaches, this paper seeks to directly incorporate source-side syntactic structure i"
C18-1146,P17-1108,0,0.549738,"r research contributions include the following: • we introduce novel neural architectures that encourage salient source words/relations to be preserved in summaries. The framework naturally combines the dependency parse tree structure with the copy mechanism of an abstractive summarization system. To the best of our knowledge, this is the first attempt at comparing various neural architectures for this purpose; • we study the effectiveness of several important components, including the vocabulary size, a coveragebased regularizer (See et al., 2017), and a beam search with reference mechanism (Tan et al., 2017); • through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective. Our approach surpasses state-of-the-art published systems on the benchmark dataset.1 2 Related Work Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al"
C18-1146,W13-3508,0,0.133747,"h reference mechanism (Tan et al., 2017); • through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective. Our approach surpasses state-of-the-art published systems on the benchmark dataset.1 2 Related Work Prior to the deep learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only"
C18-1146,P13-1136,0,0.172833,"learning era, sentence syntactic structure has been utilized to generate summaries with an “extract-and-compress” framework. Compressed summaries are generated using a joint model to extract sentences and drop non-important syntactic constituents (Daume III and Marcu, 2002; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Durrett et al., 2016), or a pipeline approach that combines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However"
C18-1146,D17-1062,0,0.0607704,"Missing"
C18-1146,P17-1101,0,0.0969242,"ombines generic sentence compression (McDonald, 2006; Clarke and Lapata, 2008; Filippova et al., 2015) with a sentence pre-selection or post-selection process (Zajic et al., 2007; Galanis and Androutsopoulos, 2010; Wang et al., 2013; Li et al., 2013; Li et al., 2014). Although syntactic information is helpful for summarization, there has been little prior work investigating how best to combine sentence syntactic structure with the neural abstractive summarization systems. Existing neural summarization systems handle syntactic structure only implicitly (Kikuchi et al., 2016; Chen et al., 2016; Zhou et al., 2017; Tan et al., 2017; Paulus et al., 2017). Most systems adopt a “cut-andstitch” scheme that picks words either from the vocabulary or the source text and stitch them together using a recurrent language model. However, there lacks a mechanism to ensure structurally salient words and relations in source sentences are preserved in the summaries. The resulting summary sentences can contain misleading information (e.g., “mozambican man arrested for murder” flips the meaning of the original) or grammatical errors (e.g., verbless, as in “alaska father who was too drunk to drive”). Natural language gen"
C18-1146,W01-0100,0,\N,Missing
C18-1146,P11-1049,0,\N,Missing
D13-1047,D10-1047,0,0.0959622,"h has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A gr"
D13-1047,P13-1020,0,0.137564,"constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the"
D13-1047,P11-1049,0,0.810124,"ummaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extra"
D13-1047,C12-1029,0,0.553418,"insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to us"
D13-1047,N10-1131,0,0.0191521,"(2013) propose a graph-cut based method that improves the speed of joint compression and summarization. The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression, is also popular. Knight and Marcu (2000) utilize the noisy channel and decision tree method to perform sentence compression; Lin (2003) shows that pure syntactic-based compression may not improve the system performance; Zajic et al. (2007) compare two sentence compression approaches for multi-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the doc"
D13-1047,W06-1643,0,0.0344792,"lts. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submo"
D13-1047,P13-1099,1,0.185695,"for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline a"
D13-1047,N10-1134,0,0.166154,"aximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the authors model the sentence selection problem as maximizing a submodular function under a budget constraint. A greedy algorithm is proposed to efficiently approximate the solution to this NP-hard problem. Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selec"
D13-1047,W03-1101,0,0.0326955,"rest in recent years on generating compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of"
D13-1047,W04-1013,0,0.631382,"oint modeling studies. Using our created compression data, we train a supervised compression model using a variety of word-, sentence-, and document-level features. During summarization, we generate multiple compression candidates for each sentence, and use the ILP framework to select compressed summary sentences. In addition, we also propose to apply a preselection step to select some important sentences, which can both speed up the summarization system and improve performance. We evaluate our proposed summarization approach on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004). Our results show that by incorporating a guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been expl"
D13-1047,P09-2066,1,0.644677,"Missing"
D13-1047,W09-1801,0,0.668396,"ntence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before"
D13-1047,E06-1038,0,0.0976203,"ulti-document summarization, including a ‘parse-and-trim’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sente"
D13-1047,C12-1128,0,0.124461,"The above ILP method can offer an exact solution to the defined objective function. However, ILP is computationally expensive when the formulation involves large quantities of variables, i.e, when we have many sentences and a large number of candidate compressions for each sentence. We therefore propose to apply a sentence pre-selection step before the compression. This kind of selection step has been used in previous ILP-based summarization systems (Berg-Kirkpatrick et al., 2011; Gillick et al., 2009). In this work, we propose to use a simple supervised support vector regression (SVR) model (Ng et al., 2012) to predict a salience score for each sentence and select the top ranked sentences for further processing (compression and summarization). To train the SVR model, the target value for each sentence is the ROUGE-2 score between the sentence and the four human abstracts (this same value is used for sentence selection in corpus annotation (Section 3)). We employ three commonly used features: (1) sentence position in the document; (2) sentence length as indicated by a binary feature: it takes the value of 0 if the number of words in the sentence is greater than 50 or less than 10, otherwise the fe"
D13-1047,W02-0401,0,0.0970346,"rformance gain as compared to the state-of-the-art reported results. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation – to extract a set of salient and non-redundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. The supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain condi491 tional random fields (CRF) (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous approaches include the integer linear programming (ILP) and submodular functions, which are used to solve the optimization problem. In particular, Gillick et al. (2009) proposed a concept-based ILP approach for summarization. Li et al. (2013) improved it by using supervised stragety to estimate concept weight in ILP framework. In (Lin and Bilmes, 2010), the aut"
D13-1047,N07-1051,0,0.0111604,"rds and the current word. • POS n-grams: same as the word n-grams, but use the part-of-speech tags instead. • Named entity tags: binary features representing whether the current word is a person, location, or temporal expression. We use the Stanford CoreNLP tools3 for named entity tagging. • Stopwords: whether the current word is a stopword or not. • Conjunction features: (1) conjunction of the current word with its relative position in the sentence; (2) conjunction of the NER tag with its relative position. • Syntactic features: We obtain the syntactic parsing tree using the Berkeley Parser (Petrov and Klein, 2007), then obtain the following features: (1) the last sentence constituent tag in the path from the root to the word; (2) depth: length of the path starting from the root node to the word; (3) normalized depth: depth divided by the longest path in the parsing tree; (4) whether the word is under an SBAR node; (5) depth and normalized depth of the SBAR node if the word is under an SBAR node; • Dependency features: We employ the Penn2Malt toolkit 4 to convert the parse result from the Berkeley parser to the dependency parsing tree, and use these dependency 3 4 http://nlp.stanford.edu/software/corenl"
D13-1047,D13-1156,1,0.64139,"for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work using such pipeline methods simply uses generic sentence-based compression for each sentence in the documents, no matter whether compression is done before or after summary sentence extraction. We propose to use sum490 Proceedings of the 2013 Conference on E"
D13-1047,J02-4001,0,0.0756133,"Missing"
D13-1047,W13-3508,0,0.478243,"m’ and a noisy-channel approach; Galanis and Androutsopoulos (2010) use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences; Liu and Liu (2013) couple the sentence compression and extraction approaches for summarizing the spoken documents; Wang et al. (2013) design a series of learning-based compression models built on parse trees, and integrate them in query-focused multi-document summarization. Prior studies often rely heavily on the generic sentence compression approaches (McDonald, 2006; Nomoto, 2007; Clarke and Lapata, 2008; Thadani and McKeown, 2013) for compressing the sentences in the documents, yet a generic compression system may not be the best fit for the summarization purpose. In this paper, we adopt the pipeline-based compressive summarization framework, but propose a novel guided compression method that is catered to the summarization task. We expect this approach to take advantage of the efficient pipeline processing while producing satisfying results as the joint models. We train a supervised guided compression model to produce n-best compressions for each sentence, and use an ILP formulation to select the best set of summary s"
D13-1047,P05-1036,0,0.0137398,"ssion model using our created compression data, with a variety of features.then we use this model to generate n-best compressions for each sentence; we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. In addition, we propose a sentence pre-selection step that can both speed up the summarization system and improve the performance. 4.1 Guided Sentence Compression Sentence compression has been explored in previous studies using both supervised and unsupervised approaches, including the noisy-channel and decision tree model (Knight and Marcu, 2000; Turner and Charniak, 2005), discriminative learning (McDonald, 2006), integer linear programming (Clarke and Lapata, 2008; Thadani and McKeown, 2013), conditional random fields (CRF) (Nomoto, 2007; Liu and Liu, 2013), etc. In this paper, we employ the CRF-based compression approach due to its proved performance and its flexibility to integrate different levels of discriminative features. Under this framework, sentence compression is formulated as a sequence labeling problem, where each word is labeled as either “0” (retained) or “1” (removed). We develop different levels of features to capture word-specific characteris"
D13-1047,P07-1070,0,0.0111622,"tators were explicitly informed about the important summary words during the compression annotation. We then train a supervised compression model to capture the guided compression process using a set of word-, sentence-, and document-level features. We conduct experiments on the TAC 2008 and 2011 summarization data sets and show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art. In future, we would like to further explore the reinforcement relationship between keywords and summaries (Wan et al., 2007), improve the readability of the sentences generated from the guided compression system, and report results using multiple evaluation metrics (Nenkova et al., 2007; Louis and Nenkova, 2012) as well as performing human evaluations. Acknowledgments Part of this work was done during the first author’s internship in Bosch Research and Technology Center. The work is also partially supported by NSF award IIS-0845484 and DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views of the fund"
D13-1047,P13-1136,0,0.610583,"ing compressed document summaries as a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. They have been shown to achieve promising performance (Daum´e, 2006; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013). One popular approach for such joint compression and summarization is via integer linear programming (ILP). However, since words are the units in the optimization framework, solving this ILP problem can be expensive. In this study, we use the pipeline compression and summarization method because of its computational efficiency. Prior work"
D13-1047,D12-1022,0,0.508507,"ses, or through a pipeline approach that integrates a generic sentence compression model with a summary sentence pre-selection or post-selection step. Many studies explore the joint sentence compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut base"
D13-1047,P12-2068,0,0.0263344,"e compression and selection setting. Martins and Smith (2009) jointly perform sentence extraction and compression by solving an ILP problem; Berg-Kirkpatrick et al. (2011) propose an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They train the model using a margin-based objective whose loss captures the final summary quality. Woodsend and Lapata (2012) present a method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup; Yoshikawa et al. (2012) incorporate semantic role information in the ILP model; Chali and Hasan (2012) investigate three strategies in compressive summarization: compression before extraction, after extraction, or joint compression and extraction in one global optimization framework. These joint models offer a promise for high quality summaries, but they often have high computational cost. Qian and Liu (2013) propose a graph-cut based method that improves the speed of joint compression and summarization. The pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compres"
D13-1047,W01-0100,0,\N,Missing
D14-1076,D10-1047,0,0.018162,". 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that int"
D14-1076,P13-1020,0,0.0722131,"s since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constit"
D14-1076,P11-1049,0,0.380047,"and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick et al. (2011) proposed an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They trained the model using a margin-based objective whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly i"
D14-1076,briscoe-carroll-2002-robust,0,0.0757113,"preserve the grammar relationship; Wang et al. (2013) developed a novel beam search decoder using the treebased compression model on the constituent parse tree, which could find the most probable compression efficiently. Table 4 shows the compression results of various systems, along with the compression ratio (C Rate) of the system output. We adopt the compression metrics as used in (Martins and Smith, 2009) that measures the macro F-measure for the retained unigrams (Uni-F1), and the one used in (Clarke and Lapata, 2008) that calculates the F1 score of the grammatical relations labeled by (Briscoe and Carroll, 2002) (Rel-F1). We can see that our proposed compression method performs well, similar to the state-of-the-art systems. To evaluate the power of using the expanded parse tree in our model, we conducted another experiment where we only consider the bottom level of the constituent parse tree. In some sense, this could be considered as the system in (Clarke and Lapata, 2008). Furthermore, we use two different setups: one uses the lexical features (about the words) and the other does not. Table 5 shows the results using the data in (Clarke and Lapata, 2008). For a comparison, we also include the result"
D14-1076,C12-1029,0,0.118385,"sed extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features fo"
D14-1076,D13-1047,1,0.165504,"constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality"
D14-1076,P13-1099,1,0.254022,"constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality"
D14-1076,N10-1134,0,0.234957,"ndant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick e"
D14-1076,W03-1101,0,0.0207147,"li,yangl@hlt.utdallas.edu} {feiliu@cs.cmu.edu} {lin.zhao,fuliang.weng@us.bosch.com} Abstract a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminati"
D14-1076,W04-1013,0,0.220689,"ered more difficult, involving sophisticated techniques for meaning representation, content planning, surface realization, etc. There has been a surge of interest in recent years on generating compressed document summaries as 691 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 691–701, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics based summarization model is used to select the final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, w"
D14-1076,W02-1001,0,0.0347668,"icked during the spring are tasty. partmod(truffles,picked) Oil price futures. nn(futures,oil) She looks very beautiful. acomp(looks,beautiful) He felt sad after learning that tragedy. pcomp(after,learning) I am certain that he did it. ccomp(certain,did) Last night I swam in the pool. tmod(swam,night) Table 1: Some dependency relations used for extra constraints. All the examples are from (Marneffe and Manning, 2002) • For type III relations, if the parent node in these relations is retained, the child node should be kept as well. 3.4 weights using the structured perceptron learning strategy (Collins, 2002). The reference label for every node in the expanded constituent parse tree is obtained automatically from the bottom to the top of the tree. Since every leaf node (word) is human annotated (removed or retain), we annotate the internal nodes as removed if all of its children are removed. Otherwise, the node is annotated as retained. During perceptron training, a fixed learning rate is used and parameters are averaged to prevent overfitting. In our experiment, we observe stable convergence using the held-out development corpus, with best performance usually obtained around 10-20 epochs. Feature"
D14-1076,W03-0501,0,0.147922,"Missing"
D14-1076,D13-1155,0,0.0186923,"summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. Thadani and McKeown (2013) presented an approach for discriminative sentence compression that jointly produces sequential and syntactic representations for output text. Filippova and Altun (2013) presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence sele"
D14-1076,W09-1801,0,0.800543,"P) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression processes, or through a pipeline approach that integrates a sentence compression model with a summary sentence pre-selection or post-selection step. Many studies have explored the joint sentence compression and selection setting. Martins and Smith (2009) jointly performed sentence extraction and compression by solving an ILP problem. Berg-Kirkpatrick et al. (2011) proposed an approach to score the candidate summaries according to a combined linear model of extractive sentence selection and compression. They trained the model using a margin-based objective whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compre"
D14-1076,E06-1038,0,0.489741,"al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression"
D14-1076,N10-1131,0,0.0361554,"e whose loss captures the final summary qualIn addition to the work on sentence compression as a stand-alone task, prior studies have also investigated compression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly improve the summarization performance. Zajic et al. (2007) compared two sentence compression approaches for multidocument summarization, including a ‘parse-andtrim’ and a noisy-channel approach. Galanis and Androutsopoulos (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddhartha"
D14-1076,N07-1023,0,0.0221158,"an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. This is an advantage of treebased compression technique (Knight and Marcu, 2000; Galley and McKeown, 2007; Wang et al., 2013). Similar to (Li et al., 2013a), we use a pipeline summarization framework where multiple compression candidates are generated for each pre-selected important sentence, and then an ILPIn this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status – remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate vari"
D14-1076,W06-1643,0,0.0329105,"sed compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence selection and compression proc"
D14-1076,C12-1128,0,0.243243,"ethod to select the best summary sentences from the multiple compressed sentences. The sentence pre-selection model is a simple supervised support vector regression (SVR) model that predicts a salience score for each sentence and selects the top ranked sentences for further processing (compression and summarization). The target value for each sentence during training is the ROUGE-2 score between the sentence and the human written abstracts. We use three common features: (1) sentence position in the document; (2) sentence length; and (3) interpolated n-gram document frequency as introduced in (Ng et al., 2012). The final sentence selection process follows the Table 2: Features used in our system besides those used in (Clarke and Lapata, 2008). 3.5 Learning To learn the feature weights during training, we perform ILP decoding on every sentence in the training set, to find the best hypothesis for each node in the expanded constituent parse tree. If the hypothesis is incorrect, we update the feature 696 CRF4 to implement the CRF sentence compression model. SVMlight5 is used for the summary sentence pre-selection model. Gurobi ILP solver6 does all ILP decoding. ILP method introduced in (Gillick et al.,"
D14-1076,W02-0401,0,0.0272814,"f instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulation: to extract a set of salient and nonredundant sentences from the given document set. Both unsupervised and supervised approaches have been explored for sentence selection. Supervised approaches include the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), discriminative reranking (Aker et al., 2010), among others. The extractive summary sentence selection problem can also be formulated in an optimization framework. Previous methods include using integer linear programming (ILP) and submodular functions to solve the optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010). Compressive summarization receives increasing attention in recent years, since it offers a viable step towards abstractive summarization. The compressed summaries can be generated through a joint model of the sentence"
D14-1076,P06-1055,0,0.0140636,"nnotated by (Li et al., 2013a) using TAC2010 data. In the compression module, for each word we also used its document level feature.2 Compression Data We also evaluate our compression model using the data set from (Clarke and Lapata, 2008). It includes 82 newswire articles with manually produced compression for each sentence. We use the same partitions as (Martins and Smith, 2009), i.e., 1,188 sentences for training and 441 for testing. Data Processing We use Stanford CoreNLP toolkit3 to tokenize the sentences, extract name entity tags, and generate the dependency parse tree. Berkeley Parser (Petrov et al., 2006) is adopted to obtain the constituent parse tree for every sentence and POS tag for every token. We use Pocket 4 http://sourceforge.net/projects/pocket-crf-1/ http://svmlight.joachims.org/ 6 http://www.gurobi.com 7 We chose to evaluate the linguistic quality for this system because of two reasons: one is that we have an implementation of that method; the other more important one is that it has the highest reported ROUGE results among the compared methods. 1 5 http://www.nist.gov/tac/data/index.html 2 Document level features for a word include information such as the word’s document frequency i"
D14-1076,D13-1156,1,0.838736,"ignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke and Lapata, 2008), but our method uses an expanded constituent parse tree rather than only the leaf nodes in previous work. Therefore we can extract rich features for every node in the constituent parser tree. Thi"
D14-1076,C04-1129,0,0.0108443,"los (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the parse trees and thus the compression decision can be made for a constituent, instead of a single word. ( 1 δi = 0 ∀i ∈ [1..n] ( 1 αi = 0 3 Sentence Compression Method if xi starts the compression otherwise ∀i ∈ [1..n] Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important content from the original sentence, and be grammatical. In some sense, sentence compression can be described as a"
D14-1076,W13-3508,0,0.0719819,"where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve grammatical structure. Nomoto (2007) treated the compression task as a sequence labeling problem and used CRF for it. Thadani and McKeown (2013) presented an approach for discriminative sentence compression that jointly produces sequential and syntactic representations for output text. Filippova and Altun (2013) presented a method to automatically build a sentence compression corpus with hundreds of thousands of instances on which deletion-based compression algorithms can be trained. 2 Related Work Summarization research has seen great development over the last fifty years (Nenkova and McKeown, 2011). Compared to the abstractive counterpart, extractive summarization has received considerable attention due to its clear problem formulat"
D14-1076,P05-1036,0,0.0120166,"m entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the parse trees and thus the compression decision can be made for a constituent, instead of a single word. ( 1 δi = 0 ∀i ∈ [1..n] ( 1 αi = 0 3 Sentence Compression Method if xi starts the compression otherwise ∀i ∈ [1..n] Sentence compression is a task of producing a summary for a single sentence. The compressed sentence should be shorter, contain important content from the original sentence, and be grammatical. In some sense, sentence compression can be described as a ‘scaled down version of the"
D14-1076,P13-1136,0,0.654749,"fuliang.weng@us.bosch.com} Abstract a viable step towards abstractive summarization. These compressive summaries often contain more information than sentence-based extractive summaries since they can remove insignificant sentence constituents and make space for more salient information that is otherwise dropped due to the summary length constraint. Two general strategies have been used for compressive summarization. One is a pipeline approach, where sentencebased extractive summarization is followed or proceeded by sentence compression (Lin, 2003; Zajic et al., 2007; Vanderwende et al., 2007; Wang et al., 2013). Another line of work uses joint compression and summarization. Such methods have been shown to achieve promising performance (Daum´e, 2006; Chali and Hasan, 2012; Almeida and Martins, 2013; Qian and Liu, 2013), but they are typically computationally expensive. In this study, we propose an innovative sentence compression model based on expanded constituent parse trees. Our model uses integer linear programming (ILP) to search the entire space of compression, and is discriminatively trained. It is built based on the discriminative sentence compression model from (McDonald, 2006) and (Clarke an"
D14-1076,P10-1058,0,0.0165962,"ompression for the summarization task. Knight and Marcu (2000) utilized the noisy channel and decision tree method to perform sentence compression in the summarization task. Lin (2003) showed that pure syntactic-based compression may not significantly improve the summarization performance. Zajic et al. (2007) compared two sentence compression approaches for multidocument summarization, including a ‘parse-andtrim’ and a noisy-channel approach. Galanis and Androutsopoulos (2010) used the maximum entropy model to generate the candidate compressions by removing branches from the source sentences. Woodsend and Lapata (2010) presented a joint content selection and compression model for single-document summarization. They operated over a phrase-based representation of the source document which they obtained by merging information from PCFG parse trees and dependency graphs. Liu and Liu (2013) adopted the CRFbased sentence compression approach for summa692 rizing spoken documents. Unlike the word-based operation, some of these models e.g (Knight and Marcu, 2000; Siddharthan et al., 2004; Turner and Charniak, 2005; Galanis and Androutsopoulos, 2010; Wang et al., 2013), are tree-based approaches that operate on the p"
D14-1076,D12-1022,0,0.489972,"al Methods in Natural Language Processing (EMNLP), pages 691–701, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics based summarization model is used to select the final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the"
D14-1076,P12-2068,0,0.0436108,"final compressed sentences. We evaluate our proposed method on the TAC 2008 and 2011 data sets using the standard ROUGE metric (Lin, 2004) and human evaluation of the linguistic quality. Our results show that using our proposed sentence compression model in the summarization system can yield significant performance gain in linguistic quality, without losing much performance on the ROUGE metric. ity. Woodsend and Lapata (2012) presented another method where the summary’s informativeness, succinctness, and grammaticality are learned separately from data but optimized jointly using an ILP setup. Yoshikawa et al. (2012) incorporated semantic role information in the ILP model. Our work is closely related with the pipeline approach, where sentence-based extractive summarization is followed or proceeded by sentence compression. There have been many studies on sentence compression, independent of the summarization task. McDonald (2006) firstly introduced a discriminative sentence compression model to directly optimize the quality of the compressed sentences produced. Clarke and Lapata (2008) improved the above discriminative model by using ILP in decoding, making it convenient to add constraints to preserve gram"
D14-1076,W01-0100,0,\N,Missing
D15-1228,P13-1020,0,0.165137,"P might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary ("
D15-1228,P11-1049,0,0.107664,"mmarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formali"
D15-1228,D12-1065,0,0.0165877,"s We use SVD in this study for computing sentence embeddings. As mentioned previously, our summariza1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In this work, the length of a sentence vector is not tailored to encode quality in terms of representativeness directly. In contrast, we rely on sentence embedding methods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one tha"
D15-1228,P14-1062,0,0.031379,"igh relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume (§2), and we provide a fast greedy algorithm that can be used to maximize it (§3). We show that our method outperforms competing extractive ba"
D15-1228,D12-1022,0,0.0258427,"e that is not especially tuned for this task—produces reasonably good results. We leave exploration of other sentence embedding methods to future work. Finally, an interesting future direction is finding an exact tractable solution to the volume maximization problem (or demonstrating that one does not exist). Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending"
D15-1228,D13-1047,1,0.453076,"d be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonnegative matrix factorization – provably. In Proc. of STOC."
D15-1228,N10-1134,0,0.0157165,"Goldstein, 1998) considers the following scoring function: score(D, y) = N X yi Rel(si ) − i=1 N X yi yj Sim(si , sj ) i,j=1 where Rel(si ) measures the relevancy of sentence i and Sim(si , sj ) measures the (e.g., cosine) similarity between sentence i and sentence j. The intuition is to choose sentences that are highly relevant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (McDonald, 2007). A greedy algorithm that approximates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often used in practice. 2.2 Coverage-Based Summarization Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. Gillick et al. (2008) use bigrams as a surrogate for concepts. Following convention, we extract bigrams from each sentence si ∈ D. Denote the number of unique bigrams extracted from all sentences by B. We introduce another binary vector z ∈ RB to indicate the presence or absence of a bigram in the summary, and a binary indicator matrix M ∈ RN ×B , where mi,j is 1 if and only if bigram j is present in sentence i and 0 otherwi"
D15-1228,W03-1101,0,0.0240553,"t one does not exist). Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonne"
D15-1228,W04-1013,0,0.0182322,". “Volume” refers to our method, shown with two embedding sizes. 12.0 We evaluate our proposed method on the nonupdate portion of TAC-2008 and TAC-2009. The datasets contain 48 and 44 multi-document summarization problems, respectively. Each problem has 10 news articles as input; each is to be summarized in a maximum of L = 100 words. There are 4 human reference summaries for each problem, against which an automatically generated summary is compared. We compare our method with two baselines: Maximal Marginal Relevance (MMR, §2.1) and the coverage-based summarization method (CBS, §2.2). ROUGE (Lin, 2004) is used to evaluate the summarization results. For preprocessing, we tokenize, stem with the Porter (1980) stemmer, and split documents into sentences. We remove bigrams consisting of only stopwords and bigrams which appear in less than 3 sentences. As a result, we have 2,746 and 3,273 bigrams for the TAC-2008 and TAC-2009 datasets respectively. Unlabeled data can help generate better sentence representations. For each summarization problem in each dataset, we use other problems in the same dataset as unlabeled data. We concatenate every problem in each dataset and perform SVD on this matrix"
D15-1228,W09-1801,1,0.64519,"this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideall"
D15-1228,P13-1136,0,0.0175161,"ork Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonnegative matrix factorization – provably."
D18-1310,P17-1177,0,0.0343573,"s through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computati"
D18-1310,C02-1025,0,0.0325374,"both Lample et al. (2016) and Ma and Hovy (2016) propose end-to-end models for sequence labelling task and achieve state-of-the-art results. ∗ † https://github.com/minghao-wu/CRF-AE Work carried out at The University of Melbourne Orthogonal to the advances in deep learning is the effort spent on feature engineering. A representative example is the task of named entity recognition (NER), one that requires both lexical and syntactic knowledge, where, until recently, most models heavily rely on statistical sequential labelling models taking in manually engineered features (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005). Typical features include POS and chunk tags, prefixes and suffixes, and external gazetteers, all of which represent years of accumulated knowledge in the field of computational linguistics. The work of Collobert et al. (2011) started the trend of feature engineering-free modelling by learning internal representations of compositional components of text (e.g., word embeddings). Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Penn"
D18-1310,Q16-1026,0,0.114368,"n auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance. Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss. An illustration of the model architecture is presented in Figure 1. Char-CNN. Previous studies (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016) have demonstrated that CNNs are highly capable of capturing character-level features. Here, our character-level CNN is similar to that used in Ma and Hovy (2016) but differs in that we use a ReLU activation (Nair and Hinton, 2010).1 Bi-LSTM. We use a Bi-LSTM to learn contextual information of a sequence of words. As inputs to the Bi-LSTM, we first concatenate the pre-trained embedding of each word wi with its character-level representation cwi (the output of the char-CNN) and a vector of manually crafted features fi (described in Section 2.2): → − − −−−−→ → h i = LSTM( h i"
D18-1310,W14-4012,0,0.151854,"Missing"
D18-1310,D14-1179,0,0.0421717,"Missing"
D18-1310,W03-0425,0,0.163953,"Missing"
D18-1310,E17-2068,0,0.0619774,"Missing"
D18-1310,N16-1030,0,0.44,"R shared task dataset (Tjong Kim Sang and De Meulder, 2003). 3.1 Experimental Setup Dataset. We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997. The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). 3 https://spacy.io/ 2852 Gazetteer data is included in the code release. Model configuration. Following the work of Ma and Hovy (2016), we initialise word embeddings with GloVe (Pennington et al., 2014) (300dimensional, trained on a 6B-token corpus). Character embeddings are 30-dimensional and randomly initialised withqa uniform distribution in q 3 3 the range [− dim , + dim ]. Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9. Exponential learning rate decay is applied every 5 epochs with a fact"
D18-1310,P17-1064,0,0.0720343,"mpressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Associ"
D18-1310,P18-2045,1,0.825962,"The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model. Ablation Study To gain a better understanding of the impacts of each feature, we perform an abModel F1 Chieu and Ng (2002) Florian et al. (2003) Ando and Zhang (2005) Collobert et al. (2011) Huang et al. (2015) Passos et al. (2014) Lample et al. (2016) Luo et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Peters et al. (2018) Peters et al. (2018)+ELMo Neural-CRF‡ Neural-CRF+A"
D18-1310,N18-2045,1,0.81702,"The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model. Ablation Study To gain a better understanding of the impacts of each feature, we perform an abModel F1 Chieu and Ng (2002) Florian et al. (2003) Ando and Zhang (2005) Collobert et al. (2011) Huang et al. (2015) Passos et al. (2014) Lample et al. (2016) Luo et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Peters et al. (2018) Peters et al. (2018)+ELMo Neural-CRF‡ Neural-CRF+A"
D18-1310,D15-1104,0,0.0447723,"Missing"
D18-1310,P16-1101,0,0.503843,"ully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Output Layer NER Feature Encoder Feature Concatenation Auto-Encoder Bi-directional LSTM Character Representations Input Sentence Word Embeddings Hand-crafted Features EU rejects German call to ... Figure 1: Main architecture of our neural network. Character representations are extracted by a characterlevel CNN. The dash line indicates we use an autoencoder loss to reconstruct h"
D18-1310,D17-1179,0,0.0460435,"Missing"
D18-1310,W14-1609,0,0.0502575,"Missing"
D18-1310,D14-1162,0,0.0795918,"2002; Ando and Zhang, 2005). Typical features include POS and chunk tags, prefixes and suffixes, and external gazetteers, all of which represent years of accumulated knowledge in the field of computational linguistics. The work of Collobert et al. (2011) started the trend of feature engineering-free modelling by learning internal representations of compositional components of text (e.g., word embeddings). Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we in"
D18-1310,N18-1202,0,0.0211338,"e. 3.2 Results The experimental results are presented in Table 2. Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss. Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features. Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model. Ablation Study To gain a better understanding of the impacts of each feature, we perform an abModel F1 Chieu and Ng (2002) Florian et al. (2003) Ando and Zhang (2005) Collobert et al. (2011) Huang et al. (2015) Passos et al. (2014) Lample et al. (2016) Luo et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Peters et al. (2018) Peters et al. (2018)+ELMo Neural-CRF‡ Neural-CRF+A"
D18-1310,W09-1119,0,0.114352,"ntity recognition over the CoNLL 2003 English NER shared task dataset (Tjong Kim Sang and De Meulder, 2003). 3.1 Experimental Setup Dataset. We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997. The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC. We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). 3 https://spacy.io/ 2852 Gazetteer data is included in the code release. Model configuration. Following the work of Ma and Hovy (2016), we initialise word embeddings with GloVe (Pennington et al., 2014) (300dimensional, trained on a 6B-token corpus). Character embeddings are 30-dimensional and randomly initialised withqa uniform distribution in q 3 3 the range [− dim , + dim ]. Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9. Exponential learning rat"
D18-1310,W03-0419,0,0.624297,"Missing"
D18-1310,P17-1065,0,0.0321078,"syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014). Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering. More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance. Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features. Of particular interest to this paper is the work by Ma and Hovy (2016) where they 2850 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics"
D18-1310,yang-etal-2017-neural-reranking,0,0.0328918,"Missing"
D18-1387,W17-3534,0,0.0238659,"of vagueness. Recent years have seen a growing interest in using natural language processing techniques to improve the effectiveness of website privacy policies. Sadeh et al. (2013) describe a Usable Privacy Policy Project that seeks to semi-automate the extraction of salient details from privacy policies. Other studies include crowdsourcing privacy policy annotations and categorizing data practices (Ammar et al., 2012; Massey et al., 2013; Wilson et al., 2016b,a), grouping text segments related to certain policy issues (Liu et al., 2014; Ramanath et al., 2014), summarizing terms of services (Braun et al., 2017), identifying user optout choices (Sathyendra et al., 2017), and many others. These studies emphasize the “too long to read” issue of privacy policies but leave behind the “difficult to understand” aspect, such as identifying and eliminating vague content. The work of (Liu et al., 2016) is close to ours. The authors attempt to learn vector representations of words in privacy policies using deep neural networks, where the vectors encode not only semantic/syntactic aspects but also vagueness of words. The model is later fed to an interactive visualization tool (Strobelt et al., 2016) to test its"
D18-1387,N18-1133,0,0.0144127,"gueness is an understudied property and it spans multiple syntactic categories (e.g., “usually,” “personal data,” “necessary”). Neural network classifiers such as CNN and LSTM have demonstrated prior success on text classification tasks (Zhang and Wallace, 2015), but whether they can be utilized to identify vague terms is not well understood. For sentence classification, we investigate auxiliary classifier generative adversarial networks (AC-GAN, Odena et al., 2018). GANs have seen growing popularity in recent years (Mirza and Osindero, 2014; Yu et al., 2016; Li et al., 2017; Gu et al., 2018; Cai and Wang, 2018). AC-GAN is a variant of GAN that generates word sequences using class-conditional probabilities. E.g., it generates “fake” privacy policy sentences exhibiting different degrees of vagueness (e.g., “clear,” “vague,” “extremely vague”). AC-GAN nicely combines real (human-annotated) and fake (synthetic) privacy policy sentences in a discriminative framework to improve the model’s generalization capabilities. This can be equated to a semisupervised learning paradigm through augmentation of the dataset with generated sentences. Data augmentation is particularly valuable for vagueness detection, wh"
D18-1387,Q16-1026,0,0.0179557,"the surrounding context words. Given its strong performance, we construct a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) for this purpose. A word is replaced by its word2vec embedding (Mikolov et al., 2013) before it is fed to the model. For each time step, we concatenate the hidden states obtained from the forward and backward passes and use it as input to a feedforward layer with sigmoid activation to predict if a word is vague or non-vague. Because single words consist of the majority of the human-annotated vague terms, we choose to use binary word labels instead of a BIO scheme (Chiu and Nichols, 2016) for sequence tagging. Figure 2 shows the architecture. Context-agnostic classifier. It uses intrinsic feature representations of words without considering the context. Specifically, we represent a word using its word2vec embedding, then feed it to a feedforward layer with sigmoid activation to obtain the prediction (Figure 2). We train the classifier using a list of unique words obtained from the training data; a word is considered positive if it has a ground truth label of 1 in any sentence, otherwise negative. Note that the ratio of positive/negative unique words in our corpus is 1068/3176="
D18-1387,N18-2081,0,0.139075,"task, because vagueness is an understudied property and it spans multiple syntactic categories (e.g., “usually,” “personal data,” “necessary”). Neural network classifiers such as CNN and LSTM have demonstrated prior success on text classification tasks (Zhang and Wallace, 2015), but whether they can be utilized to identify vague terms is not well understood. For sentence classification, we investigate auxiliary classifier generative adversarial networks (AC-GAN, Odena et al., 2018). GANs have seen growing popularity in recent years (Mirza and Osindero, 2014; Yu et al., 2016; Li et al., 2017; Gu et al., 2018; Cai and Wang, 2018). AC-GAN is a variant of GAN that generates word sequences using class-conditional probabilities. E.g., it generates “fake” privacy policy sentences exhibiting different degrees of vagueness (e.g., “clear,” “vague,” “extremely vague”). AC-GAN nicely combines real (human-annotated) and fake (synthetic) privacy policy sentences in a discriminative framework to improve the model’s generalization capabilities. This can be equated to a semisupervised learning paradigm through augmentation of the dataset with generated sentences. Data augmentation is particularly valuable for va"
D18-1387,P82-1020,0,0.755003,"Missing"
D18-1387,D14-1181,0,0.00515386,"Missing"
D18-1387,D17-1230,0,0.0728791,"is a challenging task, because vagueness is an understudied property and it spans multiple syntactic categories (e.g., “usually,” “personal data,” “necessary”). Neural network classifiers such as CNN and LSTM have demonstrated prior success on text classification tasks (Zhang and Wallace, 2015), but whether they can be utilized to identify vague terms is not well understood. For sentence classification, we investigate auxiliary classifier generative adversarial networks (AC-GAN, Odena et al., 2018). GANs have seen growing popularity in recent years (Mirza and Osindero, 2014; Yu et al., 2016; Li et al., 2017; Gu et al., 2018; Cai and Wang, 2018). AC-GAN is a variant of GAN that generates word sequences using class-conditional probabilities. E.g., it generates “fake” privacy policy sentences exhibiting different degrees of vagueness (e.g., “clear,” “vague,” “extremely vague”). AC-GAN nicely combines real (human-annotated) and fake (synthetic) privacy policy sentences in a discriminative framework to improve the model’s generalization capabilities. This can be equated to a semisupervised learning paradigm through augmentation of the dataset with generated sentences. Data augmentation is particularl"
D18-1387,C14-1084,1,0.886034,"tic case studies but not on developing resources for automatic detection of vagueness. Recent years have seen a growing interest in using natural language processing techniques to improve the effectiveness of website privacy policies. Sadeh et al. (2013) describe a Usable Privacy Policy Project that seeks to semi-automate the extraction of salient details from privacy policies. Other studies include crowdsourcing privacy policy annotations and categorizing data practices (Ammar et al., 2012; Massey et al., 2013; Wilson et al., 2016b,a), grouping text segments related to certain policy issues (Liu et al., 2014; Ramanath et al., 2014), summarizing terms of services (Braun et al., 2017), identifying user optout choices (Sathyendra et al., 2017), and many others. These studies emphasize the “too long to read” issue of privacy policies but leave behind the “difficult to understand” aspect, such as identifying and eliminating vague content. The work of (Liu et al., 2016) is close to ours. The authors attempt to learn vector representations of words in privacy policies using deep neural networks, where the vectors encode not only semantic/syntactic aspects but also vagueness of words. The model is later"
D18-1387,P14-2099,1,0.40374,"ut not on developing resources for automatic detection of vagueness. Recent years have seen a growing interest in using natural language processing techniques to improve the effectiveness of website privacy policies. Sadeh et al. (2013) describe a Usable Privacy Policy Project that seeks to semi-automate the extraction of salient details from privacy policies. Other studies include crowdsourcing privacy policy annotations and categorizing data practices (Ammar et al., 2012; Massey et al., 2013; Wilson et al., 2016b,a), grouping text segments related to certain policy issues (Liu et al., 2014; Ramanath et al., 2014), summarizing terms of services (Braun et al., 2017), identifying user optout choices (Sathyendra et al., 2017), and many others. These studies emphasize the “too long to read” issue of privacy policies but leave behind the “difficult to understand” aspect, such as identifying and eliminating vague content. The work of (Liu et al., 2016) is close to ours. The authors attempt to learn vector representations of words in privacy policies using deep neural networks, where the vectors encode not only semantic/syntactic aspects but also vagueness of words. The model is later fed to an interactive vi"
D18-1387,D17-1294,0,0.347448,"t in using natural language processing techniques to improve the effectiveness of website privacy policies. Sadeh et al. (2013) describe a Usable Privacy Policy Project that seeks to semi-automate the extraction of salient details from privacy policies. Other studies include crowdsourcing privacy policy annotations and categorizing data practices (Ammar et al., 2012; Massey et al., 2013; Wilson et al., 2016b,a), grouping text segments related to certain policy issues (Liu et al., 2014; Ramanath et al., 2014), summarizing terms of services (Braun et al., 2017), identifying user optout choices (Sathyendra et al., 2017), and many others. These studies emphasize the “too long to read” issue of privacy policies but leave behind the “difficult to understand” aspect, such as identifying and eliminating vague content. The work of (Liu et al., 2016) is close to ours. The authors attempt to learn vector representations of words in privacy policies using deep neural networks, where the vectors encode not only semantic/syntactic aspects but also vagueness of words. The model is later fed to an interactive visualization tool (Strobelt et al., 2016) to test its ability to discover related vague terms. While promising,"
D18-1387,P16-1126,0,0.421297,"l”, “many”) are all susceptible to reasoning. These studies often focus on linguistic case studies but not on developing resources for automatic detection of vagueness. Recent years have seen a growing interest in using natural language processing techniques to improve the effectiveness of website privacy policies. Sadeh et al. (2013) describe a Usable Privacy Policy Project that seeks to semi-automate the extraction of salient details from privacy policies. Other studies include crowdsourcing privacy policy annotations and categorizing data practices (Ammar et al., 2012; Massey et al., 2013; Wilson et al., 2016b,a), grouping text segments related to certain policy issues (Liu et al., 2014; Ramanath et al., 2014), summarizing terms of services (Braun et al., 2017), identifying user optout choices (Sathyendra et al., 2017), and many others. These studies emphasize the “too long to read” issue of privacy policies but leave behind the “difficult to understand” aspect, such as identifying and eliminating vague content. The work of (Liu et al., 2016) is close to ours. The authors attempt to learn vector representations of words in privacy policies using deep neural networks, where the vectors encode not o"
D18-1446,P18-1063,0,0.0605652,"has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The work of Baumel et al. (2018) and Zhang et al. (2018) are related to ours. In particular, Baumel et al. (2018) propose to extend an abstractive summarization system to generate query-focused summaries; Zhang et al. (2018) add a document set encoder to their hierarchical summarization framework. With these few exceptions, little research has been dedicated to investigate the feasibility of extending the encoder-decoder framework to generate abstractive summaries from multi-document inpu"
D18-1446,P16-1046,0,0.35489,"in a mega-document can be repeatedly used for summary generation under the current framework. The attention mechanism of an encoder-decoder model (Bahdanau et al., 2014) is position-based and lacks an awareness of semantics. If a text piece has been attended to during summary generation, it is unlikely to be used again. However, the attention value assigned to a similar text piece in a different position is not affected. The same content can thus be repeatedly used for summary generation. These issues may be alleviated by improving the encoder-decoder architecture and its attention mechanism (Cheng and Lapata, 2016; Tan et al., 2017). However, in these cases the model has to be re-trained on large-scale MDS datasets that are not available at the current stage. There is thus an increasing need for a lightweight adaptation of an encoder-decoder model trained on SDS datasets to work with multidocument inputs at test time. In this paper, we present a novel adaptation method, named PG-MMR, to generate abstracts from multi-document inputs. The method is robust and requires no MDS training data. It combines a recent neural encoder-decoder model (PG for Pointer-Generator networks; See et al., 2017) that generat"
D18-1446,P02-1057,0,0.413641,"Missing"
D18-1446,P99-1071,0,0.466657,"et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarizatio"
D18-1446,P16-1188,0,0.0538922,"Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan e"
D18-1446,P11-1049,0,0.184129,"lti-document summarization; • we demonstrate the effectiveness of the proposed method through extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion."
D18-1446,W14-4408,0,0.0594435,"Missing"
D18-1446,P15-1153,0,0.146645,"presentations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou"
D18-1446,D15-1042,0,0.0478428,"n standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and"
D18-1446,N10-1131,0,0.358896,"h pointer-generator networks for multi-document summarization; • we demonstrate the effectiveness of the proposed method through extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, pa"
D18-1446,W08-1106,0,0.0327304,"et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016;"
D18-1446,C10-1039,0,0.393368,"l., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016;"
D18-1446,D18-1443,0,0.0937348,"hes construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The work of Baumel et al. (2018) and Zhang et al. (2018) are related to ours. In particular, Baumel et al. (2018) propose to extend an abstractive summarization syste"
D18-1446,D14-1168,0,0.14178,"lus et al., 2017; See et al., 2017). These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student responses to post-class questionnaires (Luo and Litman, 2015; Luo et al., 2016), and sets of news articles discussing certain topics (Hong et al., 2014). State-of-the-art MDS systems are mostly extractive (Nenkova and McKeown, 2011). Despite their promising results, such systems cannot perform text abstraction, e.g., paraphrasing, generalization, and sentence fusion (Jing and McKeown, 1999). Further, annotated MDS datasets are often scarce, containing only hundreds of training pairs (see Table 1). The cost to create ground-truth summaries from multiple-document inputs can be prohibit"
D18-1446,W09-1802,0,0.379493,"l relevance algorithm with pointer-generator networks for multi-document summarization; • we demonstrate the effectiveness of the proposed method through extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capa"
D18-1446,N18-1065,0,0.0599899,"tten by single authors. For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary (Rush et al., 2015; Nallapati et al., 2016; Takase et al., 2016; Song et al., 2018); single-document summarization (SDS) focuses on condensing a news article to a handful of bullet points (Paulus et al., 2017; See et al., 2017). These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student responses to post-class questionnaires (Luo and Litman, 2015; Luo et al., 2016), and sets of news articles discussing certain topics (Hong et al., 2014). State-of-the-art MDS systems are mostly extractive (Nenkova and McKeown, 2011). Despite their promising res"
D18-1446,P16-1154,0,0.0280631,"m and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The work of Baumel et al. (2018) and Zhang et al. (2018) are related to ours. In particular, Baumel et al. (2018) propose to extend an abstractive summarization system to generate query-focused summaries; Zhang et al. (2018) add a"
D18-1446,P16-1014,0,0.0222163,"ner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The work of Baumel et al. (2018) and Zhang et al. (2018) are related to ours. In particular, Baumel et al. (2018) propose to extend an abstractive summarization system to generate query-focused summaries; Zhang et"
D18-1446,N09-1041,0,0.347479,"ing steps are set to 120/100 words respectively, corresponding to the max/min lengths of the PG-MMR summaries. Because the focus of this work is on multi-document summarization (MDS), we do not report results for the CNN/Daily Mail dataset. Baselines. We compare PG-MMR against a broad spectrum of baselines, including state-of-the-art extractive (‘ext-’) and abstractive (‘abs-’) systems. They are described below.4 • ext-SumBasic (Vanderwende et al., 2007) is an extractive approach assuming words occurring frequently in a document set are more likely to be included in the summary; • ext-KL-Sum (Haghighi and Vanderwende, 2009) greedily adds source sentences to the summary if it leads to a decrease in KL divergence; • ext-LexRank (Erkan and Radev, 2004) uses a graph-based approach to compute sentence importance based on eigenvector centrality in a graph representation; • ext-Centroid (Hong et al., 2014) computes the importance of each source sentence based on its cosine similarity with the document centroid; • ext-ICSISumm (Gillick et al., 2009) leverages the ILP framework to identify a globally-optimal set of sentences covering the most important concepts in the document set; 3 The hyperparameters for all PG-MMR va"
D18-1446,P82-1020,0,0.820495,"Missing"
D18-1446,hong-etal-2014-repository,0,0.676267,"g Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student responses to post-class questionnaires (Luo and Litman, 2015; Luo et al., 2016), and sets of news articles discussing certain topics (Hong et al., 2014). State-of-the-art MDS systems are mostly extractive (Nenkova and McKeown, 2011). Despite their promising results, such systems cannot perform text abstraction, e.g., paraphrasing, generalization, and sentence fusion (Jing and McKeown, 1999). Further, annotated MDS datasets are often scarce, containing only hundreds of training pairs (see Table 1). The cost to create ground-truth summaries from multiple-document inputs can be prohibitive. The MDS datasets are thus too small to be used to train neural encoder-decoder models with millions of parameters without overfitting. A promising route to g"
D18-1446,D17-1223,0,0.0303884,"ent summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner"
D18-1446,D16-1140,0,0.0182955,"; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The"
D18-1446,D18-1207,0,0.0664785,"Missing"
D18-1446,D13-1047,1,0.933249,"Missing"
D18-1446,C18-1101,1,0.841589,"ocument summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See"
D18-1446,W04-1013,0,0.0633323,"et al., 2018) is a recent approach that scores sentences using LexRank and generates a title-like summary for each sentence using an encoderdecoder model trained on Gigaword data. • abs-PG-Original (See et al., 2017) introduces an encoderdecoder model that encourages the system to copy words from the source text via pointing, while retaining the ability to produce novel words through the generator. 6 Results Having described the experimental setup, we next compare the PG-MMR method against the baselines on standard MDS datasets, evaluated by both automatic metrics and human assessors. ROUGE (Lin, 2004). This automatic metric measures the overlap of unigrams (R-1), bigrams (R2) and skip bigrams with a maximum distance of 4 words (R-SU4) between the system summary and a set of reference summaries. ROUGE scores of various systems are presented in Table 2 and 3 respectively for the DUC-04 and TAC-11 datasets. We explore variants of the PG-MMR method. 4136 As seen in Table 2 and 3, our PG-MMR method surpasses all unsupervised extractive baselines, including SumBasic, KLSumm, and LexRank. On the DUC-04 dataset, ICSISumm and DPP show good performance, but these systems are trained directly on MDS"
D18-1446,N15-1114,1,0.880173,"ingle- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paul"
D18-1446,D15-1227,0,0.271385,"mpowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student responses to post-class questionnaires (Luo and Litman, 2015; Luo et al., 2016), and sets of news articles discussing certain topics (Hong et al., 2014). State-of-the-art MDS systems are mostly extractive (Nenkova and McKeown, 2011). Despite their promising results, such systems cannot perform text abstraction, e.g., paraphrasing, generalization, and sentence fusion (Jing and McKeown, 1999). Further, annotated MDS datasets are often scarce, containing only hundreds of training pairs (see Table 1). The cost to create ground-truth summaries from multiple-document inputs can be prohibitive. The MDS datasets are thus too small to be used to train neural en"
D18-1446,N16-1010,1,0.884475,"llel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student responses to post-class questionnaires (Luo and Litman, 2015; Luo et al., 2016), and sets of news articles discussing certain topics (Hong et al., 2014). State-of-the-art MDS systems are mostly extractive (Nenkova and McKeown, 2011). Despite their promising results, such systems cannot perform text abstraction, e.g., paraphrasing, generalization, and sentence fusion (Jing and McKeown, 1999). Further, annotated MDS datasets are often scarce, containing only hundreds of training pairs (see Table 1). The cost to create ground-truth summaries from multiple-document inputs can be prohibitive. The MDS datasets are thus too small to be used to train neural encoder-decoder model"
D18-1446,D16-1031,0,0.0172139,"14; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The work of Baumel et al. (2018) and Zhang et a"
D18-1446,K16-1028,0,0.387064,"-sent 100 words multi-sent a news article 10 news articles related to a topic 10 news articles related to a topic #PAIRS 4 Million 312 K 728 320 Table 1: A comparison of datasets available for sent. summarization (Gigaword), single-doc (CNN/DM) and multi-doc summarization (DUC/TAC). The labelled data for multi-doc summarization are much less. Introduction Neural abstractive summarization has primarily focused on summarizing short texts written by single authors. For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary (Rush et al., 2015; Nallapati et al., 2016; Takase et al., 2016; Song et al., 2018); single-document summarization (SDS) focuses on condensing a news article to a handful of bullet points (Paulus et al., 2017; See et al., 2017). These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of"
D18-1446,N18-1158,0,0.0288553,"tant sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural lang"
D18-1446,P14-1084,0,0.0616527,"earn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng"
D18-1446,P13-1136,0,0.181364,"sed method through extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been inv"
D18-1446,C08-1124,0,0.0800336,"ocument summarization datasets (Over and Yen, 4135 2 https://github.com/ucfnlp/multidoc summarization 2004; Dang and Owczarzak, 2008). These include DUC-03, DUC-04, TAC-08, TAC-10, and TAC11, containing 30/50/48/46/44 topics respectively. The summarization system is tasked with generating a concise, fluent summary of 100 words or less from a set of 10 documents discussing a topic. All documents in a set are chronologically ordered and concatenated to form a mega-document serving as input to the PG-MMR system. Sentences that start with a quotation mark or do not end with a period are excluded (Wong et al., 2008). Each system summary is compared against 4 human abstracts created by NIST assessors. Following convention, we report results on DUC-04 and TAC-11 datasets, which are standard test sets; DUC-03 and TAC-08/10 are used as a validation set for hyperparameter tuning.3 The PG model is trained for single-document summarization using the CNN/Daily Mail (Hermann et al., 2015) dataset, containing single news articles paired with summaries (human-written article highlights). The training set contains 287,226 articles. An article contains 781 tokens on average; and a summary contains 56 tokens (3.75 sen"
D18-1446,K17-1045,0,0.183312,"been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et al., 1999; Carenini and Cheung, 2008; Ganesan et al., 2010; Gerani et al., 2014; Fabbrizio et al., 2014; Pighin et al., 2014; Bing et al., 2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system an"
D18-1446,D15-1044,0,0.182671,"marization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors. 1 D ATASET S OURCE S UMMARY Gigaword (Rush et al., 2015) CNN/Daily Mail (Hermann et al., 2015) TAC (08-11) (Dang et al., 2008) DUC (03-04) (Over and Yen, 2004) the first sentence of a news article 8.3 words title-like 56 words multi-sent 100 words multi-sent 100 words multi-sent a news article 10 news articles related to a topic 10 news articles related to a topic #PAIRS 4 Million 312 K 728 320 Table 1: A comparison of datasets available for sent. summarization (Gigaword), single-doc (CNN/DM) and multi-doc summarization (DUC/TAC). The labelled data for multi-doc summarization are much less. Introduction Neural abstractive summarization has primaril"
D18-1446,D15-1228,1,0.929407,"extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summarization has been investigated (Barzilay et"
D18-1446,P17-1099,0,0.0594498,"or sent. summarization (Gigaword), single-doc (CNN/DM) and multi-doc summarization (DUC/TAC). The labelled data for multi-doc summarization are much less. Introduction Neural abstractive summarization has primarily focused on summarizing short texts written by single authors. For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary (Rush et al., 2015; Nallapati et al., 2016; Takase et al., 2016; Song et al., 2018); single-document summarization (SDS) focuses on condensing a news article to a handful of bullet points (Paulus et al., 2017; See et al., 2017). These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple authors to a short and informative summary. It has practical applications, such as summarizing product reviews (Gerani et al., 2014), student respo"
D18-1446,C18-1146,1,0.900033,"news articles related to a topic 10 news articles related to a topic #PAIRS 4 Million 312 K 728 320 Table 1: A comparison of datasets available for sent. summarization (Gigaword), single-doc (CNN/DM) and multi-doc summarization (DUC/TAC). The labelled data for multi-doc summarization are much less. Introduction Neural abstractive summarization has primarily focused on summarizing short texts written by single authors. For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary (Rush et al., 2015; Nallapati et al., 2016; Takase et al., 2016; Song et al., 2018); single-document summarization (SDS) focuses on condensing a news article to a handful of bullet points (Paulus et al., 2017; See et al., 2017). These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely written by multiple aut"
D18-1446,D16-1112,0,0.0196236,"nt a news article 10 news articles related to a topic 10 news articles related to a topic #PAIRS 4 Million 312 K 728 320 Table 1: A comparison of datasets available for sent. summarization (Gigaword), single-doc (CNN/DM) and multi-doc summarization (DUC/TAC). The labelled data for multi-doc summarization are much less. Introduction Neural abstractive summarization has primarily focused on summarizing short texts written by single authors. For example, sentence summarization seeks to reduce the first sentence of a news article to a title-like summary (Rush et al., 2015; Nallapati et al., 2016; Takase et al., 2016; Song et al., 2018); single-document summarization (SDS) focuses on condensing a news article to a handful of bullet points (Paulus et al., 2017; See et al., 2017). These summarization studies are empowered by large parallel datasets automatically harvested from online news outlets, including Gigaword (Rush et al., 2015), CNN/Daily Mail (Hermann et al., 2015), NYT (Sandhaus, 2008), and Newsroom (Grusky et al., 2018). To date, multi-document summarization (MDS) has not yet fully benefited from the development of neural encoder-decoder models. MDS seeks to condense a set of documents likely wri"
D18-1446,P17-1101,0,0.0411557,"2015; Liu et al., 2015; Liao et al., 2018). These approaches construct domain templates using a text planner or an open-IE system and employ a natural language generator for surface realization. Limited by the availability of labelled data, experiments are often performed on small domain-specific datasets. Neural abstractive summarization utilizing the encoder-decoder architecture has shown promising results but studies focus primarily on singledocument summarization (Nallapati et al., 2016; Kikuchi et al., 2016; Chen et al., 2016; Miao and Blunsom, 2016; Tan et al., 2017; Zeng et al., 2017; Zhou et al., 2017; Paulus et al., 2017; See et al., 2017; Gehrmann et al., 2018). The pointing mechanism (Gulcehre et al., 2016; Gu et al., 2016) allows a summarization system to both copy words from the source text and generate new words from the vocabulary. Reinforcement learning is exploited to directly optimize evaluation metrics (Paulus et al., 2017; Kry´sci´nski et al., 2018; Chen and Bansal, 2018). These studies fo4132 cus on summarizing single documents in part because the training data are abundant. The work of Baumel et al. (2018) and Zhang et al. (2018) are related to ours. In particular, Baumel et"
D18-1446,P17-1108,0,0.164865,"e repeatedly used for summary generation under the current framework. The attention mechanism of an encoder-decoder model (Bahdanau et al., 2014) is position-based and lacks an awareness of semantics. If a text piece has been attended to during summary generation, it is unlikely to be used again. However, the attention value assigned to a similar text piece in a different position is not affected. The same content can thus be repeatedly used for summary generation. These issues may be alleviated by improving the encoder-decoder architecture and its attention mechanism (Cheng and Lapata, 2016; Tan et al., 2017). However, in these cases the model has to be re-trained on large-scale MDS datasets that are not available at the current stage. There is thus an increasing need for a lightweight adaptation of an encoder-decoder model trained on SDS datasets to work with multidocument inputs at test time. In this paper, we present a novel adaptation method, named PG-MMR, to generate abstracts from multi-document inputs. The method is robust and requires no MDS training data. It combines a recent neural encoder-decoder model (PG for Pointer-Generator networks; See et al., 2017) that generates abstractive summ"
D18-1446,W13-3508,0,0.172169,"effectiveness of the proposed method through extensive experiments on standard MDS datasets. Our system compares favorably to state-of-the-art extractive and abstractive summarization systems measured by both automatic metrics and human judgments. 2 Related Work Popular methods for multi-document summarization have been extractive. Important sentences are extracted from a set of source documents and optionally compressed to form a summary (Daume III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Galanis and Androutsopoulos, 2010; Berg-Kirkpatrick et al., 2011; Li et al., 2013; Thadani and McKeown, 2013; Wang et al., 2013; Yogatama et al., 2015; Filippova et al., 2015; Durrett et al., 2016). In recent years neural networks have been exploited to learn word/sentence representations for single- and multi-document summarization (Cheng and Lapata, 2016; Cao et al., 2017; Isonuma et al., 2017; Yasunaga et al., 2017; Narayan et al., 2018). These approaches remain extractive; and despite encouraging results, summarizing a large quantity of texts still requires sophisticated abstraction capabilities such as generalization, paraphrasing and sentence fusion. Prior to deep learning, abstractive summari"
D18-1446,W01-0100,0,\N,Missing
D19-1053,J13-2002,0,0.353775,"Missing"
D19-1053,hovy-etal-2006-automated,0,0.0762672,"lenging. CIDEr (Vedantam et al., 2015) uses tf-idf weighted n-grams for similarity estimation; and SPICE (Anderson et al., 2016) incorporates Summarization A dominant metric for summarization evaluation is ROUGE (Lin, 2004), which measures the degree of lexical overlap between a system summary and a set of reference summaries. Its variants consider overlap of unigrams (-1), bigrams (-2), unigrams and skip bigrams with a maximum gap of 4 words (-SU4), longest common subsequences (-L) and its weighted version (-W-1.2), among others. Metrics such as Pyramid (Nenkova and Passonneau, 2004) and BE (Hovy et al., 2006; 564 scribe our method in detail. synonym matching over scene graphs. Novikova et al. (2017) examine a large number of word- and grammar-based metrics and demonstrate that they only weakly reflect human judgments of system outputs generated by data-driven, end-to-end natural language generation systems. 3 Our MoverScore Meric We have motivated the need for better metrics capable of evaluating disparate NLG tasks. We now describe our metric, namely MoverScore, built upon a combination of (i) contextualized representations of system and reference texts and (ii) a distance between these represen"
D19-1053,P19-1269,0,0.0446107,"(Peyrard et al., 2017; Shimanaka et al., 2018) to improve semantic similarity estimation, replacing lexical overlaps. In contemporaneous work, Zhang et al. (2019) describe a method comparing system and reference texts for semantic similarity leveraging the BERT representations (Devlin et al., 2018), which can be viewed as a special case of our metrics and will be discussed in more depth later. More recently, Clark et al. (2019) present a semantic metric relying on sentence mover’s similarity and the ELMo representations (Peters et al., 2018) and apply them to summarization and essay scoring. Mathur et al. (2019) introduce unsupervised and supervised metrics based on the BERT representations to improve MT evaluation, while Peyrard (2019a) provides a composite score combining redundancy, relevance and informativeness to improve summary evaluation. In this paper, we seek to accurately measure the (dis)similarity between system and reference texts drawing inspiration from contextualized representations and Word Mover’s Distance (WMD; Kusner et al., 2015). WMD finds the “traveling distance” of moving from the word frequency distribution of the system text to that of the reference, which is essential to ca"
D19-1053,W18-6319,0,0.0179801,"er metrics include SentBLEU, NIST, chrF, TER, WER, PER, CDER, and METEOR (Lavie and Agarwal, 2007) that are used and described in the WMT metrics shared task (Bojar et al., 2017; Ma et al., 2018). RUSE (Shimanaka et al., 2018) is a recent effort to improve MT evaluation by training sentence embeddings on large-scale data obtained in other tasks. Additionally, preprocessing reference texts is crucial in MT evaluation, e.g., normalization, tokenization, compound splitting, etc. If not handled properly, different preprocessing strategies can lead to inconsistent results using word-based metrics (Post, 2018). • Our metric outperforms or performs comparably to strong baselines on four text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, suggesting this is a promising direction moving forward. 2 Related Work It is of fundamental importance to design evaluation metrics that can be applied to natural language generation tasks of similar nature, including summarization, machine translation, data-to-text generation, image captioning, and many others. All these tasks involve generating texts of sentence or paragraph length. The system texts a"
D19-1053,J18-3002,0,0.0128134,"e text as well as generating unseen words (See et al., 2017). This aspect is hardly covered by existing metrics. With greater flexibility comes increased demand for unbiased evaluation. Diversity-promoting objectives make it possible to generate diverse natural language descriptions (Li et al., 2016; Wiseman et al., 2018). But standard evaluation metrics including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) compute the scores based primarily on n-gram co-occurrence statistics, which are originally proposed for diagnostic evaluation of systems but not capable of evaluating text quality (Reiter, 2018), as they are not designed to measure if, and to what extent, the system and reference texts with distinct surface forms have conveyed the same meaning. Recent effort on the applicability of these metrics reveals that while compelling text generation system ascend on standard metrics, the text quality of system output is still hard to be improved (B¨ohm et al., 2019). A robust evaluation metric has a profound impact on the development of text generation systems. A desirable metric compares system output against references based on their semantics rather than surface forms. In this paper we inv"
D19-1053,N04-1019,0,0.885746,"quality of image captions can be challenging. CIDEr (Vedantam et al., 2015) uses tf-idf weighted n-grams for similarity estimation; and SPICE (Anderson et al., 2016) incorporates Summarization A dominant metric for summarization evaluation is ROUGE (Lin, 2004), which measures the degree of lexical overlap between a system summary and a set of reference summaries. Its variants consider overlap of unigrams (-1), bigrams (-2), unigrams and skip bigrams with a maximum gap of 4 words (-SU4), longest common subsequences (-L) and its weighted version (-W-1.2), among others. Metrics such as Pyramid (Nenkova and Passonneau, 2004) and BE (Hovy et al., 2006; 564 scribe our method in detail. synonym matching over scene graphs. Novikova et al. (2017) examine a large number of word- and grammar-based metrics and demonstrate that they only weakly reflect human judgments of system outputs generated by data-driven, end-to-end natural language generation systems. 3 Our MoverScore Meric We have motivated the need for better metrics capable of evaluating disparate NLG tasks. We now describe our metric, namely MoverScore, built upon a combination of (i) contextualized representations of system and reference texts and (ii) a dista"
D19-1053,D15-1222,0,0.124306,"emantic textual similarity measures (Peters et al., 2018; Devlin et al., 2018); but also to accurately reflect to what extent the system text has deviated from the reference, i.e., union(A,B) - intersect(A,B), which is the intuition behind using a distance metric. Metrics based on Continuous Representations Moving beyond traditional metrics, we envision a new generation of automated evaluation metrics comparing system and reference texts based on semantics rather than surface forms to achieve better correlation with human judgments. A number of previous studies exploit static word embeddings (Ng and Abrecht, 2015; Lo, 2017) and trained classifers (Peyrard et al., 2017; Shimanaka et al., 2018) to improve semantic similarity estimation, replacing lexical overlaps. In contemporaneous work, Zhang et al. (2019) describe a method comparing system and reference texts for semantic similarity leveraging the BERT representations (Devlin et al., 2018), which can be viewed as a special case of our metrics and will be discussed in more depth later. More recently, Clark et al. (2019) present a semantic metric relying on sentence mover’s similarity and the ELMo representations (Peters et al., 2018) and apply them to"
D19-1053,P17-1099,0,0.237822,"Missing"
D19-1053,W18-6456,0,0.329942,"trics are commonly used in MT evaluation. Most of these metrics compare system and reference translations based on surface forms such as word/character n-gram overlaps and edit distance, but not the meanings they convey. BLEU (Papineni et al., 2002) is a precision metric measuring how well a system translation overlaps with human reference translations using n-gram co-occurrence statistics. Other metrics include SentBLEU, NIST, chrF, TER, WER, PER, CDER, and METEOR (Lavie and Agarwal, 2007) that are used and described in the WMT metrics shared task (Bojar et al., 2017; Ma et al., 2018). RUSE (Shimanaka et al., 2018) is a recent effort to improve MT evaluation by training sentence embeddings on large-scale data obtained in other tasks. Additionally, preprocessing reference texts is crucial in MT evaluation, e.g., normalization, tokenization, compound splitting, etc. If not handled properly, different preprocessing strategies can lead to inconsistent results using word-based metrics (Post, 2018). • Our metric outperforms or performs comparably to strong baselines on four text generation tasks including summarization, machine translation, image captioning, and data-to-text generation, suggesting this is a p"
D19-1053,P02-1040,0,0.106926,"Central Florida, US zhao@aiphes.tu-darmstadt.de, maxime.peyrard@epfl.ch feiliu@cs.ucf.edu, yang.gao@rhul.ac.uk meyer@ukp.informatik.tu-darmstadt.de eger@aiphes.tu-darmstadt.de Abstract the flexibility to copy content from source text as well as generating unseen words (See et al., 2017). This aspect is hardly covered by existing metrics. With greater flexibility comes increased demand for unbiased evaluation. Diversity-promoting objectives make it possible to generate diverse natural language descriptions (Li et al., 2016; Wiseman et al., 2018). But standard evaluation metrics including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) compute the scores based primarily on n-gram co-occurrence statistics, which are originally proposed for diagnostic evaluation of systems but not capable of evaluating text quality (Reiter, 2018), as they are not designed to measure if, and to what extent, the system and reference texts with distinct surface forms have conveyed the same meaning. Recent effort on the applicability of these metrics reveals that while compelling text generation system ascend on standard metrics, the text quality of system output is still hard to be improved (B¨ohm et al., 2019). A robust ev"
D19-1053,N18-1202,0,0.554801,"latedness between abstractive summaries and their references, as a system abstract can convey the same meaning using different surface forms. Furthermore, large-scale summarization datasets such as CNN/Daily Mail (Hermann et al., 2015) and Newsroom (Grusky et al., 2018) use a single reference summary, making it harder to obtain unbiased results when only lexical overlap is considered during summary evaluation. multiple natural language generation tasks. Our new metric quantifies the semantic distance between system and reference texts by harnessing the power of contextualized representations (Peters et al., 2018; Devlin et al., 2018) and a powerful distance metric (Rubner et al., 2000) for better content matching. Our contributions can be summarized as follows: • We formulate the problem of evaluating generation systems as measuring the semantic distance between system and reference texts, assuming powerful continuous representations can encode any type of semantic and syntactic deviations. • We investigate the effectiveness of existing contextualized representations and Earth Mover’s Distance (Rubner et al., 2000) for comparing system predictions and reference texts, leading to our new automated eva"
D19-5408,D18-1443,0,0.0604507,"s. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, while system abstracts can sometimes “hallucinate” facts and distort the original meaning. Our contributions in this work include the following: • we introduce a new task formulation of creating sub-se"
D19-5408,P14-1119,0,0.0150071,"portant, a human uses a single stroke to highlight them all, up to the whole sentence. If only a part of the sentence is relevant, she only picks out that particular sentence part. Introduction Highlighting at an appropriate level of granularity is important to emphasize salient content in an unobtrusive manner. A small collection of keywords may be insufficient to deliver the main points of an article, while highlighting whole sentences often provide superfluous information. In domains such as newswire, scholarly publications, legal and policy documents (Kim et al., 2010; Sadeh et al., 2013; Hasan and Ng, 2014), people are tempted to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous"
D19-5408,N19-1264,1,0.865038,"Missing"
D19-5408,S10-1004,0,0.044016,"If multiple parts of a sentence are important, a human uses a single stroke to highlight them all, up to the whole sentence. If only a part of the sentence is relevant, she only picks out that particular sentence part. Introduction Highlighting at an appropriate level of granularity is important to emphasize salient content in an unobtrusive manner. A small collection of keywords may be insufficient to deliver the main points of an article, while highlighting whole sentences often provide superfluous information. In domains such as newswire, scholarly publications, legal and policy documents (Kim et al., 2010; Sadeh et al., 2013; Hasan and Ng, 2014), people are tempted to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating"
D19-5408,P11-1049,0,0.0331564,"en provide superfluous information. In domains such as newswire, scholarly publications, legal and policy documents (Kim et al., 2010; Sadeh et al., 2013; Hasan and Ng, 2014), people are tempted to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to b"
D19-5408,D18-1446,1,0.828226,"esirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, while system abstracts can sometimes “hallucinate” facts and distort the original meaning. Our contributions in this work include the following: • we introduce a new task formulation of creating sub-sentence summary highligh"
D19-5408,N18-1150,0,0.0250638,"ly important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, while system abstracts can sometimes “hallucinate” facts and distort the original meaning. Our contributions in this work include the following: • we introduce a new task formulation of creating sub-sentence summary highlights, then describe 64 Procee"
D19-5408,P18-1063,0,0.0662055,"nd complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, while system abstracts can sometimes “hallucinate” facts and distort the original meaning. Our contributions in this work include the following: • we introduce a new task formula"
D19-5408,D13-1047,1,0.895331,"Missing"
D19-5408,P16-1046,0,0.157722,"Missing"
D19-5408,P02-1057,0,0.0613248,"Missing"
D19-5408,W09-1801,0,0.0321943,"le, while highlighting whole sentences often provide superfluous information. In domains such as newswire, scholarly publications, legal and policy documents (Kim et al., 2010; Sadeh et al., 2013; Hasan and Ng, 2014), people are tempted to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be di"
D19-5408,P17-1099,0,0.248282,"ed to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, while system abstracts can sometimes “hallucinate” facts and distort the original meaning. Our contributions in this work include the following: • we intro"
D19-5408,W13-3508,0,0.0174133,"ion. In domains such as newswire, scholarly publications, legal and policy documents (Kim et al., 2010; Sadeh et al., 2013; Hasan and Ng, 2014), people are tempted to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, whi"
D19-5408,P13-1136,0,0.0229532,"swire, scholarly publications, legal and policy documents (Kim et al., 2010; Sadeh et al., 2013; Hasan and Ng, 2014), people are tempted to write long and complicated sentences. It is particularly desirable to pick out only important sentence parts as opposed to whole sentences. Generating highlights at the sub-sentence level has not been thoroughly investigated in the past. A related thread of research is extractive and compressive summarization (Daum´e III and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; Filippova, 2010; Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Durrett et al., 2016). Generating sub-sentence highlights is advantageous over abstraction (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018; Celikyilmaz et al., 2018) in several aspects. The highlights can be overlaid on the source document, allowing them to be interpreted in context. The number of highlights is controllable by limiting sentence selection. In contrast, adjusting summary length in an end-to-end, abstractive system can be difficult. Further, highlights are guaranteed to be true-to-the-original, while system abstracts"
D19-5412,C10-1039,0,0.699654,"are incorporated in this work to capture the position of a sentence in the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35"
D19-5412,W09-1802,0,0.288394,"resentations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the qualit"
D19-5412,hong-etal-2014-repository,0,0.313108,"e modelled using shallow and linguistically informed features, but the rise of deep contextualized representations raises an interesting question of whether, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and select"
D19-5412,P19-1098,1,0.830669,"ters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019) by incorporating deep contextualized representations into DPP, with an emphasis on better sentence selection for extractive multi-document summarization. The major research contributions of this work include the following: (i) we make a first attempt to combine DPP with BERT representations to measure sentence quality and diversity and report encouraging results on benchmark summarization datasets; (ii) our findings suggest that it is best to model sentence quality, i.e., how important a sentence is to the summary, by combining semantic representations and surface indicators of the sentence,"
D19-5412,P19-1285,0,0.0200765,"high quality, any set containing it will have a high probability score. If two sentences contain redundant information, they cannot both be included in the summary, thus any set containing both of them will have a low probability. DPP focuses on selecting the most probable set of sentences to form a summary according to sentence quality and diversity measures. To better measure quality and diversity aspects, we draw on deep contextualized representations. A number of models have been proposed recently, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019) by incorporating deep contextualized representations into DPP, with an em"
D19-5412,P19-1209,1,0.829815,"s a feature vector for sentence i and θ are feature weights to be learned during DPP training. We optimize θ by maximizing log-likelihood with gradient descent, illustrated as follows: L(θ)= M X logP(Yˆ (m);L(m)(θ)), 2.1 We introduce two models that fine-tune the BERTbase architecture (Devlin et al., 2018) to calculate the similarity between a pair of sentences (BERTsim) and learn representations that characterize the importance of a single sentence (BERT-imp). Importantly, training instances for both BERT models are derived from single-document summarization dataset (Hermann et al., 2015) by Lebanoff et al. (2019), containing a collection of single sentences (or sentence pairs) and their associated labels. During testing, the trained BERT models are applied to single sentences and sentence pairs derived from multi-document input to obtain quality and similarity measures. BERT-sim takes as input a pair of sentences and transforms each token in the sentence into an embedding using an embedding layer. They are then passed through the BERT-base architecture to pro(3) m=1 ∇θ = M X X m=1 i∈Yˆ (m) f (i)− X (m) f (j)Kjj , BERT Architecture (4) j 99 duce a vector representing the input sentence pair. The vector"
D19-5412,D15-1228,1,0.848755,"dings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar, 2012). A quality measure is a positive number indicating how important 98 Proceedings of the 2nd Worksh"
D19-5412,W04-1013,0,0.028359,"ce pairs and the instances are balanced. 1 The sentence features include the length and position of a sentence, the cosine similarity between sentence and document TF-IDF vectors (Kulesza and Taskar, 2011). We abstain from using sophisticated features to avoid model overfitting. 2 100 The coefficient is set to be 0.9 for both datasets. DUC/TAC We evaluate our DPP approach (§2) on multi-document summarization datasets including DUC and TAC (Over and Yen, 2004; Dang and Owczarzak, 2008). The task is to generate a summary of 100 words from a collection of news articles. We report ROUGE F-scores (Lin, 2004)3 on DUC-04 (trained on DUC-03) and TAC-11 (trained on TAC-08/09/10) following standard settings (Hong et al., 2014). Ground-truth extractive summaries used in DPP training are obtained from Cho et al. (2019). 3.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm (Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) Experiment Settings We"
D19-5412,N10-1134,0,0.0438514,"DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar, 2012). A quality measure is a positive number indicating how important 98 Proce"
D19-5412,2021.ccl-1.108,0,0.129635,"Missing"
D19-5412,N18-1202,0,0.0532157,"ty measure compares a pair of sentences for redundancy. If a sentence is of high quality, any set containing it will have a high probability score. If two sentences contain redundant information, they cannot both be included in the summary, thus any set containing both of them will have a low probability. DPP focuses on selecting the most probable set of sentences to form a summary according to sentence quality and diversity measures. To better measure quality and diversity aspects, we draw on deep contextualized representations. A number of models have been proposed recently, including ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019; Dai et al., 2019), RoBERTa (Liu et al., 2019) and many others. These representations encode a given text into a vector based on left and right context. With carefully designed objectives and billions of words used for pretraining, they have achieved astonishing results in several tasks including predicting entailment relationship, semantic textual similarity, and question answering. We are particularly interested in leveraging BERT for better sentence quality and diversity estimates. This paper extends on previous work (Cho et al., 2019)"
D19-5412,P17-1099,0,0.52447,"the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35 Table 2: Results on the DUC-04 dataset evaluated by ROUGE. † indica"
D19-5412,C10-1111,0,0.0374697,"ther, and to what extent, contextualized representations can be used to improve DPP modeling. Our findings suggest that, despite the success of deep representations, it remains necessary to combine them with surface indicators for effective identification of summary sentences. 1 Introduction Determinantal point processes, shortened as DPP, is one of a number of optimization techniques that perform remarkably well in summarization competitions (Hong et al., 2014). These optimizationbased summarization methods include integer linear programming (Gillick and Favre, 2009), minimum dominating set (Shen and Li, 2010), maximizing submodular functions under a budget constraint (Lin and Bilmes, 2010; Yogatama et al., 2015), and DPP (Kulesza and Taskar, 2012). DPP is appealing to extractive summarization, since not only has it demonstrated promising performance on summarizing text/video content (Gong et al., 2014; Zhang et al., 2016; Sharghi et al., 2018), but it has the potential of being combined with deep neural networks for better representation and selection (Gartrell et al., 2018). The most distinctive characteristic of DPP is its decomposition into the quality and diversity measures (Kulesza and Taskar"
D19-5412,C18-1146,1,0.940578,"e the position of a sentence in the article. It is utilized only by BERT-imp, as position matters for sentence importance but not quite so for pairwise similarity. As shown in Table 1, positive sentences in the training data (see §3.1) tend to appear at the beginning of an article, consistently more so than negative sentences. Further, ground-truth summary sentences of the DUC and TAC datasets are likely to appear among the first five sentences of an article, indicating position embeddings are crucial for training the BERT-imp model. 2.2 System Opinosis (Ganesan et al., 2010) Extract+Rewrite (Song et al., 2018) Pointer-Gen (See et al., 2017) SumBasic (Vanderwende et al., 2007) KLSumm(Haghighi et al., 2009) LexRank (Erkan and Radev, 2004) ICSISumm (Gillick and Favre, 2009) DPP (Kulesza and Taskar, 2012)† DPP-Caps (Cho et al., 2019) DPP-Caps-Comb (Cho et al., 2019) DPP-BERT (ours) DPP-BERT-Comb 64 (ours) DPP-BERT-Comb 128 (ours) R-1 27.07 28.90 31.43 29.48 31.04 34.44 37.31 38.10 38.25 39.35 38.14 38.78 39.05 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 9.36 9.14 9.22 10.14 9.30 9.78 10.23 8.63 8.76 10.01 8.64 10.23 11.19 13.12 13.40 13.40 14.15 13.47 14.04 14.35 Table 2: Results on the DUC-04 datas"
D19-5412,N09-1041,0,\N,Missing
D19-5413,J05-3002,0,0.451038,"nces are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fu"
D19-5413,P18-1015,0,0.0197885,"kim,wachang}@adobe.com Abstract Introduction Modern abstractive summarizers excel at finding and extracting salient content (See et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018; Liu and Lapata, 2019). However, one of the key tenets of summarization is consolidation of information, and these systems can struggle to combine content from multiple source texts, yielding output summaries that contain poor grammar and even incorrect facts. Truthfulness of summaries is a vitally important feature in order for summarization to be widely accepted in real-world applications (Reiter, 2018; Cao et al., 2018b). In this work, we perform an extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@"
D19-5413,N18-1150,0,0.235026,"vily on automatic metrics. However, ROUGE (Lin, 2004) and other n-gram based metrics are limited in evaluation power and do not tell the whole story (Novikova et al., 2017). They often focus on informativeness, which misses out on important facets These authors contributed equally to this work. 104 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 104–110 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics System PG (See et al., 2017) Novel (Kryciski et al., 2018) Fast-Abs-RL (Chen and Bansal, 2018) Bottom-Up (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) Reference Summaries R-1 39.53 40.19 40.88 41.22 41.69 - ROUGE R-2 17.28 17.38 17.80 18.68 19.47 - R-L 36.38 37.52 38.54 38.34 37.92 - Compress 63.14 71.25 96.65 71.15 64.11 60.65 Created By Fuse Copy 6.44 30.24 19.77 5.39 0.83 2.21 16.35 11.76 23.96 7.07 31.93 1.36 Fail 0.18 3.59 0.31 0.74 4.86 6.06 Avg Summ Sent Len 15.7 11.8 15.6 10.7 14.5 19.3 Table 1: Comparison of state-of-the-art summarization systems. Middle column describes how summary sentences are generated. Compress: single sentence is shortened. Fuse: multiple sentences are merged. Copy: sentence is copied word-for-word. Fail: did"
D19-5413,P18-1063,0,0.369441,"valuation Setup Evaluation of summarization systems relies heavily on automatic metrics. However, ROUGE (Lin, 2004) and other n-gram based metrics are limited in evaluation power and do not tell the whole story (Novikova et al., 2017). They often focus on informativeness, which misses out on important facets These authors contributed equally to this work. 104 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 104–110 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics System PG (See et al., 2017) Novel (Kryciski et al., 2018) Fast-Abs-RL (Chen and Bansal, 2018) Bottom-Up (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) Reference Summaries R-1 39.53 40.19 40.88 41.22 41.69 - ROUGE R-2 17.28 17.38 17.80 18.68 19.47 - R-L 36.38 37.52 38.54 38.34 37.92 - Compress 63.14 71.25 96.65 71.15 64.11 60.65 Created By Fuse Copy 6.44 30.24 19.77 5.39 0.83 2.21 16.35 11.76 23.96 7.07 31.93 1.36 Fail 0.18 3.59 0.31 0.74 4.86 6.06 Avg Summ Sent Len 15.7 11.8 15.6 10.7 14.5 19.3 Table 1: Comparison of state-of-the-art summarization systems. Middle column describes how summary sentences are generated. Compress: single sentence is shortened. Fuse: multiple senten"
D19-5413,D14-1085,0,0.0388355,"Missing"
D19-5413,D18-1207,0,0.075492,"Missing"
D19-5413,W19-4828,0,0.0791459,"Missing"
D19-5413,C08-1018,0,0.0273932,"rld applications (Reiter, 2018; Cao et al., 2018b). In this work, we perform an extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a gra"
D19-5413,P19-1209,1,0.913218,"on interface. A sentence from a random summarization system is shown along with four questions. • Replacement: a pronoun or description of an entity in one sentence is replaced by a different description of that entity in the other sentence. 16, and Reference: 100. The number of sentences we evaluate for each system is proportional to the number of observed fusion cases. In order to answer the Method of Merging and Coverage questions, the annotator must be provided with which two article sentences were fused together to create the summary sentence in question. We use the heuristic proposed by Lebanoff et al. (2019) to estimate which pair of sentences should be chosen. They use averaged ROUGE-1, 2, -L scores (Lin, 2004) to represent sentence similarity. The heuristic calculates the ROUGE similarity between the summary sentence and each article sentence. The article sentence with the highest similarity is chosen as the first sentence, then overlapping words are removed from the summary sentence. It continues to find the article sentence most similar to the remaining summary sentence, which is chosen as the second sentence. Our interface automatically highlights this pair of sentences (Figure 1). The same"
D19-5413,P19-1213,0,0.242649,"Missing"
D19-5413,D13-1047,1,0.892723,"Missing"
D19-5413,D14-1076,1,0.905021,"Missing"
D19-5413,D15-1042,0,0.0230084,"nerated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summariz"
D19-5413,C18-1121,0,0.0605997,"Missing"
D19-5413,D08-1019,0,0.505392,"Missing"
D19-5413,D18-1443,0,0.0879495,"arization systems relies heavily on automatic metrics. However, ROUGE (Lin, 2004) and other n-gram based metrics are limited in evaluation power and do not tell the whole story (Novikova et al., 2017). They often focus on informativeness, which misses out on important facets These authors contributed equally to this work. 104 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 104–110 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics System PG (See et al., 2017) Novel (Kryciski et al., 2018) Fast-Abs-RL (Chen and Bansal, 2018) Bottom-Up (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) Reference Summaries R-1 39.53 40.19 40.88 41.22 41.69 - ROUGE R-2 17.28 17.38 17.80 18.68 19.47 - R-L 36.38 37.52 38.54 38.34 37.92 - Compress 63.14 71.25 96.65 71.15 64.11 60.65 Created By Fuse Copy 6.44 30.24 19.77 5.39 0.83 2.21 16.35 11.76 23.96 7.07 31.93 1.36 Fail 0.18 3.59 0.31 0.74 4.86 6.06 Avg Summ Sent Len 15.7 11.8 15.6 10.7 14.5 19.3 Table 1: Comparison of state-of-the-art summarization systems. Middle column describes how summary sentences are generated. Compress: single sentence is shortened. Fuse: multiple sentences are merged. Copy: sentence is"
D19-5413,W04-1013,0,0.191302,"un or description of an entity in one sentence is replaced by a different description of that entity in the other sentence. 16, and Reference: 100. The number of sentences we evaluate for each system is proportional to the number of observed fusion cases. In order to answer the Method of Merging and Coverage questions, the annotator must be provided with which two article sentences were fused together to create the summary sentence in question. We use the heuristic proposed by Lebanoff et al. (2019) to estimate which pair of sentences should be chosen. They use averaged ROUGE-1, 2, -L scores (Lin, 2004) to represent sentence similarity. The heuristic calculates the ROUGE similarity between the summary sentence and each article sentence. The article sentence with the highest similarity is chosen as the first sentence, then overlapping words are removed from the summary sentence. It continues to find the article sentence most similar to the remaining summary sentence, which is chosen as the second sentence. Our interface automatically highlights this pair of sentences (Figure 1). The same heuristic is also employed in deciding whether a summary sentence was generated by sentence compression or"
D19-5413,N15-1114,1,0.922296,"Missing"
D19-5413,D14-1168,0,0.0619594,"Missing"
D19-5413,P19-1500,0,0.118887,"Missing"
D19-5413,N19-1348,0,0.056554,"Missing"
D19-5413,W05-1612,0,0.738677,"Missing"
D19-5413,E06-1038,0,0.0547777,"educes the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fusion, since fusion is the task that requires the most improvement. We analyze several dimensions of the outputs, including faithfulness to the original article, grammaticality, and method of fusion. We present three main findings: Wh"
D19-5413,P13-1000,0,0.235438,"Missing"
D19-5413,N10-1044,0,0.0568235,"is work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fusion, since fusion is"
D19-5413,W13-2117,0,0.2105,"Missing"
D19-5413,C18-1102,0,0.0563215,"Missing"
D19-5413,D17-1238,0,0.0620009,"Missing"
D19-5413,J18-3002,0,0.0218832,"du {dernonco,dkim,wachang}@adobe.com Abstract Introduction Modern abstractive summarizers excel at finding and extracting salient content (See et al., 2017; Chen and Bansal, 2018; Celikyilmaz et al., 2018; Liu and Lapata, 2019). However, one of the key tenets of summarization is consolidation of information, and these systems can struggle to combine content from multiple source texts, yielding output summaries that contain poor grammar and even incorrect facts. Truthfulness of summaries is a vitally important feature in order for summarization to be widely accepted in real-world applications (Reiter, 2018; Cao et al., 2018b). In this work, we perform an extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazo"
D19-5413,P17-1099,0,0.359587,"Missing"
D19-5413,C18-1146,1,0.890007,"Missing"
D19-5413,I13-1198,0,0.47836,"irst in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We examine the outputs of five abstractive summarization systems on CNN/DailyMail (Hermann et al., 2015) using human judgments. Particularly, we focus on summary sentences that involve sentence fusion, since fusion is the task that requires the m"
D19-5413,P13-1136,0,0.0201008,"n extensive analysis of summary outputs generated by state-of-the-art systems, examining features such as truthfulness to the original document, grammaticality, and method of how sentences are merged together. This work presents the first in-depth human evaluation of multiple diverse summarization models. We differentiate between two methods of shortening text: sentence compression and sentence fusion. Sentence compression reduces the length of a single sentence by removing words or rephrasing parts of the sentence (Cohn and Lapata, 2008; ∗ Amazon Alexa AI feiliu@cs.ucf.edu seokhwk@amazon.com Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). Sentence fusion reduces two or more sentences to one by taking content from each sentence and merging them together (Barzilay and McKeown, 2005; McKeown et al., 2010; Thadani and McKeown, 2013). Compression is considered an easier task because unimportant clauses within the sentence can be removed while retaining the grammaticality and truth of the sentence (McDonald, 2006). In contrast, fusion requires selection of important content and stitching of that content in a grammatical and meaningful way. We focus on sentence fusion in this work. We"
E17-1001,W15-4640,0,0.0136794,"Missing"
E17-1001,P16-1223,0,0.0211987,"Missing"
E17-1001,D16-1147,0,0.0664511,"Missing"
E17-1001,D13-1020,0,0.124518,"Missing"
E17-1001,W14-4337,0,0.0498764,"Missing"
E17-1029,D14-1159,0,0.0175312,"Missing"
E17-1029,P13-1046,0,0.014592,"nd Ma, 2011). To cope with such limitations, discriminative methods of state tracking presented in the next part of this section aim at directly model the posterior distribution of the tracked state using a chosen parametric form. 2.3 Discriminative Dialog State Tracking Figure 1: T-SNE transformation of the final state of DSTC-2 train set. The discriminative approach of dialog state tracking computes the belief over a state via a parametric model that directly represents the belief b(st+1 ) = p(ss+1 |st , zt ). For example, Maximum Entropy has been widely used in the discriminative approach (Metallinou et al., 2013). It formulates the belief as follows: b(s) = P(s|x) = T η.ew φ (x,s) , where η is the normalizing constant, x = (d1u , d1m , s1 , . . . , dtu , dtm , st ) is the history of user dialog acts, diu , i ∈ {1, . . . ,t}, the system dialog acts, dim , i ∈ {1, . . . ,t}, and the sequence of states leading to the current dialog turn at time t. Then, φ (.) is a vector of feature functions on x and s. Finally, w is the set of model parameters to be learned from annotated dialog data. Finally, deep neural models, performing on a sliding window of features extracted from previous user turns, have also be"
E17-1029,P17-1163,0,0.0749729,"Missing"
E17-1029,W13-4065,0,0.13811,"Missing"
E17-1029,D13-1020,0,0.0123381,"ed. In this last part, several reasoning enhancements are suggested to the DSTC-2 dataset. solve it using a memory-enhanced neural architecture of inference. 3.1 Machine Reading The task of textual understanding has recently been formulated as a supervised learning problem (Kumar et al., 2015; Hermann et al., 2015). This task consists in estimating the conditional probability p(a|d, q) of an answer a to a question q where d is a document. Such an approach requires a large training corpus of {Document - Query Answer} triples and until now such corpora have been limited to hundreds of examples (Richardson et al., 2013). In the context of dialog state tracking, it can be understood as the capability of inferring a set of latent values l associated with a set of variables v related to a given dyadic or multi-party conversation d, from direct correlation and/or reasoning, using the course of exchanges of utterances, p(l|d, v). State updates at an utterance-level are rarely provided off-the-shelf from a production environment. In these environments, annotation is often performed afterhand for the purpose of logging, monitoring or quality assessment. In the limit cases, as in human-to-human dialog systems, dialo"
E17-1029,W14-4340,0,0.579534,"the belief as follows: b(s) = P(s|x) = T η.ew φ (x,s) , where η is the normalizing constant, x = (d1u , d1m , s1 , . . . , dtu , dtm , st ) is the history of user dialog acts, diu , i ∈ {1, . . . ,t}, the system dialog acts, dim , i ∈ {1, . . . ,t}, and the sequence of states leading to the current dialog turn at time t. Then, φ (.) is a vector of feature functions on x and s. Finally, w is the set of model parameters to be learned from annotated dialog data. Finally, deep neural models, performing on a sliding window of features extracted from previous user turns, have also been proposed in (Henderson et al., 2014c; Mrksic et al., 2016). Of the current literature, this family of approaches have proven to be the most efficient for publicly available state tracking datasets. Recently, deep learning based models implementing this strategy (Mrksic et al., 2016; Henderson et al., 2014a; Williams et al., 2016) have shown state of the art results. This approaches tends to leverage unsupervised training word representation (Mikolov et al., 2013). 2.4 inter-slot correlation, Figure 1 depicted the t-SNE (van der Maaten and Hinton, 2008) projected final state of the dialog of the DSTC-2 training set. On the other"
E17-1029,2005.sigdial-1.4,0,0.162843,"rt dialog state tracking dataset, DSTC-2, to several simple reasoning capabilities. Section 4 illustrates the approach with experimental results obtained using a state of the art benchmark for dialog state tracking. 2 2.1 Dialog state tracking Main Definitions A dialog state tracking task is formalized as follows: at each turn of a dyadic dialog, the dialog agent chooses a dialog act d to express and the user answers with an utterance u. In the simplest case, the dialog state at each turn is defined as a distribution over a set of predefined variables, which define the structure of the state (Williams et al., 2005). This classic state structure is commonly called slot filling or semantic frame. In this context, the state tracking task consists of estimating the value of a set of predefined variables in order to perform a procedure or transaction which is the purpose of the dialog. Typically, a natural language understanding module processes the user utterance and generates an Nbest list o = {(d1 , f1 ), . . . , (dn , fn )}, where di is the hypothesized user dialog act and fi is its confidence score. Various approaches have been proposed to define dialog state trackers. The traditional methods used in mo"
E17-1029,W14-4339,0,0.104418,"ural language understanding module processes the user utterance and generates an Nbest list o = {(d1 , f1 ), . . . , (dn , fn )}, where di is the hypothesized user dialog act and fi is its confidence score. Various approaches have been proposed to define dialog state trackers. The traditional methods used in most commercial implementations use hand-crafted rules that typically rely on the most likely result from an NLU module (Yeh et al., 2014) and hardly models uncertainty. However, these rule-based systems are prone to frequent errors as the most likely result is not always the correct one (Williams, 2014). More recent methods employ statistical approaches to estimate the posterior distribution over the dialog states allowing them to leverage the uncertainty of the results of the NLU module. In the simplest case where no ASR and NLU modules are employed, as in a text based dialog system (Henderson et al., 2013), the utterance is taken as the observation using a so-called bag of words representation. If an NLU module is available, stan2.2 Generative Dialog State Tracking A generative approach to dialog state tracking computes the belief over the state using Bayes’ rule, using the belief from the"
E17-1071,P11-1038,0,0.107648,"ers, pages 754–764, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics In order to address these challenges we propose a novel feature-engineering-free, deep-learningbased approach to the problem of personality trait recognition, enabling the model to work in various languages without the need to create languagespecific linguistic features. We frame the problem as a supervised sequence regression task, taking only the joint atomic representation of the text: hierarchically on the character and word level. In this work, we focus on short texts. As pointed out by Han and Baldwin (2011), classification of such texts can often be challenging for even state-ofthe-art BoW based approaches, which is, in part, caused by the noisy nature of such data. In this work, we address this by proposing a novel hierarchical neural network architecture, free of feature engineering and, once trained, capable of inferring personality across five traits and three languages. The paper is structured as follows: we consider previous approaches to computational personality recognition, including those few which have a deep-learning component, and subsequently describe our model. We report two sets"
E17-1071,P14-1062,0,0.0648476,"of tweets, this method shows stateof-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. 1 Introduction Deep-learning methods are becoming increasingly applied to problems in the area of Natural Language Processing (NLP) (Manning, 2016). Such techniques can be applied to tasks such as partof-speech-tagging (Ling et al., 2015; Huang et al., 2015) and sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014). At their core, these tasks can be seen as learning representations of language at different levels. Our work reported here is no different, though we choose a less commonplace level of representation – rather than the text itself, we focus on the author behind the text. Automatically modelling individuals from their language use is a task founded on the long-standing understanding that language use is influenced by sociodemographic characteristics ∗ Work carried out at Xerox Research Centre Europe 754 Proceedings of the 15th Conference of the European Chapter of the Association f"
E17-1071,D14-1181,0,0.0121588,"ws stateof-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. 1 Introduction Deep-learning methods are becoming increasingly applied to problems in the area of Natural Language Processing (NLP) (Manning, 2016). Such techniques can be applied to tasks such as partof-speech-tagging (Ling et al., 2015; Huang et al., 2015) and sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014). At their core, these tasks can be seen as learning representations of language at different levels. Our work reported here is no different, though we choose a less commonplace level of representation – rather than the text itself, we focus on the author behind the text. Automatically modelling individuals from their language use is a task founded on the long-standing understanding that language use is influenced by sociodemographic characteristics ∗ Work carried out at Xerox Research Centre Europe 754 Proceedings of the 15th Conference of the European Chapter of the Association for Computati"
E17-1071,N13-1039,0,0.0682181,"Missing"
E17-1071,J81-4005,0,0.730814,"Missing"
E17-1071,D15-1176,0,0.573794,"orial word and sentence representations for the task of trait inference. On a corpus of tweets, this method shows stateof-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. 1 Introduction Deep-learning methods are becoming increasingly applied to problems in the area of Natural Language Processing (NLP) (Manning, 2016). Such techniques can be applied to tasks such as partof-speech-tagging (Ling et al., 2015; Huang et al., 2015) and sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014). At their core, these tasks can be seen as learning representations of language at different levels. Our work reported here is no different, though we choose a less commonplace level of representation – rather than the text itself, we focus on the author behind the text. Automatically modelling individuals from their language use is a task founded on the long-standing understanding that language use is influenced by sociodemographic characteristics ∗ Work carried out at Xerox Research Centr"
E17-1071,D14-1162,0,0.112958,"niques. In the PAN task alone1 , there were features, in the form of surface forms of text, present on multiple levels of In Section 3.2, we propose a model inspired by the work of Ling et al. (2015) where representations are hierarchically constructed from characters to words. This is based on the assumption that character sequences are syntactically and semantically informative of the words they compose. Their model – a widely used RNN vari1 Due to space consideration we are unable to cite the individual works. 755 ion, e.g. word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b) and GloVe (Pennington et al., 2014), in order to obtain a sensible set of embeddings. While this approach has demonstrated its strong capabilities of capturing syntactic and semantic information and been successfully applied to a number of NLP tasks (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014), as identified by Ling et al. (2015), there are two practical problems with it. First, given that language is flexible, previously unseen words are bound to occur regardless of the size of the unsupervised training corpus. This problem is even more pronounced when dealing with user-generated text, such as from social media"
E17-1071,D15-1130,1,0.661045,"utilised in this work, each user is assigned a single score for a particular personality trait and every tweet collected from the same account inherits the same five personality trait assignments as its author. The predicted user level trait score is calculated: yˆuseri = 1 PTi ˆsj where Ti is the total number of tweets j=1 y Ti of useri . In Section 4.3 and 4.4, the results, measured with RM SEuser and RM SEtweet , in the two settings, i.e. against models with and without feature-engineering, are presented respectively. Consistent with prior work in author profiling (Sulea and Dichiu, 2015; Mirkin et al., 2015; Nowson et al., 2015), we employ exactly the same evaluation metric on the same dataset to enable direct comparison. 4.3 C2W2S4PT outperforms the current state of the art in EN and ES. In the 5-fold crossvalidation group, C2W2S4PT demonstrates its advantages, attaining superior performance to the baselines except for CON in ES. In terms of 10fold cross validation, the superiority of our model is even more evident, supported by the dominating performance over the two selected baselines across all personality traits and two languages. In both groups, 5 or 10-fold cross validation, not only does"
E17-1071,D13-1170,0,0.0124379,"nference. On a corpus of tweets, this method shows stateof-the-art performance across five traits and three languages (English, Spanish and Italian) compared with prior work in author profiling. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. 1 Introduction Deep-learning methods are becoming increasingly applied to problems in the area of Natural Language Processing (NLP) (Manning, 2016). Such techniques can be applied to tasks such as partof-speech-tagging (Ling et al., 2015; Huang et al., 2015) and sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014). At their core, these tasks can be seen as learning representations of language at different levels. Our work reported here is no different, though we choose a less commonplace level of representation – rather than the text itself, we focus on the author behind the text. Automatically modelling individuals from their language use is a task founded on the long-standing understanding that language use is influenced by sociodemographic characteristics ∗ Work carried out at Xerox Research Centre Europe 754 Proceedings of the 15th Conference of the European C"
E17-1071,L16-1258,0,0.0344596,"Missing"
E17-1071,N16-1174,0,0.00785297,"ters. When applied to the tasks of language modelling and part-of-speech tagging, the model successfully improves the accuracy upon traditional baselines, performing particularly well in morphologically rich languages. Not only does the model achieve better performance on both tasks, it also has significantly fewer parameters to learn compared to a word look-up table based approach as the number of different characters is much fewer than the number of different words in a vocabulary. Moreover, the model is able to generate a sensible representation for previously unseen words. Following this, Yang et al. (2016) took it further to the document level, introducing Hierarchical Attention Networks where two bi-directional Gated Recurrent Units (GRUs) are used to process the sequence of words and then sentences respectively with the sentence-level GRU taking as input the output of the word-level GRU and returning the representation of the document. While Yang et al. (2016) describe a means to hierarchically build representations of documents from words to sentences and eventually to documents (Word to Sentence to Document, W2S2D), the work of (Ling et al., 2015) is positioned at a more fine-grained level,"
I11-1125,C04-1053,0,0.0442906,"Missing"
I11-1125,P11-1135,0,0.0371814,"Missing"
I11-1125,P04-1074,0,0.0266503,"ed lexically instead of morphologically. There are useful contextual cues that can help determine the tense for the whole Chinese sentence or individual verbs, such as temporal adverbs or phrases, aspect auxiliary words and prepositions. For instance, in example (1a), the aspect particle “ (a particle word in Chinese, there is no literal translation to English)” and temporal phrase “AUc(several days ago)” together indicate the past tense of the sentence, and thus the correct translation of the sentence is “I arrived at Shanghai several days ago”. Most of the previous work on tense prediction (Li et al., 2004; Cao et al., 2004; Ye and Zhang, 2005; Lin, 2006) has been conducted using relatively small data sets (e.g., hundreds of 1116 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1116–1124, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Chinese sentences) and typically news article domain. They often require hand crafted rules in the systems. In this paper, we adopt a statistical classification framework for Chinese tense prediction. This study is different from previous work in that (a) we propose to utilize the parallel ChineseEnglish corpor"
I11-1125,E91-1047,0,0.0805091,"Missing"
I11-1125,C04-1101,0,0.031452,"ead of morphologically. There are useful contextual cues that can help determine the tense for the whole Chinese sentence or individual verbs, such as temporal adverbs or phrases, aspect auxiliary words and prepositions. For instance, in example (1a), the aspect particle “ (a particle word in Chinese, there is no literal translation to English)” and temporal phrase “AUc(several days ago)” together indicate the past tense of the sentence, and thus the correct translation of the sentence is “I arrived at Shanghai several days ago”. Most of the previous work on tense prediction (Li et al., 2004; Cao et al., 2004; Ye and Zhang, 2005; Lin, 2006) has been conducted using relatively small data sets (e.g., hundreds of 1116 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1116–1124, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Chinese sentences) and typically news article domain. They often require hand crafted rules in the systems. In this paper, we adopt a statistical classification framework for Chinese tense prediction. This study is different from previous work in that (a) we propose to utilize the parallel ChineseEnglish corpora to automatically"
I11-1125,N06-1020,0,0.029863,"Missing"
I11-1125,W09-2201,0,0.0253171,"Missing"
I11-1125,J03-1002,0,0.00514727,", for each instance we compare the labels generated based on three sources: original automatically derived label, prediction from the current classifier, and prediction from the initial self-trained classifier. We filter out cases using two different constraints: (a) Constraint I: neither the prediction from the current classifier nor the self-trained one is the same as the automatically derived label. (b) Constraint II: none of those three labels agree with others, that is, they are all different. 4 4.1 data (mainly word segmentation for Chinese and tokenization for English), we used GIZA++ (Och and Ney, 2003) to obtain a word-level alignment. After applying heuristic rules to eliminate verbs that have no aligned English verbs or are aligned to multiple verbs with conflicting tenses (see Section 3.3 for reference tense creation), we finally created a training set consisting of 279,379 verb instances and 38,087 sentences. We chose the maximum entropy (ME) model as the classifier for tense prediction, because ME can effectively utilize many features and performs competitively with other approaches in many classification tasks. We used the ME implementation from (Zhang, 2006) with a Gaussian prior of"
I11-1125,W09-2307,0,0.02469,"Missing"
I11-1125,P06-1102,0,0.0199689,"Missing"
I11-1125,P05-1022,0,0.0139753,"Missing"
I11-1125,P02-1006,0,0.0141151,"Missing"
I11-1125,W09-2209,0,0.0538955,"Missing"
I11-1125,N10-1113,0,0.225704,"Missing"
I11-1125,W99-0613,0,0.137963,"Missing"
I11-1125,feldman-etal-2006-cross,0,0.0520664,"Missing"
I11-1125,D07-1117,0,0.0416627,"Missing"
I11-1125,xue-etal-2008-annotating,0,0.233337,"Missing"
I11-1125,I05-1077,0,0.155041,"ally. There are useful contextual cues that can help determine the tense for the whole Chinese sentence or individual verbs, such as temporal adverbs or phrases, aspect auxiliary words and prepositions. For instance, in example (1a), the aspect particle “ (a particle word in Chinese, there is no literal translation to English)” and temporal phrase “AUc(several days ago)” together indicate the past tense of the sentence, and thus the correct translation of the sentence is “I arrived at Shanghai several days ago”. Most of the previous work on tense prediction (Li et al., 2004; Cao et al., 2004; Ye and Zhang, 2005; Lin, 2006) has been conducted using relatively small data sets (e.g., hundreds of 1116 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1116–1124, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Chinese sentences) and typically news article domain. They often require hand crafted rules in the systems. In this paper, we adopt a statistical classification framework for Chinese tense prediction. This study is different from previous work in that (a) we propose to utilize the parallel ChineseEnglish corpora to automatically generate reference"
I11-1125,W01-1305,0,0.0887181,"Missing"
I17-1056,N16-1030,0,0.47323,"shared task dataset consists of 14, 041/3, 250/3, 453 sentences in the training/development/test set, resp., extracted from 946/216/231 Reuters news articles from the period 1996–97. The goal is to identify individual token occurrences of NEs, and tag each with its class (e.g. LOCATION or ORGANISATION). Here, we use the IOB tagging scheme. In terms of tagging schemes, while some have shown improvements with a more expressive IOBES marginally (Ratinov and Roth, 2009; Dai et al., 2015), we stick to the BIO scheme for simplicity and the observation of little improvement between these schemes by Lample et al. (2016). 5.2 Evaluation Evaluation is based on span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of either a simple LSTM or bidirectional LSTM (“LSTMCRF” and “B I -LSTM-CRF”, resp.: Huang et al. (2015)) Note that for our word embeddings, while we ob"
I17-1056,P05-1045,0,0.658022,"t.cohn@unimelb.edu.au Abstract linear-chain Conditional Random Fields (CRFs) (Wang et al., 2011; Zhang et al., 2017), framing the task as one of sequence tagging. Although CRFs are adept at capturing local structure, the problem does not naturally suit a linear sequential structure, i.e. , a post may be a reply to either a neighbouring post or one posted far earlier within the same thread. In both cases, contextual dependencies can be long-range, necessitating the ability to capture dependencies between arbitrarily distant items. Identifying this key limitation, Sutton and McCallum (2004) and Finkel et al. (2005) proposed the use of CRFs with skip connections to incorporate long-range dependencies. In both cases the graph structure must be supplied a priori, rather than learned, and both techniques incur the need for costly approximate inference. Recurrent neural networks (RNNs) have been proposed as an alternative technique for encoding sequential inputs, however plain RNNs are unable to capture long-range dependencies (Bengio et al., 1994; Hochreiter et al., 2001) and variants such as LSTMs (Hochreiter and Schmidhuber, 1997), although more capabable of capturing non-local patterns, still exhibit a s"
I17-1056,P10-1081,0,0.0317937,"y said ··· B-ORG O B-MISC O Interfax quoted Russian military ··· Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a sentence boundary. incompatibility with exact inference. Similar ideas have also been explored by Krishnan and Manning (2006) for NER, where they apply two CRFs, the first of which makes predictions based on local information, and the second combines named entities identified by the first CRF in a single cluster, thereby enforcing label consistency and enabling the use of a richer set of features to capture non-local dependencies. Liao and Grishman (2010) make a strong case for going beyond sentence boundaries and leveraging document-level information for event extraction. While we take inspiration from these earlier studies, we do not enforce label consistency as a hard constraint, and additionally do not sacrifice inference tractability: our model is capable of incorporating non-local features, and is compatible with exact inference methods. mentation of the model is available at: https: //github.com/liufly/mecrf. The paper is organised as follows: after reviewing previous studies on capturing long range contextual dependencies and related m"
I17-1056,D15-1161,0,0.0641599,"Missing"
I17-1056,Q16-1037,0,0.435365,"porate long-range dependencies. In both cases the graph structure must be supplied a priori, rather than learned, and both techniques incur the need for costly approximate inference. Recurrent neural networks (RNNs) have been proposed as an alternative technique for encoding sequential inputs, however plain RNNs are unable to capture long-range dependencies (Bengio et al., 1994; Hochreiter et al., 2001) and variants such as LSTMs (Hochreiter and Schmidhuber, 1997), although more capabable of capturing non-local patterns, still exhibit a significant locality bias in practice (Lai et al., 2015; Linzen et al., 2016). In this paper, taking inspiration from the work of Weston et al. (2015) on memory networks (M EM N ETs), we propose to extend CRFs by integrating external memory mechanisms, thereby enabling the model to look beyond localised features and have access to the entire sequence. This is achieved with attention over every entry in the memory. Experiments on named entity recognition and forum thread parsing, both of which involve long-range contextual dependencies, demonstrate the effectiveness of the proposed model, achieving state-of-the-art performance on the former, and outperforming a number o"
I17-1056,D14-1162,0,0.0826714,"span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of either a simple LSTM or bidirectional LSTM (“LSTMCRF” and “B I -LSTM-CRF”, resp.: Huang et al. (2015)) Note that for our word embeddings, while we observe better performance with G LOV E (Pennington et al., 2014), for fair comparison purposes, we adopt the same S ENNA embeddings (Collobert et al., 2011) as are used in the baseline methods.5 Experimental Setup We choose Φ(xt ) to be a lookup function, returning the corresponding embedding xt of the word xt . In addition to the word features, we employ a subset of the lexical features described in Huang et al. (2015), based on whether the word: • starts with a capital letter; • is composed of all capital letters; • is composed of all lower case letters; • contains non initial capital letters; • contains both letters and digits; • contains punctuation. T"
I17-1056,W10-2923,1,0.914292,"Missing"
I17-1056,W09-1119,0,0.0736863,"g the ability of ME-CRF to capture document context, to aid in the identification and disambiguation of NEs. 5.1 Dataset and Task 5.3 The CoNLL 2003 NER shared task dataset consists of 14, 041/3, 250/3, 453 sentences in the training/development/test set, resp., extracted from 946/216/231 Reuters news articles from the period 1996–97. The goal is to identify individual token occurrences of NEs, and tag each with its class (e.g. LOCATION or ORGANISATION). Here, we use the IOB tagging scheme. In terms of tagging schemes, while some have shown improvements with a more expressive IOBES marginally (Ratinov and Roth, 2009; Dai et al., 2015), we stick to the BIO scheme for simplicity and the observation of little improvement between these schemes by Lample et al. (2016). 5.2 Evaluation Evaluation is based on span-level NE F-score, based on the official CoNLL evaluation script.4 We compare against the following baselines: 1. a CRF over hand-tuned lexical features (“CRF”: Huang et al. (2015)) 2. an LSTM and bi-directional LSTM (“LSTM” and “B I -LSTM”, resp.: Huang et al. (2015)) 3. a CRF taking features from a convolutional neural network as input (“C ONV-CRF”: Collobert et al. (2011)) 4. a CRF over the output of"
I17-1056,P06-1141,0,0.0305568,"istent identification and disambiguation. A related example is forum thread discourse analysis. Previous work has largely focused on 555 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 555–565, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP ··· B-ORG O O O Interfax news agency said ··· B-ORG O B-MISC O Interfax quoted Russian military ··· Figure 1: A NER example with long-range contextual dependencies. The vertical dash line indicates a sentence boundary. incompatibility with exact inference. Similar ideas have also been explored by Krishnan and Manning (2006) for NER, where they apply two CRFs, the first of which makes predictions based on local information, and the second combines named entities identified by the first CRF in a single cluster, thereby enforcing label consistency and enabling the use of a richer set of features to capture non-local dependencies. Liao and Grishman (2010) make a strong case for going beyond sentence boundaries and leveraging document-level information for event extraction. While we take inspiration from these earlier studies, we do not enforce label consistency as a hard constraint, and additionally do not sacrifice"
I17-1056,D16-1021,0,0.0221529,"so associated with a title. We therefore use two encoders, Φtitle (·) and Φtext (·), to process them separately and then concatenate xt = [Φtitle (xt ); Φtext (xt )]&gt; . Here, Φtitle (·) and Φtext (·) take word embeddings as input, processing each post at the word level, as opposed to the post-level bi-directional GRU in Equations (1) and (2), and the representation of a post xt (either title or text) is obtained by transforming the last and first hidden states of the forward and backward word-level GRU, similar to Equation (3). Note that Φtitle (·) and Φtext (·) do not share parameters. As in Tang et al. (2016), we further restrict mki = cki to curb overfitting. In keeping with Wang (2014), we complement the textual representations with hand-crafted structural features Φs (xt ) ∈ R2 : • initiator: a binary feature indicating whether the author of the current post is the same as the initiator of the thread, • position: the relative position of the current post, as a ratio over the total number of posts in the thread; Also drawing on Wang (2014), we incorporate punctuation-based features Φp (xt ) ∈ R3 : the number of question marks, exclamation marks and URLs in the current post. The resultant feature"
I17-1056,W03-0419,0,0.193167,"Missing"
I17-1056,D11-1002,1,0.920639,"Missing"
I17-1056,N16-1174,0,0.0839786,"rk In this section, we review the different families of models that are relevant to this work, in capturing long-range contextual dependencies in different ways. Recurrent Neural Networks (RNNs). Recently, the broad adoption of deep learning methods in NLP has given rise to the prevalent use of RNNs. Long short-term memories (“LSTMs”: Hochreiter and Schmidhuber (1997)), a particular variant of RNN, have become particularly popular, and been successfully applied to a large number of tasks: speech recognition (Graves et al., 2013), sequence tagging (Huang et al., 2015), document categorisation (Yang et al., 2016), and machine translation (Cho et al., 2014; Bahdanau et al., 2014). However, as pointed out by Lai et al. (2015) and Linzen et al. (2016), RNNs — including LSTMs — are biased towards immediately preceding (or neighbouring, in the case of bidirectional RNNs) items, and perform poorly in contexts which involve long-range contextual dependencies, despite the inclusion of memory cells. This is further evidenced by the work of Cho et al. (2014), who show that the performance of a basic encoder–decoder deteriorates as the length of the input sentence increases. Conditional Random Fields (CRFs). CRF"
I17-1056,W14-4012,0,\N,Missing
N09-1070,A00-1031,0,0.0167113,"o key phrases are generated. 2 Note that by unsupervised methods, we mean that no data annotated with keywords is needed. These methods do require the use of some data to generate information such as IDF, or possibly a development set to optimize some parameters or heuristic rules. 623 In addition to using a stopword list to remove words from consideration, we also leverage POS information to filter unlikely keywords. Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only. We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts. (C) Integrating word clustering One weakness of the baseline TFIDF is that it counts the frequency for a particular word, without considering any words that are similar to it in terms of semantic meaning. In addition, when the document is short, the TF may not be a reliable indicator of the importance of the word. Our idea is therefore to account for the frequency of other similar words when calculating the TF of a word in the document. For this, we group all the words into clusters in an unsupervised fashion. If the total term"
N09-1070,P03-1071,0,0.0084388,", 2005)) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5. One example of the annotated keywords for a topic segment is: • Annotator I: analysis, constraints, template matcher; • Annotator II: syntactic analysis, parser, pattern matcher, finite-state transducers; • Annotator III: lexicon, set processing, chunk parser. Note that these meetings are research discussions, and that the annotators may not be very familiar with 1 We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005). 622 the topics discussed and often had trouble deciding the important sentences or keywords. In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. Sometimes there are more possible keywords and the annotators felt it is hard to decide which five are the most topic indicative. Among the three annotators, we notice that in general the quality of annotator I is the poorest. This is based on the authors’ judgment, and is also confirmed later by an independent human evaluation (in Section 6). For a better understa"
N09-1070,W03-1028,0,0.88654,"Missing"
N09-1070,W05-0905,0,0.0131623,"Missing"
N09-1070,N04-1019,0,0.0805389,"Missing"
N09-1070,van-der-plas-etal-2004-automatic,0,0.0142481,"nforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004), where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002). (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina,"
N09-1070,W04-2319,0,0.0181216,"Missing"
N09-1070,P07-1070,0,0.636413,", since keywords often appear early in the document (e.g., in the first paragraph). However, for the less well structured meeting domain (lack of title and paragraph), these kinds of features may not be indicative. A supervised approach to keyword extraction was used in (Liu et al., 2008). Even though the data set in that study is not very big, it seems that a supervised learning approach can achieve reasonable performance for this task. Another line of research for keyword extraction has adopted graph-based methods similar to Google’s PageRank algorithm (Brin and Page, 1998). In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant"
N13-1135,P12-1056,0,0.0515025,"information. The identified events are often represented using a set of keywords. (Petrovic et al., 2010) proposed an algorithm based on locality-sensitive hashing for detecting new events from a stream of Twitter posts. (O’Connor et al., 2010; Becker et al., 2011b; Becker et al., 2011a; Weng et al., 2011) proposed demo systems to display the event-related themes and popular tweets, allowing the users to navigate through their topic of interest. (Zhao et al., 2011) described an effort to perform data collection and event recognition despite various limits to the free access of Twitter data. (Diao et al., 2012) integrated both temporal information and users’ personal interests for bursty topic detection from the microblogs. (Ritter et al., 2012) described an open-domain event-extraction and categorization system, which extracts an open-domain calendar of significant events from Twitter. With the identified events of interest, there is an ever-increasing demand for event summarization, which distills the huge volume of Twitter discussions into a concise and representative textual description of the events. Many studies start with the text summarization approaches that have been shown to perform well"
N13-1135,N13-1037,0,0.00629299,"nt, namely the Apple CEO’s keynote speech in the Apple Worldwide Developers Conference (WWDC 2012)1 . All these events have excited great discussion among the Twitter community. Summarizing the Twitter event is a challenging task that has yet been fully explored in the past. Most previous summarization studies focus on the well-formatted news documents, as driven by the annual DUC2 and TAC3 evaluations. In contrast, the Twitter messages (a.k.a., tweets) are very short and noisy, containing nonstandard terms such as abbreviations, acronyms, emoticons, etc. (Liu et al., 2011b; Liu et al., 2012; Eisenstein, 2013). The noisy contents also cause great difficulties to the traditional NLP tools such as NER and dependency parser (Ritter et al., 2011; Foster et al., 2011), limiting the possibility of applying finer-grained event analysis tools. In nature, the event tweets are closely associated with the timeline and are drastically different from a static collection of news documents. The tweets converge into text streams that pulse along the timeline and cluster around the important moments or sub-events. These “sub-events” are of crucial importance since they represent a surge of interest from the Twitter"
N13-1135,P11-2008,0,0.1,"Missing"
N13-1135,N09-1041,0,0.0516729,"Figure 2 shows the plate notation. In the proposed model, each tweet d in the data stream D is generated from a topic z, weighted by πz . Each topic is characterized by both its content and time aspects. The content aspect is captured by a multinomial distribution over the words, parameterized by θ; while the time aspect is characterized by a Gaussian distribution, parameterized by µ and σ, with µ represents the average time point that the sub-event emerges and σ determines the duration of the sub-event. These distributions bear similarities with the previous work (Hofmann, 1999; Allan, 2002; Haghighi and Vanderwende, 2009). In addition, there are often background or “noise” topics that are being constantly discussed over the entire event evolvement process and do not present the desired “burstiness” property. We use a uniform distribution U (tb , te ) to model the time aspect of these “background” topics, with tb and te being the event beginning and end time points. The content aspect of a background topic is modeled by similar multinomial distribution, parameterized by θ0 . We use the maximum likelihood parameter estimation. The data likelihood can be represented as: Y YX pz (w)} {πz pz (td ) L(D) = d∈D z w∈d"
N13-1135,P99-1004,0,0.0204374,"ackground topics if their σ values are greater than a threshold β 7 . We then re-run the EM to obtain the updated parameters. The topic re-adjustment process continues until the number of sub-events and background topics do not change further. We obtain the “participant sub-events” by applying this sub-event detection approach to each of the participant streams. The “global sub-events” are obtained by merging the participant sub-events along the timeline. We merge two participant subevents into a global sub-event if (1) their peaks are within a 2-minute window, and (2) the Jaccard similarity (Lee, 1999) between their associated tweets is greater than a threshold (set to 0.1 empirically). The tweets associated with each global sub-event are the ones with p(z|d) greater than a threshold γ, where z is one of the participant sub-events and γ was set to 0.7 empirically. After the sub-event detection process, we obtain a set of global sub-events and their associated event tweets.8 3.3 Summary Tweet Extraction d p(w; θj0 ) ∝ X We extract a representative tweet from each of the global sub-events and concatenate them to form an informative event summary. Note that our goal in this work is to identify"
N13-1135,W04-1013,0,0.0398898,"6 39.62 25.16 31.73 16.87 22.40 31.53 31.82 33.33 36.07 Table 5: ROUGE-1 scores of summarization Summarization evaluation has been a longstanding issue in the literature (Nenkova and Mckeown, 2011; Liu and Liu, 2010). There are even less studies focusing on evaluating the event summaries generated from data streams. Since the summary annotation takes quite some effort, we sample a 10-minute segment from each of the seven event streams and ask a human annotator to select representative tweets for each segment. We then compare the system summaries against the manual summaries using the ROUGE-1 (Lin, 2004) metric. The quantitative results and qualitative analysis are presented in Table 5 and Table 6 respectively. Note that the ROUGE scores are based solely on the n-gram overlap between the system and reference summaries, which may not be the most appropriate measure for evaluating the Twitter event summaries. However, we do notice that the accurate sub-event detection performance can successfully translate into a gain of the ROUGE scores. Qualitatively, the participantbased event summarization approach focus more on extracting tweets associated with the targeted participants, which could lead t"
N13-1135,W11-0709,1,0.921251,"s and a representative conference event, namely the Apple CEO’s keynote speech in the Apple Worldwide Developers Conference (WWDC 2012)1 . All these events have excited great discussion among the Twitter community. Summarizing the Twitter event is a challenging task that has yet been fully explored in the past. Most previous summarization studies focus on the well-formatted news documents, as driven by the annual DUC2 and TAC3 evaluations. In contrast, the Twitter messages (a.k.a., tweets) are very short and noisy, containing nonstandard terms such as abbreviations, acronyms, emoticons, etc. (Liu et al., 2011b; Liu et al., 2012; Eisenstein, 2013). The noisy contents also cause great difficulties to the traditional NLP tools such as NER and dependency parser (Ritter et al., 2011; Foster et al., 2011), limiting the possibility of applying finer-grained event analysis tools. In nature, the event tweets are closely associated with the timeline and are drastically different from a static collection of news documents. The tweets converge into text streams that pulse along the timeline and cluster around the important moments or sub-events. These “sub-events” are of crucial importance since they represen"
N13-1135,P11-2013,1,0.847005,"s and a representative conference event, namely the Apple CEO’s keynote speech in the Apple Worldwide Developers Conference (WWDC 2012)1 . All these events have excited great discussion among the Twitter community. Summarizing the Twitter event is a challenging task that has yet been fully explored in the past. Most previous summarization studies focus on the well-formatted news documents, as driven by the annual DUC2 and TAC3 evaluations. In contrast, the Twitter messages (a.k.a., tweets) are very short and noisy, containing nonstandard terms such as abbreviations, acronyms, emoticons, etc. (Liu et al., 2011b; Liu et al., 2012; Eisenstein, 2013). The noisy contents also cause great difficulties to the traditional NLP tools such as NER and dependency parser (Ritter et al., 2011; Foster et al., 2011), limiting the possibility of applying finer-grained event analysis tools. In nature, the event tweets are closely associated with the timeline and are drastically different from a static collection of news documents. The tweets converge into text streams that pulse along the timeline and cluster around the important moments or sub-events. These “sub-events” are of crucial importance since they represen"
N13-1135,P12-1109,1,0.780962,"ive conference event, namely the Apple CEO’s keynote speech in the Apple Worldwide Developers Conference (WWDC 2012)1 . All these events have excited great discussion among the Twitter community. Summarizing the Twitter event is a challenging task that has yet been fully explored in the past. Most previous summarization studies focus on the well-formatted news documents, as driven by the annual DUC2 and TAC3 evaluations. In contrast, the Twitter messages (a.k.a., tweets) are very short and noisy, containing nonstandard terms such as abbreviations, acronyms, emoticons, etc. (Liu et al., 2011b; Liu et al., 2012; Eisenstein, 2013). The noisy contents also cause great difficulties to the traditional NLP tools such as NER and dependency parser (Ritter et al., 2011; Foster et al., 2011), limiting the possibility of applying finer-grained event analysis tools. In nature, the event tweets are closely associated with the timeline and are drastically different from a static collection of news documents. The tweets converge into text streams that pulse along the timeline and cluster around the important moments or sub-events. These “sub-events” are of crucial importance since they represent a surge of intere"
N13-1135,C12-2075,0,0.106551,"to the same topic. Regarding summarizing the data streams, (Marcus et al., 2011) introduced a “TwitInfo” system to visually summarize and track the events on Twitter. They proposed an automatic peak detection and labeling algorithm for the social streams. (Takamura et al., 2011) proposed a summarization model based on the facility location problem, which generates summary for a stream of short documents along the timeline. (Chakrabarti and Punera, 2011) proposed an event summarization algorithm based on learning an underlying hidden state representation of the event via hidden Markov models. (Louis and Newman, 2012) presented a method for summarizing a collection of tweets related to a business. The proposed procedure aggregates tweets into subtopic clusters which are then ranked and summarized by a few representative tweets from each cluster. (Nichols et al., 2012; Zubiaga et al., 2012) focused on real-time event summarization, which detects the sub-events by identifying those moments where the tweet volume has increases sharply, then uses various weighting schemes to perform tweet selection and finally generates the event summary. Our work is different from the above research studies in three folds: fi"
N13-1135,N10-1021,0,0.0340385,"f the participant streams, and the participantbased event summarization can effectively capture the sub-events that have otherwise been shadowed by the long-tail of other dominant sub-events, yielding summaries with considerably better coverage than the state-of-the-art approach. 2 Related Work Mining Twitter for event information has received increasing attention in recent years. Many research studies focus on identifying the trending events from Twitter and providing a concise and dynamic visualization of the information. The identified events are often represented using a set of keywords. (Petrovic et al., 2010) proposed an algorithm based on locality-sensitive hashing for detecting new events from a stream of Twitter posts. (O’Connor et al., 2010; Becker et al., 2011b; Becker et al., 2011a; Weng et al., 2011) proposed demo systems to display the event-related themes and popular tweets, allowing the users to navigate through their topic of interest. (Zhao et al., 2011) described an effort to perform data collection and event recognition despite various limits to the free access of Twitter data. (Diao et al., 2012) integrated both temporal information and users’ personal interests for bursty topic det"
N13-1135,D11-1141,0,0.0149767,"great discussion among the Twitter community. Summarizing the Twitter event is a challenging task that has yet been fully explored in the past. Most previous summarization studies focus on the well-formatted news documents, as driven by the annual DUC2 and TAC3 evaluations. In contrast, the Twitter messages (a.k.a., tweets) are very short and noisy, containing nonstandard terms such as abbreviations, acronyms, emoticons, etc. (Liu et al., 2011b; Liu et al., 2012; Eisenstein, 2013). The noisy contents also cause great difficulties to the traditional NLP tools such as NER and dependency parser (Ritter et al., 2011; Foster et al., 2011), limiting the possibility of applying finer-grained event analysis tools. In nature, the event tweets are closely associated with the timeline and are drastically different from a static collection of news documents. The tweets converge into text streams that pulse along the timeline and cluster around the important moments or sub-events. These “sub-events” are of crucial importance since they represent a surge of interest from the Twitter audience and the correspond1 https://developer.apple.com/wwdc/ http://duc.nist.gov/ 3 http://www.nist.gov/tac/ 2 1152 Proceedings of"
N13-1135,N10-1100,0,0.032053,"Missing"
N13-1135,P11-4023,0,0.0216791,"Missing"
N13-1135,W01-0100,0,\N,Missing
N15-1114,W13-2322,0,0.0643628,"et al., 2013), but also for non-textual media (e.g., videos and image collections; Kim et al., 2014; Kuznetsova et al., 2014; Zhao and Xing, 2014), where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuse"
N15-1114,P11-1049,0,0.0249704,"el that is able to better discriminate among the enlarged output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discours"
N15-1114,P13-2131,0,0.168393,"generation module (step 3; §5). 3 Dataset To build and evaluate our framework, we require a dataset that includes inputs and summaries, each with gold-standard AMR annotations.4 This allows us to use a statistical model for step 2 (graph summarization) and to separate its errors from those in step 1 (AMR parsing), which is important in determining whether this approach is worth further investment. Fortunately, the “proxy report” section of the AMR Bank (Knight et al., 2014) suits our needs. A 2 http://www.isi.edu/˜ulf/amr/help/ amr-guidelines.pdf 3 AMR parse quality is evaluated using smatch (Cai and Knight, 2013), which measures the accuracy of concept and relation predictions. JAMR was trained on the in-domain training portion of LDC2014T12 for our experiments. 4 Traditional multi-document summarization datasets, such as the ones used in DUC and TAC competitions, do not have gold-standard AMR annotations. Ave. # Sents. Source Graph Summ. Doc. Nodes Edges Expand 298 1.5 17.5 127 188 2,670 35 1.4 19.2 143 220 3,203 33 1.4 20.5 162 255 4,002 # Docs. Train Dev. Test Table 1: Statistics of our dataset. “Expand” shows the number of edges after performing graph expansion. The numbers are averaged across all"
N15-1114,W08-1106,0,0.0320492,"more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vande"
N15-1114,D14-1085,0,0.0432116,"ther decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) transform input to logical forms, score nodes using PageRank, and grow the graph from high-value nodes using heuristics. In Erkan and Radev (2004) and Mihalcea and Tarau (2004), the graph connects surface terms that co-occur. In both cases, t"
N15-1114,W02-1001,0,0.065115,"nstraint can be used to fix the size of the summary graph (measured by the number of edges) to L: XX ei,j = L (8) i j The performance of summarization systems depends strongly on their compression rate, so systems are only directly comparable when their compression rates are similar (Napoles et al., 2011). L is supplied to the system to control summary graph size. 5 http://www.gurobi.com 4.2.2 6 Parameter Estimation Given a collection of input and output pairs (here, source graphs and summary graphs), a natural starting place for learning the coefficients θ and ψ is the structured perceptron (Collins, 2002), which is easy to implement and often performs well. Alternatively, incorporating factored cost functions through a structured hinge loss leads to a structured support vector machine (SVM; Taskar et al., 2004) which can be learned with a very similar stochastic optimization algorithm. In our scenario, however, the gold-standard summary graph may not actually be a subset of the source graph. In machine translation, ramp loss has been found to work well in situations where the gold-standard output may not even be in the hypothesis space of the model (Gimpel and Smith, 2012). The structured perc"
N15-1114,dorr-etal-1998-thematic,0,0.039752,"where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuses on step 2, treating it as a structured prediction problem. We assume text documents as input1 and use JAMR for step 1. We use a simple metho"
N15-1114,P14-1134,1,0.755856,"ttention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuses on step 2, treating it as a structured prediction problem. We assume text documents as input1 and use JAMR for step 1. We use a simple method to read a bag of words off the summary graph, allowing evaluation with ROUGE-1, and leave full text generation from AMR (step 3) to future work. The graph summar"
N15-1114,C10-1039,0,0.125403,"d within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) tr"
N15-1114,D14-1168,0,0.0974604,"s and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) transform input to logical forms, score nodes using PageRank, and grow the graph from high-value nodes using heuristics"
N15-1114,W09-1802,0,0.0247998,"a more sophisticated model that is able to better discriminate among the enlarged output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive"
N15-1114,N12-1023,1,0.581372,"is the structured perceptron (Collins, 2002), which is easy to implement and often performs well. Alternatively, incorporating factored cost functions through a structured hinge loss leads to a structured support vector machine (SVM; Taskar et al., 2004) which can be learned with a very similar stochastic optimization algorithm. In our scenario, however, the gold-standard summary graph may not actually be a subset of the source graph. In machine translation, ramp loss has been found to work well in situations where the gold-standard output may not even be in the hypothesis space of the model (Gimpel and Smith, 2012). The structured perceptron, hinge, and ramp losses are compared in Table 4. We explore learning by minimizing each of the perceptron, hinge, and ramp losses, each optimized using Adagrad (Duchi et al., 2011), a stochastic optimization procedure. Let β be one model parameter (coefficient from θ or ψ). Let g (t) be the subgradient of the loss on the instance considered on the tth iteration with respect to β. Given an initial step size η, the update for β on iteration t is: β (t+1) ← β (t) − qP t η τ =1 5 (g (τ ) )2 g (t) (9) Generation Generation from AMR-like representations has received some"
N15-1114,N06-2015,0,0.105943,"Missing"
N15-1114,H89-1022,0,0.119216,"d Xing, 2014), where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuses on step 2, treating it as a structured prediction problem. We assume text documents as input1 and use JAMR for step 1. W"
N15-1114,Q14-1028,0,0.0105344,"ents on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization 1 Introduction Abstractive summarization is an elusive technological capability in which textual summaries of content are generated de novo. Demand is on the rise for high-quality summaries not just for lengthy texts (e.g., books; Bamman and Smith, 2013) and texts known to be prohibitively difficult for people to understand (e.g., website privacy policies; Sadeh et al., 2013), but also for non-textual media (e.g., videos and image collections; Kim et al., 2014; Kuznetsova et al., 2014; Zhao and Xing, 2014), where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier for"
N15-1114,P98-1116,0,0.0155022,"ceptron, hinge, and ramp losses are compared in Table 4. We explore learning by minimizing each of the perceptron, hinge, and ramp losses, each optimized using Adagrad (Duchi et al., 2011), a stochastic optimization procedure. Let β be one model parameter (coefficient from θ or ψ). Let g (t) be the subgradient of the loss on the instance considered on the tth iteration with respect to β. Given an initial step size η, the update for β on iteration t is: β (t+1) ← β (t) − qP t η τ =1 5 (g (τ ) )2 g (t) (9) Generation Generation from AMR-like representations has received some attention, e.g., by Langkilde and Knight (1998) who described a statistical method. Though we know of work in progress driven by the goal of machine translation using AMR, there is currently no system available. We therefore use a heuristic approach to generate a bag of words. Given a predicted subgraph, a system summary is created by finding the most frequently aligned word span for each concept node. (Recall that the JAMR parser provides these alignments; §2). The words in the resulting spans are generated in no particular order. While this is not a natural language summary, it is suitable for unigram-based summarization evaluation metho"
N15-1114,D14-1076,1,0.828156,"output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit"
N15-1114,N10-1134,0,0.0333914,"o Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive sum"
N15-1114,W04-1013,0,0.0178592,"Missing"
N15-1114,W09-1801,1,0.713494,"th more training data, or a more sophisticated model that is able to better discriminate among the enlarged output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 20"
N15-1114,P09-1039,1,0.53547,"Missing"
N15-1114,W04-3252,0,0.00680701,"tion (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) transform input to logical forms, score nodes using PageRank, and grow the graph from high-value nodes using heuristics. In Erkan and Radev (2004) and Mihalcea and Tarau (2004), the graph connects surface terms that co-occur. In both cases, the graphs are constructed based on surface text; it is not a representation of propositional semantics like AMR. However, future 1084 work might explore similar graph-based calculations to contribute features for subgraph selection in our framework. Our constructed source graph can easily reach ten times or more of the size of a"
N15-1114,W11-1611,0,0.0238146,"Missing"
N15-1114,D13-1156,0,0.00841813,"extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of t"
N15-1114,W13-3508,0,0.026987,"In this preliminary study we force the summary graph to be tree-structured, requiring that there is at most one incoming edge for each node: i ∀j ≤ N, (5) ∀i, j ≤ N. (6) 1081 X ei,j ≤ 1, ∀j ≤ N. (7) j Interestingly, the formulation so far equates to an ILP for solving the prize-collecting Steiner tree problem (PCST; Segev, 1987), which is known to be NP-complete (Karp, 1972). Our ILP formulation is modified from that of Ljubi´c et al. (2006). Flow-based constraints for tree structures have also previously been used in NLP for dependency parsing (Martins et al., 2009) and sentence compression (Thadani and McKeown, 2013). In our experiments, we use an exact ILP solver,5 though many approximate methods are available. Finally, an optional constraint can be used to fix the size of the summary graph (measured by the number of edges) to L: XX ei,j = L (8) i j The performance of summarization systems depends strongly on their compression rate, so systems are only directly comparable when their compression rates are similar (Napoles et al., 2011). L is supplied to the system to control summary graph size. 5 http://www.gurobi.com 4.2.2 6 Parameter Estimation Given a collection of input and output pairs (here, source"
N15-1114,C98-1112,0,\N,Missing
N15-1114,P13-1020,0,\N,Missing
N15-1114,W01-0100,0,\N,Missing
N16-1010,P13-1020,0,0.0345312,"Missing"
N16-1010,P11-1049,0,0.676317,"ight of wi , often measured by the number of sentences or documents that contain the concept. The ILP-based summarization approach (Gillick and Favre, 2009) searches for an optimal assignment to the sentence and concept variables so that the selected summary sentences maximize coverage of important concepts. The relationship between concepts and sentences is captured by a co-occurrence matrix A ∈ RN ×M , where Aij = 1 indicates the i-th concept appears in the j-th sentence, and Aij = 0 otherwise. In the literature, bigrams are frequently used as a surrogate for concepts (Gillick et al., 2008; Berg-Kirkpatrick et al., 2011). We follow the convention and use ‘concept’ and ‘bigram’ interchangeably in the paper. max y,z s.t. PN i=1 wi zi PM j=1 Aij yj ≥ zi Aij yj ≤ zi PM j=1 lj yj ≤ L yj ∈ {0, 1}, zi ∈ {0, 1} (1) (2) (3) (4) (5) Two sets of linear constraints are specified to ensure the ILP validity: (1) a concept is selected if and only if at least one sentence carrying it has been selected (Eq. 2), and (2) all concepts in a sentence will 81 be selected if that sentence is selected (Eq. 3). Finally, the selected summary sentences are allowed to contain a total of L words or less (Eq. 4). 3 Our Approach Because of"
N16-1010,C12-1056,0,0.0177446,"ed of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another important component of the ILP framework. Most summarization work focuses on summarizing news documents, as driven by the DUC/TAC conferences. Notable systems include maximal marginal relevance (Carbonell and Goldstein, 1998), 84 Conclusion We make the first effort to summarize"
N16-1010,W09-1802,0,0.138358,"student feedback summarization task in terms of both ROUGE scores and human evaluation. 2 ILP Formulation Let D be a set of student responses that consist of M sentences in total. Let yj ∈ {0, 1}, j = {1, · · · , M } indicate if a sentence j is selected (yj = 1) or not (yj = 0) in the summary. Similarly, let N be the number of unique concepts in D. zi ∈ {0, 1}, i = {1, · · · , N } indicate the appearance of concepts in the summary. Each concept i is assigned a weight of wi , often measured by the number of sentences or documents that contain the concept. The ILP-based summarization approach (Gillick and Favre, 2009) searches for an optimal assignment to the sentence and concept variables so that the selected summary sentences maximize coverage of important concepts. The relationship between concepts and sentences is captured by a co-occurrence matrix A ∈ RN ×M , where Aij = 1 indicates the i-th concept appears in the j-th sentence, and Aij = 0 otherwise. In the literature, bigrams are frequently used as a surrogate for concepts (Gillick et al., 2008; Berg-Kirkpatrick et al., 2011). We follow the convention and use ‘concept’ and ‘bigram’ interchangeably in the paper. max y,z s.t. PN i=1 wi zi PM j=1 Aij y"
N16-1010,hong-etal-2014-repository,0,0.153072,"Missing"
N16-1010,P13-1099,0,0.0162714,"tman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another important component of the ILP framework. Most summarization work focuses on summarizing news documents, as driven by the DUC/TAC conferences. Notable syst"
N16-1010,N10-1134,0,0.145343,"Missing"
N16-1010,W04-1013,0,0.110635,"Missing"
N16-1010,D15-1227,1,0.365046,"es - importance of cell direction on materials properties System Summary (I LP BASELINE) - drawing and indexing unit cell direction - it was interesting to understand how to find apf and fd from last weeks class - south pole explorers died due to properties of tin System Summary (O UR A PPROACH) - crystal structure directions - surprisingly i found nothing interesting today . - unit cell indexing - vectors in unit cells - unit cell drawing and indexing - the importance of cell direction on material properties 7 Table 4: Example reference and system summaries. 6 Related Work Our previous work (Luo and Litman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al.,"
N16-1010,N15-3004,1,0.513825,"class - south pole explorers died due to properties of tin System Summary (O UR A PPROACH) - crystal structure directions - surprisingly i found nothing interesting today . - unit cell indexing - vectors in unit cells - unit cell drawing and indexing - the importance of cell direction on material properties 7 Table 4: Example reference and system summaries. 6 Related Work Our previous work (Luo and Litman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequenc"
N16-1010,D12-1022,0,0.0171666,"r previous work (Luo and Litman, 2015) proposes to summarize student responses by extracting phrases rather than sentences in order to meet the need of aggregating and displaying student responses in a mobile application (Luo et al., 2015; Fan et al., 2015). It adopts a clustering paradigm to address the lexical variety issue. In this work, we leverage matrix imputation to solve this problem and summarize student response at a sentence level. The integer linear programming framework has demonstrated substantial success on summarizing news documents (Gillick et al., 2008; Gillick et al., 2009; Woodsend and Lapata, 2012; Li et al., 2013). Previous studies try to improve this line of work by generating better estimates of concept weights. Galanis et al. (2012) proposed a support vector regression model to estimate bigram frequency in the summary. Berg-Kirkpatrick et al. (2011) explored a supervised approach to learn parameters using a cost-augmentative SVM. Different from the above approaches, we focus on the co-occurrence matrix instead of concept weights, which is another important component of the ILP framework. Most summarization work focuses on summarizing news documents, as driven by the DUC/TAC confere"
N16-1010,C14-1187,1,0.862246,"l marginal relevance (Carbonell and Goldstein, 1998), 84 Conclusion We make the first effort to summarize student feedback using an integer linear programming framework with data imputation. Our approach allows sentences to share co-occurrence statistics and alleviates sparsity issue. Our experiments show that the proposed approach performs competitively against a range of baselines and shows promise for future automation of student feedback analysis. In the future, we may take advantage of the high quality student responses (Luo and Litman, 2016) and explore helpfulness-guided summarization (Xiong and Litman, 2014) to improve the summarization performance. We will also investigate whether the proposed approach benefits other informal text such as product reviews, social media discussions or spontaneous speech conversations, in which we expect the same sparsity issue occurs and the language expression is diverse. Acknowledgments This research is supported by an internal grant from the Learning Research and Development Center at the University of Pittsburgh. We thank Muhsin Menekse for providing the data set. We thank Jingtao Wang, Fan Zhang, Huy Nguyen and Zahra Rahimi for valuable suggestions about the"
N18-2045,C16-1251,0,0.0412767,"n of the task is presented in Section 2. Evaluation. We benchmark against baseline systems presented in the works of Saeidi et al. (2016) and Ma et al. (2018): (1) LR: a logistic regression classifier with n-gram and POS tag features; (2) LSTM-Final: a biLSTM taking the final states as representations; (3) LSTM-Loc: a biLSTM taking the states at the location where target t is mentioned as representations; (4) LSTM+TA+SA: a biLSTM equipped with complex target and sentence-level attention mechanisms; (5) SenticLSTM: an improved version of (4) incorporating the SenticNet external knowledge base (Cambria et al., 2016). We additionally implement a bi-directional EntNet with the same hyper-parameter settings and GloVe embeddings as our model (Henaff et al., 2017). In terms of evaluation, we adopt the standard 70/10/20 train/validation/test split, and report the test performance corresponding to the model with the best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT- LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentime"
N18-2045,D14-1162,0,0.0836611,"best validation score. Following Saeidi et al. (2016), we consider the top 4 aspects only (GENERAL, PRICE, TRANSIT- LOCATION, and SAFETY) and employ the following evaluation metrics: macro-average F1 and AUC for aspect detection ignoring the none class, and accuracy and macro-average AUC for sentiment classification. Following Ma et al. (2018), we also report strict accuracy for aspect detection, as the fraction of sentences where all aspects are detected correctly. Model configuration. We initialise our model with GloVe (300-D, trained on 42B tokens, 1.9M vocab, not updated during training: Pennington et al. (2014)) 4 and pre-process the corpus with tokenisation using NLTK (Bird et al., 2009) and case folding. Training is carried out over 800 epochs with the FTRL optimiser (McMahan et al., 2013) and a batch size of 128 and learning rate of 0.05. We use the following hyper-parameters for weight matrices in both directions: R ∈ R300×3 , H, U, V, W are all matrices of size R300×300 , v ∈ R300 , and hidden size of the GRU in Equation (4) is 300. Dropout is applied to the output of φ in the final classifier (Equation (8)) with a rate of 0.2. Moreover, we employ the technique introduced by Gal and Ghahramani"
N18-2045,D17-1047,0,0.0420584,"rack of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the tw"
N18-2045,C16-1146,0,0.218549,"ohn Timothy Baldwin School of Computing and Information Systems The University of Melbourne Victoria, Australia fliu3@student.unimelb.edu.au t.cohn@unimelb.edu.au tb@ldwin.net Abstract The earliest work on (T)ABSA relied heavily on feature engineering (Wagner et al., 2014; Kiritchenko et al., 2014), but more recent work based on deep learning has used models such as LSTMs to automatically learn aspect-specific word and sentence representations (Tang et al., 2016a). Despite these successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed fo"
N18-2045,C16-1311,0,0.0845094,"successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of"
N18-2045,D16-1021,0,0.0933057,"successes, keeping track of multiple entity–aspect pairs remains a difficult task, even for an LSTM. As reported in Saeidi et al. (2016), a target-dependent biLSTM is ineffective, both in terms of aspect detection and sentiment classification, compared to a simple logistic regression model with n-gram features. Intuitively, we would expect that a model which better captures linguistic structure via the original word sequencing should perform better, which provides the motivation for this research. More recently, successful works in (T)ABSA have explored the idea of leveraging external memory (Tang et al., 2016b; Chen et al., 2017). Their models are largely based on memory networks (Weston et al., 2015), originally developed for reasoning-focused machine reading comprehension tasks. In contrast to memory networks, where each input sentence/word occupies a memory slot and is then accessed via attention independently, recent advances in machine reading suggest that processing inputs sequentially is beneficial to overall performance (Seo et al., 2017; Henaff et al., 2017). However, successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of"
N18-2045,S14-2036,0,0.234655,"Missing"
N18-2045,S14-2076,0,0.172009,"Missing"
N19-1264,P18-3015,1,0.943037,"vely resembles the human abstract. In this paper, we construct an extractive summary by selecting consecutive word sequences from the source document. To accomplish this we utilize a novel reinforcement learning framework to explore the space of possible extractive summaries and assess each summary using a novel reward function judging the summary’s adequacy, fluency, length, and its competency to answer important questions. The system learns to sample extractive summaries yielding the highest expected rewards, with no pre-derived extraction labels needed. This work extends the methodology of Arumae and Liu (2018) with new representations of extraction units and thorough experimental evaluation. The contributions of this research can be summarized as follows: • we describe a novel framework generating extractive summaries by selecting consecutive sequences of words from source documents. This new system explores various encoding mechanisms, as well as new sampling techniques to capture phrase level data. Such a framework has not been thoroughly investigated in the past; • We conduct a methodical empirical evaluation from the point of view of information saliency. Rather than solely relying on automatic"
N19-1264,N18-1150,0,0.0804442,"Missing"
N19-1264,P16-1223,0,0.14966,"leveraging the extractive summary to answer these questions. We first encode each question Qk to a vector representation (qk ). This is achieved by concatenating the last hidden states of the forward/backward passes of a bidirectional LSTM (Eq. (6)). Next, we exploit the attention mechanism to locate summary parts that are relevant to answering the k-th question. Given the attention mechanism, an extractive summary S can be used to answer multiple questions related to the document. We define αt,k to be the semantic relatedness between the t-th source text unit and the k-th question. Following Chen et al. (2016a), we introduce a bilinear term to characterize their relationship (αt,k ∝ het Wα qk ; see Eq. (7)). In this process, we consider only those source text units selected in summary S. Using αt,k as weights, we then compute a context vector ck condensing summary content related to the k-th question (Eq. (8)). qk = f4Bi-LSTM (Qk ) exp(het Wα qk ) αt,k = P exp(het Wα qk ) Pt ck = t αt,k het uk = [ck ; qk ; |ck − qk |; ck ⊗ qk ] (6) (7) (8) (9) To predict the most probable answer, we construct a fully-connected network as the output layer. The input to the network includes a concatenation of the co"
N19-1264,P18-1224,0,0.0127061,"k ; |ck − qk |; ck ⊗ qk ] (6) (7) (8) (9) To predict the most probable answer, we construct a fully-connected network as the output layer. The input to the network includes a concatenation of the context vector (ck ), question vector (qk ), absolute difference (|ck − qk |) and element-wise product (ck ⊗ qk ) of the two vectors (Eq. (9)). A softmax function is used to estimate a probability distribution over the space of candidate answers: P (ek |S, Qk ) = softmax(We f ReLU (Wu uk + bu )). Such a fully-connected output layer has achieved success on natural language inference (Mou et al., 2016; Chen et al., 2018); here we test its efficacy on answer selection. The model parameters include {Wα , We , Wu , bu } and those of f4Bi-LSTM . 3.4 A Reinforcement Learning Framework In this section we introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function to promote summaries that are adequate, 2570 fluent, restricted in length, and competent in question answering. Our reward function consists of four components, whose interpolation weights γ, α, and β are tuned on the dev set. yields the highest probability to generate the system su"
N19-1264,P16-1046,0,0.523312,"018). Neural extractive summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018). However, sentence extraction can be coarse and in many cases, only a part of the sentence is worthy to be added to the summary. In this study, we perform finer-grained extractive summarization by allowing the system to select consecutive sequences of words rather than sentences to form a summary. Interestingly, studies reveal that summaries generated by recent neural abstractive systems are, in fact, quite “extractive.” Abstractive s"
N19-1264,W06-0707,0,0.0573339,"sequences of source words to be selected by preventing many 0/1 switches in the label sequence (i.e., |yt − yt−1 |). Finally, we limit the summary size by setting the ratio of selected words to be close to a threshold δ (Eq. (13)). K QA Rc (y) = 1 X log P (e∗k |y, Qk ) (10) K k=1 1 Adequ. Ra (y) = ∗ U(y, y∗ ) |y | Rf (y) = − |y| X (11) |yt − yt−1 | (12) 1 X yt − δ Length Rl (y) = − |y |t (13) Fluency t=2 The reward function R(y) successfully combines intrinsic measures of summary fluency and adequacy (Goldstein et al., 2005) with extrinsic measure of summary responsiveness to given questions (Dang, 2006; Murray et al., 2008). A reinforcement learning agent finds a policy P (y|x) to maximize the expected reward EP (y|x) [R(y)]. Training the system with policy gradient (Eq. (14)) involves repeatedly sampling an extractive sumˆ from the source document x. At time t, mary y the agent takes an action by sampling a decision based on p(yt |ˆ y<t , x) (Eq. (5)) indicating whether the t-th source text unit is to be included in the ˆ is summary. Once the full summary sequence y generated, it is compared to the ground-truth sequence to compute the reward R(ˆ y). In this way, reinforcement learning expl"
N19-1264,D18-1443,0,0.110504,"Missing"
N19-1264,W09-1802,0,0.0491645,"essing the summary quality with reading comprehension tasks. Our summaries compare favorably with the automatic metrics against state of the art, and show promising results against baselines when evaluated by humans for question answering. 2 Related Work Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011). The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013, 2014; Hong et al., 2014; Yogatama et al., 2015). A majority of the methods are unsupervised. They estimate sentence importance based on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from t"
N19-1264,W05-0900,0,0.11793,"tem (y) and reference summary (y∗ ). The fluency criterion (Eq. (12)) encourages consecutive sequences of source words to be selected by preventing many 0/1 switches in the label sequence (i.e., |yt − yt−1 |). Finally, we limit the summary size by setting the ratio of selected words to be close to a threshold δ (Eq. (13)). K QA Rc (y) = 1 X log P (e∗k |y, Qk ) (10) K k=1 1 Adequ. Ra (y) = ∗ U(y, y∗ ) |y | Rf (y) = − |y| X (11) |yt − yt−1 | (12) 1 X yt − δ Length Rl (y) = − |y |t (13) Fluency t=2 The reward function R(y) successfully combines intrinsic measures of summary fluency and adequacy (Goldstein et al., 2005) with extrinsic measure of summary responsiveness to given questions (Dang, 2006; Murray et al., 2008). A reinforcement learning agent finds a policy P (y|x) to maximize the expected reward EP (y|x) [R(y)]. Training the system with policy gradient (Eq. (14)) involves repeatedly sampling an extractive sumˆ from the source document x. At time t, mary y the agent takes an action by sampling a decision based on p(yt |ˆ y<t , x) (Eq. (5)) indicating whether the t-th source text unit is to be included in the ˆ is summary. Once the full summary sequence y generated, it is compared to the ground-truth"
N19-1264,N18-1065,0,0.0418702,"Missing"
N19-1264,P18-1064,0,0.0380062,"Missing"
N19-1264,N09-1041,0,0.0569865,"5 bytes. articles and their QA pairs. The summary ratio δ is set to 0.15, yielding extractive summaries of about 60 words. Following Arumae and Liu (2018), we set hyperparameters β = 2α; α and γ are tuned on the dev set using grid search. 4.2 Experimental Results Comparison systems We compare our method with a number of extractive and abstractive systems that have reported results on the CNN/DM datasets. We consider non-neural approaches that extract sentences from the source article to form a summary. These include LexRank (Radev et al., 2004), SumBasic (Vanderwende et al., 2007), and KLSum (Haghighi and Vanderwende, 2009). Such methods treat sentences as bags of words, and then select sentences containing topically important words. We further include the Lead-3 baseline that extracts the first 3 sentences from any given article. The method has been shown to be a strong baseline for summarizing news articles. Neural extractive approaches focus on learning vector representations for sentences and words, then performing extraction based on the learned representations. Cheng et al. (2016) describe a neural network method composed of a hierarchical document encoder and an attention-based extractor. The system has t"
N19-1264,P82-1020,0,0.814635,"Missing"
N19-1264,hong-etal-2014-repository,0,0.0231026,"bly with the automatic metrics against state of the art, and show promising results against baselines when evaluated by humans for question answering. 2 Related Work Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011). The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013, 2014; Hong et al., 2014; Yogatama et al., 2015). A majority of the methods are unsupervised. They estimate sentence importance based on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Neural extracti"
N19-1264,D17-1223,0,0.0221849,"sed on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Neural extractive summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve"
N19-1264,D18-1208,0,0.0299999,"the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Neural extractive summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018). However, sentence extraction"
N19-1264,P18-1027,0,0.0185592,"from 1 to 5 words. Additionally, word level modeling can be considered a special case of chunks where the length of each phrase is 1. It is important to note that using sentences as extraction units is out of the scope of this paper, because our work focuses on finer-grained extraction units such as words and phrases and this is notably a more challenging task. The most successful neural models for encoding a piece of text to a fixed-length vector include the recurrent (Hochreiter and Schmidhuber, 1997) and convolutional neural networks (CNN; Kim et al., 2014), among others. A recent study by Khandelwal et al. (2018) reported that the recurrent networks are capable of memorizing a recent context of about 20 tokens and the model is highly sensitive to word order, whereas this is less the case for CNN whose max-pooling operation makes it agnostic to word order. We implement both networks and are curious to compare their effectiveness at encoding extraction units for summarization. Our Approach Let S be an extractive summary consisting of text segments selected from a source document x. The summary can be mapped to a sequence of binary labels y assigned to document words. In this section we first present a s"
N19-1264,D18-1446,1,0.884358,"Missing"
N19-1264,D16-1011,0,0.150774,"Missing"
N19-1264,D13-1047,1,0.91344,"Missing"
N19-1264,W04-1013,0,0.103423,"es a graph-based attention mechanism to enhance the encoderdecoder framework. PointerGen+Cov. (See et al., 2017) allows the system to not only copy words from the source text but also generate summary words by selecting them from a vocabulary. Abstractive methods can thus introduce new words to the summary that are not present in the source article. However, system summaries may change the meaning of the original texts due to this flexibility. Results We present summarization results of various systems in Tables 2 and 3, evaluated on the standard CNN/DM test sets by R-1, R-2, and R-L metrics (Lin, 2004), which respectively measure the overlap of unigrams, bigrams, and longest common subsequences between system and reference summaries. We investigate four variants of our method: QASumm+NoQ does not utilize any question-answer pairs during training. It extracts summary text chunks by learning from groundtruth labels (§3.2) and the chunks are encoded by f1Bi-LSTM . Other variants initialize their models using pretrained parameters from QASumm+NoQ, then integrate the reinforcement learning objective (§3.4) to exploit the space of possible extractive summaries and reward those that are useful for"
N19-1264,N10-1134,0,0.0430935,"y with reading comprehension tasks. Our summaries compare favorably with the automatic metrics against state of the art, and show promising results against baselines when evaluated by humans for question answering. 2 Related Work Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011). The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013, 2014; Hong et al., 2014; Yogatama et al., 2015). A majority of the methods are unsupervised. They estimate sentence importance based on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008"
N19-1264,P14-5010,0,0.00314827,"Cloze-style question-answer pair (see Table 1). When a sentence contains multiple answer tokens, a set of QA pairs can be obtained from it. It is important to note that at least one QA pair should be extracted from each sentence of the abstract. Because a system summary is trained to contain content useful for answering all questions (≈human abstract), any missing QA pair is likely to cause the summary to be insufficient. We collect answer tokens using the following methods: (a) we extract a set of entities with tag {PER, LOC, ORG, MISC} from each sentence using the Stanford CoreNLP toolkit (Manning et al., 2014); (b) we also identify the ROOT word of each sentence’s dependency parse tree along with the sentence’s subject/object word, whose type is {NSUBJ, CSUBJ, OBJ, IOBJ} (if exists), then add them to the collection of answer tokens. Further, we prune the answer space by excluding those which appear fewer than 5 times overall. Having several methods for question construction allows us to explore the answer space properly. In the results section we perform experiments on root, subject/object, and named entities to see which model provides the best extraction guide. Given an extractive summary S conta"
N19-1264,W09-1801,0,0.104444,"ages of our system by assessing the summary quality with reading comprehension tasks. Our summaries compare favorably with the automatic metrics against state of the art, and show promising results against baselines when evaluated by humans for question answering. 2 Related Work Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011). The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013, 2014; Hong et al., 2014; Yogatama et al., 2015). A majority of the methods are unsupervised. They estimate sentence importance based on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale train"
N19-1264,D18-1206,0,0.0295593,"et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018). However, sentence extraction can be coarse and in many cases, only a part of the sentence is worthy to be added to the summary. In this study, we perform finer-grained extractive summarization by allowing the system to select consecutive sequences of words rather than sentences to form a summary. Interestingly, studies reveal that summaries generated by recent neural abstractive systems are, in fact, quite “extractive.” Abstractive systems often adopt the encoder-decoder architecture with an attention mechanism (Rush et al., 2015; Nallapati e"
N19-1264,N18-1158,0,0.0343251,"et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018). However, sentence extraction can be coarse and in many cases, only a part of the sentence is worthy to be added to the summary. In this study, we perform finer-grained extractive summarization by allowing the system to select consecutive sequences of words rather than sentences to form a summary. Interestingly, studies reveal that summaries generated by recent neural abstractive systems are, in fact, quite “extractive.” Abstractive systems often adopt the encoder-decoder architecture with an attention mechanism (Rush et al., 2015; Nallapati e"
N19-1264,D14-1162,0,0.0806526,"Missing"
N19-1264,D15-1044,0,0.0373559,"photos of the victim on and computer. Table 1: An example extractive summary bolded in the article (top). Highlighted sections indicate salient segments useful for answering fill-in-the-blank questions generated from human abstracts (bottom). Introduction Our increasingly digitized lifestyle calls for summarization techniques to produce short and accurate summaries that can be accessed at any time. These summaries should factually adhere to the content of the source text and present the reader with the key points therein. Although neural abstractive summarization has shown promising results (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), these methods can have potential drawbacks. It was revealed that abstracts generated by neural systems sometimes alter or falsify objective details, and introduce new meanings not present in the original text (Cao et al., 2018). Reading these abstracts can lead to misinterpretation of the source materials, which is clearly undesirable. In this work, we focus on extractive summarization, where the summaries are guaranteed to remain faithful to the original content. Our system seeks to identify salient and consecutive sequences of words from the sourc"
N19-1264,P16-2022,0,0.0211897,"k het uk = [ck ; qk ; |ck − qk |; ck ⊗ qk ] (6) (7) (8) (9) To predict the most probable answer, we construct a fully-connected network as the output layer. The input to the network includes a concatenation of the context vector (ck ), question vector (qk ), absolute difference (|ck − qk |) and element-wise product (ck ⊗ qk ) of the two vectors (Eq. (9)). A softmax function is used to estimate a probability distribution over the space of candidate answers: P (ek |S, Qk ) = softmax(We f ReLU (Wu uk + bu )). Such a fully-connected output layer has achieved success on natural language inference (Mou et al., 2016; Chen et al., 2018); here we test its efficacy on answer selection. The model parameters include {Wα , We , Wu , bu } and those of f4Bi-LSTM . 3.4 A Reinforcement Learning Framework In this section we introduce a reinforcement learning framework to explore the space of possible extractive summaries and present a novel reward function to promote summaries that are adequate, 2570 fluent, restricted in length, and competent in question answering. Our reward function consists of four components, whose interpolation weights γ, α, and β are tuned on the dev set. yields the highest probability to ge"
N19-1264,P17-1099,0,0.772351,"e 1: An example extractive summary bolded in the article (top). Highlighted sections indicate salient segments useful for answering fill-in-the-blank questions generated from human abstracts (bottom). Introduction Our increasingly digitized lifestyle calls for summarization techniques to produce short and accurate summaries that can be accessed at any time. These summaries should factually adhere to the content of the source text and present the reader with the key points therein. Although neural abstractive summarization has shown promising results (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017), these methods can have potential drawbacks. It was revealed that abstracts generated by neural systems sometimes alter or falsify objective details, and introduce new meanings not present in the original text (Cao et al., 2018). Reading these abstracts can lead to misinterpretation of the source materials, which is clearly undesirable. In this work, we focus on extractive summarization, where the summaries are guaranteed to remain faithful to the original content. Our system seeks to identify salient and consecutive sequences of words from the source document, and highlight them in the text"
N19-1264,C18-1146,1,0.894776,"Missing"
N19-1264,P13-1136,0,0.0717119,"ension tasks. Our summaries compare favorably with the automatic metrics against state of the art, and show promising results against baselines when evaluated by humans for question answering. 2 Related Work Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011). The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013, 2014; Hong et al., 2014; Yogatama et al., 2015). A majority of the methods are unsupervised. They estimate sentence importance based on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2"
N19-1264,P10-1058,0,0.0230436,"to remain faithful to the original content. Our system seeks to identify salient and consecutive sequences of words from the source document, and highlight them in the text to assist users in browsing and comprehending lengthy documents. An example is illustrated in Table 1. A primary challenge faced by extractive summarizers is the lack of annotated data. The cost of hiring humans to label a necessary amount of source articles with summary words, good for training a modern classifier, can be prohibitive. Previous work has exploited using human abstracts to derive labels for extraction units (Woodsend and Lapata, 2010). E.g., a source word is tagged 1 if it appears in the abstract, 0 otherwise. Although pairs of source articles and human abstracts are abundant, labels derived in this way are not necessarily best since summary saliency can not be easily captured with a rule based categorization. Considering that human abstracts involve generalization, paraphrasing, and can con2566 Proceedings of NAACL-HLT 2019, pages 2566–2577 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tain words not present in the source text, leveraging them to derive labels for extracti"
N19-1264,K17-1045,0,0.0145127,"summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018). However, sentence extraction can be coarse and in many cases, only a part of the sentence is worthy to be added to the summary. In this study, we perform finer-grained extractive summarization by allowing the system to select consecutive sequences of words rather than sentences to form a summary. Interestingly, studies reveal that summaries generated by recent neural abstractive systems are, in fact, quite “extractive.” Abstractive systems often adopt the e"
N19-1264,D15-1228,1,0.856586,"tic metrics against state of the art, and show promising results against baselines when evaluated by humans for question answering. 2 Related Work Extractive summarization has seen growing popularity in the past decades (Nenkova and McKeown, 2011). The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary (Knight and Marcu, 2002; Radev et al., 2004; Zajic et al., 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Lin and Bilmes, 2010; Wang et al., 2013; Li et al., 2013, 2014; Hong et al., 2014; Yogatama et al., 2015). A majority of the methods are unsupervised. They estimate sentence importance based on the sentence’s length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Neural extractive summarization has foc"
N19-1264,P18-1061,0,0.0268556,"e document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Neural extractive summarization has focused primarily on extracting sentences (Nallapati et al., 2017; Cao et al., 2017; Isonuma et al., 2017; Tarnpradab et al., 2017; Zhou et al., 2018; Kedzie et al., 2018). These studies exploit parallel training data consisting of source articles and story highlights (i.e., human abstracts) to create ground-truth labels for sentences. A neural extractive summarizer learns to predict a binary label for each source sentence indicating if it is to be included in the summary. These studies build distributed sentence representations using neural networks (Cheng and Lapata, 2016; Yasunaga et al., 2017) and use reinforcement learning to optimize the evaluation metric (Narayan et al., 2018b) and improve summary coherence (Wu and Hu, 2018). Howeve"
P09-2066,N01-1016,0,0.0102414,"sentence in the example in Sec 1. Human: we have to refine the tasks in order to avoid rephrasing Markov (S1): we have to refine the tasks more and more which we haven’t done in order to avoid this rephrasing Markov (S2): we have to refine the tasks which we haven’t done order to avoid this rephrasing FP + IP: we have to refine the tasks more and more which we haven’t done to avoid this rephrasing 2.2.4 Compression Using Lexicalized Markov Grammars The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001). Two outputs were generated using this method with different compression rates (defined as the number of words preserved in the compression divided by the total number of words in the original sentence).5 We name them “Markov (S1)” and “Markov (S2)” respectively. 3 Info. 4.35 3.64 2.89 3.70 Since our goal is to answer the question if we can use sentence compression to generate abstractive summaries, we compare the compressed summaries, as well as the original extractive summaries, against the reference abstractive summaries. The ROUGE-1 results along with the word compression ratio for each c"
P09-2066,N07-1023,0,0.121649,"mpression on an extractive summary to improve its readability and make it more like an abstractive summary. Compressing sentences could be a first step toward our ultimate goal of creating an abstract for spoken documents. Sentence compression has been widely studied in language processing. (Knight and Marcu, 2002; Cohn and Lapata, 2009) learned rewriting rules that indicate which words should be dropped in a given context. (Knight and Marcu, 2002; Turner and Charniak, 2005) applied the noisy-channel framework to predict the possibilities of translating a sentence to a shorter word sequence. (Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous contextfree grammar (SCFG) deletion rules. Unlike these approaches that need a training corpus, (Clarke and Lapata, 2008) encoded the language model and a variety of linguistic constraints as linear inequalities, and employed the integer programming approach to find a subset of words that maximize an objective function. Our focus in this paper is not on new compression algorithms, but rather on using compression to bridge the gap of extractive and abstractive summarization. We use different"
P09-2066,W06-1643,0,0.0494182,"ummarization has focused on extractive summarization, that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output. Various approaches to extractive summarization have been evaluated recently. Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009). Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006). (Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score. Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compress the extracted sentences and merge them into a concise summary. Simply concatenating 261 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261–264, c Suntec, Singapore, 4 August 2009."
P09-2066,W04-1013,0,0.0305099,"est results in the future. 2.2.3 Compression Using Integer Programming We employ the integer programming (IP) approach in the same way as (Clarke and Lapata, 2008). Given an utterance S = w1 , w2 , ..., wn , the IP approach forms a compression of this utterance only by dropping words and preserving the word sequence that maximizes an objective function, defined as the sum of the signifi1 The extractive units are DAs. We use DAs and sentences interchangeably in this paper when there is no ambiguity. 2 http://www.mturk.com/mturk/welcome 262 For a comparison, we also include the ROUGE-1 Fscores (Lin, 2004) of each system output against the human compressed sentences. cance scores of the consisting words and n-gram probabilities from a language model: n P yi · Sig(wi ) max λ · i=1 n−2 P P n−1 + (1 − λ) · n P i=0 j=i+1 k=j+1 Approach Human Markov (S1) Markov (S2) FP + IP xijk · P (wk |wi , wj ) where yi and xijk are two binary variables: yi = 1 represents that word wi is in the compressed sentence; xijk = 1 represents that the sequence wi , wj , wk is in the compressed sentence. A trade-off parameter λ is used to balance the contribution from the significance scores for individual words and the l"
P09-2066,N06-2023,0,0.0442652,", that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output. Various approaches to extractive summarization have been evaluated recently. Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009). Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006). (Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score. Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compress the extracted sentences and merge them into a concise summary. Simply concatenating 261 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 261–264, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2.2.2 Filler Phrase Detection We define filler ph"
P09-2066,W05-0905,0,0.0461979,"framework, where we also introduce a filler phrase (FP) detection Introduction Meeting summaries provide an efficient way for people to browse through the lengthy recordings. Most current research on meeting summarization has focused on extractive summarization, that is, it extracts important sentences (or dialogue acts) from speech transcripts, either manual transcripts or automatic speech recognition (ASR) output. Various approaches to extractive summarization have been evaluated recently. Popular unsupervised approaches are maximum marginal relevance (MMR), latent semantic analysis (LSA) (Murray et al., 2005a), and integer programming (Gillick et al., 2009). Supervised methods include hidden Markov model (HMM), maximum entropy, conditional random fields (CRF), and support vector machines (SVM) (Galley, 2006; Buist et al., 2005; Xie et al., 2008; Maskey and Hirschberg, 2006). (Hori et al., 2003) used a word based speech summarization approach that utilized dynamic programming to obtain a set of words to maximize a summarization score. Most of these summarization approaches aim for selecting the most informative sentences, while less attempt has been made to generate abstractive summaries, or compr"
P09-2066,W04-2319,0,0.0161906,"on, we also use human compression. All of these compressed sentences are compared to abstractive summaries. Our experiments using the ICSI meeting corpus show that compressing extractive summaries can improve human readability and the ROUGE scores against the reference abstractive summaries. 2 Sentence Compression of Extractive Summaries 2.1 Corpus We used the ICSI meeting corpus (Janin et al., 2003), which contains naturally occurring meetings, each about an hour long. All the meetings have been transcribed and annotated with dialogue acts (DAs), topics, abstractive and extractive summaries (Shriberg et al., 2004; Murray et al., 2005b). In this study, we use the extractive and abstractive summaries of 6 meetings from this corpus. These 6 meetings were chosen because they have been used previously in other related studies, such as summarization and keyword extraction (Murray et al., 2005a). On average, an extractive summary contains 76 sentences1 (1252 words), and an abstractive summary contains 5 sentences (111 words). 2.2 where N is the total number of sentences and Ni is the number of sentences containing this phrase. Phrases with low ISF scores mean that they appear in many occasions and are not do"
P09-2066,P05-1036,0,0.0281651,"roup decided to hire the wizard and continue with the refinement... In this paper, our goal is to answer the question if we can perform sentence compression on an extractive summary to improve its readability and make it more like an abstractive summary. Compressing sentences could be a first step toward our ultimate goal of creating an abstract for spoken documents. Sentence compression has been widely studied in language processing. (Knight and Marcu, 2002; Cohn and Lapata, 2009) learned rewriting rules that indicate which words should be dropped in a given context. (Knight and Marcu, 2002; Turner and Charniak, 2005) applied the noisy-channel framework to predict the possibilities of translating a sentence to a shorter word sequence. (Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous contextfree grammar (SCFG) deletion rules. Unlike these approaches that need a training corpus, (Clarke and Lapata, 2008) encoded the language model and a variety of linguistic constraints as linear inequalities, and employed the integer programming approach to find a subset of words that maximize an objective function. Our focus in this paper is"
P11-2013,W10-0513,0,0.051345,"tylistic variation (6) letter repetition tgthr, weeknd, shudnt 4got, sumbody, kulture t0gether, h3r3, 5top, doinq thimg, macam betta, hubbie, cutie pleeeaas, togtherrr (7) any combination of (1) to (6) luvvvin, 2moro, m0rnin Table 2: Nonstandard tokens that can be processed by the unified letter transformation approach. 2.2 Web based Data Collection w/o Supervision We propose to automatically collect training data (annotate nonstandard words with the corresponding English forms) using a web-based approach, therefore avoiding the expensive human annotation. We use the Edinburgh Twitter corpus (Petrovic et al., 2010) for data collection, which contains 97 million Twitter messages. The English tweets were extracted using the TextCat language identification toolkit (Cavnar and Trenkle, 1994), and tokenized into a sequence of clean tokens consisting of letters, digits, and apostrophe. For the out-of-vocabulary (OOV) tokens consisting of letters and apostrophe, we form n Google queries for each of them in the form of either “w1 w2 w3 ” OOV or OOV “w1 w2 w3 ”, where w1 to w3 are consecutive context words extracted from the tweets that contain this OOV. n is set to 6 in this study. The first 32 returned snippet"
P11-2013,P02-1019,0,0.0674139,"001), Cook and Stevenson (2009) employed the noisy channel model to find the most probable word sequence given the observed noisy message. Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consum"
P11-2013,N09-2069,0,0.0566637,"ortland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics (for text messages or other domains), Sproat et al. (2001), Cook and Stevenson (2009) employed the noisy channel model to find the most probable word sequence given the observed noisy message. Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve ca"
P11-2013,P06-2005,0,0.717287,"rved noisy message. Their approaches first classified the nonstandard tokens into various categories (e.g., abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consuming to obtain; (2) it is hard to establish a standard taxonomy for categorizing the tokens found in text mess"
P11-2013,P10-1079,0,0.460264,"lculated the posterior probability of the nonstandard tokens based on each category. Choudhury et al. (2007) developed a hidden Markov model using hand annotated training data. Yang et al. (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. Aw et al. (2006), Kobus et al. (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. Beaufort et al. (2010) experimented with the weighted finitestate machines for normalizing French SMS messages. Most of the above approaches rely heavily on the hand annotated data and involve categorizing the nonstandard tokens in the first place, which gives rise to three problems: (1) the labeled data is very expensive and time consuming to obtain; (2) it is hard to establish a standard taxonomy for categorizing the tokens found in text messages; (3) the lack of optimized way to integrate various category-specific models often compromises the system performance, as confirmed by (Cook and Stevenson, 2009). In thi"
P11-2013,N07-1047,0,0.0213641,"feature vector for each letter in the dictionary word, using its mapped character as the reference label. This labeled data set is used to train a CRF model with LBFGS (Lafferty et al., 2001; Kudo, 2005). We use the following features: • Character-level features Character n-grams: c−1 , c0 , c1 , (c−2 c−1 ), (c−1 c0 ), (c0 c1 ), (c1 c2 ), (c−3 c−2 c−1 ), (c−2 c−1 c0 ), (c−1 c0 c1 ), (c0 c1 c2 ), (c1 c2 c3 ). The relative position of character in the word. • Phonetic-level features Phoneme n-grams: p−1 , p0 , p1 , (p−1 p0 ), (p0 p1 ). We use the many-to-many letterphoneme alignment algorithm (Jiampojamarn et al., 2007) to map each letter to multiple phonemes (1-to-2 alignment). We use three binary features to indicate whether the current, previous, or next character is a vowel. • Syllable-level features Relative position of the current syllable in the 74 word; two binary features indicating whether the character is at the beginning or the end of the current syllable. The English hyphenation dictionary (Hindson, 2006) is used to mark all the syllable information. The trained CRF model can be applied to any English word to generate its variants with probabilities. 3 Experiments We evaluate the system performa"
P11-2013,C08-1056,0,\N,Missing
P11-2013,W09-2010,0,\N,Missing
P12-1109,P06-2005,0,0.470288,"Missing"
P12-1109,P10-1079,0,0.0625224,"by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al., 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisytoken under each of the HMM models; (Cook and Stevenson, 2009) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine translation (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS no"
P12-1109,P00-1037,0,0.0292448,"number of candidates and the broad word-coverage can be successfully translated into message-level performance gain. In addition, our system requires no human annotations, therefore can be easily adapted to different domains. 2 Related work Text normalization, in its traditional sense, is the first step of a speech synthesis system, where the numbers, dates, acronyms, etc. found in the realworld text were converted into standard dictionary words, so that the system can pronounce them correctly. Spell checking plays an important role in this process. (Church and Gale, 1991; Mays et al., 1991; Brill and Moore, 2000) proposed to use the noisy channel framework to generate a list of corrections for any misspelled word, ranked by the corresponding posterior probabilities. (Sproat et al., 2001) enhanced this framework by calculating the likelihood probability as the chance of a noisy token and its associated tag being generated by a specific word. With the rapid growth of SMS and social media content, text normalization system has drawn increasing attention in the recent decade, where the focus is on converting the noisy nonstandard tokens in the informal text into standard dictionary words. (Choudhury et al"
P12-1109,D11-1052,0,0.0154422,"Missing"
P12-1109,W11-0704,0,0.0129206,"expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-based abbreviations, misspellings) or require large-scale human annotated corpus for training, which greatly hinders the scalability of the system. In this paper, we propose a novel cognitively-driven text normalization system that robustly tackle both the unintentional misspellings and the intentionally-created noisy tokens. We propose a global context-based approach to purify the automatically collected training data and learn"
P12-1109,P11-1038,0,0.551338,"as also tackled under the machine translation (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion factors in calculating the likelihood probabilities. (Gouws et al., 2011) revealed that different populations exhibit different shortening styles. Most of the above systems limit their processing scope to certain categories (e.g., deletion-bas"
P12-1109,N07-1047,0,0.0226771,"unction features to accurately pinpoint the current character position. We consider conjunction features formed by concatenating character position in syllable and current syllable position in the word (e.g., conjunction feature “L B” for the letter “d” in Table 2). A similar set of features are also developed on morpheme level. We consider conjunction of character/vowel feature and their boundary tags on the syllable/morpheme/word level; conjunction of phoneme and phoneme boundary tags, and absolute position of current character within the corre5 Phoneme decomposition is generated using the (Jiampojamarn et al., 2007) algorithm to map up to two letters to phonemes (2-to-2 alignment); syllable boundary acquired by the hyphenation algorithm (Liang, 1983); morpheme boundary determined by toolkit Morfessor 1.0 (Creutz and Lagus, 2005). 6 For phoneme boundary, we use “B1” and “L1” to represent two different characters aligned to one phoneme and “B2”, “L2” represent same characters aligned to one phoneme. 1039 sponding syllable/morpheme/word. We use the aforementioned features to train the CRF model, then apply the model on dictionary words si to generate multiple variations ti for each word. When a nonstandard"
P12-1109,C08-1056,0,0.0391618,"9) calculated the sum of the probabilities of a noisy token being generated by a specific word and a word formation process; (Beaufort et al., 2010) employed the weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine translation (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens withou"
P12-1109,W11-0709,1,0.433867,"r sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3, 10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2gether (6326) 2getha (1266) 2gthr (178) togetha (919) togather (207) togehter (94) tgthr (250) t0gether (57) togeter (49) togeda (20) toqethaa (10) 2getter (10) u (3240535) yaaa (7740) yoooooou (186) ya (460963) yew (7591) youy (105) yo (2"
P12-1109,P11-2013,1,0.851193,"r sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3, 10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2gether (6326) 2getha (1266) 2gthr (178) togetha (919) togather (207) togehter (94) tgthr (250) t0gether (57) togeter (49) togeda (20) toqethaa (10) 2getter (10) u (3240535) yaaa (7740) yoooooou (186) ya (460963) yew (7591) youy (105) yo (2"
P12-1109,P11-1037,0,0.455253,"r sense, Twitter messages, SMS messages, Facebook updates, chat logs, Emails, etc. can all be considered as “social text”, The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3, 10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2gether (6326) 2getha (1266) 2gthr (178) togetha (919) togather (207) togehter (94) tgthr (250) t0gether (57) togeter (49) togeda (20) toqethaa (10) 2getter (10) u (3240535) yaaa (7740) yoooooou (186) ya (460963) yew (7591) youy (105) yo (2"
P12-1109,N07-1025,0,0.0097514,"election Manual annotation of the noisy nonstandard tokens takes a lot of time and effort. (Liu et al., 2011b) proposed to use Google search engine to automatically collect large amount of training pairs. Yet the resulting (work, token) pairs are often noisy, containing pairs such as (events, “ents”), (downtown, “downto”), etc. The ideal training data should consist of the most frequent nonstandard tokens paired with the corresponding corrections, so that the system can learn from the most representative letter transformation patterns. Motivated by research on word sense disambiguation (WSD) (Mihalcea, 2007), we hypothesize the nonstandard token and the standard word share a lot of common terms in their global context. For example, “luv” and “love” share “i”, “you”, “u”, “it”, etc. among their top context words. Based on this finding, we propose to filter out the low-quality train1038 where wi,k is the weight of term tk within the context of term ti . The term weights are defined using a normalized TF-IDF method: wi,k = T Fi,k N × log( ) T Fi DFk where T Fi,k is the count of term tk appearing within the context of term ti ; T Fi is the total count of ti in TF the corpus. T Fi,k is therefore the r"
P12-1109,I11-1109,0,0.810278,"weighted finite-state machines (FSMs) and rewriting rules for normalizing French SMS; (Pennell and Liu, 2010) focused on tweets created by handsets and developed a CRF tagger for deletion-based abbreviation. The text normalization problem was also tackled under the machine translation (MT) or speech recognition (ASR) framework. (Aw et al., 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. (Kobus et al., 2008) showed that using a statistical MT system in combination with an analogy of the ASR system improved performance in French SMS normalization. (Pennell and Liu, 2011) proposed a two-phase character-level MT system for expanding the abbreviations into standard text. Recent work also focuses on normalizing the Twitter messages, which is generally considered a more challenging task. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed words and generated corrections based on the morphophonemic similarity. (Liu et al., 2011b) proposed to normalize the nonstandard tokens without explicitly categorizing them. (Xue et al., 2011) adopted the noisy-channel framework and incorporated orthographic, phonetic, contextual, and acronym expansion fac"
P12-1109,W10-0513,0,0.0268813,"ial text”, The goal of this work is to automatically convert the noisy nonstandard tokens observed in the social text into standard English words. We aim for a robust text normalization system with “broad coverage”, i.e., for any user-created nonstandard token, the system should be able to restore the correct word within its top n candidates (n = 1, 3, 10...). This is a very challenging task due to two facts: first, there exists huge amount and a wide variety of nonstandard tokens. (Liu et al., 2011b) found more than 4 million distinct out-of-vocabulary tokens in the Edinburgh Twitter corpus (Petrovic et al., 2010); second, the nonstandard tokens consist 1035 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1035–1044, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics 2gether (6326) 2getha (1266) 2gthr (178) togetha (919) togather (207) togehter (94) tgthr (250) t0gether (57) togeter (49) togeda (20) toqethaa (10) 2getter (10) u (3240535) yaaa (7740) yoooooou (186) ya (460963) yew (7591) youy (105) yo (252274) yuo (467) yoiu (128) yaa (17015) youz (426) yoooouuuu (82) Table 1: Nonstandard tokens and their frequencies in"
P12-1109,W09-1119,0,0.00421153,"letter transformation process. We notice that in creating the nonstandard tokens, humans tend to drop certain letter units from the word or replace them with other letters. For example, in abbreviating “advertisements” to “ads”, humans may first break the word into smaller units “ad-ver-tise-ment-s”, then drop the middle parts. This also conforms with the word construction theory where a word is composed of smaller units and construction rules. Based on this assumption, we decompose the dictionary words on the phoneme-, syllable-, morpheme-, and word-level5 and use the “BILOU” tagging scheme (Ratinov and Roth, 2009) to represent the unit boundary, where “BILOU” stands for B(egin), I(nside), L(ast), O(utside), and U(nit-length) of the corresponding unit6 . Example “BILOU” boundary tags were shown in Table 2. On top of the boundary tags, we develop a set of conjunction features to accurately pinpoint the current character position. We consider conjunction features formed by concatenating character position in syllable and current syllable position in the word (e.g., conjunction feature “L B” for the letter “d” in Table 2). A similar set of features are also developed on morpheme level. We consider conjunct"
P12-1109,D11-1141,0,0.0407864,"Missing"
P12-1109,N10-2012,0,0.0192054,"Missing"
P12-1109,W11-2210,0,\N,Missing
P12-1109,N09-2069,0,\N,Missing
P12-1109,N03-1028,0,\N,Missing
P12-1109,I05-3027,0,\N,Missing
P12-1109,W09-2010,0,\N,Missing
P14-2099,N04-1015,0,0.012771,"licy sections. While we expect that different kinds of websites will likely address different privacy issues, we believe that many policies will discuss roughly the same set of issues. Aligning the policies is a first step in a larger effort to (i) automatically analyze policies to make them less opaque to users and (ii) support legal experts who wish to characterize the state of privacy online and make recommendations (Costante et al., 2012; Ammar et al., 2012; Costante et al., 2013). We are inspired by multiple sequence alignment methods in computational biology (Durbin et al., 1998) and by Barzilay and Lee (2004), who described a hidden Markov model (HMM) for document content where each state corresponds to a distinct topic and generates sentences relevant to that topic according to a language model. We estimate an HMM-like model on our corpus, exploiting similarity across privacy policies to the extent it is evident in the data. In our formulation, each hidden state corresponds to an issue or topic, characterized by a distribution over words and bigrams appearing in privacy policy sections addressing that issue. The transition distribution captures tendencies of privacy policy authors to organize the"
P18-2045,W17-0908,0,0.0287999,"Missing"
P18-2045,P14-5010,0,0.00454304,"ect: lij = 1; otherwise lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision 280 of each chain by adding the second term. More formally, the model is trained to minimise the loss: L = XEntropy(y, yˆ) + α X XEntropy(lij , gij ) i,j where yˆ and gij are defined in Equations 7 and 4 respective"
P18-2045,D17-1168,0,0.413973,"erent Ending: Sam was happy. Story comprehension requires a deep semantic understanding of the narrative, making it a challenging task. Inspired by previous studies on ROC Story Cloze Test, we propose a novel method, tracking various semantic aspects with external neural memory chains while encouraging each to focus on a particular semantic aspect. Evaluated on the task of story ending prediction, our model demonstrates superior performance to a collection of competitive baselines, setting a new state of the art. 1 1 Figure 1: Story Cloze Test example. The current state-of-the-art approach of Chaturvedi et al. (2017) is based on understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. Chaturvedi et al. (2017) adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial in"
P18-2045,D14-1082,0,0.00766782,"e lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss of the final prediction of right/wrong endings, we also take into account the memory update gate supervision 280 of each chain by adding the second term. More formally, the model is trained to minimise the loss: L = XEntropy(y, yˆ) + α X XEntropy(lij , gij ) i,j where yˆ and gij are defined in Equations 7 and 4 respectively, y is the gold label f"
P18-2045,W17-0913,0,0.06733,"he necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by Henaff et al. (2017) for reasoning-focused question answering and clozestyle reading comprehensio"
P18-2045,N10-1138,0,0.0149388,"upervise the memory update gates of these chains, we design three sequences of binary labels: lj = {l1j , l2j , . . . , lTj } for j ∈ [1, 3] representing event, sentiment, and topic, and lij ∈ {0, 1}. The label at time i for the j-th aspect is only assigned a value of 1 if the word is a trigger for that particular aspect: lij = 1; otherwise lij = 0. During training, we utilise such sequences of binary labels to supervise the memory update gate activations of each chain. Specifically, each chain is encouraged Event sequence. We parse each sentence into its FrameNet representation with SEMAFOR (Das et al., 2010), and identify each frame target (word or phrase tokens evoking a frame). Sentiment trajectory. Following Chaturvedi et al. (2017), we utilise a pre-compiled list of sentiment words (Liu et al., 2005). To take negation into account, we parse each sentence with the Stanford Core NLP dependency parser (Manning et al., 2014; Chen and Manning, 2014) and include negation words as trigger words. Topical consistency. We process each sentence with the Stanford Core NLP POS tagger and identify nouns and verbs, following Chaturvedi et al. (2017). 2.3 Training Loss In addition to the cross entropy loss o"
P18-2045,N16-1098,0,0.31157,"he classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial intelligence (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpor"
P18-2045,W17-0906,0,0.06849,"ves, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstrated by Henaff et al. (2017) for reasoning-focused question answering and clozestyle reading comprehension, we introduce a novel multi-task learning objec"
P18-2045,P16-1028,0,0.0303386,"Missing"
P18-2045,K17-1004,0,0.339321,"e (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al., 2017; Chaturvedi et al., 2017), or representation learning via deep learning models (Mihaylov and Frank, 2017; Bugert et al., 2017; Mostafazadeh et al., 2017). Another widely used component of competitive systems is language model-based features, which require training on large corpora in the story domain. Inspired by the argument for tracking the dynamics of events, sentiment and topic, we propose to address the issues identified above with multiple external memory chains, each responsible for a single aspect. Building on Recurrent Entity Networks (EntNets), a superior framework to LSTMs demonstr"
P18-2045,H89-1033,0,0.701499,"understanding the context from three perspectives: (1) event sequence, (2) sentiment trajectory, and (3) topic consistency. Chaturvedi et al. (2017) adopt external tools to recognise relevant aspect-triggering words, and manually design features to incorporate them into the classifier. While identifying triggers has been made easy by the use of various linguistic resources, crafting such features is time consuming and requires domain-specific knowledge along with repeated experimentation. Introduction Story narrative comprehension has been a longstanding challenge in artificial intelligence (Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). The difficulties of this task arise from the necessity for understanding not only narratives, but also commonsense and normative social behaviour (Charniak, 1972). Of particular interest in this paper is the work by Mostafazadeh et al. (2016) on understanding commonsense stories in the form of a Story Cloze Test: given a short story, we must predict the most coherent sentential ending from two options (e.g. see Figure 1). Many attempts have been made to solve this problem, based either on linear classifiers with handcrafted features (Schwartz et al.,"
P18-3015,P06-1039,0,0.153101,"Missing"
P18-3015,W09-1802,0,0.534906,"summaries remain faithful to the original. Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization (Cao et al., 2018; Song and Liu, 2018). This study thus chooses to focus on neural extractive summarization. Introduction We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary (Nenkova and McKeown, 2011). Existing supervised approaches to extractive summarization frequently use human abstracts to create annotations for extraction units (Gillick and Favre, 2009; Li et al., 2013; Cheng and Lapata, 2016). E.g., a source word is labelled 1 if it appears in the abstract, 0 otherwise. Despite the usefulness, there are two issues with this scheme. First, a vast majority of the source words are tagged 0s, only a small portion are 1s. This is due to the fact that human abstracts are short and concise; they often contain words not present in the source. Second, We explore a new training paradigm for extractive summarization. We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the abstract with"
P18-3015,N18-2081,0,0.0145962,"they often contain words not present in the source. Second, We explore a new training paradigm for extractive summarization. We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the abstract with a blank, and the answer is an entity or a keyword. Table 1 shows an example. Because the 105 Proceedings of ACL 2018, Student Research Workshop, pages 105–111 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing dialog generation (Li et al., 2017), machine translation (MT) (Ranzato et al., 2016; Gu et al., 2018), question answering (Choi et al., 2017), and summarization and sentence simplification (Zhang and Lapata, 2017; Paulus et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018). This study leverages RL to explore the space of possible extractive summaries. The summaries are encouraged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts. questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source content that is relevant to the questions (≈ human abstract) such tha"
P18-3015,P16-1223,0,0.076354,"Missing"
P18-3015,D16-1011,0,0.519812,"s are encouraged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts. questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source content that is relevant to the questions (≈ human abstract) such that the summaries can work as a document surrogate to predict correct answers. We use an attention mechanism to locate segments of a summary that are relevant to a given question so that the summary can be used to answer multiple questions. This study extends the work of (Lei et al., 2016) to use reinforcement learning to explore the space of extractive summaries. While the original work focuses on generating rationales to support supervised classification, the goal of our study is to produce fluent, generic document summaries. The question-answering (QA) task is designed to fulfill this goal and the QA performance is only secondary. Our research contributions can be summarized as follows: 3 Given a source document X, our system generates a summary Y = (y1 , y2 , · · · , y|Y |) by identifying consecutive sequences of words: yt is 1 if the t-th source word is included in the sum"
P18-3015,D13-1047,1,0.907472,"Missing"
P18-3015,D17-1230,0,0.0239791,"his is due to the fact that human abstracts are short and concise; they often contain words not present in the source. Second, We explore a new training paradigm for extractive summarization. We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the abstract with a blank, and the answer is an entity or a keyword. Table 1 shows an example. Because the 105 Proceedings of ACL 2018, Student Research Workshop, pages 105–111 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing dialog generation (Li et al., 2017), machine translation (MT) (Ranzato et al., 2016; Gu et al., 2018), question answering (Choi et al., 2017), and summarization and sentence simplification (Zhang and Lapata, 2017; Paulus et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018). This study leverages RL to explore the space of possible extractive summaries. The summaries are encouraged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts. questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source con"
P18-3015,D13-1020,0,0.0392473,"is used to predict the answer. We define the QA reward Ra (Y ) as the log-likelihood of correctly predictRelated Work This study focuses on generic summarization. It is different from the query-based summarization (Daum´e III and Marcu, 2006; Dang and Owczarzak, 2008), where systems are trained to select text pieces related to predefined queries. In this work we have no predefined queries but the system carefully generates questions from human abstracts and learns to produce generic summaries that are capable of answering all questions. Cloze questions have been used in reading comprehension (Richardson et al., 2013; Weston et al., 2016; Mostafazadeh et al., 2016; Rajpurkar et al., 2016) to test the system’s ability to perform reasoning and language understanding. Hermann et al. (2015) describe an approach to extract (context, question, answer) triples from news articles. Our work draws on this approach to automatically create questions from human abstracts. Reinforcement learning (RL) has been recently applied to a number of NLP applications, includ106 Document Extractive Summary 1 Bavarians have 0 been 0 second 1 most 1 valuable 1 Answers Questions q1 c1 @entity1 Germany @entity1: “Bayern Munich” @enti"
P18-3015,W04-1013,0,0.116282,"Missing"
P18-3015,D15-1044,0,0.0773644,"the top sentence of the abstract, and system-generated Cloze-style questions. Source content related to the abstract is italicized. not all labels are accurate. Source words that are labelled 0 may be paraphrases, generalizations, or otherwise related to words in the abstracts. These source words are often mislabelled. Consequently, leveraging human abstracts to provide supervision for extractive summarization remains a challenge. Neural abstractive summarization can alleviate this issue by allowing the system to either copy words from the source texts or generate new words from a vocabulary (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). While the techniques are promising, they face other challenges, such as ensuring the summaries remain faithful to the original. Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization (Cao et al., 2018; Song and Liu, 2018). This study thus chooses to focus on neural extractive summarization. Introduction We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary (Nenkova and McKeown, 2011). Exist"
P18-3015,W04-3252,0,0.0202939,"and we describe their parameter tuning in §4. R(Y )=Ra(Y )+γRb(Y )−αRf (Y )−βRs(Y ) (5) 3.2 In the following we seek to optimize a policy P (Y |X) for generating extractive summaries so that the expected reward EP (Y |X) [R(Y )] is maximized. Taking derivatives of this objective with respect to model parameters θ involves repeatedly sampling summaries Yˆ = (ˆ y1 , yˆ2 , · · · , yˆ|Y |) (illustrated in Eq. (6)). In this way reinforcement learning exploits the space of extractive summaries of a source document. R-2 R-L LSA (Steinberger and Jezek, 2004) LexRank (Erkan and Radev, 2004) TextRank (Mihalcea and Tarau, 2004) SumBasic (Vanderwende et al., 2007) KL-Sum (Haghighi and Vanderwende, 2009) Distraction-M3 (Chen et al., 2016b) Seq2Seq w/ Attn (See et al., 2017) Pointer-Gen w/ Cov (See et al., 2017) Graph-based Attn (Tan et al., 2017) 21.2 26.1 23.3 22.9 20.7 27.1 25.0 29.9 30.3 6.2 9.6 7.7 5.5 5.9 8.2 7.7 10.9 9.8 14.0 17.7 15.8 14.8 13.7 18.7 18.8 21.1 20.0 Extr+EntityQ (this paper) Extr+KeywordQ (this paper) 31.4 31.7 11.5 11.6 21.7 21.5 where we set yt∗ as 1 if (xt , xt+1 ) is a bigram in the human abstract. For reinforcement learning, our goal is to optimize the policy P (Y |X) using the reward functi"
P18-3015,P17-1099,0,0.442303,"m-generated Cloze-style questions. Source content related to the abstract is italicized. not all labels are accurate. Source words that are labelled 0 may be paraphrases, generalizations, or otherwise related to words in the abstracts. These source words are often mislabelled. Consequently, leveraging human abstracts to provide supervision for extractive summarization remains a challenge. Neural abstractive summarization can alleviate this issue by allowing the system to either copy words from the source texts or generate new words from a vocabulary (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). While the techniques are promising, they face other challenges, such as ensuring the summaries remain faithful to the original. Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization (Cao et al., 2018; Song and Liu, 2018). This study thus chooses to focus on neural extractive summarization. Introduction We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary (Nenkova and McKeown, 2011). Existing supervised approaches to extractive sum"
P18-3015,N16-1098,0,0.0283783,"A reward Ra (Y ) as the log-likelihood of correctly predictRelated Work This study focuses on generic summarization. It is different from the query-based summarization (Daum´e III and Marcu, 2006; Dang and Owczarzak, 2008), where systems are trained to select text pieces related to predefined queries. In this work we have no predefined queries but the system carefully generates questions from human abstracts and learns to produce generic summaries that are capable of answering all questions. Cloze questions have been used in reading comprehension (Richardson et al., 2013; Weston et al., 2016; Mostafazadeh et al., 2016; Rajpurkar et al., 2016) to test the system’s ability to perform reasoning and language understanding. Hermann et al. (2015) describe an approach to extract (context, question, answer) triples from news articles. Our work draws on this approach to automatically create questions from human abstracts. Reinforcement learning (RL) has been recently applied to a number of NLP applications, includ106 Document Extractive Summary 1 Bavarians have 0 been 0 second 1 most 1 valuable 1 Answers Questions q1 c1 @entity1 Germany @entity1: “Bayern Munich” @entity2: “Manchester United” ‘s @placeholder second"
P18-3015,C18-1146,1,0.855691,"tly, leveraging human abstracts to provide supervision for extractive summarization remains a challenge. Neural abstractive summarization can alleviate this issue by allowing the system to either copy words from the source texts or generate new words from a vocabulary (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). While the techniques are promising, they face other challenges, such as ensuring the summaries remain faithful to the original. Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization (Cao et al., 2018; Song and Liu, 2018). This study thus chooses to focus on neural extractive summarization. Introduction We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary (Nenkova and McKeown, 2011). Existing supervised approaches to extractive summarization frequently use human abstracts to create annotations for extraction units (Gillick and Favre, 2009; Li et al., 2013; Cheng and Lapata, 2016). E.g., a source word is labelled 1 if it appears in the abstract, 0 otherwise. Despite the usefulness, there are two issues with this sch"
P18-3015,K16-1028,0,0.244,"the abstract, and system-generated Cloze-style questions. Source content related to the abstract is italicized. not all labels are accurate. Source words that are labelled 0 may be paraphrases, generalizations, or otherwise related to words in the abstracts. These source words are often mislabelled. Consequently, leveraging human abstracts to provide supervision for extractive summarization remains a challenge. Neural abstractive summarization can alleviate this issue by allowing the system to either copy words from the source texts or generate new words from a vocabulary (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). While the techniques are promising, they face other challenges, such as ensuring the summaries remain faithful to the original. Failing to reproduce factual details has been revealed as one of the main obstacles for neural abstractive summarization (Cao et al., 2018; Song and Liu, 2018). This study thus chooses to focus on neural extractive summarization. Introduction We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary (Nenkova and McKeown, 2011). Existing supervised approache"
P18-3015,N18-1158,0,0.091226,"yle comprehension questions, where the question body is a sentence of the abstract with a blank, and the answer is an entity or a keyword. Table 1 shows an example. Because the 105 Proceedings of ACL 2018, Student Research Workshop, pages 105–111 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing dialog generation (Li et al., 2017), machine translation (MT) (Ranzato et al., 2016; Gu et al., 2018), question answering (Choi et al., 2017), and summarization and sentence simplification (Zhang and Lapata, 2017; Paulus et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018). This study leverages RL to explore the space of possible extractive summaries. The summaries are encouraged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts. questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source content that is relevant to the questions (≈ human abstract) such that the summaries can work as a document surrogate to predict correct answers. We use an attention mechanism to locate segments of a summary that are relevant to a given question s"
P18-3015,P17-1108,0,0.0120591,"is maximized. Taking derivatives of this objective with respect to model parameters θ involves repeatedly sampling summaries Yˆ = (ˆ y1 , yˆ2 , · · · , yˆ|Y |) (illustrated in Eq. (6)). In this way reinforcement learning exploits the space of extractive summaries of a source document. R-2 R-L LSA (Steinberger and Jezek, 2004) LexRank (Erkan and Radev, 2004) TextRank (Mihalcea and Tarau, 2004) SumBasic (Vanderwende et al., 2007) KL-Sum (Haghighi and Vanderwende, 2009) Distraction-M3 (Chen et al., 2016b) Seq2Seq w/ Attn (See et al., 2017) Pointer-Gen w/ Cov (See et al., 2017) Graph-based Attn (Tan et al., 2017) 21.2 26.1 23.3 22.9 20.7 27.1 25.0 29.9 30.3 6.2 9.6 7.7 5.5 5.9 8.2 7.7 10.9 9.8 14.0 17.7 15.8 14.8 13.7 18.7 18.8 21.1 20.0 Extr+EntityQ (this paper) Extr+KeywordQ (this paper) 31.4 31.7 11.5 11.6 21.7 21.5 where we set yt∗ as 1 if (xt , xt+1 ) is a bigram in the human abstract. For reinforcement learning, our goal is to optimize the policy P (Y |X) using the reward function R(Y ) (§3.1) during the training process. Once the policy P (Y |X) is learned, we do not need the reward function (or any QA pairs) at test time to generate generic summaries. Instead we choose yˆt that yields the high"
P18-3015,D14-1162,0,0.0855817,", X) (Eq. 7). 4.1 Hyperparameters The hyperparameters, tuned on the validation set, include the following: the hidden state size of the Bi-LSTM is 256; the hidden state size of the single-direction LSTM encoder is 30. Dropout rate (Srivastava, 2013), used twice in the sampling component, is set to 0.2. The minibatch size is set to 256. We apply early stopping on the validation set, where the maximum number of epochs is set to 50. Our source vocabulary contains 150K words; words not in the vocabulary are replaced by the hunki token. We use 100-dimensional word embeddings, initialized by GloVe (Pennington et al., 2014) and remain trainable. We set β = 2α and select the best α ∈ {10, 20, 50} and γ ∈ {5, 6, 7, 8} using the valid set (best value underlined). The maximum length of input is set to 100 words; δ is set to be 0.4 (≈40 words). We use the Adam optimizer (Kingma and Ba, 2015) with h P (yt |ˆ y1:t−1 , X) = σ(Wh [hD t ||st−1 ] + b ) (7) st = LSTM([hD yt ], st−1 ) t ||ˆ Q |Y | P (Yˆ |X) = t=1 P (ˆ yt |ˆ y1:t−1 , X) R-1 Table 2: Results on the CNN test set (full-length F1 scores). Reinforcement Learning = EP (Y |X) [R(Y )∇θ log P (Y |X)] P ˆ (n) )∇θ log P (Yˆ (n) |X) ≈ N1 N n=1 R(Y System (8) (9) Note tha"
P18-3015,D17-1062,0,0.0262614,"ctive summarization. We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the abstract with a blank, and the answer is an entity or a keyword. Table 1 shows an example. Because the 105 Proceedings of ACL 2018, Student Research Workshop, pages 105–111 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing dialog generation (Li et al., 2017), machine translation (MT) (Ranzato et al., 2016; Gu et al., 2018), question answering (Choi et al., 2017), and summarization and sentence simplification (Zhang and Lapata, 2017; Paulus et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018). This study leverages RL to explore the space of possible extractive summaries. The summaries are encouraged to preserve salient source content useful for answering questions as well as sharing common words with the abstracts. questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source content that is relevant to the questions (≈ human abstract) such that the summaries can work as a document surrogate to predict correct answers. We use an attention mechanism to l"
P18-3015,N09-1041,0,\N,Missing
P18-3015,W01-0100,0,\N,Missing
P18-3015,P16-1046,0,\N,Missing
P18-3015,D16-1264,0,\N,Missing
P18-3015,P18-1063,0,\N,Missing
P19-1098,P11-1049,0,0.216394,"Missing"
P19-1098,D15-1075,0,0.429961,"ontain redundant information based on both surface word form and their underlying semantics. E.g., the two sentences “Snowstorm slams eastern US on Friday” and “A strong wintry storm was dumping snow in eastern US after creating traffic havoc that claimed at least eight lives” are considered similar because they carry redundant information and cannot both be included in the summary. These sentences are by no means semantically equivalent, nor do they exhibit a clear entailment relationship. The task thus should be distinguished from similar tasks such as predicting natural language inference (Bowman et al., 2015; Williams et al., 2018) or semantic textual similarity (Cer et al., 2017). In this work, we describe a novel method to collect a large amount of sentence pairs that are deemed similar for summarization purpose. We contrast this new dataset with those used for textual entailment for modeling sentence similarity and demonstrate its effectiveness on discriminating sentences and generating diverse summaries. The contributions of this work can be summarized as follows: • we present a novel method inspired by the determinantal point process for multi-document summarization. The method includes a di"
P19-1098,N18-1150,0,0.0334793,"ur code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the reliability. These models are equipped with the capability of generating new words not present in the source. With greater freedom of lexical choices, the system summaries can contain inaccurate factual details and falsified content that prevent them from staying “true-to-original.” In this paper"
P19-1098,S17-2001,0,0.473148,"lying semantics. E.g., the two sentences “Snowstorm slams eastern US on Friday” and “A strong wintry storm was dumping snow in eastern US after creating traffic havoc that claimed at least eight lives” are considered similar because they carry redundant information and cannot both be included in the summary. These sentences are by no means semantically equivalent, nor do they exhibit a clear entailment relationship. The task thus should be distinguished from similar tasks such as predicting natural language inference (Bowman et al., 2015; Williams et al., 2018) or semantic textual similarity (Cer et al., 2017). In this work, we describe a novel method to collect a large amount of sentence pairs that are deemed similar for summarization purpose. We contrast this new dataset with those used for textual entailment for modeling sentence similarity and demonstrate its effectiveness on discriminating sentences and generating diverse summaries. The contributions of this work can be summarized as follows: • we present a novel method inspired by the determinantal point process for multi-document summarization. The method includes a diversity measure assessing the redundancy between sentences, and a quality"
P19-1098,P18-1224,0,0.0185765,"a nonlinear activation function (e.g., ReLU); {Wu , bu } are model parameters. uai,k = f (Wu xai:i+k−1 + bu ) (5) We use uai ∈ RD to denote the concatenation of local features generated using various filter sizes. Following Kim et al. (2014), we employ filter sizes k ∈ {3, 4, 5, 6, 7} with an equal number of filters (d) for each size (D = 5d). After applying maxpooling to local features of all positions, we obtain a representation ua = max-pooling(uai ) ∈ RD for sentence a; and similarly we obtain ub ∈ RD for sentence b. It is not uncommon for state-ofthe-art sentence similarity classifiers (Chen et al., 2018) to concatenate the two sentence vectors, their absolute difference and element-wise product [ua ; ub ; |ua − ub |; ua ◦ ub ], and feed this representation to a fully connected layer to predict if two sentences are similar. 1030 1 L 7 5d d 1 256 L 6 5 E 5d 3 M d L 4 1 50K LSTM-256 d E 1 1 B L 1 256 5d d Capsnet 1 4× 5d + 2L + B 1 100 1 1 1 1 d L 5d L L 1 5d Figure 2: The system architecture utilizing CapsNet for predicting sentence similarity. denotes the inputs and the convolutional layer; max-pooling layer; fully-connected layer; and ReLU intermediate outputs; activation. Nevertheless, we co"
P19-1098,P18-1063,0,0.106425,"Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the reliability. These models are equipped with"
P19-1098,P16-1046,0,0.0749064,"vre, 2009), determinantal point processes (Kulesza and Taskar, 2012), submodular functions (Lin and Bilmes, 2010), and minimum dominating set (Shen and Li, 2010). In this paper we employ the DPP framework because of its remarkable performance on various summarization problems (Zhang et al., 2016). Recent years have also seen considerable interest in neural approaches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018). These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are p"
P19-1098,P06-1039,0,0.350062,"Missing"
P19-1098,R15-1020,0,0.0244559,"ors containing redundant yet lexically diverse expressions.1 1 Introduction Multi-document summarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014; Yang et al., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry"
P19-1098,P16-1188,0,0.0598015,"(Carbonell and Goldstein, 1998; Daum´e III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015). These approaches focus on identifying representative sentences from a single document or set of documents to form a summary. The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are used to indicate whether their corresponding source sentences are to be included in the summary. The summary sentences are selected to maximize th"
P19-1098,D15-1042,0,0.0232609,"real-world applications (Carbonell and Goldstein, 1998; Daum´e III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015). These approaches focus on identifying representative sentences from a single document or set of documents to form a summary. The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are used to indicate whether their corresponding source sentences are to be included in the summary. The summary sentences are"
P19-1098,N10-1131,0,0.0609357,"ng redundancy between sentences; and this notion of similarity is different from that of entailment and semantic textual similarity (STS); • our findings suggest that effectively modeling pairwise sentence similarity is crucial for increasing summary diversity and boosting summarization performance. Our DPP system with improved similarity measure performs competitively, outperforming strong summarization baselines on benchmark datasets. 2 Related Work Extractive summarization approaches are the most popular in real-world applications (Carbonell and Goldstein, 1998; Daum´e III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015). These approaches focus on identifying representative sentences from a single document or set of documents to form a summary. The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-su"
P19-1098,D13-1158,0,0.0261817,"ary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are used to indicate whether their corresponding source sentences are to be included in the summary. The summary sentences are selected to maximize the coverage of important source content, while minimizing the summary redundancy and subject to a length constraint. The optimization can be performed using an off-the-shelf tool such as Gurobi, IBM CPLEX, or via a greedy approximation algorithm. Notable optimization frameworks include integer linear 1028 programming (Gillick and Favre, 2009), determinantal point processes (Kulesza and Taskar, 2012)"
P19-1098,hong-etal-2014-repository,0,0.553575,"ummarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014; Yang et al., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarizati"
P19-1098,C10-1039,0,0.622729,"Missing"
P19-1098,D18-1443,0,0.0987652,"Missing"
P19-1098,D18-1208,0,0.0505213,"ches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018). These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are performed for single document summarization. Extracting summary-worthy sentences from the source documents is important even if the ultimate goal is to generate abstracts. Recent abstractive studies recognize the importance of separating “salience estimation” from “text generation” so as to reduce the amount of training data required by encoder-decoder models (G"
P19-1098,D14-1168,0,0.0134832,"arization baselines on benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.1 1 Introduction Multi-document summarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014; Yang et al., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decod"
P19-1098,W09-1802,0,0.756338,"iginal.” In this paper we instead focus on an extractive method exploiting the determinantal point process (DPP; Kulesza and Taskar, 2012) for multidocument summarization. DPP can be trained on small data, and because extractive summaries are free from manipulation, they largely remain true to the original. DPP selects a set of most representative sentences from the given source documents to form a summary, while maintaining high diversity among summary sentences. It is one of a family of optimization-based summarization methods that performed strongest in previous summarization competitions (Gillick and Favre, 2009; Lin and Bilmes, 2010; Kulesza and Taskar, 2011). Diversity is an integral part of the DPP model. It is modelled by pairwise repulsion between sentences. In this paper we exploit the capsule net1027 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1027–1038 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics works (Hinton et al., 2018) to measure pairwise sentence (dis)similarity, then leverage DPP to obtain a set of diverse summary sentences. Traditionally, the DPP method computes similarity scores based on"
P19-1098,P18-1064,0,0.0232694,"t in neural approaches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018). These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are performed for single document summarization. Extracting summary-worthy sentences from the source documents is important even if the ultimate goal is to generate abstracts. Recent abstractive studies recognize the importance of separating “salience estimation” from “text generation” so as to reduce the amount of training data required by enc"
P19-1098,N09-1041,0,0.593195,"Missing"
P19-1098,D18-1207,0,0.0203053,"cly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the reliability. These models are equipped with the capability of generating new words not present in the source. With greater freedom of lexical choices, the system summaries can contain inaccurate factual details and falsified content that prevent them from staying “true-to-original.” In this paper we instead focus on an ext"
P19-1098,P19-1209,1,0.870652,"Missing"
P19-1098,D18-1446,1,0.808222,"however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are performed for single document summarization. Extracting summary-worthy sentences from the source documents is important even if the ultimate goal is to generate abstracts. Recent abstractive studies recognize the importance of separating “salience estimation” from “text generation” so as to reduce the amount of training data required by encoder-decoder models (Gehrmann et al., 2018; Lebanoff et al., 2018, 2019). An extractive method is often leveraged to identify salient source sentences, then a neural text generator rewrites the selected sentences into an abstract. Our pursuit of the DPP method is especially meaningful in this context. As described in the next section, DPP has an extraordinary ability to distinguish redundant descriptions, thereby avoiding passing redundant content to the abstractor that can cause an encoderdecoder model to fail. 3 The DPP Framework Let Y = {1, 2, · · · , N} be a ground set containing N items, corresponding to all sentences of the source documents. Our goal"
P19-1098,D13-1047,1,0.929436,"Missing"
P19-1098,W04-1013,0,0.199471,"mans create summaries using generalization, paraphrasing, and other high-level text operations, a summary sentence and its source sentence can be semantically similar, yet contain diverse expressions. Fortunately, such source/summary sentence pairs can be conveniently derived from singledocument summarization data. We analyze the CNN/Daily Mail dataset (Hermann et al., 2015) that contains a massive collection of single news articles and their human-written summaries. For each summary sentence, we identify its most similar source sentence by calculating the averaged R-1, R-2, and R-L F-scores (Lin, 2004) between a source and summary sentences. We consider a summary sentence to have no match if the score is lower than a threshold. We obtain negative examples by randomly sampling two sentences from a news article. In total, our training / dev / test sets contain 2,084,798 / 105,936 / 86,144 sentence R-1 27.07 28.90 31.43 29.48 31.04 34.44 35.49 37.31 38.10 38.25 39.35 DUC-04 R-2 R-SU4 5.03 5.33 6.03 4.25 6.03 7.11 7.80 9.36 9.14 9.22 10.14 8.63 8.76 10.01 8.64 10.23 11.19 12.02 13.12 13.40 13.40 14.15 Table 1: ROUGE results on DUC-04. † indicates our reimplementation of Kulesza and Taskar (2011"
P19-1098,N10-1134,0,0.453632,"instead focus on an extractive method exploiting the determinantal point process (DPP; Kulesza and Taskar, 2012) for multidocument summarization. DPP can be trained on small data, and because extractive summaries are free from manipulation, they largely remain true to the original. DPP selects a set of most representative sentences from the given source documents to form a summary, while maintaining high diversity among summary sentences. It is one of a family of optimization-based summarization methods that performed strongest in previous summarization competitions (Gillick and Favre, 2009; Lin and Bilmes, 2010; Kulesza and Taskar, 2011). Diversity is an integral part of the DPP model. It is modelled by pairwise repulsion between sentences. In this paper we exploit the capsule net1027 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1027–1038 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics works (Hinton et al., 2018) to measure pairwise sentence (dis)similarity, then leverage DPP to obtain a set of diverse summary sentences. Traditionally, the DPP method computes similarity scores based on the bag-of-words repr"
P19-1098,D15-1227,0,0.0249117,"larly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.1 1 Introduction Multi-document summarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014; Yang et al., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017;"
P19-1098,N16-1010,1,0.855757,"ummarizing documents created by multiple authors containing redundant yet lexically diverse expressions.1 1 Introduction Multi-document summarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014; Yang et al., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2"
P19-1098,W09-1801,0,0.0832824,"g summarization baselines on benchmark datasets. 2 Related Work Extractive summarization approaches are the most popular in real-world applications (Carbonell and Goldstein, 1998; Daum´e III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015). These approaches focus on identifying representative sentences from a single document or set of documents to form a summary. The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are"
P19-1098,N18-1158,0,0.123519,"ubmodular functions (Lin and Bilmes, 2010), and minimum dominating set (Shen and Li, 2010). In this paper we employ the DPP framework because of its remarkable performance on various summarization problems (Zhang et al., 2016). Recent years have also seen considerable interest in neural approaches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018). These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are performed for single document summarization. Extracting summary-worthy"
P19-1098,D15-1044,0,0.0846098,"l., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive su"
P19-1098,P17-1099,0,0.778904,"survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the"
P19-1098,C10-1111,0,0.192991,"er their corresponding source sentences are to be included in the summary. The summary sentences are selected to maximize the coverage of important source content, while minimizing the summary redundancy and subject to a length constraint. The optimization can be performed using an off-the-shelf tool such as Gurobi, IBM CPLEX, or via a greedy approximation algorithm. Notable optimization frameworks include integer linear 1028 programming (Gillick and Favre, 2009), determinantal point processes (Kulesza and Taskar, 2012), submodular functions (Lin and Bilmes, 2010), and minimum dominating set (Shen and Li, 2010). In this paper we employ the DPP framework because of its remarkable performance on various summarization problems (Zhang et al., 2016). Recent years have also seen considerable interest in neural approaches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to co"
P19-1098,C18-1146,1,0.949724,"ies for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the reliability. These models are equipped with the capability of generating new words not present in the source. With greater freedom of lexical choices, the system summaries can contain inaccurate factual details and falsified content that prevent them from staying “true-to-original.” In this paper we instead focus on an extractive method exploiting the determinantal point process (DPP; Kulesza and Taskar, 2012) for multidocument"
P19-1098,E09-1089,0,0.0239853,"as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are used to indicate whether their corresponding source sentences are to be included in the summary. The summary sentences are selected to maximize the coverage of important source content, while minimizing the summary redundancy and subject to a length constraint. The optimization can be performed using an off-the-shelf tool such as Gurobi, IBM CPLEX, or via a greedy approximation algorithm. Notable optimization frameworks include integer linear 1028 programming (Gillick and Favre, 2009), determinantal"
P19-1098,P17-1108,0,0.0322345,"nsiderable interest in neural approaches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018). These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are performed for single document summarization. Extracting summary-worthy sentences from the source documents is important even if the ultimate goal is to generate abstracts. Recent abstractive studies recognize the importance of separating “salience estimation” from “text generation” so as to reduce the amount of training da"
P19-1098,W13-3508,0,0.0218746,"ted Work Extractive summarization approaches are the most popular in real-world applications (Carbonell and Goldstein, 1998; Daum´e III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015). These approaches focus on identifying representative sentences from a single document or set of documents to form a summary. The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are used to indicate whether their corresponding source sente"
P19-1098,P13-1136,0,0.0334246,"zation approaches are the most popular in real-world applications (Carbonell and Goldstein, 1998; Daum´e III and Marcu, 2006; Galanis and Androutsopoulos, 2010; Hong et al., 2014; Yogatama et al., 2015). These approaches focus on identifying representative sentences from a single document or set of documents to form a summary. The summary sentences can be optionally compressed to remove unimportant constituents such as prepositional phrases to yield a succinct summary (Knight and Marcu, 2002; Zajic et al., 2007; Martins and Smith, 2009; BergKirkpatrick et al., 2011; Thadani and McKeown, 2013; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015; Durrett et al., 2016). Extractive summarization methods are mostly unsupervised or lightly-supervised using thousands of training examples. Given its practical importance, we explore an extractive method in this work for multi-document summarization. It is not uncommon to cast summarization as a discrete optimization problem (Gillick and Favre, 2009; Takamura and Okumura, 2009; Lin and Bilmes, 2010; Hirao et al., 2013). In this formulation, a set of binary variables are used to indicate whether their corresponding source sentences are to be incl"
P19-1098,N18-1101,0,0.0286884,"rmation based on both surface word form and their underlying semantics. E.g., the two sentences “Snowstorm slams eastern US on Friday” and “A strong wintry storm was dumping snow in eastern US after creating traffic havoc that claimed at least eight lives” are considered similar because they carry redundant information and cannot both be included in the summary. These sentences are by no means semantically equivalent, nor do they exhibit a clear entailment relationship. The task thus should be distinguished from similar tasks such as predicting natural language inference (Bowman et al., 2015; Williams et al., 2018) or semantic textual similarity (Cer et al., 2017). In this work, we describe a novel method to collect a large amount of sentence pairs that are deemed similar for summarization purpose. We contrast this new dataset with those used for textual entailment for modeling sentence similarity and demonstrate its effectiveness on discriminating sentences and generating diverse summaries. The contributions of this work can be summarized as follows: • we present a novel method inspired by the determinantal point process for multi-document summarization. The method includes a diversity measure assessin"
P19-1098,C18-1095,0,0.0180039,"n benchmark datasets. Our findings are particularly meaningful for summarizing documents created by multiple authors containing redundant yet lexically diverse expressions.1 1 Introduction Multi-document summarization is arguably one of the most important tools for information aggregation. It seeks to produce a succinct summary from a collection of textual documents created by multiple authors concerning a single topic (Nenkova and McKeown, 2011). The summarization technique has seen growing interest in a broad spectrum of domains that include summarizing product reviews (Gerani et al., 2014; Yang et al., 2018), student survey responses (Luo and Litman, 2015; Luo et al., 2016), forum discussion threads (Ding and Jiang, 2015; Tarnpradab et al., 2017), and news articles about a particular event (Hong et al., 2014). Despite the empirical success, most of the datasets remain small, and the cost of hiring hu1 Our code and data are publicly available at https://github. com/ucfnlp/summarization-dpp-capsnet man annotators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et a"
P19-1098,K17-1045,0,0.0414948,"l point processes (Kulesza and Taskar, 2012), submodular functions (Lin and Bilmes, 2010), and minimum dominating set (Shen and Li, 2010). In this paper we employ the DPP framework because of its remarkable performance on various summarization problems (Zhang et al., 2016). Recent years have also seen considerable interest in neural approaches to summarization. In particular, neural extractive approaches focus on learning vector representations of source sentences; then based on these representations they determine if a source sentence is to be included in the summary (Cheng and Lapata, 2016; Yasunaga et al., 2017; Nallapati et al., 2017; Narayan et al., 2018). Neural abstractive approaches usually include an encoder used to convert the entire source document to a continuous vector, and a decoder for generating an abstract word by word conditioned on the document vector (Paulus et al., 2017; Tan et al., 2017; Guo et al., 2018; Kedzie et al., 2018). These neural models, however, require large training data containing hundreds of thousands to millions of examples, which are still unavailable for the multi-document summarization task. To date, most neural summarization studies are performed for single doc"
P19-1098,D18-1350,0,0.0380573,"level representations (i.e., capsules). We seek to transform them B to high-level capsules {vj }M j=1 ∈ R that characterize the interaction between low-level components. Each low-level capsule ui ∈ RD is multiplied by a linear transformation matrix to dedicate ˆ j|i ∈ RB , to the cona portion of it, denoted by u struction of a high-level capsule j (Eq. (6)); where v } ∈ RD×B are model parameters. To re{Wij duce parameters and prevent overfitting, we further encourage sharing parameters over all lowv = Wv = · · · , and level capsules, yielding W1j 2j the same parameter sharing is described in (Zhao et al., 2018). By computing the weighted sum of ˆ j|i , whose weights cij indicate the strength of inu teraction between a low-level capsule i and a highlevel capsule j, we obtain an (unnormalized) capsule (Eq. (7)); we then apply a nonlinear squash function g(·) to normalize the length the vector to be less than 1, yielding vj ∈ RB . v ˆ j|i = Wij u ui X  ˆ j|i vj = g cij u (6) (7) i Routing (Sabour et al., 2017; Zhao et al., 2019) aims to adjust the interaction weights (cij ) using an iterative, EM-like method. Initially, we set {bij } to be zero for all i and j. Per Eq. (8), ci becomes a uniform distri"
P19-1098,P17-1101,0,0.0351344,"otators to create ground-truth summaries for multi-document inputs can be prohibitive. Impressive progress has been made on neural abstractive summarization using encoder-decoder models (Rush et al., 2015; See et al., 2017; Paulus et al., 2017; Chen and Bansal, 2018). These models, nonetheless, are data-hungry and learn poorly from small datasets, as is often the case with multidocument summarization. To date, studies have primarily focused on single-document summarization (See et al., 2017; Celikyilmaz et al., 2018; Kryscinski et al., 2018) and sentence summarization (Nallapati et al., 2016; Zhou et al., 2017; Cao et al., 2018; Song et al., 2018) in part because parallel training data are abundant and they can be conveniently acquired from the Web. Further, a notable issue with abstractive summarization is the reliability. These models are equipped with the capability of generating new words not present in the source. With greater freedom of lexical choices, the system summaries can contain inaccurate factual details and falsified content that prevent them from staying “true-to-original.” In this paper we instead focus on an extractive method exploiting the determinantal point process (DPP; Kulesz"
P19-1209,P18-1063,0,0.276081,"., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012). Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as t"
P19-1209,C16-1101,0,0.0219898,"thar Abbas said the report “unfounded and malicious” and an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence. Compressed Sentence: Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B co"
P19-1209,P16-1046,0,0.250922,"Missing"
P19-1209,P19-1098,1,0.870684,"Missing"
P19-1209,P02-1057,0,0.244805,"Missing"
P19-1209,W04-1016,0,0.0791209,"Missing"
P19-1209,P16-1188,0,0.0590186,"able 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B contains a reference (“the attack”) and A contains a more complete description for it (“bombing that killed 58”). Sentences A and B each contain certain valuable information, and an appropriate way to merge them exists. As a result, a sente"
P19-1209,C10-1037,0,0.204438,"s been studied in the literature, but there lacks a mechanism to weigh sentence singletons and pairs in a unified space. Extractive methods focus on selecting sentence singletons using greedy (Carbonell and Goldstein, 1998), optimization-based (Gillick and Favre, 2009; Kulesza and Taskar, 2011; Cho et al., 2019), and (non-)autoregressive methods (Cheng and Lapata, 2016; Kedzie et al., 2018). In contrast, existing sentence fusion studies tend to assume ground sets of source sentences are already provided, and the system fuses each set of sentences into a single one (Daum´e III and Marcu, 2004; Filippova, 2010; Thadani and McKeown, 2013). There is thus a crucial gap between sentence selection and fusion to support summarizing by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences. The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175"
P19-1209,D15-1042,0,0.0711279,"le-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only. A sentence can also be generated through fusing multiple source sentences. However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, 2176 Sentence Pair: (A) The bombing killed 58 people. (B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul, former head of the ISI, firmly denied the agency’s involvement in the attack. Merged Sentence: Pakistan denies its spy agency helped pla"
P19-1209,C12-1056,0,0.0226888,"sive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012). Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an articl"
P19-1209,C10-1039,0,0.33218,"Missing"
P19-1209,D18-1443,0,0.220446,"Missing"
P19-1209,W09-1802,0,0.15857,"decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only. A sentence can also be generated through fusing multiple source sentences. However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, 2176 Sentence Pair: (A) The bombing killed 58 people. (B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul, former head of the ISI, firmly denied the agency’s involvement in the"
P19-1209,P18-1064,0,0.0317942,"c words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only"
P19-1209,N09-1041,0,0.119576,"ffectively train on single-document inputs and transfer to the multi-document setting. 5 Results Evaluation Setup In this section we evaluate our proposed methods on identifying summaryworthy instances including singletons and pairs. We compare this scheme with traditional methods extracting only singletons, then introduce novel evaluation strategies to compare results. We exploit several strong extractive baselines: (i) SumBasic (Vanderwende et al., 2007) extracts sentences by assuming words occurring frequently in a document have higher chances of being included in the summary; (ii) KL-Sum (Haghighi and Vanderwende, 2009) greedily adds sentences to the summary to minimize KL divergence; (iii) LexRank (Erkan and Radev, 2004) estimates sentence importance based on eigenvector centrality in a document graph representation. Further, we include the L EAD method that selects the first N sentences from each document. We then require all systems to extract N instances, i.e., either singletons or pairs, from the input document(s).5 We compare system-identified instances with ground-truth instances, and in particular, we compare against the primary, secondary, and full set of ground-truth sentences. A primary sentence i"
P19-1209,E14-1075,0,0.0126762,"and pairs. BERT learns instance representations by attending to important content words, where the importance is signaled by word and position embeddings as well as pairwise word relationships. Nonetheless, it remains an open question whether BERT can successfully weave the meaning of topically important words into representations. A word “border” is topically important if the input document discusses border security. A topic word is likely to be repeatedly mentioned in the input document but less frequently elsewhere. Because sentences containing topical words are often deemed summaryworthy (Hong and Nenkova, 2014), it is desirable to represent sentence singletons and pairs based on the amount of topical content they convey. VSM represents each sentence as a sparse vector. Each dimension of the vector corresponds to an n-gram weighted by its TF-IDF score. A high TF-IDF score suggests the n-gram is important to the topic of discussion. We further strengthen the sentence vector with position and centrality information, i.e., the sentence position in the document and the cosine similarity between the sentence and document vector. We obtain a document vector by averaging over its sentence vectors, and we si"
P19-1209,P18-1013,0,0.0358373,"ng by both compressing single sentences and fusing pairs. This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences. The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to l"
P19-1209,D18-1208,0,0.116907,"Missing"
P19-1209,D18-1446,1,0.933437,"nd fusing pairs. This paper attempts to bridge the gap by ranking singletons and pairs together by their likelihoods of producing summary sentences. The selection of sentence singletons and pairs can bring benefit to neural abstractive summarization, as a number of studies seek to separate content selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to learn to select sentence singletons and pairs, w"
P19-1209,D13-1047,1,0.937752,"Missing"
P19-1209,D17-1222,0,0.0239747,"cy, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and"
P19-1209,C18-1101,1,0.867493,"t “unfounded and malicious” and an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence. Compressed Sentence: Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B contains a reference ("
P19-1209,W04-1013,0,0.299189,"Missing"
P19-1209,W09-1801,0,0.104518,"Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows an example. Sentence B contains a reference (“the attack”) and A contains a more complete description for it (“bombing that killed 58”). Sentences A and B each contain certain valuable inform"
P19-1209,D18-1206,0,0.20014,"ntence singletons and pairs. We perform extensive experiments and report findings on sentence selection and abstraction.1 2 Related Work Content selection is integral to any summarization system. Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 1 We make our code and models publicly available at https: //github.com/ucfnlp/summarization-sing-pair-mix 2016; Chen et al., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features"
P19-1209,D15-1044,0,0.156666,"ng a pair. Compared to abstractive summarizers that perform content selection implicitly, our method is flexible and can be extended to multi-document summarization where training data is limited; • we investigate the factors involved in representing sentence singletons and pairs. We perform extensive experiments and report findings on sentence selection and abstraction.1 2 Related Work Content selection is integral to any summarization system. Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 1 We make our code and models publicly available at https: //github.com/ucfnlp/summarization-sing-pair-mix 2016; Chen et al., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit ex"
P19-1209,P17-1099,0,0.81231,"tent selection from summary generation (Chen 2175 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to learn to select sentence singletons and pairs, which then serve as the basis for an abstractive summarizer to compose a summary sentence-by-sentence, where singletons are shortened (i.e., compressed) and pairs are merged (i.e., fused). We exploit stateof-the-art neural representations and traditional vector space models to characterize"
P19-1209,C10-1111,0,0.168223,"kistani Maj. Gen. Athar Abbas said the report “unfounded and malicious” and an “effort to malign the ISI,” – Pakistan’s directorate of inter-services intelligence. Compressed Sentence: Maj. Gen. Athar Abbas said the report was an “effort to malign the ISI.” Table 1: Example sentence singleton and pair, before and after compression/merging. and identifying the sentence relationships for performing fusion. Previous studies assume a set of similar source sentences can be gathered by clustering sentences or by comparing to a reference summary sentence (Barzilay and McKeown, 2005; Filippova, 2010; Shen and Li, 2010; Chenal and Cheung, 2016; Liao et al., 2018); but these methods can be suboptimal. Joint models for sentence selection and fusion implicitly perform content planning (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Bing et al., 2015; Durrett et al., 2016) and there is limited control over which sentences are merged and how. In contrast, this work attempts to teach the system to determine if a sentence singleton or a pair should be selected to produce a summary sentence. A sentence pair (A, B) is preferred over its consisting sentences if they carry complementary content. Table 1 shows"
P19-1209,C18-1146,1,0.947592,"ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2175–2189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics and Bansal, 2018; Hsu et al., 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). Content selection draws on domain knowledge to identify relevant content, while summary generation weaves together selected source and vocabulary words to form a coherent summary. Despite having local coherence, system summaries can sometimes contain erroneous details (See et al., 2017) and forged content (Cao et al., 2018b; Song et al., 2018). Separating the two tasks of content selection and summary generation allows us to closely examine the compressing and fusing mechanisms of an abstractive summarizer. In this paper we propose a method to learn to select sentence singletons and pairs, which then serve as the basis for an abstractive summarizer to compose a summary sentence-by-sentence, where singletons are shortened (i.e., compressed) and pairs are merged (i.e., fused). We exploit stateof-the-art neural representations and traditional vector space models to characterize singletons and pairs; we then provide suggestions on the"
P19-1209,P17-1108,0,0.0475676,"training data is limited; • we investigate the factors involved in representing sentence singletons and pairs. We perform extensive experiments and report findings on sentence selection and abstraction.1 2 Related Work Content selection is integral to any summarization system. Neural approaches to abstractive summarization often perform content selection jointly with surface realization using an encoder-decoder architecture (Rush et al., 2015; Nallapati et al., 1 We make our code and models publicly available at https: //github.com/ucfnlp/summarization-sing-pair-mix 2016; Chen et al., 2016b; Tan et al., 2017; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018; Narayan et al., 2018). Training these models end-to-end means learning to perform both tasks simultaneously and can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Geh"
P19-1209,I13-1198,0,0.208774,"Missing"
P19-1209,P13-1136,0,0.0812164,"the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus on selecting and compressing sentence singletons only. A sentence can also be generated through fusing multiple source sentences. However, many aspects of this approach are largely underinvestigated, such as determining the set of source sentences to be fused, handling its large cardinality, 2176 Sentence Pair: (A) The bombing killed 58 people. (B) Wajid Shamsul Hasan, Pakistan’s high commissioner to Britain, and Hamid Gul, former head of the ISI, firmly denied the agency’s involvement in the attack. Merged Sen"
P19-1209,C08-1124,0,0.0538945,"d can require a massive amount of data that is unavailable and unaffordable for many summarization tasks. Recent approaches emphasize the importance of separating content selection from summary generation for abstractive summarization. Studies exploit extractive methods to identify content words and sentences that should be part of the summary and use them to guide the generation of abstracts (Chen and Bansal, 2018; Gehrmann et al., 2018; Lebanoff et al., 2018). On the other hand, surface lexical features have been shown to be effective in identifying pertinent content (Carenini et al., 2006; Wong et al., 2008; Galanis et al., 2012). Examples include sentence length, position, centrality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the firs"
P19-1209,P17-1101,0,0.0565003,"ality, word frequency, whether a sentence contains topic words, and others. The surface cues can also be customized for new domains relatively easily. This paper represents a step forward in this direction, where we focus on developing lightweight models to select summary-worthy sentence singletons and pairs and use them as the basis for summary generation. A succinct sentence can be generated by shortening or rewriting a lengthy source text. Recent studies have leveraged neural encoder-decoder models to rewrite the first sentence of an article to a title-like summary (Nallapati et al., 2016; Zhou et al., 2017; Li et al., 2017; Song et al., 2018; Guo et al., 2018; Cao et al., 2018a). Compressive summaries can be generated in a similar vein by selecting important source sentences and then dropping inessential sentence elements such as prepositional phrases. Before the era of deep neural networks it has been an active area of research, where sentence selection and compression can be accomplished using a pipeline or a joint model (Daum´e III and Marcu, 2002; Zajic et al., 2007; Gillick and Favre, 2009; Wang et al., 2013; Li et al., 2013, 2014; Filippova et al., 2015). A majority of these studies focus"
P19-1209,J05-3002,0,\N,Missing
P19-1209,D14-1076,1,\N,Missing
P19-1209,W01-0100,0,\N,Missing
P19-1209,P18-1015,0,\N,Missing
P19-1209,E06-1039,0,\N,Missing
P19-1209,P11-1049,0,\N,Missing
P19-1593,D16-1053,0,0.0312085,"ison with subject and object position (Grosz et al., 1995). It is possible that the reader has learned this principle, and that this is why it chooses not to store these names in memory. However, the reader also learns from the GAP supervision that pronouns are important, and therefore stores the pronoun his even though it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a core"
P19-1593,P15-1136,0,0.179541,"Missing"
P19-1593,N19-1423,0,0.0266509,"ellelism+URL). Language model pretraining yields an absolute gain of 3.2 in F1 . This demonstrates the ability of RefReader to leverage unlabeled text, which is a distinctive feature in comparison with prior work. When training is carried out in the unsupervised setting (with the language modeling objective only), the model is still capable of learning the latent coreferential structure between pronouns and names to some extent, outperforming a supervised coreference system that gives competitive results on OntoNotes (Clark and Manning, 2015). We also test a combination of RefReader and BERT (Devlin et al., 2019), using BERT’s contextualized word embeddings as base features xt (concatenation of the top 4 layers), which yields substantial improvements in accuracy. While this model still resolves references incrementally, it cannot be said to be purely incremental, because BERT uses “future” information to build its contextualized embeddings.5 Note that the gender 5 Future work may explore the combination of RefReader bias increases slightly, possibly due to bias in the data used to train BERT. GAP examples are short, containing just a few entity mentions. To test the applicability of our method to long"
P19-1593,N18-2007,0,0.0650583,"is a learnable vector. Figure 2: Overview of the model architecture. (i) (i) value vt ∈ RDv , and a salience st ∈ [0, 1]. There are two components to the model: the memory unit, which stores and tracks the states of the entities in the text; and the recurrent unit, which controls the memory via a set of gates. An overview is presented in Figure 2. 2.1 Recurrent Unit The recurrent unit is inspired by the CoreferentialGRU, in which the current hidden state of a gated recurrent unit (GRU; Chung et al., 2014) is combined with the state at the time of the most recent mention of the current entity (Dhingra et al., 2018). However, instead of relying on the coreferential structure to construct a dynamic computational graph, we use an external memory unit to keep track of previously mentioned entities and let the model learn to decide what to store in each cell. The memory state is summarized by the P (i) (i) weighted sum over values: mt = N i=1 s vt . The current hidden state and the input are ˜t = combined into a pre-recurrent state h tanh(W ht−1 + U xt ), which is used to control the memory operations; the matrices W and U are trained parameters. To compute the next hidden state ht , we perform a recurrent u"
P19-1593,J95-2003,0,0.139821,"update (indicating coreference with Padbury), and a weaker update to m1. If the update to m0 is above the threshold, then the reader may receive credit for this coreference edge, which would otherwise be scored as a false negative. The reader ignores the names Braylon Edwards, Piers Haggard, and Cathy Vespers, leaving them out of the memory. Edwards and Vespers appear in prepositional phrases, while Haggard is a possessive determiner of the object of a prepositional phrase. Centering theory argues that these syntactic positions have low salience in comparison with subject and object position (Grosz et al., 1995). It is possible that the reader has learned this principle, and that this is why it chooses not to store these names in memory. However, the reader also learns from the GAP supervision that pronouns are important, and therefore stores the pronoun his even though it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to"
P19-1593,D17-1195,0,0.0335153,"t, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data. 5 Conclusion This paper demonstrates the viability of incremental reference resolution, using an end-to-end differentiable memory network. This enables semisupervised learning from a language modeling objective, which substantially improves performance. A key question for future work is the performance on longer texts, such as the full-length news articles encountered in"
P19-1593,I17-1048,0,0.0201279,"sive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data. 5 Conclusion This"
P19-1593,D17-1018,1,0.869104,"o3 Ismael told (2) u4 Captain Ahab (2) o7 he saw Moby-Dick self link coreferential not coreferential Figure 1: A referential reader with two memory cells. (i) (i) Overwrite and update are indicated by ot and ut ; in practice, these operations are continuous gates. Thickness and color intensity of edges between memory cells at neighboring steps indicate memory salience; 7 indicates an overwrite. Introduction Reference resolution is fundamental to language understanding. Current state-of-the-art systems employ the mention-pair model, in which a classifier is applied to all pairs of spans (e.g., Lee et al., 2017). This approach is expensive in both computation and labeled data, and it is also cognitively implausible: human readers interpret text in a nearly online fashion (Tanenhaus et al., 1995). We present a new method for reference resolution, which reads the text left-to-right while storing entities in a fixed-size working memory (Figure 1). As each token is encountered, the reader must decide whether to: (a) link the token to an existing memory, thereby creating a coreference link, (b) overwrite an existing memory and store a new entity, or (c) disregard the token and move ahead. As memories are"
P19-1593,D14-1162,0,0.0808162,"Missing"
P19-1593,W09-3905,0,0.0353313,"et of pronoun-name anaphora demonstrates strong performance with purely incremental text processing. Work carried out as an intern at Facebook AI Research ston et al., 2015), in which memory operations are differentiable, enabling end-to-end training from gold anaphora resolution data. Furthermore, the memory can be combined with a recurrent hidden state, enabling prediction of the next word. This makes it possible to train the model from unlabeled data using a language modeling objective. To summarize, we present a model that processes the text incrementally, resolving references on the fly (Schlangen et al., 2009). The model yields promising results on the GAP dataset of pronoun-name references.1 2 Model For a given document consisting of a sequence of tokens {wt }Tt=1 , we represent text at two levels: • Tokens: represented as {xt }Tt=1 , where the vector xt ∈ RDx is computed from any token-level encoder. • Entities: represented by a fixed-length mem(i) (i) (i) ory Mt = {(kt , vt , st )}N i=1 , where each (i) memory is a tuple of a key kt ∈ RDk , a 1 Code available at: liufly/refreader https://github.com/ 5918 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pag"
P19-1593,Q18-1042,0,0.267101,"f links). P The Pcoreference loss is then the cross-entropy Ti=1 Tj=i+1 H(ψˆi,j , yi,j ). Because the hidden state ht is computed recurrently from w1:t , the reader can also be trained from a language modeling objective, even when coreference annotations are unavailable. Word probabilities P (wt+1 |ht ) are computed by projecting the hidden state ht by a matrix of output embeddings, and applying the softmax operation. 3 Experiments As an evaluation of the ability of the referential reader to correctly track entity references in text, we evaluate against the GAP dataset, recently introduced by Webster et al. (2018). Each instance consists of: (1) a sequence of tokens w1 , . . . , wT extracted from Wikipedia biographical pages; (2) two person names (A and B, whose token index spans are denoted sA and sB ); (3) a single-token pronoun (P with the token index sP ); and (4) two binary labels (yA and yB ) indicating whether P is referring to A or B. Language modeling. Given the limited size of GAP, it is difficult to learn a strong recurrent model. We therefore consider the task of language modeling as a pre-training step. We make use of the page text of the original Wikipedia articles from GAP, the URLs to w"
P19-1593,D17-1197,0,0.0309738,"it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabele"
S12-1072,W08-2134,0,\N,Missing
S12-1072,W10-4145,1,\N,Missing
S16-1145,S14-2082,0,0.0716102,"Missing"
S16-1145,N13-1090,0,0.0935626,"Missing"
S16-1145,K15-1009,1,0.887877,"Missing"
S16-1145,N15-1099,1,0.882823,"Missing"
S16-1145,P12-2050,0,0.277402,"Missing"
S16-1145,E09-1087,0,\N,Missing
S16-1145,P16-1158,1,\N,Missing
U14-1025,W11-0705,0,0.130598,"Missing"
U14-1025,I13-1041,0,0.0247437,"Missing"
U14-1025,N13-1037,0,0.033995,"Missing"
U14-1025,P05-1045,0,0.00720314,"at identifying spatial named entities in tweets. Matching place references in a gazetteer (Hill, 2000) is another widely-used approach. Paradesi (2011) investigated the approach of combining NER and external gazetteers. Further, Gelernter and Balaji (2013) built a geoparser incorporating the results of four parsers. In our attempt to build an automatic EoL identification system, we employed a conditional random field (CRF) (Lafferty et al., 2001), which can be found and has proved to be successful in various Natural Language Processing (NLP) tasks (Sha and Pereira, 2003; Gimpel et al., 2011; Finkel et al., 2005; Ritter et al., 2011). In this paper, we present our approach to building such a system as well as a variety of features, such as lexical, structural and geospatial features and show major improvements on the task of EoL identification over earlier attempts. Our best-performing system is ranked among the top 3 systems (2nd in the public leaderboard). The paper is organised as follows: the dataset and external resources used in our system is described in Section 2 and Section 3. We introduce the tools involved in this paper in Section 4. In Section 5 and Section 6, we provide the description o"
U14-1025,P11-2008,0,0.119325,"Missing"
U14-1025,D12-1039,0,0.0193456,"urch New Zealand. It incorporates the output of four parsers: a lexico-semantic named location parser, a rulebased street name parser, a rule-based building name parser and a trained NER. 5 System Description 3.1 User Meta Data We extracted location meta information of the authors of the messages in the training data and created a list of such location mentions. In this section, we describe our approach to creating an automatic EoL identification system. 3.2 Text Retrieved from URLs We pre-processed both the training and test dataset with lexical normalisation (using the dictionary created by Han et al. (2012)), POS tagging and full-text chunk parsing. Recognising the incompetent performance of traditional NLP tools when applied to social media text (Java, 2007; Becker et al., 2009; Yin et al., 2012; Preotiuc-Pietro et al., 2012; Baldwin et al., 2013; Gelernter and Balaji, 2013), we adopted ARK Tweet NLP POS Tagger v0.3 (Owoputi et al., 2013) with the Penn Treebank tagset model for the task of word tokenisation and POS tagging. For chunk parsing, we used OpenNLP2 . Additionally, for the purposes of self-training, we also downloaded the text of the articles whose URLs are contained in the tweets (37"
U14-1025,P11-1037,0,0.0850832,"Missing"
U14-1025,N13-1039,0,0.0321544,"Missing"
U14-1025,C14-1168,0,0.0232116,"Missing"
U14-1025,D11-1141,0,0.292702,"Missing"
U14-1025,W04-1221,0,0.0291915,"tools we utilised in our system. CRF++ CRF++ is an open source, general-purpose implementation of CRF by Kudo (2005) and can be applied to a wide variety of NLP tasks. Since it 1 5.1 Pre-processing We trained our model (based on CRF++) with various features, which can be categorised into three categories: lexical features, structural features and geospatial features. Note that we used a context window of 2 for each feature. • Lexical features include lemmatised words (using NLTK (Bird et al., 2009), POS, 2 http://www.geonames.org/ 172 http://opennlp.apache.org/ brief word class introduced by Settles (2004) where capitial and lowercase letters are replaced with ‘A’ and ‘a’, digits with ‘0’ and all other characters with ‘ ’ and consecutive identical characters are collapsed into one (e.g. #Adelaide → Aa), capitalisation and locative indicator (Liu, 2013). • Structural features include position of the word in the chunk and POS of the first word in the chunk. • Geospatial features include GeoNames geospatial feature class described by Liu (2013). As pointed out by Wolpert (1992), stacking is able to generate better results than any single one of the trained model. We therefore also applyed stacking"
U14-1025,N03-1028,0,0.15319,"Missing"
U17-1002,W14-4337,0,0.0495726,"Missing"
U17-1002,D14-1181,0,0.00276342,"ch performs consistently well over all tasks. We propose a unified model generalising weight tying and in doing so, make the model more expressive. The proposed model achieves uniformly high performance, improving on the best results for memory network-based models on the bAbI dataset, and competitive results on Dialog bAbI. 1 Introduction Deep neural network models have demonstrated strong performance on a number of challenging tasks, such as image classification (He et al., 2016), speech recognition (Graves et al., 2013), and various natural language processing tasks (Bahdanau et al., 2014; Kim, 2014; Xiong et al., 2016). Recently, the augmentation of neural networks with external memory components has been shown to be a powerful means of capturing context of different types (Graves et al., 2014, 2016; Rae et al., 2016). Of particular interest to this work is the work by Sukhbaatar et al. (2015), on end-toend memory networks (N2Ns), which exhibit remarkable reasoning capabilities, e.g. for reasoning (Weston et al., 2016) and goal-oriented dialogue tasks (Bordes and Weston, 2016). Typically, such tasks consist of three key components: a sequence of supporting facts (the story), a question,"
U17-1002,E17-1001,1,0.652188,"ayer-wise; see Section 2 for a technical description). While N2Ns generally work well with either weight tying approach, as reported in Sukhbaatar et al. (2015), the performance is uneven on some difficult tasks. That is, for some tasks, one weight tying approach attains nearperfect accuracy and the other performs poorly, but for other tasks, this trend is reversed. In this paper, focusing on improving N2N, we propose a unified model, UN2N, capable of dynamically determining the appropriate type of weight tying for a given task. This is realised through the use of a gating vector, inspired by Liu and Perez (2017). Our method achieves the best performance for a memory network-based model on the bAbI dataset, superior to both adjacent and layer-wise weight tying, and competitive results on Dialog bAbI. The paper is organised as follows: after we review N2N and related reasoning models in Section 2, we describe our motivation and detail the elements of our proposed model in Section 3. Section 4 and 5 present the experimental results on the bAbI and Dialog bAbI datasets with analyses in Section 6. Lastly, Section 7 concludes the paper. 2 Related Work End-to-End Memory Networks: Building on top of memory n"
U17-1002,W14-4012,0,0.0143448,"Missing"
W08-0112,J96-2004,0,0.252946,"n that of Data Set (III), which used a direct sentence extraction scheme on the whole transcript. This suggests that even using the abstracts as a guidance, people still have a high variation in extracting summary sentences. We also calculated the pairwise Kappa score between annotations in different data sets. The inter-group Kappa score is much lower than those of the intragroup agreement, most likely due to the different annotation specifications used in the two different data sets. 3.2 0 400 600 800 1000 1200 1400 Topic length 200 400 600 800 1000 1200 1400 Topic length Kappa coefficient (Carletta, 1996) is commonly used as a standard to reflect inter-annotator agreement. Table 1 shows the average Kappa results, calculated for each meeting using the data sets described in Section 2. Compared to Kappa score on text summarization, which is reported to be 0.38 by (Mani et al., 2002) on a set of TREC documents, the inter-annotator agreement on meeting corpus is lower. This is likely due to the difference between the meeting style and written text. Data Set Avg-Kappa 200 -0.2 0 Impacting Factors We further analyze inter-annotator agreement with respect to two factors: topic length and meeting part"
W08-0112,W06-1643,0,0.0159421,"ores, we also proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the ab"
W08-0112,N03-1020,0,0.418284,"es, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hori et al., 2003). Typically multiple reference human summaries are used in evaluation in order to account for the inconsistency among human annotations. While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear. For example, what are speech summaries? Do humans agree with each other on summary extraction? In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation. Meetings often have several participants. Its speech is spontaneou"
W08-0112,W05-0905,0,0.0669308,"proposed a sentence distance score and divergence distance as a quantitative measure. This study is expected to help better define the speech summarization problem. 1 Introduction With the fast development of recording and storage techniques in recent years, speech summarization has received more attention. A variety of approaches have been investigated for speech summarization, for example, maximum entropy, conditional random fields, latent semantic analysis, support vector machines, maximum marginal relevance (Maskey and Hirschberg, 2003; Hori et al., 2003; Buist et al., 2005; Galley, 2006; Murray et al., 2005; Zhang et al., 2007; Xie and Liu, 2008). These studies used different domains, such as broadcast news, lectures, and meetings. In these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization res"
W08-0112,N04-1019,0,0.031141,"these approaches, different information sources have been examined from both text and speech related features (e.g., prosody, speaker activity, turn-taking, discourse). How to evaluate speech summaries has also been studied recently, but so far there is no consensus on evaluation yet. Often the goal in evaluation is to develop an automatic metric to have a high correlation with human evaluation scores. Different methods have been used in the above summarization research to compare system generated summaries with human annotation, such as Fmeasure, ROUGE, Pyramid, sumACCY (Lin and Hovy, 2003; Nenkova and Passonneau, 2004; Hori et al., 2003). Typically multiple reference human summaries are used in evaluation in order to account for the inconsistency among human annotations. While there have been efforts on speech summarization approaches and evaluation, some fundamental problems are still unclear. For example, what are speech summaries? Do humans agree with each other on summary extraction? In this paper, we focus on the meeting domain, one of the most challenging speech genre, to analyze human summary annotation. Meetings often have several participants. Its speech is spontaneous, contains disfluencies, and"
W08-0112,W04-2319,0,0.0507011,"Missing"
W11-0709,W06-1643,0,0.0190831,"aluation tracks of the DUC (Document Understanding Conference) and TAC (Text Analysis Conference). To some extent, Twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency. The summary sentence is selected as one of the highest weighted paths in the graph. (Shari"
W11-0709,W04-1013,0,0.0534228,"Missing"
W11-0709,P11-2013,1,0.558985,"Missing"
W11-0709,N06-1047,0,0.0248835,"iven by the annual evaluation tracks of the DUC (Document Understanding Conference) and TAC (Text Analysis Conference). To some extent, Twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency. The summary sentence is selected as one of the highest weighted paths in the"
W11-0709,N10-1132,0,0.0120899,"remaining sentence collection, until no sentence can be removed. On average, the reference summary for general and hashtag topics contains 44 and 40 words respectively. 4 Summarization System For each of the topic phrases, our goal is to generate a short textual summary that can best convey the main ideas of the topic contents. We explore and compare multiple text sources as summarization input, including the user-contributed tweets, web contents linked from the tweets, as well as combination of the two sources. The concept-based optimization approach (Gillick et al., 2009; Xie et al., 2009; Murray et al., 2010) was employed for selecting informative summary sentences and minimizing the redundancy. Note that our focus of this paper is not developing new summarization systems, but rather utilizing and integrating different text sources for generating more informative Twitter topic summaries. 4.1 Concept-based Optimization Framework Concept-based summarization approach first extracts a set of important concepts for each topic, then selects a collection of sentences that can cover as many important concepts as possible, while within the specified length limit. This idea is realized using the integer lin"
W11-0709,A97-1004,0,0.0224761,"y in the 2 For each Twitter topic, we collect a set of web pages linked by the topic tweets and use them as another source of summarization input. For each topic, we select up to n (n = 10) URLs that appear most frequently in the topic tweets and infrequently across different Twitter topics. This scheme is similar to the TF-IDF measure. This way we can select the salient URLs for each topic while avoiding the spam URLs. The contents of these URLs were collected and only distinct web pages were retained. We use an HTML parser3 to extract the textual contents, and perform sentence segmentation (Reynar and Ratnaparkhi, 1997) on the parsed web pages. All the pages corresponding to the same topic were sorted by the date they were first cited in the tweets. These web pages were taken as another input text source for the summarization system, denoted as “Web”. 4.2.4 We expect that taking advantage of both tweets and linked web contents would benefit the topic summarization system. Consolidating the distinct text sources may help boost the weight of key concepts and eliminate the spam information. As a preliminary study, we investigate concatenating either the original tweets or the normalized tweets with the linked w"
W11-0709,N10-1100,0,0.643955,"onversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proportion to its distance to the root and the corresponding phrase frequency. The summary sentence is selected as one of the highest weighted paths in the graph. (Sharifi et al., 2010b; Inouye, 2010) introduced a hybrid TF-IDF approach to extract one- or multiple-sentence summary for each topic. Sentences were ranked according to the average TF-IDF score of the consisting words; top weighted s"
W11-0709,J02-4003,0,0.0236621,"e is not much previous work on summarizing the Twitter topics. Most previous summarization literature focused on the written text domain, as driven by the annual evaluation tracks of the DUC (Document Understanding Conference) and TAC (Text Analysis Conference). To some extent, Twitter topic summarization is related to spoken document summarization, since both tasks deal with the conversational text that is contributed by multiple participants and contains lots of ill-formed sentences, colloquial expressions, nonstandard word tokens or high word error rate, etc. To summarize the spoken text, (Zechner, 2002) aimed to address problems related to disfluencies, extraction units, crossspeaker coherence, etc. (Maskey and Hirschberg, 2005; Murray et al., 2006; Galley, 2006; Xie et al., 2008; Liu and Liu, 2010a) incorporated lexical, structural, speaker, and discourse cues to generate textual summaries for broadcast news and meeting conversations. For microblog summarization, (Sharifi et al., 2010a) proposed a phrase reinforcement (PR) algorithm to summarize the Twitter topic in one sentence. The algorithm builds a word graph using the topic phrase as the root node; each word node is weighted in proport"
W16-4303,W14-4012,0,0.109546,"Missing"
W16-4303,P11-1038,0,0.162799,".0/ ∗ Licence details: http:// Work carried out at Xerox Research Centre Europe 20 Proceedings of the Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 20–29, Osaka, Japan, December 12 2016. as one of supervised sequence regression based on a joint atomic representation of the text: specifically on the character and word level. In this context, we are exploring short texts. Typically, classification of such texts tends to be particularly challenging for state-of-the-art BoW based approaches due, in part, to the noisy nature of such data (Han and Baldwin, 2011). To cope with this we propose a novel recurrent and compositional neural network architecture, capable of constructing representations at character, word and sentence level. The paper is structured as follows: after we consider previous approaches to the task of computational personality recognition, including those which have a deep-learning component, we describe our model. We report on two sets of experiments, the first of which demonstrates the effectiveness of the model in inferring personality for users, while the second reports on the short text level analysis. In both settings, the pr"
W16-4303,D15-1176,0,0.357538,"the characters, to build hierarchical, vectorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits compared with prior work. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. 1 Introduction Techniques falling under the umbrella of “deep-learning” are increasingly commonplace in the space of Natural Language Processing (NLP) (Manning, 2016). Such methods have been applied to a number of tasks from part-of-speech-tagging (Ling et al., 2015) to sentiment analysis (Socher et al., 2013). Essentially, each of these tasks is concerned with learning representations of language at different levels. The work we outline here is no different in essence, though we choose perhaps the highest level of representation – that of the author of a given text rather than the text itself. This task, modelling people from their language, is one built on the long-standing foundation that language use is known to be influenced by sociodemographic characteristics such as gender and personality (Tannen, 1990; Pennebaker et al., 2003). The study of person"
W16-4303,D15-1130,1,0.836158,"rsonality trait score of the ith tweet, similarly yuseri and yˆuseri are their user-level counterparts. Each tweet in the dataset inherits the same five trait scores as assigned to the author from whom they were drawn. 1 PTi yˆuseri = Ti j=1 yˆsj where Ti refers to the total number of tweets of useri . In Section 4.3 and 4.4, we present the results measured at the user and tweet level using RM SEuser and RM SEtweet respectively. It is important to note that, to enable direct comparison, we use exactly the same dataset and evaluation metric RM SEuser as in the works of (Sulea and Dichiu, 2015; Mirkin et al., 2015; Nowson et al., 2015). 4.3 Personality Trait Prediction at User Level We test the proposed models on the dataset described in Section 4.1 and train our model to predict the personality trait scores based purely on the text with no additional features supplied. To demonstrate the effectiveness of the proposed model, we evaluate the performance on the user level against models incorporating linguistic and psychologically motivated features. This allows us to directly compare the performance of current state-of-the-art models and C2W2S4PT. For 5-fold cross-validation, we compare to the tied-high"
W16-4303,N13-1039,0,0.0485869,"Missing"
W16-4303,D13-1170,0,0.02899,"ctorial word and sentence representations for trait inference. This method, applied to a corpus of tweets, shows state-of-the-art performance across five traits compared with prior work. The results, supported by preliminary visualisation work, are encouraging for the ability to detect complex human traits. 1 Introduction Techniques falling under the umbrella of “deep-learning” are increasingly commonplace in the space of Natural Language Processing (NLP) (Manning, 2016). Such methods have been applied to a number of tasks from part-of-speech-tagging (Ling et al., 2015) to sentiment analysis (Socher et al., 2013). Essentially, each of these tasks is concerned with learning representations of language at different levels. The work we outline here is no different in essence, though we choose perhaps the highest level of representation – that of the author of a given text rather than the text itself. This task, modelling people from their language, is one built on the long-standing foundation that language use is known to be influenced by sociodemographic characteristics such as gender and personality (Tannen, 1990; Pennebaker et al., 2003). The study of personality traits in particular is supported by t"
W16-4303,L16-1258,0,0.0985863,"Missing"
W16-4303,N16-1174,0,0.0441666,"predict trait scores from a single short text, arguably a more challenging task considering the limited amount of information. The model we present in Section 3.2 is inspired by Ling et al. (2015), who proposed a character-level word representation learning model under the assumption that character sequences are syntactically and semantically informative of the words they compose. Based on a widely used RNN named long shortterm memory network (LSTM) (Hochreiter and Schmidhuber, 1997), the model learns the embeddings of characters and how they can be used to construct words. Inspired by this, Yang et al. (2016) introduced Hierarchical Attention Networks where the representation of a document is hierarchically built up. The work of (Ling et al., 2015) provides a way to construct words from their constituent characters (Character to Word, C2W) while Yang et al. (2016) describe a hierarchical approach to building representations of documents from words to sentences, and eventually to documents (Word to Sentence to Document, W2S2D). In this work, inspired by the above works, we present a hierarchical model situated between the above two models, connecting characters, words and sentences, and ultimately"
