2021.bppf-1.1,"Benchmarking: Past, Present and Future",2021,-1,-1,3,0,3453,kenneth church,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future",0,"Where have we been, and where are we going? It is easier to talk about the past than the future. These days, benchmarks evolve more bottom up (such as papers with code). There used to be more top-down leadership from government (and industry, in the case of systems, with benchmarks such as SPEC). Going forward, there may be more top-down leadership from organizations like MLPerf and/or influencers like David Ferrucci, who was responsible for IBM{'}s success with Jeopardy, and has recently written a paper suggesting how the community should think about benchmarking for machine comprehension. Tasks such as reading comprehension become even more interesting as we move beyond English. Multilinguality introduces many challenges, and even more opportunities."
P18-5005,Beyond Multiword Expressions: Processing Idioms and Metaphors,2018,0,0,1,1,12066,valia kordoni,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"Idioms and metaphors are characteristic to all areas of human activity and to all types of discourse. Their processing is a rapidly growing area in NLP, since they have become a big challenge for NLP systems. Their omnipresence in language has been established in a number of corpus studies and the role they play in human reasoning has also been confirmed in psychological experiments. This makes idioms and metaphors an important research area for computational and cognitive linguistics, and their automatic identification and interpretation indispensable for any semantics-oriented NLP application. This tutorial aims to provide attendees with a clear notion of the linguistic characteristics of idioms and metaphors, computational models of idioms and metaphors using state-of-the-art NLP techniques, their relevance for the intersection of deep learning and natural language processing, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in machine learning, parsing (syntactic and semantic) and language technology, not necessarily experts in idioms and metaphors, who are interested in tasks that involve or could benefit from considering idioms and metaphors as a pervasive phenomenon in human language and communication."
L18-1073,A Multilingual Wikified Data Set of Educational Material,2018,0,0,10,0,16715,iris hendrickx,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1075,Translation Crowdsourcing: Creating a Multilingual Corpus of Online Educational Content,2018,0,0,9,0,20877,vilelmini sosoni,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"The present work describes a multilingual corpus of online content in the educational domain, i.e. Massive Open Online Coursen material, ranging from course forum text to subtitles of online video lectures, that has been developed via large-scale crowdsourcing.n The English source text is manually translated into 11 European and BRIC languages using the CrowdFlower platform. During then process several challenges arose which mainly involved the in-domain text genre, the large text volume, the idiosyncrasies of eachn target language, the limitations of the crowdsourcing platform, as well as the quality assurance and workflow issues of then crowdsourcing process. The corpus constitutes a product of the EU-funded TraMOOC project and is utilised in the project in order ton train, tune and test machine translation engines."
L18-1528,Improving Machine Translation of Educational Content via Crowdsourcing,2018,0,1,12,0,16478,maximiliana behnke,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"The limited availability of in-domain training data is a major issue in the training of application-specific neural machine translationn models. Professional outsourcing of bilingual data collections is costly and often not feasible. In this paper we analyze the influence ofn using crowdsourcing as a scalable way to obtain translations of target in-domain data having in mind that the translations can be of an lower quality. We apply crowdsourcing with carefully designed quality controls to create parallel corpora for the educational domainn by collecting translations of texts from MOOCs from English to eleven languages, which we then use to fine-tune neural machinen translation models previously trained on general-domain data. The results from our research indicate that crowdsourced data collectedn with proper quality controls consistently yields performance gains over general-domain baseline systems, and systems fine-tuned withn pre-existing in-domain corpora."
P17-5005,Beyond Words: Deep Learning for Multiword Expressions and Collocations,2017,0,0,1,1,12066,valia kordoni,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"Deep learning has recently shown much promise for NLP applications. Traditionally, in most NLP approaches, documents or sentences are represented by a sparse bag-of-words representation. There is now a lot of work which goes beyond this by adopting a distributed representation of words, by constructing a so-called ``neural embedding'' or vector space representation of each word or document. The aim of this tutorial is to go beyond the learning of word vectors and present methods for learning vector representations for Multiword Expressions and bilingual phrase pairs, all of which are useful for various NLP applications.This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of Multiword Expressions (MWEs), their relevance for the intersection of deep learning and natural language processing, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in machine learning, parsing (syntactic and semantic) and language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication."
W16-4813,Enlarging Scarce In-domain {E}nglish-{C}roatian Corpus for {SMT} of {MOOC}s Using {S}erbian,2016,6,0,3,0,5059,maja popovic,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"Massive Open Online Courses have been growing rapidly in size and impact. Yet the language barrier constitutes a major growth impediment in reaching out all people and educating all citizens. A vast majority of educational material is available only in English, and state-of-the-art machine translation systems still have not been tailored for this peculiar genre. In addition, a mere collection of appropriate in-domain training material is a challenging task. In this work, we investigate statistical machine translation of lecture subtitles from English into Croatian, which is morphologically rich and generally weakly supported, especially for the educational domain. We show that results comparable with publicly available systems trained on much larger data can be achieved if a small in-domain training set is used in combination with additional in-domain corpus originating from the closely related Serbian language."
W16-1808,Using Word Embeddings for Improving Statistical Machine Translation of Phrasal Verbs,2016,16,1,2,0.931234,33554,kostadin cholakov,Proceedings of the 12th Workshop on Multiword Expressions,0,"We examine the employment of word embeddings for machine translation (MT) of phrasal verbs (PVs), a linguistic phenomenon with challenging semantics. Using word embeddings, we augment the translation model with two features: one modelling distributional semantic properties of the source and target phrase and another modelling the degree of compositionality of PVs. We also obtain paraphrases to increase the amount of relevant training data. Our method leads to improved translation quality for PVs in a case study with English to Bulgarian MT system."
L16-1003,Enhancing Access to Online Education: Quality Machine Translation of {MOOC} Content,2016,19,1,1,1,12066,valia kordoni,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The present work is an overview of the TraMOOC (Translation for Massive Open Online Courses) research and innovation project, a machine translation approach for online educational content. More specifically, videolectures, assignments, and MOOC forum text is automatically translated from English into eleven European and BRIC languages. Unlike previous approaches to machine translation, the output quality in TraMOOC relies on a multimodal evaluation schema that involves crowdsourcing, error type markup, an error taxonomy for translation model comparison, and implicit evaluation via text mining, i.e. entity recognition and its performance comparison between the source and the translated text, and sentiment analysis on the students{'} forum posts. Finally, the evaluation output will result in more and better quality in-domain parallel data that will be fed back to the translation engine for higher quality output. The translation service will be incorporated into the Iversity MOOC platform and into the VideoLectures.net digital library portal."
2016.eamt-2.20,{T}ra{MOOC} (Translation for Massive Open Online Courses): providing reliable {MT} for {MOOC}s,2016,0,1,1,1,12066,valia kordoni,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,0,None
W15-4935,{T}ra{MOOC}: Translation for Massive Open Online Courses,2015,0,0,1,1,12066,valia kordoni,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
D15-2007,Robust Semantic Analysis of Multiword Expressions with {F}rame{N}et,2015,-1,-1,2,0,1268,miriam petruck,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"This tutorial will give participants a solid understanding of the linguistic features of multiword expressions (MWEs), focusing on the semantics of such expressions and their importance for natural language processing and language technology, with particular attention to the way that FrameNet (framenet.icsi.berkeley.edu) handles this wide spread phenomenon. Our target audience includes researchers and practitioners of language technology, not necessarily experts in MWEs or knowledgeable about FrameNet, who are interested in NLP tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication.NLP research has been interested in automatic processing of multiword expressions, with reports on and tasks relating to such efforts presented at workshops and conferences for at least ten years (e.g. ACL 2003, LREC 2008, COLING 2010, EACL 2014). Overcoming the challenge of automatically processing MWEs remains elusive in part because of the difficulty in recognizing, acquiring, and interpreting such forms.Indeed the phenomenon manifests in a range of linguistic forms (as Sag et al. (2001), among many others, have documented), including: noun + noun compounds (e.g. fish knife, health hazard etc.); adjective + noun compounds (e.g. political agenda, national interest, etc.); particle verbs (shut up, take out, etc.); prepositional verbs (e.g. look into, talk into, etc.); VP idioms, such as kick the bucket, and pull someone{'}s leg, along with less obviously idiomatic forms like answer the door, mention someone{'}s name, etc.; expressions that have their own mini-grammars, such as names with honorifics and terms of address (e.g. Rabbi Lord Jonathan Sacks), kinship terms (e.g. second cousin once removed), and time expressions (e.g. January 9, 2015); support verb constructions (e.g. verbs: take a bath, make a promise, etc; and prepositions: in doubt, under review, etc.). Linguists address issues of polysemy, compositionality, idiomaticity, and continuity for each type included here.While native speakers use these forms with ease, the treatment and interpretation of MWEs in computational systems requires considerable effort due to the very issues that concern linguists."
2015.eamt-1.36,{T}ra{MOOC}: Translation for Massive Open Online Courses,2015,0,0,1,1,12066,valia kordoni,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
kordoni-simova-2014-multiword,Multiword Expressions in Machine Translation,2014,17,7,1,1,12066,valia kordoni,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This work describes an experimental evaluation of the significance of phrasal verb treatment for obtaining better quality statistical machine translation (SMT) results. The importance of the detection and special treatment of phrasal verbs is measured in the context of SMT, where the word-for-word translation of these units often produces incoherent results. Two ways of integrating phrasal verb information in a phrase-based SMT system are presented. Automatic and manual evaluations of the results reveal improvements in the translation quality in both experiments."
E14-1032,Subcategorisation Acquisition from Raw Text for a Free Word-Order Language,2014,28,1,3,1,25732,will roberts,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,We describe a state-of-the-art automatic system that can acquire subcategorisation frames from raw text for a free word-order language. We use it to construct a subcategorisation lexicon of German verbs from a large Web page corpus. With an automatic verb classification paradigm we evaluate our subcategorisation lexicon against a previous classification of German verbs; the lexicon produced by our system performs better than the best previous results.
D14-1024,Better Statistical Machine Translation through Linguistic Treatment of Phrasal Verbs,2014,14,5,2,1,33554,kostadin cholakov,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This article describes a linguistically informed method for integrating phrasal verbs into statistical machine translation (SMT) systems. In a case study involving English to Bulgarian SMT, we show that our method does not only improve translation quality but also outperforms similar methods previously applied to the same task. We attribute this to the fact that, in contrast to previous work on the subject, we employ detailed linguistic information. We found out that features which describe phrasal verbs as idiomatic or compositional contribute most to the better translation quality achieved by our method."
P13-5005,Robust Automated Natural Language Processing with Multiword Expressions and Collocations,2013,9,1,1,1,12066,valia kordoni,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials),0,"This tutorial aims to provide attendees with a clear notion of the linguistic and distributional characteristics of multiword expressions (MWEs), their relevance for robust automated natural language processing and language technology, what methods and resources are available to support their use, and what more could be done in the future. Our target audience are researchers and practitioners in language technology, not necessarily experts in MWEs, who are interested in tasks that involve or could benefit from considering MWEs as a pervasive phenomenon in human language and communication."
2013.mtsummit-wmwumttt.9,Improving {E}nglish-{B}ulgarian statistical machine translation by phrasal verb treatment,2013,-1,-1,2,0,31051,iliana simova,Proceedings of the Workshop on Multi-word Units in Machine Translation and Translation Technologies,0,None
konstantopoulos-etal-2012-task,Task-Driven Linguistic Analysis based on an Underspecified Features Representation,2012,29,0,2,0,34540,stasinos konstantopoulos,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper we explore a task-driven approach to interfacing NLP components, where language processing is guided by the end-task that each application requires. The core idea is to generalize feature values into feature value distributions, representing under-specified feature values, and to fit linguistic pipelines with a back-channel of specification requests through which subsequent components can declare to preceding ones the importance of narrowing the value distribution of particular features that are critical for the current task."
roberts-kordoni-2012-using,Using Verb Subcategorization for Word Sense Disambiguation,2012,12,3,2,1,25732,will roberts,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We develop a model for predicting verb sense from subcategorization information and integrate it into SSI-Dijkstra, a wide-coverage knowledge-based WSD algorithm. Adding syntactic knowledge in this way should correct the current poor performance of WSD systems on verbs. This paper also presents, for the first time, an evaluation of SSI-Dijkstra on a standard data set which enables a comparison of this algorithm with other knowledge-based WSD systems. Our results show that our system is competitive with current graph-based WSD algorithms, and that the subcategorization model can be used to achieve better verb sense disambiguation performance."
R11-1049,Adaptability of Lexical Acquisition for Large-scale Grammars,2011,19,1,3,1,33554,kostadin cholakov,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In this paper, we demonstrate the portability of the lexical acquisition (LA) method proposed in Cholakov and van Noord (2010a). Here, LA refers to the acquisition of linguistic descriptions for words which are not listed in the lexicon of a given computational grammar, i.e., words which are unknown to this grammar. The method we discuss was originally developed for the Dutch Alpino system, and the paper shows that the method also applies to the GG (Crysmann, 2003), a computational HPSG grammar of German. The LA method obtains very similar results for German (84% F-measure on learning unknown words). Extending the GG with the lexical entries proposed by the LA method causes an important improvement in parsing accuracy for a test set of sentences containing unknown words. Furthermore, in a smaller experiment, we show that the linguistic knowledge the LA method provides can also be used for sentence generation."
I11-1086,An Empirical Comparison of Unknown Word Prediction Methods,2011,23,1,3,1,33554,kostadin cholakov,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We compare two types of methods which deal with unknown words in the context of computational grammars. Methods of the first type are based on the idea of supertagging and use a tagger to predict lexical descriptions for unknown tokens in a given input. The second type of methods perform lexical acquisition (LA) which, in the context of this paper, refers to the automatic acquisition of new lexical entries for the lexicon of a given grammar. The methods are compared based on the effect their application has on the parsing coverage and accuracy of the GG grammar of German (Crysmann, 2003). In particular, we adapt the LA method of Cholakov and van Noord (2010) which was originally developed for the Dutch Alpino system to be used with the GG. Its impact on coverage and accuracy on a test corpus of German newspaper texts is compared to the results reported previously on the same corpus for methods which employed a tagger. Furthermore, in a smaller experiment, we show that the linguistic knowledge this LA method provides can also be used for sentence realisation."
P10-5003,"Discourse Structure: Theory, Practice and Use",2010,8,0,3,0,9578,bonnie webber,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,0,"This tutorial consists of four parts. Part I starts with a brief introduction to different bases for discourse structuring, properties of discourse structure that are relevant to LT, and accessible evidence for discourse structure. For discourse structure to be useful for language technologies, one must be able to automatically recognize or generate with it. Hence, Part II surveys computational approaches to recognizing and generating discourse structure, both manuallyauthored approaches and ones developed through Machine Learning. Part III of the tutorial describes applications of discourse structure recognition and generation in LT, as well as discourse-related resources being made available in English, German, Turkish, Hindi, Czech, Arabic and Chinese. Part IV concludes with a list of future possibilities."
N10-1002,Chart Mining-based Lexical Acquisition with Precision Grammars,2010,34,4,3,0.541789,3425,yi zhang,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars. The general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks, and is particularly suited to domain-specific lexical tuning and lexical acquisition using low-coverage grammars. As an illustration of the functionality of our proposed technique, we develop a lexical acquisition model for English verb particle constructions which operates over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features."
jakob-etal-2010-mapping,Mapping between Dependency Structures and Compositional Semantic Representations,2010,9,4,3,0,43095,max jakob,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper investigates the mapping between two semantic formalisms, namely the tectogrammatical layer of the Prague Dependency Treebank 2.0 (PDT) and (Robust) Minimal Recursion Semantics ((R)MRS). It is a first attempt to relate the dependency-based annotation scheme of PDT to a compositional semantics approach like (R)MRS. A mapping algorithm that converts PDT trees to (R)MRS structures is developed, associating (R)MRSs to each node on the dependency tree. Furthermore, composition rules are formulated and the relation between dependency in PDT and semantic heads in (R)MRS is analyzed. It turns out that structure and dependencies, morphological categories and some coreferences can be preserved in the target structures. Moreover, valency and free modifications are distinguished using the valency dictionary of PDT as an additional resource. The validation results show that systematically correct underspecified target representations can be obtained by a rule-based mapping approach, which is an indicator that (R)MRS is indeed robust in relation to the formal representation of Czech data. This finding is novel, for Czech, with its free word order and rich morphology, is typologically different than languages analyzed with (R)MRS to date."
ben-gera-etal-2010-semantic,Semantic Feature Engineering for Enhancing Disambiguation Performance in Deep Linguistic Processing,2010,18,0,3,0,46134,danielle bengera,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The task of parse disambiguation has gained in importance over the last decade as the complexity of grammars used in deep linguistic processing has been increasing. In this paper we propose to employ the fine-grained HPSG formalism in order to investigate the contribution of deeper linguistic knowledge to the task of ranking the different trees the parser outputs. In particular, we focus on the incorporation of semantic features in the disambiguation component and the stability of our model cross domains. Our work is carried out within DELPH-IN (http://www.delph-in.net), using the LinGo Redwoods and the WeScience corpora, parsed with the English Resource Grammar and the PET parser."
kordoni-zhang-2010-disambiguating,Disambiguating Compound Nouns for a Dynamic {HPSG} Treebank of {W}all {S}treet {J}ournal Texts,2010,14,3,1,1,12066,valia kordoni,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The aim of this paper is twofold. We focus, on the one hand, on the task of dynamically annotating English compound nouns, and on the other hand we propose disambiguation methods and techniques which facilitate the annotation task. Both the aforementioned are part of a larger on-going effort which aims to create HPSG annotation for the texts from theWall Street Journal (henceforward WSJ) sections of the Penn Treebank (henceforward PTB) with the help of a hand-written large-scale and wide-coverage grammar of English, the English Resource Grammar (henceforward ERG; Flickinger (2002)). As we show in this paper, such annotations are very rich linguistically, since apart from syntax they also incorporate semantics, which does not only ensure that the treebank is guaranteed to be a truly sharable, re-usable and multi-functional linguistic resource, but also calls for the necessity of a better disambiguation of the internal (syntactic) structure of larger units of words, such as compound nouns, since this has an impact on the representation of their meaning, which is of utmost interest if the linguistic annotation of a given corpus is to be further understood as the practice of adding interpretative linguistic information of the highest quality in order to give Âadded valueÂ to the corpus."
C10-2166,Discriminant Ranking for Efficient Treebanking,2010,10,9,2,0.541789,3425,yi zhang,Coling 2010: Posters,0,"Treebank annotation is a labor-intensive and time-consuming task. In this paper, we show that a simple statistical ranking model can significantly improve treebanking efficiency by prompting human annotators, well-trained in disambiguation tasks for treebanking but not necessarily grammar experts, to the most relevant linguistic disambiguation decisions. Experiments were carried out to evaluate the impact of such techniques on annotation efficiency and quality. The detailed analysis of outputs from the ranking model shows strong correlation to the human annotator behavior. When integrated into the tree-banking environment, the model brings a significant annotation speed-up with improved inter-annotator agreement."
W09-4107,Enabling Adaptation of Lexicalised Grammars to New Domains,2009,7,0,1,1,12066,valia kordoni,Proceedings of the Workshop on Adaptation of Language Resources and Technology to New Domains,0,"This extended abstract focuses on the main points we will be touching upon during our talk, the aim of which is to present in a concise manner our group's work on enhancing robustness of lexicalised grammars for real-life applications and thus also on enabling their adaptation to new domains in its entirety."
W09-3836,Using Treebanking Discriminants as Parse Disambiguation Features,2009,10,0,3,0,5863,md chowdhury,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"This paper presents a novel approach of incorporating fine-grained treebanking decisions made by human annotators as discriminative features for automatic parse disambiguation. To our best knowledge, this is the first work that exploits treebanking decisions for this task. The advantage of this approach is that use of human judgements is made. The paper presents comparative analyses of the performance of discriminative models built using treebanking decisions and state-of-the-art features. We also highlight how differently these features scale when these models are tested on out-of-domain data. We show that, features extracted using treebanking decisions are more efficient, informative and robust compared to traditional features."
W09-3032,Annotating {W}all {S}treet {J}ournal Texts Using a Hand-Crafted Deep Linguistic Grammar,2009,8,13,1,1,12066,valia kordoni,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"This paper presents an on-going effort which aims to annotate the Wall Street Journal sections of the Penn Treebank with the help of a hand-written large-scale and wide-coverage grammar of English. In doing so, we are not only focusing on the various stages of the semi-automated annotation process we have adopted, but we are also showing that rich linguistic annotations, which can apart from syntax also incorporate semantics, ensure that the treebank is guaranteed to be a truly sharable, re-usable and multi-functional linguistic resource."
J09-2001,Prepositions in Applications: A Survey and Introduction to the Special Issue,2009,194,42,2,0.00377921,1468,timothy baldwin,Computational Linguistics,0,"Prepositions1xe2x80x94as well as prepositional phrases (PPs) and markers of various sortsxe2x80x94 have a mixed history in computational linguistics (CL), as well as related fields such as artificial intelligence, information retrieval (IR), and computational psycholinguistics: On the one hand they have been championed as being vital to precise language understanding (e.g., in information extraction), and on the other they have been ignored on the grounds of being syntactically promiscuous and semantically vacuous, and relegated to the ignominious rank of xe2x80x9cstop wordxe2x80x9d (e.g., in text classification and IR). Although NLP in general has benefitted from advances in those areas where prepositions have received attention, there are still many issues to be addressed. For example, in machine translation, generating a preposition (or xe2x80x9ccase markerxe2x80x9d in languages such as Japanese) incorrectly in the target language can lead to critical semantic divergences over the source language string. Equivalently in information retrieval and information extraction, it would seem desirable to be able to predict that book on NLP and book about NLPmean largely the same thing, but paranoid about drugs and paranoid on drugs suggest very different things. Prepositions are often among the most frequent words in a language. For example, based on the British National Corpus (BNC; Burnard 2000), four out of the top-ten most-frequent words in English are prepositions (of, to, in, and for). In terms of both parsing and generation, therefore, accurate models of preposition usage are essential to avoid repeatedly making errors. Despite their frequency, however, they are notoriously difficult to master, even for humans (Chodorow, Tetreault, and Han 2007). For example, Lindstromberg (2001) estimates that less than 10% of upper-level English as a Second"
W08-1708,Towards Domain-Independent Deep Linguistic Processing: Ensuring Portability and Re-Usability of Lexicalised Grammars,2008,12,6,2,0.833333,33554,kostadin cholakov,Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks,0,"In this paper we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and at the same time ensuring maintainability and re-usability of deep lexicalised grammars. Using the error mining techniques proposed in (van Noord, 2004) we show very convincingly that the main hindrance to portability of deep lexicalised grammars to domains other than the ones originally developed in, as well as to robustness of systems using such grammars is low lexical coverage. To this effect, we develop linguistically-driven methods that use detailed morphosyntactic information to automatically enhance the performance of deep lexicalised grammars maintaining at the same time their usually already achieved high linguistic quality."
P08-2048,Mapping between Compositional Semantic Representations and Lexical Semantic Resources: Towards Accurate Deep Semantic Parsing,2008,10,2,2,0,47891,sergio roa,"Proceedings of ACL-08: HLT, Short Papers",0,"This paper introduces a machine learning method based on bayesian networks which is applied to the mapping between deep semantic representations and lexical semantic resources. A probabilistic model comprising Minimal Recursion Semantics (MRS) structures and lexicalist oriented semantic features is acquired. Lexical semantic roles enriching the MRS structures are inferred, which are useful to improve the accuracy of deep semantic parsing. Verb classes inference was also investigated, which, together with lexical semantic information provided by VerbNet and PropBank resources, can be substantially beneficial to the parse disambiguation task."
P08-1070,Enhancing Performance of Lexicalised Grammars,2008,18,4,2,0,39167,rebecca dridan,Proceedings of ACL-08: HLT,1,"This paper describes how external resources can be used to improve parser performance for heavily lexicalised grammars, looking at both robustness and efficiency. In terms of robustness, we try using different types of external data to increase lexical coverage, and find that simple POS tags have the most effect, increasing coverage on unseen data by up to 45%. We also show that filtering lexical items in a supertagging manner is very effective in increasing efficiency. Even using vanilla POS tags we achieve some efficiency gains, but when using detailed lexical types as supertags we manage to halve parsing time with minimal loss of coverage or precision."
nicholson-etal-2008-evaluating,Evaluating and Extending the Coverage of {HPSG} Grammars: A Case Study for {G}erman,2008,12,3,2,0,41188,jeremy nicholson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this work, we examine and attempt to extend the coverage of a German HPSG grammar. We use the grammar to parse a corpus of newspaper text and evaluate the proportion of sentences which have a correct attested parse, and analyse the cause of errors in terms of lexical or constructional gaps which prevent parsing. Then, using a maximum entropy model, we evaluate prediction of lexical types in the HPSG type hierarchy for unseen lexemes. By automatically adding entries to the lexicon, we observe that we can increase coverage without substantially decreasing precision."
zhang-kordoni-2008-robust,Robust Parsing with a Large {HPSG} Grammar,2008,22,7,2,1,3425,yi zhang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we propose a partial parsing model which achieves robust parsing with a large HPSG grammar. Constraint-based precision grammars, like the HPSG grammar we are using for the experiments reported in this paper, typically lack robustness, especially when applied to real world texts. To maximally recover the linguistic knowledge from an unsuccessful parse, a proper selection model must be used. Also, the efficiency challenges usually presented by the selection model must be answered. Building on the work reported in (Zhang et al., 2007), we further propose a new partial parsing model that splits the parsing process into two stages, both of which use the bottom-up chart-based parsing algorithm. The algorithm is implemented and a preliminary experiment shows promising results."
W07-1217,Partial Parse Selection for Robust Deep Processing,2007,24,12,2,1,3425,yi zhang,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"This paper presents an approach to partial parse selection for robust deep processing. The work is based on a bottom-up chart parser for HPSG parsing. Following the definition of partial parses in (Kasper et al., 1999), different partial parse selection methods are presented and evaluated on the basis of multiple metrics, from both the syntactic and semantic viewpoints. The application of the partial parsing in spontaneous speech texts processing shows promising competence of the method."
W07-1220,The Corpus and the Lexicon: Standardising Deep Lexical Acquisition Evaluation,2007,20,3,3,1,3425,yi zhang,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"This paper is concerned with the standardisation of evaluation metrics for lexical acquisition over precision grammars, which are attuned to actual parser performance. Specifically, we investigate the impact that lexicons at varying levels of lexical item precision and recall have on the performance of pre-existing broad-coverage precision grammars in parsing, i.e., on their coverage and accuracy. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JaCY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. Our results show convincingly that traditional F-score-based evaluation of lexical acquisition does not correlate with actual parsing performance. What we argue for, therefore, is a recall-heavy interpretation of F-score in designing and optimising automated lexical acquisition algorithms."
D07-1110,Validation and Evaluation of Automatically Acquired Multiword Expressions for Grammar Engineering,2007,19,50,2,0,7141,aline villavicencio,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), xefxbfxbd 2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to differentiate MWEs from non-MWEs. We then investigate the influence of the size and quality of different corpora, using the BNC and the Web search engines Google and Yahoo. We conclude that, in terms of language usage, web generated corpora are fairly similar to more carefully built corpora, like the BNC, indicating that the lack of control and balance of these corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted."
W06-1206,Automated Multiword Expression Prediction for Grammar Engineering,2006,19,36,2,1,3425,yi zhang,Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,0,"However large a hand-crafted wide-coverage grammar is, there are always going to be words and constructions that are not included in it and are going to cause parse failure. Due to their heterogeneous and flexible nature, Multiword Expressions (MWEs) provide an endless source of parse failures. As the number of such expressions in a speaker's lexicon is equiparable to the number of single word units (Jackendoff, 1997), one major challenge for robust natural language processing systems is to be able to deal with MWEs. In this paper we propose to semi-automatically detect MWE candidates in texts using some error mining techniques and validating them using a combination of the World Wide Web as a corpus and some statistical measures. For the remaining candidates possible lexico-syntactic types are predicted, and they are subsequently added to the grammar as new lexical entries. This approach provides a significant increase in the coverage of these expressions."
zhang-kordoni-2006-automated,Automated Deep Lexical Acquisition for Robust Open Texts Processing,2006,16,31,2,1,3425,yi zhang,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we report on methods to detect and repair lexical errors for deep grammars. The lack of coverage has for long been the major problem for deep processing. The existence of various errors in the hand-crafted large grammars prevents their usage in real applications. The manual detection and repair of errors requires asignificant amount of human effort. An experiment with the British National Corpus shows about 70{\%} of the sentences contain unknownword(s) for the English Resource Grammar. With the help of error mining methods, many lexical errors are discovered, which cause a large part of the parsing failures. Moreover, with a lexical type predictor based on a maximum entropy model, new lexical entries are automatically generated. The contribution of various features for the model is evaluated. With the disambiguated full parsing results, the precision of the predictor is enhanced significantly."
W05-0108,Language Technology from a {E}uropean Perspective,2005,0,1,2,0,23887,hans uszkoreit,Proceedings of the Second {ACL} Workshop on Effective Tools and Methodologies for Teaching {NLP} and {CL},0,This paper describes the cooperation of four European Universities aiming at attracting more students to European master studies in Language and Communication Technologies. The cooperation has been formally approved within the framework of the new European program Erasmus Mundus as a Specific Support Action in 2004. The consortium also aims at creating a sound basis for a joint master program in the field of language technology and computer science.
U05-1006,A Statistical Approach towards Unknown Word Type Prediction for Deep Grammars,2005,19,4,2,1,3425,yi zhang,Proceedings of the Australasian Language Technology Workshop 2005,0,"This paper presents a statistical approach to unknown word type prediction for a deep HPSG grammar. Our motivation is to enhance robustness in deep processing. With a predictor which predicts lexical types for unknown words according to the context, new lexical entries can be generated on the fly. The predictor is a maximum entropy based classifier trained on a HPSG treebank. By exploring various feature templates and the feedback from parse disambiguation results, the predictor achieves precision over 60%. The models are general enough to be applied to other constraint-based grammar formalisms."
W04-1905,Towards a Dependency-Based Gold Standard for {G}erman Parsers. The {TIGER} Dependency Bank,2004,0,22,6,0,44825,martin forst,Proceedings of the 5th International Workshop on Linguistically Interpreted Corpora,0,None
kordoni-neu-2004-creating,Creating Multi-purpose Linguistic Resources for {M}odern {G}reek: a Deep {M}odern {G}reek Grammar,2004,12,1,1,1,12066,valia kordoni,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Here we describe the development of a re-usable, multi-purpose linguistic resource for Modern Greek: a deep computational Modern Greek Grammar. The grammar is written in the HPSG formalism and is being developed in a multilingual context with MRS semantics, contributing to an open-source repository of software and linguistic resources with wide usage in education, research, and application"
E03-1044,The key role of semantics in the development of large-scale grammars of natural language,2003,6,4,1,1,12066,valia kordoni,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The aim of this paper is to show how large-scale (computational) grammars of natural language benefit from an organization of semantics which is based on Minimal Recursion Semantics (MRS; Copestake et al. (1999)). This we are doing by providing an account of valence alternations in German based on MRS, showing how such an account makes a computational grammar more efficient and less complicated for the grammar writer."
