2021.adaptnlp-1.14,On the Hidden Negative Transfer in Sequential Transfer Learning for Domain Adaptation from News to Tweets,2021,-1,-1,2,1,12382,sara meftah,Proceedings of the Second Workshop on Domain Adaptation for NLP,0,"Transfer Learning has been shown to be a powerful tool for Natural Language Processing (NLP) and has outperformed the standard supervised learning paradigm, as it takes benefit from the pre-learned knowledge. Nevertheless, when transfer is performed between less related domains, it brings a negative transfer, i.e. hurts the transfer performance. In this research, we shed light on the hidden negative transfer occurring when transferring from the News domain to the Tweets domain, through quantitative and qualitative analysis. Our experiments on three NLP taks: Part-Of-Speech tagging, Chunking and Named Entity recognition, reveal interesting insights."
2020.socialnlp-1.8,Multi-Task Supervised Pretraining for Neural Domain Adaptation,2020,-1,-1,2,1,12382,sara meftah,Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,0,"Two prevalent transfer learning approaches are used in recent works to improve neural networks performance for domains with small amounts of annotated data: Multi-task learning which involves training the task of interest with related auxiliary tasks to exploit their underlying similarities, and Mono-task fine-tuning, where the weights of the model are initialized with the pretrained weights of a large-scale labeled source domain and then fine-tuned with labeled data of the target domain (domain of interest). In this paper, we propose a new approach which takes advantage from both approaches by learning a hierarchical model trained across multiple tasks from a source domain, and is then fine-tuned on multiple tasks of the target domain. Our experiments on four tasks applied to the social media domain show that our proposed approach leads to significant improvements on all tasks compared to both approaches."
N19-1416,Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging,2019,17,0,3,1,12382,sara meftah,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Fine-tuning neural networks is widely used to transfer valuable knowledge from high-resource to low-resource domains. In a standard fine-tuning scheme, source and target problems are trained using the same architecture. Although capable of adapting to new domains, pre-trained units struggle with learning uncommon target-specific patterns. In this paper, we propose to augment the target-network with normalised, weighted and randomly initialised units that beget a better adaptation while maintaining the valuable source knowledge. Our experiments on POS tagging of social media texts (Tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets."
2019.jeptalnrecital-court.15,Exploration de l{'}apprentissage par transfert pour l{'}analyse de textes des r{\\'e}seaux sociaux (Exploring neural transfer learning for social media text analysis ),2019,-1,-1,2,1,12382,sara meftah,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"L{'}apprentissage par transfert repr{\'e}sente la capacit{\'e} qu{'}un mod{\`e}le neuronal entra{\^\i}n{\'e} sur une t{\^a}che {\`a} g{\'e}n{\'e}raliser suffisamment et correctement pour produire des r{\'e}sultats pertinents sur une autre t{\^a}che proche mais diff{\'e}rente. Nous pr{\'e}sentons dans cet article une approche fond{\'e}e sur l{'}apprentissage par transfert pour construire automatiquement des outils d{'}analyse de textes des r{\'e}seaux sociaux en exploitant les similarit{\'e}s entre les textes d{'}une langue bien dot{\'e}e (forme standard d{'}une langue) et les textes d{'}une langue peu dot{\'e}e (langue utilis{\'e}e en r{\'e}seaux sociaux). Nous avons exp{\'e}riment{\'e} notre approche sur plusieurs langues ainsi que sur trois t{\^a}ches d{'}annotation linguistique ({\'e}tiquetage morpho-syntaxique, annotation en parties du discours et reconnaissance d{'}entit{\'e}s nomm{\'e}es). Les r{\'e}sultats obtenus sont tr{\`e}s satisfaisants et montrent l{'}int{\'e}r{\^e}t de l{'}apprentissage par transfert pour tirer profit des mod{\`e}les neuronaux profonds sans la contrainte d{'}avoir {\`a} disposition une quantit{\'e} de donn{\'e}es importante n{\'e}cessaire pour avoir une performance acceptable."
W18-3927,Using Neural Transfer Learning for Morpho-syntactic Tagging of {S}outh-{S}lavic Languages Tweets,2018,0,0,2,1,12382,sara meftah,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"In this paper, we describe a morpho-syntactic tagger of tweets, an important component of the CEA List DeepLIMA tool which is a multilingual text analysis platform based on deep learning. This tagger is built for the Morpho-syntactic Tagging of Tweets (MTT) Shared task of the 2018 VarDial Evaluation Campaign. The MTT task focuses on morpho-syntactic annotation of non-canonical Twitter varieties of three South-Slavic languages: Slovene, Croatian and Serbian. We propose to use a neural network model trained in an end-to-end manner for the three languages without any need for task or domain specific features engineering. The proposed approach combines both character and word level representations. Considering the lack of annotated data in the social media domain for South-Slavic languages, we have also implemented a cross-domain Transfer Learning (TL) approach to exploit any available related out-of-domain annotated data."
W18-1203,A Comparison of Character Neural Language Model and Bootstrapping for Language Identification in Multilingual Noisy Texts,2018,0,2,4,0.952381,17314,wafia adouane,Proceedings of the Second Workshop on Subword/Character {LE}vel Models,0,"This paper seeks to examine the effect of including background knowledge in the form of character pre-trained neural language model (LM), and data bootstrapping to overcome the problem of unbalanced limited resources. As a test, we explore the task of language identification in mixed-language short non-edited texts with an under-resourced language, namely the case of Algerian Arabic for which both labelled and unlabelled data are limited. We compare the performance of two traditional machine learning methods and a deep neural networks (DNNs) model. The results show that overall DNNs perform better on labelled data for the majority categories and struggle with the minority ones. While the effect of the untokenised and unlabelled data encoded as LM differs for each category, bootstrapping, however, improves the performance of all systems and all categories. These methods are language independent and could be generalised to other under-resourced languages for which a small labelled data and a larger unlabelled data are available."
L18-1047,A Hybrid Approach for Automatic Extraction of Bilingual Multiword Expressions from Parallel Corpora,2018,0,2,1,1,12383,nasredine semmar,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1446,A Neural Network Model for Part-Of-Speech Tagging of Social Media Texts,2018,0,4,2,1,12382,sara meftah,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1575,Automatic Identification of Maghreb Dialects Using a Dictionary-Based Approach,2018,0,2,5,0.714286,30131,houda saadane,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
semmar-laib-2017-building,Building Multiword Expressions Bilingual Lexicons for Domain Adaptation of an Example-Based Machine Translation System,2017,0,1,1,1,12383,nasredine semmar,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,We describe in this paper a hybrid ap-proach to build automatically bilingual lexicons of Multiword Expressions (MWEs) from parallel corpora. We more specifically investigate the impact of using a domain-specific bilingual lexicon of MWEs on domain adaptation of an Example-Based Machine Translation (EBMT) system. We conducted experiments on the English-French language pair and two kinds of texts: in-domain texts from Europarl (European Parliament proceedings) and out-of-domain texts from Emea (European Medicines Agency documents) and Ecb (European Central Bank corpus). The obtained results indicate that integrating domain-specific bilingual lexicons of MWEs improves translation quality of the EBMT system when texts to translate are related to the specific domain and induces a relatively slight deterioration of translation quality when translating general-purpose texts.
2017.jeptalnrecital-court.8,Une approche hybride pour la construction de lexiques bilingues d{'}expressions multi-mots {\\`a} partir de corpus parall{\\`e}les (A hybrid approach to build bilingual lexicons of multiword expressions from parallel corpora),2017,-1,-1,1,1,12383,nasredine semmar,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Les expressions multi-mots jouent un r{\^o}le important dans diff{\'e}rentes applications du Traitement Automatique de la Langue telles que la traduction automatique et la recherche d{'}information interlingue. Cet article, d{'}une part, d{\'e}crit une approche hybride pour l{'}acquisition d{'}un lexique bilingue d{'}expressions multi-mots {\`a} partir d{'}un corpus parall{\`e}le anglais-fran{\c{c}}ais, et d{'}autre part, pr{\'e}sente l{'}impact de l{'}utilisation d{'}un lexique bilingue sp{\'e}cialis{\'e} d{'}expressions multi-mots produit par cette approche sur les r{\'e}sultats du syst{\`e}me de traduction statistique libre Moses. Nous avons explor{\'e} deux m{\'e}triques bas{\'e}es sur la co-occurrence pour {\'e}valuer les liens d{'}alignement entre les expressions multi-mots des langues source et cible. Les r{\'e}sultats obtenus montrent que la m{\'e}trique utilisant un dictionnaire bilingue amorce de mots simples am{\'e}liore aussi bien la qualit{\'e} de l{'}alignement d{'}expressions multi-mots que celle de la traduction."
W16-4807,{R}omanized {B}erber and {R}omanized {A}rabic Automatic Language Identification Using Machine Learning,2016,0,5,2,0.952381,17314,wafia adouane,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"The identification of the language of text/speech input is the first step to be able to properly do any language-dependent natural language processing. The task is called Automatic Language Identification (ALI). Being a well-studied field since early 1960{'}s, various methods have been applied to many standard languages. The ALI standard methods require datasets for training and use character/word-based n-gram models. However, social media and new technologies have contributed to the rise of informal and minority languages on the Web. The state-of-the-art automatic language identifiers fail to properly identify many of them. Romanized Arabic (RA) and Romanized Berber (RB) are cases of these informal languages which are under-resourced. The goal of this paper is twofold: detect RA and RB, at a document level, as separate languages and distinguish between them as they coexist in North Africa. We consider the task as a classification problem and use supervised machine learning to solve it. For both languages, character-based 5-grams combined with additional lexicons score the best, F-score of 99.75{\%} and 97.77{\%} for RB and RA respectively."
W16-4809,Automatic Detection of {A}rabicized {B}erber and {A}rabic Varieties,2016,8,7,2,0.952381,17314,wafia adouane,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"Automatic Language Identification (ALI) is the detection of the natural language of an input text by a machine. It is the first necessary step to do any language-dependent natural language processing task. Various methods have been successfully applied to a wide range of languages, and the state-of-the-art automatic language identifiers are mainly based on character n-gram models trained on huge corpora. However, there are many languages which are not yet automatically processed, for instance minority and informal languages. Many of these languages are only spoken and do not exist in a written format. Social media platforms and new technologies have facilitated the emergence of written format for these spoken languages based on pronunciation. The latter are not well represented on the Web, commonly referred to as under-resourced languages, and the current available ALI tools fail to properly recognize them. In this paper, we revisit the problem of ALI with the focus on Arabicized Berber and dialectal Arabic short texts. We introduce new resources and evaluate the existing methods. The results show that machine learning models combined with lexicons are well suited for detecting Arabicized Berber and different Arabic varieties and distinguishing between them, giving a macro-average F-score of 92.94{\%}."
W16-4821,{ASIREM} Participation at the Discriminating Similar Languages Shared Task 2016,2016,0,3,2,0.952381,17314,wafia adouane,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"This paper presents the system built by ASIREM team for the Discriminating between Similar Languages (DSL) Shared task 2016. It describes the system which uses character-based and word-based n-grams separately. ASIREM participated in both sub-tasks (sub-task 1 and sub-task 2) and in both open and closed tracks. For the sub-task 1 which deals with Discriminating between similar languages and national language varieties, the system achieved an accuracy of 87.79{\%} on the closed track, ending up ninth (the best results being 89.38{\%}). In sub-task 2, which deals with Arabic dialect identification, the system achieved its best performance using character-based n-grams (49.67{\%} accuracy), ranking fourth in the closed track (the best result being 51.16{\%}), and an accuracy of 53.18{\%}, ranking first in the open track."
C16-1044,Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks,2016,40,2,2,1,35701,othman zennaki,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This work focuses on the development of linguistic analysis tools for resource-poor languages. We use a parallel corpus to produce a multilingual word representation based only on sentence level alignment. This representation is combined with the annotated source side (resource-rich language) of the parallel corpus to train text analysis tools for resource-poor languages. Our approach is based on Recurrent Neural Networks (RNN) and has the following advantages: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. In a previous study, we proposed a method based on Simple RNN to automatically induce a Part-Of-Speech (POS) tagger. In this paper, we propose an improvement of our neural model. We investigate the Bidirectional RNN and the inclusion of external information (for instance low level information from Part-Of-Speech tags) in the RNN to train a more complex tagger (for instance, a multilingual super sense tagger). We demonstrate the validity and genericity of our method by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers."
2016.jeptalnrecital-long.7,Etude de l{'}impact d{'}un lexique bilingue sp{\\'e}cialis{\\'e} sur la performance d{'}un moteur de traduction {\\`a} base d{'}exemples (Studying the impact of a specialized bilingual lexicon on the performance of an example-based machine translation engine),2016,-1,-1,1,1,12383,nasredine semmar,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"La traduction automatique statistique bien que performante est aujourd{'}hui limit{\'e}e parce qu{'}elle n{\'e}cessite de gros volumes de corpus parall{\`e}les qui n{'}existent pas pour tous les couples de langues et toutes les sp{\'e}cialit{\'e}s et que leur production est lente et co{\^u}teuse. Nous pr{\'e}sentons, dans cet article, un prototype d{'}un moteur de traduction {\`a} base d{'}exemples utilisant la recherche d{'}information interlingue et ne n{\'e}cessitant qu{'}un corpus de textes en langue cible. Plus particuli{\`e}rement, nous proposons d{'}{\'e}tudier l{'}impact d{'}un lexique bilingue de sp{\'e}cialit{\'e} sur la performance de ce prototype. Nous {\'e}valuons ce prototype de traduction et comparons ses r{\'e}sultats {\`a} ceux du syst{\`e}me de traduction statistique Moses en utilisant les corpus parall{\`e}les anglais-fran{\c{c}}ais Europarl (European Parliament Proceedings) et Emea (European Medicines Agency Documents). Les r{\'e}sultats obtenus montrent que le score BLEU du prototype du moteur de traduction {\`a} base d{'}exemples est proche de celui du syst{\`e}me Moses sur des documents issus du corpus Europarl et meilleur sur des documents extraits du corpus Emea."
2016.jeptalnrecital-long.21,Projection Interlingue d{'}{\\'E}tiquettes pour l{'}Annotation S{\\'e}mantique Non Supervis{\\'e}e (Cross-lingual Annotation Projection for Unsupervised Semantic Tagging),2016,-1,-1,2,1,35701,othman zennaki,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Nos travaux portent sur la construction rapide d{'}outils d{'}analyse linguistique pour des langues peu dot{\'e}es en ressources. Dans une pr{\'e}c{\'e}dente contribution, nous avons propos{\'e} une m{\'e}thode pour la construction automatique d{'}un analyseur morpho-syntaxique via une projection interlingue d{'}annotations linguistiques {\`a} partir de corpus parall{\`e}les (m{\'e}thode fond{\'e}e sur les r{\'e}seaux de neurones r{\'e}currents). Nous pr{\'e}sentons, dans cet article, une am{\'e}lioration de notre mod{\`e}le neuronal, avec la prise en compte d{'}informations linguistiques externes pour un annotateur plus complexe. En particulier, nous proposons d{'}int{\'e}grer des annotations morpho-syntaxiques dans notre architecture neuronale pour l{'}apprentissage non supervis{\'e} d{'}annotateurs s{\'e}mantiques multilingues {\`a} gros grain (annotation en SuperSenses). Nous montrons la validit{\'e} de notre m{\'e}thode et sa g{\'e}n{\'e}ricit{\'e} sur l{'}italien et le fran{\c{c}}ais et {\'e}tudions aussi l{'}impact de la qualit{\'e} du corpus parall{\`e}le sur notre approche (g{\'e}n{\'e}r{\'e} par traduction manuelle ou automatique). Nos exp{\'e}riences portent sur la projection d{'}annotations de l{'}anglais vers le fran{\c{c}}ais et l{'}italien."
Y15-2013,Improving the Performance of an Example-Based Machine Translation System Using a Domain-specific Bilingual Lexicon,2015,34,1,1,1,12383,nasredine semmar,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"In this paper, we study the impact of using a domain-specific bilingual lexicon on the performance of an Example-Based Machine Translation system. We conducted experiments for the EnglishFrench language pair on in-domain texts from Europarl (European Parliament Proceedings) and out-of-domain texts from Emea (European Medicines Agency Documents), and we compared the results of the Example-Based Machine Translation system against those of the Statistical Machine Translation system Moses. The obtained results revealed that adding a domain-specific bilingual lexicon (extracted from a parallel domain-specific corpus) to the general-purpose bilingual lexicon of the Example-Based Machine Translation system improves translation quality for both in-domain as well as outof-domain texts, and the Example-Based Machine Translation system outperforms Moses when texts to translate are related to the specific domain."
Y15-1016,Unsupervised and Lightly Supervised Part-of-Speech Tagging Using Recurrent Neural Networks,2015,32,10,2,1,35701,othman zennaki,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we propose a novel approach to induce automatically a Part-Of-Speech (POS) tagger for resource-poor languages (languages that have no labeled training data). This approach is based on cross-language projection of linguistic annotations from parallel corpora without the use of word alignment information. Our approach does not assume any knowledge about foreign languages, making it applicable to a wide range of resource-poor languages. We use Recurrent Neural Networks (RNNs) as multilingual analysis tool. Our approach combined with a basic cross-lingual projection method (using word alignment information) achieves comparable results to the state-of-the-art. We also use our approach in a weakly supervised context, and it shows an excellent potential for very low-resource settings (less than 1k training utterances)."
R15-1075,Evaluating the Impact of Using a Domain-specific Bilingual Lexicon on the Performance of a Hybrid Machine Translation Approach,2015,31,3,1,1,12383,nasredine semmar,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"This paper describes an Example-Based Machine Translation prototype and presents an evaluation of the impact of using a domainspecific vocabulary on its performance. This prototype is based on a hybrid approach which needs only monolingual texts in the target language and consists to combine translation candidates returned by a cross-language search engine with translation hypotheses provided by a finite-state transducer. The results of this combination are evaluated against a statistical language model of the target language in order to obtain the n-best translations. To measure the performance of this hybrid approach, we achieved several experiments using corpora on two domains from the European Parliament proceedings (Europarl) and the European Medicines Agency documents (Emea). The obtained results show that the proposed approach outperforms the state-of-the-art Statistical Machine Translation system Moses when texts to translate are related to the specialized domain."
2015.jeptalnrecital-court.32,Utilisation des r{\\'e}seaux de neurones r{\\'e}currents pour la projection interlingue d{'}{\\'e}tiquettes morpho-syntaxiques {\\`a} partir d{'}un corpus parall{\\`e}le,2015,-1,-1,2,1,35701,othman zennaki,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La construction d{'}outils d{'}analyse linguistique pour les langues faiblement dot{\'e}es est limit{\'e}e, entre autres, par le manque de corpus annot{\'e}s. Dans cet article, nous proposons une m{\'e}thode pour construire automatiquement des outils d{'}analyse via une projection interlingue d{'}annotations linguistiques en utilisant des corpus parall{\`e}les. Notre approche n{'}utilise pas d{'}autres sources d{'}information, ce qui la rend applicable {\`a} un large {\'e}ventail de langues peu dot{\'e}es. Nous proposons d{'}utiliser les r{\'e}seaux de neurones r{\'e}currents pour projeter les annotations d{'}une langue {\`a} une autre (sans utiliser d{'}information d{'}alignement des mots). Dans un premier temps, nous explorons la t{\^a}che d{'}annotation morpho-syntaxique. Notre m{\'e}thode combin{\'e}e avec une m{\'e}thode de projection d{'}annotation basique (utilisant l{'}alignement mot {\`a} mot), donne des r{\'e}sultats comparables {\`a} ceux de l{'}{\'e}tat de l{'}art sur une t{\^a}che similaire."
F14-1024,Study of the impact of proper name transliteration on the performance of word alignment in {F}rench-{A}rabic parallel corpora (Etude de l{'}impact de la translitt{\\'e}ration de noms propres sur la qualit{\\'e} de l{'}alignement de mots {\\`a} partir de corpus parall{\\`e}les fran{\\c{c}}ais-arabe) [in {F}rench],2014,-1,-1,1,1,12383,nasredine semmar,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
2014.tc-1.4,Using cross-language information retrieval and statistical language modelling in example-based machine translation,2014,-1,-1,1,1,12383,nasredine semmar,Proceedings of Translating and the Computer 36,0,None
W13-2503,Using {W}ord{N}et and Semantic Similarity for Bilingual Terminology Mining from Comparable Corpora,2013,23,8,2,1,35102,dhouha bouamor,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study of the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors. For this purpose, we augment the standard approach by a Word Sense Disambiguation process relying on a WordNet-based semantic similarity measure. The aim of this process is to identify the translations that are more likely to give the best representation of words in the target language. On two specialized French-English comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach."
P13-2133,Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora,2013,19,16,2,1,35102,dhouha bouamor,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches."
I13-1125,Building Specialized Bilingual Lexicons Using Word Sense Disambiguation,2013,17,4,2,1,35102,dhouha bouamor,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach."
I13-1139,Using Transliteration of Proper Names from {A}rabic to {L}atin Script to Improve {E}nglish-{A}rabic Word Alignment,2013,21,1,1,1,12383,nasredine semmar,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Bilingual lexicons of proper names play a vital role in machine translation and cross-language information retrieval. Word alignment approaches are generally used to construct bilingual lexicons automatically from parallel corpora. Aligning proper names is a task particularly difficult when the source and target languages of the parallel corpus do not share a same written script. We present in this paper a system to transliterate automatically proper names from Arabic to Latin script, and a tool to align single and compound words from English-Arabic parallel texts. We particularly focus on the impact of using transliteration to improve the performance of the word alignment tool. We have evaluated the word alignment tool integrating transliteration of proper names from Arabic to Latin script using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the open source statistical machine translation system Moses. Experiments show that integrating transliteration of proper names into the alignment process improves the Fmeasure of word alignment from 72% to 81% and the translation BLEU score from 20.15% to 20.63%."
F13-1024,(Utilisation de la similarit{\\'e} s{\\'e}mantique pour l{'}extraction de lexiques bilingues {\\`a} partir de corpus comparables) [in {F}rench],2013,0,2,2,1,35102,dhouha bouamor,Proceedings of TALN 2013 (Volume 1: Long Papers),0,"This paper presents a new method that aims to improve the results of the standard approach used for bilingual lexicon extraction from specialized comparable corpora. We attempt to solve the problem of context vector word polysemy. Instead of using all the entries of the dictionary to translate a context vector, we only use the words of the lexicon that are more likely to give the best characterization of context vectors in the target language. On two specialised French-English comparable corpora, empirical experimental results show that our method improves the results obtained by the standard approach especially when many words are ambiguous. MOTS-CLES : lexique bilingue, corpus comparable specialise, desambiguisation semantique, WordNet."
D13-1046,Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge,2013,18,8,3,1,35102,dhouha bouamor,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested."
2013.mtsummit-papers.18,Towards a Generic Approach for Bilingual Lexicon Extraction from Comparable Corpora,2013,-1,-1,2,1,35102,dhouha bouamor,Proceedings of Machine Translation Summit XIV: Papers,0,None
W12-5108,Automatic Construction of a {M}ulti{W}ord Expressions Bilingual Lexicon: A Statistical Machine Translation Evaluation Perspective,2012,26,11,2,1,35102,dhouha bouamor,Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon,0,"Identifying and translating MultiWord Expressions (MWES) in a text represent a key issue for numerous applications of Natural Language Processing (NLP), especially for Machine Translation (MT). In this paper, we present a method aiming to construct a bilingual lexicon of MWES from a French-English parallel corpus. In order to assess the quality of the mined lexicon, a Statistical Machine Translation (SMT) task-based evaluation is conducted. We investigate the performance of three dynamic strategies and of one static strategy to integrate the mined bilingual MWES lexicon in a SMT system. Experimental results shows that such a lexicon improves the quality of translation."
bouamor-etal-2012-identifying,Identifying bilingual Multi-Word Expressions for Statistical Machine Translation,2012,23,37,2,1,35102,dhouha bouamor,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"MultiWord Expressions (MWEs) repesent a key issue for numerous applications in Natural Language Processing (NLP) especially for Machine Translation (MT). In this paper, we describe a strategy for detecting translation pairs of MWEs in a French-English parallel corpus. In addition we introduce three methods aiming to integrate extracted bilingual MWE S in M OSES, a phrase based Statistical Machine Translation (SMT) system. We experimentally show that these textual units can improve translation quality."
F12-2010,Utilisation de la translitt{\\'e}ration arabe pour l{'}am{\\'e}lioration de l{'}alignement de mots {\\`a} partir de corpus parall{\\`e}les fran{\\c{c}}ais-arabe (Using {A}rabic Transliteration to Improve Word Alignment from {F}rench-{A}rabic Parallel Corpora) [in {F}rench],2012,0,0,2,0.714286,30131,houda saadane,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
2012.amta-caas14.6,Using {A}rabic Transliteration to Improve Word Alignment from {F}rench- {A}rabic Parallel Corpora,2012,-1,-1,3,0.714286,30131,houda saadane,Fourth Workshop on Computational Approaches to Arabic-Script-based Languages,0,"In this paper, we focus on the use of Arabic transliteration to improve the results of a linguistics-based word alignment approach from parallel text corpora. This approach uses, on the one hand, a bilingual lexicon, named entities, cognates and grammatical tags to align single words, and on the other hand, syntactic dependency relations to align compound words. We have evaluated the word aligner integrating Arabic transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the Moses statistical machine translation system. The obtained results show that Arabic transliteration improves the quality of both alignment and translation."
besancon-etal-2010-lima,{LIMA} : A Multilingual Framework for Linguistic Analysis and Linguistic Resources Development and Evaluation,2010,17,31,7,0,5626,romaric besanccon,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The increasing amount of available textual information makes necessary the use of Natural Language Processing (NLP) tools. These tools have to be used on large collections of documents in different languages. But NLP is a complex task that relies on many processes and resources. As a consequence, NLP tools must be both configurable and efficient: specific software architectures must be designed for this purpose. We present in this paper the LIMA multilingual analysis platform, developed at CEA LIST. This configurable platform has been designed to develop NLP based industrial applications while keeping enough flexibility to integrate various processes and resources. This design makes LIMA a linguistic analyzer that can handle languages as different as French, English, German, Arabic or Chinese. Beyond its architecture principles and its capabilities as a linguistic analyzer, LIMA also offers a set of tools dedicated to the test and the evaluation of linguistic modules and to the production and the management of new linguistic resources."
cruz-lara-etal-2010-mlif,{MLIF} : A Metamodel to Represent and Exchange Multilingual Textual Information,2010,2,4,4,0,39029,samuel cruzlara,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The fast evolution of language technology has produced pressing needs in standardization. The multiplicity of language resources representation levels and the specialization of these representations make difficult the interaction between linguistic resources and components manipulating these resources. In this paper, we describe the MultiLingual Information Framework (MLIF â ISO CD 24616). MLIF is a metamodel which allows the representation and the exchange of multilingual textual information. This generic metamodel is designed to provide a common platform for all the tools developed around the existing multilingual data exchange formats. This platform provides, on the one hand, a set of generic data categories for various application domains, and on the other hand, strategies for the interoperability with existing standards. The objective is to reach a better convergence between heterogeneous standardisation activities that are taking place in the domain of data modeling (XML; W3C), text management (TEI; TEIC), multilingual information (TMX-LISA; XLIFF-OASIS) and multimedia (SMILText; W3C). This is a work in progress within ISO-TC37 in order to define a new ISO standard."
2010.tc-1.14,A hybrid word alignment approach to improve translation lexicons with compound words and idiomatic expressions,2010,10,4,1,1,12383,nasredine semmar,Proceedings of Translating and the Computer 32,0,"In this paper, we present a hybrid approach to align single words, compound words and idiomatic expressions from bilingual parallel corpora. The objective is to develop, improve and maintain automatically translation lexicons. This approach combines linguistic and statistical information in order to improve word alignment results. The linguistic improvements taken into account refer to the use of an existing bilingual lexicon, named entities recognition, grammatical tags matching and detection of syntactic dependency relations between words. Statistical information refer to the number of occurrences of repeated words, their positions in the parallel corpus and their lengths in terms of number of characters. Single-word alignment uses an existing bilingual lexicon, named entities and cognates detection and grammatical tags matching. Compound-word alignment consists in establishing correspondences between the compound words of the source sentence and the compound words of the target sentences. A syntactic analysis is applied on the source and target sentences in order to extract dependency relations between words and to recognize compound words. Idiomatic expressions alignment starts with a monolingual term extraction for each of the source and target languages, which provides a list of sequences of repeated words and a list of potential translations. These sequences are represented with vectors which indicate their numbers of occurrences and the numbers of segments in which they appear. Then, the translation relation between the source and target expressions are evaluated with a distance metric. The single and compound word aligners have been evaluated on a subset of 1103 sentences in English and French of the JOC (Official Journal of the European Community) corpus. The obtained results showed that these aligners generate a translation lexicon with 90 % of precision for single words and 84 % of precision for compound words. We evaluated the idiomatic expressions aligner on a subset of the Canadian Parliament Hansard corpus and we obtained a precision of 81%."
W07-0810,{A}rabic to {F}rench Sentence Alignment: Exploration of A Cross-language Information Retrieval Approach,2007,15,8,1,1,12383,nasredine semmar,Proceedings of the 2007 Workshop on Computational Approaches to {S}emitic Languages: Common Issues and Resources,0,"Sentence alignment consists in estimating which sentence or sentences in the source language correspond with which sentence or sentences in a target language. We present in this paper a new approach to aligning sentences from a parallel corpus based on a cross-language information retrieval system. This approach consists in building a database of sentences of the target text and considering each sentence of the source text as a query to that database. The cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents to be indexed. This system is composed of a multilingual linguistic analyzer, a statistical analyzer, a reformulator, a comparator and a search engine. The multilingual linguistic analyzer includes a morphological analyzer, a part-of-speech tagger and a syntactic analyzer. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The statistical analyzer computes for documents to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands queries during the search. The expansion is used to infer from the original query words other words expressing the same concepts. The search engine retrieves the ranked, relevant documents from the indexes according to the corresponding reformulated query and then merges the results obtained for each language, taking into account the original words of the query and their weights in order to score the documents. The sentence aligner has been evaluated on the MD corpus of the ARCADE II project which is composed of news articles from the French newspaper Le Monde Diplomatique. The part of the corpus used in evaluation consists of the same subset of sentences in Arabic and French. Arabic sentences are aligned to their French counterparts. Results showed that alignment has correct precision and recall even when the corpus is not completely parallel (changes in sentence order or missing sentences)."
2007.jeptalnrecital-long.38,Utilisation d{'}une approche bas{\\'e}e sur la recherche cross-lingue d{'}information pour l{'}alignement de phrases {\\`a} partir de textes bilingues Arabe-Fran{\\c{c}}ais,2007,-1,-1,1,1,12383,nasredine semmar,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}alignement de phrases {\`a} partir de textes bilingues consiste {\`a} reconna{\^\i}tre les phrases qui sont traductions les unes des autres. Cet article pr{\'e}sente une nouvelle approche pour aligner les phrases d{'}un corpus parall{\`e}le. Cette approche est bas{\'e}e sur la recherche crosslingue d{'}information et consiste {\`a} construire une base de donn{\'e}es des phrases du texte cible et consid{\'e}rer chaque phrase du texte source comme une requ{\^e}te {\`a} cette base. La recherche crosslingue utilise un analyseur linguistique et un moteur de recherche. L{'}analyseur linguistique traite aussi bien les documents {\`a} indexer que les requ{\^e}tes et produit un ensemble de lemmes normalis{\'e}s, un ensemble d{'}entit{\'e}s nomm{\'e}es et un ensemble de mots compos{\'e}s avec leurs {\'e}tiquettes morpho-syntaxiques. Le moteur de recherche construit les fichiers invers{\'e}s des documents en se basant sur leur analyse linguistique et retrouve les documents pertinents {\`a} partir de leur indexes. L{'}aligneur de phrases a {\'e}t{\'e} {\'e}valu{\'e} sur un corpus parall{\`e}le Arabe-Fran{\c{c}}ais et les r{\'e}sultats obtenus montrent que 97{\%} des phrases ont {\'e}t{\'e} correctement align{\'e}es."
semmar-etal-2006-deep,A Deep Linguistic Analysis for Cross-language Information Retrieval,2006,6,2,1,1,12383,nasredine semmar,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Cross-language information retrieval consists in providing a query in one language and searching documents in one or different languages. These documents are ordered by the probability of being relevant to the user's request. The highest ranked document is considered to be the most likely relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents. This system is composed of a linguistic analyzer, a statistic analyzer, a reformulator, a comparator and a search engine. The linguistic analysis processes both documents to be indexed and queries to extract concepts representing their content. This analysis includes a morphological analysis, a part-of-speech tagging and a syntactic analysis. In this paper, we present the deep linguistic analysis used in the LIC2M cross-lingual search engine and we will particularly focus on the impact of the syntactic analysis on the retrieval effectiveness."
chiao-etal-2006-evaluation,Evaluation of multilingual text alignment systems: the {ARCADE} {II} project,2006,4,21,5,0,50126,yunchuang chiao,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the ARCADE II project, concerned with the evaluation of parallel text alignment systems. The ARCADE II project aims at exploring the techniques of multilingual text alignment through a fine evaluation of the existing techniques and the development of new alignment methods. The evaluation campaign consists of two tracks devoted to the evaluation of alignment at sentence and word level respectively. It differs from ARCADE I in the multilingual aspect and the investigation of lexical alignment."
2006.jeptalnrecital-long.29,Using Stemming in Morphological Analysis to Improve {A}rabic Information Retrieval,2006,-1,-1,1,1,12383,nasredine semmar,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Information retrieval (IR) consists in finding all relevant documents for a user query in a collection of documents. These documents are ordered by the probability of being relevant to the user{'}s query. The highest ranked document is considered to be the most likely relevant document. Natural Language Processing (NLP) for IR aims to transform the potentially ambiguous words of queries and documents into unambiguous internal representations on which matching and retrieval can take place. This transformation is generally achieved by several levels of linguistic analysis, morphological, syntactic and so forth. In this paper, we present the Arabic linguistic analyzer used in the LIC2M cross-lingual search engine. We focus on the morphological analyzer and particularly the clitic stemmer which segments the input words into proclitics, simple forms and enclitics. We demonstrate that stemming improves search engine recall and precision."
2006.bcs-1.9,Using Cross-language Information Retrieval for Sentence Alignment,2006,-1,-1,1,1,12383,nasredine semmar,Proceedings of the International Conference on the Challenge of Arabic for NLP/MT,0,"Cross-language information retrieval consists in providing a query in one language and searching documents in different languages. Retrieved documents are ordered by the probability of being relevant to the user's request with the highest ranked being considered the most relevant document. The LIC2M cross-language information retrieval system is a weighted Boolean search engine based on a deep linguistic analysis of the query and the documents to be indexed. This system, designed to work on Arabic, Chinese, English, French, German and Spanish, is composed of a multilingual linguistic analyzer, a statistical analyzer, a reformulator, a comparator and a search engine. The multilingual linguistic analyzer includes a morphological analyzer, a part-of-speech tagger and a syntactic analyzer. In the case of Arabic, a clitic stemmer is added to the morphological analyzer to segment the input words into proclitics, simple forms and enclitics. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The statistical analyzer computes for documents to be indexed concept weights based on concept database frequencies. The comparator computes intersections between queries and documents and provides a relevance weight for each intersection. Before this comparison, the reformulator expands queries during the search. The expansion is used to infer from the original query words other words expressing the same concepts. The expansion can be in the same language or in different languages. The search engine retrieves the ranked, relevant documents from the indexes according to the corresponding reformulated query and then merges the results obtained for each language, taking into account the original words of the query and their weights in order to score the documents. Sentence alignment consists in estimating which sentence or sentences in the source language correspond with which sentence or sentences in a target language. We present in this paper a new approach to aligning sentences from a parallel corpora based on the LIC2M cross-language information retrieval system. This approach consists in building a database of sentences of the target text and considering each sentence of the source text as a ``query'' to that database. The aligned bilingual parallel corpora can be used as a translation memory in a computer-aided translation tool."
W05-0705,Modifying a Natural Language Processing System for {E}uropean Languages to Treat {A}rabic in Information Processing and Information Retrieval Applications,2005,12,9,2,0,34941,gregory grefenstette,Proceedings of the {ACL} Workshop on Computational Approaches to {S}emitic Languages,0,"The goal of many natural language processing platforms is to be able to someday correctly treat all languages. Each new language, especially one from a new language family, provokes some modification and design changes. Here we present the changes that we had to introduce into our platform designed for European languages in order to handle a Semitic language. Treatment of Arabic was successfully integrated into our cross language information retrieval system, which is visible online."
