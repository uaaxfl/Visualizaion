2020.lrec-1.36,D18-1547,0,0.245799,"lex tasks. This framework uses semi-guided dialogue to avoid interactions that breach procedures and processes only known to experts, while enabling the capture of a wide variety of interactions. The framework is available at https://github.com/JChiyah/crwiz. Keywords: Wizard-of-Oz, Data Collection, Crowdsourcing, Dialogue System 1. Introduction Recent machine learning breakthroughs in dialogue systems and their respective components have been made possible by training on publicly available large scale datasets, such as ConvAI (Logacheva et al., 2018), bAbI (Weston et al., 2016) and MultiWoZ (Budzianowski et al., 2018), many of which are collected on crowdsourcing services, such as Amazon Mechanical Turk and Figure-eight. These data collection methods have the benefits of being cost-effective, time-efficient to collect and scalable, enabling the collection of large numbers of dialogues. Where this crowdsourcing method has its limitations is when specific domain expert knowledge is required, rather than general conversation. These tasks include, for example, call centre agents (Peskov et al., 2019) or clerks with access to a database, as is required for tourism information and booking (Budzianowski et al., 2"
2020.lrec-1.36,D19-1459,0,0.0831648,"be achieved simply by letting two non-experts converse with free text. Therefore, in recent data collections, there have been a number of attempts to control the data quality in order to produce a desired behaviour. For example, in (El Asri et al., 2017), the data collection was done with a limited number of subjects who performed the task several days in a row, behaving both as the Wizard and the customer of a travel agency. The same idea was followed in (Wei et al., 2018), where a number of participants took part in the data collection over a period of 6 months and, in (Peskov et al., 2019; Byrne et al., 2019) where a limited number of subjects were trained to be the Wizard. This quality control, however, naturally comes with the cost of recruiting and paying these subjects accordingly. The solution we propose in this paper tries to minimise these costs by increasing the pool of Wizards to anyone wanting to collaborate in the data collection, by providing them the necessary guidance to generate the desired dialogue behaviour. This is a valuable solution for collecting dialogues in domains where specific expertise is required and the cost of training capable Wizards is high. We required fine-grained"
2020.lrec-1.36,W17-5526,0,0.0402605,"Missing"
2020.lrec-1.36,W17-5506,0,0.146172,", in most cases these datasets are aimed at creating resources to train the conversational system itself. Self-authoring the dialogues (Krause et al., 2017) or artificially creating data (Weston et al., 2016) could be a solution to rapidly collect data, but this solution has been shown to produce low quality unnatural data (Jonell et al., 2019). One way to mitigate the necessity of pairing two users simultaneously is to allow several participants to contribute to the dialogue, one turn at the time. This approach has been used both in task-oriented (Wen et al., 2017; Budzianowski et al., 2018; Eric and Manning, 2017) and chitchat (Jonell et al., 2019). This means that the same dialogue can be authored by several participants. However, this raises issues in terms of coherence and forward-planning. These can be addressed by carefully designing the data collection to provide the maximum amount of information to the participants (e.g. providing the task, personality traits of the bot, goals, etc.) but then this adds to cognitive load, time, cost and participant fatigue. Pairing is a valid option, which has been used in a number of recent data collections in various domains, such as navigating in a city (de Vr"
2020.lrec-1.36,P17-1162,0,0.0478524,"Missing"
2020.lrec-1.36,W19-1601,1,0.887483,"Missing"
2020.lrec-1.36,C08-3005,1,0.689394,"icipants not collaborating and these dialogues had to be discarded as they were not of use5 . 5.1. Future Work In future work, we want to expand and improve the platform. Dialogue system development can greatly benefit from better ways of obtaining data for rich task-oriented domains such as ours. Part of fully exploiting the potential of crowdsourcing services lies in having readily available tools that help in the generation and gathering of data. One such tool would be a method to take a set of rules, procedures or business processes and automatically convert to a FSM, in a similar way to (Lemon et al., 2008), ready to be uploaded to the Wizard interface. Regarding quality and coherence, dialogues are particularly challenging to automatically rate. In our data collection, there was not a correct or wrong dialogue option for the messages that the Emergency Assistant sent during the conversation, but some were better than others depending on the 4 There was no live connection with the simulated physical environment implemented. 5 Participants who collaborated still received the full payment regardless of their partner’s behaviour. 294 Dialogues Collected (145) Q1. Q2. Q3. Q4. Partner collaboration I"
2020.lrec-1.36,D17-1259,0,0.0943691,"that the same dialogue can be authored by several participants. However, this raises issues in terms of coherence and forward-planning. These can be addressed by carefully designing the data collection to provide the maximum amount of information to the participants (e.g. providing the task, personality traits of the bot, goals, etc.) but then this adds to cognitive load, time, cost and participant fatigue. Pairing is a valid option, which has been used in a number of recent data collections in various domains, such as navigating in a city (de Vries et al., 2018), playing a negotiation game (Lewis et al., 2017), talking about a person (He et al., 2017), playing an image game (Manuvinakurike and DeVault, 2015) or having a chat about a particular image that is shown to both participants (Ilinykh et al., 2019; Das et al., 2017). Pairing frameworks exist such as Slurk (Schlangen et al., 2018). Besides its pairing management feature, Slurk is designed in order to allow researchers to modify it and implement their own data collection rapidly. The scenarios for the above-mentioned data collections are mostly intuitive tasks that humans do quite regularly, unlike our use-case scenario of emergency response."
2020.lrec-1.36,D19-1460,0,0.138378,"e large scale datasets, such as ConvAI (Logacheva et al., 2018), bAbI (Weston et al., 2016) and MultiWoZ (Budzianowski et al., 2018), many of which are collected on crowdsourcing services, such as Amazon Mechanical Turk and Figure-eight. These data collection methods have the benefits of being cost-effective, time-efficient to collect and scalable, enabling the collection of large numbers of dialogues. Where this crowdsourcing method has its limitations is when specific domain expert knowledge is required, rather than general conversation. These tasks include, for example, call centre agents (Peskov et al., 2019) or clerks with access to a database, as is required for tourism information and booking (Budzianowski et al., 2018). In the near future, there will be a demand to extend this to workplace-specific tasks and procedures. Therefore, a method of gathering crowdsourced dialogue data is needed that ensures compliance with such procedures, whilst providing coverage of a wide variety of dialogue phenomena that could be observed in deployment of a trained dialogue system. Wizard-of-Oz data collections in the past have provided such a mechanism. However, these have traditionally not been scalable becau"
2020.lrec-1.36,P19-1566,0,0.0204535,"lar image that is shown to both participants (Ilinykh et al., 2019; Das et al., 2017). Pairing frameworks exist such as Slurk (Schlangen et al., 2018). Besides its pairing management feature, Slurk is designed in order to allow researchers to modify it and implement their own data collection rapidly. The scenarios for the above-mentioned data collections are mostly intuitive tasks that humans do quite regularly, unlike our use-case scenario of emergency response. Role playing is one option. For example, recent work has tried to create datasets for non-collaborative scenarios (Li et al., 2019; Wang et al., 2019), requesting participants to incarnate a particular role during the data collection. This is particularly challenging when the recruitment is done via a crowdsourcing platform. In (Wang et al., 2019), the motivation for the workers to play the role is intrinsic to the scenario. In this data collection, one of the participants tries to persuade their partner to contribute to a charity with a certain amount of money. As a result of their dialogue, the money that the persuadee committed to donate was actually donated to a charity organising. However, for scenarios such as ours, the role playing r"
2020.lrec-1.36,D18-1419,0,0.0602612,"ever, for scenarios such as ours, the role playing requires a certain expertise and it is questionable whether the desired behaviour would be achieved simply by letting two non-experts converse with free text. Therefore, in recent data collections, there have been a number of attempts to control the data quality in order to produce a desired behaviour. For example, in (El Asri et al., 2017), the data collection was done with a limited number of subjects who performed the task several days in a row, behaving both as the Wizard and the customer of a travel agency. The same idea was followed in (Wei et al., 2018), where a number of participants took part in the data collection over a period of 6 months and, in (Peskov et al., 2019; Byrne et al., 2019) where a limited number of subjects were trained to be the Wizard. This quality control, however, naturally comes with the cost of recruiting and paying these subjects accordingly. The solution we propose in this paper tries to minimise these costs by increasing the pool of Wizards to anyone wanting to collaborate in the data collection, by providing them the necessary guidance to generate the desired dialogue behaviour. This is a valuable solution for co"
2020.lrec-1.36,E17-1042,0,0.191085,"er since only one partner is lacking. However, in most cases these datasets are aimed at creating resources to train the conversational system itself. Self-authoring the dialogues (Krause et al., 2017) or artificially creating data (Weston et al., 2016) could be a solution to rapidly collect data, but this solution has been shown to produce low quality unnatural data (Jonell et al., 2019). One way to mitigate the necessity of pairing two users simultaneously is to allow several participants to contribute to the dialogue, one turn at the time. This approach has been used both in task-oriented (Wen et al., 2017; Budzianowski et al., 2018; Eric and Manning, 2017) and chitchat (Jonell et al., 2019). This means that the same dialogue can be authored by several participants. However, this raises issues in terms of coherence and forward-planning. These can be addressed by carefully designing the data collection to provide the maximum amount of information to the participants (e.g. providing the task, personality traits of the bot, goals, etc.) but then this adds to cognitive load, time, cost and participant fatigue. Pairing is a valid option, which has been used in a number of recent data collections in"
2020.lrec-1.36,P17-1062,0,0.0759234,"Missing"
2021.eacl-main.202,E14-1074,1,0.65691,"Missing"
2021.eacl-main.202,H89-2010,0,0.278343,"Missing"
2021.eacl-main.202,N19-1423,0,0.0165723,"Missing"
2021.eacl-main.202,hastie-belz-2014-comparative,1,0.804114,". Inter-annotator agreement measured by Krippendorff’s Alpha 4 NLG Evaluation Metrics Here, we describe the reasoning behind our choice of subjective measures that attempt to capture both the content and its correctness (Informativeness) and quality of expression (Clarity). We also describe objective measures commonly used for automatic evaluation of NLG, and which we will extract from the ExBAN corpus. 4.1 Subjective NLG Evaluation Metrics Human evaluation is considered a primary evaluation criterion for NLG systems (Gatt and Krahmer, 2018; Mellish and Dale, 1998; Gkatzia and Mahamood, 2015; Hastie and Belz, 2014). Through Explainable AI, we want to achieve Clarity and understanding in communicating the process of AI systems. Therefore, explanations should be clear and easily understood by users. Traditional human evaluation metrics are clearly needed for increasing transparency, avoiding confusion and misunderstanding. Informativeness. As defined in the field of NLG, Informativeness targets relevance or correctness of an NLG output relative to an input (Duˇsek et al., 2020). According to the literature, Informativeness can provide “timely, relevant and useful information” (Novikova et al., 2018) and “"
2021.eacl-main.202,P19-1346,0,0.0128154,"uation of text accuracy 2377 is indeed related to explanations because any explanation must contain enough statements to support decision-making and understanding. These statements should be accurate and true. in a two step process: (1) NL explanations were produced by human subjects; (2) in a separate study, these explanations were evaluated in terms of Informativeness and Clarity. The growing interest in the AI community to investigate the potential of NL explanations for bridging the gap between AI and HCI has resulted in an increasing number of NL explanations datasets. The ELI5 dataset1 (Fan et al., 2019) is composed of explanations represented as multisentence answers for diverse questions where users are encouraged to provide answers, which are comprehensible for a five-year-old. WorldTree V22 (Jansen et al., 2019) is a corpus of ScienceDomain that contains explanation graphs for elementary science questions, where explanations represent interconnected sets of facts. CoSE3 is a dataset of human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations (Rajani et al., 2019). Multimodal Explanations Datasets (VQA-X and ACT-X) contain textual a"
2021.eacl-main.202,hardcastle-scott-2008-evaluate,0,0.0558737,"red to one or more “Gold Standard” text as inspired by the Machine Translation community and adopted for evaluating document summarisation and NLG (Belz and Reiter, 2006). The gold standard is normally a piece of natural language text, annotated by humans as correct, i.e. a solution for a given task. Automatic evaluation is based on this gold standard, by verifying potential similarity (Kov´aˇr et al., 2016). However, the selection of gold standards involves subjectivity and specificity (Kov´aˇr et al., 2016), and this is part of the reason that automatic metrics have received some criticism (Hardcastle and Scott, 2008). BLEU (B) (Papineni et al., 2001) is widely used in the field of NLG and compares n-grams of a candidate text (e.g. that generated by an algorithm) with the n-grams of a reference text. The number of matches defines the goodness of the candidate text. SacreBLEU (SB) (Post, 2018) is a new version of BLEU that calculates scores on the detokenized text. METEOR (M) was created to try to address BLEU&apos;s weaknesses (Lavie and Agarwal, 2007). METEOR evaluates text by computing a score based on explicit word-to-word matches between a candidate and a reference. When using multiple references, the candi"
2021.eacl-main.202,L18-1433,0,0.0452618,"Missing"
2021.eacl-main.202,D14-1181,0,0.00250424,"any generalisations about this subset of the data. However, it does seem to be the case that BLEURT is more sensitive to Informativeness than Clarity (e.g. explanation 7 vs 8-9 in the table), but a larger study would be needed to show this empirically. 6 Conclusions and Future work Human evaluation is an expensive and timeconsuming process. On the other hand, automatic evaluation is a cheaper and more efficient method for evaluating NLG systems. However, finding accurate measures is challenging, particularly for explanations. We have discussed word embedding techniques (Mikolov et al., 2013; Kim, 2014; Reimers and Gurevych, 2020), which enable the use of pre-trained models and so reduces the need to collect large amounts of data in our domain of explanations, which is a challenging task. The embedding-based metrics mentioned here perform better than the word-overlap based ones. We speculate that this is in part due to the fact that the former capture semantics more effectively and are thus more invariant to paraphrases. These metrics have also been shown to be useful across multiple tasks (Sellam et al., 2020) but with some variation across datasets (Novikova et al., 2017). Therefore, futu"
2021.eacl-main.202,N19-1238,0,0.016704,"a of adapting explanations to the explainee’s knowledge and expertise level, as well as the explainer’s goals and intentions. One such goal of the explainer could be to maximise the trustworthiness of the explanation (Ribeiro et al., 2016). How this aspect is consistently subjectively and objectively measured will be an interesting area of investigation. Finally, the ExBAN corpus and this study will inform the development of NLG algorithms for NL explanations from graphical representations. We will explore NLG techniques for structured data, such as graph neural networks and knowledge graphs (Koncel-Kedziorski et al., 2019). Thus the corpus and metrics discussed here will contribute to a variety of fields linguistics, cognitive science as well as NLG and Explainable AI. 2383 Acknowledgments This work was supported by the EPSRC Centre for Doctoral Training in Robotics and Autonomous Systems at Heriot-Watt University and the University of Edinburgh. Clinciu’s PhD is funded by Schlumberger Cambridge Research Limited (EP/L016834/1, 2018-2021). This work was also supported by the EPSRC ORCA Hub (EP/R026173/1, 2017-2021) and UKRI Trustworthy Autonomous Systems Node on Trust (EP/V026682/1, 2020-2024). References Vijay"
2021.eacl-main.202,2020.acl-main.771,0,0.0709954,"ions has been emphasised by researchers within the social cognitive sciences (Leake, 2014; Zemla et al., 2017; Doshi-Velez and Kim, 2017). To date, explanations have mostly been evaluated by collecting human judgements, which is both time-consuming and costly. Here, we view generating explanations as a special case of Natural Language Generation (NLG), and so we explore mapping existing automatic evaluation methods for NLG onto explanations. We study whether general, domain-independent evaluation metrics within NLG are sensitive enough to capture the peculiarities inherent in NL explanations (Kumar and Talukdar, 2020), such as causality; or whether NL explanations constitute a sui-generis category, thus requiring their own automatic evaluation methods and criteria. As transparency becomes key for robotics and AI, it will be necessary to evaluate the methods through which transparency is provided, including automatically generated natural language (NL) explanations. Here, we explore parallels between the generation of such explanations and the much-studied field of evaluation of Natural Language Generation (NLG). Specifically, we investigate which of the NLG evaluation measures map well to explanations. We"
2021.eacl-main.202,W13-2106,0,0.0111322,"ova et al., 2018) and “make information immediately accessible” (Maxwell et al., 2017). Sometimes, Informativeness is linked with accuracy, or adequacy (Novikova et al., 2018). As mentioned previously, explanations contain statements with some prior knowledge that must be accurate and true (Goodrich et al., 2019; Xu et al., 2020). Clarity. An explanation should be clear to achieve effective communication. In the NLG field, Clarity implies that text is easily understood (Belz and Kow, 2009; van der Lee et al., 2017) and that the reader is familiar with basic information introduced in the text (Lampouras and Androutsopoulos, 2013). In addition, Clarity can also help expose the truthfulness and correctness of textual data (Mahapatra et al., 2016). 4.2 Automatic Evaluation Metrics This section describes a number of automatic metrics commonly used in NLG evaluation and selected for this study. These fall into two categories: 1) word-overlap metrics, e.g. BLEU, METEOR and ROUGE (Novikova et al., 2017); 2380 and 2) embedding-based metrics, e.g. BERTScore and BLEURT (Sellam et al., 2020). Each of these metrics is compared to one or more “Gold Standard” text as inspired by the Machine Translation community and adopted for eva"
2021.eacl-main.202,W07-0734,0,0.0400775,"ld standards involves subjectivity and specificity (Kov´aˇr et al., 2016), and this is part of the reason that automatic metrics have received some criticism (Hardcastle and Scott, 2008). BLEU (B) (Papineni et al., 2001) is widely used in the field of NLG and compares n-grams of a candidate text (e.g. that generated by an algorithm) with the n-grams of a reference text. The number of matches defines the goodness of the candidate text. SacreBLEU (SB) (Post, 2018) is a new version of BLEU that calculates scores on the detokenized text. METEOR (M) was created to try to address BLEU&apos;s weaknesses (Lavie and Agarwal, 2007). METEOR evaluates text by computing a score based on explicit word-to-word matches between a candidate and a reference. When using multiple references, the candidate text is scored against each reference, and the best score is reported. ROUGE (R) (Lin, 1971) evaluates n-gram overlap of the generated text (candidate) with a reference. ROUGE-L (RL) (Longest Common Subsequence) computes the longest common subsequence (LCS) between a pair of sentences. BERTScore (BS) (Zhang et al., 2020) is a tokenlevel matching metric with pre-trained contextual embeddings using BERT (Devlin et al., 2019) that m"
2021.eacl-main.202,W17-3513,0,0.0333878,"Missing"
2021.eacl-main.202,N16-1082,0,0.0347086,"ap based ones. We speculate that this is in part due to the fact that the former capture semantics more effectively and are thus more invariant to paraphrases. These metrics have also been shown to be useful across multiple tasks (Sellam et al., 2020) but with some variation across datasets (Novikova et al., 2017). Therefore, future work would involve examining the effectiveness of automatic metrics across a wider variety of explanation tasks and datasets, as outlined in the Related Work section. Embeddings are quite opaque in themselves. Whilst some attempts have been made to visualise them (Li et al., 2016), it remains that embeddingbased metrics do not provide much insight into what makes a good/bad explanation. It would thus be necessary to look more deeply into the linguistic phenomena that may indicate the quality of explanations. In ExBAN, initial findings show that the number of nouns and coordinating conjunctions correlate with human judgements, however further in-depth analysis is needed. Additional metrics to add to the set explored here could include grammar-based metrics, such as readability and grammaticality, as in the study described in (Novikova et al., 2017). Furthermore, an inve"
2021.eacl-main.202,W04-1013,0,0.123587,"ams of a candidate text (e.g. that generated by an algorithm) with the n-grams of a reference text. The number of matches defines the goodness of the candidate text. SacreBLEU (SB) (Post, 2018) is a new version of BLEU that calculates scores on the detokenized text. METEOR (M) was created to try to address BLEU&apos;s weaknesses (Lavie and Agarwal, 2007). METEOR evaluates text by computing a score based on explicit word-to-word matches between a candidate and a reference. When using multiple references, the candidate text is scored against each reference, and the best score is reported. ROUGE (R) (Lin, 1971) evaluates n-gram overlap of the generated text (candidate) with a reference. ROUGE-L (RL) (Longest Common Subsequence) computes the longest common subsequence (LCS) between a pair of sentences. BERTScore (BS) (Zhang et al., 2020) is a tokenlevel matching metric with pre-trained contextual embeddings using BERT (Devlin et al., 2019) that matches words in candidate and reference sentences using cosine similarity. BLEURT (BRT) (Sellam et al., 2020) is a text generation metric also based on BERT, pre-trained on synthetic data; it uses random perturbations of Wikipedia sentences augmented with a d"
2021.eacl-main.202,W16-6624,0,0.0182171,"accuracy, or adequacy (Novikova et al., 2018). As mentioned previously, explanations contain statements with some prior knowledge that must be accurate and true (Goodrich et al., 2019; Xu et al., 2020). Clarity. An explanation should be clear to achieve effective communication. In the NLG field, Clarity implies that text is easily understood (Belz and Kow, 2009; van der Lee et al., 2017) and that the reader is familiar with basic information introduced in the text (Lampouras and Androutsopoulos, 2013). In addition, Clarity can also help expose the truthfulness and correctness of textual data (Mahapatra et al., 2016). 4.2 Automatic Evaluation Metrics This section describes a number of automatic metrics commonly used in NLG evaluation and selected for this study. These fall into two categories: 1) word-overlap metrics, e.g. BLEU, METEOR and ROUGE (Novikova et al., 2017); 2380 and 2) embedding-based metrics, e.g. BERTScore and BLEURT (Sellam et al., 2020). Each of these metrics is compared to one or more “Gold Standard” text as inspired by the Machine Translation community and adopted for evaluating document summarisation and NLG (Belz and Reiter, 2006). The gold standard is normally a piece of natural lang"
2021.eacl-main.202,D17-1238,0,0.0517139,"Missing"
2021.eacl-main.202,N18-2012,0,0.0508489,"Missing"
2021.eacl-main.202,W18-6319,0,0.0535574,"task. Automatic evaluation is based on this gold standard, by verifying potential similarity (Kov´aˇr et al., 2016). However, the selection of gold standards involves subjectivity and specificity (Kov´aˇr et al., 2016), and this is part of the reason that automatic metrics have received some criticism (Hardcastle and Scott, 2008). BLEU (B) (Papineni et al., 2001) is widely used in the field of NLG and compares n-grams of a candidate text (e.g. that generated by an algorithm) with the n-grams of a reference text. The number of matches defines the goodness of the candidate text. SacreBLEU (SB) (Post, 2018) is a new version of BLEU that calculates scores on the detokenized text. METEOR (M) was created to try to address BLEU&apos;s weaknesses (Lavie and Agarwal, 2007). METEOR evaluates text by computing a score based on explicit word-to-word matches between a candidate and a reference. When using multiple references, the candidate text is scored against each reference, and the best score is reported. ROUGE (R) (Lin, 1971) evaluates n-gram overlap of the generated text (candidate) with a reference. ROUGE-L (RL) (Longest Common Subsequence) computes the longest common subsequence (LCS) between a pair of"
2021.eacl-main.202,P19-1487,0,0.0245192,"in an increasing number of NL explanations datasets. The ELI5 dataset1 (Fan et al., 2019) is composed of explanations represented as multisentence answers for diverse questions where users are encouraged to provide answers, which are comprehensible for a five-year-old. WorldTree V22 (Jansen et al., 2019) is a corpus of ScienceDomain that contains explanation graphs for elementary science questions, where explanations represent interconnected sets of facts. CoSE3 is a dataset of human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations (Rajani et al., 2019). Multimodal Explanations Datasets (VQA-X and ACT-X) contain textual and visual explanations from human annotators (Park et al., 2018). e-SNLI4 is a corpus of explanations built on the question: “Why is a pair of sentences in a relation of entailment, neutrality, or contradiction?” (Camburu et al., 2018). Finally, the SNLI corpus5 is a large annotated corpus for learning natural language inference (Bowman et al., 2015), considered one of the first corpora of NL explanations. For Step 1, each subject was shown graphical representations of three Bayesian Networks (BN), in random order. They were"
2021.eacl-main.202,N16-3020,0,0.270856,"e overall behaviour of the entire model (Arya et al., 2019); (2) local explanations, commonly taking the form of counterfactuals (Sokol and Flach, 2019) that describe why particular events happened (known also as “everyday explanations”); and (3) examplebased explanations that present examples from the training set to explain algorithmic behaviour (Cai et al., 2019). Recently, various explanation systems provide different types of explanations for AI systems: the LIME method visually explains how sampling and local model training works by using local interpretable model-agnostic explanations (Ribeiro et al., 2016); MAPLE can provide feedback for all three types of explanations: example-based, local and global explanations (Plumb et al., 2018); CLEAR explains a single prediction by using local explanations that include statements of key counterfactual cases (White and d’Avila Garcez, 2019). Whilst these techniques and tools gain some ground in explaining deep machine learning, the explanations they provide are not necessarily aimed at the (non-expert) end-user and so are not always intuitive. NLG has traditionally been broken down into “what” to say (content selection) and “how” to say it (surface reali"
2021.eacl-main.202,E17-2007,0,0.0240773,"01). This way may not be appropriate for explanations, as good explanations may need to be lengthy by their very nature. METEOR takes into consideration F1-measure by computing scores for unigram precision and recall. The fragmentation penalty is calculated using the total number of matched words (m, averaged over hypothesis and reference) and the number of chunks. In this way, it could identify synonyms, but perhaps not as well as the embedding-based metrics, as evidenced by the correlation figures in our results. With regards to ROUGE-based scores, due to the upper bound issues presented by Schluter (2017), it is impossible to obtain perfect ROUGE-n scores. Furthermore, ROUGE-L cannot differentiate if the reference and the candidate have the same longest common subsequence (LCS), but different word ordering. Again, word ordering may be important for the explanation in terms of explainee scaffolding (Palincsar, 1986). Figure 2. Heatmap of Spearman rank correlation between automatic evaluation metrics and human evaluation metrics (Informativeness and Clarity) Metric Diagram 1 Diagram 2 Diagram 3 All Diagrams BLEU-1 BLEU-2 BLEU-3 BLEU-4 0.27 0.24 0.15 0.02 0.25 0.27 0.23 0.21 0.41* 0.44* 0.39 0.13"
2021.eacl-main.202,2020.acl-main.704,0,0.263865,"elz and Kow, 2009; van der Lee et al., 2017) and that the reader is familiar with basic information introduced in the text (Lampouras and Androutsopoulos, 2013). In addition, Clarity can also help expose the truthfulness and correctness of textual data (Mahapatra et al., 2016). 4.2 Automatic Evaluation Metrics This section describes a number of automatic metrics commonly used in NLG evaluation and selected for this study. These fall into two categories: 1) word-overlap metrics, e.g. BLEU, METEOR and ROUGE (Novikova et al., 2017); 2380 and 2) embedding-based metrics, e.g. BERTScore and BLEURT (Sellam et al., 2020). Each of these metrics is compared to one or more “Gold Standard” text as inspired by the Machine Translation community and adopted for evaluating document summarisation and NLG (Belz and Reiter, 2006). The gold standard is normally a piece of natural language text, annotated by humans as correct, i.e. a solution for a given task. Automatic evaluation is based on this gold standard, by verifying potential similarity (Kov´aˇr et al., 2016). However, the selection of gold standards involves subjectivity and specificity (Kov´aˇr et al., 2016), and this is part of the reason that automatic metric"
2021.eacl-main.202,2020.acl-main.455,0,0.0444867,"Missing"
2021.splurobonlp-1.2,N19-1423,0,0.0222745,"ecast as classification tasks: a landmark, a bearing and a distance. where V is a vocabulary of words and the corresponding geographic map I is represented as a set of M landmark objects oi = (bb, r, n) where bb is a 4-dimensional vector with bounding box coordinates, r is the corresponding Region of Interest (RoI) feature vector produced by an object detector and n =&lt; n1 , n2 . . . nK &gt;, is a multi-token name. We define a function f : V N × R4∗M × R2048∗M × V M ∗K → R × R to predict the GPS destination location yˆ: yˆ = f w, {oi = (bb, r, n)}M  weights are initialized using pretrained BERT (Devlin et al., 2019). hw0 is the hidden state for the special token [CLS]. Metadata Encoder OSM comes with useful metadata in the form of bounding boxes (around the landmark symbols) and names of landmarks on the map. We represent each bounding box as a 4-dimensional vector bbmetak and each name (nk ) using another Transformer initialized with pretrained BERT weights. We treat metadata as a bag of names but since each word can have multiple tokens, we output position embeddings posnk for each name separately; hnk are the resulting hidden states with hnk,0 being the hidden state for [CLS]. (1) Since predicting yˆ"
2021.splurobonlp-1.2,L16-1605,0,0.0684861,"Missing"
2021.splurobonlp-1.2,N16-1088,0,0.0123599,"elligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the destination prediction task is two-fold. The first stage is a data collection for the “Robot Open Street Map Instructions” (ROSMI) (Katsakioris"
2021.splurobonlp-1.2,D18-1287,0,0.0433435,"Missing"
2021.splurobonlp-1.2,W19-8621,0,0.01909,"log, such as assisting with emergency response and remote robot instruction that require knowledge of maps or building schemas. Effective communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruct"
2021.splurobonlp-1.2,D17-1106,0,0.0623215,"Missing"
2021.splurobonlp-1.2,D19-1681,0,0.0209743,"ive communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the destination prediction task is two-fold. The first stage is a data collection for the “Robot Open Street Map Instructio"
2021.splurobonlp-1.2,D14-1086,0,0.0455654,"with emergency response and remote robot instruction that require knowledge of maps or building schemas. Effective communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the d"
2021.splurobonlp-1.2,P06-1131,0,0.0600078,"is able to understand instructions referring to previously unseen maps. 2 Related Work Situated dialog encompasses various aspects of interaction. These include: situated Natural Language Processing (Bastianelli et al., 2016); situated reference resolution (Misu, 2018); language grounding (Johnson et al., 2017); visual question answer/visual dialog (Antol et al., 2015); dialog agents for learning visually grounded word meanings and learning from demonstration (Yu et al., 2017); and Natural Language Generation (NLG), e.g. of situated instructions and referring expressions (Byron et al., 2009; Kelleher and Kruijff, 2006). Here, work on instruction processing for destination mapping and navigation are discussed, as well as language grounding and referring expression resolution, with an emphasis on 2D/3D real world and map-based application. Language grounding refers to interpreting language in a situated context and includes collaborative language grounding toward situated humanrobot dialog (Chai et al., 2016), city exploration (Boye et al., 2014), as well as following high-level navigation instructions (Blukis et al., 2018). Mapping instructions to low level actions has been explored in structured environment"
2021.splurobonlp-1.2,D19-1514,0,0.112491,"i = (bb, r, n)}M Visual Encoder Each map image is fed into a pretrained Faster R-CNN detector (Ren et al., 2015), which outputs bounding boxes and RoI feature vectors bbk and rk for k objects. In order to learn better representation for landmarks, we fine-tuned the detector on around 27k images of maps to recognize k objects {o1 , .., ok } and classify landmarks of 213 manually-cleaned classes from OSM; we fixed k to 73 landmarks. Finally, a combined position-aware embedding vk was learned by adding together the vectors bbk and rk as in LXMERT:  (2) 4.2 Model Architecture Inspired by LXMERT (Tan and Bansal, 2019), we present MAPERT, a Transformer-based (Vaswani et al., 2017) model with three separate singlemodality encoders (for NL instructions, metadata and visual features) and a cross-modality encoder that merges them. Fig. 2 depicts the architecture. In the following sections, we describe each component separately. F F (bbk ) + F F (rk ) (3) 2 where F F are feed-forward layers with no bias. vk = 4.3 We describe three different approaches to combining knowledge from maps with the NL instructions: Instructions Encoder The word sequence w is fed to a Transformer encoder and output hidden states hw and"
2021.splurobonlp-1.2,H93-1005,0,0.650364,"ire knowledge of maps or building schemas. Effective communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the destination prediction task is two-fold. The first stage is a"
2021.splurobonlp-1.2,W17-2802,0,0.0284002,"ree outputs, i.e., reference landmark location on the map, bearing and distance. Our contributions are thus three-fold: • A model that is able to understand instructions referring to previously unseen maps. 2 Related Work Situated dialog encompasses various aspects of interaction. These include: situated Natural Language Processing (Bastianelli et al., 2016); situated reference resolution (Misu, 2018); language grounding (Johnson et al., 2017); visual question answer/visual dialog (Antol et al., 2015); dialog agents for learning visually grounded word meanings and learning from demonstration (Yu et al., 2017); and Natural Language Generation (NLG), e.g. of situated instructions and referring expressions (Byron et al., 2009; Kelleher and Kruijff, 2006). Here, work on instruction processing for destination mapping and navigation are discussed, as well as language grounding and referring expression resolution, with an emphasis on 2D/3D real world and map-based application. Language grounding refers to interpreting language in a situated context and includes collaborative language grounding toward situated humanrobot dialog (Chai et al., 2016), city exploration (Boye et al., 2014), as well as followin"
C08-3005,E06-2009,1,0.807393,"rmine what taskbased steps to take next, such as asking for a cinema name. General aspects of dialogue, such as confirmation and clarification strategies, are handled by the domain-general DM. Values for constraints on transitions and branching in the BPM, for example “present insurance option if the user is business-class”, are compiled into domain-specific parts of the DM’s update rules. XML format is used for BPMs, and they are compiled into finite state machines consulted by the spoken dialogue system through the BPM module. The domaingeneral DM was mostly abstracted from the TALK system (Lemon et al., 2006). 2.2 Compiling Grammars for Business User Resources and Databases Figure 2: Part of an example Business Process Model for searching for Hotels The resulting spoken dialogue system deploys the following main modules: • Speech Recogniser module, e.g. ATK/HTK (Young, 2007; Young, 1995) or Nuance (Nuance, 2002) For Spoken Language Understanding, ADT currently uses Grammatical Framework (GF) (Ranta, 2004) which is a language for writing multilingual grammars, on top of which various applications such as machine translation and human-machine interaction have been built. A GF grammar not only define"
D12-1008,W10-4342,0,0.0390158,"Missing"
D12-1008,W11-2814,1,0.887242,"Missing"
D12-1008,W12-1509,1,0.78501,"here Qij (s, a) specifies the expected cumulative reward for executing action a in state s and then following π ∗ . We use HSMQ-Learning to induce dialogue policies, see (Cuay´ahuitl, 2009), p. 92. 5 Experimental Setting 5.1 Hierarchy of Learning Agents The HRL agent in Figure 3 shows how the tasks of (1) dealing with incrementally changing input hypotheses, (2) choosing a suitable IP strategy and (3) presenting information, are connected. Note that 1 we focus on a detailed description of models M0...3 here, which deal with barge-ins and backchannels and are the core of this paper. Please see Dethlefs et al. (2012) for details of an RL model that deals with the remaining decisions. Briefly, model M00 deals with dynamic input hypotheses. It chooses when to listen to an incoming user utterance (M31 ) and when and how to present 1 ) by calling and passing control information (M0...2 to a child subtask. The variable ‘incrementalStatus’ characterises situations in which a particular (incremental) action is triggered, such as a floor holder ‘let me see’, a correction or self-correction. The variable ‘presStrategy’ indicates whether a strategy for IP has been chosen or not, and the variable ‘userReaction’ show"
D12-1008,W09-3902,0,0.0268795,"crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context"
D12-1008,P02-1026,0,0.0136262,"that for an utterance u consisting of the word sequence w1 . . . wi−1 , we can compute the ID at each point during the utterance as: Information Theory n Information Theory as introduced by Shannon (1948) is based on two main concepts: a communication channel through which information is transferred in bits and the information gain, i.e. the information load that each bit carries. For natural language, the assumption is that people aim to com84 log X 1 1 log = P (u) P (wi |w1 . . . wi−1 ) (1) i=1 While typically the context of a word is given by all preceding words of the utterance, we follow Genzel and Charniak (2002) in restricting our computation to tri-grams for computability reasons. Given a I want Italian food in the city centre. Yes, I need a moderately priced restaurant in the New Chesterton area. I need the address of a Thai restaurant. 20 Information Density language model of the domain, we can therefore optimise ID in system-generated discourse, where we treat ID as “an optimal solution to the problem of rapid yet error-free communication in a noisy environment” (Levy and Jaeger (2007), p.2). We will now transfer the notion of ID to IP and investigate the distribution of information over user res"
D12-1008,J08-4002,1,0.70664,"ser utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incrementa"
D12-1008,P10-1008,1,0.72985,"d accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output plan"
D12-1008,W03-2311,0,0.103331,"oretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93"
D12-1008,W11-2706,0,0.0354197,"ins based on a partially data-driven reward function. Generating backchannels can be beneficial for grounding in interaction. Similarly, barge-ins can lead to more efficient interactions, e.g. when a system can clarify a bad recognition result immediately before acting based on a misrecognition. A central concept to our approach is Information Density (ID) (Jaeger, 2010), a psycholinguistic hypothesis that human utterance production is sensitive to a uniform distribution of information across the utterance. This hypothesis has also been adopted for low level output planning recently, see e.g. Rajkumar and White (2011). Our results in terms of average rewards and a human rating study show that a learning agent that is sensitive to ID can learn when it is most beneficial to generate feedback to a user, and outperforms several other agents that are not sensitive to ID. 2 Incremental Information Presentation 2.1 Information Presentation Strategies Our example domain of application is the Information Presentation phase in an interactive system for restaurant recommendations, extending previous work by Rieser et al. (2010). This previous work incrementally constructs IP strategies according to the predicted user"
D12-1008,N09-1071,0,0.0304254,"architecture offers inherently incremental mechanisms to update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation"
D12-1008,P10-1103,1,0.93045,"ech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output planning that is based on"
D12-1008,W11-2014,0,0.0143645,"they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et"
D12-1008,W10-4301,0,0.322476,"Missing"
D12-1008,W11-2813,1,\N,Missing
E14-1074,W09-0609,0,0.0155285,"lected by Mairesse et al. (2010),3 using Amazon Mechanical Turk. Turkers typed in recommendations for various specified semantics; e.g. “I recommend the restaurant Beluga near the cathedral.” • CLASSIC is a dataset of transcribed spoken user utterances from the CLASSiC project.4 The utterances consist of user queries for restaurants, such as “I need an Italian restaurant with a moderate price range.” User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality of realisations and that these can be modelled in trainable generation. They train two versions of a rank-and-boost generator, a first version of which is trained on the average population of user ratings, whereas a second one is trained on the ratings of individual users. The authors show statistically that ratings from different u"
E14-1074,D12-1008,1,0.847662,"al and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s using: score(s) = ni θi f"
E14-1074,W12-1509,1,0.848115,"al and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s using: score(s) = ni θi f"
E14-1074,P13-1123,1,0.900886,"Missing"
E14-1074,W06-1405,0,0.0346695,"t personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 LIST MAI CLASSIC 4 Estimation of Style Prediction Models Corpora and Style Dimensions Our domain of interest is the automatic generation of restaurant recommendations that differ with respect to their colloquialism and politeness and are as natural as possible. All three stylistic dimension were identified from a qualitative analysis of human domain data. To estimate the strength of each of them in a single utterance, we c"
E14-1074,J14-4006,1,0.602156,"ly by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utterances, so that their ratings can be estimated more accurately than 702 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702–711, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics based on an average population of users. This is similar to Janarthanam and Lemon (2014), who show that clustering users and adapting to their level of domain expertise can significantly improve task success and user ratings. Our resulting model is evaluated with realisers not originally built to deal with stylistic variation, and produces natural variation recognisable by humans. User Clusters Ranking + Evaluation 2 Architecture and Domain Regressor We aim to with generating restaurant recommendations as part of an interactive system. To do this, we assume that a generator input is provided by a preceding module, e.g. the interaction manager, and that the task of the surface rea"
E14-1074,J11-3002,0,0.336021,"es, keeping traces of each generator decision, and obtaining style scores for each output based on the estimated factor model. The result is a dataset of <generator decision, style score&gt; pairs which can be used in a correlation analysis to identify the predictors of particular output styles. During generation, the correlation equations inform the generator at each choice point so as to best express the desired style. Unfortunately, no human evaluation of the model is presented so that it remains unclear to what extent the generated styles are perceivable by humans. Closely related is work by Mairesse and Walker (2011) who present the PERSONAGE system, which aims to generate language reflecting particular personalities. Instead of choosing generator decisions by considering their predicted style scores, however, Mairesse and Walker (2011) directly predict generator decisions based on target personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of sev"
E14-1074,P10-1157,0,0.0952924,"Missing"
E14-1074,P05-1008,0,0.223977,"ormation inform the resulting stylistic regression model. For surface realisation (topright box, blue), a semantic input from a preceding model is given as input to a surface realiser. Any realiser is suitable that returns a ranked list of output candidates. The resulting list is re-ranked according to stylistic scores estimated by the regressor, so that the utterance which most closely reflects the target score is ranked highest. The reranking process is shown in the lower box (red). 3 Related Work 3.1 Stylistic Variation in Surface Realisation Our approach is most closely related to work by Paiva and Evans (2005) and Mairesse and Walker 1 http://parlance-project.eu Surface Realisation Figure 1: Architecture of stylistic realisation model. Top left: user clusters are estimated from corpus utterances described by linguistic features and ratings. Top right: surface realisation ranks a list of output candidates based on a semantic input. These are ranked stylistically given a trained regressor. (2011), discussed in turn here. Paiva and Evans (2005) present an approach that uses multivariate linear regression to map individual linguistic features to distinguishable styles of text. The approach works in thr"
E14-1074,P05-1015,0,0.0770249,"so as to reflect stylistic variation that is as natural as possible. On the other hand, we aim to minimise the amount of annotation and human engineering that informs the design of the system. To this end, we estimate a mapping function between automatically identifiable shallow linguistic features characteristic of an utterance and its human-assigned style ratings. In addition, we aim to address the high degree of variability that is often encountered in subjective rating studies, such as assessments of recommender systems (O’Mahony et al., 2006; Amatriain et al., 2009), sentiment analysis (Pang and Lee, 2005), or surface realisations, where user ratings have been shown to differ significantly (p<0.001) for the same utterance (Walker et al., 2007). Such high variability can affect the performance of systems which are trained from an average population of user ratings. However, we are not aware of any work that has addressed this problem principally by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utte"
E14-1074,W10-4301,0,0.0314935,"ions per dimension between actual and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s us"
E14-1074,J97-1007,0,\N,Missing
E14-1074,W13-4026,1,\N,Missing
E14-1074,W11-2813,1,\N,Missing
E14-4041,W13-2115,1,0.807008,"Summarisation of time-series data refers to the task of automatically generating summaries from attributes whose values change over time. Content selection is the task of choosing what to say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance 210 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210–214, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics then decides whether to talk about it or not. The states and actions are similar for all systems. Section 5, directions for future work are discussed. 2 Methodology Lecturer-adapted reward function The reward function is derived from"
E14-4041,W10-4324,1,0.857758,"t are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, whereas the students disprefer this content. Generating the same summary for both groups allows for meaningful further discussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the programming effort is reduced as only one system needs to"
E14-4041,P02-1040,0,0.1008,"esented in Table 2. 3.1 Evaluation in Simulation 26 summaries were produced by each system. The output of each system was evaluated with the three reward functions. Table 3 shows the results. As expected, all systems score highly when evaluated with the reward function for which they were trained, with the second highest reward scored from the MO function. Table 2 illustrates this with the MO Policy clearly between the other two policies. Moreover, the MO function reduces the variability between summaries as is also reflected in the standard deviation given in Table 3. We used BLEU (4-grams) (Papineni et al., 2002) to measure the similarities between the feedback summaries generated by the three systems. BLEU score is between 0-1 with values closer to 1 indicating texts are more similar. Our results demonstrate that the summaries generated by the three systems are quite different (BLEU score between 0.33 and 0.36). This shows that the framework presented here is capable of producing quite different summaries based on the various reward functions. 4 Discussion It is interesting to examine the weights derived from the multiple-linear regression to determine the preferences of the different user groups. Fo"
E14-4041,P10-1103,1,0.795582,"ere is a direct mapping between the values of factor and reference type and the surface text. The timeseries attributes are listed in Table 1 (bottom left). Student-adapted reward function The Student-adapted system uses the same RL algorithm as the Lecturer-adapted one. The difference lies in the reward function. The reward function used for training is of a similar style as the Lecturer-adapted reward function. This function was derived by manipulating the student ratings in a previous experiment and estimating the weights using linear regression in a similar way as Walker et al. (1997) and Rieser et al. (2010). Multi-objective function The function used for the multi-objective method is derived by weighting the sum of the individual reward functions. RM O = 0.5 ∗ RLECT + 0.5 ∗ RST U DEN T To reduce the confounding variables, we kept the ordering of content in all systems the same. 2.2 Time-series summarisation systems Actions and states: The state consists of the timeseries data and the selected templates. In order to explore the state space the agent selects a timeseries attribute (e.g. marks, deadlines etc.) and 3 Evaluation The output of the above-mentioned three systems were evaluated both in s"
E14-4041,P97-1035,0,0.440656,"s of time-series data. There is a direct mapping between the values of factor and reference type and the surface text. The timeseries attributes are listed in Table 1 (bottom left). Student-adapted reward function The Student-adapted system uses the same RL algorithm as the Lecturer-adapted one. The difference lies in the reward function. The reward function used for training is of a similar style as the Lecturer-adapted reward function. This function was derived by manipulating the student ratings in a previous experiment and estimating the weights using linear regression in a similar way as Walker et al. (1997) and Rieser et al. (2010). Multi-objective function The function used for the multi-objective method is derived by weighting the sum of the individual reward functions. RM O = 0.5 ∗ RLECT + 0.5 ∗ RST U DEN T To reduce the confounding variables, we kept the ordering of content in all systems the same. 2.2 Time-series summarisation systems Actions and states: The state consists of the timeseries data and the selected templates. In order to explore the state space the agent selects a timeseries attribute (e.g. marks, deadlines etc.) and 3 Evaluation The output of the above-mentioned three systems"
hastie-belz-2014-comparative,N07-1021,1,\N,Missing
hastie-belz-2014-comparative,E09-1078,0,\N,Missing
hastie-belz-2014-comparative,W06-1410,0,\N,Missing
hastie-belz-2014-comparative,W09-2816,1,\N,Missing
hastie-belz-2014-comparative,W08-0128,0,\N,Missing
hastie-belz-2014-comparative,W07-2307,0,\N,Missing
hastie-belz-2014-comparative,P00-1019,0,\N,Missing
hastie-belz-2014-comparative,P13-1123,1,\N,Missing
hastie-belz-2014-comparative,W11-2832,1,\N,Missing
hastie-belz-2014-comparative,P01-1056,0,\N,Missing
hastie-belz-2014-comparative,W11-2002,1,\N,Missing
hastie-belz-2014-comparative,P04-1011,0,\N,Missing
hastie-belz-2014-comparative,E14-1074,1,\N,Missing
hastie-belz-2014-comparative,P08-1073,0,\N,Missing
hastie-belz-2014-comparative,P10-1008,0,\N,Missing
hastie-belz-2014-comparative,P89-1009,0,\N,Missing
hastie-belz-2014-comparative,2007.sigdial-1.23,0,\N,Missing
hastie-belz-2014-comparative,W11-2853,0,\N,Missing
hastie-belz-2014-comparative,D10-1049,0,\N,Missing
hastie-belz-2014-comparative,W11-2838,0,\N,Missing
hastie-belz-2014-comparative,W10-4233,0,\N,Missing
hastie-belz-2014-comparative,W11-2017,1,\N,Missing
hastie-belz-2014-comparative,P12-3009,0,\N,Missing
hastie-etal-2002-automatic,H92-1005,0,\N,Missing
hastie-etal-2002-automatic,H92-1009,0,\N,Missing
hastie-etal-2002-automatic,W02-0221,1,\N,Missing
hastie-etal-2002-automatic,P01-1066,1,\N,Missing
hastie-etal-2002-automatic,bonneau-maynard-etal-2000-predictive,0,\N,Missing
P02-1049,H01-1028,0,0.0216091,"Missing"
P02-1049,P98-1122,0,0.0638596,"Missing"
P02-1049,P99-1040,1,0.891002,"Missing"
P02-1049,W02-0221,1,0.829319,"us labelling was done because we found that systems had augmented their inventory of named entities and utterance patterns from 2000 to 2001, and these were not accounted for by the 2000 tagger database. For the extension, we collected a fresh set of vocabulary lists from the sites and augmented the pattern database with additional 800 labelled utterance patterns. We also implemented a contextual rule-based postprocessor that takes any remaining unlabelled utterances and attempts to label them by looking at their surrounding DATE labels. More details about the extended tagger can be found in (Prasad and Walker, 2002). On the 2001 corpus, we were able to label 98.4 of the data. A hand evaluation of 10 randomly selected dialogues from each system shows that we achieved a classification accuracy of 96 at the utterance level. For User Satisfaction Prediction, we found that the distribution of DATE acts were better captured by using the frequency normalized over the total number of dialogue acts. In addition to these unigram proportions, the bigram frequencies of the DATE dialogue acts were also calculated. In the following two sections, we discuss which DATE labels are discriminatory for predicting Task Compl"
P02-1049,P01-1066,1,0.851463,"ialogues that provide training data for further system development. As a spoken dialogue system is developed, it is first tested as a prototype, then fielded in a limited setting, possibly running with human supervision (Gorin et al., 1997), and finally deployed. At each stage from research prototype to deployed commercial application, the system is constantly undergoing further development. When a system is prototyped in house or first tested in the field, human subjects are often paid to use the system and give detailed feedback on task completion and user satisfaction (Baggia et al., 1998; Walker et al., 2001). Even when a system is deployed, it often keeps evolving, either because customers want to do different things with it, or because new tasks arise out of developments in the underlying application. However, real customers of a deployed system may not be willing to give detailed feedback. Thus, the widespread use of these systems has created a data management and analysis problem. System designers need to constantly track system performance, identify problems, and fix them. System modules such as automatic speech recognition (ASR), natural language understanding (NLU) and dialogue management m"
P02-1049,C98-1117,0,\N,Missing
P13-1123,W11-2814,1,0.903062,"Missing"
P13-1123,P11-2115,1,0.328589,"Missing"
P13-1123,D12-1008,1,0.878448,"iming and Ordering Interaction Manager The Beluga is a great Italian restaurant y0 y1 y2 Micro-turn dialogue act, inform(food=Thai) semantics of user utterance intervening modules Semantic tree Lexical Syntactic Semantic features features features speech User String of words (synthesised) (b) Surface Realisation root root inform( name= Beluga) Figure 1: Architecture of our SDS with a focus on the NLG components. While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. (2012b)). This paper focuses on surface realisation from these trees using a CRF as shown in the surface realisation module. Slot Example A DDRESS A REA F OOD NAME P HONE P OSTCODE Q UALITY P RICE S IGNATURE V ENUE The venue’s address is . . . It is located in . . . The restaurant serves . . . cuisine. The restaurant’s name is . . . The venue’s phone number is . . . The postcode is . . . This is a . . . venue. It is located in the . . . price range. The venue specialises in . . . This venue is a . . . Table 1: Semantic slots required for our domain along with example realisations. Attributes can be"
P13-1123,W12-1509,1,0.93412,"iming and Ordering Interaction Manager The Beluga is a great Italian restaurant y0 y1 y2 Micro-turn dialogue act, inform(food=Thai) semantics of user utterance intervening modules Semantic tree Lexical Syntactic Semantic features features features speech User String of words (synthesised) (b) Surface Realisation root root inform( name= Beluga) Figure 1: Architecture of our SDS with a focus on the NLG components. While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. (2012b)). This paper focuses on surface realisation from these trees using a CRF as shown in the surface realisation module. Slot Example A DDRESS A REA F OOD NAME P HONE P OSTCODE Q UALITY P RICE S IGNATURE V ENUE The venue’s address is . . . It is located in . . . The restaurant serves . . . cuisine. The restaurant’s name is . . . The venue’s phone number is . . . The postcode is . . . This is a . . . venue. It is located in the . . . price range. The venue specialises in . . . This venue is a . . . Table 1: Semantic slots required for our domain along with example realisations. Attributes can be"
P13-1123,P12-1039,0,0.0291171,"lar importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In our case, they are produced by a reinforcement learning"
P13-1123,D09-1042,0,0.149375,"Missing"
P13-1123,P10-1157,0,0.230168,"ased generation which takes long-range dependencies into account outperforms several baselines. However, Lu et al.’s generator does not take context beyond the current utterance into account and is thus restricted to local features. Furthermore, their model is not able to modify generation results on the fly due to new or updated inputs. In terms of surface realisation from graphical models (and within the context of SDSs), our approach is also related to work by Georgila et al. (2002) and Dethlefs and Cuay´ahuitl (2011b), who use HMMs, Dethlefs and Cuay´ahuitl (2011a) who use Bayes Nets, and Mairesse et al. (2010) who use Dynamic Bayes Nets within an Active Learning framework. The last approach is also concerned with generating restaurant recommendations within an SDS. Specifically, their system optimises its performance online, during the interaction, by asking users to provide it with new textual descriptions of concepts, for which it is unsure of the best realisation. In contrast to these related approaches, we use undirected graphical models which are useful when the natural directionality between the input variables is unknown. In terms of surface realisation for SDSs, Oh and Rudnicky (2000) prese"
P13-1123,de-marneffe-etal-2006-generating,0,0.00319151,"Missing"
P13-1123,P06-1140,0,0.0206843,"ime SDS. This could present a problem in incremental settings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface r"
P13-1123,W00-0306,0,0.136525,", and Mairesse et al. (2010) who use Dynamic Bayes Nets within an Active Learning framework. The last approach is also concerned with generating restaurant recommendations within an SDS. Specifically, their system optimises its performance online, during the interaction, by asking users to provide it with new textual descriptions of concepts, for which it is unsure of the best realisation. In contrast to these related approaches, we use undirected graphical models which are useful when the natural directionality between the input variables is unknown. In terms of surface realisation for SDSs, Oh and Rudnicky (2000) present foundational work in using an n-gram-based system. They train a surface realiser based on a domain-dependent language model and use an overgeneration and ranking approach. Candidate utterances are ranked according to a penalty function which penalises too long or short utterances, repetitious utterances and utterances which either contain more or less information than required by the dialogue act. While their approach is fast to execute, it has the disadvantage of not being able to model long-range dependencies. They show that humans rank their output equivalently to template-based ge"
P13-1123,E09-1081,0,0.0386098,"sometimes be difficult to model, because they require rich contextawareness that keeps track of all (or much) of what was generated before, i.e. a growing generation history. In text generation, cohesion can span over the entire text. In interactive settings such as generation within a spoken dialogue system (SDS), a challenge is often to keep track of cohesion over several utterances. In addition, since interactions are dynamic, generator inputs from the dialogue manager can sometimes be partial or subject to subsequent modification. This has been addressed by work on incremental processing (Schlangen and Skantze, 2009). Since dialogue acts are passed on to the generation module as soon as possible, this can sometimes lead to incomplete generator inputs (because the user is still speaking), or inputs that are subject to later modification (because of an initial ASR mis-recognition). In this paper, we propose to formulate surface realisation as a sequence labelling task. We use conditional random fields (Lafferty et al., 2001; Sutton and McCallum, 2006), which are suitable for modelling rich contexts, in combination with semantic trees for rich linguistic information. This combination is able to keep track of"
P13-1123,W10-4301,0,0.00679094,"e seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local context. Especially for long utte"
P13-1123,E09-1085,0,0.00902622,"[barges in] No, I’m looking for a restaurant with good quality food. inform(quality=good [0.6], name=Beluga [0.6]) Oh sorry, so a nice restaurant located . . . [barges in] . . . in the city centre. inform(area=centre [0.8]) Table 4: Example dialogue where the dialogue manager needs to send incremental updates to the NLG. Incremental surface realisation from semantic trees for this dialogue is shown in Figure 3. because the authors tested only for Phrasing and Natural, respectively. 5 Incremental Surface Realisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and"
P13-1123,W06-1404,0,0.0319321,"ettings, where generation speed is of particular importance. The SPaRKy system is also used by Rieser et al. (2011), who focus on information presentation strategies for restaurant recommendations, summaries or comparisons within an SDS. Their surface realiser is informed by the highest ranked SPaRKy outputs for a particular information presentation strategy and will constitute one of our baselines in the evaluation. More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. 3 Cohesion across Utterances 3.1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. The semantic forms underlying surface realisation can be produced in many ways. In"
P13-1123,W03-2311,0,0.0285797,"lisation Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). The main characteristic of incremental architectures is that instead of waiting for the end of a user turn, they begin to process the input stream as soon as possible, updating their processing hypotheses as more information becomes available. From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012a), which limits the flexibility and quality of output generation. Buschmeier et al. (2012) have presented a system which systematically takes the user’s acoustic understanding problems into account by pausing, repeating or re-phrasing if necessary. Their approach is based on SPUD (Stone et al., 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. However, given the human labour involved in constraint specification, cohesion is often limited to a local co"
P13-1123,W12-1641,0,\N,Missing
P13-1123,W11-2813,1,\N,Missing
P14-1116,D10-1049,0,0.0269502,"s. In the next section, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include"
P14-1116,N04-1015,0,0.0190901,"evious methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boosting algorithm) and the identification of links between the entities with similar labels. In cont"
P14-1116,W10-4217,0,0.0265119,"tween the two methods. In the next section, we refer to the related work on Natural Language Generation from time-series data and on Content Selection. In Section 4.2, we describe our approach and we carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for cont"
P14-1116,E06-1040,0,0.0463478,"Missing"
P14-1116,W10-1301,0,0.0150193,"e carry out a comparison with simple classification methods. In Section 5, we present the evaluation setup and in Section 6 we discuss the results, obtained in simulation and with real students. Finally, in Section 8, directions for future work are discussed. 2 Related Work Natural Language Generation from time-series data has been investigated for various tasks such as weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras a"
P14-1116,W03-1016,0,0.0505137,"entation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on the values of entity attributes and is computed using a boos"
P14-1116,W08-1113,0,0.0606181,"Missing"
P14-1116,P04-1044,1,0.73951,"e results of the production phase, i.e. the ensemble of LPs with the corresponding klabelsets, the set of labels L, and the new instance x and it outputs the result vector of predicted labels for instance x. During run time, RAkEL estimates the average decision for each label in L and if the average is greater than a threshold t (determined by the developer) it includes the label in the predicted labelset. We used the standard parameter values of t, k and m (t = 0.5, k = 3 and m equals to 58 (2*29 templates)). In future, we could perform parameter optimisation by using a technique similar to (Gabsdil and Lemon, 2004). 5 Evaluation Firstly, we performed a preliminary evaluation on classification methods, comparing our proposed ML classification with multiple iterated classification approaches. The summaries generated by the ML classification system are then compared with the output of a RL system and two baseline systems in simulation and with real students. 5.1 Comparison with Simple Classification We compared the RAkEL algorithm with singlelabel (SL) classification. Different SL classifiers were trained using WEKA: JRip, Decision Trees, Naive Bayes, k-nearest neighbour, logistic regression, multi-layer p"
P14-1116,W13-2115,1,0.859104,"describe these factors in four different ways: 1. <trend>: referring to the trend of a factor over the semester (e.g. “Your performance was increasing...”), 2. <weeks>: explicitly describing the factor value at specific weeks (e.g. “In weeks 2, 3 and 9...”), 3. <average>: considering the average of a factor value (e.g. “You dedicated 1.5 hours studying on average...”), and 4. <other>: mentioning other relevant information (e.g. “Revising material will improve your performance”). For the corpus creation, 11 lecturers selected the content to be conveyed in a summary, given the set of raw data (Gkatzia et al., 2013). As a result, for the same student there are various summaries provided by the different experts. This characteristic of the dataset, that each instance is associated with more than one solution, additionally motivates the use of multi-label classification, which is concerned with learning from examples, where each example is associated with multiple labels. Our analysis of the dataset showed that there are significant correlations between the factors, for example, the number of lectures attended (LA) correlates with the student’s understanding of the material (Und), see Table 2. As we will d"
P14-1116,E14-4041,1,0.751077,"2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classifica"
P14-1116,P13-1138,0,0.0126066,"), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the two methods lies in that the 1232 collective content selection requires the consideration of an individual preference score (which is defined as the preference of the entity to be selected or omitted, and it is based on"
P14-1116,P13-2100,0,0.0128051,"al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is similar to our proposed method in that it is a classification task that predicts the templates from the same instance simultaneously. The difference between the t"
P14-1116,P10-1103,1,0.868322,"report generation from clinical data (Hunter et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debrief generation from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The important tasks of time-series data summarisation systems are content selection (what to say), surface realisation (how to say it) and information presentation (Document Planning, Ordering, etc.). In this work, we concentrate on content selection. Previous methods for content selection include Reinforcement Learning (Rieser et al., 2010); multi-objective optimisation (Gkatzia et al., 2014); Gricean Maxims (Sripada et al., 2003); Integer Linear Programming (Lampouras and Androutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores assigned to content (Androutsopoulos et al., 2013); a combination of statistical and template-based approaches to NLG (Kondadadi et al., 2013); statistical acquisition of rules (Duboue and McKeown, 2003) and the Hidden Markov model approach for Content Selection and ordering (Barzilay and Lee, 2004). Collective content selection (Barzilay and Lapata, 2004) is simi"
P14-1116,H05-1042,0,\N,Missing
W07-0312,N06-1034,0,0.0119265,"that 87 scores for this question were significantly higher for interactive mode (M = 3.93) than stealth mode (M=3.27). Our current evaluation model uses User Satisfaction as a response variable in line with previous PARADISE evaluations (Walker et al., 2001). However, User Satisfaction may not be the most appropriate metric for military applications. Unlike commercial applications, the goal of a military system is not to please the user but rather to complete a mission in a highly effective and safe manner. Therefore, a metric such as mission effectiveness may be more appropriate. Similarly, (Forbes-Riley and Litman, 2006) use the domain-specific response variable, of student learning in their evaluation model. An obvious extension to this study is to test in more realistic environments where the users may be experiencing stress in noisy environments. Initial studies have been performed whereby users are physically exerted. These studies did not show a degradation in performance. In addition, initial tests outside in noisy and windy environments emphasize the need for a high quality noise canceling microphone. Further, more extensive tests of this type are needed. In summary, we have presented the WIRE spoken l"
W07-0312,P97-1035,0,0.065734,"support for Army basic training groups. The session began with a brief introduction to the WIRE system. Following that, participants reviewed a series of self-paced training slides. They then completed two sets of four scenarios, with one set completed in stealth mode and the other in interactive mode. A total of 523 utterances were collected. Participants were asked to complete five-question surveys at the end of each set of scenarios. For the regression model described below, we averaged User Satisfaction scores for both types of interaction modes. We adopted the PARADISE evaluation method (Walker et al., 1997). PARADISE is a “decisiontheoretic framework to specify the relative contribution of various factors to a system’s overall performance.” Figure 2 shows the PARADISE model which defines system performance as a weighted function of task-based success measures and dialogue-based cost measures. Dialogue costs are further divided into dialogue efficiency measures and qualitative measures. Weights are calculated by correlating User Satisfaction with performance. Q5: I would recommend that this system be fielded (Future Use). These questions are modified from the more traditional User Satisfaction qu"
W07-0312,P01-1066,0,\N,Missing
W09-3922,E06-2009,1,0.749865,"take next, such as asking for price range after establishing preferred cuisine type. General aspects of dialogue, such as confirmation and clarification strategies, are handled by the domain-general DM. Values for constraints on transitions and branching in the BPM, for example “present insurance offer if the user is business-class”, are compiled into domainspecific parts of the Information State. XML format is used for BPMs, and they are compiled into finite state machines consulted by the spoken dialogue system. The domain-general dialogue manager was mostly abstracted from the TALK system (Lemon et al., 2006). 2 The DUDE Development Environment Figure 1 shows the DUDE Development Environment architecture whereby the main algorithm takes the business-user resources and databases as input and uses these to automatically generate the spoken dialogue system which includes a Voice XML generator. Advantages of using businessuser resources such as Business Process Models (BPM) (Williams, 1967) include the fact that graphical interfaces and authoring environments are widely available (e.g. Eclipse). In addition, business-user resources can contain a lot of additional information as well as call flow inclu"
W09-3922,W03-2123,1,\N,Missing
W11-2002,2007.sigdial-1.23,1,\N,Missing
W11-2017,N07-1053,0,0.0734426,"Missing"
W11-2017,N06-1018,0,0.0202095,"ns are linguistic expressions that are used to refer to a date and are often a source of confusion in human-human, human-computer and text interactions such as emails and instant messaging. For example, “Let’s meet next Sunday”– “do you mean Sunday this week or a week on Sunday?”. (Mccoy and Strube, 1999) state that changes in temporal structure in text are often indicated by either Much work in the field of Natural Language Processing concerns understanding and resolving these temporal expressions in text (Gerber et al., 2002; Pustejovsky et al., 2003; Ahn et al., 2007; Mazur and Dale, 2007; Han et al., 2006), however, little work has looked at how best to plan and realise temporal expressions in order to minimize ambiguity and confusion in a Spoken Dialogue System (SDS). (Reiter et al., 2005) presented a data driven approach to generating TEs to refer to time in weather forecast information where appropriate expressions were identified using contextual features using supervised learning. We adopt an adaptive, data-driven reinforcement learning approach instead. Similar data-driven approaches have been applied to information presentation (Rieser et al., 2010; Walker et al., 2007) where each Natura"
W11-2017,P10-1008,1,0.720143,"appropriate expressions were identified using contextual features using supervised learning. We adopt an adaptive, data-driven reinforcement learning approach instead. Similar data-driven approaches have been applied to information presentation (Rieser et al., 2010; Walker et al., 2007) where each Natural Language Generation (NLG) action is a sequential decision point, based on the current dialogue context and expected long-term reward of that action. A data-driven approach has also been applied to the problem of referring expression generation in dialogue for expert and noviceusers of a SDS (Janarthanam and Lemon, 2010). However, to date, there has been no previous work on adaptive data-driven approaches for temporal referring expression generation, where uncertainty in 142 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 142–151, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics the stochastic environment is explicitly modelled. The data-driven approach to temporal expression generation presented here is in the context of appointment scheduling dialogues. The fact that there are multiple ways that a tim"
W11-2017,J88-2003,0,0.0258364,"Missing"
W11-2017,P10-1103,1,0.822432,"Missing"
W12-1509,D10-1049,0,0.0275246,"ave also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances. This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation. In order to 50 Buffer-Based Incremental Processing A general abstract model of incremental processing based on buffers and a processor was developed by Schlangen and Skantze (2009) and is illustrated in Figure 2. It assumes that the left"
W12-1509,W11-2015,0,0.0714349,"rved in human-human conversation. Doing this in a deterministic fashion through hand-written rules would be time consuming and potentially inaccurate, with no guarantee of optimality. In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion. 2 Previous Work: Incremental Processing Architectures The smallest unit of processing in incremental systems is called incremental unit (IU). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010;"
W12-1509,W10-4342,0,0.253369,"n rules would be time consuming and potentially inaccurate, with no guarantee of optimality. In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion. 2 Previous Work: Incremental Processing Architectures The smallest unit of processing in incremental systems is called incremental unit (IU). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has esta"
W12-1509,W11-2814,1,0.901339,"Missing"
W12-1509,W11-2011,1,0.88201,"Missing"
W12-1509,W09-3902,0,0.0484885,"ontent selection (IU2 - IU5) and surface realisations (IU6 - IU9, etc.). 51 Beat-Driven Incremental Processing In contrast to the buffer-based architectures, alternative incremental systems do not reuse previous partial hypotheses of the user’s input (or the system’s best output) but recompute them at each processing step. We follow Baumann et al. (2011) in calling them ‘beat-driven’ systems. Raux and Eskenazi (2009) use a cost matrix and decision theoretic principles to optimise turn-taking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. 2.3 Decision-making in Incremental Systems Some of the main advantages of the buffer- and ISUbased approaches include their inherently incremental mechanisms for updating and revising system hypotheses. They are able to process input of varying size and type and, at the same time, produce arbitrarily complex output which is monitored and can be modified at any time. On the other hand, current models are based on deterministic decision making and thus shar"
W12-1509,P10-1008,1,0.261639,"In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data,"
W12-1509,P10-1157,0,0.111541,"Missing"
W12-1509,W03-2311,0,0.489373,"remental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process. In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient. Users also indicated that they knew better when to start speaking themselves. Alternative approaches to incremental NLG include Kilger and Finkler (1995) who present an early approach based on Tree-Adjoining Grammar, and Purver and Otsuka (2003) who define an incremental generator based on Dynamic Syntax. Both of these generators can monitor their own output and initiate corrections if necessary. Over recent years, adaptive and data-driven ap49 INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49–58, c Utica, May 2012. 2012 Association for Computational Linguistics Self-correction (the system made a mistake) USR I want Italian food in the centre of town . . . SYS OK. I found 35 Indian restaurants . . . USR No, I want Italian. SYS oh sorry . . . SYS I have 24 Italian restaurants in the city c"
W12-1509,N09-1071,0,0.0613976,"c. and all types of dialogue acts. The incremental ISU model is shown in Figure 3. Note that this hierarchical architecture transfers well to the “classical” division of NLG levels into utterance (IU1), content selection (IU2 - IU5) and surface realisations (IU6 - IU9, etc.). 51 Beat-Driven Incremental Processing In contrast to the buffer-based architectures, alternative incremental systems do not reuse previous partial hypotheses of the user’s input (or the system’s best output) but recompute them at each processing step. We follow Baumann et al. (2011) in calling them ‘beat-driven’ systems. Raux and Eskenazi (2009) use a cost matrix and decision theoretic principles to optimise turn-taking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. 2.3 Decision-making in Incremental Systems Some of the main advantages of the buffer- and ISUbased approaches include their inherently incremental mechanisms for updating and revising system hypotheses. They are able to process input of varying size and type"
W12-1509,P10-1103,1,0.862448,"and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contex"
W12-1509,E09-1081,0,0.20724,"explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances. This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation. In order to 50 Buffer-Based Incremental Processing A general abstract model of incremental processing based on buffers and a processor was developed by Schlangen and Skantze (2009) and is illustrated in Figure 2. It assumes that the left buffer of a module, such as the NLG module, receives IUs from one or more other processing modules, such as the dialogue manager. These input IUs are then passed on to the processor, where they are mapped to corresponding (higher-level) IUs. For an NLG module, this could be a mapping from the dialogue act present(cuisine=Indian) to the realisation ‘they serve Indian food’. The resulting IUs are passed on to the right buffer which co-incides with the left buffer of another module (for example the speech synthesis module in our example)."
W12-1509,W10-4301,0,0.366113,"ypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence. 1 Introduction Traditionally, the smallest unit of speech processing for interactive systems has been a full utterance with strict, rigid turn-taking. Components of these interactive systems, including NLG systems, have so far treated the utterance as the smallest processing unit that triggers a module into action. More recently, work on incremental systems has shown that processing smaller ‘chunks’ of user input can improve Skantze and Hjalmarsson (2010) present a model of incremental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process. In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient. Users also indicated that they knew better when to start speaking themselves. Alternative approaches to incremental NLG include Kilger and Finkler (1995) who present an early approach based on Tree-"
W12-1509,E09-1085,0,0.198264,"se, purge and commit. Whenever new IUs enter the module’s left buffer, the module’s knowledge base is updated to reflect the new information. Such information typically corresponds to the current best hypothesis of a preceding processing module. As a property of incremental systems, however, such hypotheses can be revised by the respective preceding module and, as a result, the knowledge bases of all subsequent modules need to be purged and updated to the newest hypothesis. Once a hypothesis is certain to not be revised anymore, it is committed. For concrete implementations of this model, see Skantze and Schlangen (2009), Skantze and Hjalmarsson (2010), Baumann and Schlangen (2011). An implementation of an incremental dialogue manager is based on the Information State Update (ISU) model (Buss et al., 2010; Buss and Schlangen, 2011). The model is related in spirit to the bufferbased architecture, but all of its input processing and output planning is realised by ISU rules. This is true for the incremental ‘house-keeping’ actions update, revise, etc. and all types of dialogue acts. The incremental ISU model is shown in Figure 3. Note that this hierarchical architecture transfers well to the “classical” division"
W12-1509,W09-0626,0,0.028565,"Missing"
W12-1509,P04-1011,0,\N,Missing
W12-1808,W10-4342,0,0.0138484,"n is in fact incremental (Tanenhaus and Brown-Schmidt, 2008; Levelt, 1989). Using a whole utterance as the unit of choice makes dialogues longer, unnatural and stilted and ultimately interferes with a user’s ability to focus on their goal (Allen et al., 2001). A new generation of Incremental SDS (ISDS) are being developed that deal with ‘micro-turns’ (subutterance processing units) resulting in dialogues that are more fluid and responsive. Recent work has shown that processing smaller ‘chunks’ of input and output can improve the user experience (Aist et al., 2007; Skantze and Schlangen, 2009; Buss et al., 2010; Baumann et al., 2011; Selfridge et al., 2011). Incrementality enables the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system). In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a recognition error or"
W12-1808,W12-1509,1,0.837166,"). Figure 1: Incremental phenomena observed in humanhuman dialogue that systems should be able to model. of-Oz experiments can be used to collect data from the system side, but user-initiated phenomena, such as the user changing his/her mind are more difficult to instigate. Therefore, data collections of naturally occurring incremental phenomena in human-human settings will be essential for further development of incremental systems. Such data can inform user simulations which provide means of training stochastic SDS with less initial data and can compensate for data sparsity. For example, in Dethlefs et al. (2012) the user simulation can change its mind and react to different NLG strategies such as giving information with partial input or waiting for complete input from the user. Both the academic community and industry would benefit from open access data, such as will be collected in the Parlance project and made available to the dialogue community2 . There would also need to be a clear path from academic research on ISDS to industry standards such as VoiceXML to facilitate adoption. Various components and techniques of ISDS are needed to handle ‘micro-turns’. Challenges here include recognizing and u"
W12-1808,W11-2014,0,0.0122953,"rown-Schmidt, 2008; Levelt, 1989). Using a whole utterance as the unit of choice makes dialogues longer, unnatural and stilted and ultimately interferes with a user’s ability to focus on their goal (Allen et al., 2001). A new generation of Incremental SDS (ISDS) are being developed that deal with ‘micro-turns’ (subutterance processing units) resulting in dialogues that are more fluid and responsive. Recent work has shown that processing smaller ‘chunks’ of input and output can improve the user experience (Aist et al., 2007; Skantze and Schlangen, 2009; Buss et al., 2010; Baumann et al., 2011; Selfridge et al., 2011). Incrementality enables the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system). In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a recognition error or a change in user’s preferences. Some examples o"
W12-1808,E09-1085,0,0.013845,"s that human-human interaction is in fact incremental (Tanenhaus and Brown-Schmidt, 2008; Levelt, 1989). Using a whole utterance as the unit of choice makes dialogues longer, unnatural and stilted and ultimately interferes with a user’s ability to focus on their goal (Allen et al., 2001). A new generation of Incremental SDS (ISDS) are being developed that deal with ‘micro-turns’ (subutterance processing units) resulting in dialogues that are more fluid and responsive. Recent work has shown that processing smaller ‘chunks’ of input and output can improve the user experience (Aist et al., 2007; Skantze and Schlangen, 2009; Buss et al., 2010; Baumann et al., 2011; Selfridge et al., 2011). Incrementality enables the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These include more natural turn-taking through rapid system responses, grounding through the generation of backchannels and feedback, and barge-ins (from both user and system). In addition, corrections and self-corrections through constant monitoring of user and system utterances play an important role, enabling the system to recover smoothly from a re"
W13-2115,D10-1049,0,0.25429,"entire semester. As a case study, we took a module in Artificial Intelligence and asked students to fill out a very short diarytype questionnaire on a weekly basis. Questions included, for example, number of deadlines, number of classes attended, severity of personal issues. These data were then combined with the marks from the weekly lab reflecting the students’ performance. As data is gathered each week in the 2 Related Work Report generation from time-series data has been researched widely and existing methods have been used in several domains such as weather forecasts (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), clinical data summarisation (Hunter 1 http://www.thestudentsurvey.com/ 115 Proceedings of the 14th European Workshop on Natural Language Generation, pages 115–124, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content Selection) and how to"
W13-2115,W11-2017,1,0.905463,"Missing"
W13-2115,N04-1015,0,0.0917522,"et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content Selection) and how to say it (Surface Realisation). In this work we concentrate on the former. Previous methods for content selection include Gricean Maxims (Sripada et al., 2003); collective content selection (Barzilay and Lapata, 2004); and the Hidden Markov model approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven systems that seek to simultaneously optimize both content selection and surface realisation have the potential to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods"
W13-2115,W10-4324,1,0.830817,"to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods (see Section 4 for motivation) which have been successfully applied to other NLG tasks, such as Temporal Expressions Generation (Janarthanam et al., 2011), Lexical Choice (Janarthanam and Lemon, 2010), generation of adaptive restaurant summaries in the context of a dialogue system (Rieser et al., 2010) and generating instructions (Dethlefs and Cuayahuitl, 2011). 3 Figure 1 shows graphically our approach to the development of a generation system. Firstly, we collected data from students including marks, demographic details and weekly study habits. Next, we created templates for surface realisation with the help of a Teaching and Learning expert. These templates were used to generate summaries that were rated by lecturers. We used these ratings to train the learning agent. The output of the"
W13-2115,W10-4217,0,0.248767,"students across the entire semester. As a case study, we took a module in Artificial Intelligence and asked students to fill out a very short diarytype questionnaire on a weekly basis. Questions included, for example, number of deadlines, number of classes attended, severity of personal issues. These data were then combined with the marks from the weekly lab reflecting the students’ performance. As data is gathered each week in the 2 Related Work Report generation from time-series data has been researched widely and existing methods have been used in several domains such as weather forecasts (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), clinical data summarisation (Hunter 1 http://www.thestudentsurvey.com/ 115 Proceedings of the 14th European Workshop on Natural Language Generation, pages 115–124, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content S"
W13-2115,W10-1301,0,0.112214,"s data is gathered each week in the 2 Related Work Report generation from time-series data has been researched widely and existing methods have been used in several domains such as weather forecasts (Belz and Kow, 2010; Angeli et al., 2010; Sripada et al., 2004), clinical data summarisation (Hunter 1 http://www.thestudentsurvey.com/ 115 Proceedings of the 14th European Workshop on Natural Language Generation, pages 115–124, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics et al., 2011; Gatt et al., 2009), narrative to assist children with communication needs (Black et al., 2010) and audiovisual debriefs from sensor data from Autonomous Underwater Vehicles missions (Johnson and Lane, 2011). The two main challenges for time-series data summarisation are what to say (Content Selection) and how to say it (Surface Realisation). In this work we concentrate on the former. Previous methods for content selection include Gricean Maxims (Sripada et al., 2003); collective content selection (Barzilay and Lapata, 2004); and the Hidden Markov model approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven syste"
W13-2115,N12-1093,0,0.0127296,"del approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven systems that seek to simultaneously optimize both content selection and surface realisation have the potential to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods (see Section 4 for motivation) which have been successfully applied to other NLG tasks, such as Temporal Expressions Generation (Janarthanam et al., 2011), Lexical Choice (Janarthanam and Lemon, 2010), generation of adaptive restaurant summaries in the context of a dialogue system (Rieser et al., 2010) and generating instructions (Dethlefs and Cuayahuitl, 2011). 3 Figure 1 shows graphically our approach to the development of a generation system. Firstly, we collected data from students including marks, demographic details and weekly study habits."
W13-2115,P10-1103,1,0.747106,"face Realisation). In this work we concentrate on the former. Previous methods for content selection include Gricean Maxims (Sripada et al., 2003); collective content selection (Barzilay and Lapata, 2004); and the Hidden Markov model approach for content selection and ordering (Barzilay and Lee, 2004). NLG systems tend to be very domain-specific and data-driven systems that seek to simultaneously optimize both content selection and surface realisation have the potential to be more domain-independent, automatically optimized and lend themselves to automatic generalization (Angeli et al., 2010; Rieser et al., 2010; Dethlefs and Cuayahuitl, 2011). Recent work on report generation uses statistical techniques from Machine Translation (Belz and Kow, 2010), supervised learning (Angeli et al., 2010) and unsupervised learning (Konstas and Lapata, 2012). Here we apply Reinforcement Learning methods (see Section 4 for motivation) which have been successfully applied to other NLG tasks, such as Temporal Expressions Generation (Janarthanam et al., 2011), Lexical Choice (Janarthanam and Lemon, 2010), generation of adaptive restaurant summaries in the context of a dialogue system (Rieser et al., 2010) and generatin"
W13-2115,W11-2814,0,\N,Missing
W13-2115,H05-1042,0,\N,Missing
W13-4026,W12-1509,1,0.682243,"Missing"
W13-4026,P13-1123,1,0.0875028,"Missing"
W13-4026,W11-2014,0,0.0111653,"d phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive search through a Spoken Dialogue System (SDS). Th"
W13-4026,E09-1085,0,0.210008,"uthentic Afghan cuisine. USR What is the address and phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive se"
W13-4047,W11-2814,1,0.884276,"Missing"
W13-4047,W12-1625,0,0.0224747,"Missing"
W13-4047,W10-4306,0,0.0606422,"Missing"
W13-4047,J00-3003,0,0.245043,"Missing"
W14-4336,P13-1123,1,0.850652,"Missing"
W14-4336,W13-4026,1,0.906157,"LG) components. We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions. 1 Introduction With the advent of evaluations “in the wild”, emphasis is being put on converting research prototypes into mobile applications that can be used for evaluation and data collection by real users downloading the application from the market place. This is the motivation behind the work demonstrated here where we present a modular framework whereby research components from the Parlance project (Hastie et al., 2013) can be plugged in, tested and evaluated in a mobile environment. The goal of Parlance is to perform interactive search through speech in multiple languages. The domain for the demonstration system is interactive search for restaurants in Cambridge, UK for Mandarin and San Francisco, USA for English. The scenario is that Mandarin speaking tourists would be able to download the application and use it to learn about restaurants in English speaking towns and cities. 2 Figure 1: Overview of the Parlance Mandarin mobile application system architecture Figure 2: Overview of the Parlance English mobi"
W14-4422,P10-1103,1,0.810776,"ture describes both (1) the trend of a factor (e.g. marks increasing, see also Table 1) and (2) the way that this factor could be conveyed in the summary (e.g. one possible way is referring to average, another possible way is referring to increasing/decreasing trend). If both conditions are met, the value of the feature is 1, otherwise 0. The 91 binary features describe all the different possible combinations. For both the Lecturer-adapted and Studentadapted systems, the reward function is derived from a linear regression analysis of the provided dataset, similarly to Walker et al. (1997) and Rieser et al. (2010). factor trend (1) marks stable (2) hours studied decreasing (3) health issues decreasing (4) lectures attended stable (5) personal issues increasing way it is mentioned average trend weeks average trend Table 2: The top 5 features out of the 18 selected through PCR analysis. 4 Evaluation FeedbackGen is evaluated with real users against two alternative systems: one that adapts to lecturers’ preferences and one that adapts to students’ preferences. The output of the three systems is ranked by 30 computer science students from a variety of years of study. Time-series data of three students are p"
W14-4422,W13-2115,1,0.906156,"d Oliver Lemon School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh {dg106, h.hastie, o.lemon}@hw.ac.uk Abstract In addition, different stakeholders often have conflicting goals, needs and preferences, for example managers with employees, or doctors with patients and relatives, or novice and expert users. In our data, for instance, lecturers tend to comment on the hours that the student studied, whereas the students disprefer this content. In our previous work, we showed that lecturers and students have different perceptions regarding what constitutes good feedback (Gkatzia et al., 2013). Here, we present a novel approach to generation by adapting its content to two user groups simultaneously. Producing the same summary for two groups is important as it allows for shared context and meaningful further discussion and reduces development time. We present FeedbackGen, a system that uses a multi-adaptive approach to Natural Language Generation. With the term ‘multi-adaptive’, we refer to a system that is able to adapt its content to different user groups simultaneously, in our case adapting to both lecturers and students. We present a novel approach to student feedback generation"
W14-4422,P14-1116,1,0.843129,"le on the top left shows an example of the time-series data. The table on the bottom left shows an example of described trends. The box on the right presents a target summary. respond to a sequence of actions (see Section 3.2). Temporal Difference (TD) learning (Sutton and Barto, 1990) is used for training three agents in a simulated environment to learn to make optimal content selection decisions: of time-series data. Our previous work showed that when comparing RL and supervised learning in the context of student feedback generation, students preferred the output generated by the RL system (Gkatzia et al., 2014a). Therefore, here, we used RL rather than a supervised learning method. The work described here builds on work reported in (Gkatzia et al., 2014b), which uses as a reward function the average of the Lecturer-adapted and Student-adapted reward functions. However, that method seems to cancel out the preferences of the two groups whereas PCR is able to identify relevant content for both groups. In the next section, we describe the data used, and the methodology for the multi-adaptive NLG, as well as two alternative systems. In Section 4, we describe the comparison of these three systems in a su"
W14-4422,E14-4041,1,0.531209,"le on the top left shows an example of the time-series data. The table on the bottom left shows an example of described trends. The box on the right presents a target summary. respond to a sequence of actions (see Section 3.2). Temporal Difference (TD) learning (Sutton and Barto, 1990) is used for training three agents in a simulated environment to learn to make optimal content selection decisions: of time-series data. Our previous work showed that when comparing RL and supervised learning in the context of student feedback generation, students preferred the output generated by the RL system (Gkatzia et al., 2014a). Therefore, here, we used RL rather than a supervised learning method. The work described here builds on work reported in (Gkatzia et al., 2014b), which uses as a reward function the average of the Lecturer-adapted and Student-adapted reward functions. However, that method seems to cancel out the preferences of the two groups whereas PCR is able to identify relevant content for both groups. In the next section, we describe the data used, and the methodology for the multi-adaptive NLG, as well as two alternative systems. In Section 4, we describe the comparison of these three systems in a su"
W14-4422,P97-1035,0,0.173961,"binary features. Each feature describes both (1) the trend of a factor (e.g. marks increasing, see also Table 1) and (2) the way that this factor could be conveyed in the summary (e.g. one possible way is referring to average, another possible way is referring to increasing/decreasing trend). If both conditions are met, the value of the feature is 1, otherwise 0. The 91 binary features describe all the different possible combinations. For both the Lecturer-adapted and Studentadapted systems, the reward function is derived from a linear regression analysis of the provided dataset, similarly to Walker et al. (1997) and Rieser et al. (2010). factor trend (1) marks stable (2) hours studied decreasing (3) health issues decreasing (4) lectures attended stable (5) personal issues increasing way it is mentioned average trend weeks average trend Table 2: The top 5 features out of the 18 selected through PCR analysis. 4 Evaluation FeedbackGen is evaluated with real users against two alternative systems: one that adapts to lecturers’ preferences and one that adapts to students’ preferences. The output of the three systems is ranked by 30 computer science students from a variety of years of study. Time-series dat"
W14-4422,W10-4324,1,0.821445,"by extracting the most relevant features using Principal Component Regression (PCR) analysis. We then model a reward function that is used for training a Reinforcement Learning agent. Our results with students suggest that, from the students’ perspective, such an approach can generate more preferable summaries than a purely lecturer-adapted approach. 2 Related Work Previous work on NLG systems that address more than one user group employs different versions of a system for each different user group (Gatt et al., 2009; Hunter et al., 2011; Mahamood and Reiter, 2011), makes use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001) or personalises the output to individual users using rules (Reiter et al., 1999). Our proposed system adapts the output to the preferences of more than one user type1 , lecturers and students, but instead of developing many different systems or using User Models that describe different users, it attempts to model the middle ground between the preferences. In order to identify the users’ preferences, we apply Principal Components Regression (PCR (Jolliffe, 1982)) analysis to two datasets that contain lecturers’ and students’ ratings and identi"
W14-4422,W11-2803,0,0.0184742,"e knowledge derived from ratings on feedback summaries by extracting the most relevant features using Principal Component Regression (PCR) analysis. We then model a reward function that is used for training a Reinforcement Learning agent. Our results with students suggest that, from the students’ perspective, such an approach can generate more preferable summaries than a purely lecturer-adapted approach. 2 Related Work Previous work on NLG systems that address more than one user group employs different versions of a system for each different user group (Gatt et al., 2009; Hunter et al., 2011; Mahamood and Reiter, 2011), makes use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001) or personalises the output to individual users using rules (Reiter et al., 1999). Our proposed system adapts the output to the preferences of more than one user type1 , lecturers and students, but instead of developing many different systems or using User Models that describe different users, it attempts to model the middle ground between the preferences. In order to identify the users’ preferences, we apply Principal Components Regression (PCR (Jolliffe, 1982)) analysis to two datasets t"
W18-3911,dreuw-etal-2008-benchmark,0,0.0193153,"tail the setup for the experiments in this paper. Section 5 presents the results of the models employed for this research and discusses these results and the limitations of the approach taken in terms of the data used in Section 5.3. The paper is then concluded and future work is proposed. 2 Related Work 2.1 Sign Language Modelling Despite the availability of many alternatives for language modelling, such as count-based n-grams and their variations (Chen and Goodman, 1999; Rosenfeld, 2000; MacCartney, 2005; Bulyko et al., 2007; Guthrie et al., 2006), hidden Markov models (Dreuw and Ney, 2008; Dreuw et al., 2008), decision trees and decision forests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010), research in sign language modelling predominantly employs simple n-gram models, such as in Cate and Hussain (2017), Forster et al. (2012), and Mass´o and Badia (2010). The reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the method. However, there is a disconnect between n-grams and sign language in that signing is embodied and perceived visually, while the n-grams are commonly applied to text sequence modelling. For this reason, the"
W18-3911,forster-etal-2012-rwth,0,0.0177956,"luded and future work is proposed. 2 Related Work 2.1 Sign Language Modelling Despite the availability of many alternatives for language modelling, such as count-based n-grams and their variations (Chen and Goodman, 1999; Rosenfeld, 2000; MacCartney, 2005; Bulyko et al., 2007; Guthrie et al., 2006), hidden Markov models (Dreuw and Ney, 2008; Dreuw et al., 2008), decision trees and decision forests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010), research in sign language modelling predominantly employs simple n-gram models, such as in Cate and Hussain (2017), Forster et al. (2012), and Mass´o and Badia (2010). The reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the method. However, there is a disconnect between n-grams and sign language in that signing is embodied and perceived visually, while the n-grams are commonly applied to text sequence modelling. For this reason, the authors in Stein et al. (2007), Zhao et al. (2000), Dreuw et al. (2008), Mass´o and Badia (2010), and Forster et al. (2013) model glosses, such as the ones shown on Figure 2, which are obtained from the transcribed sign languages, in a similar way to how lan"
W18-3911,W13-3908,0,0.0396836,"Missing"
W18-3911,N18-1032,0,0.0326673,"lar, the annotation used is missing some of the grammatical aspects of the BSL, such as classifier signs and others. Inclusion of these into the BSL language modelling would increase the OOV count as the English language does not have equivalent language constructs. This raises a question whether a sign language can be modelled using other languages that may have these constructs. More generally, is it possible to model a language with transfer learning using other lessrelated languages? Similar questions have been partly answered for the written languages in the field of machine translation (Gu et al., 2018) by bringing words of different languages close to each other in the latent space. However, nothing similar has been done for the sign languages. From the methodological side of the modelling, additional advanced state of the art techniques should be experimented with to achieve greater quality of the generated models, such as attention mechanism for the recurrent neural networks. Finally, this paper focuses on key techniques for sign processing, which could be part of a larger conversational system whereby signers could interact with computers and home devices through their natural communicat"
W18-3911,guthrie-etal-2006-closer,0,0.0638803,"corpora for statistical model training. Section 4 describes in detail the setup for the experiments in this paper. Section 5 presents the results of the models employed for this research and discusses these results and the limitations of the approach taken in terms of the data used in Section 5.3. The paper is then concluded and future work is proposed. 2 Related Work 2.1 Sign Language Modelling Despite the availability of many alternatives for language modelling, such as count-based n-grams and their variations (Chen and Goodman, 1999; Rosenfeld, 2000; MacCartney, 2005; Bulyko et al., 2007; Guthrie et al., 2006), hidden Markov models (Dreuw and Ney, 2008; Dreuw et al., 2008), decision trees and decision forests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010), research in sign language modelling predominantly employs simple n-gram models, such as in Cate and Hussain (2017), Forster et al. (2012), and Mass´o and Badia (2010). The reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the method. However, there is a disconnect between n-grams and sign language in that signing is embodied and perceived visually, while the n-grams are co"
W18-3911,2007.tmi-papers.26,0,0.0392517,"trees and decision forests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010), research in sign language modelling predominantly employs simple n-gram models, such as in Cate and Hussain (2017), Forster et al. (2012), and Mass´o and Badia (2010). The reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the method. However, there is a disconnect between n-grams and sign language in that signing is embodied and perceived visually, while the n-grams are commonly applied to text sequence modelling. For this reason, the authors in Stein et al. (2007), Zhao et al. (2000), Dreuw et al. (2008), Mass´o and Badia (2010), and Forster et al. (2013) model glosses, such as the ones shown on Figure 2, which are obtained from the transcribed sign languages, in a similar way to how language modelling is applied to automatic transcribed words from speech. Glosses model the meaning of a sign in a written language, but not the execution (i.e. facial expressions, hand movement). Therefore, the more detailed meaning of what was signed may get lost when working with the higher-level glosses. To overcome this issue and to incorporate valuable information in"
W18-3911,zhao-etal-2000-machine,0,0.123436,"rests (Filimonov, 2011), and neural networks (Deena et al., 2016; Mikolov et al., 2010), research in sign language modelling predominantly employs simple n-gram models, such as in Cate and Hussain (2017), Forster et al. (2012), and Mass´o and Badia (2010). The reason for the wide-spread use of n-grams in sign language modelling is the simplicity of the method. However, there is a disconnect between n-grams and sign language in that signing is embodied and perceived visually, while the n-grams are commonly applied to text sequence modelling. For this reason, the authors in Stein et al. (2007), Zhao et al. (2000), Dreuw et al. (2008), Mass´o and Badia (2010), and Forster et al. (2013) model glosses, such as the ones shown on Figure 2, which are obtained from the transcribed sign languages, in a similar way to how language modelling is applied to automatic transcribed words from speech. Glosses model the meaning of a sign in a written language, but not the execution (i.e. facial expressions, hand movement). Therefore, the more detailed meaning of what was signed may get lost when working with the higher-level glosses. To overcome this issue and to incorporate valuable information into sign language mod"
W18-6511,J14-4006,0,0.0241712,"to ours (song recommendations) and their users required no specific training or domain knowledge to perform their task. In addition, given the cost of autonomous systems and effort to run missions, the stakes are considerably higher in our case. Adapting explanations to the various users and their existing mental models is touched upon here. Natural language generation has benefited from such personalisation to the user and this applies to explanation generation also. Previous studies in NLG have included adapting to style (Dethlefs et al., 2014), preferences (Walker et al., 2004), knowledge (Janarthanam and Lemon, 2014) and the context (Dethlefs, 2014) of the user. Whilst there has been much work on personalisation of explanations for recommender systems (Tintarev and Masthoff, 2012), there has been little done specifically for explainable AI/Autonomy. 100 Figure 2: Part of the autonomy model, showing reasons for a vehicle spiralling up. Above/below the dashed line shows what part of the model is used for low/high soundness. dle anaphoric references over multiple utterances e.g. “Where is Vehicle0?” ... “What is its estimated time to completion?”. It also handles ellipsis e.g.“What is the battery level of ve"
W18-6511,E14-1074,1,0.831662,"Missing"
W18-6511,N16-3020,0,0.0278907,"an provide the user with a high fidelity mental model, along with increased confidence and performance (Bras et al., 2018; Lim et al., 2009). Mental models, in cognitive theory, provide one view on how humans reason either functionally (understanding what the robot does) or structurally (understanding how it works) (Johnson-Laird, 1980). Mental models are important as they strongly impact how and whether robots and systems are used. In previous work, explainability has been investigated for a variety of systems and users including: 1) explanation of deep learning models for developers, as in (Ribeiro et al., 2016) who showed that such explanations can increase trust; 2) explanations of planning systems (Tintarev and Kutlak, 2014; Chakraborti et al., 2017); and 3) verbalising robot (Rosenthal et al., 2016) or agent (Harrison et al., 2017) rationalisation. Here, we will be looking at verbalising rationalisation of behaviour of the autonomous system, in a similar way to 3). However, these explanations will not be in terms of a constant stream as in (Harrison et al., 2017), rather as part of a mixed-initiative conversational agent where explanations are available on request. Gregor and Benbasat (1999) desc"
W18-6511,P97-1035,0,0.693805,"Missing"
W19-1601,D18-1287,0,0.0642789,"Missing"
W19-1601,D13-1038,0,0.0319802,"l Linguistics on a nautical chart rather than passively following instructions. In addition, our environment is dynamic. New objects are being created and the user with the agent, together, come up with the desired referring expressions (see Figure 3). A similar interactive method is described in (Schlangen, 2016), where they ground non-linguistic visual information through conversation. In situated dialog, each user can perceive the environment in a different way, meaning that referring expressions need to be carefully selected and verified, especially if the shared environment is ambiguous (Fang et al., 2013). Our contributions include: 1) a generic dialog framework and the implemented software to conduct multiple wizard WoZ experiments for multimodal collaborative planning interaction; 2) available on request, a corpus of 22 dialogs on 2 missions with varying complexities and 3) a corpus analysis (Section 4) indicating that incorporating an extra modality in conjunction with spatial referencing in a chatting interface is crucial for successfully planning missions. 3 Figure 1: Experimental Set-Up, where a) SeeTrack Wizard, b) Chatting Wizard, and c) Subject console. Figure contains images from See"
W19-1601,P97-1035,0,0.73387,"Missing"
W19-1601,P19-1651,0,0.0319595,"d spatial requirements, such as situated robot planning (Misra et al., 2018), developing accurate goal-oriented dialog systems can be extremely challenging, especially in dynamic environments, such as underwater. The ultimate goal of this work is to learn a dialog strategy that optimizes interaction for quality and speed of plan creation, thus linking interaction style with extrinsic task success metrics. Therefore, we conducted a Wizard of Oz (WoZ) study for data collection that can be used to derive reward functions for Reinforcement Learning, as in (Rieser, 2008). Similar work is shown in (Kitaev et al., 2019), where the task involves two humans collaboratively drawing objects with one being the teller and the other the person who draws. The agents must be able to adapt and hold a dialog about novel scenes that will be dynamically constructed. However, in our scenario the agent must be capable of not only adapting but also identifying and editing specific attributes of the dynamic objects that are being created in the process. Previous data collection on situated dialog, such as the Map Task Corpus (Anderson et al., 1991), tackle the importance of referencing objects while giving instructions on a"
W19-1601,E17-1042,0,0.111885,"Missing"
W19-1601,P17-1062,0,0.0662446,"sed in the development of a mixed-initiative datadriven multimodal conversational agent, for planning missions collaboratively with a human operator. With the collected WoZ data, we can capture the main strategies of how to plan a mission and make data-driven simulations possible. Therefore, we can train a Reinforcement Learning agent on simulated dialogs that are fully data-driven with the reward function being derived from our subjects’ preferences, optimizing for plan quality and speed. Moreover, supervised approaches that require less data to learn, such as the Hybrid Code Networks (HCN) (Williams et al., 2017), could be used for the creation of such a system. Finally, the system will be compared to a baseline in a further human evaluation study. Theme 1 Suggestions for extra functionality: Due to delays some subjects were not sure if the program crashed. We had a dialog act “wait” but feedback indicated it would be better to have a visual indicator as well. Note, in the actual future working system, we will not have the same delays as in the WoZ experiment. Theme 2 Chart meta-data: Some subjects (P5 most specifically) desired more meta-data on the plan images they were receiving when referring to a"
W19-1601,P16-1094,0,0.0610886,"Missing"
W19-8403,E14-1074,1,0.774678,"Missing"
W19-8403,hastie-belz-2014-comparative,1,0.791239,"pt close to explainability (Biran and Cotton, 2017). Our Venn diagram given in Figure 1 illustrates that transparent systems could be, by their nature interpretable, without providing explanations and that the activities of interpreting a model and explaining why a system behaves the way it does are fundamentally different. We posit, therefore, that the field moving forward should be wary of using such terms interchangeably. Natural Language Generation will be key to providing explanations, and rationalisation is one approach that we have discussed here. Evaluation of NLG is challenging area (Hastie and Belz, 2014) with objective measures such as BLEU being shown not to reflect human ratings (Liu et al., 2016). How natural language explanations are evaluated will likely be based on, in the near term at least, subjective measures that try to evaluate an explanation in terms of whether it improves a system’s intelligibility, interpretability and transparency along with other typical metrics related to the quality and clarity of the language used (Curry et al., 2017). In future work, it would be advisable to perform empirical analysis of research papers related to the various terms and notions introduced h"
W19-8403,D16-1230,0,0.0226429,"at transparent systems could be, by their nature interpretable, without providing explanations and that the activities of interpreting a model and explaining why a system behaves the way it does are fundamentally different. We posit, therefore, that the field moving forward should be wary of using such terms interchangeably. Natural Language Generation will be key to providing explanations, and rationalisation is one approach that we have discussed here. Evaluation of NLG is challenging area (Hastie and Belz, 2014) with objective measures such as BLEU being shown not to reflect human ratings (Liu et al., 2016). How natural language explanations are evaluated will likely be based on, in the near term at least, subjective measures that try to evaluate an explanation in terms of whether it improves a system’s intelligibility, interpretability and transparency along with other typical metrics related to the quality and clarity of the language used (Curry et al., 2017). In future work, it would be advisable to perform empirical analysis of research papers related to the various terms and notions introduced here and continuously being added into the field of XAI. Table 4: Various notions of Explainabilit"
