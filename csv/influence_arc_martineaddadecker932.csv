2016.jeptalnrecital-jep.6,goryainova-etal-2014-morpho,0,0.0528345,"Missing"
2016.jeptalnrecital-jep.6,A00-2038,0,0.196863,"Missing"
2016.jeptalnrecital-jep.71,gravier-etal-2012-etape,0,0.0593527,"Missing"
2016.jeptalnrecital-jep.81,J81-4005,0,0.771917,"Missing"
2016.jeptalnrecital-jep.81,gravier-etal-2012-etape,0,0.044814,"Missing"
2016.jeptalnrecital-jep.81,N10-1025,0,0.043427,"Missing"
2016.jeptalnrecital-jep.81,D14-1162,0,0.0742615,"Missing"
2016.jeptalnrecital-jep.81,P06-2093,0,0.0997918,"Missing"
2016.jeptalnrecital-jep.81,P10-1040,0,0.158083,"Missing"
2020.jeptalnrecital-jep.33,gravier-etal-2012-etape,0,0.0416051,"Missing"
2020.sltu-1.31,L18-1189,0,0.0199586,"r Romanian are lacking. National oral annotated corpora for Romanian are scarce and rather difficult to access (Mîrzea-Vasile, 2017), needing permission from the coordinator. Either stored on cassette tapes (ROVA, CORV, IVLRA; Dascălu, 2002, 2011; Ionescu-Ruxăndoiu 2002) or magnetic tapes (AFLR; Marin, 1996), there are certain drawbacks which prove harder to overcome such as poor audio quality, vague metadata, unstructured interview making the data challenging to compare, ambiguous policy of data collection and speaker consent. Newer corpora, such as ROMBAC (Ion et al., 2012) or CoRoLa (Barbu Mititelu et al., 2018), are more inclined towards written text data acquisition and processing. During the last decade, however, there have been several attempts to build ASR systems on small corpora (Petrea et al., 2010; Burileanu et al., 2012). As part of the speech technology development in the Quaero program1, a Romanian ASR system targeting broadcast and web audio was built but the acoustic models were developed in an unsupervised manner similarly to the method employed in Lamel & Vieru (2010), as no detailed annotations were available for the audio training data downloaded from a variety of websites. A few st"
2020.sltu-1.31,gravier-etal-2012-etape,0,0.0850651,"Missing"
adda-decker-etal-2008-annotation,J96-2004,0,\N,Missing
adda-decker-etal-2008-developments,pellegrini-lamel-2006-experimental,1,\N,Missing
ben-jannet-etal-2014-eter,E12-1018,1,\N,Missing
ben-jannet-etal-2014-eter,W11-0411,1,\N,Missing
ben-jannet-etal-2014-eter,galibert-etal-2014-etape,1,\N,Missing
ben-jannet-etal-2014-eter,I11-1058,1,\N,Missing
ben-jannet-etal-2014-eter,doddington-etal-2004-automatic,0,\N,Missing
ben-jannet-etal-2014-eter,C96-1079,0,\N,Missing
ben-jannet-etal-2014-eter,M93-1007,0,\N,Missing
F12-2028,bonneau-maynard-etal-2006-results,0,0.0343222,"Missing"
F12-2028,garnier-rizet-etal-2008-callsurf,0,0.0337718,"Missing"
F12-2028,W11-0411,1,0.887978,"Missing"
F12-2028,sekine-nobata-2004-definition,0,0.0299042,"Missing"
F12-2028,sekine-etal-2002-extended,0,0.106932,"Missing"
F12-2028,2009.jeptalnrecital-court.23,0,0.0625935,"Missing"
L18-1674,L18-1531,1,0.879738,"Missing"
lavergne-etal-2014-automatic,adda-decker-etal-2008-developments,1,\N,Missing
lavergne-etal-2014-automatic,J96-1002,0,\N,Missing
lavergne-etal-2014-automatic,P10-1052,1,\N,Missing
luzzati-etal-2014-human,nemoto-etal-2008-speech,1,\N,Missing
luzzati-etal-2014-human,gravier-etal-2012-etape,0,\N,Missing
rouas-etal-2010-comparison,ernestus-etal-2014-nijmegen,0,\N,Missing
rouas-etal-2010-comparison,galliano-etal-2006-corpus,0,\N,Missing
snoeren-etal-2010-study,adda-decker-etal-2008-developments,1,\N,Missing
vasilescu-etal-2010-role,toney-etal-2008-evaluation,1,\N,Missing
vasilescu-etal-2010-role,C90-2044,0,\N,Missing
vasilescu-etal-2010-role,J06-3004,0,\N,Missing
vasilescu-etal-2010-role,J99-4003,0,\N,Missing
vasilescu-etal-2010-role,rosset-petel-2006-ritel,1,\N,Missing
W18-5804,U17-1006,0,0.0305163,"world’s languages are expected to go extinct during this century – as much as half of them according to Crystal (2002) and Janson (2003). Such predictions have subsequently fostered a growing interest for a new field, Computational Language Documentation (CLD), as it is now clear that traditional field linguistics alone will not meet the challenge of preserving and documenting all of these languages. CLD attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation (e.g. (Anastasopoulos and Chiang, 2017; Adams et al., 2017)). A remarkable effort in this direction has improved the data collection 1 We indifferently use the terms word discovery and word segmentation to denote the task defined in Section 2.2. 32 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 32–42 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 capture simple aspects of the syntax and some less trivial aspects of the morphological and phonological structures. As discussed below, both"
W18-5804,C16-1086,0,0.0160834,"nguage, Basaa (A43 (Hamlaoui and Makasso, 2015)). mboshi/myene corresponds to a somewhat crude morphology of Mboshi, also applicable to Myene. Last mboshi/myene_NV refines mboshi/myene with a specification of the morphology of nouns and verbs. Additionally, for basaa, mboshi/myene and mboshi/myene_NV which introduce a notion of prefix, we also test a variant (called respectively basaa+, mboshi/myene+ and mboshi/myene_NV+) containing an explicit list of prefixes in Mboshi. Grammars 4.1 Structuring Grammar Sets Our starting point is the set of grammars used in (Johnson and Goldwater, 2009) and (Eskander et al., 2016) which we progressively specialize through an iterative refinement process involving both field linguists and computer scientists. As we wish to evaluate specific linguistic hypotheses, the initial space of interesting grammars has been generalized in a modular, systematic, and hierarchical way as follows. We distinguish four sections in each grammar: sentence, word, syllable, character. For each section, we test multiple hypotheses, gradually incorporating more linguistic structure. Every hypothesis inside a given section can be combined with every hypothesis of any other section,7 thereby al"
W18-5804,L18-1531,1,0.878737,"Missing"
W18-5804,W17-0123,0,0.0281211,"troduction A large number of the world’s languages are expected to go extinct during this century – as much as half of them according to Crystal (2002) and Janson (2003). Such predictions have subsequently fostered a growing interest for a new field, Computational Language Documentation (CLD), as it is now clear that traditional field linguistics alone will not meet the challenge of preserving and documenting all of these languages. CLD attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation (e.g. (Anastasopoulos and Chiang, 2017; Adams et al., 2017)). A remarkable effort in this direction has improved the data collection 1 We indifferently use the terms word discovery and word segmentation to denote the task defined in Section 2.2. 32 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 32–42 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17 capture simple aspects of the syntax and some less trivial aspects of the morphological and phonological structures. As d"
W18-5804,W14-2201,0,0.0714917,"Missing"
W18-5804,P06-1085,0,0.108277,"Missing"
W18-5804,Q14-1008,0,0.0362652,"Missing"
W18-5804,D13-1034,0,0.0370391,"Missing"
W18-5804,J99-1004,0,0.0168827,"of the form A → β, with A ∈ N and β ∈ (N ∪ W )∗ , and S ∈ N the start symbol. Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level. Assuming that S, Words, and Word belong to N , the top level rules will typically look like: S → Words; Words → Word Words; Words → Word, the last two rules abbreviated as Words → Word +. Probabilistic CFGs (PCFGs) (Johnson, 1998) extend this model by associating each rule with a scalar value θA→β , such that for each A ∈ N , P β θA→β = 1. Under some technical conditions (Chi, 1999), PCFGs define probability distributions over the set of parse trees, where the probability of a tree is a product of the probability of the rules it contains. PCFGs can be learned in a supervised way from treebanks or in a unsupervised manner using, for instance, the EM algorithm (Lari and Young, 1990). PCFGs make unrealistic independence assumptions between the different subparts of a tree, an observation that has yielded many subsequent variations and extensions. Adaptor grammars (AGs) (Johnson et al., 2007) define a powerful mechanism to manipulate PCFG distributions to better match the oc"
W18-5804,J98-4004,0,0.0832912,"e useful for word discovery. A CFG is a 4-tuple G = (N, W, R, S) where N and W are respectively the non-terminal and terminal symbols, R a finite set of rules of the form A → β, with A ∈ N and β ∈ (N ∪ W )∗ , and S ∈ N the start symbol. Our grammars will be used to analyze the structure of complete utterances and the start symbol S will always correspond to the sentence top-level. Assuming that S, Words, and Word belong to N , the top level rules will typically look like: S → Words; Words → Word Words; Words → Word, the last two rules abbreviated as Words → Word +. Probabilistic CFGs (PCFGs) (Johnson, 1998) extend this model by associating each rule with a scalar value θA→β , such that for each A ∈ N , P β θA→β = 1. Under some technical conditions (Chi, 1999), PCFGs define probability distributions over the set of parse trees, where the probability of a tree is a product of the probability of the rules it contains. PCFGs can be learned in a supervised way from treebanks or in a unsupervised manner using, for instance, the EM algorithm (Lari and Young, 1990). PCFGs make unrealistic independence assumptions between the different subparts of a tree, an observation that has yielded many subsequent v"
W18-5804,W08-0704,0,0.775201,"Missing"
W18-5804,P08-1046,0,0.0606297,"Missing"
W18-5804,P14-1027,0,0.0181962,"es of studies, AGs are shown to generalize models of unsupervised word segmentations such as the Bayesian nonparametric model of Goldwater (2006), delivering hierarchical (rather than flat) decompositions for words or sentences. While AGs are essentially viewed as an unsupervised grammatical inference tool, several authors have also tried to better inform grammar inference with external knowledge sources. This is the case of Sirts and Goldwater (2013), who study a semi-supervised learning scheme combining annotated data (parse trees) with raw sentences. The linguistic knowledge considered in (Johnson et al., 2014) aims to better model function words in a Conclusion This paper had two main goals: (1) improve upon a strong baseline for the unsupervised discovery of words in two very low-resource Bantu languages; (2) explore the Adaptor Grammar framework as an analysis and prediction tool for linguists studying a new language. Systematic experiments with 162 grammar configurations for each language have shown that using AGs for word segmentation is a way to test linguistic hypotheses during a language documentation process. Conversely, we have also shown that specializing a generic grammar with language s"
W18-5804,N09-1036,0,0.0291861,"phology of a well-studied Bantu language, Basaa (A43 (Hamlaoui and Makasso, 2015)). mboshi/myene corresponds to a somewhat crude morphology of Mboshi, also applicable to Myene. Last mboshi/myene_NV refines mboshi/myene with a specification of the morphology of nouns and verbs. Additionally, for basaa, mboshi/myene and mboshi/myene_NV which introduce a notion of prefix, we also test a variant (called respectively basaa+, mboshi/myene+ and mboshi/myene_NV+) containing an explicit list of prefixes in Mboshi. Grammars 4.1 Structuring Grammar Sets Our starting point is the set of grammars used in (Johnson and Goldwater, 2009) and (Eskander et al., 2016) which we progressively specialize through an iterative refinement process involving both field linguists and computer scientists. As we wish to evaluate specific linguistic hypotheses, the initial space of interesting grammars has been generalized in a modular, systematic, and hierarchical way as follows. We distinguish four sections in each grammar: sentence, word, syllable, character. For each section, we test multiple hypotheses, gradually incorporating more linguistic structure. Every hypothesis inside a given section can be combined with every hypothesis of an"
W18-5804,Q13-1021,0,0.298022,"licit hierarchical model of word internal structure ; an observation that was one of our primary motivations for using AGs in our language documentation work. In this series of studies, AGs are shown to generalize models of unsupervised word segmentations such as the Bayesian nonparametric model of Goldwater (2006), delivering hierarchical (rather than flat) decompositions for words or sentences. While AGs are essentially viewed as an unsupervised grammatical inference tool, several authors have also tried to better inform grammar inference with external knowledge sources. This is the case of Sirts and Goldwater (2013), who study a semi-supervised learning scheme combining annotated data (parse trees) with raw sentences. The linguistic knowledge considered in (Johnson et al., 2014) aims to better model function words in a Conclusion This paper had two main goals: (1) improve upon a strong baseline for the unsupervised discovery of words in two very low-resource Bantu languages; (2) explore the Adaptor Grammar framework as an analysis and prediction tool for linguists studying a new language. Systematic experiments with 162 grammar configurations for each language have shown that using AGs for word segmentat"
