2009.mtsummit-posters.15,2005.mtsummit-papers.11,0,0.176867,"corpus consisting of 2100 United Nations General Assembly Resolutions with translations in the six official languages of the United Nations, with an average of around 3 million tokens per language. The corpus is available in a preprocessed, formatting-normalized TMX format with paragraphs aligned across multiple languages. We describe the background to the corpus and its content, the process of its construction, and some of its interesting properties. 1 Introduction Parallel corpora are a useful resource for a wide variety of purposes including the training of machine translation algorithms (Koehn, 2005), multilingual terminology extraction (Le An Ha and Corpas, 2008) and even bootstrapping algorithms for languages that do not enjoy the research resources of English (Yarowsky et al., 2001). Multi-parallel corpora can be more useful than bilingual corpora when the additional languages may be used to assist text alignment (Simard, 1999) or as translation bridge languages, in the sense of (Kumar et al., 2007). While many European and Germanic languages already have good parallel research corpus resources, such as JRC-Acquis (Steinberger et al., 2006) and EuroParl (Koehn, 2005), material in the S"
2009.mtsummit-posters.15,D07-1005,0,0.0127414,"construction, and some of its interesting properties. 1 Introduction Parallel corpora are a useful resource for a wide variety of purposes including the training of machine translation algorithms (Koehn, 2005), multilingual terminology extraction (Le An Ha and Corpas, 2008) and even bootstrapping algorithms for languages that do not enjoy the research resources of English (Yarowsky et al., 2001). Multi-parallel corpora can be more useful than bilingual corpora when the additional languages may be used to assist text alignment (Simard, 1999) or as translation bridge languages, in the sense of (Kumar et al., 2007). While many European and Germanic languages already have good parallel research corpus resources, such as JRC-Acquis (Steinberger et al., 2006) and EuroParl (Koehn, 2005), material in the Slavic, Sino-Tibetian and Semitic language families is much rarer. The corpus presented here consists of a collection of documents containing manually Robert Dale2 Centre for Language Technology Macquarie University Sydney, Australia rdale@science.mq.edu.au 2 translated official resolutions of the General Assembly of the United Nations (UN) in the six official languages of the UN: Arabic, Chinese, English, F"
2009.mtsummit-posters.15,ha-etal-2008-mutual,0,0.0467939,"Missing"
2009.mtsummit-posters.15,W99-0602,0,0.0247399,"e the background to the corpus and its content, the process of its construction, and some of its interesting properties. 1 Introduction Parallel corpora are a useful resource for a wide variety of purposes including the training of machine translation algorithms (Koehn, 2005), multilingual terminology extraction (Le An Ha and Corpas, 2008) and even bootstrapping algorithms for languages that do not enjoy the research resources of English (Yarowsky et al., 2001). Multi-parallel corpora can be more useful than bilingual corpora when the additional languages may be used to assist text alignment (Simard, 1999) or as translation bridge languages, in the sense of (Kumar et al., 2007). While many European and Germanic languages already have good parallel research corpus resources, such as JRC-Acquis (Steinberger et al., 2006) and EuroParl (Koehn, 2005), material in the Slavic, Sino-Tibetian and Semitic language families is much rarer. The corpus presented here consists of a collection of documents containing manually Robert Dale2 Centre for Language Technology Macquarie University Sydney, Australia rdale@science.mq.edu.au 2 translated official resolutions of the General Assembly of the United Nations"
2009.mtsummit-posters.15,steinberger-etal-2006-jrc,0,0.046574,"including the training of machine translation algorithms (Koehn, 2005), multilingual terminology extraction (Le An Ha and Corpas, 2008) and even bootstrapping algorithms for languages that do not enjoy the research resources of English (Yarowsky et al., 2001). Multi-parallel corpora can be more useful than bilingual corpora when the additional languages may be used to assist text alignment (Simard, 1999) or as translation bridge languages, in the sense of (Kumar et al., 2007). While many European and Germanic languages already have good parallel research corpus resources, such as JRC-Acquis (Steinberger et al., 2006) and EuroParl (Koehn, 2005), material in the Slavic, Sino-Tibetian and Semitic language families is much rarer. The corpus presented here consists of a collection of documents containing manually Robert Dale2 Centre for Language Technology Macquarie University Sydney, Australia rdale@science.mq.edu.au 2 translated official resolutions of the General Assembly of the United Nations (UN) in the six official languages of the UN: Arabic, Chinese, English, French, Russian, and Spanish. Since resolutions are legally significant, they pass through multiple levels of human translation and verification,"
2009.mtsummit-posters.15,H01-1035,0,0.0412617,"okens per language. The corpus is available in a preprocessed, formatting-normalized TMX format with paragraphs aligned across multiple languages. We describe the background to the corpus and its content, the process of its construction, and some of its interesting properties. 1 Introduction Parallel corpora are a useful resource for a wide variety of purposes including the training of machine translation algorithms (Koehn, 2005), multilingual terminology extraction (Le An Ha and Corpas, 2008) and even bootstrapping algorithms for languages that do not enjoy the research resources of English (Yarowsky et al., 2001). Multi-parallel corpora can be more useful than bilingual corpora when the additional languages may be used to assist text alignment (Simard, 1999) or as translation bridge languages, in the sense of (Kumar et al., 2007). While many European and Germanic languages already have good parallel research corpus resources, such as JRC-Acquis (Steinberger et al., 2006) and EuroParl (Koehn, 2005), material in the Slavic, Sino-Tibetian and Semitic language families is much rarer. The corpus presented here consists of a collection of documents containing manually Robert Dale2 Centre for Language Techno"
bird-etal-2008-acl,D07-1089,0,\N,Missing
bird-etal-2008-acl,W06-1613,0,\N,Missing
bird-etal-2008-acl,N04-1042,0,\N,Missing
bird-etal-2008-acl,radev-etal-2004-mead,1,\N,Missing
C08-1070,P00-1010,0,0.884197,"egri and Marseglia (2005), in their rule-based system for temporal expression recognition and normalisation, use what they call ‘context words’, such as following or later, to decide on the interpretation of a weekday name. Consider the following example: (7) He started studying on March 30 2004, and passed the exam the following Friday. Here, having identified the date March 30 2004 (which happens to be a Tuesday), they then recognise the structure ‘following + trigger’ and reason that the Friday is three days later. 3 Although Ahn et al. (2007) compared their results with those presented by Mani and Wilson (2000), they went on to point out that, for a variety of reasons, the numbers they provided were not really comparable. 4 Filatova and Hovy use the term reference point for what we call the temporal focus. 554 There have also been machine-learning approaches to the interpretation of temporal expressions. Ahn et al. (2005) describes a system developed and tested on the ACE 2004 TERN test corpus. Using lexical features, such as the occurrence of last or earlier in a context window of three words, their maximum entropy classifier picked the correct direction (‘backward’, ‘same’, or ‘forward’) with an a"
C08-1070,N07-1053,0,0.299554,"Missing"
C08-1070,W06-0902,1,0.873658,"Missing"
C08-1070,N06-1018,0,0.112407,"the magnitude and direction of offset. As noted above, in some cases the tense of the controlling verb will indicate the direction of offset; but prepositional attachment ambiguity can easily damage the reliability of such an approach, as demonstrated by the following minimal pair: (5) We can show you some pictures on Monday. (6) We can show you some pictures from Monday. In example (5), the correct PP attachment is required in order to determine that Monday is in the 1 In the literature, a variety of different terms are used: (Schilder and Habel, 2001) call these expressions indexicals, and (Han et al., 2006b) uses the term relative for what we call anaphoric references: in our terminology, both deictic and anaphorical expressions are relative. 2 This reference point is often referred to as the temporal focus or temporal anchor. scope of the verb group can show, allowing us to infer that the Monday in question is in the future. Example (6), on the other hand, is quite ambiguous and requires world knowledge in order to determine the correct attachment. We are interested, therefore, in determining whether some heuristic method might provide good results. In the rest of this paper, we focus on the d"
C08-1070,W01-1309,0,0.0135754,"t from this reference point requires us to determine the magnitude and direction of offset. As noted above, in some cases the tense of the controlling verb will indicate the direction of offset; but prepositional attachment ambiguity can easily damage the reliability of such an approach, as demonstrated by the following minimal pair: (5) We can show you some pictures on Monday. (6) We can show you some pictures from Monday. In example (5), the correct PP attachment is required in order to determine that Monday is in the 1 In the literature, a variety of different terms are used: (Schilder and Habel, 2001) call these expressions indexicals, and (Han et al., 2006b) uses the term relative for what we call anaphoric references: in our terminology, both deictic and anaphorical expressions are relative. 2 This reference point is often referred to as the temporal focus or temporal anchor. scope of the verb group can show, allowing us to infer that the Monday in question is in the future. Example (6), on the other hand, is quite ambiguous and requires world knowledge in order to determine the correct attachment. We are interested, therefore, in determining whether some heuristic method might provide g"
C08-1070,de-marneffe-etal-2006-generating,0,0.00967532,"Missing"
C08-1070,W01-1313,0,0.0960382,"irection of offset. We will not explicitly address the question of determining the temporal focus: although this is clearly a key ingredient, we have found that using the document creation date performs well for the kinds of documents (typically newswire stories and similar document types) we are working with. More sophisticated strategies for temporal focus tracking would likely be required in other genres. 3 Related Work The literature contains a number of approaches to the interpretation of weekday names, although we are not aware of any pre-existing direct comparison of these approaches.3 Filatova and Hovy (2001) assign time stamps to clauses in which an event is mentioned. As part of the overall process, they use a heuristic for the interpretation of weekday names: if the day name in a clause is the same as that of the temporal focus, then the temporal focus is used;4 otherwise, they look for any ‘signal words’ or check the tense of the verb in the clause. An analogous approach is taken for the interpretation of month names. Negri and Marseglia (2005), in their rule-based system for temporal expression recognition and normalisation, use what they call ‘context words’, such as following or later, to d"
C10-1154,P01-1017,0,0.161396,"Missing"
C10-1154,P04-1005,1,0.887791,"nnotations in the corpus. An alternative means of evaluation would be to simply generate a new signal with the reparandum and filler removed, and compare this against a ‘cleaned-up’ version of the utterance; however, Core and Schubert (1999) argue that, especially in the case of speech repairs, it is important not to simply throw away the disfluent elements of an utterance, since they can carry meaning that needs to be recovered for proper interpretation of the utterance. We are therefore interested in the first instance in a model of speech error detection, rather than a model of correction. Johnson and Charniak (2004) describe such a model, using a noisy-channel based approach to the detection of the start and end points of reparanda, interregna and repairs. Since we use this model as our starting point, we provide a more detailed explanation in Section 3. The idea of using a noisy channel model to identify speech repairs has been explored for languages other than English. Honal and Schultz (2003) use such a model, comparing speech disfluency detection in spontaneous spoken Mandarin against that in English. The approach performs well in Mandarin, although better still in English. Both the models just descr"
C10-1154,J10-1001,0,0.0276848,"and Schultz (2003) use such a model, comparing speech disfluency detection in spontaneous spoken Mandarin against that in English. The approach performs well in Mandarin, although better still in English. Both the models just described operate on transcripts of completed utterances. Ideally, however, when we deal with speech we would like to process the input word by word as it is received. Being able to do this would enable tighter integration in both speech recognition 1372 and interpretation, which might in turn improve overall accuracy. The requirement for incrementality is recognised by Schuler et al. (2010), who employ an incremental Hierarchical Hidden Markov Model (HHMM) to detect speech disfluencies. The HHMM is trained on manually annotated parse trees which are transformed by a right corner transformation; the HHMM is then used in an incremental fashion on unseen data, growing the parse structure each time a new token comes in. Special subtrees in this parse can carry a marker indicating that the span of the subtree consists of tokens corresponding to a speech disfluency. Schuler et al.’s approach thus provides scope for detecting disfluencies in an incremental fashion. However, their repor"
C10-1154,C90-3045,0,0.173222,"Missing"
C10-1154,W90-0102,0,\N,Missing
C92-1038,P83-1007,0,0.0115018,"COLING-92, NAtCrES,23-28 AO&apos;t~q""1992 232 domain-independent: the core algorithm should work in any domain, once an appropriate knowledge base and user model has been set up. A version of the algorithm has been implemented within the IDAS natural-language generation system [RML92], and it is performing satisfactorily. The algorithm presented in this paper only generates definite noun phrases that identify an object that is in the current focus of attention. Algorithms and models that can be used to generate pronominal and one-anaphoric referring expressions have been presented elsewhere, e.g., [Sid81,GJW83,Da189]. We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work. Background Distinguishing Descriptions The term &apos;referring expression&apos; has been used by different people to mean different things. In this paper, we define a referring expression in intentional terms: a noun phrase is considered to be a referring expression if and only if its only communicative purpose is to identify an object to the hearer, in Kronfeld&apos;s terminology [Kro86], we only use the modal aspect"
C92-1038,H89-1022,0,0.0349963,"Missing"
C92-1038,P86-1029,0,0.0579534,"elsewhere, e.g., [Sid81,GJW83,Da189]. We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work. Background Distinguishing Descriptions The term &apos;referring expression&apos; has been used by different people to mean different things. In this paper, we define a referring expression in intentional terms: a noun phrase is considered to be a referring expression if and only if its only communicative purpose is to identify an object to the hearer, in Kronfeld&apos;s terminology [Kro86], we only use the modal aspect of Donefian&apos;s distinction between attributive and referential descriptions [Don66]; we consider a noun phrase to be referential if it is intended to identify the object it describes to the hearer, and attributive if it is intended to communicate information about that object to the hearer. This usage is similar to that adopted by Reiter [Rei90b] and Dale and Haddock [DH91], but differs from the terminology used by Appelt [App85], who allowed &apos;referring expressions&apos; to satisfy any communicative goal that could be stated in the underlying logical framework. We here"
C92-1038,P90-1013,1,0.900512,"Missing"
C92-1038,A92-1009,1,0.88093,"sensitive to h u m a n preferences: it attempts to use easily perceivable attributes and basic-level [Ros78] attribute values; and • Supported by SERC grant GR/F/36750. E-mail address is E.Reiter@ed. ac.uk. tAiso of the Centre for Cognitive Science at the University of Edinburgh. E-mail address is R. DaleQed. ac .uk. ACRESDECOLING-92, NAtCrES,23-28 AO&apos;t~q""1992 232 domain-independent: the core algorithm should work in any domain, once an appropriate knowledge base and user model has been set up. A version of the algorithm has been implemented within the IDAS natural-language generation system [RML92], and it is performing satisfactorily. The algorithm presented in this paper only generates definite noun phrases that identify an object that is in the current focus of attention. Algorithms and models that can be used to generate pronominal and one-anaphoric referring expressions have been presented elsewhere, e.g., [Sid81,GJW83,Da189]. We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work. Background Distinguishing Descriptions The term &apos;referring expressio"
C92-1038,P89-1009,1,\N,Missing
C92-1038,J86-3001,0,\N,Missing
C92-2072,J83-3002,0,0.44498,"Missing"
C92-2072,A88-1027,0,0.554888,"Missing"
C92-2072,J80-2003,0,\N,Missing
C92-2072,J81-2002,0,\N,Missing
C92-2072,P91-1031,0,\N,Missing
D08-1057,N04-1015,0,0.199313,"ith [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern hDisplacedPersons[1], HostingCommunities[2]i. ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show that even simple modelling"
D08-1057,J05-3002,0,0.176208,"mations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the input sentences. We see this as an example of conservation. In our work, this general proposition is equivalent to the core information for the summary sentence before the incorporation of supplementary material. In contrast to both compression and conservation work, we focus on augmenting th"
D08-1057,J96-1002,0,0.0370589,"Missing"
D08-1057,D07-1001,0,0.0182264,"nformation. Roughly, text-to-text transformations fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be r"
D08-1057,P02-1057,0,0.0720033,"Missing"
D08-1057,J05-4004,0,0.0500513,"Missing"
D08-1057,J95-2003,0,0.0080821,"ns fall into three categories: those in which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identifie"
D08-1057,P03-1069,0,0.0983791,"ing in camps and with [host families]2 in the districts; A total or partial destruction of over 3,000 homes in Dili affecting at least 14,000 IDPs Figure 2: Examples of the pattern hDisplacedPersons[1], HostingCommunities[2]i. ure 2 showed that mentions of the plight of internationally displaced persons are often followed by descriptions of the impact on the host communities that look after them. In this particular example, this is realised lexically in the co-occurrences of the words displaced and host. Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. However, to our knowledge, using word co-occurrences in this manner to represent schematic knowledge for the purposes of selecting content in a statistically-generated summary sentence has not previously been explored. This paper seeks to determine whether or not such patterns exist in homogeneous data; and furthermore, whether such patterns can be used to better select words from auxiliary sentences. In particular, we propose the “Seed and Grow” approach for this task. The results show"
D08-1057,N03-1020,0,0.12957,"e what words were actually chosen in the summary sentence of the aligned sentence tuple. We are specifically interested in open-class words, and so a stopword list of closed-class words is used to filter the sentences in each test case. We evaluate against the set of open-class words in the human-authored summary sentence using recall and precision metrics. Recall is the size of the intersection of the selected and gold-standard sets, normalised by the length of the gold-standard sentence (in words). This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric (Lin and Hovy, 2003) used in the Document Understanding Conferences2 (DUC). Precision is the size of the intersection normalised by the number of words selected. We also report the F-measure, which is the harmonic mean of the recall and precision scores. Recall, precision and F-measure are measured at various values of n ranging from 1 to the number of open-class words in the gold-standard summary sentence for a particular test case. For the purposes of evaluation, differences in tokens due to morphology were explored crudely via the use of Porter’s stemming algorithm. However, the results from stemming are not t"
D08-1057,J91-1002,0,0.0540133,"which information is compressed, conserved, and augmented. We use these distinctions to organise this overview of the literature. In Sentence Compression work, a single sentence undergoes pruning to shorten its length. Previous approaches have focused on statistical syntactic transformations (Knight and Marcu, 2002). For content selection, discourse-level considerations were proposed by Daum´e III and Marcu (2002), who explored the use of Rhetorical Structure Theory (Mann and Thompson, 1988). More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al., 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. Our work is similar in incorporating discourse-level phenomena for content selection. However, we look at schemalike information as opposed to chains of references and focus on the sentence augmentation task. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. Each sentence set ideally tightly clusters around a single news event. Thus, there is one general proposition to be realised in the summary sentence, identified by finding the common elements in the inpu"
D08-1057,P05-1009,0,0.0682517,"Missing"
D08-1057,P08-2033,1,0.823543,"least one auxiliary sentence). Of the 580 cases, 50 cases were set aside for testing. The remaining 530 cases were used for training. Statistics for the training portion of the sentence augmentation set are provided in Table 1. In this paper, aligned sentence tuples are obtained via manual annotation. Automatic construction of these sentence-level alignments is possible and has been explored by Jing and McKeown (1999). We also envisage using tools for scoring sentence similarity (for example, see Hatzivassiloglou et al. (2001)) for automatically constructing them; this is the focus of work by Wan and Paris (2008). 3 http://ochaonline3.un.org/humanitarianappeal/index.htm 549 5.2 The Baselines Three baselines were used in this work: the random, tf-idf and position baselines. A random word selector shows what performance might be achieved in the absence of any linguistic knowledge. We also sorted all words in the aligned source sentences by their weighted tf-idf scores. This baseline selects words in order until the desired word limit is reached. This baseline is referred to as the tf-idf baseline. Finally, we selected words based on their sentence order, choosing first those words from the key sentence."
D08-1057,I05-5012,1,0.846344,"Missing"
D09-1096,N07-1041,0,0.0236858,"Missing"
D09-1096,W04-1008,0,0.0704116,"benefit from better knowledge of this message structure, facilitating focus on relevant content in specific parts of a message. In particular, access to zone information would allow email classification, summarisation and analysis tools to separate or filter out ‘noise’ and focus on the content in specific zones of a message that are relevant to the application at hand. Email contact mining tools such as that developed by Culotta et al. (2004), for example, might access the email signature zone, while tools that attempt to identify tasks or action items in email (e.g., (Bellotti et al., 2003; Corston-Oliver et al., 2004; Bennett 919 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 919–928, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP content (bottom-posting) or interleaved with the quoted content (inline replying). Confounding the issue further is that users are able to configure their email client to suit their individual tastes, and can change both the syntax of quoting and their quoting style (top, bottom or inline replying) on a permessage basis. To address these challenges, in this paper we describe Zebra, our email zone classification system. First we de"
D09-1096,J97-1003,0,0.427287,"Missing"
D09-1096,W97-0304,0,\N,Missing
D10-1089,N07-1053,0,0.0169626,"Missing"
D10-1089,P00-1010,0,0.278929,", such as information extraction, question answering, and document summarisation. Consequently, the tasks of identifying and assigning values to temporal expressions have recently received significant attention, resulting in the creation of mature corpus annotation guidelines (e.g. TIMEX21 and TimeML2 ), publicly 1 See http://fofoca.mitre.org. 2 See http://timeml.org. 2 Centre for Language Technology, Macquarie University, NSW 2109, Sydney, Australia Pawel.Mazur@mq.edu.au Robert.Dale@mq.edu.au available annotated corpora (ACE,3 TimeBank4 ) and a number of automatic taggers (see, for example, (Mani and Wilson, 2000; Schilder, 2004; Hacioglu et al., 2005; Negri and Marseglia, 2005; Saquete, 2005; Han et al., 2006; Ahn et al., 2007)). However, existing corpora have their limitations. In particular, the documents in these corpora tend to be limited in length and, in consequence, discourse structure. This impacts on the number, range and variety of temporal expressions they contain. Existing research carried out on the interpretation of temporal expressions, e.g. by (Baldwin, 2002; Ahn et al., 2005; Mazur and Dale, 2008), suggests that many temporal expressions in documents, especially news stories, can be"
D10-1089,C08-1070,1,0.814753,"otated corpora (ACE,3 TimeBank4 ) and a number of automatic taggers (see, for example, (Mani and Wilson, 2000; Schilder, 2004; Hacioglu et al., 2005; Negri and Marseglia, 2005; Saquete, 2005; Han et al., 2006; Ahn et al., 2007)). However, existing corpora have their limitations. In particular, the documents in these corpora tend to be limited in length and, in consequence, discourse structure. This impacts on the number, range and variety of temporal expressions they contain. Existing research carried out on the interpretation of temporal expressions, e.g. by (Baldwin, 2002; Ahn et al., 2005; Mazur and Dale, 2008), suggests that many temporal expressions in documents, especially news stories, can be interpreted fairly simply as being relative to a reference date that is typically the document creation date. This phenomenon does not carry over to longer, more narrative-style documents that describe extended sequences of events, as found, for example, in biographies or descriptions of protracted geo-political events. Consequently, existing corpora are not ideal as development data for systems intended to work on such historical narrations. In this paper we introduce a new annotated corpus of temporal exp"
D10-1089,P06-1050,0,0.0186165,", so if the anchor is mid-August, the value of the anchor must refer to August, which is not entirely correct as the semantics of mid- is lost. TIMEX2 only supports nonspecific expressions which have explicit information about granularity. Expressions such as a very short time or a short period of time therefore cannot be provided with any value, since the context does not indicate whether the period involved should be measured in days, weeks, or months. One might consider using the typical durations of events of the corresponding types in such cases, but this solution also has problems (see (Pan et al., 2006)). As is acknowledged in the TIMEX2 guidelines, the treatment of set expressions (i.e. recurring times 916 and durations and frequencies, e.g. twice a month) is underdeveloped. One rule states that set expressions should not be anchored (Ferro et al., 2005, p. 42); this has the consequence that the full semantics of the expression annually since 1955 cannot be provided, and the expression is therefore treated as two separate expressions, annually and 1955. Finally, alternative calendars are not supported, so an expression like February in the pre-revolutionary Russian calendar cannot receive a"
D10-1089,pustejovsky-etal-2010-iso,0,0.0294759,"rtefacts of Wikipedia articles that impact on the annotation process and the use of this corpus. Then, in Section 5 we analyse the differences between the WikiWars corpus and the widely-used ACE corpora. In Section 6 we report on the performance of our temporal expression tagger on this data set. Finally, in Section 7, we conclude. 2 Related Work At the time of writing, there are two mature, widecoverage schemes for the annotation of temporal information in texts: TIMEX2 (Ferro et al., 2005) and TimeML (Pustejovsky et al., 2003; Boguraev et al., 2005), which is soon to become an ISO standard (Pustejovsky et al., 2010). These schemes were used to annotate corpora that are often used in research on temporal expression recognition and normalisation: the series of corpora used for training and evaluation in the Automatic Content Extraction (ACE) program6 run in 2004, 2005 and 2007, and the TimeBank Corpus. The ACE corpora were prepared for the development and evaluation of systems participating in the ACE program. However, the evaluation corpora have never been publicly released, and thus are currently, for all practical purposes, unavailable. The ACE 2004 corpus contains news data only (broadcast news, newspa"
D10-1089,teissedre-etal-2010-resources,0,0.0198828,"Missing"
D11-1107,W09-0612,0,0.158602,"sational participants share this form of reference, or a form of reference derived from it, when they subsequently refer to that entity. Recent work by Goudbeek and Krahmer (2010) supports the view that subconscious alignment does indeed take place at the level of content selection for referring expressions. The participants in their study were more likely to use a dispreferred attribute to describe a target referent if this attribute had recently been used in a description by a confederate. There is some work within natural language generation that attempts to model the process of alignment (Buschmeier et al., 2009; Janarthanam and Lemon, 2009), but this is predominantly concerned with what we might think of as the ‘lexical perspective’, focussing on lexical choice rather than the selection of appropriate semantic content for distinguishing descriptions. 2.3 Combined Models This paper is not the first to look at how the algorithmic approach and the alignment approach might be integrated in REG. An early machine learning apFigure 1: An example pair of maps. proach to content selection was presented by Jordan and Walker (2000; 2005); they were also interested in an exploration of the validity of different"
D11-1107,P89-1009,1,0.469926,"an error analysis in Section 6. Section 7 draws some conclusions and discusses future work. modified head noun), and not with the content of the reference; and their data set consisted of only 1242 referring expressions. 2 2.2 2.1 Related Work The Algorithmic Approach We use the term algorithmic approach here to refer to the perspective that is common to the considerable body of work within computational linguistics on the problem of referring expression generation developed over the last 20 years. Much of this work takes as its starting point the characterisation of the problem expressed in (Dale, 1989). This work has focused on the design of algorithms which take into account the context of reference in order to decide what properties of an entity should be mentioned in order to distinguish that entity from others with which it might be confused. Early work was concerned with subsequent reference in discourse, inspired by Grosz and Sidner’s (1986) observations on how the attentional structure of a discourse made particular referents accessible at any given point. More recently, attention has shifted to initial reference in visual domains, driven in large part by the availability of the TUNA"
D11-1107,W08-1131,0,0.608627,"which take into account the context of reference in order to decide what properties of an entity should be mentioned in order to distinguish that entity from others with which it might be confused. Early work was concerned with subsequent reference in discourse, inspired by Grosz and Sidner’s (1986) observations on how the attentional structure of a discourse made particular referents accessible at any given point. More recently, attention has shifted to initial reference in visual domains, driven in large part by the availability of the TUNA dataset and the shared tasks that make use of it (Gatt et al., 2008). The construction of distinguishing descriptions has consistently been a key consideration in this body of work. Scenarios that require the generation of references in multi-turn dialogues that concern visual scenes are likely to be among the first where we can expect computational approaches to referring expression generation to be practically useful. Surprisingly, however, the more recent work on initial reference in visual domains and the earlier work on subsequent reference in discourse remain somewhat distinct and separate from each other, despite much the same algorithms having been use"
D11-1107,P10-2011,0,0.546052,"and Garrod (2004). With respect to reference in particular, speakers are said to form conceptual pacts in their use of language (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996). Although there is disagreement about the exact mechanisms that enable alignment and conceptual pacts, the implication of much of this work is that one speaker introduces an entity by means of some description, and then (perhaps after some negotiation) both conversational participants share this form of reference, or a form of reference derived from it, when they subsequently refer to that entity. Recent work by Goudbeek and Krahmer (2010) supports the view that subconscious alignment does indeed take place at the level of content selection for referring expressions. The participants in their study were more likely to use a dispreferred attribute to describe a target referent if this attribute had recently been used in a description by a confederate. There is some work within natural language generation that attempts to model the process of alignment (Buschmeier et al., 2009; Janarthanam and Lemon, 2009), but this is predominantly concerned with what we might think of as the ‘lexical perspective’, focussing on lexical choice ra"
D11-1107,J86-3001,0,0.599184,"Missing"
D11-1107,W09-0611,0,0.0136645,"re this form of reference, or a form of reference derived from it, when they subsequently refer to that entity. Recent work by Goudbeek and Krahmer (2010) supports the view that subconscious alignment does indeed take place at the level of content selection for referring expressions. The participants in their study were more likely to use a dispreferred attribute to describe a target referent if this attribute had recently been used in a description by a confederate. There is some work within natural language generation that attempts to model the process of alignment (Buschmeier et al., 2009; Janarthanam and Lemon, 2009), but this is predominantly concerned with what we might think of as the ‘lexical perspective’, focussing on lexical choice rather than the selection of appropriate semantic content for distinguishing descriptions. 2.3 Combined Models This paper is not the first to look at how the algorithmic approach and the alignment approach might be integrated in REG. An early machine learning apFigure 1: An example pair of maps. proach to content selection was presented by Jordan and Walker (2000; 2005); they were also interested in an exploration of the validity of different psycholinguistic models of re"
D11-1107,P00-1024,0,0.0431368,"hin natural language generation that attempts to model the process of alignment (Buschmeier et al., 2009; Janarthanam and Lemon, 2009), but this is predominantly concerned with what we might think of as the ‘lexical perspective’, focussing on lexical choice rather than the selection of appropriate semantic content for distinguishing descriptions. 2.3 Combined Models This paper is not the first to look at how the algorithmic approach and the alignment approach might be integrated in REG. An early machine learning apFigure 1: An example pair of maps. proach to content selection was presented by Jordan and Walker (2000; 2005); they were also interested in an exploration of the validity of different psycholinguistic models of reference production, including Grosz and Sidner’s (1986) model of discourse structure, the conceptual pacts model of Clark and colleagues, and the intentional influences model developed by Jordan (2000). However, their data set consists of only 393 referring expressions, compared to our 16,358, and these expressions had functions other than identification; most importantly, the entities referred to were not part of a shared visual scene as is the case in our data. 1294 and 471 referrin"
D11-1107,J03-1003,0,0.458325,"Missing"
D11-1107,W06-1412,0,0.153994,"among the first where we can expect computational approaches to referring expression generation to be practically useful. Surprisingly, however, the more recent work on initial reference in visual domains and the earlier work on subsequent reference in discourse remain somewhat distinct and separate from each other, despite much the same algorithms having been used in both. There is very little work that brings these two strands together by looking at both initial and subsequent references in dialogues that concern visual scenes. An exception here is the machine learning approach developed by Stoia et al. (2006), who aimed at building a dialogue system for a situated agent giving instructions in a virtual 3D world. However, their approach was concerned with choosing the type of reference to use (definite or indefinite, pronominal, bare or 1159 The Alignment Approach Meanwhile, starting with the early work of Carroll (1980), a quite distinct strand of research in psycholinguistics has explored how a speaker’s form of reference to an entity is impacted by the way that entity has been previously referred to in the discourse or dialogue. The general idea behind what we will call the alignment approach is"
D11-1107,W08-1109,1,0.917025,"Missing"
D11-1107,viethen-etal-2010-dialogue,1,0.838364,"ce production, including Grosz and Sidner’s (1986) model of discourse structure, the conceptual pacts model of Clark and colleagues, and the intentional influences model developed by Jordan (2000). However, their data set consists of only 393 referring expressions, compared to our 16,358, and these expressions had functions other than identification; most importantly, the entities referred to were not part of a shared visual scene as is the case in our data. 1294 and 471 referring expressions from two different corpora, compared to our test set of 4947 referring expressions. More recently in (Viethen et al., 2010), we presented a rule-based system that addressed a specific instance of the problem we consider here, using the same corpus as we do: we singled out 2579 first references to landmarks by the second speaker (‘second speaker initial references’) and attempted to reproduce these using a system based on Dale and Reiter’s (1995) Incremental Algorithm. Although the data set was a subset of the one used here, the system did not reach the same performance (see Section 5). Gupta and Stent (2005) instantiated Dale and Reiter’s (1995) Incremental Algorithm with a preference ordering that favours the att"
D11-1107,W09-0629,0,\N,Missing
dale-narroway-2012-framework,W11-2838,1,\N,Missing
dale-narroway-2012-framework,W10-4236,1,\N,Missing
E09-1097,C00-1007,0,0.0278773,"reinserted. Since the LMO baseline reduces to bigram generation when concatenating single words, we test a second language model baseline which always uses a 4-gram window size. A Viterbi-like generator with a 4-gram model and a beam of 100 is used to generate a sequence. For this baseline, referred to as the Viterbi baseline, base noun phrases were separated into their constituent words and included in the input word set. 6 Related Work 6.1 Statistical Surface Realisers The work in this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viter"
E09-1097,P99-1071,0,0.00878955,"with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example German, have been explored (Filippova and Strube, 2007). 6.2 Text-to-Text Generation As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. In this work, sentences presenting the same information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to paraphrase generation (Bannard and Callison-Burch, 2005) and multi-sentence alignment in summarisation (Daum´e III and Marcu, 2004). These ap"
E09-1097,W07-2216,0,0.0153048,"ely if called on an 1 Adapted from (McDonald et al., 2005) and . The difference concerns the direction of the edge and the edge weight function. We have also folded the function ‘contract’ in McDonald et al. (2005) into the main algorithm. Again following that work, we treat the function s as a data structure permitting storage of updated edge weights. probLM O (w0 . . . wj ) http://www.ce.rit.edu/˜ sjyeec/dmst.html = j−k−1 Y probM LE (wi+k |wi . . . wi+k−1 ) i=0 (3) 854 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 surrounding tree context. This makes the search for an optimal tree an NP-hard problem (McDonald and Satta, 2007) as all possible trees must be considered to find an optimal solution. Consequently, we must choose a heuristic search algorithm for finding the locally optimum spanning tree. By representing argument positions that can be filled only once, we allow modifiers to compete for argument positions and vice versa. The CLE algorithm only considers this competition in one direction. In line 3 of Algorithm 1, only heads compete for modifiers, and thus the solution will be sub-optimal. In Wan et al. (2007), we showed that introducing a model of argument positions into a greedy spanning tree algorithm ha"
E09-1097,H05-1066,0,0.586947,". This is not surprising as these methods are unable to model grammaticality at the sentence level, unless the size of n is sufficiently large. In practice, the lack of sufficient training data means that n is often smaller than the average sentence length. Even if data exists, increasing the size of n corresponds to a higher degree polynomial complexity search for the best word sequence. In response, we introduce an algorithm for searching for the best word sequence in a way that attempts to model grammaticality at the sentence level. Mirroring the use of spanning tree algorithms in parsing (McDonald et al., 2005), we present an approach to statistical sentence generation. Given a set of scrambled words, the approach searches for the most probable dependency tree, as defined by some corpus, such that it contains each word of the input set. The tree is then traversed to obtain the final word ordering. In particular, we present two spanning tree algorithms. We first adapt the Chu-Liu-Edmonds (CLE) algorithm (see Chu and Liu (1965) and Edmonds (1967)), used in McDonald et al. (2005), to include a basic argument model, added to keep track of linear precedence between heads and modifiers. While our adapted"
E09-1097,P07-1044,1,0.690541,"surement of grammaticality, we use the string regeneration task. Beginning with a human-authored sentence with its word order randomised, the goal is to regenerate the original sentence. Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric (Papineni et al., 2002). One benefit to this evaluation is that content selection, as a factor, is held constant. Specifically, the probability of word selection is uniform for all words. 3 Alternative grammaticality measures have been developed recently (Mutton et al., 2007). We are currently exploring the use of this and other metrics. 4 This would correspond to the use of a chunking algorithm or a named-entity recogniser to find noun phrases that could be re-used for sentence generation. 857 Algorithms Viterbi baseline LMO baseline CLE AB PTB-LM 14.9 24.3 26.4 33.6 AB: the dow at this point was down about 35 points CLE: was down about this point 35 points the dow at LMO: was this point about at down the down 35 points Viterbi: the down 35 points at was about this point down BLLIP-LM 18.0 26.0 26.8 33.7 Original: at this point, the dow was down about 35 points F"
E09-1097,P04-1030,0,0.0114709,"nal (WSJ) with human annotations of syntactic structures. Dependency events were sourced from the events file of the Collins parser package, which contains the dependency events found in training sections 2-22 of the corpus. Development was done on section 00 and testing was performed on section 23. A 4-gram language model (LM) was also obtained from the PTB training data, referred to as PTB-LM. Additionally, a 4-gram language model was obtained from a subsection of the BLLIP’99 Corpus (LDC number: LDC2000T43) containing three years of WSJ data from 1987 to 1989 (Charniak et al., 1999). As in Collins et al. (2004), the 1987 portion of the BLLIP corpus containing 20 million words was also used to create a language model, referred to here as BLLIP-LM. Ngram models were smoothed using Katz’s method, backing off to smaller values of n. For this evaluation, tokenisation was based on that provided by the PTB data set. This data set also delimits base noun phrases (noun phrases without nested constituents). Base noun phrases were treated as single tokens, and the rightmost word assumed to be the head. For the algorithms tested, the input set for any test case consisted of the single tokens identified by the P"
E09-1097,P05-1066,0,0.0320327,"this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a sema"
E09-1097,P06-1146,0,0.0168536,"Missing"
E09-1097,P96-1025,0,0.199273,"rs. Adopting an approach similar to Johnson (2007), we look at the direction (left or right) of the head with respect to the modifier; we consequently define a set D = {l, r} to represent this. Set D represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object. Each edge has a pair of associated weights, one for each direction, defined by the function s : E × D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relation type (represented in the original as triples of non-terminals). Given a corpus, for some edge e = (u, v) ∈ E and direction d ∈ D, we calculate the edge weight as: not always correspond with a linguistically valid dependency tree, primarily because it does not attempt to ensure that words in the tree have plausible numbers of arguments. We propose an alternative dependencyspanning tree algorithm which uses a more fine-grained argument model representing argument positions. To find the best modifiers for argument positions, we treat the attachment of edges"
E09-1097,P02-1040,0,0.0950057,"density up to a fixed maximum, in this case 7 argument positions, and assume zero probability beyond that. 5 Evaluation 5.1 String Generation Task The best-performing word ordering algorithm is one that makes fewest grammatical errors. As a surrogate measurement of grammaticality, we use the string regeneration task. Beginning with a human-authored sentence with its word order randomised, the goal is to regenerate the original sentence. Success is indicated by the proportion of the original sentence regenerated, as measured by any string comparison method: in our case, using the BLEU metric (Papineni et al., 2002). One benefit to this evaluation is that content selection, as a factor, is held constant. Specifically, the probability of word selection is uniform for all words. 3 Alternative grammaticality measures have been developed recently (Mutton et al., 2007). We are currently exploring the use of this and other metrics. 4 This would correspond to the use of a chunking algorithm or a named-entity recogniser to find noun phrases that could be re-used for sentence generation. 857 Algorithms Viterbi baseline LMO baseline CLE AB PTB-LM 14.9 24.3 26.4 33.6 AB: the dow at this point was down about 35 poin"
E09-1097,W04-3216,0,0.0625091,"Missing"
E09-1097,D07-1002,0,0.0191291,"ain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). Our work is also strongly related to that of Wong and Mooney (2007) which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections,"
E09-1097,P05-1009,0,0.127674,"ame information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to paraphrase generation (Bannard and Callison-Burch, 2005) and multi-sentence alignment in summarisation (Daum´e III and Marcu, 2004). These approaches typically use n-gram models to find the best word sequence. The WIDL formalism (Soricut and Marcu, 2005) was proposed to efficiently encode constraints that restricted possible word sequences, for example dependency information. Though similar, our work here does not explicitly represent the word lattice. For these text-to-text systems, the order of elements in the generated sentence is heavily based on the original order of words and phrases in the input sentences from which lattices are built. Our approach has the benefit of considering all possible orderings of words, corresponding to a wider range of paraphrases, provided with a suitable dependency model is available. 7 Conclusions In this p"
E09-1097,P07-1041,0,0.0565153,"semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example German, have been explored (Filippova and Strube, 2007). 6.2 Text-to-Text Generation As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. In this work, sentences presenting the same information, for example multiple news articles describing the same event, are merged to form a single summary by aligning repeated words and phrases across sentences. Other text-to-text approaches for generating novel sentences also aim to recycle sentence fragments where possible, as we do. Work on phrasebased statistical machine translation has been applied to parap"
E09-1097,D08-1019,0,0.0364206,"eline reduces to bigram generation when concatenating single words, we test a second language model baseline which always uses a 4-gram window size. A Viterbi-like generator with a 4-gram model and a beam of 100 is used to generate a sequence. For this baseline, referred to as the Viterbi baseline, base noun phrases were separated into their constituent words and included in the input word set. 6 Related Work 6.1 Statistical Surface Realisers The work in this paper is similar to research in statistical surface realisation (for example, Langkilde and Knight (1998); Bangalore and Rambow (2000); Filippova and Strube (2008)). These start with a semantic representation for which a specific rendering, an ordering of words, must be determined, often using language models to govern tree traversal. The task in this paper is different as it is a text-to-text scenario and does not begin with a representation of semantics. 5.4 Results The results are presented in Table 2. Significance was measured using the sign test and the sampling method outlined in (Collins et al., 2005). We will examine the results in the PTB-LM column first. The gain of 10 BLEU points by the LMO baseline over the Viterbi baseline shows the perform"
E09-1097,P07-1022,0,0.0163055,"irected edges. For each sentence w = w1 . . . wn , we define the digraph Gw = (Vw , Ew ) where Vw = {w0 , w1 , . . . , wn }, with w0 a dummy root vertex, and Ew = {(u, v)|u ∈ Vw , v ∈ Vw  {w0 }}. The graph is fully connected (except for the root vertex w0 which is only fully connected outwards) and is a representation of possible dependencies. For an edge (u, v), we refer to u as the head and v as the modifier. We extend the original formulation of McDonald et al. (2005) by adding a notion of argument positions for a word, providing points to attach modifiers. Adopting an approach similar to Johnson (2007), we look at the direction (left or right) of the head with respect to the modifier; we consequently define a set D = {l, r} to represent this. Set D represents the linear precedence of the words in the dependency relation; consequently, it partially approximates the distinction between syntactic roles like subject and object. Each edge has a pair of associated weights, one for each direction, defined by the function s : E × D → R, based on a probabilistic model of dependency relations. To calculate the edge weights, we adapt the definition of Collins (1996) to use direction rather than relati"
E09-1097,N07-1022,0,0.0160194,"hows the performance improvement that can be gained when reinserting the base noun phrases. 858 structures whilst searching for the best word ordering. As a result, an argument model is needed to identify linguistically plausible spanning trees. We treated the alignment of modifiers to head words as a bipartite graph matching problem. This is similar to work in semantic role labelling by Pad´o and Lapata (2006). The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). Our work is also strongly related to that of Wong and Mooney (2007) which constructs symbolic semantic structures via an assignment process in order to provide surface realisers with input. Our approach differs in that we do not begin with a fixed set of semantic labels. Additionally, our end goal is a dependency tree that encodes word precedence order, bypassing the surface realisation stage. The dependency model and the LMO linearisation algorithm are based heavily on word order statistics. As such, the utility of this approach is limited to human languages with minimal use of inflections, such as English. Approaches for other language types, for example Ge"
E09-1097,W98-1426,0,\N,Missing
E09-1097,P05-1074,0,\N,Missing
E09-1097,W00-1401,0,\N,Missing
E09-2009,P08-2050,0,0.0555286,"lenge is a new Internetbased evaluation effort for natural language generation systems. In this paper, we motivate and describe the software infrastructure that we developed to support this challenge. 1 Introduction Natural language generation (NLG) systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard is not appropriate because there can be multiple generated outputs that are equally good, and finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). On the other hand, lab-based evaluations with human subjects to assess each aspect of the system’s functionality are expensive and time-consuming. These characteristics make it hard to compare different systems and measure progress. GIVE (“Generating Instructions in Virtual Environments”) (Koller et al., 2007) is a research challenge for the NLG community designed to provide a new approach to NLG system evaluation. In the GIVE scenario, users try to solve a treasure hunt in a virtual 3D world that they have not seen before. The computer has a complete symbo"
E09-2009,W08-1113,0,0.0796843,"ffort for natural language generation systems. In this paper, we motivate and describe the software infrastructure that we developed to support this challenge. 1 Introduction Natural language generation (NLG) systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard is not appropriate because there can be multiple generated outputs that are equally good, and finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). On the other hand, lab-based evaluations with human subjects to assess each aspect of the system’s functionality are expensive and time-consuming. These characteristics make it hard to compare different systems and measure progress. GIVE (“Generating Instructions in Virtual Environments”) (Koller et al., 2007) is a research challenge for the NLG community designed to provide a new approach to NLG system evaluation. In the GIVE scenario, users try to solve a treasure hunt in a virtual 3D world that they have not seen before. The computer has a complete symbolic representation of the virtual e"
E91-1028,T87-1035,0,0.023293,"Missing"
E91-1028,P89-1009,1,\N,Missing
I05-5012,W98-1426,0,\N,Missing
I05-5012,C00-1007,0,\N,Missing
I05-5012,P98-1035,0,\N,Missing
I05-5012,C98-1035,0,\N,Missing
I05-5012,P96-1025,0,\N,Missing
I05-5012,P99-1071,0,\N,Missing
J98-3001,P97-1011,1,0.853938,"Missing"
J98-3001,W96-0501,0,0.0270557,"and graphics and attempt to combine these in sensible ways. We predict that the World Wide Web will be a major factor in forcing some of the issues here: if systems are to automatically generate the text on Web pages (see, for example, Milosavljevic and Dale [1996]), then they also need to consider other elements of that container. Reusable resources. It may be an indication of a maturing of some subareas of NLG research that we are now in a position where there are reusable components for particular tasks. Specifically, three linguistic realization packages, FUF/SURGE (Elhadad 1993a, 1993b; Elhadad and Robin 1996), PENMAN/NIGEL (Penman group 1989), and its descendant KPML/NIGEL (Bateman 1997), are widely used in the field. For anything other than simple applications, it is now questionable whether it makes sense to build a linguistic realization component from scratch. We may expect other kinds of reusable components to be developed within the research community within the next 5-10 years; it is developments of this kind that signal significant progress, since being able to reuse the work of others obviously has the potential to increase research productivity. In related developments, there is a growin"
J98-3001,J97-1004,0,0.120327,"ial to increase research productivity. In related developments, there is a growing interest within the community in defining a reference architecture for NLG; if successful, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &quot;task efficacy&quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to"
J98-3001,W96-0417,1,0.887119,"Missing"
J98-3001,J97-1007,0,0.0299135,"ning a reference architecture for NLG; if successful, this is likely to stimulate further research and development in NLG through the provision of a modular baseline for development, comparison, and evaluation. Evaluation. Although there have been attempts at the evaluation of NLG techniques and systems in the past, formal evaluation has only recently come to the fore. For example, systems have been evaluated by using human judges to assess the quality of the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue); by comparing the system&apos;s performance to that of humans (Yeh and Mellish 1997); by corpus-based evaluation (Robin and McKeown 1996); and indirectly through &quot;task efficacy&quot; measures (Young 1997). The major stumbling block for progress is determining what metrics and methods should be used: for example, how can the quality of an output text be measured? Because of the different nature of the task, it is unlikely that methods that have been used in NLU, such as the evaluation process adopted in the Message Understanding Conferences, can be carried over to generation. Dale and Mellish (1998) suggest that the NLG community could make progress by devising specific evaluation"
mazur-dale-2006-named,M98-1004,0,\N,Missing
mazur-dale-2006-named,M98-1012,0,\N,Missing
mazur-dale-2006-named,M98-1014,0,\N,Missing
mazur-dale-2006-named,M98-1021,0,\N,Missing
mazur-dale-2006-named,W03-0419,0,\N,Missing
mazur-dale-2006-named,W02-2024,0,\N,Missing
mazur-dale-2006-named,C96-1079,0,\N,Missing
N10-1142,P09-1032,0,0.055028,"has been processed to remove duplicate messages and to normalise sender and recipient names, resulting in just over 250,000 email messages. No attachments are included. Our request classifier training data is drawn from a collection of 664 messages that were selected at random from the Enron corpus. Each message was annotated by three annotators, with overall kappa agreement of 0.681. From the full dataset of 664 messages, we remove all messages where annotators disagreed for training and evaluating our request classifier, in order to mitigate the effects of annotation noise, as discussed in (Beigman and Klebanov, 2009). The unanimously agreed data set used for training consists of 505 email messages. 4.2 • whether the message contains any sentences that end with a question mark; and • binary word unigram and word bigram features for n-grams that occur at least three times across the training set. Before generating n-gram features, we normalise the message text as shown in Table 1, in a manner similar to Carvalho and Cohen (2006). We also add tokens marking the start and end of sentences, detected using a modified version of Scott Piao’s sentence splitter (Piao et al., 2002), and tokens marking the start and"
N10-1142,W06-3406,0,0.0590319,"sages, we remove all messages where annotators disagreed for training and evaluating our request classifier, in order to mitigate the effects of annotation noise, as discussed in (Beigman and Klebanov, 2009). The unanimously agreed data set used for training consists of 505 email messages. 4.2 • whether the message contains any sentences that end with a question mark; and • binary word unigram and word bigram features for n-grams that occur at least three times across the training set. Before generating n-gram features, we normalise the message text as shown in Table 1, in a manner similar to Carvalho and Cohen (2006). We also add tokens marking the start and end of sentences, detected using a modified version of Scott Piao’s sentence splitter (Piao et al., 2002), and tokens marking the start and end of the message. Symbol Used Pattern numbers day pronoun-object pronoun-subject filetype multi-dash multi-underscore Any sequence of digits Day names or abbreviations Objective pronouns: me, her, him, us, them Subjective pronouns: I, we, you, he, she, they .doc, .pdf, .ppt, .txt, .xls, .rtf 3 or more sequential ‘-’ characters 3 or more sequential ‘ ’ characters Table 1: Normalisation applied to n-gram features"
N10-1142,W04-3240,0,0.118205,"Missing"
N10-1142,W04-1008,0,0.014809,"Missing"
N10-1142,U08-1009,1,0.933378,"cuss in Section 4. A distinction can be drawn between messagelevel identification—i.e., the task of determining whether an email message contains a request — and utterance-level identification—i.e., determining precisely where and how the request is expressed. In this paper, we focus on the task of message-level identification, since utterance-level identification is a significantly more problematic task: it is often the case that, while we might agree that a message contains a request or commitment, it is much harder to determine the precise extent of the text that conveys this request (see (Lampert et al., 2008b) for a detailed discussion of some of the issues here). 3 Related Work Our request classification work builds on influential ideas proposed by Winograd and Flores (1986) in taking a language/action perspective and identifying speech acts in email. While this differs from the approach of most currently-used email systems, which routinely treat the content of email messages as homogeneous bags-of-words, there is a growing body of research applying ideas from Speech Act Theory (Austin, 1962; Searle, 1969) to analyse and enhance email communication. Khosravi and Wilks (1999) were among the first"
N10-1142,D09-1096,1,0.931459,"ferent functional parts, which we call email zones, and then using this information to consider only content from certain parts of a message for request classification, would improve request classifi984 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 984–992, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics cation performance. To test this hypothesis, we developed an SVMbased automated email zone classifier configured with graphic, orthographic and lexical features; this is described in more detail in (Lampert et al., 2009). Section 5 describes how we improve request classification performance using this email zone classifier. Section 6 summarises the performance of our request classifiers, with and without automated email zoning, along with an analysis of the contribution of lexical features to request classification, discussion of request classification learning curves, and a detailed error analysis that explores the sources of request classification errors. Finally, in Section 7, we offer pointers to future work and some concluding remarks. 2 Background and Motivation Previous research has established that us"
N10-1142,scerri-etal-2008-evaluating,0,0.0249841,"e-level, marking emails as requests if they contain one or more request utterances. As noted earlier, we define a request as an utterance from the email sender that places an obligation on a recipient to schedule an action (e.g., add to a calendar or task list), perform an action, or respond. Requests may be conditional or unconditional in terms of the obligation they impose on the recipient. Conditional requests require action only if a stated condition is satisfied. Previous annotation experiments have shown that conditional requests are an important phenomena and occur frequently in email (Scerri et al., 2008; Lampert et al., 2008a). Requests may also be phrased as either a direct or indirect speech act. Although some linguists distinguish between speech acts that require a physical response and those that require a verbal or information response, e.g., (Sinclair and Coulthard, 1975), we follow Searle’s approach and make no such distinction. We thus consider questions requiring an informational response to be requests, since they place an obliga987 tion on the recipient to answer.1 Additionally, there are some classes of request which have been the source of systematic human disagreement in our pr"
P06-4009,W06-0902,1,0.823327,"sions. Alias-i’s LingPipe also reported results for extraction, but not interpretation, of temporal expressions at TERN 2004. In contrast to this collection of work, which comes at the problem from a now-traditional information extraction perspective, there is also of course an extensive prior literature on the semantic of temporal expressions. Some more recent work attempts to bridge the gap between these two related enterprises; see, for example, Hobbs and Pan (2004). 3 The Underlying Model We describe briefly here our underlying conceptual model; a more detailed description is provided in (Dale and Mazur, 2006). 3.1 Processes We take the ultimate goal of the interpretation of temporal expressions to be that of computing, for each temporal expression in a text, the point in time or duration that is referred to by that expression. We distinguish two stages of processing: 3.2 Data Types We view the temporal world as consisting of two basic types of entities, these being points in time and durations; each of these has an internal hierarchical structure. It is convenient to represent these as feature structures like the following:3 Recognition: the process of identifying a temporal expression in text, an"
P07-1044,J99-2004,0,0.00523097,", we introduced varying amounts of structure into the generation process. 5.1 Structural Generation Methods PoStag In the first of these, we constructed a rough approximation of typical sentence grammar structure by taking bigrams over part-of-speech tags.6 Then, given a string of PoS tags of length n, t1 . . . tn , we start by assigning the probabilities for the word in position 1, w1 , according to the conditional probability P (w1 |t1 ). Then, for position j (2 ≤ j ≤ n), we assign to candidate words the value P (wj |tj ) × P (wj |wj−1 ) to score word sequences. 6 We used the supertagger of Bangalore and Joshi (1999). 349 So, for example, we might generate the PoS tag template Det NN Adj Adv, take all the words corresponding to each of these parts of speech, and combine bigram word sequence probability with the conditional probability of words with respect to these parts of speech. We then use a Viterbi-style algorithm to find the most likely word sequence. In this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal (1999) in their combination of content and language model probabilities, by backtracking at every state in order to discourage repeated words and"
P07-1044,W00-1401,0,0.227088,"the letters for correct spelling, good grammar, rhythm and flow, appropriateness of tone, and several other specific characteristics of good text. In terms of automatic evaluation, we are not aware of any technique that measures only fluency or similar characteristics, ignoring content, apart from that of Wan et al. (2005). Even in NLG, where, given the variability of the input representations (and hence difficulty in verifying faithfulness), it might be expected that such measures would be available, the available metrics still conflate content and form. For example, the metrics proposed in Bangalore et al. (2000), such as Simple Accuracy and Generation Accuracy, measure changes with respect to a reference string based on the idea of string-edit distance. Similarly, B LEU has been used in NLG, for example by Langkilde-Geary (2002). 3 Parsers as Evaluators There are three parts to verifying the usefulness of parsers as evaluators: choosing the parsers and the metrics derived from them; generating some texts for human and parser evaluation; and, the key part, getting human judgements on these texts and correlating them with parser metrics. 1 http://projects.ldc.upenn.edu/TIDES/ Translation/TranAssessSpec"
P07-1044,E06-1032,0,0.00508865,"xt Generation Methods The method used to generate text in Section 3.2 is a variation of the standard n-gram language model. A question that arises is: Are any of the metrics defined above strongly influenced by the type of language model used to generate the text? It may be the case, for example, that a parser implementation uses its own language model that predisposes it to favour a similar model in the text generation process. This is a phenomenon seen in MT, where B LEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al., 2006). Our previous approach used only sequences of words concatenated together. To define some new methods for generating text, we introduced varying amounts of structure into the generation process. 5.1 Structural Generation Methods PoStag In the first of these, we constructed a rough approximation of typical sentence grammar structure by taking bigrams over part-of-speech tags.6 Then, given a string of PoS tags of length n, t1 . . . tn , we start by assigning the probabilities for the word in position 1, w1 , according to the conditional probability P (w1 |t1 ). Then, for position j (2 ≤ j ≤ n),"
P07-1044,1995.iwpt-1.15,0,0.0668745,"parsers as evaluators: choosing the parsers and the metrics derived from them; generating some texts for human and parser evaluation; and, the key part, getting human judgements on these texts and correlating them with parser metrics. 1 http://projects.ldc.upenn.edu/TIDES/ Translation/TranAssessSpec.pdf 3.1 The Parsers In testing the idea of using parsers to judge fluency, we use three parsers, from which we derive four parser metrics, to investigate the general applicability of the idea. Those chosen were the Connexor parser,2 the Collins parser (Collins, 1999), and the Link Grammar parser (Grinberg et al., 1995). Each produces output that can be taken as representing degree of ungrammaticality, although this output is quite different for each. Connexor is a commercially available dependency parser that returns head–dependant relations as well as stemming information, part of speech, and so on. In the case of an ungrammatical sentence, Connexor returns tree fragments, where these fragments are defined by transitive head–dependant relations: for example, for the sentence Everybody likes big cakes do it returns fragments for Everybody likes big cakes and for do. We expect that the number of fragments sh"
P07-1044,J00-4006,0,0.00141925,"echnologies can be characterised as having at least two aspects: how well the generated text reflects the source data, whether it be text in another language for machine translation (MT), a natural language generation (NLG) input representation, a document to be summarised, and so on; and how well it conforms to normal human language usage. These two aspects are often made explicit in approaches to creating the text. For example, in statistical MT the translation model and the language model are treated separately, characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin (2000)). Similarly, the ultrasummarisation model of Witbrock and Mittal (1999) consists of a content model, modelling the probability that a word in the source text will be in the summary, and a language model. Evaluation methods can be said to fall into two categories: a comparison to gold reference, or an appeal to human judgements. Automatic evaluation methods carrying out a comparison to gold reference tend to conflate the two aspects of faithfulness and fluency in giving a goodness score for generated output. B LEU (Papineni et al., 2002) is a canonical example: in matching n-grams in a candida"
P07-1044,2004.tmi-1.8,0,0.0447561,"o use something like these as indicators of generated sentence fluency. The aim of the next section is to build a better predictor than the individual parser metrics alone. Metric Humans Collins Parser Connexor Link Grammar Nulled Tokens Link Grammar Invalid Parses G LEU Corr. 0.6529 0.4057 -0.3804 -0.3310 0.1619 0.4606 In MT, one problem with most metrics like B LEU is that they are intended to apply only to documentlength texts, and any application to individual sentences is inaccurate and correlates poorly with human judgements. A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber (2004) is to use a Support Vector Machine, using features such as word error rate, to estimate sentence-level translation quality. The two main insights in applying SVMs here are, first, noting that human translations are generally good and machine translations poor, that binary training data can be created by taking the human translations as positive training instances and machine translations as negative ones; and second, that a non-binary metric of translation goodness can be derived by the distance from a test instance to the support vectors. In an empirical evaluation, Kulesza and Shieber found"
P07-1044,W02-2103,0,0.0171253,"easures only fluency or similar characteristics, ignoring content, apart from that of Wan et al. (2005). Even in NLG, where, given the variability of the input representations (and hence difficulty in verifying faithfulness), it might be expected that such measures would be available, the available metrics still conflate content and form. For example, the metrics proposed in Bangalore et al. (2000), such as Simple Accuracy and Generation Accuracy, measure changes with respect to a reference string based on the idea of string-edit distance. Similarly, B LEU has been used in NLG, for example by Langkilde-Geary (2002). 3 Parsers as Evaluators There are three parts to verifying the usefulness of parsers as evaluators: choosing the parsers and the metrics derived from them; generating some texts for human and parser evaluation; and, the key part, getting human judgements on these texts and correlating them with parser metrics. 1 http://projects.ldc.upenn.edu/TIDES/ Translation/TranAssessSpec.pdf 3.1 The Parsers In testing the idea of using parsers to judge fluency, we use three parsers, from which we derive four parser metrics, to investigate the general applicability of the idea. Those chosen were the Conne"
P07-1044,P02-1040,0,0.111683,"ess and fluency respectively (as in the treatment in Jurafsky and Martin (2000)). Similarly, the ultrasummarisation model of Witbrock and Mittal (1999) consists of a content model, modelling the probability that a word in the source text will be in the summary, and a language model. Evaluation methods can be said to fall into two categories: a comparison to gold reference, or an appeal to human judgements. Automatic evaluation methods carrying out a comparison to gold reference tend to conflate the two aspects of faithfulness and fluency in giving a goodness score for generated output. B LEU (Papineni et al., 2002) is a canonical example: in matching n-grams in a candidate translation text with those in a reference text, the metric measures faithfulness by counting the matches, and fluency by implicitly using the reference n-grams as a language model. Often we are interested in knowing the quality of the two aspects separately; many human judgement frameworks ask specifically for separate judgements on elements of the task that correspond to faithfulness and to fluency. In addition, the need for reference texts for an evaluation metric can be problematic, and intuitively seems unnecessary for characteri"
P07-1044,2003.mtsummit-papers.51,0,0.0346475,"nerally good and machine translations poor, that binary training data can be created by taking the human translations as positive training instances and machine translations as negative ones; and second, that a non-binary metric of translation goodness can be derived by the distance from a test instance to the support vectors. In an empirical evaluation, Kulesza and Shieber found that their SVM gave a correlation of 0.37, which was an improvement of around half the gap between the B LEU correlations with the human judgements (0.25) and the lowest pairwise human inter-judge correlation (0.46) (Turian et al., 2003). We take a similar approach here, using as features the four parser metrics described in Section 3. We trained an SVM,4 taking as positive training data the 1000 instances of sentences of sequence length 24 (i.e. sentences extracted from the corpus) and as negative training data the 1000 sentences of sequence length 1. We call this learner G LEU. 5 As a check on the ability of the G LEU SVM to distinguish these ‘positive’ sentences from ‘negative’ ones, we evaluated its classification accuracy on a (new) test set of size 300, split evenly between sentences of sequence length 24 and sequence l"
P07-1044,W05-1628,1,0.8657,"d using their own language model, rather than human-authored sentences already existing in a test corpus; and so it is not obvious what language model would be an objective assessment of sentence naturalness. In the case of evaluating a single system, using the language model that generated the sentence will only confirm that the sentence does fit the language model; in situations such as comparing two systems which each generate text using a different language model, it is not obvious that there is a principled way of deciding on a fair language model. Quite a different idea was suggested in Wan et al. (2005), of using the grammatical judgement of a parser to assess fluency, giving a measure independent of the language model used to generate the text. The idea is that, assuming the parser has been trained on an appropriate corpus, the poor performance of the parser on one sentence relative to another might be an indicator of some degree of ungrammaticality and possibly disfluency. In that work, however, correlation with human judgements was left uninvestigated. The goal of this paper is to take this idea and develop it. In Section 2 we look at some related work on metrics, in particular for NLG. I"
P07-1044,C96-1043,0,\N,Missing
P07-1044,J03-4003,0,\N,Missing
P09-2076,P08-2050,0,0.0421031,"U. of Edinburgh Robert.Dale@mq.edu.au Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost. Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online. However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-bas"
P09-2076,J09-1008,0,0.0108405,"In this paper, we validate this novel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 dbyron@ccs.neu.edu justine@northwestern.edu Sara Dalzel-Job U. of Edinburgh Robert.Dale@mq.edu.au Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent"
P09-2076,W09-0628,1,0.930466,"tems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach to NLG evaluation: By connecting NLG systems to experimental subjects over the Internet, it achieves a true task-based evaluation at a much lower cost. Indeed, the first GIVE Challenge acquired data from over 1100 experimental subjects online. However, it still remains to be shown that the results that can be obtained in this way are in fact comparable to more established task-based evaluation efforts, which are based on a carefully selected subject pool and carried out in a controlled laboratory Justine Cassell Northwestern U. Johanna Moor"
P09-2076,W08-1131,0,0.0426993,"ovel NLG evaluation methodology by comparing the Internet-based results with results we collected in a lab experiment. We find that the results delivered by both methods are consistent, but the Internetbased approach offers the statistical power necessary for more fine-grained evaluations and is cheaper to carry out. 1 dbyron@ccs.neu.edu justine@northwestern.edu Sara Dalzel-Job U. of Edinburgh Robert.Dale@mq.edu.au Introduction Recently, there has been an increased interest in evaluating and comparing natural language generation (NLG) systems on shared tasks (Belz, 2009; Dale and White, 2007; Gatt et al., 2008). However, this is a notoriously hard problem (Scott and Moore, 2007): Task-based evaluations with human experimental subjects are time-consuming and expensive, and corpus-based evaluations of NLG systems are problematic because a mismatch between humangenerated output and system-generated output does not necessarily mean that the system’s output is inferior (Belz and Gatt, 2008). This lack of evaluation methods which are both effective and efficient is a serious obstacle to progress in NLG research. The GIVE Challenge (Byron et al., 2009) is a recent shared task which takes a third approach t"
P09-2076,W09-0629,0,\N,Missing
P89-1009,P83-1007,0,0.0112704,"Missing"
P89-1009,C86-1016,0,0.0304991,".e., references to entities which have already been mentioned in the discourse. The most notable features of the approach taken here are as follows: (a) the use of a sophisticated underlying ontology, to permit the representation of non-singular entities; (b) the use of two levels of semantic representation, in conjunction with a model of the discourse, to produce appropriate anaphoric referring expressions; (c) the use of a notion of discrimiaatory power, to determine what properties should be used in describing an entity; and (d) the use of a PATR-1ike unification grammar (see, for example, Karttunen (1986); Shieber (1986)) to pro68 THE REPRESENTATION INGREDIENTS OF In most natural language systems, it is assumed that all the entities in the domain of discourse are singular individuals. In more complex domains, such as recipes, this simplification is of limited value, since a large proportion of the objects we find are masses or sets, such as those described by the noun phrases two ounces of salt and three pounds of carrots respectively. In order to permit the representation of entities such as these, EPICURE makes use of a notion of a generalized physical object or physob]. This permits a consi"
P89-1009,J86-3001,0,\N,Missing
U03-1002,C92-1038,1,0.700521,"Missing"
U03-1002,P83-1000,0,0.141309,"ssible; the major problem here is coming up with an independently motivated notion of what it means to be ‘in focus’. The generation of one-anaphoric expressions, however, has been virtually ignored, apart from some initial explorations in Davey [1979], Jameson and Wahlster [1982], and Dale [1992, 1995]. A high-level characterisation of the algorithm that underlies much work in referring expression generation is shown in Figure 1. This is deﬁcient in a number of regards: pronouns may be used even if the intended referent is not in focus—see, for example, the centering algorithm of Grosz et al [1983]—and a deﬁnite noun phrase may be used even if the referent has not been mentioned before, or alternatively its form may be further constrained in some way by the structure of the discourse. However, these complications are not important for our present purposes. The question this paper addresses is as follows: how does the decision to use a one-anaphoric expression ﬁt into this kind of algorithm? 3 3.1 One-Anaphora One-Anaphora as Syntactic Substitution The phenomenon of one-anaphora is reasonably well discussed in the linguistics literature: in terms of X-bar theory, for example, the pro-for"
U03-1002,P83-1007,0,0.366461,"Missing"
U03-1012,P00-1041,0,0.0212721,"vation behind discovering segments in a text is that a sentence extraction summary should choose the most representative sentence for each segment, resulting in a comprehensive summary. In the view of Gong and Liu (2001), segments form the main themes of a document. They present a theme interpretation of the SVD analysis, as it is used for discourse segmentation, upon which our use of the technique is based. However, Gong and Liu use SVD for creating sentence extraction summaries, not for generating a single sentence summary by re-using words. In subsequent work to Witbrock and Mittal (1999), Banko et al. (2000) describe the use of information about the position of words within four quarters of the source document. The headline candidacy score of a word is weighted by its position in one of the quarters. We interpret this use of position information as a means of guiding the generation of a headline towards the central theme of the document, which for news articles typically occurs in the first quarter. SVD potentially offers a more general mechanism for handling the discovery of the central themes and their positions within the document. Jin et al. (2002) have also examined a statistical model for h"
U03-1012,W97-0704,0,0.0468846,"generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Hybrid methods for abstract-like summarisation, which combine statistical and symbolic approaches, have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus statistics and does not use statistical information about the distribution of words in the document itself. Our work differs in that we utilise an SVD analysis to provide information about the document to be summarized, specifically its main theme. Discourse segmentation for sentence extracti"
U03-1012,J00-2011,0,0.290873,"mentation, where sentences naturally cluster in terms of their ‘aboutness’. $ is about, a characteristic described by Borko as being indicative. 5 Using Singular Value Decomposition for Content Selection As an alternative to the Conditional probability, we examine the use of SVD in determining the Content Selection probability. Before we outline the procedure for basing this probability on SVD, we will first outline our interpretation of the SVD analysis, based on that of Gong and Liu (2001). Our description is not intended to be a comprehensive explanation of SVD, and we direct the reader to Manning and Schütze (2000) for a description of how SVD is used in information retrieval. Conceptually, when used to analyse documents, SVD can discover relationships between word co-occurrences in a corpus of text. For example, in the context of information retrieval, this provides one way to retrieve additional documents that contain synonyms of query terms, where synonymy is defined by similarity of word co-occurrences. By discovering patterns in word co-occurrences, SVD also provides information that can be used to cluster documents based on similarity of themes. In the context of single document summarisation, we"
U03-1012,J98-3005,0,0.171898,"ppropriate, given that the performance does not differ considerably. In such a situation, a collection of documents is only necessary for collecting bigram statistics. 7 Related Work As the focus of this paper is on statistical singlesentence summarisation we will not focus on preceding work which generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Hybrid methods for abstract-like summarisation, which combine statistical and symbolic approaches, have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus"
U03-1012,W03-1202,1,0.166119,"ence summaries using SVD. In Section 6, we present our experimental design in which we evaluated our approach, along with the results and corresponding discussion. Section 7, provides an overview of related work. Finally, in Section 8, we present our conclusions and future work. 2 Searching for a Probable Headline We re-implemented the work described in Witbrock and Mittal (1999) to provide a single sentence summarisation mechanism. For full details of their approach, we direct the reader to their paper (Witbrock and Mittal, 1999). For an overview of our implementation of their algorithm, see Wan et al. (2003). For convenience, a brief description is presented here. In a search, n words are selected on the basis of the two criteria. Conceptually, the task is twofold. Witbrock and Mittal (1999) label these two tasks as Content Selection and Realisation. Each criterion is scored probabilistically, whereby the probability is estimated by prior collection of corpus statistics. To estimate Content Selection probability for each word, we use the Maximum Likelihood Estimate (MLE). In an offline training stage, the system counts the number of times a word is used in a headline, with the condition that it o"
U04-1010,P99-1017,0,0.0157516,"(set; (van Deemter, 2002)) extends the basic approach to references to sets, as in the red cups. Some approaches combine algorithms which reuse only parts of other algorithms: the Branch and Bound (bab; (Krahmer et al., 2003)) algorithm uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. We have identified here what we believe to be the most cited strands of research in this area, but of course there are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002; Dale, 2003). 3"
U04-1010,E91-1028,1,0.832964,"articular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981), the first formally explicit algorithm was introduced in Dale (1989). This algorithm, which we will refer to as the Full Brevity (fb) algorithm, is still frequently used as a basis for other gre algorithms. The fb algorithm searches for the best solution amongst all possible referring expressions for an entity; the algorithm derives the smallest set of attributes for the referent in question, producing a referring expression that is both adequate and efficient. This initial algorithm limited its application to one-place predicates. Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations (henceforth ir), using a greedy heuristic to guide the search. As a response to the computational complexity of greedy algorithms, (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (ia). The most used and adapted algorithm, this is based on the observation that people often produce referring expressions which are informationally redundant; the algorithm uses a preference ordering over the attributes to be used in a referring exp"
U04-1010,P89-1009,1,0.697278,"ade it very difficult to compare and contrast the algorithms provided in any meaningful way. In this paper, we propose a characterisation of the problem of referring expression generation as a search problem; this allows us to recast existing algorithms in a way that makes their similarities and differences clear. 1 Introduction A major component task in natural language generation (nlg) is the generation of referring expressions: given an entity that we want to refer to, how do we determine the content of a referring expression that uniquely identifies that intended referent? Since at least (Dale, 1989), the standard conception of this task in the literature has been as follows: 1. We assume we have a knowledge base that characterises the entities in the domain in terms of a set of attributes and the values that the entities have for these attributes; so, for example, our knowledge base might represent the fact that entity e1 has the value cup for the attribute type, and the value red for the attribute colour. 2. In a typical context where we want to refer to some ei , which we call the intended referent, there will be other entities from which the intended referent must be distinguished; th"
U04-1010,U03-1002,1,0.85466,"; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002; Dale, 2003). 3 gre from the Perspective of Problem Solving With so many algorithms to choose from, it would be useful to have a uniform framework in which to discuss and compare algorithms; unfortunately, this is rather difficult given the variety of different approaches that have been taken to the problem. Within the wider context of ai, Russell and Norvig (2003) present an elegant definition of a general algorithm for problem solving by search. The search graph consists of nodes with the components state and path-cost; the problem is represented by an initial-state, an expandmethod which identifies new"
U04-1010,P97-1027,0,0.0201059,"Sets algorithm (set; (van Deemter, 2002)) extends the basic approach to references to sets, as in the red cups. Some approaches combine algorithms which reuse only parts of other algorithms: the Branch and Bound (bab; (Krahmer et al., 2003)) algorithm uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. We have identified here what we believe to be the most cited strands of research in this area, but of course there are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002;"
U04-1010,J03-1003,0,0.0309219,"Missing"
U04-1010,C92-1038,1,0.589916,"lgorithms. The fb algorithm searches for the best solution amongst all possible referring expressions for an entity; the algorithm derives the smallest set of attributes for the referent in question, producing a referring expression that is both adequate and efficient. This initial algorithm limited its application to one-place predicates. Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations (henceforth ir), using a greedy heuristic to guide the search. As a response to the computational complexity of greedy algorithms, (Reiter and Dale, 1992; Dale and Reiter, 1995) introduced the psycholinguistically motivated Incremental Algorithm (ia). The most used and adapted algorithm, this is based on the observation that people often produce referring expressions which are informationally redundant; the algorithm uses a preference ordering over the attributes to be used in a referring expression, accumulating those attributes which rule out at least one potential distractor. In recent years there have been a number of important extensions to the ia. The Context-Sensitive extension (cs; (Krahmer and Theune, 2002)) is able to generate referr"
U04-1010,W00-1416,0,0.0267671,"ter, 2002)) extends the basic approach to references to sets, as in the red cups. Some approaches combine algorithms which reuse only parts of other algorithms: the Branch and Bound (bab; (Krahmer et al., 2003)) algorithm uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. We have identified here what we believe to be the most cited strands of research in this area, but of course there are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000). Space limitations prevent a complete summary of this other work here, but our intention is to extend the analysis presented in this paper to as many of these other algorithms as possible. All these algorithms focus on the generation of definite references; they are typically embedded in a higher-level algorithm that includes cases for when the entity has not been previously mentioned (thus leading to an initial indefinite reference) or when the referent is in focus (thus leading to a pronominal reference); see, for example, (Dale, 1989; Krahmer and Theune, 2002; Dale, 2003). 3 gre from the P"
U04-1010,J02-1003,0,0.0322508,"Missing"
U04-1010,H89-1033,0,0.0115928,"problems in an elegant and uniform way (see, for example, (Simon and Newell, 1963); (Russell and Norvig, 2003)), sketching how gre algorithms can be expressed in terms of problem-solving by search. In Section 4, we explore how the most well-known algorithms can be expressed in this framework. In Section 5, we discuss how this approach enables a more fruitful comparison of existing algorithms, and we point to ways of taking this work further. 2 A Brief Review of Work To Date Although the task of referring expression generation is discussed informally in earlier work on nlg (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981), the first formally explicit algorithm was introduced in Dale (1989). This algorithm, which we will refer to as the Full Brevity (fb) algorithm, is still frequently used as a basis for other gre algorithms. The fb algorithm searches for the best solution amongst all possible referring expressions for an entity; the algorithm derives the smallest set of attributes for the referent in question, producing a referring expression that is both adequate and efficient. This initial algorithm limited its application to one-place predicates. Dale and Haddock (1991) introd"
U05-1004,C96-1079,0,0.221929,"Missing"
U05-1004,M98-1021,0,0.0638379,"Missing"
U05-1004,M98-1004,0,\N,Missing
U05-1004,M98-1012,0,\N,Missing
U05-1004,M98-1014,0,\N,Missing
U06-1007,W04-3240,0,0.244905,"Missing"
U06-1007,J00-3003,0,0.26356,"Missing"
U06-1007,W04-1008,0,0.0607951,"Missing"
U06-1007,A97-1011,0,0.00863765,"exical or contentless utterances; terms of address or salutation. 2nd person; verb implies an attribute or ability of the other; terms of evaluation. 2nd person; verb implies internal experience or volitional action. Table 4: VRM form criteria from (Stiles, 1992) The intuition for including the utterance length as a feature is that different VRMs are often associated with longer or shorter utterances - e.g., Acknowledgement utterances are often short, while Edifications are often longer. To compute our utterance features, we made use of the Connexor Functional Dependency Grammar (FDG) parser (Tapanainen and Jarvinen, 1997) for grammatical analysis and to extract syntactic dependency information for the words in each utterance. We also used the morphological tags assigned by Connexor. This information was used to calculate utterance features as follows: • Utterance Length: The number of words in the utterance. • First Word: The first word in each utterance, represented as a series of independent boolean features (one for each unique first word present in the corpus). • Last Token: The last token in each utterance – either the final punctuation (if present) or the final word in the utterance. As for the First Wor"
U06-1017,2001.mtsummit-papers.3,0,0.0135327,", this means that the algorithms start out with the disadvantage of only being able to enter one submission per referent into the competition, when there are a multitude of possible ‘right’ answers. This issue of the inherent non-determinism of natural language significantly increases the degree of difficulty in evaluating referring expression algorithms, and other NLG systems, against natural data. Of course, this problem is not unique to NLG : recent evaluation exercises in both statistical machine translation and document summarisation have faced the problem of multiple gold standards (see Akiba et al. (2001) and Nenkova and Passonneau (2004), respectively). However, it is not obvious that such a fine-grained task as referring expression generation can similarly be evaluated by comparison against a gold standard set of correct answers, since even a large evaluation corpus of natural referring expressions can never be guaranteed to contain all acceptable descriptions 120 5 Measuring Performance Related to the above discussion is the question of how we measure the performance of these systems even when we do have a gold standard corpus that contains the referring expressions generated by our algorit"
U06-1017,W00-1401,0,0.0348496,"on their performance on the same task. A large number of different research communities within NLP, such as Question Answering, Machine Translation, Document Summarisation, Word Sense Disambiguation, and Information Retrieval, have adopted a shared evaluation metric and in many cases a shared-task evaluation competition. The NLG community has so far withstood this trend towards a joint evaluation metric and a competitive evaluation task, but the idea has surfaced in a number of discussions, and most intensely at the 2006 International Natural Language Generation Conference (see, for example, Bangalore et al. (2000), Reiter and Sripada (2002), Reiter and Belz (2006), Belz and Reiter (2006), Belz and Kilgarriff (2006), Paris et al. (2006), and van Deemter et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This"
U06-1017,W06-1421,0,0.0694261,"such as Question Answering, Machine Translation, Document Summarisation, Word Sense Disambiguation, and Information Retrieval, have adopted a shared evaluation metric and in many cases a shared-task evaluation competition. The NLG community has so far withstood this trend towards a joint evaluation metric and a competitive evaluation task, but the idea has surfaced in a number of discussions, and most intensely at the 2006 International Natural Language Generation Conference (see, for example, Bangalore et al. (2000), Reiter and Sripada (2002), Reiter and Belz (2006), Belz and Reiter (2006), Belz and Kilgarriff (2006), Paris et al. (2006), and van Deemter et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This suggests that, if formal shared tasks for NLG are to be developed, the generation of referring express"
U06-1017,E06-1040,0,0.0558702,"communities within NLP, such as Question Answering, Machine Translation, Document Summarisation, Word Sense Disambiguation, and Information Retrieval, have adopted a shared evaluation metric and in many cases a shared-task evaluation competition. The NLG community has so far withstood this trend towards a joint evaluation metric and a competitive evaluation task, but the idea has surfaced in a number of discussions, and most intensely at the 2006 International Natural Language Generation Conference (see, for example, Bangalore et al. (2000), Reiter and Sripada (2002), Reiter and Belz (2006), Belz and Reiter (2006), Belz and Kilgarriff (2006), Paris et al. (2006), and van Deemter et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This suggests that, if formal shared tasks for NLG are to be developed, the gen"
U06-1017,E91-1028,1,0.746311,"Missing"
U06-1017,P89-1009,1,0.816012,"eferring expression to be minimal means that all of the facts about the referent that are contained in the expression are essential for the hearer to be able to uniquely distinguish the referent from the other objects in the domain. If any part of the referring expression was dropped, 117 The Algorithms Many detailed descriptions of algorithms are available in the literature on the generation of referring expressions. For the purpose of our evaluation experiment, we focussed here on three algorithms on which many subsequently developed algorithms have been based: • The Full Brevity algorithm (Dale, 1989) uses a greedy heuristic for its attempt to build a minimal distinguishing description. At each step, it always selects the most discriminatory property available. • The Relational Algorithm from (Dale and Haddock, 1991) uses constraint satisfaction to incorporate relational properties into the framework of the Full Brevity algorithm. It uses a simple mechanism to avoid infinite regress. • The Incremental Algorithm (Reiter and Dale, 1992; Dale and Reiter, 1995) considers the available properties to be used in a description via a predefined preference ordering over those properties. We re-imple"
U06-1017,N04-1019,0,0.0200592,"gorithms start out with the disadvantage of only being able to enter one submission per referent into the competition, when there are a multitude of possible ‘right’ answers. This issue of the inherent non-determinism of natural language significantly increases the degree of difficulty in evaluating referring expression algorithms, and other NLG systems, against natural data. Of course, this problem is not unique to NLG : recent evaluation exercises in both statistical machine translation and document summarisation have faced the problem of multiple gold standards (see Akiba et al. (2001) and Nenkova and Passonneau (2004), respectively). However, it is not obvious that such a fine-grained task as referring expression generation can similarly be evaluated by comparison against a gold standard set of correct answers, since even a large evaluation corpus of natural referring expressions can never be guaranteed to contain all acceptable descriptions 120 5 Measuring Performance Related to the above discussion is the question of how we measure the performance of these systems even when we do have a gold standard corpus that contains the referring expressions generated by our algorithms. In Section 2.3, we noted that"
U06-1017,W06-1419,0,0.0492393,"Machine Translation, Document Summarisation, Word Sense Disambiguation, and Information Retrieval, have adopted a shared evaluation metric and in many cases a shared-task evaluation competition. The NLG community has so far withstood this trend towards a joint evaluation metric and a competitive evaluation task, but the idea has surfaced in a number of discussions, and most intensely at the 2006 International Natural Language Generation Conference (see, for example, Bangalore et al. (2000), Reiter and Sripada (2002), Reiter and Belz (2006), Belz and Reiter (2006), Belz and Kilgarriff (2006), Paris et al. (2006), and van Deemter et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This suggests that, if formal shared tasks for NLG are to be developed, the generation of referring expressions is a very strong"
U06-1017,W06-1422,0,0.0179284,"er of different research communities within NLP, such as Question Answering, Machine Translation, Document Summarisation, Word Sense Disambiguation, and Information Retrieval, have adopted a shared evaluation metric and in many cases a shared-task evaluation competition. The NLG community has so far withstood this trend towards a joint evaluation metric and a competitive evaluation task, but the idea has surfaced in a number of discussions, and most intensely at the 2006 International Natural Language Generation Conference (see, for example, Bangalore et al. (2000), Reiter and Sripada (2002), Reiter and Belz (2006), Belz and Reiter (2006), Belz and Kilgarriff (2006), Paris et al. (2006), and van Deemter et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This suggests that, if formal shared tasks for NLG are"
U06-1017,C92-1038,1,0.805419,"urpose of our evaluation experiment, we focussed here on three algorithms on which many subsequently developed algorithms have been based: • The Full Brevity algorithm (Dale, 1989) uses a greedy heuristic for its attempt to build a minimal distinguishing description. At each step, it always selects the most discriminatory property available. • The Relational Algorithm from (Dale and Haddock, 1991) uses constraint satisfaction to incorporate relational properties into the framework of the Full Brevity algorithm. It uses a simple mechanism to avoid infinite regress. • The Incremental Algorithm (Reiter and Dale, 1992; Dale and Reiter, 1995) considers the available properties to be used in a description via a predefined preference ordering over those properties. We re-implemented these algorithms and applied them to a knowledge base made up of the properties evidenced collectively in the human-generated data. We then analysed to which extent the output of the algorithms for each drawer was semantically equivalent to the descriptions produced by the human participants. The following section gives a short account of this analysis. 2.3 Coverage of the Human Data Out of the 103 natural descriptions that do not"
U06-1017,W02-2113,0,0.0239091,"the same task. A large number of different research communities within NLP, such as Question Answering, Machine Translation, Document Summarisation, Word Sense Disambiguation, and Information Retrieval, have adopted a shared evaluation metric and in many cases a shared-task evaluation competition. The NLG community has so far withstood this trend towards a joint evaluation metric and a competitive evaluation task, but the idea has surfaced in a number of discussions, and most intensely at the 2006 International Natural Language Generation Conference (see, for example, Bangalore et al. (2000), Reiter and Sripada (2002), Reiter and Belz (2006), Belz and Reiter (2006), Belz and Kilgarriff (2006), Paris et al. (2006), and van Deemter et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This suggests that, if formal s"
U06-1017,W01-0804,0,0.0668699,"Missing"
U06-1017,W06-1420,0,0.0654633,"Missing"
U06-1017,W06-1410,1,0.908469,"r et al. (2006)). Amongst the various component tasks that make up Natural Language Generation, the generation of referring expressions is probably the subtask for which there is the most agreement on problem definition; a significant body of work now exists in the development of algorithms for generating referring expressions, with almost all published contributions agreeing on the general characterisation of the task and what constitutes a solution. This suggests that, if formal shared tasks for NLG are to be developed, the generation of referring expressions is a very strong candidate. In (Viethen and Dale, 2006), we argued that the evaluation of referring expression generation algorithms against natural, human-generated data is of fundamental importance in assessing their usefulness for the generation of understandable, naturalsounding referring expressions. In this paper, we discuss a number of issues that arise from the evaluation carried out in (Viethen and Dale, 2006), and consider what these issues mean for any attempt to define a shared task in this area. The remainder of this paper has the following structure. In Section 2, we briefly describe the evaluation experiment we carried out for three"
U06-1019,P05-1074,0,0.019642,"known performance for this corpus. We also examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 1 Introduction In recent years, interest has grown in paraphrase generation methods. The use of paraphrase generation tools has been envisaged for applications ranging from abstract-like summarisation (see for example, Barzilay and Lee (2003), Daum´e and Marcu (2005), Wan et al. (2005)), questionanswering (for example, Marsi and Krahmer (2005)) and Machine Translation Evaluation (for example, Bannard and Callison-Burch (2005) and Yves Lepage (2005)). These approaches all employ a loose definition of paraphrase attributable to Dras (1999), who defines a ‘paraphrase pair’ operationally to be “a pair of units of text deemed to be interchangeable”. Notably, such a definition of paraphrase lends itself easily to corpora based methods. Furthermore, what the more modern approaches share is the fact that often they generate new paraphrases from raw text not semantic representations. The generation of paraphrases from raw text is a specific type of what is commonly referred to as text-to-text generation (Barzilay and Lee,"
U06-1019,W05-1612,0,0.0149558,"Missing"
U06-1019,N03-1003,0,0.0184087,"Missing"
U06-1019,U03-1014,0,0.036602,"Missing"
U06-1019,P96-1025,0,0.0188936,"Missing"
U06-1019,P02-1040,0,0.0775439,"equence overlap, where tokenisation is delimited by white space. We considered unigram overlap and explored two metrics, recall (feature 1) and precision (feature 2), where a precision score is defined as: precision = word-overlap(sentence1 , sentence2 ) word-count(sentence1 ) and recall is defined as: recall = word-overlap(sentence1 , sentence2 ) word-count(sentence2 ) nexor parser2 which provides lemmatisation information. For both sentences, each original word is replaced by its lemma. We then calculated our unigram precision and recall scores as before (features 3 and 4). The Bleu metric (Papineni et al., 2002), which uses the geometric average of unigram, bigram and trigram precision scores, is implemented as feature 5. The score was obtained using the original Bleu formula3 with a brevity penalty set to 1 (that is, the brevity penalty is ignored). Note that in our usage, there is only one ’reference’ sentence. By reversing which sentence was considered the ‘test’ sentence and which was considered the ‘reference’, a recall version of Bleu was obtained (feature 6). Lemmatised versions provided features 7 and 8. Finally, because of the bi-directionality property of paraphrase, the F-Measure4 , which"
U06-1019,W05-1203,0,0.0338777,"Missing"
U06-1019,C04-1051,0,0.253524,"Missing"
U06-1019,W06-1603,0,0.516548,"Missing"
U06-1019,I05-5012,1,0.793999,"nd sentence was generated from an input news article statistically using a four-gram language model and a probabilistic word selection module. Although other paraphrase generation approaches differ in their underlying mechanisms1 , most generate a novel sentence that cannot be found verbatim in the input text. The generated second sentence of the example is intended to be a paraphrase of the article headline. One might be convinced that the first exam1 The details of the generation algorithm used for this example are peripheral to the focus of this paper and we direct the interested reader to Wan et al. (2005) for more details. Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 131–138. 131 2 Paraphrase Classification and Related Work Example 1: Original Headline: European feeds remain calm on higher dollar. Generated Sentence: The European meals and feeds prices were firm on a stronger dollar; kept most buyers in this market. Example 2: Original Headline: India’s Gujral says too early to recognise Taleban. Generated Sentence: Prime Minister Inder Kumar Gujral of India and Pakistan to recognise the Taleban government in Kabul. Figure 1: Two examples of generated nov"
U06-1019,I05-5008,0,0.0109142,"examine the strengths and weaknesses of dependency based features and conclude that they may be useful in more accurately classifying cases of False Paraphrase. 1 Introduction In recent years, interest has grown in paraphrase generation methods. The use of paraphrase generation tools has been envisaged for applications ranging from abstract-like summarisation (see for example, Barzilay and Lee (2003), Daum´e and Marcu (2005), Wan et al. (2005)), questionanswering (for example, Marsi and Krahmer (2005)) and Machine Translation Evaluation (for example, Bannard and Callison-Burch (2005) and Yves Lepage (2005)). These approaches all employ a loose definition of paraphrase attributable to Dras (1999), who defines a ‘paraphrase pair’ operationally to be “a pair of units of text deemed to be interchangeable”. Notably, such a definition of paraphrase lends itself easily to corpora based methods. Furthermore, what the more modern approaches share is the fact that often they generate new paraphrases from raw text not semantic representations. The generation of paraphrases from raw text is a specific type of what is commonly referred to as text-to-text generation (Barzilay and Lee, 2003). As techniques fo"
U06-1019,U05-1023,0,0.485398,"Missing"
U06-1019,I05-5003,0,\N,Missing
U07-1012,W04-3224,0,0.0185269,"s then repeated for each successful child node. 5. When the tree is fully populated by terminal nodes, the ﬁnal chart is returned as a Penn Treebank parse representation for evaluation. We now describe these stages in more detail. 3.2 and is spanned thus: t h e c a t s a t 0 1 2 3 4 5 6 7 8 9 10 11 Parsing The ﬁrst stage in our process is to parse each sentence with the individual contributing parsers. In the experiments reported here, we use three parsers: the Stanford lexicalised parser (Klein and Manning, 2003); Collins generative parsing model number 2 (Collins, 1999) as re-implemented by Bikel (2004); and the OpenNLP parser (Baldridge et al., 2003). These were chosen for two reasons: This analysis would be represented as a chart consisting of edges: • all three parsers output parses in the standard Penn Treebank notation, making conversion to our chart representation the same process for all; and The fundamental difference between our approach and those of Henderson and Brill (1999) and Sagae and Lavie (2006) described earlier is in the strategy used when selecting constituents. Previous approaches have considered constituents in isolation: Sagae and Lavie’s charts contain all possible co"
U07-1012,N03-1004,0,0.0389529,"Missing"
U07-1012,W05-1102,0,0.0301414,"Missing"
U07-1012,P98-1081,0,0.0948966,"Missing"
U07-1012,W99-0623,0,0.654201,"es. VP HH  H VBP lock PP HH  H IN in NP HH  H NP PP H  H  NNS IN proﬁts by H S VP HH VBG NP buying NNS 2 Background: Combining Parsers futures The combination of the results of several different components that carry out the same task— sometimes referred to as the ensemble-based approach—has been employed and shown to be successful in a number of ﬁelds such as part-of-speech tagging (Halteren et al., 1998), word sense disambiguation (Pederson, 2000) and question answering (Chu-Carroll et al., 2003). There are a number of approaches that have been employed for parser combination. Henderson and Brill (1999) describe experiments that fall within two general approaches they label parse hybridization and parse switching. The most basic form of hybridization is constituent voting, whereby constituents in a parse are included if they can be found in the majority of contributing parses. A second approach is to use a na¨ıve Bayes classiﬁer in order to learn how much each parser should be trusted. The alternative to this approach is to deal only with complete parses. Henderson and Brill again experimented with two approaches: similarity switching, whereby the parse chosen is the one which scores highest"
U07-1012,A00-2005,0,0.0604012,"Missing"
U07-1012,P03-1054,0,0.0196231,"arting with the root node, voting takes place as to what the children of that node should be. 4. Step 3 is then repeated for each successful child node. 5. When the tree is fully populated by terminal nodes, the ﬁnal chart is returned as a Penn Treebank parse representation for evaluation. We now describe these stages in more detail. 3.2 and is spanned thus: t h e c a t s a t 0 1 2 3 4 5 6 7 8 9 10 11 Parsing The ﬁrst stage in our process is to parse each sentence with the individual contributing parsers. In the experiments reported here, we use three parsers: the Stanford lexicalised parser (Klein and Manning, 2003); Collins generative parsing model number 2 (Collins, 1999) as re-implemented by Bikel (2004); and the OpenNLP parser (Baldridge et al., 2003). These were chosen for two reasons: This analysis would be represented as a chart consisting of edges: • all three parsers output parses in the standard Penn Treebank notation, making conversion to our chart representation the same process for all; and The fundamental difference between our approach and those of Henderson and Brill (1999) and Sagae and Lavie (2006) described earlier is in the strategy used when selecting constituents. Previous approache"
U07-1012,A00-2009,0,0.0732542,"Missing"
U07-1012,P07-1078,0,0.0390075,"Missing"
U07-1012,P07-1052,0,0.0365256,"Missing"
U07-1012,N06-2033,0,0.430536,"chieved equally promising results from their variants of parse switching. The ﬁrst of these was fallback cascades in which parsers are stacked in order of decreasing levels of sophistication. When the more complex model fails, the next parser attempts to parse. The bottom parser may be less accurate, but will be the least likely to fail. Their second whole-parse approach they simply termed parse selection, though it is similar to Henderson and Brill’s similarity switching. Clegg and Shepherd varied this by trying different similarity metrics, such as constituent overlap or lineage similarity. Sagae and Lavie (2006) apply a notion of reparsing to a two stage parser combination chartbased approach. Once all single parses are complete, the ﬁrst stage is to store all possible constituents in a chart with a label, start and end positions, and a weighting. Identical constituents from different parses are merged by adding their weights. The second stage of the process is to run a bottomup parsing algorithm, but rather than use a weighted grammar, the parser is guided by the weighted set of constituents. They experimented with different approaches to setting the initial weights of each nonterminal label. By com"
U07-1012,W05-1518,0,0.317171,"Missing"
U07-1012,J03-4003,0,\N,Missing
U07-1012,C98-1078,0,\N,Missing
U08-1009,W04-3240,0,0.209927,"Missing"
U08-1009,W04-1008,0,0.581779,"y existing definitions deal only with requests and ignore conditionality (which is very common) as a feature (for example, (Camino et al., 1998; Khosravi and Wilks, 1999; Leuski, 2004)). Others define requests and commitments in terms of specific conversation states, requiring the creation of multiple categories for the same speech act in different stages of a conversation. Often not all of the many combinations of speech acts and conversation states are modeled, resulting in uncodable utterances (Cohen et al., 2004; Goldstein and Sabin, 2006). Previous work on utterance-level classification (Corston-Oliver et al., 2004) relied on short, simple definitions and canonical examples. These lack the detail and clarity required for unambiguous classification of the complex requests and commitments we find in real-world email. Since our review of related work, Scerri et al. (2008) have noted some similar concerns. Unfortunately, their interannotator agreement for requests and commitments remains low; we believe this could be improved through the careful consideration of the edge cases we outline in this paper. Conditionality is an important part of our definitions. Conditional requests and commitments require action"
U08-1009,scerri-etal-2008-evaluating,0,0.0779736,", requiring the creation of multiple categories for the same speech act in different stages of a conversation. Often not all of the many combinations of speech acts and conversation states are modeled, resulting in uncodable utterances (Cohen et al., 2004; Goldstein and Sabin, 2006). Previous work on utterance-level classification (Corston-Oliver et al., 2004) relied on short, simple definitions and canonical examples. These lack the detail and clarity required for unambiguous classification of the complex requests and commitments we find in real-world email. Since our review of related work, Scerri et al. (2008) have noted some similar concerns. Unfortunately, their interannotator agreement for requests and commitments remains low; we believe this could be improved through the careful consideration of the edge cases we outline in this paper. Conditionality is an important part of our definitions. Conditional requests and commitments require action only if a stated condition is satisfied. Our early annotation experiments, summarised in Section 3 and detailed in (Lampert et al., 2007), show that annotators require guidance about how to classify conditional requests and commitments to achieve even moder"
U08-1020,C94-2182,0,0.15617,"Missing"
U08-1020,P02-1013,0,0.0343235,"n of locations in ‘omniscient room’ scenarios, where an intelligent agent might try to tell you where you left your RFID-tagged keys. In these scenarios, it is very likely that the referring expressions generated will need to make use of the spatial relationships that hold between the intended referent and other entities in the domain; but, surprisingly, the Robert Dale Centre for Language Technology Macquarie University Sydney, Australia rdale@ics.mq.edu.au generation of relational references is a relatively unexplored task. The few algorithms that address this task (Dale and Haddock (1991), Gardent (2002), Krahmer and Theune (2002), Varges (2005), Kelleher and Kruiff (2005, 2006)) typically adopt fairly simple approaches: they only consider spatial relations if it is not possible to fully distinguish the target referent from the surrounding objects in any other way, or they treat them in exactly the same as nonrelational properties. As acknowledged by some of this work, this creates additional problems such as infinite regress and the inclusion of relations without regard for the properties of the landmarks that are associated with them. To be able to develop algorithms that meet the requireme"
U08-1020,H93-1005,0,0.0265328,"ich referring expressions are used can be very complex. Consider the following hypothetical references in the motivating scenarios we used in the introduction: (1) Turn left after the second shopfront that has a ‘For lease’ sign in the window. (2) Your keys are under the loose leaf folder on the desk in the upstairs study. In such real life situations, there are generally too many variables to permit carefully controlled experiments that would allow us to derive general principles for content determination. In line with almost all work in this area (see, for example, Brennan and Clark (1996), Thompson et al. (1993), Gorniak and Roy (2004), Jordan and Walker (2005), Byron and Fosler-Lussier (2006)), we therefore begin our explorations with very much simpler scenarios that allow us to explore specific hypotheses and to characterise the general strategies that humans seem to adopt; we can then apply these strategies in more complex scenes to see whether they continue to be applicable. Our goal is to determine what characteristics of scenes impact on the use of spatial relations. The data gathering experiment we conducted had the form of a self-paced on-line language production study. Participants visited a"
U08-1020,W05-1627,0,0.020734,"os, where an intelligent agent might try to tell you where you left your RFID-tagged keys. In these scenarios, it is very likely that the referring expressions generated will need to make use of the spatial relationships that hold between the intended referent and other entities in the domain; but, surprisingly, the Robert Dale Centre for Language Technology Macquarie University Sydney, Australia rdale@ics.mq.edu.au generation of relational references is a relatively unexplored task. The few algorithms that address this task (Dale and Haddock (1991), Gardent (2002), Krahmer and Theune (2002), Varges (2005), Kelleher and Kruiff (2005, 2006)) typically adopt fairly simple approaches: they only consider spatial relations if it is not possible to fully distinguish the target referent from the surrounding objects in any other way, or they treat them in exactly the same as nonrelational properties. As acknowledged by some of this work, this creates additional problems such as infinite regress and the inclusion of relations without regard for the properties of the landmarks that are associated with them. To be able to develop algorithms that meet the requirements of applications like those just mentio"
U08-1020,W08-1109,1,0.665595,"Missing"
U08-1020,P06-1131,0,\N,Missing
U08-1020,byron-fosler-lussier-2006-osu,0,\N,Missing
U10-1013,W08-1132,0,0.157408,"Missing"
U10-1013,W09-0631,0,0.143927,"Missing"
U10-1013,N01-1002,0,0.0431908,"Missing"
U10-1013,P89-1009,1,0.461562,"xt of reference is widely considered to be a primary determinant of content in referring expression generation, so we explore whether a model can be trained to predict the collection of descriptive attributes that should be used in a given situation. Our experiments demonstrate that speaker-specific preferences play a much more important role than existing approaches to referring expression generation acknowledge. 1 Introduction Since at least the late 1980s, referring expression generation (REG) has been a key topic of interest in the natural language generation community (see, for example, (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; Jordan and Walker, 2005; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006)); and it has recently served as the focus for the first major evaluation efforts in natural language generation (see, for example, (Belz et al., 2009; Gatt et al., 2009)). This level of attention is due in large part to the consensus view that has arisen as to what is involved in referring expression generation: the task is widely accepted as involving a process of selecting thos"
U10-1013,W08-1135,0,0.0641668,"Missing"
U10-1013,W09-0633,0,0.0432783,"Missing"
U10-1013,W08-1133,0,0.102222,"Missing"
U10-1013,2007.mtsummit-ucnlg.14,0,0.191232,"ference orderings or cost functions over the available properties in order to choose those that should appear in a referring expression (Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006). However, only very limited attempts have been made to determine how these parameters should best be instantiated in order to allow an algorithm to mimic human-produced referring expressions. Furthermore, the results of recent evaluation exercises (Gupta and Stent, 2005; Viethen and Dale, 2006; Belz and Gatt, 2007; Gatt et al., 2007; Gatt et al., 2008) show that none of these algorithms can be considered an accurate model of human production of referring expressions in any of their instantiations. In this paper, we take a speaker-oriented perspective on REG that is aimed in part at exploring the factors that impact on the choices that humans make when they refer, and ultimately at finding models for REG which can claim at least a certain level of cognitive plausibility by being able to replicate human referring behaviour. To this end we use two large corpora of referring expressions to train machine le"
U10-1013,W09-2816,1,0.829609,"pproaches to referring expression generation acknowledge. 1 Introduction Since at least the late 1980s, referring expression generation (REG) has been a key topic of interest in the natural language generation community (see, for example, (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; Jordan and Walker, 2005; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006)); and it has recently served as the focus for the first major evaluation efforts in natural language generation (see, for example, (Belz et al., 2009; Gatt et al., 2009)). This level of attention is due in large part to the consensus view that has arisen as to what is involved in referring expression generation: the task is widely accepted as involving a process of selecting those attributes of an intended referent that distinguish it from other potential distractors in a given context, resulting Robert Dale Centre for Language Technology Macquarie University Sydney, Australia robert.dale@mq.du.au in what is often referred to as a distinguishing description. Most existing REG algorithms rely on handcrafted decision procedures whose behavio"
U10-1013,P02-1013,0,0.127184,"e consensus view that has arisen as to what is involved in referring expression generation: the task is widely accepted as involving a process of selecting those attributes of an intended referent that distinguish it from other potential distractors in a given context, resulting Robert Dale Centre for Language Technology Macquarie University Sydney, Australia robert.dale@mq.du.au in what is often referred to as a distinguishing description. Most existing REG algorithms rely on handcrafted decision procedures whose behaviour is either entirely deterministic (Dale, 1989; Dale and Haddock, 1991; Gardent, 2002) or can be influenced to some degree using parameters such as preference orderings or cost functions over the available properties in order to choose those that should appear in a referring expression (Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006). However, only very limited attempts have been made to determine how these parameters should best be instantiated in order to allow an algorithm to mimic human-produced referring expressions. Furthermore, the results of recent eval"
U10-1013,P06-2033,0,0.0373165,"Missing"
U10-1013,W07-2307,0,0.706383,"Missing"
U10-1013,W08-1131,0,0.537706,"the available properties in order to choose those that should appear in a referring expression (Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006). However, only very limited attempts have been made to determine how these parameters should best be instantiated in order to allow an algorithm to mimic human-produced referring expressions. Furthermore, the results of recent evaluation exercises (Gupta and Stent, 2005; Viethen and Dale, 2006; Belz and Gatt, 2007; Gatt et al., 2007; Gatt et al., 2008) show that none of these algorithms can be considered an accurate model of human production of referring expressions in any of their instantiations. In this paper, we take a speaker-oriented perspective on REG that is aimed in part at exploring the factors that impact on the choices that humans make when they refer, and ultimately at finding models for REG which can claim at least a certain level of cognitive plausibility by being able to replicate human referring behaviour. To this end we use two large corpora of referring expressions to train machine learning models on the task of content de"
U10-1013,J03-1003,0,0.705167,"Missing"
U10-1013,W09-0629,0,0.447573,"ing expression generation acknowledge. 1 Introduction Since at least the late 1980s, referring expression generation (REG) has been a key topic of interest in the natural language generation community (see, for example, (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; Jordan and Walker, 2005; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006)); and it has recently served as the focus for the first major evaluation efforts in natural language generation (see, for example, (Belz et al., 2009; Gatt et al., 2009)). This level of attention is due in large part to the consensus view that has arisen as to what is involved in referring expression generation: the task is widely accepted as involving a process of selecting those attributes of an intended referent that distinguish it from other potential distractors in a given context, resulting Robert Dale Centre for Language Technology Macquarie University Sydney, Australia robert.dale@mq.du.au in what is often referred to as a distinguishing description. Most existing REG algorithms rely on handcrafted decision procedures whose behaviour is either entirel"
U10-1013,W06-1412,0,0.0430126,"Missing"
U10-1013,W08-1134,0,0.203789,"Missing"
U10-1013,W09-0632,0,0.0456387,"Missing"
U10-1013,J06-2002,0,0.0799114,"Missing"
U10-1013,W06-1410,1,0.87986,"g parameters such as preference orderings or cost functions over the available properties in order to choose those that should appear in a referring expression (Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006). However, only very limited attempts have been made to determine how these parameters should best be instantiated in order to allow an algorithm to mimic human-produced referring expressions. Furthermore, the results of recent evaluation exercises (Gupta and Stent, 2005; Viethen and Dale, 2006; Belz and Gatt, 2007; Gatt et al., 2007; Gatt et al., 2008) show that none of these algorithms can be considered an accurate model of human production of referring expressions in any of their instantiations. In this paper, we take a speaker-oriented perspective on REG that is aimed in part at exploring the factors that impact on the choices that humans make when they refer, and ultimately at finding models for REG which can claim at least a certain level of cognitive plausibility by being able to replicate human referring behaviour. To this end we use two large corpora of referring expression"
U10-1013,P06-1131,0,0.0595672,"tributes that should be used in a given situation. Our experiments demonstrate that speaker-specific preferences play a much more important role than existing approaches to referring expression generation acknowledge. 1 Introduction Since at least the late 1980s, referring expression generation (REG) has been a key topic of interest in the natural language generation community (see, for example, (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; van der Sluis, 2001; Krahmer and Theune, 2002; Krahmer et al., 2003; Jordan and Walker, 2005; van Deemter, 2006; Gatt and van Deemter, 2006; Kelleher and Kruijff, 2006)); and it has recently served as the focus for the first major evaluation efforts in natural language generation (see, for example, (Belz et al., 2009; Gatt et al., 2009)). This level of attention is due in large part to the consensus view that has arisen as to what is involved in referring expression generation: the task is widely accepted as involving a process of selecting those attributes of an intended referent that distinguish it from other potential distractors in a given context, resulting Robert Dale Centre for Language Technology Macquarie University Sydney, Australia robert.dale@mq."
U10-1013,W08-1137,0,0.0506392,"Missing"
U10-1015,H01-1052,0,0.0391686,"y detection in transcripts of spontaneous spoken language. Many models have been proposed for this task in the literature; the best performing models so far are statistical by nature and have large data needs. A statistical natural language processing algorithm typically has two important components: a model that describes the behaviour of interest, and the training data which is necessary to guide that model. It has been observed that simple algorithms can outperform more complex models when these simple algorithms have the advantage in terms of the amount of data available; so, for example, Brill and Banko (2001) argue that more data is more important than better algorithms for some natural language processing tasks. It is this insight that drives the work described in this paper. Our current approach to speech disfluency detection is trained on manually-constructed spoken language corpora which contain annotations of all disfluencies as part of the transcription process. Our model is based on the noisy channel model and consists of a language model and a channel model. As we have reported elsewhere (Zwarts et al., 2010), we are able to achieve reasonable results when using Switchboard data: we obtain"
U10-1015,P01-1017,0,0.0929352,"Missing"
U10-1015,P04-1005,1,0.818224,"our work typically have this characteristic: when a speaker edits her speech for meaningrelated reasons, rather than errors that arise from performance, the resulting disfluency can be by itself fluent. We can see this in Example (1): the repair and the reparandum are equally fluent. This makes it difficult to distinguish reparanda as being part of disfluencies when only lexical cues are available. Since the transcripts we work with do not have prosodic cues annotated, we need to look elsewhere for a solution to this problem. Noisy Channel models have done very well in this area; the work of Johnson and Charniak (2004) explores such an approach. This approach performs very well when compared with other approaches. Johnson et al. (2004) adds some handwritten rules to the noisy channel model, providing the current state of the art in disfluency detection. Lease and Johnson (2006) also use this approach, but they are particularly interested in finding fillers; they use early filler detection and deletion in this model. The following section describes the noisy channel approach in more detail. 3.2 The Noisy Channel Approach The approach we build on is that first introduced by Johnson and Charniak (Johnson and C"
U10-1015,N06-2019,1,0.850171,"re equally fluent. This makes it difficult to distinguish reparanda as being part of disfluencies when only lexical cues are available. Since the transcripts we work with do not have prosodic cues annotated, we need to look elsewhere for a solution to this problem. Noisy Channel models have done very well in this area; the work of Johnson and Charniak (2004) explores such an approach. This approach performs very well when compared with other approaches. Johnson et al. (2004) adds some handwritten rules to the noisy channel model, providing the current state of the art in disfluency detection. Lease and Johnson (2006) also use this approach, but they are particularly interested in finding fillers; they use early filler detection and deletion in this model. The following section describes the noisy channel approach in more detail. 3.2 The Noisy Channel Approach The approach we build on is that first introduced by Johnson and Charniak (Johnson and Charniak, 2004). This approach is modular by nature, making it possible to interchange different sub-components. The original paper explores the use of different types of language models, and demonstrates how some models provide better overall performance than othe"
U10-1015,J10-1001,0,0.01946,"nd end positions of each of these three components. We can think of each word in an utterance as belonging to one of four categories: fluent material, reparandum, filler, or repair. We can then assess the accuracy of techniques that attempt to detect disfluencies by computing precision and recall values for the assignment of the correct categories to each of the words in the utterance, as compared to the gold standard as indicated by annotations in the corpus. 3 3.1 Disfluency Detection Models Related Work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees which it considers to be made up of disfluent material. Although this is one of the few models that actually builds up a syntactic analysis of the utterance being analysed, its final F-score for fluency detection is lower than that of other models. Snover et al. (2004) investigate the use of purely lexical features combined with part-ofspeech tags to detect disfluencies. This approach is compared against approaches which use primaril"
U10-1015,C90-3045,0,0.0515901,"Missing"
U10-1015,N04-4040,0,0.0326869,"d by annotations in the corpus. 3 3.1 Disfluency Detection Models Related Work A number of different techniques have been proposed for automatic disfluency detection. Schuler et al. (2010) propose a Hierarchical Hidden Markov Model approach; this is a statistical approach which builds up a syntactic analysis of the sentence and marks those subtrees which it considers to be made up of disfluent material. Although this is one of the few models that actually builds up a syntactic analysis of the utterance being analysed, its final F-score for fluency detection is lower than that of other models. Snover et al. (2004) investigate the use of purely lexical features combined with part-ofspeech tags to detect disfluencies. This approach is compared against approaches which use primarily prosodic cues, and appears to perform equally well. However, the authors note that this model finds it difficult to identify disfluencies which by themselves are very fluent. The edit repairs which are the focus of our work typically have this characteristic: when a speaker edits her speech for meaningrelated reasons, rather than errors that arise from performance, the resulting disfluency can be by itself fluent. We can see t"
U10-1015,C10-1154,1,0.707346,"s have the advantage in terms of the amount of data available; so, for example, Brill and Banko (2001) argue that more data is more important than better algorithms for some natural language processing tasks. It is this insight that drives the work described in this paper. Our current approach to speech disfluency detection is trained on manually-constructed spoken language corpora which contain annotations of all disfluencies as part of the transcription process. Our model is based on the noisy channel model and consists of a language model and a channel model. As we have reported elsewhere (Zwarts et al., 2010), we are able to achieve reasonable results when using Switchboard data: we obtain an F-score of 0.757 in determining which constituents of an utterance belong to a disfluency. We would like to see if we can improve on our previously reported performance by adding more data. Our language model does not need any special annotation, and so our first set of experiments investigates whether we can improve results by vastly increasing the training data for the language model. The task of increasing the training data for the channel model is a more difficult one, since here we require the annotation"
U10-1015,W90-0102,0,\N,Missing
U11-1013,U11-1013,1,0.0512755,"Missing"
U11-1013,C90-3059,0,0.853246,"Missing"
U11-1013,W02-1503,0,0.334737,"Missing"
U11-1013,cahill-etal-2000-enabling,0,0.711189,"Missing"
U11-1013,W07-2303,0,0.328285,"Missing"
U11-1013,E89-1018,0,0.73144,"Missing"
U11-1013,C92-3158,0,0.687291,"Missing"
U11-1013,W00-1403,0,0.695306,"involved in building a new system from scratch (Bateman et al., 1999). Such claims have been made since the very first MNLG systems; the FoG system generating weather forecasts in English and French (Bourbeau et al., 1990) is a case in point. Consequently, MNLG has been applied for a large number of text types: government statistics reports (Iordanskaja et al., 1992), technical instruction manuals (Paris et al., 1995), fairy tales (Callaway and Lester, 2002), museum tours (Callaway et al., 2005), medical terminology (Rassinoux et al., 2007), codes of practice (Evans et al., 2008), and so on. Marcu et al. (2000), in reviewing some of the earlier work, comment that MNLG systems need to abstract as much as possible away from the individual language generated: If an [MNLG] system needs to develop language dependent knowledge bases, and language dependent algorithms for content selection, text planning, and sentence planning, it is difficult to justify its economic viability. However, if most of these components are language independent and/or much of the code can be reused, an [MNLG] system becomes a viable option. Bateman et al. (1999) similarly emphasise the importance of reducing language dependence."
U11-1013,C90-1021,0,\N,Missing
U11-1013,J93-4001,0,\N,Missing
viethen-etal-2008-controlling,W06-1410,1,\N,Missing
viethen-etal-2008-controlling,W07-2307,0,\N,Missing
viethen-etal-2008-controlling,W07-2318,1,\N,Missing
viethen-etal-2008-controlling,J03-1003,1,\N,Missing
viethen-etal-2008-controlling,2007.mtsummit-ucnlg.14,0,\N,Missing
viethen-etal-2010-dialogue,passonneau-2006-measuring,0,\N,Missing
viethen-etal-2010-dialogue,W06-1410,1,\N,Missing
viethen-etal-2010-dialogue,W08-1109,1,\N,Missing
viethen-etal-2010-dialogue,P00-1024,0,\N,Missing
viethen-etal-2010-dialogue,W09-0629,0,\N,Missing
viethen-etal-2010-dialogue,P02-1040,0,\N,Missing
viethen-etal-2010-dialogue,P83-1007,0,\N,Missing
viethen-etal-2010-dialogue,P89-1009,1,\N,Missing
viethen-etal-2010-dialogue,J86-3001,0,\N,Missing
W02-0104,J81-4005,0,0.501411,"Missing"
W02-0104,J00-4006,0,0.0120649,"Missing"
W02-1112,W97-1301,0,\N,Missing
W02-1112,C92-2082,0,\N,Missing
W02-1112,W01-0703,0,\N,Missing
W02-1112,A97-1011,0,\N,Missing
W03-1202,P00-1041,0,0.0570253,"vation behind discovering segments in a text is that a sentence extraction summary should choose the most representative sentence for each segment, resulting in a comprehensive summary. In the view of Gong and Liu (2001), segments form the main themes of a document. They present a theme interpretation of the SVD analysis, as it is used for discourse segmentation, upon which our use of the technique is based. However, Gong and Liu use SVD for creating sentence extraction summaries, not for generating a single sentence summary by re-using words. In subsequent work to Witbrock and Mittal (1999), Banko et al. (2000) describe the use of information about the position of words within four quarters of the source document. The headline candidacy score of a word is weighted by its position in one of quarters. We interpret this use of position information as a means of guiding the generation of a headline towards the central theme of the document, which for news articles typically occurs in the first quarter. SVD potentially offers a more general mechanism for handling the discovery of the central themes and their positions within the document. Jin et al. (2002) have also examined a statistical model for headl"
W03-1202,W97-0704,0,0.072912,"g abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction 1 Theme is a term that is used in many ways by many researchers, and generally without any kind of formal definition. Our use of the term here is akin to the notion that underlies work on text segmentation, where sentences naturally cluster in terms of their ‘aboutness’. and natural language processing. Hybrid methods for abstract-like summarisation which combine statistical and symbolic approaches have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summarisation has been explored by a number of researchers (see for example, Witbrock and Mittal, 1999; Zajic et al., 2002). We build on the approach employed by Witbrock and Mittal (1999) which we will describe in more detail in Section 3. Interestingly, in the work of Witbrock and Mittal (1999), the selection of words for inclusion in the headline is decided solely on the basis of corpus statistics and does not use statistical information about the distribution of words in the document itself. Our work differs in that we utilise an SVD analysis to provide inform"
W03-1202,J00-2011,0,0.557111,"th probabilities, directing the search towards the more probable word sequences first. The use of repeated words in the path is not permitted. 5.2 Using Singular Value Decomposition for Content Selection As an alternative to the Conditional probability, we examine the use of SVD in determining the Content Selection probability. Before we outline the procedure for basing this probability on SVD, we will first outline our interpretation of the SVD analysis, based on that of Gong and Liu (2001). Our description is not intended to be a comprehensive explanation of SVD, and we direct the reader to Manning and Schütze (2000) for a description of how SVD is used in information retrieval. Conceptually, when used to analyse documents, SVD can discover relationships between word co-occurrences in a collection of text. For example, in the context of information retrieval, this provides one way to retrieve additional documents that contain synonyms of query terms, where synonymy is defined by similarity of word co-occurrences. By discovering patterns in word co-occurrences, SVD also provides information that can be used to cluster documents based on similarity of themes. In the context of single document summarisation,"
W03-1202,J98-3005,0,0.0911491,"erated headline using SVD: “singapore shares fall” Figure 2. The headline generated using an SVDbased word selection criterion. The movement in share price is correct. 4 Related Work As the focus of this paper is on statistical singlesentence summarisation we will not focus on preceding work which generates summaries greater in length than a sentence. We direct the reader to Paice (1990) for an overview of summarisation based on sentence extraction. Examples of recent systems include Kupiec et al. (1995) and Brandow et al. (1995). For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction 1 Theme is a term that is used in many ways by many researchers, and generally without any kind of formal definition. Our use of the term here is akin to the notion that underlies work on text segmentation, where sentences naturally cluster in terms of their ‘aboutness’. and natural language processing. Hybrid methods for abstract-like summarisation which combine statistical and symbolic approaches have also been explored; see, for example, McKeown et al. (1999), Jing and McKeown (1999), and Hovy and Lin (1997). Statistical single sentence summar"
W03-1202,X98-1026,0,\N,Missing
W05-1628,C00-1007,0,0.0816988,"Missing"
W05-1628,P99-1071,0,0.0814331,"Missing"
W05-1628,P96-1025,0,0.137913,"Missing"
W05-1628,W04-3216,0,0.0395668,"Missing"
W05-1628,W03-1202,1,0.866597,"Missing"
W06-1410,P97-1027,0,0.487627,"Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms The natural language generation lite"
W06-1410,J03-1003,0,0.71443,"Missing"
W06-1410,P99-1017,0,0.0459586,"orithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms The natural language generation literature provides"
W06-1410,C92-1038,1,0.874894,"hms like FB, 1 The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natural Language Generation Conference, pages 63–70, c Sydney, July 2006. 2006 Association for Computational Linguistics presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. There are significant differences between the referring expressions produced by humans, and those produced by the algorithms; we explore these differences and consider what it means for work in the generation of referring expressions. The remainder of this paper is structured as follows. In Section 2, we introduce the data set of human-produced referring expressions we use; in Section 3, we introduce the representational framework we use to model the domain underlying this data; in Section 4 we introduce the three algorithms c"
W06-1410,E91-1028,1,0.841292,"what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, 1 The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al., 2004; Gatt, 2006), or with multimodal referring expressions where the linguistic part is not necessarily distinguishing by itself (van der Sluis and Krahmer, 2004). 63 Proceedings of the Fourth International Natur"
W06-1410,W00-1416,0,0.280683,"mter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Some approaches reuse parts of other algorithms: the Branch and Bound algorithm (Krahmer et al., 2003) uses the Full Brevity algorithm, but is able to generate referring expressions with both attributes and relational descriptions using a graph-based technique. There are many other algorithms described in the literature: see, for example, (Horacek, 1997; Bateman, 1999; Stone, 2000; Gardent, 2002). Their general aim is to produce naturalistic referring expressions, often explicitly by means of an attempt to follow the same kinds of principles that we believe people might be following when they produce language — such as the Gricean maxims (Grice, 1975). However, the algorithms have rarely been tested against real data from human referring expression generation.1 In this paper, we present a data set containing human-produced referring expressions in a limited domain. Focussing specifically on the algorithms The natural language generation literature provides many algorit"
W06-1410,W01-0804,0,0.190369,"Missing"
W06-1410,P89-1009,1,0.785895,"these observations, we suggest some ways forward that attempt to address these differences. 1 Introduction The generation of referring expressions (henceforth GRE) — that is, the process of working out what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, 1 The only exceptions we know of to this deficit are not directly concerned with the kinds of properties people select, but with phenomena such as how people group entities together (Funakoshi et al.,"
W06-1410,J02-1003,0,0.712664,"Missing"
W06-1410,van-der-sluis-krahmer-2004-evaluating,0,0.0406428,"Missing"
W06-1410,H89-1033,0,0.0611262,"ify a number of significant differences between what people do and what these algorithms do. On the basis of these observations, we suggest some ways forward that attempt to address these differences. 1 Introduction The generation of referring expressions (henceforth GRE) — that is, the process of working out what properties of an entity should be used to describe it in such a way as to distinguish it from other entities in the context — is a recurrent theme in the natural language generation literature. The task is discussed informally in some of the earliest work on NLG (in particular, see (Winograd, 1972; McDonald, 1980; Appelt, 1981)), but the first formally explicit algorithm was introduced in (Dale, 1989); this algorithm, often referred to as the Full Brevity (FB) algorithm, has served as a starting point for many subsequent GRE algorithms. To overcome its limitation to one-place predicates, Dale and Haddock (1991) introduced a constraint-based procedure that could generate referring expressions involving relations; and as a response to the computational complexity of ‘greedy’ algorithms like FB, 1 The only exceptions we know of to this deficit are not directly concerned with the kinds of"
W06-1410,P02-1013,0,\N,Missing
W06-1410,E06-1041,0,\N,Missing
W07-2318,E91-1028,1,0.622395,"le referring expressions exist. At the same time, it is not the case that all logically possible descriptions are acceptable; so, if we remove the requirement to produce only one best solution, how do we avoid generating undesirable descriptions? Our aim in this paper is to sketch a framework that allows us to capture constraints on referring expression generation, so that the set of logically possible descriptions can be reduced to just those that are acceptable. 1 Introduction The literature contains many algorithms for the generation of referring expressions: see, for example, (Dale, 1989; Dale and Haddock, 1991; Gardent, 2002; Varges and van Deemter, 2005; Gatt, 2006). These algorithms generally attempt to produce a single ‘best’ referring expression for a given intended referent. What counts as ‘best’ is generally defined in terms of minimality and the redundancy of information: the best referring expression is the shortest possible distinguishing description, usually defined in terms of the number of properties expressed. At the same time, some researchers (for example, (Dale and Reiter, 1995; Krahmer et al., 2003)) have noted that humanproduced referring expressions are often not minimal in this"
W07-2318,P02-1013,0,0.0309034,"exist. At the same time, it is not the case that all logically possible descriptions are acceptable; so, if we remove the requirement to produce only one best solution, how do we avoid generating undesirable descriptions? Our aim in this paper is to sketch a framework that allows us to capture constraints on referring expression generation, so that the set of logically possible descriptions can be reduced to just those that are acceptable. 1 Introduction The literature contains many algorithms for the generation of referring expressions: see, for example, (Dale, 1989; Dale and Haddock, 1991; Gardent, 2002; Varges and van Deemter, 2005; Gatt, 2006). These algorithms generally attempt to produce a single ‘best’ referring expression for a given intended referent. What counts as ‘best’ is generally defined in terms of minimality and the redundancy of information: the best referring expression is the shortest possible distinguishing description, usually defined in terms of the number of properties expressed. At the same time, some researchers (for example, (Dale and Reiter, 1995; Krahmer et al., 2003)) have noted that humanproduced referring expressions are often not minimal in this sense, and so v"
W07-2318,J03-1003,0,0.301969,"Missing"
W07-2318,W06-1410,1,0.879741,"time, some researchers (for example, (Dale and Reiter, 1995; Krahmer et al., 2003)) have noted that humanproduced referring expressions are often not minimal in this sense, and so variations on these algorithms weaken this requirement, while still tending to embody a ‘shorter is better’ criterion. This focus on minimality has the consequence that it allows us to ignore the abundant evidence that any intended referent can be successfully and appropriately referred to by a large number of referring expressions, many of which involve some 113 redundancy; see, for example, the data described in (Viethen and Dale, 2006). Once we remove the requirement of minimality, and admit that there are many possible acceptable solutions to the problem of how to refer to an intended referent, we are faced with a new problem: for any given entity, there are many logically possible distinguishing descriptions, and we need some way to navigate this space of possibilities, so that we can at least separate the acceptable from the less acceptable. This paper attempts to establish a framework for thinking about this problem. In Section 2, we begin by first discussing the question of domain-specificity; our argument here is that"
W08-1109,P89-1009,1,0.812437,"hallenge used the TUNA corpus (Gatt et al., 2007), which is the most extensive collection of referring expressions to date. While there is a substantial body of experimental work in psycholinguistics that looks at the human production of referring expressions (see, amongst more recent work, (Clark and Wilkes-Gibbs, 1986; Stevenson, 2002; Haywood et al., 2003; Jordan and Walker, 2005)) the large range of factors that play a role in language production 2 Spatial Relations in Referring Expression Generation The bulk of the existing literature on referring expression generation (see, for example, Dale (1989), Dale and Reiter (1995), van Deemter (2006), Horacek (2004), Gatt and van Deemter (2006)) generally focuses on the use of non-relational properties, which can either be absolute (for example, colour) or relative (for example, size). We are interested in the 59 of Sensitivity, as well as van der Sluis and Krahmer’s (2004) production study, to motivate the ordering over the types of properties that can be used by their system; accordingly, their system only includes spatial (and hence relational) information in a referring expression if it is not possible to construct a description from non-rel"
W08-1109,C94-2182,0,0.615787,"Missing"
W08-1109,P02-1013,0,0.014871,"re data that will inform the development of algorithms, either by automatically checking their ability to replicate the corpus, or as a baseline for assessing the performance of humans in an identification task based on the output of these algorithms. In this paper, we describe an experiment that looks at how and when people use spatial relations in a simple scene. More specifically, we aim to explore the hypothesis that relations are always dispreferred over non-relational properties. This hypothesis appears to underly most approaches to referring expression generation that handle relations: Gardent (2002) adopts a constraint based approach to deal with relations specifically geared at generating referring expressions that are as short as possible. As including a relation in a referring expression always entails the additional mention of at least a head noun for the related object, this approach inherently prefers properties over relations. Krahmer and Theune (2002) extend the Incremental Algorithm (IA; Dale and Reiter (1995)) to handle relations. This requires a preference list over all properties and relations to be specified in advance. They explicitly choose to put spatial relations right a"
W08-1109,P06-2033,0,0.0637477,"Missing"
W08-1109,W07-2307,0,0.150866,"Missing"
W08-1109,W07-2302,0,0.0455447,"s to test this assumption; we determine that, even in simple scenes where the use of relations is not strictly required in order to identify an entity, relations are in fact often used. We draw some conclusions as to what this means for the development of algorithms for the generation of referring expressions. 1 Introduction In recent years, researchers working on referring expression generation have increasingly moved towards collecting their own data on the human production of referring expressions (REs) (Krahmer and Theune, 2002; van der Sluis and Krahmer, 2004; Gatt and van Deemter, 2006; Belz and Varges, 2007); and the recent Attribute Selection in the Generation of Referring Expressions (ASGRE) Challenge used the TUNA corpus (Gatt et al., 2007), which is the most extensive collection of referring expressions to date. While there is a substantial body of experimental work in psycholinguistics that looks at the human production of referring expressions (see, amongst more recent work, (Clark and Wilkes-Gibbs, 1986; Stevenson, 2002; Haywood et al., 2003; Jordan and Walker, 2005)) the large range of factors that play a role in language production 2 Spatial Relations in Referring Expression Generation T"
W08-1109,H93-1005,0,0.0204357,"ta (Viethen and Dale, 2006) contain too few relational descriptions to allow us to draw conclusions about any kind of patterns; the GREC corpus (Belz and Varges, 2007) is not concerned with content selection at all, but rather studies the form of referring expressions used over a whole text; i.e. the choice between fully descriptive NPs, reduced NPs, one-anaphora and pronouns. There are a number of corpora resulting from experiments involving human participants which contain referring expressions, such as Brennan and Clark’s (1996) collection of tangram descriptions, the HCRC Map Task Corpus (Thompson et al., 1993), the COCONUT corpus (Jordan and Walker, 2005), and Byron and Fosler-Lussier’s (2006) OSU Quake corpus. However, these contain whole conversations between communicative partners cooperating on a task, making it difficult to factor out the impact of prior discourse context on the referring expressions used. use of relational expressions, and in particular the use of spatial relations; the contexts of use we are interested in are task-specific, where, for example, we might want an omniscient domestic agent to tell us where we have placed a lost object (You left your keys under the folder on the"
W08-1109,J06-2002,0,0.0625899,"Missing"
W08-1109,W05-1627,0,0.161511,"Missing"
W08-1109,W06-1410,1,0.467089,"it is not possible to construct a description from non-relational properties. These approaches would appear to favour the production of referring expressions containing long sequences of non-relational properties when a single relational property might do the job. We are interested, then, in whether it really is the case that relational expressions are dispreferred, and in determining when they might in fact be preferred. To date, we are not aware of any substantial data sets that would allow this question to be explored. Both the TUNA corpus (Gatt et al., 2007) and the Macquarie Drawer data (Viethen and Dale, 2006) contain too few relational descriptions to allow us to draw conclusions about any kind of patterns; the GREC corpus (Belz and Varges, 2007) is not concerned with content selection at all, but rather studies the form of referring expressions used over a whole text; i.e. the choice between fully descriptive NPs, reduced NPs, one-anaphora and pronouns. There are a number of corpora resulting from experiments involving human participants which contain referring expressions, such as Brennan and Clark’s (1996) collection of tangram descriptions, the HCRC Map Task Corpus (Thompson et al., 1993), the"
W08-1109,P06-1131,0,\N,Missing
W08-1109,byron-fosler-lussier-2006-osu,0,\N,Missing
W09-0609,W08-1133,0,0.0443567,"Missing"
W09-0609,W08-1131,0,0.0566367,"opose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 1 Introduction The last few years have witnessed a considerable move towards empiricism in referring expression generation; this is evidenced both by the growing body of work that analyses and tries to replicate the content of corpora of human-produced referring expressions, and particularly by the significant participation in the TUNA and GREC challenge tasks built around such activities (see, for example, (Belz and Gatt, 2007; Belz et al., 2008; Gatt et al., 2008)). One increasingly widespread observation—obvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we revisit the corpus of"
W09-0609,W08-1134,0,0.0574449,"Missing"
W09-0609,W08-1137,0,0.0168673,"rned with determining the content of referring expressions in terms of the attributes used to build a distinguishing description. In particular, Fabbrizio et al. (2008) explore the impact of individual style and priming on attribute selection for referring expression generation, and Bohnet (2008) uses a nearestneighbour learning technique to acquire an individual referring expression generation model for each person. Other related approaches to attribute selection in the context of the TUNA task are explored in (Gerv´as et al., 2008; de Lucena and Paraboni, 2008; Kelleher and Mac Namee, 2008; King, 2008). supports, and some elements of the rules may be due to artefacts of the specific stimuli used in the data gathering. We would require a more diverse set of stimuli to determine whether this is the case, but the basic point stands: we can find correlations between characteristics of the scenes and the presence or absence of particular attributes in referring expressions, even if we cannot predict so well the particular combinations of these correlations that a given speaker will use in a given situation. 5 Related Work There is a significant body of work on the use of machine learning in refe"
W09-0609,2007.mtsummit-ucnlg.14,0,0.0395708,"significant degree. This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 1 Introduction The last few years have witnessed a considerable move towards empiricism in referring expression generation; this is evidenced both by the growing body of work that analyses and tries to replicate the content of corpora of human-produced referring expressions, and particularly by the significant participation in the TUNA and GREC challenge tasks built around such activities (see, for example, (Belz and Gatt, 2007; Belz et al., 2008; Gatt et al., 2008)). One increasingly widespread observation—obvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers."
W09-0609,W08-1127,1,0.722012,"This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 1 Introduction The last few years have witnessed a considerable move towards empiricism in referring expression generation; this is evidenced both by the growing body of work that analyses and tries to replicate the content of corpora of human-produced referring expressions, and particularly by the significant participation in the TUNA and GREC challenge tasks built around such activities (see, for example, (Belz and Gatt, 2007; Belz et al., 2008; Gatt et al., 2008)). One increasingly widespread observation—obvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we r"
W09-0609,W06-1412,0,0.0530509,"cular combinations of these correlations that a given speaker will use in a given situation. 5 Related Work There is a significant body of work on the use of machine learning in referring expression generation, although typically focussed on aspects of the problem that are distinct from those considered here. In the context of museum item descriptions, Poesio et al. (1999) explore the decision of what type of referring expression NP to use to refer to a given discourse entity, using a statistical model to choose between using a proper name, a definite description, or a pronoun. More recently, Stoia et al. (2006) attempt a similar task, but this time in an interactive navigational domain; as well as determining what type of referring expression to use, they also try to learn whether a modifier should be included. Cheng et al. (2001) try to learn rules for the incorporation of non-referring modifiers into noun phrases. A number of the contributions to the 2008 GREC 6 Conclusions We know that people’s referential behaviour varies significantly. Despite this apparent variation, we have demonstrated above that there does appear to be a reasonable correlation between characteristics of the scene and the in"
W09-0609,W08-1132,0,0.0639796,"chine learning techniques. The GREC task is primarily concerned with the choice of form of reference (i.e. whether a proper name, a descriptive NP or a pronoun should be used), and so is less relevant to the focus of the present paper. Much of the work on the TUNA Task is relevant, however, since this also is concerned with determining the content of referring expressions in terms of the attributes used to build a distinguishing description. In particular, Fabbrizio et al. (2008) explore the impact of individual style and priming on attribute selection for referring expression generation, and Bohnet (2008) uses a nearestneighbour learning technique to acquire an individual referring expression generation model for each person. Other related approaches to attribute selection in the context of the TUNA task are explored in (Gerv´as et al., 2008; de Lucena and Paraboni, 2008; Kelleher and Mac Namee, 2008; King, 2008). supports, and some elements of the rules may be due to artefacts of the specific stimuli used in the data gathering. We would require a more diverse set of stimuli to determine whether this is the case, but the basic point stands: we can find correlations between characteristics of t"
W09-0609,W08-1109,1,0.939941,"n’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we revisit the corpus of data that was introduced and discussed in (Viethen 2 2.1 The Corpus General Overview The corpus we use was collected via a data gathering experiment described in (Viethen and Dale, 2008a; Viethen and Dale, 2008b). The purpose of the data gathering was to gain some insight into how human subjects use relational referring expressions, a relatively unexplored aspect of referring expression generation. Participants visited a website, where they first saw an introductory page with a set of simple instructions and a sample stimulus scene consisting of three objects. Each participant was then assigned one of two trial sets of ten scenes each; the two trial sets are superficially Proceedings of the 12th European Workshop on Natural Language Generation, pages 58–65, c Athens, Greece,"
W09-0609,N01-1002,0,0.0306052,"cally focussed on aspects of the problem that are distinct from those considered here. In the context of museum item descriptions, Poesio et al. (1999) explore the decision of what type of referring expression NP to use to refer to a given discourse entity, using a statistical model to choose between using a proper name, a definite description, or a pronoun. More recently, Stoia et al. (2006) attempt a similar task, but this time in an interactive navigational domain; as well as determining what type of referring expression to use, they also try to learn whether a modifier should be included. Cheng et al. (2001) try to learn rules for the incorporation of non-referring modifiers into noun phrases. A number of the contributions to the 2008 GREC 6 Conclusions We know that people’s referential behaviour varies significantly. Despite this apparent variation, we have demonstrated above that there does appear to be a reasonable correlation between characteristics of the scene and the incorporation of particular attributes in a referring expression. One way to conceptualise this is that the decision as to whether or 64 not to incorporate a given feature such as colour or size may vary from speaker to speake"
W09-0609,viethen-etal-2008-controlling,1,0.877721,"Missing"
W09-0609,W08-1135,0,0.0594523,"Missing"
W09-0609,W09-0629,0,\N,Missing
W09-0628,P08-2050,0,0.0293624,"ure of the task imposes high demands on the system’s efficiency). But if Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it dif166 extended to two-way dialog, the task can also involve issues of prosody generation (i.e., research on text/concept-to-speech generation), discour"
W09-0628,W08-1113,0,0.0343662,"he system’s efficiency). But if Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it dif166 extended to two-way dialog, the task can also involve issues of prosody generation (i.e., research on text/concept-to-speech generation), discourse generation, and human-robot inte"
W09-0628,E09-2009,1,0.68788,"onnects to the Matchmaker and is randomly assigned an NLG server and a game world. The client and NLG server then communicate over the course of one game. At the end of the game, the client displays a questionnaire to the user, and the game log and questionnaire data are uploaded to the Matchmaker and stored in a database. Note that this division allows the challenge to be conducted without making any assumptions about the internal structure of an NLG system. The GIVE software is implemented in Java and available as an open-source Google Code project. For more details about the software, see (Koller et al., 2009). 3.2 3.3 Materials Figs. 3–5 show the layout of the three evaluation worlds. The worlds were intended to provide varying levels of difficulty for the direction-giving systems and to focus on different aspects of the problem. World 1 is very similar to the development world that the research teams were given to test their system on. World 2 was intended to focus on object descriptions - the world has only one room which is full of objects and buttons, many of which cannot be distinguished by simple descriptions. World 3, on the other hand, puts more emphasis on navigation directions as the wor"
W09-0628,W06-1412,1,0.744385,"appeal to younger students, the task can also be used as a pedagogical exercise to stimulate interest among secondary-school students in the research challenges found in NLG or Computational Linguistics more broadly. Embedding the NLG task in a virtual world encourages the participating research teams to consider communication in a situated setting. This makes the NLG task quite different than in other NLG challenges. For example, experiments have shown that human instruction givers make the instruction follower move to a different location in order to use a simpler referring expression (RE) (Stoia et al., 2006). That is, RE generation becomes a very different problem than the classical non-situated Dale & Reiter style RE generation, which focuses on generating REs that are single noun phrases in the context of an unchanging world. On the other hand, because the virtual environments scenario is so open-ended, it – and specifically the instruction-giving task – can potentially be of interest to a wide range of NLG researchers. This is most obvious for research in sentence planning (GRE, aggregation, lexical choice) and realization (the real-time nature of the task imposes high demands on the system’s"
W09-3606,P08-2033,1,0.801696,"nt for the cited document: Participants indicated that this was useful in appraising the cited article. These pieces of information were commonly identified as useful in helping readers make value judgements about the cited work. This is perhaps an artifact of the biomedical domain, where research has a critical nature and concerns health and medical issues. 5.2 A Contextualised Preview To generate the contextualised preview of the cited document, the system finds the set of sentences that relate to the citation context, employing approaches for summarising documents that exploit anchor text (Wan and Paris, 2008). Following Spark Jones (1998), we specify the purpose of the contextualised summary along particular dimensions, indicated here in italics: • The situation is tied to a particular context of use: an in-browser summary triggered by a citation and its citing context. • An audience of expert researchers is assumed. • The intended usage of the summary is one of preview. We assume that the reader is making a relevance judgement as to whether or not to download (and, if necessary, buy) the cited document. Specifically, the information presented should help the reader determine the level of trust to"
W09-3606,C08-1087,0,0.0172234,"al. (1995); in particular, our work focuses on browsing scenarios. Toms (2000) presents a study of browsing behaviour over electronic texts and examines the differences between searching and The genre of academic literature, and the development of technologies to support researchers as users, has been studied by several groups working in automatic text summarisation. Teufel and Moens (2002) describe a summarisation approach that extracts text from documents and highlights the rhetorical role that an extract plays within the originating document (for example, stating the Aim of an experiment). Qazvinian and Radev (2008) present an approach to summarising academic documents based on finding citation contexts in the entire set of published literature for the document in question. Both approaches, however, treat the cited document in isolation of the read46 ing context and do not actively support the reading task. 3 their own reasons. In addition, we also asked them about the frequency of their literature browsing activity. The main section of the questionnaire consisted of a series of questions, corresponding to the issues we wanted to explore: Understanding How Researchers Browse through Scientific Literature"
W09-3606,J02-4002,0,0.0152721,"constructing lists of domain-specific key words which may correspond well to user interests. However, we are interested in relating information needs to user tasks in scenarios in which there is no explicit query, as in Bystrm et al. (1995); in particular, our work focuses on browsing scenarios. Toms (2000) presents a study of browsing behaviour over electronic texts and examines the differences between searching and The genre of academic literature, and the development of technologies to support researchers as users, has been studied by several groups working in automatic text summarisation. Teufel and Moens (2002) describe a summarisation approach that extracts text from documents and highlights the rhetorical role that an extract plays within the originating document (for example, stating the Aim of an experiment). Qazvinian and Radev (2008) present an approach to summarising academic documents based on finding citation contexts in the entire set of published literature for the document in question. Both approaches, however, treat the cited document in isolation of the read46 ing context and do not actively support the reading task. 3 their own reasons. In addition, we also asked them about the freque"
W10-4233,gargett-etal-2010-give,1,0.375374,"Missing"
W10-4233,E09-2009,1,0.722559,"in the virtual world. This is in contrast to GIVE-1, where players could only turn by 90 degree increments, and jump forward and backward by discrete steps. This feature of the way the game controls were set Method Following the approach from the GIVE-1 Challenge (Koller et al., 2010), we connected the NLG systems to users over the Internet. In each game run, one user and one NLG system were paired up, with the system trying to guide the user to success in a specific game world. 3.1 Software infrastructure We adapted the GIVE-1 software to the GIVE-2 setting. The GIVE software infrastructure (Koller et al., 2009a) consists of three different modules: The client, which is the program which the user runs on their machine to interact with the virtual world (see Fig. 1); a collection of NLG servers, which generate instructions in real-time and send them to the client; and a matchmaker, which chooses a random NLG server and virtual world for each incoming connection from a client and stores the game results in a database. The most visible change compared to GIVE-1 was to modify the client so it permitted free movement in the virtual world. This change further necessitated a number of modifications to the"
W10-4233,P09-2076,1,0.840837,"in the virtual world. This is in contrast to GIVE-1, where players could only turn by 90 degree increments, and jump forward and backward by discrete steps. This feature of the way the game controls were set Method Following the approach from the GIVE-1 Challenge (Koller et al., 2010), we connected the NLG systems to users over the Internet. In each game run, one user and one NLG system were paired up, with the system trying to guide the user to success in a specific game world. 3.1 Software infrastructure We adapted the GIVE-1 software to the GIVE-2 setting. The GIVE software infrastructure (Koller et al., 2009a) consists of three different modules: The client, which is the program which the user runs on their machine to interact with the virtual world (see Fig. 1); a collection of NLG servers, which generate instructions in real-time and send them to the client; and a matchmaker, which chooses a random NLG server and virtual world for each incoming connection from a client and stores the game results in a database. The most visible change compared to GIVE-1 was to modify the client so it permitted free movement in the virtual world. This change further necessitated a number of modifications to the"
W10-4233,W11-2830,0,\N,Missing
W10-4233,W11-2848,0,\N,Missing
W10-4233,W11-2846,0,\N,Missing
W10-4233,W11-2849,0,\N,Missing
W10-4233,W11-2847,0,\N,Missing
W10-4233,W11-2851,1,\N,Missing
W10-4233,W11-2852,0,\N,Missing
W10-4233,W11-2850,0,\N,Missing
W10-4236,bird-etal-2008-acl,1,0.781642,"aring their conference and journal submissions. They will have the skills and motivation to integrate the use of prototypes into their paper-writing. 2 See the Microsoft ESL Assistant at http://www.eslassistant.com as an embodiment of a similar idea. 2.2 The ACL Anthology Over a number of years, the ACL has sponsored the ongoing development of the ACL Anthology, a large collection of papers in the domain of computational linguistics. This provides an excellent source for the construction of language models for the task described here. The more recently-prepared ACL Anthology Reference Corpus (Bird et al., 2008), in which 10,921 of the Anthology texts (around 40 million words) have been made available in plain text form, has also been made accessible via the Sketch Engine, a leading corpus query tool.3 The corpus is not perfect, of course: not everything in the ACL Anthology is written in flawless English; the ARC was prepared in 2007, so new topics, vocabulary and ideas in CL will not be represented; and the fact that the texts have been automatically extracted from PDF files means that there are errors from the conversion process. 3 The Task in More Detail 3.1 How Do We Measure Quality? To be able"
W10-4236,C08-1109,0,0.0478084,"hould be judged on its research content, not on the author’s skills in English. This problem will surface in any discipline where authors are required to provide material in a language other than their mother tongue. However, as a discipline, computational linguistics holds a privileged position: as scientists, language (of different varieties) is our object of study, and as technologists, language tasks form our agenda. Many of the research problems we focus on could assist with writing problems. There is already existing work that addresses specific problems in this area (see, for example, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. Our proposal, then, is to initiate a shared task that attempts to tackle the problem head-on; we want to ‘help our own’ by developing tools which can help non-native speakers of English (NNSs) (and maybe some native ones) write academic English prose of the kind that helps a paper get accepted. The kinds of assistance we are concerned with here go beyond that which is provided by commonly-available spelling che"
W11-2702,2007.mtsummit-ucnlg.14,0,0.461978,"riting about. This act of referring to real-world entities is one of the central tasks in human language production. Of course, it is also central when a machine is charged with the task of generating natural language, which makes referring expression generation (REG) an important subtask in any natural language generation (NLG) system. Recent work in particular has concentrated on the development of algorithms concerned with the generation of context-free identifying descriptions of objects, as emphasised by three shared-task evaluation competitions (STECs) targeting this particular problem (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009). Referring expressions of this kind are often referred to as distinguishing descriptions. We are still far from a full understanding of how such descriptions should best be generated. Much work remains to be done before many issues, such as, for example, the generation of relational descriptions and over-specified descriptions or the number of the surrounding objects to be taken into account in visual settings, can be considered resolved. Although many authors have explicitly or implicitly acknowledged the importance of generating referring expressions t"
W11-2702,P89-1009,1,0.776251,"; Gatt et al., 2009). Referring expressions of this kind are often referred to as distinguishing descriptions. We are still far from a full understanding of how such descriptions should best be generated. Much work remains to be done before many issues, such as, for example, the generation of relational descriptions and over-specified descriptions or the number of the surrounding objects to be taken into account in visual settings, can be considered resolved. Although many authors have explicitly or implicitly acknowledged the importance of generating referring expressions that sound natural (Dale, 1989; Dale and Reiter, 1995; Gardent et al., 2004; Horacek, 2004; van der Sluis and Krahmer, 2004; Kelleher and Kruijff, 2006; Gatt, 2007; Gatt et al., 2007), much of the original work in REG was neither developed based on empirical evidence about 12 Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 12–22, c Edinburgh, Scotland, UK, July 31, 2011. 2011 Association for Computational Linguistics Figure 1: The screen showing the first stimulus scene. how humans refer, nor evaluated against humanproduced referring expressions. The REG STECs on the task of content determ"
W11-2702,P02-1013,0,0.0902941,"criptions for Objects in Visual Scenes Jette Viethen1,2 jette.viethen@mq.edu.au Robert Dale2 robert.dale@mq.edu.au 1 2 TiCC University of Tilburg Tilburg, The Netherlands Abstract It is therefore not surprising that REG has attracted a great deal of attention from the NLG community over the past three decades. A key factor that has led to the popularity of REG is the widespread agreement that the central task involved is content selection: choosing those attributes of a target referent that best distinguish it from other distractor entities around it (Dale and Reiter, 1995; van Deemter, 2000; Gardent, 2002; Krahmer et al., 2003; Horacek, 2003; van der Sluis, 2005; Kelleher and Kruijff, 2006; Gatt, 2007; Viethen and Dale, 2008). Recent years have seen a trend towards empirically motivated and more data-driven approaches in the field of referring expression generation (REG). Much of this work has focussed on initial reference to objects in visual scenes. While this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. To move this research forward, we require data sets bu"
W11-2702,W07-2307,0,0.430136,"Missing"
W11-2702,W08-1131,0,0.696787,"t of referring to real-world entities is one of the central tasks in human language production. Of course, it is also central when a machine is charged with the task of generating natural language, which makes referring expression generation (REG) an important subtask in any natural language generation (NLG) system. Recent work in particular has concentrated on the development of algorithms concerned with the generation of context-free identifying descriptions of objects, as emphasised by three shared-task evaluation competitions (STECs) targeting this particular problem (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009). Referring expressions of this kind are often referred to as distinguishing descriptions. We are still far from a full understanding of how such descriptions should best be generated. Much work remains to be done before many issues, such as, for example, the generation of relational descriptions and over-specified descriptions or the number of the surrounding objects to be taken into account in visual settings, can be considered resolved. Although many authors have explicitly or implicitly acknowledged the importance of generating referring expressions that sound natural ("
W11-2702,W09-0629,0,0.316059,"eal-world entities is one of the central tasks in human language production. Of course, it is also central when a machine is charged with the task of generating natural language, which makes referring expression generation (REG) an important subtask in any natural language generation (NLG) system. Recent work in particular has concentrated on the development of algorithms concerned with the generation of context-free identifying descriptions of objects, as emphasised by three shared-task evaluation competitions (STECs) targeting this particular problem (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009). Referring expressions of this kind are often referred to as distinguishing descriptions. We are still far from a full understanding of how such descriptions should best be generated. Much work remains to be done before many issues, such as, for example, the generation of relational descriptions and over-specified descriptions or the number of the surrounding objects to be taken into account in visual settings, can be considered resolved. Although many authors have explicitly or implicitly acknowledged the importance of generating referring expressions that sound natural (Dale, 1989; Dale and"
W11-2702,E03-1017,0,0.0235739,"s Jette Viethen1,2 jette.viethen@mq.edu.au Robert Dale2 robert.dale@mq.edu.au 1 2 TiCC University of Tilburg Tilburg, The Netherlands Abstract It is therefore not surprising that REG has attracted a great deal of attention from the NLG community over the past three decades. A key factor that has led to the popularity of REG is the widespread agreement that the central task involved is content selection: choosing those attributes of a target referent that best distinguish it from other distractor entities around it (Dale and Reiter, 1995; van Deemter, 2000; Gardent, 2002; Krahmer et al., 2003; Horacek, 2003; van der Sluis, 2005; Kelleher and Kruijff, 2006; Gatt, 2007; Viethen and Dale, 2008). Recent years have seen a trend towards empirically motivated and more data-driven approaches in the field of referring expression generation (REG). Much of this work has focussed on initial reference to objects in visual scenes. While this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. To move this research forward, we require data sets built around increasingly complex scene"
W11-2702,P06-1131,0,0.650397,"edu.au Robert Dale2 robert.dale@mq.edu.au 1 2 TiCC University of Tilburg Tilburg, The Netherlands Abstract It is therefore not surprising that REG has attracted a great deal of attention from the NLG community over the past three decades. A key factor that has led to the popularity of REG is the widespread agreement that the central task involved is content selection: choosing those attributes of a target referent that best distinguish it from other distractor entities around it (Dale and Reiter, 1995; van Deemter, 2000; Gardent, 2002; Krahmer et al., 2003; Horacek, 2003; van der Sluis, 2005; Kelleher and Kruijff, 2006; Gatt, 2007; Viethen and Dale, 2008). Recent years have seen a trend towards empirically motivated and more data-driven approaches in the field of referring expression generation (REG). Much of this work has focussed on initial reference to objects in visual scenes. While this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. To move this research forward, we require data sets built around increasingly complex scenes, and we need much larger data sets to accommoda"
W11-2702,J03-1003,0,0.363094,"Missing"
W11-2702,W00-1424,0,0.102919,"Missing"
W11-2702,J06-2002,0,0.0199454,"Missing"
W11-2702,van-der-sluis-krahmer-2004-evaluating,0,0.0583757,"Missing"
W11-2702,W08-1109,1,0.953472,"u 1 2 TiCC University of Tilburg Tilburg, The Netherlands Abstract It is therefore not surprising that REG has attracted a great deal of attention from the NLG community over the past three decades. A key factor that has led to the popularity of REG is the widespread agreement that the central task involved is content selection: choosing those attributes of a target referent that best distinguish it from other distractor entities around it (Dale and Reiter, 1995; van Deemter, 2000; Gardent, 2002; Krahmer et al., 2003; Horacek, 2003; van der Sluis, 2005; Kelleher and Kruijff, 2006; Gatt, 2007; Viethen and Dale, 2008). Recent years have seen a trend towards empirically motivated and more data-driven approaches in the field of referring expression generation (REG). Much of this work has focussed on initial reference to objects in visual scenes. While this scenario of use is one of the strongest contenders for real-world applications of referring expression generation, existing data sets still only embody very simple stimulus scenes. To move this research forward, we require data sets built around increasingly complex scenes, and we need much larger data sets to accommodate their higher dimensionality. To co"
W11-2702,viethen-etal-2010-dialogue,1,0.869936,"on empirical evidence about 12 Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 12–22, c Edinburgh, Scotland, UK, July 31, 2011. 2011 Association for Computational Linguistics Figure 1: The screen showing the first stimulus scene. how humans refer, nor evaluated against humanproduced referring expressions. The REG STECs on the task of content determination form part of a recent trend towards more data-oriented development and evaluation of REG algorithms that responds directly to this concern (Gupta and Stent, 2005; Jordan and Walker, 2005; Gatt et al., 2007; Viethen et al., 2010; Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009). However, the existing data sets used in these experiments involve very simple and usually abstract visual displays of objects rather than coherent scenes. This is a reasonable starting point for bootstrapping research; but if we want to develop algorithms that can be used in real-world scenarios, we ultimately need to work with scenes which are much more realistic. At the same time, given the non-deterministic nature of choice in the production of natural language, corpora based on these scenes need to be very large, and should idea"
W11-2806,E91-1028,1,0.537223,"it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available corpora of human-produced referring expressions collected under controlled circumstances: these include the TUNA Corpus (van der Sluis et al., 2006), the Drawer Corpus (Viethen and Dale, 2"
W11-2806,P89-1009,1,0.668718,"generation are based on the idea of distinguishing the intended referent from the other entities in the context (Dale and Reiter, 1995; Gardent, 2002; Krahmer and Theune, 2002; Krahmer et al., 2003; Gatt and van Deemter, 2006). The task is generally characterised as involving the construction of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in t"
W11-2806,P02-1013,0,0.0251141,"bt on the significance of this aspect of reference. In the present paper, we look at the role of visual context in determining the content of a referring expression, and come to the conclusion that, at least in the referential scenarios underlying our data, visual context appears not to be a major factor in content determination for reference. We discuss the implications of this surprising finding. 1 Introduction Traditional approaches to referring expression generation are based on the idea of distinguishing the intended referent from the other entities in the context (Dale and Reiter, 1995; Gardent, 2002; Krahmer and Theune, 2002; Krahmer et al., 2003; Gatt and van Deemter, 2006). The task is generally characterised as involving the construction of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of d"
W11-2806,P06-2033,0,0.0254427,"Missing"
W11-2806,W08-1131,0,0.357746,"ving the construction of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available corpora of human-pr"
W11-2806,W09-0629,0,0.101574,"on of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available corpora of human-produced referring exp"
W11-2806,P10-2011,0,0.0135512,"he other party in a dialogue. This perspective is most strongly associated with the work of Pickering and Garrod (2004). With respect to reference in particular, speakers are said to form conceptual pacts in their use of language (Clark and Wilkes-Gibbs, 1986; Brennan and Clark, 1996). The implication of much of this work is that one speaker introduces an entity by means of some description, and then (perhaps after some negotiation) both conversational participants share this form of reference, or a form of reference derived from it, when they subsequently refer to that entity. Recent work by Goudbeek and Krahmer (2010) supports the view that subconscious alignment does indeed take place at the level of content selection for referring expressions: the participants in their study were more likely to use a dispreferred attribute to describe a target referent if this attribute had recently been used in a description by a confederate. One way of characterising these developments is that, on the one hand, the original very precise and somewhat rigid computational approaches to REG have been progressively weakened in the face of real human data; and on the other hand, work in a distinct discipline has offered a qu"
W11-2806,J86-3001,0,0.539881,"om the other entities in the context (Dale and Reiter, 1995; Gardent, 2002; Krahmer and Theune, 2002; Krahmer et al., 2003; Gatt and van Deemter, 2006). The task is generally characterised as involving the construction of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algori"
W11-2806,J03-1003,0,0.0717799,"Missing"
W11-2806,W06-1410,1,0.818989,"y characterised as involving the construction of a distinguishing description consisting of those attributes of the intended referent that distinguish it from the other entities with which it might be confused; building a referring expression thus requires us to have an appropriate formalisation of the notion of context. Earlier work (for example, (Dale, 1989)) took its cue from work on discourse structure (in particular, (Grosz and Sidner, 1986)), and defined the context in terms of the set of discourseaccessible referents; more recent work has tended to focus on visual scenes (for example, (Viethen and Dale, 2006; Gatt et al., 2008; Gatt et al., 2009)), with the context being defined as the set of all the objects in the scene. Most of the early approaches to REG (Dale, 1989; Dale and Haddock, 1991; Dale and Reiter, 1995; Krahmer et al., 2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available"
W11-2806,W08-1109,1,0.893361,"2003) were proposed without the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available corpora of human-produced referring expressions collected under controlled circumstances: these include the TUNA Corpus (van der Sluis et al., 2006), the Drawer Corpus (Viethen and Dale, 2006), and the GRE3D3 and GRE3D7 Corpora (Viethen and Dale, 2008; Viethen and Dale, 2011). All of these corpora contain descriptions of target referents using a small number of attributes in simple visual scenes containing only a very small number of distractor objects. The descriptions in all these cases were elicited in isolation, with no preceding discourse: the reference task they represent has sometimes been called ‘one-shot reference’. So there is no discourse context that provides a set of potential distractors, but there is a visual context of potential distractors. The idea that the process of constructing a reference to an object in a visual scen"
W11-2806,W11-2702,1,0.8289,"out the support of rigorous empirical testing. Probably the most fundamental shift in the field in the last five years has been the move towards 44 3 the development of algorithms that attempt to replicate corpora of human-produced referring expressions. This work has only really become possible with the advent of a number of publicly-available corpora of human-produced referring expressions collected under controlled circumstances: these include the TUNA Corpus (van der Sluis et al., 2006), the Drawer Corpus (Viethen and Dale, 2006), and the GRE3D3 and GRE3D7 Corpora (Viethen and Dale, 2008; Viethen and Dale, 2011). All of these corpora contain descriptions of target referents using a small number of attributes in simple visual scenes containing only a very small number of distractor objects. The descriptions in all these cases were elicited in isolation, with no preceding discourse: the reference task they represent has sometimes been called ‘one-shot reference’. So there is no discourse context that provides a set of potential distractors, but there is a visual context of potential distractors. The idea that the process of constructing a reference to an object in a visual scene needs to take account o"
W11-2806,D11-1107,1,0.895257,"iscourse context that provides a set of potential distractors, but there is a visual context of potential distractors. The idea that the process of constructing a reference to an object in a visual scene needs to take account of the other entities in that scene in order to ensure that the reference is successful seems so obvious that it might be thought ridiculous to doubt it. However, our exploration of a dataset that contains referring expressions for objects in visual scenes of somewhat greater complexity and involving dialogic discourse calls this fundamental assumption into question. In (Viethen et al., 2011), we presented a machinelearning approach to REG, and distinguished two main kinds of features that might play a role in subsequent reference: ‘traditional’ REG features, which are concerned with distinguishing the intended referent from visual and discourse distractors; and ‘alignment’ features, representing aspects of the discourse history (Clark and WilkesGibbs, 1986; Pickering and Garrod, 2004). We used feature ablation in a decision tree approach to investigate the role of the traditional features, and found that the impact of these features was negligible compared to that of the alignmen"
W11-2828,H05-1042,0,0.0173431,"00 Player Jesse White Jarrad McVeigh Patrick Ryder Event H A G B B Score HAM 6 0 6 7 0 7 7 1 6 Table 1: Sample scoring events data Player Jude Bolton Adam Goodes Heath Grundy K 16 11 8 M 3 5 2 H 20 5 8 G 0 2 0 B 0 4 0 T 12 1 1 Table 2: Sample of in-game player statistics approach, working on American football data. Formulating the problem as one of energy minimisation allows them to find a globally optimal set of database rows, in contrast to the independent row selection of Duboue and McKeown (2003). The goal of both approaches was to extract and present items that occur in the tabular data; Barzilay and Lapata (2005) explicitly restrict themselves to selecting from this raw data. Kelly et al. (2009), applying Barzilay and Lapata’s approach to the domain of cricket, go beyond looking at raw data items to a limited ‘grouping’ of data, for example in pairing player data for batting partnerships. In contrast, we are interested in presenting not just raw data, but data over which some inference has been carried out (as in the selection of time series data by Yu et al. (2004)), and the feasibility of using a machine learning approach to achieve this. 3 Correlating data and texts Our data comes in the form of ta"
W11-2828,W03-1016,0,0.0778121,"Missing"
W11-2828,W09-0623,0,0.031242,"1 6 Table 1: Sample scoring events data Player Jude Bolton Adam Goodes Heath Grundy K 16 11 8 M 3 5 2 H 20 5 8 G 0 2 0 B 0 4 0 T 12 1 1 Table 2: Sample of in-game player statistics approach, working on American football data. Formulating the problem as one of energy minimisation allows them to find a globally optimal set of database rows, in contrast to the independent row selection of Duboue and McKeown (2003). The goal of both approaches was to extract and present items that occur in the tabular data; Barzilay and Lapata (2005) explicitly restrict themselves to selecting from this raw data. Kelly et al. (2009), applying Barzilay and Lapata’s approach to the domain of cricket, go beyond looking at raw data items to a limited ‘grouping’ of data, for example in pairing player data for batting partnerships. In contrast, we are interested in presenting not just raw data, but data over which some inference has been carried out (as in the selection of time series data by Yu et al. (2004)), and the feasibility of using a machine learning approach to achieve this. 3 Correlating data and texts Our data comes in the form of tables that focus on different aspects of the game. The most important for our current"
W11-2828,W08-1124,0,0.0672643,"Missing"
W11-2828,E03-1021,0,0.0340066,"this task that can be used as a baseline for future work, and examine its adequacy for content selection (§4). 2 Related work Time series Previous work has dealt with time series data and the particular problem of segmenting them meaningfully. Time series are typically continuous processes monitored at regular intervals; ours, in contrast, are irregular sequences of discrete events. The main difference is the number of data points: for example, a pressure sensor can produce thousands of readings in a day, but we only need to consider about 50 events in a game (see §3). The S UM T IME project (Sripada et al., 2003b; Yu Led by Brownlow medallist Adam Goodes and et al., 2004) aims to produce a generic time series veteran Jude Bolton, the Swans kicked seven goals from 16 entries inside their forward 50 to summary generator. It has been applied to weather open a 30-point advantage at the final change— forecasts (Sripada et al., 2002; Sripada et al., 2003a), to that point the largest lead of the match. neo-natal intensive care (Sripada et al., 2003c; Portet There is a corresponding database which contains et al., 2009), and gas turbine monitoring (Yu et al., quantitative and other data regarding the game: w"
W11-2838,W10-4236,1,\N,Missing
W11-2838,bird-etal-2008-acl,1,\N,Missing
W12-2006,N12-1067,0,0.346936,"egard to the HOO annotation scheme is that we require precise identification of error locations and accurate specification of these locations at a character-offset level in our standoff edit notation. It is often inaccuracies at this level that contribute to the differences between a team’s detection score and the corresponding recognition score. While precise character offset information is important for some error correction tasks (for example, one would not want an automated corrector to insert corrections misplaced by one character), arguably it is too strict in the present circumstances. Dahlmeier and Ng (2012) propose an alternative evaluation scheme which, along with other properties, overcomes this by operating in terms of tokens rather than character offsets. 6.4 Summary Overall, we were immensely pleased with the level of interest in this shared task. The HOO 2012 training data and evaluation tools are publicly available, so interested parties who did not take part in the shared task can still try their hand retrospectively; unfortunately, our contract with CUP means that the test data used in this round is not publicly available. Our future plans include packaging a subset of the initially hel"
W12-2006,W10-4236,1,0.417812,"6 provides some concluding remarks and discussion, reflecting on lessons learned. 2 The Task Non-native speakers who are learning English find prepositions and determiners particularly problematic. The selection of the appropriate preposition in a given context often appears to be a matter of idiom or convention rather than being governed by a consistent set of rules; and selecting a determiner 2 HOO stands for ‘Helping Our Own’, a reflection of the historical origins of the exercise as an attempt to develop tools to help researchers in natural language processing to write better papers: see (Dale and Kilgarriff, 2010) for the background to this enterprise and (Dale and Kilgarriff, 2011) for a report on the pilot round of the task held in 2011. 54 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 54–62, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Team ID CU ET JU KU LE NA NU TC TH UD UI UT VA VT Group or Institution Computer Laboratory, University of Cambridge, UK Educational Testing Service, New Jersey, USA Jadavpur University, Kolkata, India Natural Language Processing Lab, Korea University, Seoul, Korea KU Leuven, Belgium NA"
W12-2006,W11-2838,1,0.760165,"ns learned. 2 The Task Non-native speakers who are learning English find prepositions and determiners particularly problematic. The selection of the appropriate preposition in a given context often appears to be a matter of idiom or convention rather than being governed by a consistent set of rules; and selecting a determiner 2 HOO stands for ‘Helping Our Own’, a reflection of the historical origins of the exercise as an attempt to develop tools to help researchers in natural language processing to write better papers: see (Dale and Kilgarriff, 2010) for the background to this enterprise and (Dale and Kilgarriff, 2011) for a report on the pilot round of the task held in 2011. 54 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 54–62, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Team ID CU ET JU KU LE NA NU TC TH UD UI UT VA VT Group or Institution Computer Laboratory, University of Cambridge, UK Educational Testing Service, New Jersey, USA Jadavpur University, Kolkata, India Natural Language Processing Lab, Korea University, Seoul, Korea KU Leuven, Belgium NAIST, Japan National University of Singapore, Singapore Department of C"
W12-2006,dale-narroway-2012-framework,1,0.814586,"in bold.6 Note that team ET did not participate in the correction subtask. The scores for all teams improve as a consequence of the revisions being made to the data. The result of a paired t-test on the ‘before’ and ‘after’ combined preposition and determiner scores across teams was statistically significant (t = −3.17, df(12), p &lt; .01); 6 5 Results Each team was allowed to submit up to 10 separate ‘runs’ over the test data, thus allowing them to 57 The precise definitions of these measures as implemented in the evaluation tool, and further details on the evaluation process, are provided in (Dale and Narroway, 2012) and elaborated on at the HOO website at www.correcttext.org/hoo2012. F-scores improved by a mean value of 2.32. The same analyses for preposition scores also resulted in significant improvement (t = −3.29, df(12), p &lt; .01), with a mean improvement in F-scores of 2.6. A smaller (but still statistically significant) improvement in determiner scores was also present (t = −2.86, df(12), p &lt; .05), with a mean improvement in F-scores of 1.99. There are also positive correlations between the rankings before and after revisions. Pearson correlation coefficients for the ‘before’ and ‘after’ scores for"
W12-2006,N10-1019,0,0.266662,", we would be providing a very artificial dataset where one assumes some other process has fixed all 60 the other errors before the errors of interest here are addressed. While there are some types of errors that might sensibly be addressed before others in a pipeline, in general this is not a very plausible model; any real system is going to have to address noisy data containing many different kinds of errors simultaneously. A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. Team CU ET JU KU LE NA NU TC TH UD UI UT VA VT Run 6 0 1 0 0 1 0 3 1 2 0 5 3 5 Detection P R 7.8 49.31 51.67* 28.57* 7.73 6.45 12.85 10.6 40.41 35.94 37.43 32.26 57.76 30.88 8.68 8.76 17.69 34.56 6.41 24.88 40.0 37.79 34.38 25.35 11.04 15.21 9.82 7.37 F 13.48 36.8* 7.04 11.62 38.05 34.65 40.24 8.72 23.4 10.19 38.86 29.18 12.79 8.42 Run 6 0 1 0 0 1 0 3 1 1 0 5 3 5 Recognition P R 6.86 43.32 50.83* 28.11* 7.73 6.45 6.7 5.53 37.31 33.18 36.36 31.34 57.76 30.88 7.76 7.83 17.69 34."
W12-2006,P11-1019,0,0.195176,"Missing"
W96-0417,W96-0416,0,0.0285544,"Missing"
W96-0417,J88-3005,0,0.0878947,"Missing"
W96-0417,T75-2013,0,\N,Missing
W96-0417,C92-1038,1,\N,Missing
W96-0417,A92-1009,0,\N,Missing
