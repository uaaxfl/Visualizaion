2020.rocling-1.13,A Preliminary Study on Deep Learning-based {C}hinese Text to {T}aiwanese Speech Synthesis System,2020,-1,-1,3,0,15590,wenhan hsu,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.ijclclp-2.5,åºæ¼æ·±åº¦å­¸ç¿ä¹ä¸­ææå­è½å°èªèªé³åæç³»çµ±åæ­¥æ¢è¨ (A Preliminary Study on Deep Learning-based {C}hinese Text to {T}aiwanese Speech Synthesis System),2020,-1,-1,3,0,15590,wenhan hsu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 25, Number 2, December 2020",0,None
2019.rocling-1.12,Sequence to Sequence Convolutional Neural Network for Automatic Spelling Correction,2019,-1,-1,4,0,27216,daniel hladek,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.13,åºæ¼æ·±åº¦å­¸ç¿ä¹ç°¡ç­é¡åç­ç³»çµ±åæ­¥æ¢è¨(A Preliminary Study on Deep Learning-based Short Answer Question Answering System),2019,-1,-1,2,0,27219,yuchen lin,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.14,"åºæ¼éè¿´é¡ç¥ç¶ç¶²è·¯ä¹éº¥å\
é¢¨å¯å«æå¶ç³»çµ±(Recurrent Neural Network-based Microphone Howling Suppressionk)",2019,-1,-1,2,0,27220,chengyang lin,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.15,"åºæ¼æ·±åº¦é¡ç¥ç¶ç¶²è·¯ä¹å¤æ¨¡å¼æ\
æåµæ¸¬åæ­¥æ¢è¨(A Preliminary Study on Deep Learning Neural Networks-based Multi-Model Sentiment Detection)",2019,-1,-1,2,0,27222,tairong chen,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.16,é©åæ¼¸åäººä½¿ç¨ä¹èªé³è½æç³»çµ±åæ­¥ç ç©¶(Deep Neural-Network Bandwidth Extension and Denoising Voice Conversion System for {ALS} Patients),2019,-1,-1,2,0,27224,baihong huang,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.30,Building of children speech corpus for improving automatic subtitling services,2019,-1,-1,6,0,27251,matus pleva,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.ijclclp-2.3,é©åæ¼¸åäººä½¿ç¨ä¹èªé³è½æç³»çµ±åæ­¥ç ç©¶ (Deep Neural-Network Bandwidth Extension and Denoising Voice Conversion System for {ALS} Patients),2019,-1,-1,2,0,27224,baihong huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 24, Number 2, December 2019",0,None
O17-1003,åºæ¼å·ç©é¡ç¥ç¶ç¶²è·¯ä¹å»£æ­ç¯ç®é³è¨äºä»¶åµæ¸¬ç³»çµ± (Automatic Audio Event Detection of Broadcast Radio Programs Based on Convolution Neural Networks) [In {C}hinese],2017,0,0,3,1,8064,jhihwei chen,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O17-1004,"åºæ¼æ¬¡é »ééè¿´é¡ç¥ç¶ç¶²è·¯ä¹éº¥å\
é¢¨é£åé»è¦åè²æ¶é¤ç³»çµ± (Subband Recurrent Neural Networks-based Microphone Array Television Echo Cancellation) [In {C}hinese]",2017,0,0,3,0,32695,weijung hung,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O17-1020,åºæ¼åç£ç£å¼å­¸ç¿ä¹å»£æ­ç¯ç®èªé³éå­ç¨¿èªåè½å¯«ç³»çµ± (Automatic Transcription of Broadcast Radio Speech Based on Quality Estimation-Guided Semi-Supervised Training) [In {C}hinese],2017,0,0,3,0,32710,singyue wang,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O17-1021,"å®å\
¨åºæ¼é¡ç¥ç¶ç¶²è·¯ä¹èªé³åæç³»çµ±åæ­¥ç ç©¶ (A Preliminary Study on Fully Neural Network-based Speech Synthesis System) [In {C}hinese]",2017,0,0,3,0,32711,shuhan liao,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O17-1031,"åºæ¼è½è¦ºæç¥æ¨¡åä¹é¡ç¥ç¶ç¶²è·¯åå\
¶å¨èªè\
è­å¥ä¸ä¹æç¨ (Two-stage Attentional Auditory Model Inspired Neural Network and Its Application to Speaker Identification) [In {C}hinese]",2017,0,0,2,0,32728,yuwen lo,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
I17-4020,{NCTU}-{NTUT} at {IJCNLP}-2017 Task 2: Deep Phrase Embedding using bi-{LSTM}s for Valence-Arousal Ratings Prediction of {C}hinese Phrases,2017,0,0,4,0,29221,yenhsuan lee,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"In this paper, a deep phrase embedding approach using bi-directional long short-term memory (Bi-LSTM) is proposed to predict the valence-arousal ratings of Chinese words and phrases. It adopts a Chinese word segmentation frontend, a local order-aware word, a global phrase embedding representations and a deep regression neural network (DRNN) model. The performance of the proposed method was benchmarked by the IJCNLP 2017 shared task 2. According the official evaluation results, our best system achieved mean rank 6.5 among all 24 submissions."
W16-4910,Word Order Sensitive Embedding Features/Conditional Random Field-based {C}hinese Grammatical Error Detection,2016,0,3,3,0,29234,weichieh chou,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"This paper discusses how to adapt two new word embedding features to build a more efficient Chinese Grammatical Error Diagnosis (CGED) systems to assist Chinese foreign learners (CFLs) in improving their written essays. The major idea is to apply word order sensitive Word2Vec approaches including (1) structured skip-gram and (2) continuous window (CWindow) models, because they are more suitable for solving syntax-based problems. The proposed new features were evaluated on the Test of Chinese as a Foreign Language (TOCFL) learner database provided by NLP-TEA-3{\&}CGED shared task. Experimental results showed that the new features did work better than the traditional word order insensitive Word2Vec approaches. Moreover, according to the official evaluation results, our system achieved the lowest (0.1362) false positive (FA) and the highest precision rates in all three measurements."
O16-3005,"åºæ¼å­å\
éå±¤ä¹èªé³åæç¨æèè¨æ¯æ·å (Character-Level Linguistic Features Extraction for Text-to-Speech System) [In {C}hinese]",2016,0,0,3,0,34559,kuanhung chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-1013,"åºæ¼å­å\
éå±¤ä¹èªé³åæç¨æèè¨æ¯æ·å(Character-Level Linguistic Features Extraction for Text-to-Speech System) [In {C}hinese]",2016,0,0,3,0,34559,kuanhung chen,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1028,åºæ¼æ·±å±¤é¡ç¥ç¶ç¶²è·¯ä¹é³è¨äºä»¶åµæ¸¬ç³»çµ±(Deep Neural Networks for Audio Event Detection)[In {C}hinese],2016,0,0,3,1,8064,jhihwei chen,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1029,åºæ¼å¢å¼·å¼æ·±å±¤é¡ç¥ç¶ç¶²è·¯ä¹èªè¨è¾¨èªç³»çµ±(Reinforcement Training for Deep Neural Networks-based Language Recognition)[In {C}hinese],2016,0,0,3,0,34601,yenwen hsiao,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1032,åºæ¼æ·±å±¤éè¿´é¡ç¥ç¶ç¶²è·¯ä¹å¤ééé»è¦åè²æ¶é¤ç³»çµ±(Multi-Channel Television Echo Cancellation based on Deep Recurrent Neural Networks)[In {C}hinese],2016,0,0,3,0,34605,hung huang,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
W15-3108,Word Vector/Conditional Random Field-based {C}hinese Spelling Error Detection for {SIGHAN}-2015 Evaluation,2015,7,2,2,1,27257,yihru wang,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"In order to detect Chinese spelling errors, especially for essays written by foreign learners, a word vector/conditional random field (CRF)based detector is proposed in this paper. The main idea is to project each word in a test sentence into a high dimensional vector space in order to reveal and examine their relationships by using a CRF. The results are then utilized to constrain the time-consuming language model rescoring procedure. Official SIGHAN2015 evaluation results show that our system did achieve reasonable performance with about 0.601/0.564 ac-curacies and 0.457/0.375 F1 scores in the detection/correction levels."
W14-6834,{NCTU} and {NTUT}{'}s Entry to {CLP}-2014 {C}hinese Spelling Check Evaluation,2014,7,3,2,1,27257,yihru wang,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper describes our Chinese spelling check system submitted to SIGHAN Bake-off 2014 evaluation. The systemxe2x80x99s main components are still the conditional random field (CRF)-based word segmentation/part-ofspeech (POS) tagger and tri-gram language model (LM) used last year. But we tried to refine the misspelling rules, decision-making threshold and improve LM rescoring speed to reduce false alarm rate and improve rescoring speed. Bake-off 2014 evaluation results show that one of our system (Run2) did achieve reasonable performance with about 0.485/0.468 accuracies and 0.226/0.180 F1 scores in the detection/correction metrics."
W13-4412,Conditional Random Field-based Parser and Language Model for Tradi-tional {C}hinese Spelling Checker,2013,6,9,2,1,27257,yihru wang,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,This paper describes our Chinese spelling check system submitted to SIGHAN Bake-off 2013 evaluation. The main idea is to exchange potential error character with its confusable ones and rescore the modified sentence using a conditional random field (CRF)-based word segmentation/part of speech (POS) tagger and a tri-gram language model (LM) to detect and correct possible spelling errors. Experimental results on the Bakeoff 2013 tasks showed the proposed method achieved 0.50 location detection and 0.24 error location F-scores in subtask1 and 0.49 location and 0.40 correction accuracies and 0.40 correction precision in subtask2.
W12-6340,A Conditional Random Field-based Traditional {C}hinese Base Phrase Parser for {SIGHAN} Bake-off 2012 Evaluation,2012,7,1,2,1,27257,yihru wang,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper describes our system for the subtask 1 of traditional Chinese Parsing of SIGHAN Bake-off 2012 evaluation. Since this research mainly focuses on speech recognition and synthesis applications, only base phrase chunking was implemented using three Conditional Random Field (CRF) modules, including word segmentation, POS tagging and base phrase chunking sub-systems. The official evaluation results show that the system achieved 0.5038 (0.7210/0.387) micro- and 0.5301 (0.7343/0.4147) macro-averaging F1 (precision/recall) rates on full sentence parsing task. However, if only the performance of base phrase chunking was considered, the Fmeasures may be around 0.70 and is somehow good enough for speech recognition and synthesis applications."
O11-1003,"è¯åèªè\
ãéè¨ç°å¢èèªªè©±å\
§å®¹å ç´ åæä¹å¼·å¥æ§èªé³è¾¨èª (Joint Factor Analysis for Robust Speech Recognition) [In {C}hinese]",2011,12,0,3,0,44727,shengtang wu,Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2011),0,None
O09-1010,Latent Prosody Model-Assisted {M}andarin Accent Identification,2009,12,1,1,1,15592,yuanfu liao,Proceedings of the 21st Conference on Computational Linguistics and Speech Processing,0,"A two-stage latent prosody model-language model (LPM-LM)-based approach is proposed to identify two Mandarin accent types spoken by native speakers in Mainland China and Taiwan. The frontend LPM tokenizes and jointly models the affections of speaker, tone and prosody state of an utterance. The backend LM takes the decoded prosody state sequences and builds n-grams to model the prosodic differences of the two accent types. Experimental results on a mixed TRSC and MAT database showed that fusion of the proposed LPM-LM with a SDC/GMMPPR-LMUPR-LM baseline system could further reduced the average accent identification error rate from 20.7% to 16.2%. Therefore, the proposed LPM-LM method is a promising approach."
O06-1014,"çµåé»å¾èè²å­¸è¨æ¯ä¹å¼·å¥æ§æ¼¢èªèªè\
é©è­ç³»çµ± (Incorporating Prosodic with Acoustic information for Robust Speaker Verification) [In {C}hinese]",2006,0,0,5,0,50035,wenchieh chang,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
O05-1003,"çµåè²å­¸èé»å¾è¨æ¯ä¹å¼·å¥æ§èªè\
è¾¨èªæ¹æ³ (Combination of Acoustic and Prosodic Information for Robust Speaker Identification) [In {C}hinese]",2005,0,0,1,1,15592,yuanfu liao,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
O97-1025,A First Study on {M}andarin Prosodic State Detection,1997,-1,-1,1,1,15592,yuanfu liao,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
