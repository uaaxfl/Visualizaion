2020.coling-main.460,N19-1423,0,0.0399398,"Missing"
2020.coling-main.460,I05-5002,0,0.0350157,"Missing"
2020.coling-main.460,W18-4704,0,0.0238572,"ed extensively in theoretical linguistic analysis. Many linguists have agreed that motion is a semantic fundamental (Talmy, 1985; Goddard, 1997; Beavers et al., 2010), thus identifying motion in text and its features is important, and so it is to investigate how motion features could help improve Natural Language Processing (NLP) tasks. Although analysis of spatial features in natural language has considered and investigated semantics related with motion events, their elements, including encoding schemes (Pustejovsky and Moszkowicz, 2008; Pustejovsky and Yocum, 2013; Pustejovsky et al., 2015; Lee et al., 2018), to the best of our This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 License details: http:// https://intranet.secure.griffith.edu.au/schools-departments/natural-semantic-metalanguage/what-is-nsm/semantic-primes 5250 Proceedings of the 28th International Conference on Computational Linguistics, pages 5250–5258 Barcelona, Spain (Online), December 8-13, 2020 knowledge, there has not been an investigation of how motion detection in natural language, and motion features, might impact NLP tasks. To start this investigatio"
2020.coling-main.460,marelli-etal-2014-sick,0,0.0428959,"Missing"
2020.coling-main.460,C08-2024,0,0.0553025,"press complicated events more concisely, because of its features motion has been considered extensively in theoretical linguistic analysis. Many linguists have agreed that motion is a semantic fundamental (Talmy, 1985; Goddard, 1997; Beavers et al., 2010), thus identifying motion in text and its features is important, and so it is to investigate how motion features could help improve Natural Language Processing (NLP) tasks. Although analysis of spatial features in natural language has considered and investigated semantics related with motion events, their elements, including encoding schemes (Pustejovsky and Moszkowicz, 2008; Pustejovsky and Yocum, 2013; Pustejovsky et al., 2015; Lee et al., 2018), to the best of our This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 License details: http:// https://intranet.secure.griffith.edu.au/schools-departments/natural-semantic-metalanguage/what-is-nsm/semantic-primes 5250 Proceedings of the 28th International Conference on Computational Linguistics, pages 5250–5258 Barcelona, Spain (Online), December 8-13, 2020 knowledge, there has not been an investigation of how motion detection in natural languag"
2020.coling-main.460,W13-0503,0,0.301859,"isely, because of its features motion has been considered extensively in theoretical linguistic analysis. Many linguists have agreed that motion is a semantic fundamental (Talmy, 1985; Goddard, 1997; Beavers et al., 2010), thus identifying motion in text and its features is important, and so it is to investigate how motion features could help improve Natural Language Processing (NLP) tasks. Although analysis of spatial features in natural language has considered and investigated semantics related with motion events, their elements, including encoding schemes (Pustejovsky and Moszkowicz, 2008; Pustejovsky and Yocum, 2013; Pustejovsky et al., 2015; Lee et al., 2018), to the best of our This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 License details: http:// https://intranet.secure.griffith.edu.au/schools-departments/natural-semantic-metalanguage/what-is-nsm/semantic-primes 5250 Proceedings of the 28th International Conference on Computational Linguistics, pages 5250–5258 Barcelona, Spain (Online), December 8-13, 2020 knowledge, there has not been an investigation of how motion detection in natural language, and motion features, might"
2020.coling-main.460,P18-2124,0,0.0270378,"Missing"
2020.coling-main.460,W03-0419,0,0.143496,"Missing"
2020.coling-main.460,W19-3320,0,0.0158358,"ces to improve the performance of our MET model. We also present results showing that motion features, in particular, entity in motion benefits the Named-Entity Recognition (NER) task. Finally, we present an analysis for the special co-occurrence relation between the person category in NER and animate entities in motion, which significantly improves the classification performance for the person category in NER. 1 Introduction In semantic approaches such as Semantic Role Labeling (SRL) and Abstract Meaning Representation (AMR), semantic features can have peculiarities of a particular language (Zhu et al., 2019). In contrast, the irreducible semantic core considered by the Natural Semantic Metalanguage (NSM) approach, builds a kind of universal mini-language where its core components, the semantic primes1 , are universal to all languages (Goddard, 1997; Wierzbicka, 1980). Semantic primes, are defined as universal core meanings. The hypothesis in NSM is that meaning could be reconstructed into basic elements or semantic primes. Then, any complex meaning can be decomposed without circularity and without residue into a combination of discrete other meanings, the semantic primes, among which is the meani"
atserias-etal-2004-cross,habash-dorr-2002-handling,0,\N,Missing
atserias-etal-2004-cross,briscoe-carroll-2002-robust,1,\N,Missing
atserias-etal-2004-cross,H92-1045,0,\N,Missing
atserias-etal-2004-cross,magnini-cavaglia-2000-integrating,1,\N,Missing
atserias-etal-2004-cross,agirre-etal-2004-exploring,1,\N,Missing
C10-2114,P98-1012,0,0.212887,"Missing"
C10-2114,P06-2012,0,0.022149,"Missing"
C10-2114,W03-0405,0,0.0463158,"Missing"
C10-2114,magnini-etal-2006-cab,0,0.0244791,"Missing"
C10-2114,N04-1002,0,\N,Missing
C10-2114,S07-1058,0,\N,Missing
C10-2114,S07-1019,0,\N,Missing
C10-2114,C98-1012,0,\N,Missing
C10-2114,H94-1021,0,\N,Missing
C10-2114,N09-2039,1,\N,Missing
C10-2114,sekine-2008-extended,0,\N,Missing
D09-1104,P98-1012,0,0.168991,"ata from annotated coreference corpora and we individuate a specific problem, setting up a working hypothesis. In Section 4 we develop a statistical model for computing the prior coreference probabilities and in Section 5 we present the results obtained by applying it to a large news corpus. In section 6 a direct evaluation on CDC is carried on a test corpus. In Section 7 we show how the proposed techniques extends naturally to a strategy of construction relevant test corpora for CDC task. The paper ends with the Conclusion and the Future Research section. 2 Related Work In a classical paper (Bagga and Baldwin 1998), a PCDC system based on the vector space model (VSM) is proposed. While there are many advantages in representing the context as vectors on which a similarity function is applied, it has been shown that there are inherent limitations associated with the vectorial model (Popescu 2008). These problems, related to the density in the vectorial space (superposition) and to the discriminative power of the similarity power (masking), become visible as more cases are considered. Testing the system on many names, (Gooi and Allan, 2004), it has been noted empirically that the accuracy of the results va"
D09-1104,P06-2012,0,0.0447721,"Missing"
D09-1104,H94-1021,0,0.494442,"meter that varies from corpora to corpora, which makes it difficult for usual disambiguation methods. In this paper we show that the amount of context required can be dynamically controlled on the basis of the prior probabilities of coreference and we present a new statistical model for the computation of these probabilities. The experiment we carried on a news corpus proves that the prior probabilities of coreference are an important factor for maintaining a good balance between precision and recall for cross document coreference systems. 1 Introduction The Person Cross Document Coreference (Grishman 1994) task, which requires that all and only the textual mentions of an entity of type Person be individuated in a large collection of text documents, is one of the challenging tasks for natural language processing systems. In the most general case the corpus itself is the only available source of information regarding the persons mentioned and we consider that this is the case in this paper. A PCDC system must be able to use the information existing in the corpus in order to assign to each personal name mention (PNM) a piece of context. The coreference of any two PNMs is decided mainly on the basi"
D09-1104,magnini-etal-2006-cab,0,0.163668,"Missing"
D09-1104,N04-1002,0,\N,Missing
D09-1104,S07-1058,0,\N,Missing
D09-1104,S07-1019,0,\N,Missing
D09-1104,C98-1012,0,\N,Missing
D09-1104,S07-1012,0,\N,Missing
D14-1171,A00-2019,0,0.144143,"Missing"
D14-1171,H94-1021,0,0.0607204,"Missing"
D14-1171,J96-1003,0,0.306326,"Missing"
D14-1171,D09-1104,1,0.897991,"Missing"
D14-1171,C04-1120,0,0.100033,"Missing"
D14-1171,C88-2146,0,0.245443,"Missing"
D14-1171,magnini-etal-2006-cab,0,0.0210767,"Missing"
D14-1171,J04-4003,0,0.0292128,", which is impracticably large for most of large scale applications. By comparison, the number of proper names occurring in a medium sized English news corpus is around 200, 000, which means that there are some 200, 000, 000 pairs. In order to cope with the need for a lower computation time, on the basis of ED, a series of algorithms have been developed that run in linear time (Navaro 2001). Unfortunately, this improvement is not enough for practical applications which involve a large amount of data coming from large corpora. The reason is two-fold: firstly, the linear time is still too slow (Mihov and Schulz, 2004) and secondly, the required memory depends both on the strings’ length and on the number of different characters between the source string and the correct word, and may well exceed several GBs. Another solution is to index the corpus using structures like trie trees, or large finite state automata. However, this solution may require large amounts of memory and is inefficient when the number of characters that differ between the source string and the candidate words is more than two characters (Boytsov, 2011). We focus specifically on misspellings for which there is no dictionary containing the"
D14-1171,P06-1031,0,0.0605626,"Missing"
E14-1007,P10-1024,0,0.0149542,"hey, we, ...} observe dobj:{effect} nsubj:{he, ...} observe dobj:{result} prep at:{time} nsubj:{you, child, ...} observe dobj:{bird} .. . 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation to get a set of initial frames, and Figure 1: Examples of predicate-argument structures and initial frames for the verb “observe.” 3. apply clustering to the initial frames based on the Chinese Restaurant Process to produce the final semantic frames. frequencies in the applications of semantic frames or the method proposed by Abend and Rappoport (2010). We apply the following processes to extracted predicate-argument structures: Each of these steps is described in the following sections in detail. 3.1 Extracting Predicate-argument Structures from a Raw Corpus • A verb and an argument are lemmatized, and only the head of an argument is preserved for compound nouns. We first apply dependency parsing to a large raw corpus. We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases. Then, we extract predicate-argument structures from the depende"
E14-1007,kawahara-kurohashi-2006-case,1,0.679562,"Missing"
E14-1007,P98-1013,0,0.304305,"ga-word corpora. Our semantic frames are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to"
E14-1007,N06-1023,1,0.835493,"Although these frames are based on typed dependencies and more semantic than subcategorization frames, they are not distinguished in terms of the senses of words filling a case slot. There are hand-crafted semantic frames in the lexicons of FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Corpus Pattern Analysis (CPA) frames (Hanks, 2012) are another manually created repository of patterns for verbs. Each pattern represents a prototypical word usage as extracted by lexicographers from the BNC. Creating CPA is time consuming, but our proposed For other languages than English, Kawahara and Kurohashi (2006a) proposed a method for automatically compiling Japanese semantic frames from a large web corpus. They applied conventional agglomerative clustering to predicateargument structures using word/frame similarity based on a manually-crafted thesaurus. Since Japanese is head-final and has case-marking postpositions, it seems easier to build semantic frames with it than with other languages such as English. They also achieved an improvement in dependency parsing and predicate-argument structure 59 Sentences: They observed the effects of ... This statistical ability to observe an effect ... We did n"
E14-1007,boas-2002-bilingual,0,0.0222164,"our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006"
E14-1007,korhonen-etal-2006-large,0,0.0632328,"nslation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *”"
E14-1007,A97-1052,0,0.111055,"arsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal"
E14-1007,N10-1137,0,0.0714345,"., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying thi"
E14-1007,P10-1046,0,0.106572,"the previous approaches, especially considering that no filtering procedures were applied to the induced frames. We anticipate that the results based on the prototypical induced frames with SUMO attributes would be competitive. Our post-analysis revealed that the entropy can be lowered further if an automatic filtering based on frequencies is applied. 4.5 Evaluation of the Quality of Selectional Preferences We also investigated the quality of selectional preferences within the induced semantic frames. The only publicly available test data for selectional preferences, to our knowledge, is from Chambers and Jurafsky (2010). This data consists of quadruples (verb, relation, word, confounder) and does not contain their context.7 A typical way for using our semantic frames is to select an appropriate frame for an input sentence and judge the eligibility of the word uses against the selected frame. However, due to the lack of context for the above data, it is difficult to select a corresponding semantic frame for a test quadruple and thus the induced semantic frames cannot be naturally applied to this data. To investigate the potential for selectional preferences of the semantic frames, we approximately match a qua"
E14-1007,P11-1112,0,0.121541,"orhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor"
E14-1007,de-marneffe-etal-2006-generating,0,0.00430938,"Missing"
E14-1007,D11-1122,0,0.144094,"orhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large cor"
E14-1007,W07-1424,0,0.0310914,"imental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, stude"
E14-1007,P12-1044,0,0.0467125,"information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *” means a preposition. 58"
E14-1007,W13-0117,1,0.919031,"the current number of initial frames assigned to the semantic frame fj . α is a hyper-parameter that determines how likely it is for a new semantic frame to be created. In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of vi . P (vi |fj ) is defined based on the DirichletMultinomial distribution as follows: ∏ P (vi |fj ) = P (w|fj )count(vi ,w) , (2) dobj, ccomp, nsubj, prep ∗, iobj. w∈V This selection of a predominant argument order above is justified by relative comparisons of the discriminative power of the different slots for CPA frames (Popescu, 2013). If a predicate-argument structure does not have any of the above slots, it is discarded. Then, the predicate-argument structures that have the same verb and argument pair (slot and where V is the vocabulary in all case slots cooccurring with the verb. It is distinguished by the case slot, and thus consists of pairs of slots and words, e.g., “nsubj:child” and “dobj:bird.” count(vi , w) is the number of w in the initial frame vi . P (w|fj ) is defined as follows: 3 If a predicate-argument structure has multiple prepositional phrases, one of them is randomly selected. P (w|fj ) = ∑ 61 count(fj"
E14-1007,P13-1085,0,0.309287,"urdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent observe:2 nsubj:{teacher, we, ...} dobj:{child, student, ...} prep in:{classroom, school, ...} ... observe:3 nsubj:{child, people, ...} dobj:{bird, animal, ...} prep at:{range, time, ...} ... 1 In this paper, we use the dependency relation names of the Stanford collapsed dependencies (de Marneffe et al., 2006) as the notations of case slots. For instance, “nsubj” means a nominal subject, “dobj” means a direct object, “iboj” means an indirect object, “ccomp” means a clausal complement and “prep *” means a preposition. 58 Proceedings of the 14th Confer"
E14-1007,D13-1121,1,0.84905,"hen, we extract predicate-argument structures from the dependency parses. Dependents that have the following dependency relations to a verb are extracted as arguments: • Phrasal verbs are also distinguished from non-phrasal verbs. For example, “look up” has independent frames from “look.” • The passive voice of a verb is distinguished from the active voice, and thus these have independent frames. Passive voice is detected using the part-of-speech tag “VBN” (past participle). The alignment between frames of active and passive voices will be done after the induction of frames using the model of Sasano et al. (2013) in the future. nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗ Here, we do not distinguish adjuncts from arguments. All extracted dependents of a verb are handled as arguments. This distinction is left for future work, but this will be performed using slot 2 • “xcomp” (open clausal complement) is renamed to “ccomp” (clausal complement) and “xsubj” (controlling subject) is renamed to “nsubj” (nominal subject). This is because http://nlp.stanford.edu/software/lex-parser.shtml 60 word, e.g., “dobj:effect”) are merged into an initial frame (Figure 1). After this process, we discard minor initial f"
E14-1007,D09-1067,0,0.0615122,"sien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domi"
E14-1007,P03-1002,0,0.0720399,"verage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen"
E14-1007,N13-1051,0,0.278596,"nto, Italy dk@i.kyoto-u.ac.jp, {Daniel.W.Peterson, Martha.Palmer}@colorado.edu, popescu@fbk.eu Abstract argument patterns of verbs and are purely syntactic, expressions that have the same subcategorization frame can have different meanings (e.g., metaphors). Semantics-oriented NLP applications based on frames, such as paraphrase acquisition and machine translation, require consistency in the meaning of each frame, and thus these subcategorization frames are not suitable for these semantic tasks. Recently, there have been a few studies on automatically acquiring semantic frames (Materna, 2012; Materna, 2013). Materna induced semantic frames (called LDA-Frames) from triples of (subject, verb, object) in the British National Corpus (BNC) based on Latent Dirichlet Allocation (LDA) and the Dirichlet Process. LDAFrames capture limited linguistic phenomena of these triples, and are defined across verbs based on probabilistic topic distributions. This paper presents a method for automatically building verb-specific semantic frames from a large raw corpus. Our semantic frames are verbspecific like PropBank and semantically distinguished. A frame has several syntactic case slots, each of which consists of"
E14-1007,P11-1145,0,0.0615372,"rs in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalabilit"
E14-1007,W12-1901,0,0.252813,"rowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. The most closely related work to our semantic frames are LDA-Frames, which are probabilistic semantic frames automatically induced from a raw corpus (Mate"
E14-1007,E12-1003,0,0.10559,"articipate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotated corpus. Applying this method to a large corpus could produce a frame lexicon, but its scalability would be a big problem. The"
E14-1007,C04-1100,0,0.040668,"rpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing similar frames for other languages. Most of these approaches, however, focus on syntactic frames, i.e., subcategorization frames (e.g., (Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006; Lippincott et al., 2012; Reichart and Korhonen, 2013)). Since subcategorization frames represent o"
E14-1007,W09-0210,0,0.0675922,"ct, the models of Parisien and Stevenson are related to our method (Parisien and Stevenson, 2009; Parisien and Stevenson, 2010). Parisien and Stevenson (2009) proposed a Dirichlet Process model for clustering usages of the verb “get.” Later, Parisien and Stevenson (2010) proposed a Hierarchical Dirichlet Process model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role"
E14-1007,J05-1004,1,0.186197,"es are verb-specific example-based frames that are distinguished according to their senses. We use the Chinese Restaurant Process to automatically induce these frames from a massive amount of verb instances. In our experiments, we acquire broad-coverage semantic frames from two giga-word corpora, the larger comprising 20 billion words. Our experimental results indicate the effectiveness of our approach. 1 Introduction Semantic frames are indispensable knowledge for semantic analysis or text understanding. In the last decade, semantic frames, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), have been manually elaborated. These resources are effectively exploited in many natural language processing (NLP) tasks, including not only semantic parsing but also machine translation (Boas, 2002), information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), paraphrase acquisition (Ellsworth and Janin, 2007) and recognition of textual entailment (Burchardt and Frank, 2006). There have been many attempts to automatically acquire frame knowledge from raw corpora with the goal of either adding frequency information to an existing resource or of inducing"
E14-1007,H93-1052,0,0.534394,"model for jointly clustering argument structures (i.e., subcategorization frames) and verb classes. However, their argument structures are not semantic but syntactic, and also they did not evaluate the resulting frames. There have also been related approaches to clustering verb types (Vlachos et al., 2009; Sun and Korhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. ("
E14-1007,D09-1001,0,0.0389919,"orhonen, 2009; Falk et al., 2012; Reichart and Korhonen, 2013). These methods induce verb clusters in which multiple verbs participate, and do not consider the polysemy of verbs. Our objective is different from theirs. • exploitation of the assumption of one sense per collocation (Yarowsky, 1993) to make the computation feasible, • providing broad-coverage knowledge for selectional preferences, and • evaluating induced semantic frames by using an existing annotated corpus with verb classes. 2 Related Work Another line of related work is unsupervised semantic parsing or semantic role labeling (Poon and Domingos, 2009; Lang and Lapata, 2010; Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2011; Titov and Klementiev, 2012). These approaches basically cluster predicates and their arguments to distinguish predicate senses and semantic roles of arguments. Modi et al. (2012) extended the model of Titov and Klementiev (2012) to jointly induce semantic roles and frames using the Chinese Restaurant Process, which is also used in our approach. However, they did not aim at building a lexicon of semantic frames, but at distinguishing verbs that have different senses in a relatively small annotat"
E14-1007,W06-1601,0,\N,Missing
E14-1007,P07-1028,0,\N,Missing
E14-1007,C98-1013,0,\N,Missing
E14-1007,P93-1032,0,\N,Missing
E14-1007,P12-1090,0,\N,Missing
I13-1040,P12-2051,0,0.128012,"Missing"
I13-1040,W10-0204,0,0.0183233,"agoya, Japan, 14-18 October 2013. compiled for the political and sociological domain publicly available1 . The frequency of these terms and their covariance is analyzed over the years and non-random changes are found according to the methodology presented in Section 3. The methodology itself is purely statistical and it does not depend in any way on what the list contains. We could have equally chosen terms from art or sport domain, obtaining epoch boundaries specific to each domain. The emotion words used in epoch characterization come primarily from the NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) to which the list of emotion words extracted from WordNet-Affect (Strapparava and Valitutti, 2004), distributed in the Semeval 2007 Affective Text task (Strapparava and Mihalcea, 2007), has been added. The lexicon is made up of English words to which eight possible tags are attached: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. All in all there are 14,000 words for which at least one affective tag is given. The paper is organized as follows. In Section 2 we review the relevant literature. Section 3 presents the statistical apparatus employed in epoch determination and"
I13-1040,S07-1013,1,0.921977,"ars and non-random changes are found according to the methodology presented in Section 3. The methodology itself is purely statistical and it does not depend in any way on what the list contains. We could have equally chosen terms from art or sport domain, obtaining epoch boundaries specific to each domain. The emotion words used in epoch characterization come primarily from the NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) to which the list of emotion words extracted from WordNet-Affect (Strapparava and Valitutti, 2004), distributed in the Semeval 2007 Affective Text task (Strapparava and Mihalcea, 2007), has been added. The lexicon is made up of English words to which eight possible tags are attached: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. All in all there are 14,000 words for which at least one affective tag is given. The paper is organized as follows. In Section 2 we review the relevant literature. Section 3 presents the statistical apparatus employed in epoch determination and epoch characterization. In Section 4 we present the experiments and the results we have obtained. In the last section we highlight the contribution of this paper and make an overview o"
I13-1040,strapparava-valitutti-2004-wordnet,1,0.330976,"available1 . The frequency of these terms and their covariance is analyzed over the years and non-random changes are found according to the methodology presented in Section 3. The methodology itself is purely statistical and it does not depend in any way on what the list contains. We could have equally chosen terms from art or sport domain, obtaining epoch boundaries specific to each domain. The emotion words used in epoch characterization come primarily from the NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) to which the list of emotion words extracted from WordNet-Affect (Strapparava and Valitutti, 2004), distributed in the Semeval 2007 Affective Text task (Strapparava and Mihalcea, 2007), has been added. The lexicon is made up of English words to which eight possible tags are attached: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. All in all there are 14,000 words for which at least one affective tag is given. The paper is organized as follows. In Section 2 we review the relevant literature. Section 3 presents the statistical apparatus employed in epoch determination and epoch characterization. In Section 4 we present the experiments and the results we have obtained."
jezek-etal-2014-pas,D07-1002,0,\N,Missing
jezek-etal-2014-pas,E06-2001,0,\N,Missing
jezek-etal-2014-pas,J02-3001,0,\N,Missing
jezek-etal-2014-pas,lenci-etal-2012-lexit,0,\N,Missing
jezek-etal-2014-pas,jezek-quochi-2010-capturing,1,\N,Missing
jezek-etal-2014-pas,E12-1085,0,\N,Missing
L16-1539,S14-2001,0,0.159153,"jor support from this experiment and this finding led to a system that uses this information to revise the initial estimates of SR scores. As semantic relatedness is one of the most general and difficult task in natural language processing we expect that future systems will combine different sources of information in order to solve it. Our work suggests that textual entailment plays a quantifiable role in addressing it. Keywords: Semantic Relatedness, Textual Entailment, RTE Corpora 1. Introduction In the last years, two tasks that involve meaning processing, namely Semantic Relatedness (SR) (Marelli et al., 2014b) and Textual Entailment (TE) (Dagan et al., 2006), have received particular attention. In various academic competitions, both the TE and SR tasks have been proposed, and useful benchmarks have been created. The SR scores are usually given in the [1, 2, 3, 4, 5] interval, while the TE values are discrete signaling an entailment, a contradiction or a nonrelated relationship between candidate sentences. Interestingly, the core approach seems to be similar for both of these tasks (Agirre et al., 2012; Dagan et al., 2013). Yet, till 2014, no corpus annotated with both SR and TE labels was availab"
L16-1539,S14-2047,1,0.851414,"Missing"
L16-1539,S12-1051,0,0.101775,"Missing"
L16-1539,W07-1401,0,0.429637,"s signal ENTAILMENT/NEUTRAL TE values, and from correct TE values, we can readjust the initial SR scores, by adding or subtracting a quantity, such as to minimize the SR errors of over/underestimating in those cases correlated with TE values. We applied the above strategy on SICK SR scores and TE values, and we observed a significant improvement of the results in both tasks. In order to verify further this hypothesis we considered the RTE 1-4 corpora proposed for Pascal Recognizing Textual Entailment Challenge by PASCAL and NIST between 2006 and 2009 (Dagan et al., 2006; BarHaim et al., 2006; Giampiccolo et al., 2007; Giampiccolo et al., 2009). These corpora are already annotated with TE values and we independently annotated each corpus with SR scores. We call the new corpora RTE+SR 1-4 in order to distinguish it from the original ones. The main reason that we choose to annotate the RTE corpora with semantic relatedness scores, instead of semantic similarity scores is that the RTE corpora contain not only the ”is a” relation (occurring for semantic similarity) but also other broader semantic relations such as antonymy and meronymy between two texts (corresponding to NEUTRAL and CONTRADICTION relations in"
L16-1539,marelli-etal-2014-sick,0,0.24498,"jor support from this experiment and this finding led to a system that uses this information to revise the initial estimates of SR scores. As semantic relatedness is one of the most general and difficult task in natural language processing we expect that future systems will combine different sources of information in order to solve it. Our work suggests that textual entailment plays a quantifiable role in addressing it. Keywords: Semantic Relatedness, Textual Entailment, RTE Corpora 1. Introduction In the last years, two tasks that involve meaning processing, namely Semantic Relatedness (SR) (Marelli et al., 2014b) and Textual Entailment (TE) (Dagan et al., 2006), have received particular attention. In various academic competitions, both the TE and SR tasks have been proposed, and useful benchmarks have been created. The SR scores are usually given in the [1, 2, 3, 4, 5] interval, while the TE values are discrete signaling an entailment, a contradiction or a nonrelated relationship between candidate sentences. Interestingly, the core approach seems to be similar for both of these tasks (Agirre et al., 2012; Dagan et al., 2013). Yet, till 2014, no corpus annotated with both SR and TE labels was availab"
L18-1035,W09-2416,0,0.0893137,"Missing"
L18-1035,W04-3205,0,0.10771,"e systems evaluated here are ours, PP, and PPDB. 5.1. Pair to Pair paraphrase evaluation For the 100 chosen verbs, we put together all the paraphrases created by each approach. For the DIRT and word2vec approaches we have to set a threshold under which two pairs are not consider paraphrases, as these approaches compute a score for each possible pair. We consider the first 10, 40 and 400 pairs, which create three levels of precision which we roughly equate with the s,m,l levels from PPDB. These thresholds were not exactly a random choice, because 10 is the average number existing in VerbOcean (Chklovski and Pantel, 2004), a paraphrase resource created with DIRT algorithm, while 40 is the standard number of similar phrases returned by word2vec. We also ranked the PP created by our approach based on the probability of occurrence of each pattern. In this way , we could have the same levels of 10, 40 and 400 paraphrases. So we create three distinct test corpora where each verb had the first 10, 40 and 400 returns from our approach, DIRT, word2vec and PPDB, respectively. There are not exactly 4000, 16 000, and 160 000 pairs of paraphrases, as some of the above resources may not have provided the required number of"
L18-1035,N13-1092,0,0.0932599,"Missing"
L18-1035,C94-1042,0,0.594195,"creating a paraphrase relationship, by the fact that the same meaning of the verb and the same features are used. 4. 4.1. Extracting Pattern Paraphrases from Large Corpora Extracting Sub-categorization Framework for Verbal Phrases The first step in the unsupervised extraction of pattern paraphrases is to consider a large corpus that is already parsed. We used Gigaword, LDC2012T21 (Napoles et al., 2012). For each verb, we extracted the verbal dependents. Due to parser errors, there are many such dependency paths that are noise. To filter them out we used COMLEX, http://nlp.cs.nyu.edu/comlex/, (Grishman et al., 1994). In the case where the direct object was governing a prepositional phrase, this prepositional phrase was included in the dependency path. In Figure 2 we see an example of such dependencies: the nsubj, dobj, iobj, prep * is the head word of the nominal group having the respective role in the dependency path, v marks the verb. As can be seen in this example, we also considered the partial paths, so the same sentence may lead to several instances of paths. We further removed low frequency verbs, low frequency paths so that from an initial 1, 244, 793, 787 paths we filtered out a large number of"
L18-1035,E14-1007,1,0.830308,"as the same type of constituent can appear in different syntactic positions in the two expressions. For example, the adjunct [ SHOP ] in ”[ HUMAN ] pay [ PRICE ] for [ ARTIFACT ] from [ SHOP ]” must appear in the subject position in ”[ SHOP ] sell [ ARTIFACT ] to [ HUMAN ] for [ PRICE ]”. In this paper we describe a technique able to cope with these problems which lead to the building of a resource of pattern paraphrases. 3. Pattern Paraphrases The technique to extract pattern paraphrases is driven by the idea behind chain clarifying relationships (see among others (Popescu and Magnini, 2007; Kawara et al., 2014; Popescu, 2013; Popescu et al., 2014)). A chain clarifying relationship holds between the components of a verbal phrase if there is a unique combinations of senses that is legitimate. For example, in I saw the river’s bank. vs. I saw 237 Figure 1: Examples of verbal paraphrases. a problem the verb see has two different meaning, perceive by sight vs. to understand. Also, bank has two meanings too, sloping land vs. financial institution, and problem has two meanings as well: state of difficulty, question raised. The combination of senses perceive by sight a state of difficulty is not legitimate"
L18-1035,W09-2412,0,0.0409422,"Missing"
L18-1035,W12-3018,0,0.0889516,"Missing"
L18-1035,S12-1046,0,0.0426392,"Missing"
L18-1035,S15-2001,0,0.0643723,"Missing"
L18-1469,D13-1160,0,0.132101,"Missing"
L18-1469,P16-1004,0,0.0449297,"Missing"
L18-1469,Q15-1042,0,0.0745541,"Missing"
L18-1469,D11-1140,0,0.0631235,"Missing"
L18-1469,P15-1129,0,0.0530504,"Missing"
N09-2039,P98-1012,0,0.636213,"f a PCDC system knows that the prior probability for two PNMs to corefer is high, then the amount of contextual evidence required can be lowered and vice-versa. Our goal is to precisely define a statistical model in which the prior coreference probabilities can be computed and, consequently, to design a PCDC system that dynamically revises the context relevance accordingly. We review the PCDC literature relevant for our purposes, present the statistical model and show the preliminary results. The paper ends with the Conclusion and Further Research section. 2 Related Work In a classical paper (Bagga 1998), a PCDC system based on the vector space model (VSM) is proposed. While there are many advantages in representing the context as vectors on which a similarity function is applied, it has been shown that there are 153 Proceedings of NAACL HLT 2009: Short Papers, pages 153–156, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics inherent limitations associated with the vectorial model (Popescu 2008). These problems, related to the density in the vectorial space (superposition) and to the discriminative power of the similarity power (masking), become visible as more ca"
N09-2039,P06-2012,0,0.243733,"Missing"
N09-2039,H94-1021,0,0.643618,"Missing"
N09-2039,N04-1002,0,\N,Missing
N09-2039,S07-1041,1,\N,Missing
N09-2039,C98-1012,0,\N,Missing
N15-2009,S12-1051,0,0.393877,"veloped to resolve the tasks.1,2 The systems must quantifiably identify the degree of similarity, relatedness, respectively, for pair of short pieces of text, like sentences, where the similarity or relatedness is a broad concept and its value is normally obtained by averaging the opinion of several annotators. A semantic similarity/relatedness score is usually a real number in a semantic scale, [0, 1, 2, 3, 4, 5] in STS, or [1, 2, 3, 4, 5] in SR, in 1 2 http://www.cs.york.ac.uk/semeval-2012/task6 http://alt.qcri.org/semeval2014/task1 From our reading of the literature (Marelli et al., 2014b; Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014), most of STS/SR systems rely on pairwise similarity, such as lexical similarity using taxonomies (WordNet (Fellbaum, 1998)) or distributional semantic models (LDA (Blei et al., 2003), LSA (Landauer et al., 1998), ESA (Gabrilovich and Markovitch, 2007), etc), and word/n-grams overlap as main features to train a support vector machines (Joachims, 1998) regression model (supervised), or use a word-alignment metric (unsupervised) aligning the two given texts to compute their semantic similarity. Intuitively, the syntactic structure plays an important ro"
N15-2009,S13-1004,0,0.274843,"e tasks.1,2 The systems must quantifiably identify the degree of similarity, relatedness, respectively, for pair of short pieces of text, like sentences, where the similarity or relatedness is a broad concept and its value is normally obtained by averaging the opinion of several annotators. A semantic similarity/relatedness score is usually a real number in a semantic scale, [0, 1, 2, 3, 4, 5] in STS, or [1, 2, 3, 4, 5] in SR, in 1 2 http://www.cs.york.ac.uk/semeval-2012/task6 http://alt.qcri.org/semeval2014/task1 From our reading of the literature (Marelli et al., 2014b; Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014), most of STS/SR systems rely on pairwise similarity, such as lexical similarity using taxonomies (WordNet (Fellbaum, 1998)) or distributional semantic models (LDA (Blei et al., 2003), LSA (Landauer et al., 1998), ESA (Gabrilovich and Markovitch, 2007), etc), and word/n-grams overlap as main features to train a support vector machines (Joachims, 1998) regression model (supervised), or use a word-alignment metric (unsupervised) aligning the two given texts to compute their semantic similarity. Intuitively, the syntactic structure plays an important role for human being to"
N15-2009,S12-1059,0,0.0153958,"s pairs it with some features learned from common approaches, such as bag-of-words, pairwise similarity, n-grams overlap, etc. Therefore, we use two baseline systems for evaluations, the weak and the strong ones. The weak baseline is the basic one used for evaluation in all the STS tasks, namely tokencos. It uses the bag-of-words approach which represents each sentence as a vector in the multidimensional token space (each dimension has 1 if the token is present in the sentence, 0 otherwise) and computes the cosine similarity between vectors. Besides the weak baseline, we use DKPro Similarity (Bär et al., 2012) as a strong baseline which is an open source software and intended to use as a baseline-system in the share task STS at *SEM 2013.13 It uses a simple log-linear regression model (about 18 features), to combine multiple text similarity measures of varying complexity ranging from simple character/word n-grams and common subsequences to complex features such as Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources (WordNet and Wiktionary).14,15 Evaluations and Discussions In this section, we present twelve different settings for expe"
N15-2009,P03-1054,0,0.0481728,"tal Settings, Section 4 discusses about the Evaluations and Section 5 is the Conclusions and Future Work. 2 Three Approaches for Exploiting the Syntactic Structure In this section, we describe three different approaches exploiting the syntactic structure to be used in the STS/SR tasks, which are Syntactic Tree Kernel (Moschitti, 2006), Syntactic Generalization (Galitsky, 2013), and Distributed Tree Kernel (Zanzotto and Dell’Arciprete, 2012). All these three approaches learn the syntactic information either from the dependency parse trees produced by the Stanford Parser (standard PCFG Parser) (Klein and Manning, 2003) or constituency parse trees obtained by OpenNLP.3 The output of each approach is normalized to the standard semantic scale of STS [0, 1, 2, 3, 4, 5] or SR [1, 2, 3, 4, 5] tasks to evaluate its standalone performance, or combined with other features in our baseline system for assessing its contribution to the 3 https://opennlp.apache.org 65 overall accuracy by using the same WEKA machine learning tool (Hall et al., 2009) with as same configurations and parameters as our baseline systems. 2.1 Syntactic Tree Kernel (STK) Given two trees T1 and T2, the functionality of tree kernels is to compare"
N15-2009,marelli-etal-2014-sick,0,0.0589557,"ng systems have been developed to resolve the tasks.1,2 The systems must quantifiably identify the degree of similarity, relatedness, respectively, for pair of short pieces of text, like sentences, where the similarity or relatedness is a broad concept and its value is normally obtained by averaging the opinion of several annotators. A semantic similarity/relatedness score is usually a real number in a semantic scale, [0, 1, 2, 3, 4, 5] in STS, or [1, 2, 3, 4, 5] in SR, in 1 2 http://www.cs.york.ac.uk/semeval-2012/task6 http://alt.qcri.org/semeval2014/task1 From our reading of the literature (Marelli et al., 2014b; Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014), most of STS/SR systems rely on pairwise similarity, such as lexical similarity using taxonomies (WordNet (Fellbaum, 1998)) or distributional semantic models (LDA (Blei et al., 2003), LSA (Landauer et al., 1998), ESA (Gabrilovich and Markovitch, 2007), etc), and word/n-grams overlap as main features to train a support vector machines (Joachims, 1998) regression model (supervised), or use a word-alignment metric (unsupervised) aligning the two given texts to compute their semantic similarity. Intuitively, the syntactic structure"
N15-2009,S14-2001,0,0.107657,"ng systems have been developed to resolve the tasks.1,2 The systems must quantifiably identify the degree of similarity, relatedness, respectively, for pair of short pieces of text, like sentences, where the similarity or relatedness is a broad concept and its value is normally obtained by averaging the opinion of several annotators. A semantic similarity/relatedness score is usually a real number in a semantic scale, [0, 1, 2, 3, 4, 5] in STS, or [1, 2, 3, 4, 5] in SR, in 1 2 http://www.cs.york.ac.uk/semeval-2012/task6 http://alt.qcri.org/semeval2014/task1 From our reading of the literature (Marelli et al., 2014b; Agirre et al., 2012; Agirre et al., 2013; Agirrea et al., 2014), most of STS/SR systems rely on pairwise similarity, such as lexical similarity using taxonomies (WordNet (Fellbaum, 1998)) or distributional semantic models (LDA (Blei et al., 2003), LSA (Landauer et al., 1998), ESA (Gabrilovich and Markovitch, 2007), etc), and word/n-grams overlap as main features to train a support vector machines (Joachims, 1998) regression model (supervised), or use a word-alignment metric (unsupervised) aligning the two given texts to compute their semantic similarity. Intuitively, the syntactic structure"
N15-2009,S13-1006,0,0.570857,"emantic similarity. Intuitively, the syntactic structure plays an important role for human being to understand the mean64 Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 64–70, c Denver, Colorado, June 1, 2015. 2015 Association for Computational Linguistics ing of a given text. Thus, it also may help to identify the semantic equivalence/relatedness between two given texts. However, in the STS/SR tasks, very few systems provide evidence of the contribution of syntactic structure in its overall performance. Some systems report partially on this issue, for example, iKernels (Severyn et al., 2013) carried out an analysis on the STS 2012, but not on STS 2013 datasets. They found that syntactic structure contributes 0.0271 and 0.0281 points more to the overall performance, from 0.8187 to 0.8458 and 0.8468, for adopting constituency and dependency trees, respectively. In this paper, we analyze the impact of syntactic structure on the STS 2014 and SICK datasets of STS/SR tasks. We consider three systems which are reported to perform efficiently and effectively on processing syntactic trees using three proposed approaches Syntactic Tree Kernel (Moschitti, 2006), Syntactic Generalization (Ga"
N15-2009,S14-2010,0,\N,Missing
P15-5004,E14-1007,1,0.835082,"Missing"
P15-5004,W13-3830,1,0.898671,"Missing"
P15-5004,S14-2047,1,\N,Missing
P15-5004,jezek-etal-2014-pas,1,\N,Missing
P15-5004,popescu-etal-2014-mapping,1,\N,Missing
popescu-2012-buildind,J01-3001,0,\N,Missing
popescu-etal-2014-mapping,D09-1067,0,\N,Missing
popescu-etal-2014-mapping,N06-2015,1,\N,Missing
popescu-etal-2014-mapping,J03-4003,0,\N,Missing
popescu-etal-2014-mapping,P11-1145,0,\N,Missing
popescu-etal-2014-mapping,P98-1046,1,\N,Missing
popescu-etal-2014-mapping,C98-1046,1,\N,Missing
popescu-etal-2014-mapping,P13-1085,0,\N,Missing
popescu-etal-2014-mapping,C04-1109,0,\N,Missing
popescu-etal-2014-mapping,J04-1003,0,\N,Missing
popescu-etal-2014-mapping,D09-1001,0,\N,Missing
popescu-etal-2014-mapping,W09-3021,0,\N,Missing
popescu-etal-2014-mapping,W11-1003,1,\N,Missing
popescu-etal-2014-mapping,P11-2002,1,\N,Missing
popescu-etal-2014-mapping,N13-1051,0,\N,Missing
popescu-etal-2014-mapping,E14-1007,1,\N,Missing
popescu-etal-2014-mapping,P12-1090,0,\N,Missing
popescu-etal-2014-mapping,W04-0834,0,\N,Missing
popescu-etal-2014-mapping,S01-1029,0,\N,Missing
popescu-etal-2014-mapping,D11-1122,0,\N,Missing
R15-1052,S12-1051,0,0.266425,"the system. To achieve our goal, we divide our research in two main aspects: first, we evaluate the correlation between each single MT metric and the human-annotation scores; and second, we evaluate how different classification algorithms perform using these metrics as features. Introduction Semantic related tasks have become a noticed trend in Natural Language Processing (NLP) community. Particularly, the Semantic Textual Similarity (STS) task has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012 and continuing in SemEval 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). Basically, the task requires to build systems which can compute the similarity degree between two given sentences. The similarity degree is scaled as a real score from 0 (no relevance) to 5 (semantic equivalence). The evaluation is done by computing the correlation between human judgment scores and systems’ predictions by the mean of Pearson correlation method. In contrast, Machine Translation evaluation metrics are designed to assess if the output of a The remainder of this paper is organized as follows: Section 2 presents the"
R15-1052,S13-1004,0,0.057771,"e our goal, we divide our research in two main aspects: first, we evaluate the correlation between each single MT metric and the human-annotation scores; and second, we evaluate how different classification algorithms perform using these metrics as features. Introduction Semantic related tasks have become a noticed trend in Natural Language Processing (NLP) community. Particularly, the Semantic Textual Similarity (STS) task has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012 and continuing in SemEval 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). Basically, the task requires to build systems which can compute the similarity degree between two given sentences. The similarity degree is scaled as a real score from 0 (no relevance) to 5 (semantic equivalence). The evaluation is done by computing the correlation between human judgment scores and systems’ predictions by the mean of Pearson correlation method. In contrast, Machine Translation evaluation metrics are designed to assess if the output of a The remainder of this paper is organized as follows: Section 2 presents the description of differ"
R15-1052,N12-1019,0,0.0134203,"ct MT system is semantically equivalent to a set of reference translations. In SemEval 2012, the system made by (de Souza et al., 2012) and then the system (Barrón Cedeño et al., 2013) in SemEval 2013 introduced the approach of using a set of MT evaluation metrics together with other lexical and syntactic features to predict the semantic similarity scores in STS. Although this approach shows promising results, there was no in-depth analysis on the impact of the evaluation metrics to the overall performance and how each metric behaves on STS data. Moreover, as being inspired by the literature (Madnani et al., 2012) for paraphrase recognition, which obtains the state of art result on the Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004), we decide to analyze the impact of MT evaluation metrics in STS. We present a work to evaluate the hypothesis that automatic evaluation metrics developed for Machine Translation (MT) systems have significant impact on predicting semantic similarity scores in Semantic Textual Similarity (STS) task for English, in light of their usage for paraphrase identification. We show that different metrics may have different behaviors and significance along the semanti"
R15-1052,S14-2010,0,0.107226,"our research in two main aspects: first, we evaluate the correlation between each single MT metric and the human-annotation scores; and second, we evaluate how different classification algorithms perform using these metrics as features. Introduction Semantic related tasks have become a noticed trend in Natural Language Processing (NLP) community. Particularly, the Semantic Textual Similarity (STS) task has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012 and continuing in SemEval 2013, 2014 and 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). Basically, the task requires to build systems which can compute the similarity degree between two given sentences. The similarity degree is scaled as a real score from 0 (no relevance) to 5 (semantic equivalence). The evaluation is done by computing the correlation between human judgment scores and systems’ predictions by the mean of Pearson correlation method. In contrast, Machine Translation evaluation metrics are designed to assess if the output of a The remainder of this paper is organized as follows: Section 2 presents the description of different MT evaluation met"
R15-1052,P02-1040,0,0.0934241,"e calculated based on the alignments between sentence pairs. 3 3.1 Experiments Datasets The STS (for English) dataset consists of several datasets: STS 2012, STS 2013, STS 2014 and STS 2015 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). Each sentence pair is annotated with the semantic similarity score in the scale [0, 1, 2, 3, 4, 5]. Table 1 shows the summary of STS datasets and sources over the years. For training, we use all data in STS 2012, 2013 and 2014; and for testing, we use STS 2015 datasets. BLEU (BiLingual Evaluation Understudy). We use BLEU (Papineni et al., 2002) because it is one of the most commonly used metrics and it has a high reliability. The BLEU metric computes as the amount of n-gram overlap, for different values of n=1,2,3 and 4, between the system output and the reference translation, in our case between sentence pairs. The score is tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. 3.2 Evaluation Methods We use two different evaluation methods to evaluate the impact of the metrics on our training dataset, (1) the Pearson correlation between the metri"
R15-1052,2006.amta-papers.25,0,0.0269808,"might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. 3.2 Evaluation Methods We use two different evaluation methods to evaluate the impact of the metrics on our training dataset, (1) the Pearson correlation between the metric outputs and the gold standards which is the official evaluation method used in STS task; and (2) the RELIEF (Robnik-Sikonja and Kononenko, 1997) analysis implemented in WEKA (Hall et al., 2009) to estimate the quality of MT evaluation metric output in regression. TER (Translation Error Rate). We use the 0.7.25 version of TER (Snover et al., 2006). TER computes the number of edits needed to &quot;fix&quot; the translation output so that it matches the reference. TER differs from word error rate (WER) in which it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 3.3 Settings Firstly, we employ the four metrics to compute the semantic similarity between given sentences on the training dataset. We use the default configuration for all metrics, except the &quot;-norm&quot; option for METEOR that tokenizes and normalizes punctuation and lowercase, as suggested in its documentation; and the &quot;-c&quot; option fo"
R15-1088,S12-1051,0,0.0256895,"the expressions) are capitalized. Given the above pair of formulas, the unification computes their most general specialization camera(zoom(digital), beginner), while the antiunification computes their most specific generalization, camera(zoom(AnyZoom), AnyUser). At syntactic level, we have generalization of two noun phrases as: {NN-camera, PRP-with, [digital], NN-zoom [for beginners]}. Then, the expressions in square brackets are eliminated since they occur in one expression and do not occur in 3.1 Datasets The STS dataset (English STS) consists of several datasets in STS 2012, 2013 and 2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Each sentence pair is annotated the semantic similarity score in the scale [0, 1, 2, 3, 4, 5]. Table 2 shows the 5 https://code.google.com/p/relevance-based-on-parsetrees 6 https://code.google.com/p/distributed-tree-kernels 690 year 2012 2012 2012 2012 2012 2013 2013 2013 2013 2014 2014 2014 2014 2014 2014 dataset MSRpar MSRvid OnWN SMTnews SMTeuroparl headlines FNWN OnWN SMT headlines OnWN Deft-forum Deft-news Images Tweet-news pairs 1500 1500 750 750 750 750 189 561 750 750 750 450 300 750 750 source newswire video descriptions OntoNotes, WordNet"
R15-1088,S13-1004,0,0.0136813,"capitalized. Given the above pair of formulas, the unification computes their most general specialization camera(zoom(digital), beginner), while the antiunification computes their most specific generalization, camera(zoom(AnyZoom), AnyUser). At syntactic level, we have generalization of two noun phrases as: {NN-camera, PRP-with, [digital], NN-zoom [for beginners]}. Then, the expressions in square brackets are eliminated since they occur in one expression and do not occur in 3.1 Datasets The STS dataset (English STS) consists of several datasets in STS 2012, 2013 and 2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). Each sentence pair is annotated the semantic similarity score in the scale [0, 1, 2, 3, 4, 5]. Table 2 shows the 5 https://code.google.com/p/relevance-based-on-parsetrees 6 https://code.google.com/p/distributed-tree-kernels 690 year 2012 2012 2012 2012 2012 2013 2013 2013 2013 2014 2014 2014 2014 2014 2014 dataset MSRpar MSRvid OnWN SMTnews SMTeuroparl headlines FNWN OnWN SMT headlines OnWN Deft-forum Deft-news Images Tweet-news pairs 1500 1500 750 750 750 750 189 561 750 750 750 450 300 750 750 source newswire video descriptions OntoNotes, WordNet glosses Machine Trans"
R15-1088,S12-1060,0,0.0396532,"Missing"
R15-1088,S13-1006,0,0.0248354,"Missing"
R15-1088,N15-2009,1,0.753141,"et al., 2003), LSA (Landauer et al., 1998), ESA (Gabrilovich and Markovitch, 2007), and word/n-grams overlap as main features to train supervised models, or deploy unsupervised word-alignment metrics to align two given texts. In common sense, syntactic structure may keep a crucial part for human being to understand the meaning of a given text. Thus, it also may help to identify the semantic equivalence between two given texts. However, in the STS task, very few systems provide evidence of the contribution of syntactic structure in its overall performance. Following the work in the literature (Vo and Popescu, 2015), we would like to make a deeper study and analysis whose contribution consists of two folds, on the STS 2012, 2013, and 2014 datasets (1) we assess the impact of syntactic structure towards the overall performance, and (2) analyze the behavior of syntactic structure in each score range of STS semantic scale. We consider three methods reported to perform efficiently and effectively on processing syntactic trees using three proposed approaches Syntactic Tree Kernel (Moschitti, 2006), Syntactic Generalization (Galitsky, 2013) and Distributed Tree Kernel (Zanzotto and Dell’Arciprete, 2012). The r"
R15-1088,U06-1019,0,0.0196083,"2) which is ranked 2nd at STS 2012 used two methods to learn the syntactic structure for computing the semantic similarity between given sentences. (1) Syntactic Roles Similarity uses dependency parsing to identify the lemmas with the corresponding syntactic roles in the two sentences. Given two sentences, the similarity of words or phrases that have the same syntactic roles may indicate their overall semantic similarity (Oliva et al., 2011). (2) Syntactic Dependencies Overlap computes the overlap of the dependency relations between two given sentences. A similar measure has been proposed in (Wan et al., 2006) in which if two syntactic dependencies share the same dependency type, governing lemma and dependent lemma, they are considered equal. At STS 2013, the iKernels system (Severyn et 6 Conclusions and Future Work In this paper, we deploy three different approaches to exploit and evaluate the impact of syntactic structure in the STS task. We use a bag-of-word baseline which is the official baseline of STS task for the evaluation. We also evaluate the contribution of each syntactic structure approach integrated with the bag-of-word approach in the baseline. From our observation, for the time being"
R15-1088,S14-2010,0,\N,Missing
S07-1040,W97-0109,0,\N,Missing
S07-1040,S07-1005,0,\N,Missing
S07-1040,J01-3001,0,\N,Missing
S07-1040,P97-1009,0,\N,Missing
S07-1040,W06-2106,0,\N,Missing
S07-1040,W93-0102,0,\N,Missing
S07-1041,S07-1012,0,0.549591,"Missing"
S07-1041,W06-0504,1,\N,Missing
S07-1041,P98-1012,0,\N,Missing
S07-1041,C98-1012,0,\N,Missing
S14-2046,P05-1045,0,0.00507464,"or otherwise). We implemented a simple algorithm to extract the LCS between two given texts. Then we divided the LCS length by the product of normalized lengths of two given texts and used it as a feature. 4.1 5.2 NER aims at identifying and classifying entities in a text with respect to a predefined set of categories such as person names, organizations, locations, time expressions, quantities, monetary values, percentages, etc. By exploring the training set, we observed that there are lot of texts in this task containing named entities. We deployed the Stanford Named Entity Recognizer tool (Finkel et al., 2005) to extract the similar and overlapping named entities between two given texts. Then we divided the number of similar/overlapping named entities by the sum length of two given texts. Analysis Before and After LCS After extracting the LCS between two given texts, we also considered the similarity for the parts before and after the LCS. The similarity between the text portions before and after the LSC has been obtained by means of the Lin measure and the Levenshtein distance. 5 6 Other Features Support Vector Machines (SVMs) Support vector machine (SVM) (Cortes and Vapnik, 1995) is a type of sup"
S14-2046,S13-1030,0,0.0289834,"(Fellbaum, 1999) is a lexical database for the English language in which words are grouped into sets of synonyms (namely synsets, 285 be very similar/related, otherwise dissimilar. We trained our LSA model on the British National Corpus (BNC) 1 and Wikipedia 2 corpora. 4 maximum 500 topics (20, 50, 100, 150, 200, 250, 300, 350, 400, 450 and 500). From the proportion vectors (distribution of documents over topics) of given texts, we applied three different measures to compute the distance between each pair of texts, which are Cosine similarity, Kullback-Leibler and Jensen-Shannon divergences (Gella et al., 2013). String Similarity Measures The Longest Common Substring (LCS) is the longest string in common between two or more strings. Two given texts are considered similar if they are overlapping/covering each other (e.g sentence 1 covers a part of sentence 2, or otherwise). We implemented a simple algorithm to extract the LCS between two given texts. Then we divided the LCS length by the product of normalized lengths of two given texts and used it as a feature. 4.1 5.2 NER aims at identifying and classifying entities in a text with respect to a predefined set of categories such as person names, organ"
S14-2046,S14-2003,0,0.0253575,"between given texts has drawn much attention from the Natural Language Processing community. Especially, the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc. In this paper, we, the FBK-TR team, describe our system participating in Task 3 &quot;Cross-Level Semantic Similarity&quot;, at SemEval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task. 1 At SemEval 2014, the Task 3 &quot;Cross-Level Semantic Similarity&quot; (Jurgens et al., 2014) is to evaluate the semantic similarity across different sizes of texts, in particular, a larger-sized text is compared to a smaller-sized one. The task consists of four types of semantic similarity comparison: paragraph to sentence, sentence to phrase, phrase to word, and word to sense. The degree of similarity ranges from 0 (different meanings) to 4 (similar meanings). For evaluation, systems were evaluated, first, within comparison type and second, across all comparison types. Two methods are used to evaluate between system outputs and gold standard (human annotation), which are Pearson cor"
S14-2046,S12-1060,0,\N,Missing
S14-2046,N04-3012,0,\N,Missing
S14-2047,W13-0117,1,0.746815,"have a different parsing. Each words is replaced with their possible SUMO attributes (Niles and Pease, 2003). Only the following Stanford dependencies are retained as valid [n, nsub]sbj, [d,i,p]obj, prep, [x,c]comp. We considered only the most frequent occurrences of such patterns for each verb. To cluster into a single SDP pattern, all patterns that are sense auto-determinative, we used the OntoNotes (Hovy et al., 2006) and CPA (Hanks, 2008) lexica. Inside each cluster, we searched for the most general hypernyms for each syntactic slot such that there are no common patterns between clusters (Popescu, 2013). However, the patterns thus obtained are not sufficient enough for the task. Some expressions may be the paraFigure 2: Algorithm for computing entailment. 3.1 Entailment on Affirmative Sentences Affirmative sentences use three types of entailment patterns. The switch baseline and hyponym patterns works in this way: If two sentences are matched by the same SDP, and the difference between them is that the second one contains a hypernym on the same syntactic position, then the first one is entailed by the second (i.e. ENTAILMENT). If the two SDPs are such that the difference between them is that"
S14-2047,N06-2015,0,0.0310402,"ontaining at maximum two finite verbs from BNC and Annotated English Gigaword. We parsed this corpus with the Stanford parser, discarding the sentences from the Annotated English Gigaword which have a different parsing. Each words is replaced with their possible SUMO attributes (Niles and Pease, 2003). Only the following Stanford dependencies are retained as valid [n, nsub]sbj, [d,i,p]obj, prep, [x,c]comp. We considered only the most frequent occurrences of such patterns for each verb. To cluster into a single SDP pattern, all patterns that are sense auto-determinative, we used the OntoNotes (Hovy et al., 2006) and CPA (Hanks, 2008) lexica. Inside each cluster, we searched for the most general hypernyms for each syntactic slot such that there are no common patterns between clusters (Popescu, 2013). However, the patterns thus obtained are not sufficient enough for the task. Some expressions may be the paraFigure 2: Algorithm for computing entailment. 3.1 Entailment on Affirmative Sentences Affirmative sentences use three types of entailment patterns. The switch baseline and hyponym patterns works in this way: If two sentences are matched by the same SDP, and the difference between them is that the se"
S14-2047,S14-2001,0,0.0338215,"r University of Trento Trento, Italy ngoc@fbk.eu Octavian Popescu Tommaso Caselli Fondazione Bruno Kessler TrentoRISE Trento, Italy Trento, Italy popescu@fbk.eu t.caselli@trentorise.eu Abstract At SemEval 2014, the Task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment&quot; (Marelli et al., 2014a) primarily aimed at evaluating Compositional Distributional Semantic Models (CDSMs) of meaning over two subtasks, namely semantic relatedness and textual entailment (ENTAILMENT, CONTRADICTION and NEUTRAL), over pairs of sentences (Marelli et al., 2014b). Concerning the relatedness subtask, the system outputs are evaluated against gold standard ratings in two ways, using Pearson correlation and Spearman’s rank correlation (rho). The Pearson correlation is used for evaluating and ranking the participating systems. Similarly, for the textual entailment subtask, system outputs are evaluated against a gold standard rating with respect to accuracy. This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences throug"
S14-2047,marelli-etal-2014-sick,0,0.0435581,"r University of Trento Trento, Italy ngoc@fbk.eu Octavian Popescu Tommaso Caselli Fondazione Bruno Kessler TrentoRISE Trento, Italy Trento, Italy popescu@fbk.eu t.caselli@trentorise.eu Abstract At SemEval 2014, the Task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Entailment&quot; (Marelli et al., 2014a) primarily aimed at evaluating Compositional Distributional Semantic Models (CDSMs) of meaning over two subtasks, namely semantic relatedness and textual entailment (ENTAILMENT, CONTRADICTION and NEUTRAL), over pairs of sentences (Marelli et al., 2014b). Concerning the relatedness subtask, the system outputs are evaluated against gold standard ratings in two ways, using Pearson correlation and Spearman’s rank correlation (rho). The Pearson correlation is used for evaluating and ranking the participating systems. Similarly, for the textual entailment subtask, system outputs are evaluated against a gold standard rating with respect to accuracy. This paper reports the description and scores of our system, FBK-TR, which participated at the SemEval 2014 task #1 &quot;Evaluation of Compositional Distributional Semantic Models on Full Sentences throug"
S14-2047,N04-3012,0,0.0647765,"erning the Semantic Relatedness subtask our SVM system is built on different linguistic features, ranging from relatedness at the lexical level (WordNet based measures, Wikipedia relatedness and Latent Semantic Analysis), to sentence level, including topic modeling based on Latent Dirichlet allocation (LDA) and string similarity (Longest Common Substring). 2.1 Lexical Features At the lexical level, we built a simple, yet effective Semantic Word Relatedness model, which consists of 3 components: WordNet similarity (based on the Lin measure as implemented in Pedersen package WordNet:Similarity (Pedersen et al., 2004), Wikipedia relatedness (as provided by the Wikipedia Miner package (Milne and Witten, 2013)), and Latent Semantic Analysis (Landauer et al., 1998), with a model trained on the British National Corpus (BNC) 1 and Wikipedia. At this level of analysis, we concentrated only on the best matched (lemma) pairs of content words, i.e. Noun-Noun, Verb-Verb, extracted from each sentence pair. The content words have been automatically extracted by means of part-of-speech tagging (TreeTagger (Schmid, 1994)) and lemmatization. For words which are not present in WordNet, the relatedness score has been obtai"
S15-2005,P09-1053,0,0.244205,"exts, sometime reduced to a single word and with very poor syntactic structure. We split the original dataset into two subsets, in which one is composed by sentence pairs and the other one is composed by pairs with POS and named entity tags. Because of the simple structure of given datasets, after undergoing the preprocessing, we decide to focus on exploiting the lexical and string similarity information, rather than syntactic information. 2.2 Lexical and String Similarity Firstly, for computing the lexical and string similarity between two sentences, we take advantage from the task baseline (Das and Smith, 2009) which is a system using a logistic regression model with eighteen features based on n-grams. This baseline system uses precision, recall and F1-score of 1-gram, 2-grams and 3-grams of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract these eighteen features from baseline system, without modifications, to use in our classification model. 2.3 Machine Translation Evaluation Metrics Other than similarity features, we also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition"
S15-2005,W14-3348,0,0.0178887,"hem to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not make any improvement on system performance, hence, we exclude it. 30 2.3.1 METEOR (Metric for Evaluation of Translation with Explicit ORdering) We use the latest version of METEOR (Denkowski and Lavie, 2014) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the ""norm"" option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.1 We compute the word alignment scores on sentences and on sentences with part-of-speech and named entity tags, as our idea is that if two sentences are similar, their tagged version also should be similar. 2.3.2 BLEU (Bilingual Evaluation Understudy) We use another metric for machine translation BLEU (Papineni et"
S15-2005,C04-1051,0,0.0244148,"odel with eighteen features based on n-grams. This baseline system uses precision, recall and F1-score of 1-gram, 2-grams and 3-grams of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract these eighteen features from baseline system, without modifications, to use in our classification model. 2.3 Machine Translation Evaluation Metrics Other than similarity features, we also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not make any improvement on system performance, hence, we exclude it. 30 2.3.1 METEOR (Metric for Eval"
S15-2005,N12-1019,0,0.0139816,"tage from the task baseline (Das and Smith, 2009) which is a system using a logistic regression model with eighteen features based on n-grams. This baseline system uses precision, recall and F1-score of 1-gram, 2-grams and 3-grams of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract these eighteen features from baseline system, without modifications, to use in our classification model. 2.3 Machine Translation Evaluation Metrics Other than similarity features, we also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we use two metrics for word alignment in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not mak"
S15-2005,P14-5008,0,0.0217598,"for machine translation BLEU (Papineni et al., 2002) that is one of the most commonly used and because of that has an high reliability. It is computed as the amount of n-gram overlap, for different values of n=1,2,3, and 4, between the system output and the reference translation, in our case between sentence pairs. The score is tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. 2.4 Edit Distance We use the edit distance between sentences as a feature; for that we used the Excitement Open Platform (EOP) (Magnini et al., 2014). To obtain the edit distance, we use EDITS Entailment Decision Algorithm (EDITS EDA), this algorithm classifies the pairs on the base of their edit distance, we take only this one without considering the entailment or not entailment decision. We configure the system to use lemmas and synonyms as identical words to compute sentence distance, the system normalizes the score on the number of token of the shortest sentence. We choose this configuration because it returns the best performance evaluated on training and development data. 2.5 Classification Algorithms We build two systems for the tas"
S15-2005,P02-1040,0,0.102892,"avie, 2014) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the ""norm"" option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.1 We compute the word alignment scores on sentences and on sentences with part-of-speech and named entity tags, as our idea is that if two sentences are similar, their tagged version also should be similar. 2.3.2 BLEU (Bilingual Evaluation Understudy) We use another metric for machine translation BLEU (Papineni et al., 2002) that is one of the most commonly used and because of that has an high reliability. It is computed as the amount of n-gram overlap, for different values of n=1,2,3, and 4, between the system output and the reference translation, in our case between sentence pairs. The score is tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. 2.4 Edit Distance We use the edit distance between sentences as a feature; for that we used the Excitement Open Platform (EOP) (Magnini et al., 2014). To obtain the edit distance,"
S15-2005,S15-2001,0,0.0226208,"search, T.J. Watson Yorktown, US o.popescu@us.ibm.com 2 Introduction Paraphrase identification/recognition is an important task that can be used as a feature to improve many other NLP tasks as Information Retrieval, Machine Translation Evaluation, Text Summarization, Question and Answering, and others. Besides this, analyzing social data like tweets of social network Twitter is a field of growing interest for different purposes. The interesting combination of these two tasks was brought forward as Shared Task #1 in the SemEval 2015 campaign for ""Paraphrase and Semantic Similarity in Twitter"" (Xu et al., 2015). In this task, given a set of sentence pairs, which are not necessarily full tweets, their topic and the same sentences with partof-speech and named entity tags; participating system is required to predict for each pair of sentences is a paraphrase (Subtask 1) and optionally compute a graded score between 0 and 1 for their semantic equivalence (Subtask 2). We participate in this shared System Description In order to build our system, we extract and select several different linguistic features ranging from simple (word/string similarity, edit distance) to more complex ones (machine translation"
S15-2005,J13-3001,0,\N,Missing
S15-2018,S12-1051,0,0.039712,"resulting in different sets of features. The results evaluated on both STS 2014 and 2015 datasets prove our hypothesis of building a STS system taking into consideration of syntactic information. We outperform the best system on STS 2014 datasets and achieve a very competitive result to the best system on STS 2015 datasets. 1 Introduction Semantic related tasks have been a noticed trend in Natural Language Processing (NLP) community. Particularly, the task Semantic Textual Similarity (STS) has captured a huge attention in the NLP community despite being recently introduced since SemEval 2012 (Agirre et al., 2012). Basically, the task requires to build systems which can compute the similarity degree between two given sentences. The similarity degree is scaled as a real score from 0 (no relevance) to 5 (semantic equivalence). The evaluation is done by computing the correlation between human judgment scores and system scores by the mean of Pearson correlation method. At SemEval 2015, Task #2 “Semantic Textual Similarity (STS)”, English STS subtask (Agirre et al., 2015) evaluates participating systems on five test Octavian Popescu IBM Research, T.J. Watson Yorktown, US o.popescu@us.ibm.com datasets: image"
S15-2018,W05-0909,0,0.00952289,"nderstanding of computational linguistics or machine learning is required. We apply the tool on the STS datasets to compute the similarity of syntactic structure of sentence pairs. 2.4 Further Features We also deploy other features which also may help in identifying the semantic similarity degree between two given sentences, such as word alignment in machine translation evaluation metric and the vector space model Weighted Matrix Factorization (WMF) for pairwise similarity. 2.4.1 Machine Translation Evaluation Metric METEOR METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005) is an automatic metric for machine translation evaluation, which consists of two major components: a flexible monolingual word aligner and a scorer. For machine translation evaluation, hypothesis sentences are aligned to reference sentences. Alignments are then scored to produce sentence and corpus level scores. We use this word alignment feature 6 https://opennlp.apache.org to learn the similarity between words, phrases in two given texts in case of different orders. 2.4.2 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robus"
S15-2018,S12-1059,0,0.094826,"es and system scores by the mean of Pearson correlation method. At SemEval 2015, Task #2 “Semantic Textual Similarity (STS)”, English STS subtask (Agirre et al., 2015) evaluates participating systems on five test Octavian Popescu IBM Research, T.J. Watson Yorktown, US o.popescu@us.ibm.com datasets: image description (image), news headlines (headlines), student answers paired with reference answers (answers-students), answers to questions posted in stach exchange forums (answers-forum), and English discussion forum data exhibiting commited belief (belief ). As being inspired by the UKP system (Bär et al., 2012), which was the best system in STS 2012, we build a supervised system on top of it. Our system adopts some word and string similarity features in UKP, such as string similarity, character/word n-grams, and pairwise similarity; however, we also add other distinguished features, like syntactic structure information, word alignment and semantic word similarity. As a result, our team, FBKHLT, submitted three runs and achieve very competitive results in the top-tier systems of the task. The remainder of this paper is organized as follows: Section 2 presents the System Description, Section 3 describ"
S15-2018,C10-1005,0,0.0150296,", 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources 103 Syntactic Tree Kernel Syntactic Tree Kernel (Moschitti, 2006) is a tree kernels approach to learn the syntact"
S15-2018,P12-1091,0,0.110311,"or Evaluation of Translation with Explicit ORdering) (Banerjee and Lavie, 2005) is an automatic metric for machine translation evaluation, which consists of two major components: a flexible monolingual word aligner and a scorer. For machine translation evaluation, hypothesis sentences are aligned to reference sentences. Alignments are then scored to produce sentence and corpus level scores. We use this word alignment feature 6 https://opennlp.apache.org to learn the similarity between words, phrases in two given texts in case of different orders. 2.4.2 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on average), the missing words, a feature that LSA/LDA typically overlooks, is explicitly modeled. We use the pipeline to compute the similarity score between texts. 3 Experiment Settings We generate and select 25 optimal features, ranging from lexical level to string level and syntactic level. We deploy the machine learning toolkit WEKA (Hall et al., 2009) for learning a regression model (GaussianProcesses) to predict"
S15-2018,P03-1054,0,0.0183186,"s We exploit the syntactic structure information by the mean of three different toolkits: Syntactic Tree Kernel, Distributed Tree Kernel and Syntactic Generalization. We describe how each toolkit is used to learn and extract the syntactic structure information from texts to be used in our STS system. 2.3.1 Figure 1: System Overview. ponents connect and work together. 2.1 Data Preprocessing The input data undergoes the data preprocessing in which we use Tree Tagger (Schmid, 1994) to perform tokenization, lemmatization, and Part-ofSpeech (POS) tagging. On the other hand, we use Stanford Parser (Klein and Manning, 2003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon"
S15-2018,W01-0515,0,0.0927955,"003) to obtain the dependency parsing from given sentences. 2.2 Word and String Similarity Features We adopt some word and string similarity features from the UKP system (Bär et al., 2012), which are briefly described as follows: • String Similarity: we use Longest Common Substring (Gusfield, 1997), Longest Common Subsequence (Allison and Dix, 1986) and Greedy String Tiling (Wise, 1996) measures. • Character/Word n-grams: we compare character n-grams (Barrón-Cedeno et al., 2010) with the variance n=2, 3, ..., 15. In contrast, we compare the word n-grams using Jaccard coefficient done by Lyon (Lyon et al., 2001) and containment measure (Broder, 1997) with the variance of n=1, 2, 3, and 4. • Semantic Word Similarity: we use the pairwise similarity algorithm by Resnik (Resnik, 1995) on WordNet (Fellbaum, 1998), and the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) which is constructed by two lexical semantic resources 103 Syntactic Tree Kernel Syntactic Tree Kernel (Moschitti, 2006) is a tree kernels approach to learn the syntactic structure from syntactic parsing information, particularly, the Partial Tree (PT) kernel is proposed as a new convolution kernel to"
S15-2018,S15-2045,0,\N,Missing
S15-2041,W14-3348,0,0.014104,"ranslation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine 1 2 https://code.google.com/p/relevance-based-on-parse-trees/ https://opennlp.apache.org 232 translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &quot;norm&quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.3 We compute the word alignment scores between questions and comments. 2.4 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on"
S15-2041,C04-1051,0,0.0137536,"on of constituency parse trees via chunking. Each type of phrases (NP, VP, PRP etc.) will be aligned and subject to generalization. It uses the OpenNLP system to derive constituent trees for generalization (chunker and parser).2 As it is an unsupervised approach, we apply the tool directly to the preprocessed texts to compute the similarity of syntactic structure of sentence pairs. 2.3 Machine Learning Evaluation Metric METEOR We also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine 1 2 https://code.google.com/p/relevance-based-on-parse-trees/ https://opennlp.apache.org 232 translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrase"
S15-2041,P12-1091,0,0.0126943,"but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website, using only the &quot;norm&quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.3 We compute the word alignment scores between questions and comments. 2.4 Weighted Matrix Factorization (WMF) WMF (Guo and Diab, 2012) is a dimension reduction model to extract nuanced and robust latent vectors for short texts/sentences. To overcome the sparsity problem in short texts/sentences (e.g. 10 words on average), the missing words, a feature that Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) typically overlook, is explicitly modeled. We use the pipeline to compute the similarity scores for question-comment pairs.4 2.5 User Overlapping We extract a simple binary feature focused on comment’s author. We suppose that question’s author is not usually as same as comment’s author, so if a question ha"
S15-2041,N12-1019,0,0.0242121,"entences by finding a set of maximal common subtree for a pair of parse trees, using representation of constituency parse trees via chunking. Each type of phrases (NP, VP, PRP etc.) will be aligned and subject to generalization. It uses the OpenNLP system to derive constituent trees for generalization (chunker and parser).2 As it is an unsupervised approach, we apply the tool directly to the preprocessed texts to compute the similarity of syntactic structure of sentence pairs. 2.3 Machine Learning Evaluation Metric METEOR We also use evaluation metrics for machine translation as suggested in (Madnani et al., 2012) for paraphrase recognition on Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004). In machine 1 2 https://code.google.com/p/relevance-based-on-parse-trees/ https://opennlp.apache.org 232 translation, the evaluation metric scores the hypotheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. We use the latest version of METEOR (Denkowski and Lavie, 2014) that finds alignments"
S15-2041,S15-2047,0,0.0853149,"Missing"
S15-2041,S15-2018,1,0.775713,"on evaluation metrics and task specific techniques could increase the accuracy of our system. In this paper, we outline our method and present the results for the answer selection task; the paper is organized as follows: Section 2 presents the System Description, Section 3 describes the Experiment Settings, Section 4 reports the Evaluations, Section 5 is the Error Analysis and finally, Section 6 presents the Conclusions and Future Work. 2 System Description In order to build our system, we extract and adopt several different linguistic features from a Semantic Textual Similarity (STS) system (Vo et al., 2015) and then consolidate them by a multiclass classifier. Different features can be used independently or together with others to measure the semantic similarity and recognize the paraphrase of a given sentence pair as well as to evaluate the significance of each feature to the accuracy of system’s predictions. Hence, the system is expandable and scalable for adopting more useful features aiming for improving the accuracy. 231 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 231–235, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational L"
S15-2053,W99-0201,0,0.0608554,"across subtasks is the way Precision and Recall are defined. F1verb = 2 × Precisionverb × Recallverb Precisionverb + Recallverb Pnverb F1verbi ScoreTask = i=1 nverb (1) Subtask 1. Equation 2 illustrates that Precision and Recall are computed on all tags, both syntactic and semantic. To count as correct, tags had to be set on the same token as in the gold standard. Correct tags Retrieved tags Correct tags Recall = Reference tags Precision = (2) Subtask 2. Clustering is known to be difficult to evaluate. Subtask 2 used the B-cubed definition of Precision and Recall, first used for coreference (Bagga and Baldwin, 1999) and later extended to cluster evaluation (Amig´o et al., 2009). Both measures are averages of the precision and recall over all instances. To calculate the precision of each instance we count all correct pairs associated with this instance and divide by the number of actual pairs in the candidate cluster that the instance belongs to. Recall is computed by interchanging Gold and Candidate clusterings (Eq. 3). Pairsi in Candidate found in Gold Pairsi in Candidate Pairsi in Gold found in Candidate Recalli = Pairsi in Gold (3) Precisioni = Subtask 3. This task was evaluated as a slot-filling exer"
S15-2053,S07-1018,0,0.0786767,"Missing"
S15-2053,W13-3821,1,0.796383,"Missing"
S15-2053,W06-2920,0,0.0206406,"er resources. This also implied that the dataset would be constructed so as to make it possible for systems to generalize from the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis. 3.1 arguments of the verb. The subtask is similar to Semantic Role Labelling (Carreras and Marquez, 2004) that arguments will be identified in the dependency parsing paradigm (Buchholz and Marsi, 2006), using head words instead of phrases. The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology. In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to [[Rule]]. The expected output is represented in XML format in Example (2). (1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replaced it with inheritance tax. (2) In 1981 the Conservative &lt;entity syn=‘subj’ sem=‘Institution’&gt; g"
S15-2053,W04-2412,0,0.023445,"approaches, maybe using patterns learnt in an unsupervised manner from very large corpora and other resources. This also implied that the dataset would be constructed so as to make it possible for systems to generalize from the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis. 3.1 arguments of the verb. The subtask is similar to Semantic Role Labelling (Carreras and Marquez, 2004) that arguments will be identified in the dependency parsing paradigm (Buchholz and Marsi, 2006), using head words instead of phrases. The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology. In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to [[Rule]]. The expected output is represented in XML format in Example (2). (1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replac"
S15-2053,P05-1022,0,0.0882373,"old 1,008 777 580 438 308 303 289 192 182 115 CMILLS 0.564 0.659 0.593 0.450 0.545 0.668 0.621 0.410 0.441 0.421 FANTASY 0.694 0.792 0.770 0.479 0.418 0.830 0.517 0.276 0.531 0.594 BLCUNLP 0.739 0.777 0.691 0.393 0.702 0.771 0.736 0.373 0.483 0.526 baseline 0.815 0.783 0.724 0.408 0.729 0.811 0.845 0.211 0.461 0.506 Table 6: Detailed scores for subtask 1 (10 most frequent categories). Team baseline FANTASY BLCUNLP CMILLS Score 0.624 0.589 0.530 0.516 representations to predict the output of each layer. The baseline system was a rule-based system taking as input the output of the BLLIP parser (Charniak and Johnson, 2005), and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set. Table 5: Official scores for subtask 1. subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP (Feng et al., 2015) used the Stanford CoreNLP package10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical"
S15-2053,E12-1085,1,0.868894,"Missing"
S15-2053,cinkova-etal-2012-database,1,0.891442,"Missing"
S15-2053,W13-3826,1,0.504171,"Missing"
S15-2053,el-maarouf-etal-2014-disambiguating,1,0.46533,"Missing"
S15-2053,S15-2054,0,0.493221,"e FANTASY BLCUNLP CMILLS Score 0.624 0.589 0.530 0.516 representations to predict the output of each layer. The baseline system was a rule-based system taking as input the output of the BLLIP parser (Charniak and Johnson, 2005), and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set. Table 5: Official scores for subtask 1. subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP (Feng et al., 2015) used the Stanford CoreNLP package10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical classifier. CMILLS (Mills and Levow, 2015) used three models to solve the task: one for argument detection, and the other two for each layer. Argument detection and syntactic tagging were performed using a MaxEnt supervised classifier, while the last was based on heuristics. CMILLS also reported the use of an external resource, the enTentTen12 (Jakub´ıcˇ ek et al., 2013) corpus available"
S15-2053,J02-3001,0,0.262472,"to automated reasoning. Since its birth, SEMEVAL (or Most lexical resources explored to date have had only limited success, on either front. The most obvious candidates—published dictionaries and WordNets—look like they might support the first task, but are very limited in what they offer to the second. FrameNet moved the game forward a stage. Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation. Papers such as (Gildea and Jurafsky, 2002) looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction (Baker et al., 2007) and in 2010, one on Linking Events and Their Participants (Ruppenhofer et al., 2010). While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target. • It is organised around frames, rather than words, so inevitably its priority is to give a co315 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 315–324, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Lingu"
S15-2053,C12-1073,1,0.909639,"Missing"
S15-2053,S15-2076,0,0.0972218,"in a supervised setting to predict first the syntactic tags, and then the semantic tags. The team used features from the MST parser11 , as well as Stanford CoreNLP for NE, Wordnet12 , they also applied word embedding 10 http://nlp.stanford.edu/software/ corenlp.shtml 11 http://www.seas.upenn.edu/˜strctlrn/ MSTParser/MSTParser.html 12 http://wordnet.princeton.edu/ 5.2 Subtask 2 As opposed to subtask 1, systems in subtask 2 used very few semantic and syntactic resources. BOB90 used a supervised approach to tackle the clustering problem. The main features used were preposition analyses. DULUTH (Pedersen, 2015) used an unsupervised approach and focused on lexical similarity (both first and second order representations) based on unigrams and bigrams (see SenseClusters13 ). The number of clusters was predicted on the basis of the best value for the clustering criterion function. The team also performed some corpus pre-processing, like conversion to lower case and conversion of all numeric values to a string. The baseline system clusters everything together, so its score depends on the distribution of patterns: the more a pattern covers all instances of the data (majority class), the higher the baselin"
S15-2053,W13-0117,1,0.550483,"Missing"
S15-2053,W04-1908,0,0.0809889,"Missing"
S15-2053,C04-1133,0,\N,Missing
S15-2147,E14-4004,0,0.264093,"Section 5 we discuss the main properties of the submitted systems and their results. The paper ends with a substantial section on conclusion and main future research direction in DTE. 2 Related Work The availability of large time annotated corpora like Google N-gram open the perspective of a new field of the research which focuses on the distribution of the linguistics elements in certain periods. (Popescu and Strapparava, 2014; Popescu and Strapparava, 2013) showed how such corpora can be used to infer transition periods between epoch with specific characteristics. A ground breaking paper, (Niculae et al., 2014) focuses on historical documents in three languages, English, Portuguese and Romanian. The paper shows how statistical method can be used to predict the date when the documents have been created. The similarity of the ideas in the present task and their paper, although developed in completely autonomy, prove that there is indeed a major interest in building diachronic systems and that the time is high for this task. We believe that there is a lot to do in this emergent field. 3 Task Description In this section we present the main motivations for a diachronic task and in particular, we focus on"
S15-2147,I13-1040,1,0.305261,"f specific linguistics variability in a certain epoch, location, social class etc. The statistical methods are able to discover correlations and linguistic provable evidence of language change at all levels: morphological, syntactical, semantic and discourse. It would be physically impossible for a human, or a team of humans for what it matters, to analyze and corroborate the data from hundreds of gigabytes of data and find all the relevant differences. Looking at the distribution of words across timeline, salient periods, with statistically non-random behavior, can be automatically inferred (Popescu and Strapparava, 2013). The structure of such periods, or epochs, are by far more complex than what it could be manually performed. From a practical point of view, diachronic systems have a wide range of applications from emergent fields such as computational forensics, computational journalism to more traditional tasks, such as discourse similarity, sense shifting, readability and narrative frameworks, etc. The paper is organized as follow: in the next section we review the relevant literature. In Section 3 we present the main motivation for the DTE task and the three subtasks with their specific corpora. In Secti"
W06-0504,W04-3221,0,0.0225213,"more general Ontology Population from text (cf. Buitelaar et al. 2005); in particular, mentions are well defined and there are systems for automatic mention recognition. Although there is no univocally accepted definition for the OP task, a useful approximation has been suggested by (Bontcheva and Cunningham, 2005) as Ontology Driven Information Extraction with the goal of extracting and classifying instances of concepts and relations defined in a Ontology, in place of filling a template. A similar task has been approached in a variety of perspectives, including term clustering (Lin, 1998 and Almuhareb and Poesio, 2004) and term categorization (Avancini et al. 2003). A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (Velardi et al. 2005). However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task (see Tanev and Magnini, 2006), they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community. Second, there already exist annotated data"
W06-0504,P98-2127,0,0.00489203,"espect to the more general Ontology Population from text (cf. Buitelaar et al. 2005); in particular, mentions are well defined and there are systems for automatic mention recognition. Although there is no univocally accepted definition for the OP task, a useful approximation has been suggested by (Bontcheva and Cunningham, 2005) as Ontology Driven Information Extraction with the goal of extracting and classifying instances of concepts and relations defined in a Ontology, in place of filling a template. A similar task has been approached in a variety of perspectives, including term clustering (Lin, 1998 and Almuhareb and Poesio, 2004) and term categorization (Avancini et al. 2003). A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (Velardi et al. 2005). However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task (see Tanev and Magnini, 2006), they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community. Second, the"
W06-0504,magnini-etal-2006-cab,0,0.0259059,"Missing"
W06-0504,E06-1003,1,0.677523,"assifying instances of concepts and relations defined in a Ontology, in place of filling a template. A similar task has been approached in a variety of perspectives, including term clustering (Lin, 1998 and Almuhareb and Poesio, 2004) and term categorization (Avancini et al. 2003). A rather different task is Ontology Learning, where new concepts and relations are supposed to be acquired, with the consequence of changing the definition of the Ontology itself (Velardi et al. 2005). However, since mentions have been introduced as an evolution of the traditional Named Entity Recognition task (see Tanev and Magnini, 2006), they guarantee a reasonable level of difficulty, which makes OPTM challenging both for the Computational Linguistic side and the Knowledge Representation community. Second, there already exist annotated data with mentions, delivered under the ACE (Automatic Content Extraction) initiative (Ferro et al. 2005, Linguistic Data Consortium 2004), which makes the exploitation of machine learning based approaches possible. Finally, having a limited scope with respect to OLP, the OPTM task allows for a better estimation of performance; in particular, it is possible to evaluate more easily the recall"
W06-0504,C98-2122,0,\N,Missing
W13-0117,fillmore-etal-2002-seeing,0,0.062698,"Missing"
W13-0117,1985.tmi-1.17,0,0.45194,"Missing"
W13-0117,P03-1054,0,0.00821199,"Missing"
W13-0117,W93-0102,0,0.237488,"Missing"
W13-0117,de-marneffe-etal-2006-generating,0,0.223232,"Missing"
W13-0117,W02-1006,0,0.133028,"Missing"
W13-0117,S01-1029,0,0.0281089,"Missing"
W13-0117,W97-0301,0,\N,Missing
W13-0117,J03-4003,0,\N,Missing
W13-0117,P98-1046,0,\N,Missing
W13-0117,C98-1046,0,\N,Missing
W13-0117,P05-3027,0,\N,Missing
W13-0117,P11-1056,0,\N,Missing
W13-0117,J04-1003,0,\N,Missing
W13-0117,W95-0103,0,\N,Missing
W13-3830,P05-1022,0,0.058039,"r part of a whole or location: [[ NP found in α ]], [[ NP located in α ]], and [[ NP in α ]]. An example instantiation of such a pattern is: T : The Gaspe is a North American peninsula (. . . ) in Quebec. H: The Gaspe Peninsula is located in Quebec While the main strategy remains the same, us117 ing the transformation of these types of patterns increases the recall of the system significantly. 4 Experiments We based our experiments on the freely available corpora from the Recognizing Textual Entailment competitions RTE-3, 4 and 5. All of the entailment pairs were parsed with the BLLIP parser (Charniak and Johnson, 2005) and subsequently processed with GLARF (Meyers et al., 2009). The copula pattern [[ X be α ]] was matched in all hypotheses, and only instances where the match was positive were kept, see Table 2. The method presented in the previous section does not require training. However, in order to have a direct comparison with other methods, we report only the results obtained on the gold corpus. We employed three progressively complex baselines: • BL1: Lexical overlap baseline with threshold determined by a linear SVM (Mehdad and Magnini, 2009) copula gold copula dev RTE3 202 204 RTE4 102 101 RTE5 269"
W13-3830,de-marneffe-etal-2006-generating,0,0.111231,"Missing"
W13-3830,W10-1609,0,0.0460316,"Missing"
W13-3830,H05-1049,0,0.0989598,"Missing"
W13-3830,P06-1114,0,0.0396958,"H, the textual entailment task consists in deciding whether the information in H is entailed by the information in T (Dagan et al., 2006). Many and diverse systems participated in Recognizing Textual Entailment Challenges (RTE), which helped in pointing out interesting issues with an important impact in other NLP tasks. Under some assumptions, the papers published on this topic have proven that the TE methodology is useful for machine translation, text summarization, information retrieval, question answering, fact checking etc. (Pad´o et al., 2009; Lloret et al., 2008; Clinchant et al., 2006; Harabagiu and Hickl, 2006). The two major issues emerging from this body of work are the fact that NLP applications need systems that (1) attain results which are not corpus dependent and (2) assume that the text for entailment may be incorrect or even contradictory. In this paper we propose a system which decom115 Octavian Popescu Fondazione Bruno Kessler popescu@fbk.eu poses the text into chunks via a shallow text analysis, and determines the entailment relationship by matching the information contained in the is − a pattern. The results show that the method is able to cope with the two requirements above. Our system"
W13-3830,C92-2082,0,0.345143,"erty and the entity being in separated chunks. The system resolves the coreference between the entities mentioned in each chunk by employing mostly techniques for inter-document coreference (Popescu et al., 2008; Ponzetto and Poesio, 2009). To unify the information contained in each chunk we considered a set of heuristics which identifies syntactical fixed forms and expresses them as is − a relations. For example, an apposition becomes a copula. We also recognized relations between entities which are typically expressed as a pattern, for example [[ e1 is known as e2 ]], following the work of (Hearst, 1992; Pantel et al., 2004). The basic approach is extended by considering also synonyms/antonyms and negation mismatches. For comparison purposes we considered a set of features which extend the RTE feature set (MacCartney et al., 2006) and syntactic kernels (Moschitti, 2006) with SVM. The results we obtain support the statement that integration of syntactic and semantic information can yield better results over surface based features (Pad´o et al., 2009). For a better understanding of the variance of the results according to the corpora, including robustness to noise and dependency of the veridic"
W13-3830,N06-1006,0,0.0591459,"Missing"
W13-3830,W09-2423,0,0.164274,"H and T . Usually the relevant information in T is not in a single chunk and it does not have a form directly comparable with the information in H. Let us see an example: T : Pop star Madonna has suffered “minor injuries” and bruises after falling from a startled horse on New York’s Long Island on Saturday. According to her spokeswoman, the 50-year-old singer fell when her horse . . . H: Madonna is 50 years old. 2 We use a parser to obtain the heads of all NPs. Most of the dependency parsers normalize the syntactic variant like passive, apposition, time expressions (De Marneffe et al., 2006; Meyers et al., 2009). Each head represents a possible entity and we extract as attributes all the heads of adjectival and nominal phrases which are under the respective head. For example in Figure 1, the entity Bob Iger has the attribute CEO of Disney in both cases. Notice that the dependency structures are very different and a direct comparison is likely to be of little help. The coreference of heads is carried out using a local coreference engine based on multi-pass sieve coreference resolution (MacCartney et al., 2006). For attribute matching we also considered synonyms (Roget, 1911). For example, the system c"
W13-3830,E06-1015,0,0.274549,"ntained in each chunk we considered a set of heuristics which identifies syntactical fixed forms and expresses them as is − a relations. For example, an apposition becomes a copula. We also recognized relations between entities which are typically expressed as a pattern, for example [[ e1 is known as e2 ]], following the work of (Hearst, 1992; Pantel et al., 2004). The basic approach is extended by considering also synonyms/antonyms and negation mismatches. For comparison purposes we considered a set of features which extend the RTE feature set (MacCartney et al., 2006) and syntactic kernels (Moschitti, 2006) with SVM. The results we obtain support the statement that integration of syntactic and semantic information can yield better results over surface based features (Pad´o et al., 2009). For a better understanding of the variance of the results according to the corpora, including robustness to noise and dependency of the veridical presupposition on the information in corpus, we used a technique of generating a scrambled corpus similar to the one described in (Yuret et al., 2010). The results we obtain confirm that the method is stable and overcome with a large margin other approaches. Unlike the"
W13-3830,P09-1034,0,0.0491239,"Missing"
W13-3830,P09-5006,0,0.0458367,"Missing"
W13-3830,S10-1009,0,0.0484248,"Missing"
W15-1702,P09-1053,0,0.139186,", for building either a binary classifier for detecting paraphrase or regression model to compute the similarity scores on Twitter data. Moreover, these features 12 can be used independently or together with others to measure the semantic similarity and recognize the paraphrase of given sentence pair as well as to evaluate the significance of each feature to the accuracy of system’s predictions. On top of this, the system is expandable and scalable for adopting more useful features aiming for improving the accuracy. Lexical and String Similarity. We use the system described in the literature (Das and Smith, 2009) to compute the lexical and string similarity between two sentences by using a logistic regression model with eighteen features based on n-grams. This system uses precision, recall and F1-score of 1-gram, 2-gram and 3-gram of tokens and stems from sentence pair to build a binary classification model for identifying paraphrase. We extract the eighteen features from this system to use in our classification model. Machine Translation Evaluation Metrics. Other than similarity features, we also use evaluation metrics in machine translation as suggested in (Madnani et al., 2012) for paraphrase recog"
W15-1702,W14-3348,0,0.0136152,"potheses by aligning them to one or more reference translations. We take into consideration to use all the eight metrics proposed, but we find that adding some of them without a careful process of training on the dataset may decrease the performance of the system. Thus, we only use two metrics in our system, the METEOR and BLEU. We actually also take into consideration the metric TERp (Snover et al., 2009), but it does not make any improvement on system performance, hence, we exclude it. METEOR (Metric for Evaluation of Translation with Explicit ORdering). We use the latest version of METEOR (Denkowski and Lavie, 2014) that find alignments between sentences based on exact, stem, synonym and paraphrase matches between words and phrases. We used the system as distributed on its website using only the &quot;norm&quot; option that tokenizes and normalizes punctuation and lowercase as suggested by documentation.5 We compute the word alignment scores on sentences and on sentences with partof-speech and named entity tags, as our idea is 5 http://www.cs.cmu.edu/ alavie/METEOR/index.html Classifier/Features Word/ n-grams Overlap (1) (1) +METEOR (1) +METEOR +TERp (1) +METEOR +BLEU (1) +METEOR +BLEU +EditDistance Baseline-1 Edi"
W15-1702,C04-1051,0,0.466018,"ani and Dorr, 2010). The ACL Wiki gives an excellent summary of the state-of-the-art paraphrase identification techniques; this shows how much effort researchers did to automatically detecting paraphrases.3 The different approaches can be categorized into supervised methods, i.e. (Madnani et al., 2012), (Socher et al., 2011) and (Wan et al., 2006), that, at the moment, are the most promising and unsupervised methods, i.e. (Fernando and Stevenson, 2008), (Hassan and Adviser-Mihalcea, 2011) and (Islam and Inkpen, 2009). Previous works use the Microsoft Research Paraphrase Corpus (MSRP) dataset (Dolan et al., 2004) that is obtained by extracting sentences from news sources on the web; however, this scenario is very different from social data. A few recent studies have highlighted the potentiality and importance of developing paraphrase (Zanzotto et al., 2011) and (Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for Tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges. Twitter data and, more in general, social media data have been used as dataset in a growing to"
W15-1702,P12-1091,0,0.13347,"and (Wan et al., 2006), that, at the moment, are the most promising and unsupervised methods, i.e. (Fernando and Stevenson, 2008), (Hassan and Adviser-Mihalcea, 2011) and (Islam and Inkpen, 2009). Previous works use the Microsoft Research Paraphrase Corpus (MSRP) dataset (Dolan et al., 2004) that is obtained by extracting sentences from news sources on the web; however, this scenario is very different from social data. A few recent studies have highlighted the potentiality and importance of developing paraphrase (Zanzotto et al., 2011) and (Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for Tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges. Twitter data and, more in general, social media data have been used as dataset in a growing topic of research. Twitter, at the moment the most used microblogging tool, has seen a lot of growth since it launched in October, 2006. In (Java et al., 2007) preliminary analysis they find user clusters based on user intention to topics by clique percolation methods. This research is expanded and improved in several ways in ("
W15-1702,J10-3003,0,0.027254,"Proceedings of SocialNLP 2015@NAACL-HLT, pages 10–19, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics presents the Related Work, Section 3 describes the tasks and set of features, Section 4 shows the Experiments, Section 5 reports the Evaluations, Section 6 discusses the Error Analysis, and finally Section 7 is the Conclusions and Future Work. 2 Related Work The ability to identify paraphrase, in which a sentences express the same meaning of another one but with different words, has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of the state-of-the-art paraphrase identification techniques; this shows how much effort researchers did to automatically detecting paraphrases.3 The different approaches can be categorized into supervised methods, i.e. (Madnani et al., 2012), (Socher et al., 2011) and (Wan et al., 2006), that, at the moment, are the most promising and unsupervised methods, i.e. (Fernando and Stevenson, 2008), (Hassan and Adviser-Mihalcea, 2011) and (Islam and Inkpen, 2009). Previous works use the Microsoft Research Paraphrase Corpus (MSRP) dataset (Dolan et al., 2004)"
W15-1702,N12-1019,0,0.0839538,"ion 6 discusses the Error Analysis, and finally Section 7 is the Conclusions and Future Work. 2 Related Work The ability to identify paraphrase, in which a sentences express the same meaning of another one but with different words, has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of the state-of-the-art paraphrase identification techniques; this shows how much effort researchers did to automatically detecting paraphrases.3 The different approaches can be categorized into supervised methods, i.e. (Madnani et al., 2012), (Socher et al., 2011) and (Wan et al., 2006), that, at the moment, are the most promising and unsupervised methods, i.e. (Fernando and Stevenson, 2008), (Hassan and Adviser-Mihalcea, 2011) and (Islam and Inkpen, 2009). Previous works use the Microsoft Research Paraphrase Corpus (MSRP) dataset (Dolan et al., 2004) that is obtained by extracting sentences from news sources on the web; however, this scenario is very different from social data. A few recent studies have highlighted the potentiality and importance of developing paraphrase (Zanzotto et al., 2011) and (Xu et al., 2013) and semantic"
W15-1702,P14-5008,0,0.0137141,"output and the reference translation, in our case between sentence pairs. The score is tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. As the length of tweets is relatively short, it is only 140-character message, we do not expect to have large n-gram overlaps, except 1-gram and 2-gram. Our analysis actually shows that 3-gram, 4-gram and the average score may cause more noise. Edit Distance. We use the edit distance between sentences as a feature. For that we used the Excitement Open Platform (EOP) (Magnini et al., 2014).6 To obtain the edit distance, we use EDITS Entailment Decision Algorithm (EDITS EDA) taking the edit distance instead of entailment or not entailment decision. We configure the system to use lemmas and synonyms as identical words to compute sentence 6 http://hltfbk.github.io/Excitement-Open-Platform/ 13 distance, the system normalizes the score on the number of token of the shortest sentence. We choose this configuration because it returns the best performance evaluated on training and development data. Sentiment Analysis. We speculate to improve paraphrase detection by adding a feature base"
W15-1702,P14-5010,0,0.00551181,"Missing"
W15-1702,pak-paroubek-2010-twitter,0,0.0435871,"lied geographical characterization to cluster users and also found relation between the number of following and followers of a user. These and other similar researches have helped to obtain a more precise idea about some effect that action in this microblogging platform can have; (Kwak et al., 2010) use previous works as a base to rank users adding the effect of retweets on information propagation. With the data obtained from the population of blogs and social networks, opinion mining and sentiment analysis became, in the last years, a field of interest for many researches. In the literature (Pak and Paroubek, 2010), they describe a method for an automatic collection of a corpus that can be used to train a sentiment classifier. In a further research (Kouloumpis et al., 2011), it shows that part-of-speech features may not be useful for sentiment analysis in the microblogging domain, instead using hash-tags to collect training data did prove useful, as did using data collected based on positive and negative emoticons. 3 Paraphrase and Semantic Similarity in Twitter In this section, we introduce the two tasks Paraphrase Identification and Semantic Similarity in Twitter, then we describe the set of simple fe"
W15-1702,P02-1040,0,0.0908397,"Missing"
W15-1702,W13-2515,0,0.0197671,", i.e. (Madnani et al., 2012), (Socher et al., 2011) and (Wan et al., 2006), that, at the moment, are the most promising and unsupervised methods, i.e. (Fernando and Stevenson, 2008), (Hassan and Adviser-Mihalcea, 2011) and (Islam and Inkpen, 2009). Previous works use the Microsoft Research Paraphrase Corpus (MSRP) dataset (Dolan et al., 2004) that is obtained by extracting sentences from news sources on the web; however, this scenario is very different from social data. A few recent studies have highlighted the potentiality and importance of developing paraphrase (Zanzotto et al., 2011) and (Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for Tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges. Twitter data and, more in general, social media data have been used as dataset in a growing topic of research. Twitter, at the moment the most used microblogging tool, has seen a lot of growth since it launched in October, 2006. In (Java et al., 2007) preliminary analysis they find user clusters based on user intention to topics by clique percolation methods. Thi"
W15-1702,S15-2001,0,0.0177202,"not be useful for sentiment analysis in the microblogging domain, instead using hash-tags to collect training data did prove useful, as did using data collected based on positive and negative emoticons. 3 Paraphrase and Semantic Similarity in Twitter In this section, we introduce the two tasks Paraphrase Identification and Semantic Similarity in Twitter, then we describe the set of simple features which enables us to achieve competitive performance in both tasks. 3.1 Task Description This is a shared-task proposed as the Task#1 &quot;Paraphrase and Semantic Similarity in Twitter&quot; at SemEval 2015 (Xu et al., 2015).4 In this task, the first common ground for development and comparison of Paraphrase Identification (PI) and Semantic Similarity (SS) systems for the Twitter data is provided. Given a pair of sentences from Twitter trends, systems are required to produce a binary yes/no judgment and an optionally graded similarity score in the scale [0, 1] to measure their semantic equivalence. This task is used to promote this line of research in the new challenging setting of social media data, and help to advance other NLP techniques for noisy user-generated text in the long run. Figure 1 shows examples of"
W15-1702,D11-1061,0,0.0259172,"ized into supervised methods, i.e. (Madnani et al., 2012), (Socher et al., 2011) and (Wan et al., 2006), that, at the moment, are the most promising and unsupervised methods, i.e. (Fernando and Stevenson, 2008), (Hassan and Adviser-Mihalcea, 2011) and (Islam and Inkpen, 2009). Previous works use the Microsoft Research Paraphrase Corpus (MSRP) dataset (Dolan et al., 2004) that is obtained by extracting sentences from news sources on the web; however, this scenario is very different from social data. A few recent studies have highlighted the potentiality and importance of developing paraphrase (Zanzotto et al., 2011) and (Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for Tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges. Twitter data and, more in general, social media data have been used as dataset in a growing topic of research. Twitter, at the moment the most used microblogging tool, has seen a lot of growth since it launched in October, 2006. In (Java et al., 2007) preliminary analysis they find user clusters based on user intention to topics by clique pe"
W15-1702,Q14-1034,0,\N,Missing
W15-1702,U06-1019,0,\N,Missing
W15-1702,J13-3001,0,\N,Missing
