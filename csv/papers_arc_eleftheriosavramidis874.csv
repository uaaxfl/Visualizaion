2021.mtsummit-at4ssl.8,Automatic generation of a 3{D} sign language avatar on {AR} glasses given 2{D} videos of human signers,2021,-1,-1,4,0,5137,lan nguyen,Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),0,"In this paper we present a prototypical implementation of a pipeline that allows the automatic generation of a German Sign Language avatar from 2D video material. The presentation is accompanied by the source code. We record human pose movements during signing with computer vision models. The joint coordinates of hands and arms are imported as landmarks to control the skeleton of our avatar. From the anatomically independent landmarks, we create another skeleton based on the avatar{'}s skeletal bone architecture to calculate the bone rotation data. This data is then used to control our human 3D avatar. The avatar is displayed on AR glasses and can be placed virtually in the room, in a way that it can be perceived simultaneously to the verbal speaker. In further work it is aimed to be enhanced with speech recognition and machine translation methods for serving as a sign language interpreter. The prototype has been shown to people of the deaf and hard-of-hearing community for assessing its comprehensibility. Problems emerged with the transferred hand rotations, hand gestures were hard to recognize on the avatar due to deformations like twisted finger meshes."
2021.acl-srw.20,Observing the Learning Curve of {NMT} Systems With Regard to Linguistic Phenomena,2021,-1,-1,3,0,12458,patrick stadler,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"In this paper we present our observations and evaluations by observing the linguistic performance of the system on several steps on the training process of various English-to-German Neural Machine Translation models. The linguistic performance is measured through a semi-automatic process using a test suite. Among several linguistic observations, we find that the translation quality of some linguistic categories decreased within the recorded iterations. Additionally, we notice some drops of the translation quality of certain categories when using a larger corpus."
2020.wmt-1.38,Fine-grained linguistic evaluation for state-of-the-art Machine Translation,2020,-1,-1,1,1,5140,eleftherios avramidis,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes a test suite submission providing detailed statistics of linguistic performance for the state-of-the-art German-English systems of the Fifth Conference of Machine Translation (WMT20). The analysis covers 107 phenomena organized in 14 categories based on about 5,500 test items, including a manual annotation effort of 45 person hours. Two systems (Tohoku and Huoshan) appear to have significantly better test suite accuracy than the others, although the best system of WMT20 is not significantly better than the one from WMT19 in a macro-average. Additionally, we identify some linguistic phenomena where all systems suffer (such as idioms, resultative predicates and pluperfect), but we are also able to identify particular weaknesses for individual systems (such as quotation marks, lexical ambiguity and sluicing). Most of the systems of WMT19 which submitted new versions this year show improvements."
W19-5351,Linguistic Evaluation of {G}erman-{E}nglish Machine Translation Using a Test Suite,2019,12,2,1,1,5140,eleftherios avramidis,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We present the results of the application of a grammatical test suite for German-to-English MT on the systems submitted at WMT19, with a detailed analysis for 107 phenomena organized in 14 categories. The systems still translate wrong one out of four test items in average. Low performance is indicated for idioms, modals, pseudo-clefts, multi-word expressions and verb valency. When compared to last year, there has been a improvement of function words, non verbal agreement and punctuation. More detailed conclusions about particular systems and phenomena are also presented."
N19-4006,"Train, Sort, Explain: Learning to Diagnose Translation Models",2019,29,0,4,0,10309,robert schwarzenberg,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),0,"Evaluating translation models is a trade-off between effort and detail. On the one end of the spectrum there are automatic count-based methods such as BLEU, on the other end linguistic evaluations by humans, which arguably are more informative but also require a disproportionately high effort. To narrow the spectrum, we propose a general approach on how to automatically expose systematic differences between human and machine translations to human experts. Inspired by adversarial settings, we train a neural text classifier to distinguish human from machine translations. A classifier that performs and generalizes well after training should recognize systematic differences between the two classes, which we uncover with neural explainability methods. Our proof-of-concept implementation, DiaMaT, is open source. Applied to a dataset translated by a state-of-the-art neural Transformer model, DiaMaT achieves a classification accuracy of 75{\%} and exposes meaningful differences between humans and the Transformer, amidst the current discussion about human parity."
W18-6436,Fine-grained evaluation of {G}erman-{E}nglish Machine Translation based on a Test Suite,2018,11,5,2,0.705128,12459,vivien macketanz,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present an analysis of 16 state-of-the-art MT systems on German-English based on a linguistically-motivated test suite. The test suite has been devised manually by a team of language professionals in order to cover a broad variety of linguistic phenomena that MT often fails to translate properly. It contains 5,000 test sentences covering 106 linguistic phenomena in 14 categories, with an increased focus on verb tenses, aspects and moods. The MT outputs are evaluated in a semi-automatic way through regular expressions that focus only on the part of the sentence that is relevant to each phenomenon. Through our analysis, we are able to compare systems based on their performance on these categories. Additionally, we reveal strengths and weaknesses of particular systems and we identify grammatical phenomena where the overall performance of MT is relatively low."
W18-2107,Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically motivated Test Suite,2018,0,1,1,1,5140,eleftherios avramidis,Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing,0,None
W17-4758,Sentence-level quality estimation by predicting {HTER} as a multi-component metric,2017,7,0,1,1,5140,eleftherios avramidis,Proceedings of the Second Conference on Machine Translation,0,"This submission investigates alternative machine learning models for predicting the HTER score on the sentence level. Instead of directly predicting the HTER score, we suggest a model that jointly predicts the amount of the 4 distinct post-editing operations, which are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. Without any feature exploration, a multi-layer perceptron with 4 outputs yields small but significant improvements over the baseline."
W16-6404,Deeper Machine Translation and Evaluation for {G}erman,2016,-1,-1,1,1,5140,eleftherios avramidis,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-2329,"{DFKI}{'}s system for {WMT}16 {IT}-domain task, including analysis of systematic errors",2016,16,3,1,1,5140,eleftherios avramidis,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"We are presenting a hybrid MT approach in the WMT2016 Shared Translation Task for the IT-Domain. Our work consists of several translation components based on rule-based and statistical approaches that feed into an informed selection mechanism. Additions to last yearxe2x80x99s submission include a WSD component, a syntactically-enhanced component and several improvements to the rule-based component, relevant to the particular domain. We also present detailed human evaluation on the output of all translation components, focusing on particular systematic errors."
L16-1296,Tools and Guidelines for Principled Machine Translation Development,2016,2,1,2,0,20841,nora aranberri,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work addresses the need to aid Machine Translation (MT) development cycles with a complete workflow of MT evaluation methods. Our aim is to assess, compare and improve MT system variants. We hereby report on novel tools and practices that support various measures, developed in order to support a principled and informed approach of MT development. Our toolkit for automatic evaluation showcases quick and detailed comparison of MT system variants through automatic metrics and n-gram feedback, along with manual evaluation via edit-distance, error annotation and task-based feedback."
W15-5702,Towards Deeper {MT} - A Hybrid System for {G}erman,2015,-1,-1,1,1,5140,eleftherios avramidis,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-4914,Poor man{'}s lemmatisation for automatic error classification,2015,9,0,3,0.20951,5059,maja popovic,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,This publication has emanated from research supported by QTLEAP project xe2x80x93 ECs FP7 (FP7/2007-n 2013) under grant agreement number 610516:n xe2x80x9cQTLEAP: Quality Translation by Deep Language Engineering Approachesxe2x80x9d and by a researchn grant from Science Foundation Ireland (SFI) undern Grant Number SFI/12/RC/2289. We are grateful ton the reviewers for their valuable feedbac
W15-3004,{DFKI}{'}s experimental hybrid {MT} system for {WMT} 2015,2015,15,4,1,1,5140,eleftherios avramidis,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"DFKI participated in the shared translation task of WMT 2015 with the GermanEnglish language pair in each translation direction. The submissions were generated using an experimental hybrid system based on three systems: a statistical Moses system, a commercial rule-based system, and a serial coupling of the two where the output of the rule-based system is further translated by Moses trained on parallel text consisting of the rule-based output and the original target language. The outputs of three systems are combined using two methods: (a) an empirical selection mechanism based on grammatical features (primary submission) and (b) IBM1 models based on POS 4-grams (contrastive submission)."
2015.eamt-1.15,Poor man{'}s lemmatisation for automatic error classification,2015,9,0,3,0.20951,5059,maja popovic,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,This publication has emanated from research supported by QTLEAP project xe2x80x93 ECs FP7 (FP7/2007-n 2013) under grant agreement number 610516:n xe2x80x9cQTLEAP: Quality Translation by Deep Language Engineering Approachesxe2x80x9d and by a researchn grant from Science Foundation Ireland (SFI) undern Grant Number SFI/12/RC/2289. We are grateful ton the reviewers for their valuable feedbac
W14-5104,Correlating decoding events with errors in Statistical Machine Translation,2014,23,1,1,1,5140,eleftherios avramidis,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-3337,Efforts on Machine Learning over Human-mediated Translation Edit Rate,2014,13,2,1,1,5140,eleftherios avramidis,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"In this paper we describe experiments on predicting HTER, as part of our submission in the Shared Task on Quality Estimation, in the frame of the 9th Workshop on Statistical Machine Translation. In our experiment we check whether it is possible to achieve better HTER prediction by training four individual regression models for each one of the edit types (deletions, insertions, substitutions, shifts), however no improvements were yielded. We also had no improvements when investigating the possibility of adding more data from other non-minimally post-edited and freely translated datasets. Best HTER prediction was achieved by adding deduplicated WMT13 data and additional features such as (a) rule-based language corrections (language tool) (b) PCFG parsing statistics and count of tree labels (c) position statistics of parsing labels (d) position statistics of tri-grams with low probability."
avramidis-etal-2014-taraxu,"The tara{X{\\\U}} corpus of human-annotated machine translations""",2014,7,2,1,1,5140,eleftherios avramidis,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. This paper describes the corpus developed as a result of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing."
2014.eamt-1.38,Using a new analytic measure for the annotation and analysis of {MT} errors on real data,2014,7,12,5,0,28494,arle lommel,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"This work presents the new flexible Multidimensional Quality Metrics (MQM) framework and uses it to analyze the performance of state-of-the-art machine translation systems, focusing on xe2x80x9cnearly acceptablexe2x80x9d translated sentences. A selection of WMT news data and xe2x80x9ccustomerxe2x80x9d data provided by language service providers (LSPs) in four language pairs was annotated using MQM issue types and examined in terms of the types of errors found in it. Despite criticisms of WMT data by the LSPs, an examination of the resulting errors and patterns for both types of data shows that they are strikingly consistent, with more variation between language pairs and system types than between text types. These results validate the use of WMT data in an analytic approach to assessing quality and show that analytic approaches represent a useful addition to more traditional assessment methodologies such as BLEU or METEOR."
2014.eamt-1.41,"Relations between different types of post-editing operations, cognitive effort and temporal effort",2014,-1,-1,4,0.326908,5059,maja popovic,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
W13-2240,Selecting Feature Sets for Comparative and Time-Oriented Quality Estimation of Machine Translation Output,2013,22,3,1,1,5140,eleftherios avramidis,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,None
2013.mtsummit-wptp.2,What can we learn about the selection mechanism for post-editing?,2013,-1,-1,2,0.326908,5059,maja popovic,Proceedings of the 2nd Workshop on Post-editing Technology and Practice,0,None
2013.mtsummit-posters.4,A {CCG}-based Quality Estimation Metric for Statistical Machine Translation Learning from Human Judgments of Machine Translation Output,2013,-1,-1,2,0.326908,5059,maja popovic,Proceedings of Machine Translation Summit XIV: Posters,0,None
2013.mtsummit-posters.5,Learning from Human Judgments of Machine Translation Output,2013,10,7,2,0.326908,5059,maja popovic,Proceedings of Machine Translation Summit XIV: Posters,0,"Human translators are the key to evaluating machine translation (MT) quality and also to addressing the so far unanswered question when and how to use MT in professional translation workflows. Usually, human judgments come in the form of ranking outputs of different translation systems and recently, post-edits of MT output have come into focus. This paper describes the results of a detailed large scale human evaluation consisting of three tightly connected tasks: ranking, error classification and post-editing. Translation outputs from three domains and six translation directions generated by five distinct translation systems have been analysed with the goal of getting relevant insights for further improvement of MT quality and applicability."
W12-3108,Quality estimation for Machine Translation output using linguistic analysis and decoding features,2012,25,18,1,1,5140,eleftherios avramidis,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"We describe a submission to the WMT12 Quality Estimation task, including an extensive Machine Learning experimentation. Data were augmented with features from linguistic analysis and statistical features from the SMT search graph. Several Feature Selection algorithms were employed. The Quality Estimation problem was addressed both as a regression task and as a discretised classification task, but the latter did not generalise well on the unseen testset. The most successful regression methods had an RMSE of 0.86 and were trained with a feature set given by Correlation-based Feature Selection. Indications that RMSE is not always sufficient for measuring performance were observed."
avramidis-etal-2012-involving,Involving Language Professionals in the Evaluation of Machine Translation,2012,17,12,1,1,5140,eleftherios avramidis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Significant breakthroughs in machine translation only seem possible if human translators are taken into the loop. While automatic evaluation and scoring mechanisms such as BLEU have enabled the fast development of systems, it is not clear how systems can meet real-world (quality) requirements in industrial translation scenarios today. The taraX{\""U} project paves the way for wide usage of hybrid machine translation outputs through various feedback loops in system development. In a consortium of research and industry partners, the project integrates human translators into the development process for rating and post-editing of machine translation outputs thus collecting feedback for possible improvements."
avramidis-etal-2012-richly,"A Richly Annotated, Multilingual Parallel Corpus for Hybrid Machine Translation",2012,12,3,1,1,5140,eleftherios avramidis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In recent years, machine translation (MT) research has focused on investigating how hybrid machine translation as well as system combination approaches can be designed so that the resulting hybrid translations show an improvement over the individual ÂcomponentÂ translations. As a first step towards achieving this objective we have developed a parallel corpus with source text and the corresponding translation output from a number of machine translation engines, annotated with metadata information, capturing aspects of the translation process performed by the different MT systems. This corpus aims to serve as a basic resource for further research on whether hybrid machine translation algorithms and system combination techniques can benefit from additional (linguistically motivated, decoding, and runtime) information provided by the different systems involved. In this paper, we describe the annotated corpus we have created. We provide an overview on the component MT systems and the XLIFF-based annotation format we have developed. We also report on first experiments with the ML4HMT corpus data."
federmann-etal-2012-ml4hmt,The {ML}4{HMT} Workshop on Optimising the Division of Labour in Hybrid Machine Translation,2012,22,3,2,0,6017,christian federmann,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We describe the ÂShared Task on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid Machine TranslationÂ (ML4HMT) which aims to foster research on improved system combination approaches for machine translation (MT). Participants of the challenge are requested to build hybrid translations by combining the output of several MT systems of different types. We first describe the ML4HMT corpus used in the shared task, then explain the XLIFF-based annotation format we have designed for it, and briefly summarize the participating systems. Using both automated metrics scores and extensive manual evaluation, we discuss the individual performance of the various systems. An interesting result from the shared task is the fact that we were able to observe different systems winning according to the automated metrics scores when compared to the results from the manual evaluation. We conclude by summarising the first edition of the challenge and by giving an outlook to future work."
C12-1008,Comparative Quality Estimation: Automatic Sentence-Level Ranking of Multiple Machine Translation Outputs,2012,41,10,1,1,5140,eleftherios avramidis,Proceedings of {COLING} 2012,0,"A machine learning mechanism is learned from human annotations in order to perform preference ranking. The mechanism operates on a sentence level and ranks the alternative machine translations of each source sentence. Rankings are decomposed into pairwise comparisons so that binary classifiers can be trained using black-box features of automatic linguistic analysis. In order to re-compose the pairwise decisions of the classifier, this work introduces weighing the decisions with their classification probabilities, which eliminates ranking ties and increases the coefficient of the correlation with the human rankings up to 80%. The authors also demonstrate several configurations of successful automatic ranking models; the best configuration achieves acceptable correlation with human judgments (tau=0.30), which is higher than that of state-of-the-art reference-aware automatic MT evaluation metrics such as METEOR and Levenshtein distance."
W11-2104,Evaluate with Confidence Estimation: Machine ranking of translation outputs using grammatical features,2011,20,22,1,1,5140,eleftherios avramidis,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation, given only their source sentence. The system employs a statistical classifier trained upon existing human rankings, using several features derived from analysis of both the source and the target sentences. Development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a PCFG parser."
W11-2109,Evaluation without references: {IBM}1 scores as evaluation metrics,2011,9,15,3,0.281846,5059,maja popovic,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"Current metrics for evaluating machine translation quality have the huge drawback that they require human-quality reference translations. We propose a truly automatic evaluation metric based on ibm1 lexicon probabilities which does not need any reference translations. Several variants of ibm1 scores are systematically explored in order to find the most promising directions. Correlations between the new metrics and human judgments are calculated on the data of the third, fourth and fifth shared tasks of the Statistical Machine Translation Workshop. Five different European languages are taken into account: English, Spanish, French, German and Czech. The results show that the ibm1 scores are competitive with the classic evaluation metrics, the most promising being ibm1 scores calculated on morphemes and pos-4grams."
2011.iwslt-evaluation.13,{DFKI}{'}s {SC} and {MT} submissions to {IWSLT} 2011,2011,26,4,2,0,5800,david vilar,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"We describe DFKI{'}s submission to the System Combination and Machine Translation tracks of the 2011 IWSLT Evaluation Campaign. We focus on a sentence selection mechanism which chooses the (hopefully) best sentence among a set of candidates. The rationale behind it is to take advantage of the strengths of each system, especially given an heterogeneous dataset like the one in this evaluation campaign, composed of TED Talks of very different topics. We focus on using features that correlate well with human judgement and, while our primary system still focus on optimizing the BLEU score on the development set, our goal is to move towards optimizing directly the correlation with human judgement. This kind of system is still under development and was used as a secondary submission."
P08-1087,Enriching Morphologically Poor Languages for Statistical Machine Translation,2008,23,85,1,1,5140,eleftherios avramidis,Proceedings of ACL-08: HLT,1,"We address the problem of translating from morphologically poor to morphologically rich languages by adding per-word linguistic information to the source language. We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly. In experiments, we show improved performance for translating from English into Greek and Czech. For Englishxe2x80x90Greek, we reduce the error on the verb conjugation from 19% to 5.4% and noun case agreement from 9% to 6%."
