2004.jeptalnrecital-long.24,E91-1005,0,0.460178,"Missing"
2004.jeptalnrecital-long.24,2000.iwpt-1.8,0,0.150975,"Missing"
2004.jeptalnrecital-long.24,E03-1030,1,0.899762,"Missing"
2004.jeptalnrecital-long.24,C88-2147,0,0.505641,"Missing"
2004.jeptalnrecital-long.24,J01-1004,0,\N,Missing
2004.jeptalnrecital-long.24,P94-1036,0,\N,Missing
2004.jeptalnrecital-long.24,J05-2003,1,\N,Missing
2005.jeptalnrecital-court.13,J05-2003,1,0.84386,"C1) The root of D is an instance of an initial tree α ∈ I and all other nodes are instances of trees from tree sets in A such that for all instances Γ of elementary tree sets from A and for all γ1 , γ2 ∈ Γ: if γ1 ∈ N , then γ2 ∈ N . • (MC2) For all instances Γ of elementary tree sets from A and for all γ1 , γ2 ∈ Γ, γ1 6= γ2 : hγ1 , γ2 i 6∈ DD . • (MC3) For all pairwise different instances Γ1 , Γ2 , . . . , Γn , n ≥ 2 of elementary tree sets from (i−1) (i) (n) (1) (i) (i) A: there are no γ1 , γ2 ∈ Γi , 1 ≤ i ≤ n such that hγ1 , γ2 i ∈ DD and hγ1 , γ2 i ∈ DD for 2 ≤ i ≤ n. The proof is given in Kallmeyer (2005). The lemma gives us a way to characterize non-local MCTAG via the properties of the TAG derivation trees the grammar licenses and thereby to get rid of the original simultaneity requirement: The corresponding properties are now captured in the three constraints (MC1)–(MC3). Since these constraints need to hold only for the TAG derivation trees that correspond to derived trees in the tree language, sub-derivation trees need not satisfy them. In other words, γ1 and γ2 from the same tree set can be added at different moments of the derivation as long as the final TAG derivation tree satisfies (M"
2008.jeptalnrecital-long.14,2000.iwpt-1.8,0,0.148613,"Missing"
2008.jeptalnrecital-long.14,J05-2003,1,0.851951,"Missing"
2009.jeptalnrecital-court.40,W01-1807,0,0.0400543,"Missing"
2009.jeptalnrecital-court.40,E99-1008,0,0.0745236,"Missing"
2009.jeptalnrecital-court.40,2000.iwpt-1.8,0,0.0978387,"Missing"
2009.jeptalnrecital-court.40,W05-1502,0,0.0371231,"Missing"
2009.jeptalnrecital-court.40,W08-2307,0,0.0374122,"Missing"
2009.jeptalnrecital-court.40,J91-3002,0,0.181186,"Missing"
2009.jeptalnrecital-court.40,C08-2026,0,0.0385433,"Missing"
2009.jeptalnrecital-court.40,C02-1028,0,0.0362691,"Missing"
2017.jeptalnrecital-long.8,P98-1013,0,0.0629306,"Missing"
2017.jeptalnrecital-long.8,J90-1003,0,0.329392,"Missing"
2017.jeptalnrecital-long.8,N15-1184,0,0.0476578,"Missing"
2017.jeptalnrecital-long.8,P14-5004,0,0.0563816,"Missing"
2017.jeptalnrecital-long.8,N15-1004,0,0.04208,"Missing"
2017.jeptalnrecital-long.8,N13-1092,0,0.0441884,"Missing"
2017.jeptalnrecital-long.8,J15-4004,0,0.0302592,"Missing"
2017.jeptalnrecital-long.8,Q14-1041,0,0.0637336,"Missing"
2017.jeptalnrecital-long.8,Q15-1016,0,0.0661918,"Missing"
2017.jeptalnrecital-long.8,W13-3512,0,0.0867233,"Missing"
2017.jeptalnrecital-long.8,D14-1162,0,0.0890095,"Missing"
2017.jeptalnrecital-long.8,D15-1036,0,0.0433675,"Missing"
2017.jeptalnrecital-long.8,D15-1243,0,0.0362257,"Missing"
2017.jeptalnrecital-long.8,P16-1128,0,0.0330301,"Missing"
2020.coling-main.357,W09-0201,0,0.0354572,"ames. For instance, the appearance of a verb in transitive and intransitive syntactic environments may signal participation in CIA, but it could also be explained by a number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers perform slightly better on the CIA identification task than on the ISA task; the annotatio"
2020.coling-main.357,P19-1285,0,0.0362462,"tes for both alternations. Our classifiers use count-based, perplexity-based and vector-based features obtained from the sentence types in (1) and (3). The perplexity features are based on a language model and intended to approximate acceptability judgements of sentence variations (e.g., in the case of ISA replacing with with using or moving a with-NP to the subject position; in the case of CIA discarding a transitive sentence’s subject and promoting the object to the subject position). This has become possible with the new, transformerbased generation of language models (Devlin et al., 2019; Dai et al., 2019). 2 2.1 Method Data For our ISA/CIA identification approach, we collect sentences from ENCOW (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015)1 that instantiate the syntactic patterns exemplified in (1) and (3). Table 1 describes 1 English web text from 2012/14, available in a sentence-shuffled version, containing roughly 9.6 billion tokens, dependencyparsed with MaltParser (Nivre et al., 2006). 4045 the collected subcorpora.2 For SUBJ -SC, we only use sentences whose root verbs are observed at least once in WITH -SC or USING -SC. We also create a reduced version of each corpus containing only se"
2020.coling-main.357,N19-1423,0,0.0114937,"promising new candidates for both alternations. Our classifiers use count-based, perplexity-based and vector-based features obtained from the sentence types in (1) and (3). The perplexity features are based on a language model and intended to approximate acceptability judgements of sentence variations (e.g., in the case of ISA replacing with with using or moving a with-NP to the subject position; in the case of CIA discarding a transitive sentence’s subject and promoting the object to the subject position). This has become possible with the new, transformerbased generation of language models (Devlin et al., 2019; Dai et al., 2019). 2 2.1 Method Data For our ISA/CIA identification approach, we collect sentences from ENCOW (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015)1 that instantiate the syntactic patterns exemplified in (1) and (3). Table 1 describes 1 English web text from 2012/14, available in a sentence-shuffled version, containing roughly 9.6 billion tokens, dependencyparsed with MaltParser (Nivre et al., 2006). 4045 the collected subcorpora.2 For SUBJ -SC, we only use sentences whose root verbs are observed at least once in WITH -SC or USING -SC. We also create a reduced version of each corpus"
2020.coling-main.357,E03-1040,0,0.121779,"ve syntactic environments may signal participation in CIA, but it could also be explained by a number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers perform slightly better on the CIA identification task than on the ISA task; the annotation results yield promising new candidates for both alternations. Our classifi"
2020.coling-main.357,W19-0129,0,0.0949269,"number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers perform slightly better on the CIA identification task than on the ISA task; the annotation results yield promising new candidates for both alternations. Our classifiers use count-based, perplexity-based and vector-based features obtained from the sente"
2020.coling-main.357,P15-1156,0,0.0256807,"structions are form-meaning pairs, but our subcorpus filters and sentence-alternating script operate purely on syntactic patterns. We use a transformer-based language model (Dai et al., 2019), trained on the One Billion Word benchmark (Chelba et al., 2013), to calculate perplexity scores for the original sentences and their reordered counterparts, as an approximation of acceptability. We expect the average perplexity scores of generated alternate sentences with alternating verbs to be more similar to the original sentences’ perplexity, and less so for non-alternating verbs. While studies like Lau et al. (2015) and Warstadt et al. (2019) have shown that acceptability judgments based on language models do not perform on par with human annotators, and that human acceptability judgments also do not produce perfect inter-annotator agreement, we nevertheless use perplexity scores as an approximation of acceptability; in doing so, we are less interested in the acceptability of individual sentences, and more in overall trends across all attestations of a given verb. Our results show that the perplexity scores yield useful features for the task. Cnt features (Lvw , Lvu = (head) lemmas in with-PPs in Swv res"
2020.coling-main.357,W06-2106,0,0.0596177,"ble semantic types of instruments is one of the challenges of identifying new ISA verbs: Instruments can be either canonical to a verb (e.g. opening a door with a key), objects that receive an instrument function by way of coercion (e.g. opening it with a crowbar), or events that precede or accompany the event described by the verb and thus facilitate it (e.g. opening it with a kick). Most non-candidates found in our experiments appeared with with-phrases that signified something other than instrument usage, e.g. accompaniment. with is a highly polysemous preposition; the Preposition Project (Litkowski and Hargraves, 2006) lists 10 main senses.8 Such verbs superficially seem to instantiate the constructions associated with ISA and are accordingly labeled as such by the classifier, but do not carry the corresponding semantics, and are thus bad candidates for the alternation. Issues of attachment ambiguity, where the parser has to decide whether a with-phrase is attached under the verb or under an object or another element in the sentence, were mostly circumvented by our original corpus filter that only selected sentences with with-phrases directly under the verb. 3.2.2 Discussion of new CIA candidates The verbs"
2020.coling-main.357,A00-2034,0,0.60271,"cult due to the many-to-many relationship between syntactic forms and semantic frames. For instance, the appearance of a verb in transitive and intransitive syntactic environments may signal participation in CIA, but it could also be explained by a number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers"
2020.coling-main.357,nivre-etal-2006-maltparser,0,0.0687052,"f CIA discarding a transitive sentence’s subject and promoting the object to the subject position). This has become possible with the new, transformerbased generation of language models (Devlin et al., 2019; Dai et al., 2019). 2 2.1 Method Data For our ISA/CIA identification approach, we collect sentences from ENCOW (Sch¨afer and Bildhauer, 2012; Sch¨afer, 2015)1 that instantiate the syntactic patterns exemplified in (1) and (3). Table 1 describes 1 English web text from 2012/14, available in a sentence-shuffled version, containing roughly 9.6 billion tokens, dependencyparsed with MaltParser (Nivre et al., 2006). 4045 the collected subcorpora.2 For SUBJ -SC, we only use sentences whose root verbs are observed at least once in WITH -SC or USING -SC. We also create a reduced version of each corpus containing only sentences with up to 10 tokens. The aim of these reduced corpora is to determine to what extent misparsed sentences or overly long constituents can impact the success of the annotation step. We run that part of our experiments once on reduced corpora and once on the full corpora for each alternation. We use data from VerbNet 3.3 (VN, Kipper et al. (2000)) as ground truth to train our classifie"
2020.coling-main.357,E17-1010,0,0.0145321,"quence, guess, treat, axe, scream. 3.3 Limitations and Future Work An open issue with our approach to both alternations is the impact of ambiguity and polysemy on the reliability of our results. The observed usages in different syntactic patterns for each verb do not necessarily involve the same sense: In the current setup, we do not distinguish word senses, and thus cannot verify whether the observed instances actually relate to the same lexical item. A word sense disambiguation (WSD) step may help; however, WSD is usually applied on the document level or based on immediate sentence context (Raganato et al., 2017), neither of which is available in ENCOW. Our use of corpus data allows us to identify new alternation candidates with some success, but our annotation scheme is not suited for the explicit identification of non-alternating verbs. The absence of usages of a given verb in a particular syntactic pattern in our corpus does not entail the infelicity of that pattern for the given verb. This is a well-known issue in corpus linguistics (K¨ubler and Zinsmeister, 2014). Furthermore, the work described here relies on two annotators. While the results are promising, this proof-of-concept requires an exte"
2020.coling-main.357,schafer-bildhauer-2012-building,0,0.0352941,"Missing"
2020.coling-main.357,W19-0115,1,0.90269,"A, but it could also be explained by a number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers perform slightly better on the CIA identification task than on the ISA task; the annotation results yield promising new candidates for both alternations. Our classifiers use count-based, perplexity-based and vect"
2020.coling-main.357,W03-0410,0,0.118914,"y signal participation in CIA, but it could also be explained by a number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers perform slightly better on the CIA identification task than on the ISA task; the annotation results yield promising new candidates for both alternations. Our classifiers use count-based, perplex"
2020.coling-main.357,W04-2605,0,0.317122,"ationship between syntactic forms and semantic frames. For instance, the appearance of a verb in transitive and intransitive syntactic environments may signal participation in CIA, but it could also be explained by a number of other reasons, e.g. the object of the verb being a non-obligatory argument (Levin, 1993). Since verb alternations are associated with different constructions, the automatic identification of verb alternations is usually studied for particular alternations. CIA is an extensively researched alternation and has been identified using WordNet (McCarthy, 2000; McCarthy, 2001; Tsang and Stevenson, 2004), distributional data (Baroni and Lenci, 2009), feature engineering (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Stevenson and Joanis, 2003; Seyffarth, 2019), and neural networks (Kann et al., 2019). To our knowledge, there has not been any comparable work on ISA identification so far. The contribution of this paper is twofold: First, we develop classifiers for identifying ISA and CIA verbs, and second, we find new ISA/CIA verbs by using these classifiers in a subsequent iteration of manual annotation and reclassification. Our classifiers perform slightly better on the CIA identific"
2020.coling-main.357,Q19-1040,0,0.0221579,"aning pairs, but our subcorpus filters and sentence-alternating script operate purely on syntactic patterns. We use a transformer-based language model (Dai et al., 2019), trained on the One Billion Word benchmark (Chelba et al., 2013), to calculate perplexity scores for the original sentences and their reordered counterparts, as an approximation of acceptability. We expect the average perplexity scores of generated alternate sentences with alternating verbs to be more similar to the original sentences’ perplexity, and less so for non-alternating verbs. While studies like Lau et al. (2015) and Warstadt et al. (2019) have shown that acceptability judgments based on language models do not perform on par with human annotators, and that human acceptability judgments also do not produce perfect inter-annotator agreement, we nevertheless use perplexity scores as an approximation of acceptability; in doing so, we are less interested in the acceptability of individual sentences, and more in overall trends across all attestations of a given verb. Our results show that the perplexity scores yield useful features for the task. Cnt features (Lvw , Lvu = (head) lemmas in with-PPs in Swv resp. in using-phrases in Suv"
2020.coling-main.595,N19-1018,0,0.142289,"145 trees, respectively. There are 46 # initial trees 1527 # sister-adjoining trees 1549 constituents with LDDs in the train set, 5 in the dev. set and # d-edge trees 49 27 in the test set. We extracted a TWG from this data and present in Table 1 statistics on the elementary tree templates Table 1: Statistics on the extracted TWG. (supertags) in the TWG. We compare the parsing results with the parser DiscoDOP (van Cranenburgh and Bod, 2013) which is based on the discontinuous data-oriented parsing model. We also compare our results with the stateof-the-art transition-based parser Discoparset (Coavoux and Cohen, 2019). We evaluated7 the overall performance of the parsers and also analyzed how well all three systems predict LDDs (see Tables 2 and 3). Unrelated to LDDs, the treebanks contain crossing branches (e.g., for operators and modifiers). Prior to TWG extraction, we decross these while keeping track of the transformation in order to be able to reverse it. For parsing with DiscoDOP and Discoparset, we added crossing branches for all LDDs. To evaluate LDD prediction with DiscoDOP and Discoparset we counted how many crossing branches were established in parsed trees. For ParTAGe we counted the LDD predic"
2020.coling-main.595,N19-1423,0,0.0134589,"Missing"
2020.coling-main.595,N18-1107,0,0.101647,"bolic parsing for TWGs with edge features was proposed in (Arps et al., 2019). In this work, we propose a statistical parsing approach for TWG and extend the pipeline based on supertagging and A∗ algorithm (Waszczuk, 2017; Bladier et al., 2019) originally developed for TAG to be applied to TWG. The contributions of the paper are the following: 1) We present the first approach to statistical parsing for Tree-Wrapping Grammars. 2) We propose an extraction algorithm for TWGs based on the algorithm developed for TAG by (Xia, 1999). 3) We extend and modify the neural A? TAG-parser (Waszczuk, 2017; Kasai et al., 2018; Bladier et al., 2019) to handle the operation of tree-wrapping substitution. 2 Long distance dependencies and wrapping substitution in TWG TWGs consist of elementary trees which can be combined using the operations a) substitution (replacing a leaf node with a tree), b) sister adjunction (adding a new daughter to an internal node) and c) tree-wrapping substitution (adding a tree with a d(ominance)-edge by substituting the lower part of the d-edge for a leaf node and merging the upper node of the d-edge with the root of the target tree, see Fig. 1). The latter is This work is licensed under a"
2020.coling-main.595,J03-1006,0,0.259436,"Missing"
2020.coling-main.595,P95-1021,0,0.677679,"alization or long distance wh-movement. In the present paper we show a grammar extraction algorithm for TWG, propose a TWG parser, and discuss parsing results for the grammar extracted from the RRG treebanks RRGbank and RRGparbank1 (Bladier et al., 2018). Similarly to TAG, TWG has the elementary tree combination operations of substitution and sisteradjunction. Additionally, TWG includes the operation of tree-wrapping substitution, which accounts for preserving the connection between the parts of the discontinuous constituents. Operations similar to tree-wrapping substitution were proposed by (Rambow et al., 1995) as subsertion in D-Tree Grammars (DTG) and by (Rambow et al., 2001) as generalized substitution in D-Tree substitution grammar (DSG). To our best knowledge, no statistical parsing approach was proposed for DTG or DSG. An approach to symbolic parsing for TWGs with edge features was proposed in (Arps et al., 2019). In this work, we propose a statistical parsing approach for TWG and extend the pipeline based on supertagging and A∗ algorithm (Waszczuk, 2017; Bladier et al., 2019) originally developed for TAG to be applied to TWG. The contributions of the paper are the following: 1) We present the"
2020.coling-main.595,J01-1004,0,0.278272,"a grammar extraction algorithm for TWG, propose a TWG parser, and discuss parsing results for the grammar extracted from the RRG treebanks RRGbank and RRGparbank1 (Bladier et al., 2018). Similarly to TAG, TWG has the elementary tree combination operations of substitution and sisteradjunction. Additionally, TWG includes the operation of tree-wrapping substitution, which accounts for preserving the connection between the parts of the discontinuous constituents. Operations similar to tree-wrapping substitution were proposed by (Rambow et al., 1995) as subsertion in D-Tree Grammars (DTG) and by (Rambow et al., 2001) as generalized substitution in D-Tree substitution grammar (DSG). To our best knowledge, no statistical parsing approach was proposed for DTG or DSG. An approach to symbolic parsing for TWGs with edge features was proposed in (Arps et al., 2019). In this work, we propose a statistical parsing approach for TWG and extend the pipeline based on supertagging and A∗ algorithm (Waszczuk, 2017; Bladier et al., 2019) originally developed for TAG to be applied to TWG. The contributions of the paper are the following: 1) We present the first approach to statistical parsing for Tree-Wrapping Grammars. 2"
2020.coling-main.595,W13-5701,0,0.0458749,"Missing"
2020.coling-main.595,P17-1026,0,0.0592948,"Missing"
2020.figlang-1.29,W07-1106,0,0.0591185,"ajor challenge for NLP, for instance for machine translation systems. VIDs exhibit a variety of properties exploitable for determining the correct reading of a candidate expression. On the morphosyntactic level a lot of VIDs are less flexible than their literal counterparts, e.g. the idiomatic kick the bucket is not readily passivizable. On the semantic level VIDs often disrupt the cohesion of a sentence, because of their non-compositionality, or they violate selectional preferences, for example in the sentence The city shows its teeth. Examples for a morphosyntactic approach are the works of Cook et al. (2007) and Fazly et al. (2009). They show that it is possible to leverage automatically acquired knowledge about the syntactic behaviour of VNICs, i.e. their syntactic fixedness, to perform token-level disambiguation. Katz and Giesbrecht (2006) draw on semantic properties by using dense word vectors to identify literal and idiomatic occurrences of the German VID ins Wasser fallen (idiomatically ’to be cancelled’, literally ’to fall into the water’). They assumed that the contexts of the literal and idiomatic use of this expression differ which in turn is represented by their distributional vectors."
2020.figlang-1.29,J17-4005,0,0.141623,"Missing"
2020.figlang-1.29,E17-4011,1,0.855419,"t the syntactic behaviour of VNICs, i.e. their syntactic fixedness, to perform token-level disambiguation. Katz and Giesbrecht (2006) draw on semantic properties by using dense word vectors to identify literal and idiomatic occurrences of the German VID ins Wasser fallen (idiomatically ’to be cancelled’, literally ’to fall into the water’). They assumed that the contexts of the literal and idiomatic use of this expression differ which in turn is represented by their distributional vectors. Test instances are then compared to these vectors in order to classify them. Li and Sporleder (2009) and Ehren (2017) both used cohesion-based graphs for the disambiguation task, the assumption being that semantically idiomatic expressions disrupt the cohesion of the context they appear in. The former used Normalized Google Distance, while the latter used the cosine between word embeddings to capture the semantic similarity of words. To classify the test instances in an unsupervised way, graphs were built based on the two mentioned metrics and if the mean value rose after the removal of the instance, it was classified as idiomatic. Shutova et al. (2010) and Haagsma and Bjerva (2016) employ the knowledge that"
2020.figlang-1.29,J09-1005,0,0.0433803,"for instance for machine translation systems. VIDs exhibit a variety of properties exploitable for determining the correct reading of a candidate expression. On the morphosyntactic level a lot of VIDs are less flexible than their literal counterparts, e.g. the idiomatic kick the bucket is not readily passivizable. On the semantic level VIDs often disrupt the cohesion of a sentence, because of their non-compositionality, or they violate selectional preferences, for example in the sentence The city shows its teeth. Examples for a morphosyntactic approach are the works of Cook et al. (2007) and Fazly et al. (2009). They show that it is possible to leverage automatically acquired knowledge about the syntactic behaviour of VNICs, i.e. their syntactic fixedness, to perform token-level disambiguation. Katz and Giesbrecht (2006) draw on semantic properties by using dense word vectors to identify literal and idiomatic occurrences of the German VID ins Wasser fallen (idiomatically ’to be cancelled’, literally ’to fall into the water’). They assumed that the contexts of the literal and idiomatic use of this expression differ which in turn is represented by their distributional vectors. Test instances are then"
2020.figlang-1.29,fritzinger-etal-2010-survey,0,0.0585412,"Missing"
2020.figlang-1.29,W16-1102,0,0.0143015,"sify them. Li and Sporleder (2009) and Ehren (2017) both used cohesion-based graphs for the disambiguation task, the assumption being that semantically idiomatic expressions disrupt the cohesion of the context they appear in. The former used Normalized Google Distance, while the latter used the cosine between word embeddings to capture the semantic similarity of words. To classify the test instances in an unsupervised way, graphs were built based on the two mentioned metrics and if the mean value rose after the removal of the instance, it was classified as idiomatic. Shutova et al. (2010) and Haagsma and Bjerva (2016) employ the knowledge that metaphors tend to violate selectional preferences to detect them in running text. Building on these insights from previous work, in this paper, we will use a BiLSTM architecture based on different types of word embeddings that is intended to capture the semantic properties of the VID itself, together with the context and the morphosyntactic flexibility of the specific VID instance. 3 3.1 The Creation of the Corpus The Data As mentioned above, literal occurrences of VIDs usually seem to occur quite rarely. The German dataset of the PARSEME 1.1 corpus (Ramisch et al.,"
2020.figlang-1.29,L16-1135,0,0.0375536,"Missing"
2020.figlang-1.29,W06-1203,0,0.220718,"ess flexible than their literal counterparts, e.g. the idiomatic kick the bucket is not readily passivizable. On the semantic level VIDs often disrupt the cohesion of a sentence, because of their non-compositionality, or they violate selectional preferences, for example in the sentence The city shows its teeth. Examples for a morphosyntactic approach are the works of Cook et al. (2007) and Fazly et al. (2009). They show that it is possible to leverage automatically acquired knowledge about the syntactic behaviour of VNICs, i.e. their syntactic fixedness, to perform token-level disambiguation. Katz and Giesbrecht (2006) draw on semantic properties by using dense word vectors to identify literal and idiomatic occurrences of the German VID ins Wasser fallen (idiomatically ’to be cancelled’, literally ’to fall into the water’). They assumed that the contexts of the literal and idiomatic use of this expression differ which in turn is represented by their distributional vectors. Test instances are then compared to these vectors in order to classify them. Li and Sporleder (2009) and Ehren (2017) both used cohesion-based graphs for the disambiguation task, the assumption being that semantically idiomatic expression"
2020.figlang-1.29,W09-3211,0,0.0119204,"ally acquired knowledge about the syntactic behaviour of VNICs, i.e. their syntactic fixedness, to perform token-level disambiguation. Katz and Giesbrecht (2006) draw on semantic properties by using dense word vectors to identify literal and idiomatic occurrences of the German VID ins Wasser fallen (idiomatically ’to be cancelled’, literally ’to fall into the water’). They assumed that the contexts of the literal and idiomatic use of this expression differ which in turn is represented by their distributional vectors. Test instances are then compared to these vectors in order to classify them. Li and Sporleder (2009) and Ehren (2017) both used cohesion-based graphs for the disambiguation task, the assumption being that semantically idiomatic expressions disrupt the cohesion of the context they appear in. The former used Normalized Google Distance, while the latter used the cosine between word embeddings to capture the semantic similarity of words. To classify the test instances in an unsupervised way, graphs were built based on the two mentioned metrics and if the mean value rose after the removal of the instance, it was classified as idiomatic. Shutova et al. (2010) and Haagsma and Bjerva (2016) employ t"
2020.figlang-1.29,N18-1202,0,0.00913066,"r architecture that is best suited for taking the context into account. Figure 3 shows a graph of our architecture. For an input sentence s of length n with words w1 , ..., wn we associate every word wi with its corresponding pretrained word embedding which gives us our input sequence of vectors x1:n : Word Representations During the experiments we employed word representations that were pretrained on other, considerably larger corpora with three different models: Word2vec (Skip-gram) (Mikolov et al., 2013), fastText (CBOW) (Bojanowski et al., 2016) and ELMo (Embeddings from Language Models) (Peters et al., 2018). We trained the Word2vec embeddings ourselves10 on xi = e(wi ) In the case we use Word2vec embeddings, a sequence w1:n consists of lemmas, while for fastText it consists of tokens, because the former model was trained on lemmas and the latter on n-grams. After the embedding assignment the sequence x1:n is fed into a bidirectional recurrent neural net 11 https://fasttext.cc/docs/en/ crawl-vectors.html 12 https://github.com/ t-systems-on-site-services-gmbh/ german-elmo-model 9 In order to allow for a different kind of task at a later point. 10 We used the word2vec implementation of the python ˇ"
2020.figlang-1.29,schafer-bildhauer-2012-building,0,0.0180362,"Missing"
2020.figlang-1.29,C10-1113,0,0.0764585,"Missing"
2020.figlang-1.29,sporleder-etal-2010-idioms,0,0.0702245,"Missing"
2020.lrec-1.881,P17-4019,0,0.321045,"e, a good usability helps also experienced users because it supports a efficient usage. In order to reach usability, Burghardt (2012) extends and adjusts the ten well-known heuristics of Nielsen (1994) regarding linguistic annotation tools. Most of the recommendations, which are equally relevant for semantic frame annotation, are considered in this paper and will be elaborated and referred to during the detailed system description in the following (see Section 3). Another approach to simplify the ease of use and navigation is to allow the users to restrict or filter their list of annotations (Abend et al., 2017). Furthermore, the design criteria regarding the journaling system of the annotation are important. While other tools only save the end version of the annotation, Zeldes (2016) and Marcu et al. (1999) propose to automatically log all states of the annotation during the process (including all additions and revisions). Additionally, in the annotation tools of, e.g. Ringger et al. (2008) and Tomanek and Hahn (2009), the time per annotation step is also stored. 3. System Description The semantic frame annotation tool presented in this paper (SFA) is an open-source, web-based application with a res"
2020.lrec-1.881,S19-2018,0,0.0383568,"Missing"
2020.lrec-1.881,S19-2004,0,0.0415852,"Missing"
2020.lrec-1.881,W09-1903,0,0.0951874,"Missing"
2020.lrec-1.881,burchardt-etal-2006-salto,0,0.0545292,"rchitecture, instructions to reuse and its workflow. Afterwards, Section 4, a description of an already realized use case, exemplifies the usage of the system. Limitations of the tool and points for future work are discussed in Section 5, and Section 6 concludes the paper. 7132 2. Related Work Annotation tools are mostly realized with a graphical or a command-line user interface. For example, Vossen et al. (2018) propose a typing-based command-line tool which might be challenging to use for users with low media literacy or persons using a mobile device. In contrast, the SALTO annotation tool (Burchardt et al., 2006) has a graphical user interface. It was initially developed to annotate semantic roles in the context of semantic frames, but now it is more focused on annotating syntactic structure in a graphical environment. FrameNet itself also offers a frame annotation tool with a graphical user interface2 , but it is only an online demo version for which the code is not freely available. The system provides demo annotation records with a target verb and a pre-annotated frame. On a click-based user interface a user can annotate the core and non-core frame elements. It is similar to the system proposed her"
2020.lrec-1.881,W12-3613,0,0.230867,"on to the annotation tool presented here, these tools focus more on the emerging annotations than on the metadata of the annotation process. Another group of work relevant to SFA is concerned with studying the usability3 of an annotation tool. The evaluation of the ease of use of a web based annotation tool is important because if a web interface is too difficult to use, users will leave the website (Nielsen, 2012) and are presumably not willing to annotate many data. Furthermore, a good usability helps also experienced users because it supports a efficient usage. In order to reach usability, Burghardt (2012) extends and adjusts the ten well-known heuristics of Nielsen (1994) regarding linguistic annotation tools. Most of the recommendations, which are equally relevant for semantic frame annotation, are considered in this paper and will be elaborated and referred to during the detailed system description in the following (see Section 3). Another approach to simplify the ease of use and navigation is to allow the users to restrict or filter their list of annotations (Abend et al., 2017). Furthermore, the design criteria regarding the journaling system of the annotation are important. While other to"
2020.lrec-1.881,W16-4011,0,0.0388496,"Missing"
2020.lrec-1.881,P08-2017,0,0.0521754,"est123), an example annotation record and its referred tokenized sentence are already included. The admin can add more records by uploading files in the admin interface. The upload is split into (a) the annotation records in a specified format9 as recommended in Burghardt (2012, R12-14), and (b) the referred sentences in the CONLL-U format10 . The records file can either contain raw annotation records, which indicate only the sentence and the annotating verb, or pre-annotated records, which also include a name of a frame and optionally pre-annotated frame elements. Following the assumption of Haertel et al. (2008) and Ringger et al. (2008), it is costlier to annotate data from scratch than revising pre-annotated data, so we recommend to use pre-annotated data even if priming is possible. After the upload, the admin assigns the annotation records to one or more users using the provided feature in the admin interface. In an extended version of SFA, new records will be automatically assigned to the users using an active learning algorithm. 3.3.2. Annotation Phase As mentioned before, in the annotation phase, an annotation record will be annotated following distinct annotation steps: 0. reading the annotat"
2020.lrec-1.881,W99-0307,0,0.527873,") regarding linguistic annotation tools. Most of the recommendations, which are equally relevant for semantic frame annotation, are considered in this paper and will be elaborated and referred to during the detailed system description in the following (see Section 3). Another approach to simplify the ease of use and navigation is to allow the users to restrict or filter their list of annotations (Abend et al., 2017). Furthermore, the design criteria regarding the journaling system of the annotation are important. While other tools only save the end version of the annotation, Zeldes (2016) and Marcu et al. (1999) propose to automatically log all states of the annotation during the process (including all additions and revisions). Additionally, in the annotation tools of, e.g. Ringger et al. (2008) and Tomanek and Hahn (2009), the time per annotation step is also stored. 3. System Description The semantic frame annotation tool presented in this paper (SFA) is an open-source, web-based application with a responsive design4 for a modular semantic frame annotation following the guidelines of FrameNet version 1.7. The annotation process is modular in the sense that it is separated into smaller subtasks or s"
2020.lrec-1.881,D18-2019,0,0.0158416,"hance this, we also ask the annotators about their concerns regarding each annotation record after finishing an annotation. This measurement facilitates a high quality of annotations. Furthermore, if the records are sorted by their concern labels, the difficulty of the annotation of different 5 All data which are connected to a user account can always be requested, the tool respects The EU’s regulations concerning privacy (DSGVO in Germany; i.e., in English, General Data Protection Regulation, GDPR). 6 A pro-active learning algorithm, is catered for experienced as well as inexperienced users (Nghiem and Ananiadou, 2018) so that a fault tolerance is included. The implementation of both features is planned for the next version of SFA. frames can be compared. 3.2. System Architecture, Download and Demo Site The annotation system proposed here is a web-based application, a current online version of it is accessible at http://sfa.phil.hhu.de:8080/. The tool works best with the Firefox web browser7 . On other web browsers some visual components working with HTML5 mark-ups may not be adequately rendered. Furthermore, JavaScript is needed, which is normally enabled in web browsers by default. With the user account t"
2020.lrec-1.881,S19-2003,1,0.171655,"experienced and inexperienced users perform suggestion-based and slightly-controlled annotations, the system keeps track of the time and changes annotators made during the annotation process and logs certain metadata. This collected metadata can be used to get new insights regarding the difficulty of annotating specific types of frames, and as an input of an annotation cost measurement for an active learning algorithm. The tool was already used to build a manually annotated corpus with semantic frames and its arguments for task 2 of SemEval 2019 regarding unsupervised lexical frame induction (QasemiZadeh et al., 2019). Although English sentences from the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1999) are annotated for this task, it is also possible to use the proposed tool for the annotation of sentences in any other languages. Keywords: annotation tool, semantic frames, multilingual semantic annotation tool 1. In computational linguistics, manually annotated corpora are in high demand. In machine-learning-based natural language processing tasks, corpora with manual annotations are necessary to train, evaluate, and compare systems and algorithms in terms of quantitative measures. How"
2020.lrec-1.881,S19-2019,0,0.0452938,"Missing"
2020.lrec-1.881,ringger-etal-2008-assessing,0,0.404164,"eferred to during the detailed system description in the following (see Section 3). Another approach to simplify the ease of use and navigation is to allow the users to restrict or filter their list of annotations (Abend et al., 2017). Furthermore, the design criteria regarding the journaling system of the annotation are important. While other tools only save the end version of the annotation, Zeldes (2016) and Marcu et al. (1999) propose to automatically log all states of the annotation during the process (including all additions and revisions). Additionally, in the annotation tools of, e.g. Ringger et al. (2008) and Tomanek and Hahn (2009), the time per annotation step is also stored. 3. System Description The semantic frame annotation tool presented in this paper (SFA) is an open-source, web-based application with a responsive design4 for a modular semantic frame annotation following the guidelines of FrameNet version 1.7. The annotation process is modular in the sense that it is separated into smaller subtasks or steps, the results of which are separately stored. In the following, Section 3.1 introduces the conceptual design of the tool. In Section 3.2, the architecture and use of the annotation sy"
2020.lrec-1.881,E12-2021,0,0.120794,"Missing"
2020.lrec-1.881,W09-3018,0,0.0185495,"iled system description in the following (see Section 3). Another approach to simplify the ease of use and navigation is to allow the users to restrict or filter their list of annotations (Abend et al., 2017). Furthermore, the design criteria regarding the journaling system of the annotation are important. While other tools only save the end version of the annotation, Zeldes (2016) and Marcu et al. (1999) propose to automatically log all states of the annotation during the process (including all additions and revisions). Additionally, in the annotation tools of, e.g. Ringger et al. (2008) and Tomanek and Hahn (2009), the time per annotation step is also stored. 3. System Description The semantic frame annotation tool presented in this paper (SFA) is an open-source, web-based application with a responsive design4 for a modular semantic frame annotation following the guidelines of FrameNet version 1.7. The annotation process is modular in the sense that it is separated into smaller subtasks or steps, the results of which are separately stored. In the following, Section 3.1 introduces the conceptual design of the tool. In Section 3.2, the architecture and use of the annotation system are described. Finally,"
2020.lrec-1.881,tomanek-hahn-2010-annotation,0,0.032517,"ons (here choosing a frame), VI) navigation, and VII) additional resources. number of changes per annotation are saved5 . According to Ringger et al. (2008), the data could be helpful to compare ‘inter-annotators performance’ or could be used to calculate annotation costs for a (pro-)active learning algorithm6 . Based on the annotation cost, the most costly or hardest annotation records would be assigned next to the annotators, so always the most informative annotation records would be picked. The annotation cost is mostly measured based on sentence length or word length (Arora et al., 2009). Tomanek and Hahn (2010) show that the usage of an annotation cost based on timestamps can help to create a highquality corpus of annotated named entities with less amount of annotation records and less time spent on annotation than with randomly picked annotation records. Further investigation is needed to test if the number of changes and the number of annotations for a record can also enhance the development of a corpus with less effort. Furthermore, the time per annotation record can yield information regarding the difficulty of an annotation. To enhance this, we also ask the annotators about their concerns regar"
2020.lrec-1.881,P13-4001,0,0.0629265,"Missing"
2020.lrec-1.881,N16-3001,0,0.101055,"s of Nielsen (1994) regarding linguistic annotation tools. Most of the recommendations, which are equally relevant for semantic frame annotation, are considered in this paper and will be elaborated and referred to during the detailed system description in the following (see Section 3). Another approach to simplify the ease of use and navigation is to allow the users to restrict or filter their list of annotations (Abend et al., 2017). Furthermore, the design criteria regarding the journaling system of the annotation are important. While other tools only save the end version of the annotation, Zeldes (2016) and Marcu et al. (1999) propose to automatically log all states of the annotation during the process (including all additions and revisions). Additionally, in the annotation tools of, e.g. Ringger et al. (2008) and Tomanek and Hahn (2009), the time per annotation step is also stored. 3. System Description The semantic frame annotation tool presented in this paper (SFA) is an open-source, web-based application with a responsive design4 for a modular semantic frame annotation following the guidelines of FrameNet version 1.7. The annotation process is modular in the sense that it is separated in"
2020.readi-1.12,W10-1001,0,0.247236,"oice, might be more or less relevant during the simplification process and also during its evaluation. So far, it has not been investigated whether the relevance of distinct text simplification features differs across languages and domains. We therefore address the following research questions (RQ) in this paper: 2. Related Works Several studies of text readability/simplification analyze or compare texts or sentence pairs with different complexity levels, e.g., Collins-Thompson (2014) or Kauchak et al. (2014) in English, Hancke et al. (2012) in German, Gasperin et al. (2009) or Aluisio et al. (2010) in Portuguese, Pil´an and Volodina (2018) in Swedish, and Scarton et al. (2017) in English, Italian, and Spanish. However, in contrast to the paper in hand, they focus on building either complexity level assessment models using and comparing grouped features sets or on the theoretical justification of these features (Collins-Thompson, 2014) rather than on a comparison of the relevance and statistical significance of the distinct features (see RQ1). Most of the text level features proposed in these studies, e.g., parse tree height, passive voice, length of verb phrases, are also considered in"
2020.readi-1.12,D19-3009,0,0.0647293,"me languages, this feature is only implemented for German and English. 3.2.2. Paired Features The paired features (see Table 3) are grouped into lexical, syntactic, simplification, word embeddings, and machine translation features. Lexical Features. Further, six features are grouped into lexical features. The lexical complexity (Feature 1) might be a relevant feature because a word might be more familiar for a reader the more often it occurs in texts. In order to measure the lexical complexity of the input text, the third quartile of the log-ranks of each token in the frequency table is used (Alva-Manchego et al., 2019). The lexical density –type-token-ratio– (2) is calculated using the ratio of lexical items to the total number of words in the input text (Martin et al., 2018; Collins-Thompson, 2014; Hancke et al., 2012; Scarton et al., 2018). It is assumed that a more complex text has a larger vocabulary than a simplified text (Collins-Thompson, 2014). Following Collins-Thompson (2014), the proportion of function words is a relevant feature for readability and text simplification. In this study, function words (3) are defined using the universal dependency labels “aux”, “cop”, “mark” and “case”. Additionall"
2020.readi-1.12,N19-1102,0,0.164813,"complex the sentence and the higher the amount of processing. The pre-processing with SpaCy includes sentence-splitting, tokenization, lemmatizing, POS-tagging, dependency parsing, named entity recognition, and generating word embeddings. The SpaCy word embeddings are replaced in this study by pre-trained word embeddings of FastText (Grave 11 et al., 2018) to achieve a higher quality . Unless otherwise stated, this data is used to measure the used features. Syntactic Features. We use six syntactic features, computed based on the SpaCy dependency trees and POS tags. Inspired by Niklaus et al. (2019), we measure whether the head of the text is a verb (Feature 1). If the text contains more than one sentence, at least one root must be a verb. 12 Following Universal Dependencies , a verb is most likely to be the head of a sentence in several languages. So, sentences whose heads are not verbs might be ungrammatical or hard to read due to their uncommon structure. Therefore, the feature of whether the head of the sentence is a noun is added (2). Niklaus et al. (2019) also state that a sentence is more likely to be ungrammatical and, hence, more difficult to read if no child of the root is a su"
2020.readi-1.12,C12-1065,0,0.394585,".g., parse tree height, proportion of added lemmas, or usage of passive voice, might be more or less relevant during the simplification process and also during its evaluation. So far, it has not been investigated whether the relevance of distinct text simplification features differs across languages and domains. We therefore address the following research questions (RQ) in this paper: 2. Related Works Several studies of text readability/simplification analyze or compare texts or sentence pairs with different complexity levels, e.g., Collins-Thompson (2014) or Kauchak et al. (2014) in English, Hancke et al. (2012) in German, Gasperin et al. (2009) or Aluisio et al. (2010) in Portuguese, Pil´an and Volodina (2018) in Swedish, and Scarton et al. (2017) in English, Italian, and Spanish. However, in contrast to the paper in hand, they focus on building either complexity level assessment models using and comparing grouped features sets or on the theoretical justification of these features (Collins-Thompson, 2014) rather than on a comparison of the relevance and statistical significance of the distinct features (see RQ1). Most of the text level features proposed in these studies, e.g., parse tree height, pas"
2020.readi-1.12,W18-7005,0,0.0340702,"Missing"
2020.readi-1.12,W10-0406,0,0.036681,"arma et al. (2017). 3.3. 4.1. The results concerning the question whether the feature values of complex texts and its simplified version differ significantly are summarized in Table 2. For all three sentence length features, both readability features, and the parse tree height feature, Wilcoxon signedrank tests indicate at least low but significant effects between the complex and simplified text pairs overall all corpora when analyzing the corpora solely. The result is not surprising since sentence length has already been shown to be a relevant feature in different languages, e.g., in English Napoles and Dredze (2010) and Martin et al. (2018), in German Hancke et al. (2012), and in Portuguese Aluisio et al. (2010). The parse tree height also differs significantly for all corpora in the complex and simplified texts. Pil´an and Volodina (2018) and Napoles and Dredze (2010) also conclude in their studies regarding Swedish and English that the parse tree height is a relevant complexity measurement feature. Considering differences between the proportion of verbs in complex and simplified texts, the Wilcoxon signed-rank tests indicate at least low but significant effects for each corpus except EN-QATS. So, the a"
2020.readi-1.12,W19-8615,0,0.0115659,"hrase, the more complex the sentence and the higher the amount of processing. The pre-processing with SpaCy includes sentence-splitting, tokenization, lemmatizing, POS-tagging, dependency parsing, named entity recognition, and generating word embeddings. The SpaCy word embeddings are replaced in this study by pre-trained word embeddings of FastText (Grave 11 et al., 2018) to achieve a higher quality . Unless otherwise stated, this data is used to measure the used features. Syntactic Features. We use six syntactic features, computed based on the SpaCy dependency trees and POS tags. Inspired by Niklaus et al. (2019), we measure whether the head of the text is a verb (Feature 1). If the text contains more than one sentence, at least one root must be a verb. 12 Following Universal Dependencies , a verb is most likely to be the head of a sentence in several languages. So, sentences whose heads are not verbs might be ungrammatical or hard to read due to their uncommon structure. Therefore, the feature of whether the head of the sentence is a noun is added (2). Niklaus et al. (2019) also state that a sentence is more likely to be ungrammatical and, hence, more difficult to read if no child of the root is a su"
2020.readi-1.12,I17-3001,0,0.0405536,"Missing"
2020.readi-1.12,W18-4606,0,0.104756,"Missing"
2020.readi-1.12,I17-3007,0,0.116899,"consider surface characteristics, e.g., word and sentence length, ignore other relevant factors, such as infrequent words (Collins-Thompson, 2014), and are optimized only for English. Therefore, Collins-Thompson (2014) proposes more sophisticated features, e.g., parse tree height or word frequency, which might be applicable to non-English-languages too. Similar to the research in text readability, most text simplification research is concerned with English, with some exceptions, e.g., Italian (Brunato et al., 2016) or Czech (Baranˇc´ıkov´a and Bojar, 2019), or multi-lingual approaches, e.g., Scarton et al. (2017). Text simplification or readability measurement models with the same feature set for all corpora have been shown to perform well on crosslingual (Scarton et al., 2017), multi-lingual (Yimam et al., 2017), and cross-domain (Gasperin et al., 2009) corpora. However, due to language or domain characteristics, distinct features, e.g., parse tree height, proportion of added lemmas, or usage of passive voice, might be more or less relevant during the simplification process and also during its evaluation. So far, it has not been investigated whether the relevance of distinct text simplification featu"
2020.readi-1.12,L18-1553,0,0.297651,", 1. Do complex texts and its simplified version differ significantly regarding linguistic features? Can languageindependent linguistic features explain at least partially the simplification process? 2. Is the simplification process consistent between corpora across and within domains? 1 3. Is the simplification process consistent between corpora within and across languages? https://github.com/rstodden/TS_corpora_ analysis 77 • Czech (CS) newspaper corpus COSTRA (Baranˇc´ıkov´a 3 and Bojar, 2019) , and • Italian (IT) web data corpus PaCCSS (Brunato et al., 4 2016) . e.g., (Xu et al., 2015) or Scarton et al. (2018), or to build an evaluation metric, e.g., Martin et al. (2018). Martin et al. (2018) implemented several features regarding English text simplification and test whether they correlate with human judgments in order to build an evaluation metric which does not require gold simplifications. Their work is the most similar to ours, but in comparison to them, we will analyze simplification features from another perspective: Instead of comparing with human judgments, we will evaluate the features at their simplification level, language, and domain. The analysis proposed here is based on their impleme"
2020.readi-1.12,D18-1081,0,0.0208065,"ie to the high proportion of only lexical simplification in the IT corpus and high proportion of lexical and syntactic simplification in the DE corpus. The other corpora within one domain are more similar in their distribution, which may explain why they do not differ significantly. Effect Effect ♠♣ ♣ Effect Across Domains. The only significant difference across all domains is the BLEU score (H(2)=1429.0979, p <=.01, r=.12). A Dunn-Bonferonni Post-hoc Test indicates that the web (M =0.61±0.15, N =64,900) and Wikipedia data (M =0.67±0.22, N =19,377) are differing. This confirms the findings of Sulem et al. (2018) that BLEU is not suitable for measuring text simplification. Furthermore, the domains differ also in more features even if not significantly between all domains. The following features show only a significant difference between complex and simplified texts in one of the domains. Table 3: The paired features are presented sorted by their group label. The significant effects per features are highlighted using the following symbols per research question: The ♣ symbol represents within domain results, ♠ across domains, ♦ within languages, and ■ across languages. Black illustrates an effect for al"
2020.readi-1.12,yimam-etal-2017-multilingual,0,0.061366,"Missing"
2020.readi-1.12,D16-1034,0,0.0588195,"Missing"
2020.readi-1.12,L18-1550,0,0.0150578,"implification transaction. The sentence is counted as split if the number of sentences of the complex text is lower than of the simplified text. The sentence is counted as joined if the number of sentences of the complex text is higher than of the simplified text. Word Frequency Features. As another indication for lexical simplification, the word frequency can be used (Martin et al., 2018; Collins-Thompson, 2014). Complex words are often infrequent, so word frequency features may help to identify difficult sentences. The frequency of the words is based on the ranks in the FastText Embeddings (Grave et al., 2018). The average position of all tokens in the frequency table is measured as well as the position of the most infrequent word. Simplification Features. In order to address more simplification transactions, we measure lexical, syntactical, and no changes. A complex-simplified-pair is considered as a lexical simplification if tokens are added or rewritten in the simplified text. A complex-simplified-pair is considered as a syntactic simplification if the text is split or joined. Also, a change from non-projective to projective, passive to active, and a reduction of the parse tree height are consid"
2020.readi-1.12,W13-2902,0,0.0559315,"Missing"
2020.readi-1.12,Q15-1021,0,0.0671843,"Missing"
2020.readi-1.12,Q16-1029,0,0.0187054,"changes. A complex-simplified-pair is considered as a lexical simplification if tokens are added or rewritten in the simplified text. A complex-simplified-pair is considered as a syntactic simplification if the text is split or joined. Also, a change from non-projective to projective, passive to active, and a reduction of the parse tree height are considered as syntactic simplifications. A complex-simplifiedpair is considered as identical if both texts are the same, so no simplification has been applied. As each pair is solely analyzed, the standard text simplification evaluation metric SARI (Xu et al., 2016), which needs several gold references, cannot be considered in the analysis. Word and Sentence Length Features. Word length and sentence length are well-established measurements used for readability measurement. Following Scarton et al. (2018), we distinguish word length in number of characters, and syllables and sentence length in number of characters, syllables, and words. Readability Metric Features. Furthermore, as proposed by Martin et al. (2018), we use readability metrics. Readability metrics calculate based on sentence length and number of syllables the complexity of a text and estimat"
2020.tlt-1.5,2020.coling-main.595,1,0.722134,"ster adjunction. This case is not yet covered by the extraction algorithm presented in Section 4. 4 TWG Extraction To extract TWGs from treebanks, we adapt the top-down algorithm from (Xia, 1999) for TAG. While substituting and sister-adjoining trees can be extracted following the procedure described in (Xia, 1999), we developed a new algorithm to extract d-edge trees which we describe in more detail below.4 Since TWGs do not allow for trees to have crossing branches, but the RRG trees often contain them, such edges need to 4 Additional information on the extraction algorithm can be found in (Bladier et al., 2020). be removed following a rule-based algorithm for re-attaching certain subtrees in the original tree in a preprocessing step. The process of decrossing tree branches concerns only local re-attaching of peripheral constituents and operator projections and can be reverted applying a rule-based back-transformation algorithm after the parsing step. We extract lexically unanchored elementary tree templates (i.e. supertags) for the TWGs. The lexical anchoring happens in the subsequent parsing step. 1. Decross tree branches. First, for local discontinuous constituents (for instance NUCs consisting of"
2021.konvens-1.21,E12-1004,0,0.0371934,"i Multi Multi Multi Multi 0.341 0.342 0.340 0.341 0.340 0.481 0.478 0.485 0.478 0.475 0.651 0.652 0.650 0.657 0.663 0.475 0.475 0.489 0.477 0.483 0.456 0.457 0.460 0.468 0.477 1704 1807 4993 6436 9302 LR BL 0.352 0.463 0.621 0.467 0.401 415874 should be treated as such. The vision-based space is a comparatively richer space (than text) in terms of features (Deng et al., 2009), and requires a more complex architecture for an effective treatment of compounds and constituents. The text semantic space (normalized or otherwise), on the other hand, is known to work well with straightforward inputs (Baroni et al., 2012) and to that effect our results are in line with the previous works. Second, we see that the Mono models outperform the Multi models (column 7). In an ideal scenario, the multi-modal representations should resonate better with cognitive data as compared to those generated from individual semantic spaces. This is because language users do not primarily learn word meanings from reading texts, but by encountering new words in situations that involve and necessitate the integration of various kinds of information present. Combining vision embeddings and text embedding is thus an important step tow"
2021.konvens-1.21,P14-1023,0,0.0458161,"tput layer. To avoid over-fitting, we add a dropout layer in front of each hidden layer with a standard dropout value of 0.5 (Baldi and Sandowski, 2013). We use meansquared-error as the loss function and an additional L2 weight regularization in the range [101 , 10−3 ] at the time of loss computation to further optimize over any parameters that might be outliers. For model optimization we experimented with SGD and Adadelta (Zeiler, 2012). 2.3 Datasets for the compound embeddings Semantic Spaces. The 400-dimensional text and 300-dimensional vision pretrained embeddings were obtained as-is from Baroni et al. (2014) and G¨unther et al. (2020b) respectively. Datasets 3 . The training datasets are obtained from G¨unther et al. (2020b). The dataset for the text models contains 5988 datapoints with 2387 unique constituents and 5988 compounds, the dataset for the vision models 1577 datapoints with 942 constituents and 578 compounds. Since we evaluate model performance on both text and vision data against human behavioural measures (Section 3), we create a test dataset where: 1) for each datapoint, the constituents have an overlap in the text and vision semantic spaces4 ; and, 2) the datapoints in the test set"
2021.konvens-1.21,W17-1728,0,0.0412645,"Missing"
2021.konvens-1.21,N15-1098,0,0.0125632,"model. Both types of model are built with the Keras toolkit (Chollet, 2015) with a TensorFlow back-end (G´eron, 2019). The LR model is inspired by G¨unther et al. (2020b), but does not use the position matrices of the CAOSS model. It has no hidden layers, thus treating all features as independent. In our experiments, we use the LR model as the baseline instead of the CAOSS model for two reasons: 1) In terms of architecture, the two models are analogous; however, 2) CAOSS does not train and test on distinct datasets, which potentially inflates the evaluation results (due to model memorization, Levy et al. 2015) 2 . The NN model, on the other hand, has 1 or more hidden layers that model non-linear relationships between the input and output, and facilitate interactive behavior between the input features. We experimented with 1-4 hidden layers, and report re2 Our datasets are designed towards minimizing memorization. sults up to 3 due to a decline in model performance beyond 3 hidden layers. For both text and vision compound estimations, we employ the same set of model architectures, using text-based embeddings for the former and picture-based embeddings for the latter (Section 2.3). For each datapoint"
C10-1061,W07-1506,0,0.204133,"candidate to be used for data-driven parsing. We evaluate our parser with a grammar extracted from the German NeGra treebank. Our experiments show that datadriven LCFRS parsing is feasible with a reasonable speed and yields output of competitive quality. However, given the expressivity restrictions of PCFG, work on data-driven parsing has mostly excluded non-local dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded, while in NeGra, resp. TIGER, tree transformations are applied which resolve the crossing branches (K¨ubler, 2005; Boyd, 2007, e.g.). Especially for these treebanks, such a transformation is questionable, since it is non-reversible and implies information loss. 1 Introduction Data-driven parsing has largely been dominated by Probabilistic Context-Free Grammar (PCFG). The use of PCFG is tied to the annotation principles of popular treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., l"
C10-1061,J98-2004,0,0.0189416,"ternational Conference on Computational Linguistics (Coling 2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constitue"
C10-1061,P03-1013,0,0.191753,"ents with NeGra Our results are not directly comparable with PCFG parsing results, since LCFRS parsing is a 1 SIMPLE also proved to be infeasible to compute for the small set for the markovization settings v = 2 and h = 1 due to the greatly increased label set with this settings. harder task. However, since the EVALB metric coincides for constituents without crossing branches, in order to place our results in the context of previous work on parsing NeGra, we cite some of the results from the literature which were obtained using PCFG parsers2 : K¨ubler (2005) (Tab. 1, plain PCFG) obtains 69.4, Dubey and Keller (2003) (Tab. 5, sister-head PCFG model) 71.12, Rafferty and Manning (2008) (Tab. 2, Stanford parser with markovization v = 2 and h = 1) 77.2, and Petrov and Klein (2007) (Tab. 1, Berkeley parser) 80.1. Plaehn (2004) obtains 73.16 Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 81.27. The comparison shows that our system delivers competitive results. Additionally, when comparing this to PCFG parsing results, one has to keep in mind that LCFRS parse trees contain non-context-free infor"
C10-1061,N09-1061,0,0.0916083,"Missing"
C10-1061,P02-1018,0,0.0943343,". In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) have been established as a candidate for modeling both discontinuous constituents and nonprojective dependency trees as they occur in treebanks (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). LCFRS exte"
C10-1061,W06-1508,0,0.868586,"t of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maier and Kallmeyer, 2010). This article is mainly dedicated to the presentation of several methods for context summary estimation of parse items, and to an experimental evaluation of their usefulness. The estimates either act as figures-of-merit in a best-first parsing context or as estimates for A∗ parsing. Our evaluation shows that while our parser achieves a reasonable"
C10-1061,N03-1016,0,0.128145,"August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maie"
C10-1061,E09-1055,0,0.153123,"d in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) have been established as a candidate for modeling both discontinuous constituents and nonprojective dependency trees as they occur in treebanks (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). LCFRS extend CFG such that non-terminals can span tuples of possibly non537 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set o"
C10-1061,P04-1042,0,0.216389,"l., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) h"
C10-1061,W10-4415,1,0.88234,"2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maier and Kallmeyer, 2010). This article is mainly dedicated to the presentation of several methods for context summary estimation of parse items, and to an experimental evaluation of their usefulness. The estimates either act as figures-of-merit in a best-first parsing context or as estimates for A∗ parsing. Our evaluation shows that while our parser achieves a reasonable speed already without estimates, the estimates lead to a great reduction of the number of produced items, all while preserving the output quality. Sect. 2 and 3 of the paper introduce probabilistic LCFRS and the parsing algorithm. Sect. 4 presents di"
C10-1061,W10-1407,1,0.875031,"and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an extensive evaluation (Maier, 2010; Maier and Kallmeyer, 2010). This article is mainly dedicated to the presentation of several methods for context summary estimation of parse items, and to an experimental evaluation of their usefulness. The estimates either act as figures-of-merit in a best-first parsing context or as estimates for A∗ parsing. Our evaluation shows that while our parser achieves a reasonable speed already without estimates, the estimates lead to a great reduction of the number of produced items, all while preserving the output quality. Sect. 2 and 3 of the paper introduce probabilistic LCFRS and the parsing al"
C10-1061,H94-1020,0,0.0513173,"cal dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded, while in NeGra, resp. TIGER, tree transformations are applied which resolve the crossing branches (K¨ubler, 2005; Boyd, 2007, e.g.). Especially for these treebanks, such a transformation is questionable, since it is non-reversible and implies information loss. 1 Introduction Data-driven parsing has largely been dominated by Probabilistic Context-Free Grammar (PCFG). The use of PCFG is tied to the annotation principles of popular treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and"
C10-1061,J03-1006,0,0.0835254,"2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules γ3 γ Figure 1: Different domains of locality adjacent strings (see Fig. 1). PCFG techniques, such as Best-First Parsing (Charniak and Caraballo, 1998), Weighted Deductive Parsing (Nederhof, 2003) and A∗ parsing (Klein and Manning, 2003a), can be transferred to LCFRS. Finally, German has attracted the interest of the parsing community due to the challenges arising from its frequent discontinuous constituents (K¨ubler and Penn, 2008). We bring together these developments by presenting a parser for probabilistic LCFRS. While parsers for subclasses of PLCFRS have been presented before (Kato et al., 2006), to our knowledge, our parser is the first for the entire class of PLCFRS. We have already presented an application of the parser on constituency and dependency treebanks together with an"
C10-1061,N07-1051,0,0.0227591,"e small set for the markovization settings v = 2 and h = 1 due to the greatly increased label set with this settings. harder task. However, since the EVALB metric coincides for constituents without crossing branches, in order to place our results in the context of previous work on parsing NeGra, we cite some of the results from the literature which were obtained using PCFG parsers2 : K¨ubler (2005) (Tab. 1, plain PCFG) obtains 69.4, Dubey and Keller (2003) (Tab. 5, sister-head PCFG model) 71.12, Rafferty and Manning (2008) (Tab. 2, Stanford parser with markovization v = 2 and h = 1) 77.2, and Petrov and Klein (2007) (Tab. 1, Berkeley parser) 80.1. Plaehn (2004) obtains 73.16 Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 81.27. The comparison shows that our system delivers competitive results. Additionally, when comparing this to PCFG parsing results, one has to keep in mind that LCFRS parse trees contain non-context-free information about discontinuities. Therefore, a correct parse with our grammar is actually better than a correct CFG parse, evaluated with respect to a transformation o"
C10-1061,W08-1006,0,0.0949213,"Missing"
C10-1061,A97-1014,0,0.821568,"PCFG is tied to the annotation principles of popular treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. In contrast, some other treebanks, such as the German NeGra and TIGER treebanks allow annotation with crossing branches (Skut et al., 1997). Some research has gone into incorporating nonlocal information into data-driven parsing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated"
C10-1061,P87-1015,0,0.871053,"sing. Levy and Manning (2004) distinguish three approaches: 1. Non-local information can be incorporated directly into the PCFG model (Collins, 1999), or can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004). 2. Non-local information can be incorporated into complex labels (Hockenmaier, 2003). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004). This paper pursues the third approach. Our work is motivated by the following recent developments: Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) have been established as a candidate for modeling both discontinuous constituents and nonprojective dependency trees as they occur in treebanks (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). LCFRS extend CFG such that non-terminals can span tuples of possibly non537 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 537–545, Beijing, August 2010 CFG: LCFRS: A A γ1 • • γ2 • that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple hN, T, V, P, Si where a) N is a finite set of non-terminals with a function dim: N → N that d"
C10-1061,W11-3805,0,\N,Missing
C10-1061,N10-1035,0,\N,Missing
C10-1061,E95-1034,0,\N,Missing
C10-1061,E91-1005,0,\N,Missing
C10-1061,J03-4003,0,\N,Missing
C10-1061,W08-1007,0,\N,Missing
C10-1061,P03-1054,0,\N,Missing
C10-1061,W08-1004,0,\N,Missing
C10-1061,P00-1058,0,\N,Missing
C10-1061,P11-2037,0,\N,Missing
C10-1061,N10-1049,0,\N,Missing
C10-1061,H05-1066,0,\N,Missing
C10-1061,W07-2460,0,\N,Missing
C10-1061,W08-1005,0,\N,Missing
C10-1061,W11-2913,1,\N,Missing
C10-1061,D11-1036,0,\N,Missing
C10-1061,P01-1007,0,\N,Missing
C10-1061,P83-1021,0,\N,Missing
C10-1061,W10-1409,0,\N,Missing
C10-1061,W10-4407,0,\N,Missing
C10-1061,N06-1022,0,\N,Missing
C10-1061,W98-0105,0,\N,Missing
E03-1030,W00-2001,0,0.0451495,"Missing"
E03-1030,P01-1019,0,0.247648,"e will compare our approach in work aiming at determining the correct rules and section 5. representations needed to build a representation of In this paper, we explore the idea of a semantic natural language meaning. In particular, compuconstruction method which is based on the TAG tational grammars were developed which by and derived tree and show how a Montague style (unilarge took on Montague's proposal, building sefication based) approach to semantic construction mantic representations in tandem with syntactic can be applied to Feature-Based Tree Adjoining structures. Thus for instance, (Copestake et al., 2001) Grammar (FTAG, (Vijay-Shanker and Joshi, 1988)). shows how to specify a Head Driven Phrase StrucWe relate our approach to existing proposals and ture Grammar (HPSG) which supports the parallel discuss two possibilities for implementation. construction of a phrase structure (or derived) tree 2 Hole semantics and of a semantic representation, (Zeevat et al., 1987) shows it for Unification Categorial GramWe start by introducing the semantic representamar (UCG) and (Dalrymple, 1999) for Lexical Function language we use. As mentioned above, Montional grammar (LFG). tague was using the lambda calcu"
E03-1030,W02-2218,1,0.867149,"first example can be captured as suggested in (Kallmeyer and Joshi, 2002) by ruling out multiple adjunctions (one VP modifier is adjoined to the other rather than both modifiers being applied to the verb) and treating ""usually"" as an ""opaque"" VP Ides to 10 : T(re j , PRO V P NP.V3 V hi), hi meet a 12 M(x2, 2 3) 10 : T(X1 ha), h1 &gt; 12,12 M(Xl, X3) FIG. 6 — Control verbs 5 Related work We now compare our approach with three related proposals: that of basing semantic construction on the TAG derivation tree as put forward in (Kallmeyer and Joshi, 2002); an extension of this proposal presented in (Kallmeyer, 2002b) and the 127 glue semantic approach proposed in (Frank and van Genabith, 2001). 5.1 Semantic construction and the derivation tree As can be seen there is no direct link between ""who"" and the verb introducing its scoping sentence, namely ""think"". Hence the scoping relation between ""who"" and ""does Paul think John said Bill likes"" cannot be captured. A third type of problems occur when several trees are adjoined to distinct nodes of the same tree. This typically occurs when raising verbs interact with long distance dependencies e.g., The LTAG derivation tree records how elementary trees are com"
E03-1030,C00-2087,0,0.0905448,"Missing"
E03-1030,C88-2147,0,0.833456,"ng at determining the correct rules and section 5. representations needed to build a representation of In this paper, we explore the idea of a semantic natural language meaning. In particular, compuconstruction method which is based on the TAG tational grammars were developed which by and derived tree and show how a Montague style (unilarge took on Montague's proposal, building sefication based) approach to semantic construction mantic representations in tandem with syntactic can be applied to Feature-Based Tree Adjoining structures. Thus for instance, (Copestake et al., 2001) Grammar (FTAG, (Vijay-Shanker and Joshi, 1988)). shows how to specify a Head Driven Phrase StrucWe relate our approach to existing proposals and ture Grammar (HPSG) which supports the parallel discuss two possibilities for implementation. construction of a phrase structure (or derived) tree 2 Hole semantics and of a semantic representation, (Zeevat et al., 1987) shows it for Unification Categorial GramWe start by introducing the semantic representamar (UCG) and (Dalrymple, 1999) for Lexical Function language we use. As mentioned above, Montional grammar (LFG). tague was using the lambda calculus. In compuOne grammatical framework for whic"
J05-2003,W01-1807,0,0.0290477,"of the elementary trees of eat and like, respectively. These elementary trees can then be used to add the elementary tree set for does and seem: Both auxiliary trees are adjoined to these trees. Figure 12 shows the corresponding SN-derivation structure. 4. RSN-MCTAG and Range Concatenation Grammar In the following, we show that for each RSN-MCTAG of a certain type (i.e., with an additional restriction), a weakly equivalent simple range concatenation grammar (Boullier 1999, 2000) can be constructed. It has been shown that RCGs generate exactly the class of all polynomially parsable languages (Bertsch and Nederhof 2001; appendix A). Furthermore, as shown in Boullier (1998b), simple RCGs in particular are even weakly equivalent to linear context-free rewriting systems (Weir 1988). As a consequence, one obtains that the languages generated by simple RSN-MCTAGs are mildly context-sensitive. This last property was introduced in Joshi (1985). It includes formalisms that are polynomially parsable, are semilinear, and allow only a limited number of crossing dependencies. (We do not give formal definitions of mild contextsensitivity and of LCFRS, since we do not need these definitions in this article.) Concerning R"
J05-2003,W98-0105,0,0.197861,"mentary trees can then be used to add the elementary tree set for does and seem: Both auxiliary trees are adjoined to these trees. Figure 12 shows the corresponding SN-derivation structure. 4. RSN-MCTAG and Range Concatenation Grammar In the following, we show that for each RSN-MCTAG of a certain type (i.e., with an additional restriction), a weakly equivalent simple range concatenation grammar (Boullier 1999, 2000) can be constructed. It has been shown that RCGs generate exactly the class of all polynomially parsable languages (Bertsch and Nederhof 2001; appendix A). Furthermore, as shown in Boullier (1998b), simple RCGs in particular are even weakly equivalent to linear context-free rewriting systems (Weir 1988). As a consequence, one obtains that the languages generated by simple RSN-MCTAGs are mildly context-sensitive. This last property was introduced in Joshi (1985). It includes formalisms that are polynomially parsable, are semilinear, and allow only a limited number of crossing dependencies. (We do not give formal definitions of mild contextsensitivity and of LCFRS, since we do not need these definitions in this article.) Concerning RSN-MCTAGs in general, that is, without any further res"
J05-2003,2000.iwpt-1.8,0,0.369435,"Missing"
J05-2003,W98-0106,0,0.0556798,"ughs in Figure 1, for example, contains only a nonterminal leaf for the subject NP (a substitution node), and there is no slot for a VP adjunct. The adverb always is added by adjunction at an internal node. Because of these principles, in linguistic applications, combining two elementary trees by substitution or adjunction corresponds to the application of a predicate to an argument. The derivation tree then reflects the predicate-argument structure of the sentence. This is why most approaches to semantics in TAG use the derivation tree as an interface between syntax and semantics (see, e.g., Candito and Kahane 1998; Joshi and Vijay-Shanker 1999; Kallmeyer and Joshi 2003). In this article, we are not particularly concerned with semantics, but one of the goals of the article is to obtain analyses with derivation trees representing the correct predicate-argument dependencies. 2 This minimality is actually the reason that the substitution operation is needed; formally TAGs without substitution and TAGs as introduced above have the same weak and strong generative capacity. 188 Kallmeyer Multicomponent TAGs with Shared Nodes An extension of TAG that has been shown to be useful for several linguistic applicati"
J05-2003,W02-2218,1,0.851301,"nd. Other languages that can be generated by SN-MCTAG and that are not TALs are the counting languages {an1 . . . ank |n ≥ 1} for any k &gt; 4 (for k ≤ 4, these languages are tree-adjoining languages). There are two crucial differences between V-TAG and SN-MCTAG: First, in VTAG, the adjunctions of auxiliary trees from the same set need not be simultaneous. In this respect, V-TAG differs not only from SN-MCTAG, but from any of the different 11 However, viewing a TAG as an SN-MCTAG allows us to obtain a richer set of SN-derivation structures, as introduced in the next section. This is exploited in Kallmeyer (2002) for semantics. 12 The subscript NA in the figure stands for null adjunction; that is, it disallows adjunctions at the node in question. 198 Kallmeyer Multicomponent TAGs with Shared Nodes MCTAGs mentioned above. Secondly, V-TAG is nonlocal in the sense of nonlocal MCTAG, whereas SN-MCTAG is local, even though the locality is not based on the parent relation in the derivation tree, as is the case in standard local MCTAG, but on the SNdominance relation in the derivation tree. As a consequence of the locality, we do not need dominance links (i.e., dominance constraints that have to be satisfied"
J05-2003,W04-3306,1,0.812649,"ntary trees used in the derivation occurs exactly once in the derived tree. In this sense the operation is resource-sensitive. 192 Kallmeyer Multicomponent TAGs with Shared Nodes set, providing scrambling analyses that respect the CETM. This means that the limit they impose on the complexity of the scrambling data one can analyze is variable. Based on empirical studies, it can be chosen sufficiently great such that the grammar covers all scrambling cases that one assumes to occur. 2. The Formalism An informal introduction of (restricted) tree-local MCTAG with shared nodes can also be found in Kallmeyer and Yoon (2004). 2.1 Motivation: The Idea of Shared Nodes Let us consider again example (1) in order to illustrate the general idea of shared nodes. In standard TAG, nodes to which new elementary trees are adjoined or substituted disappear; that is, they are replaced by the new elementary tree. For example, after having performed the derivation steps shown in Figure 2, the root node of the reparieren tree does not exist any longer. It is replaced by the verspricht tree, and its daughters have become daughters of the foot node of the verspricht tree. That is, the root node of the derived tree is considered to"
J05-2003,P94-1036,0,0.878319,"ed infinitive are “moved” out of the embedded VP. This occurs, for instance, in languages such as German, Hindi, Japanese, and Korean. As an example of long-distance scrambling in German, consider example (1): (1) . . . dass [es]1 der Mechaniker [t1 zu reparieren] verspricht . . . that it the mechanic to repair promises ‘. . . that the mechanic promises to repair it’ In example (1), the accusative NP es is an argument of the embedded infinitive zu reparieren, but it precedes der Mechaniker, the subject of the main verb verspricht, and it is not part of the embedded VP. It has been argued (see Rambow 1994a) that in German, there is no bound on the number of scrambled elements and no bound on the depth of scrambling (i.e., in terms of movement, the number of VP borders crossed by the moved element). TAGs are not powerful enough to describe scrambling in German in an adequate way (Becker, Joshi, and Rambow 1991). By this we mean that a TAG analysis of scrambling respecting the CETM and therefore giving the correct predicate-argument structure (i.e., an analysis with each argument attaching to the verb it depends on) is not possible. Let us consider the TAG analysis of example (1) in order to see"
J05-2003,P95-1021,0,0.11901,"Missing"
J05-2003,J01-1004,0,0.313528,"Missing"
J05-2003,J94-1004,0,0.155696,". However, the possibility of exploiting this in order to obtain multiple adjunctions combined with multicomponent tree descriptions has not been pursued so far. 193 Computational Linguistics Volume 31, Number 2 Figure 3 Derivation of (1) dass es der Mechaniker zu reparieren verspricht (‘that the mechanic promises to repair it’) using shared nodes. The notion of shared nodes means in particular that a node can be used for more than one adjunction. (E.g., in Figure 3, two trees were adjoined at the root of the reparieren tree.) A similar idea has led to the definition of extended derivation in Schabes and Shieber (1994). For certain auxiliary trees, Schabes and Shieber allow more than one adjunction at the same node. However, the definition of the derived tree in Schabes and Shieber (1994) is such that if first β1 and then β2 are adjoined at some node µ (i.e., in the derivation tree there are edges from some γ to β1 and β2 , both with the position p of the node µ in γ), then first the whole tree derived from β1 is added to position p, and afterwards the whole tree derived from β2 is added to position p. In other words, before β2 is adjoined, all the trees to be added by adjunction or substitution to β1 must"
J05-2003,J92-4004,0,0.0704918,"e root of the derived tree is the unification of the top of the root of verspricht and the top of the root of reparieren. The bottom feature structure of the lower S node is the unification of the bottom of the foot of verspricht and the bottom of the root of reparieren. In this sense, the root of the reparieren tree gets split into two parts. The upper part merges with the root node of the verspricht tree, and the lower part merges with the foot node of the verspricht tree. 6 In a way, the idea of node sharing is already present in description-based definitions of TAG-related formalisms (see Vijay-Shanker 1992; Rogers 1994; Kallmeyer 2001). This is why these formalisms are monotonic with respect to the node properties described in the tree descriptions. However, the possibility of exploiting this in order to obtain multiple adjunctions combined with multicomponent tree descriptions has not been pursued so far. 193 Computational Linguistics Volume 31, Number 2 Figure 3 Derivation of (1) dass es der Mechaniker zu reparieren verspricht (‘that the mechanic promises to repair it’) using shared nodes. The notion of shared nodes means in particular that a node can be used for more than one adjunction. (E."
J05-2003,C88-2147,0,0.812694,"generative power of the grammar (see Figure 4 for a sample tree-local MCTAG with shared nodes that generates a language that is not a tree-adjoining language).6 Let us go back to example (1). Assume the tree set in Figure 3 for the scrambled NP es. If the idea of shared nodes is adopted, this tree set can be added to reparieren using the root of the derived tree for adjunction of the first tree and the NPacc substitution node for substitution of the second tree. The operation is tree-local, since both nodes are part of the reparieren tree. 5 Actually, in a feature-structure based TAG (FTAG) (Vijay-Shanker and Joshi 1988), the top feature structure of the root of the derived tree is the unification of the top of the root of verspricht and the top of the root of reparieren. The bottom feature structure of the lower S node is the unification of the bottom of the foot of verspricht and the bottom of the root of reparieren. In this sense, the root of the reparieren tree gets split into two parts. The upper part merges with the root node of the verspricht tree, and the lower part merges with the foot node of the verspricht tree. 6 In a way, the idea of node sharing is already present in description-based definition"
J05-2003,E91-1005,0,\N,Missing
J05-2003,2004.jeptalnrecital-long.24,1,\N,Missing
J13-1006,E91-1005,0,0.786968,"Missing"
J13-1006,W98-0105,0,0.112274,"we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a nonterminal can span not only a single string but a tuple of strings of size k ≥ 1. k is thereby called its fan-out. We will notate LCFRS with the syntax of Simple Range Concatenation Grammars (SRCG) (Boullier 1998b), a formalism that is equivalent to LCFRS. A third formalism that is equivalent to LCFRS is Multiple Context-Free Grammar (MCFG) (Seki et al. 1991). Definition 1 (LCFRS) A Linear Context-Free Rewriting System (LCFRS) is a tuple N, T, V, P, S where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rules (1) (1) (m) (m) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) · · · Am (X1 , . . . , Xdim("
J13-1006,2000.iwpt-1.8,0,0.110128,"= {0, . . . , n}. 2. We call a pair l, r ∈ Pos(w) × Pos(w) with l ≤ r a range in w. Its yield l, r(w) is the substring wl+1 . . . wr . 3. For two ranges ρ1 = l1 , r1 , ρ2 = l2 , r2 , if r1 = l2 , then the concatenation of ρ1 and ρ2 is ρ1 · ρ2 = l1 , r2 ; otherwise ρ1 · ρ2 is undefined. 4.  ∈ (Pos(w) × Pos(w))k is a k-dimensional range vector for w iff Aρ  = l1 , r1 , . . . , lk , rk  where li , ri  is a range in w for 1 ≤ i ≤ k. ρ We now define instantiations of rules with respect to a given input string. This definition follows the definition of clause instantiations from Boullier (2000). An instantiated rule is a rule in which variables are consistently replaced by ranges. Because we need this definition only for parsing our specific grammars, we restrict ourselves to ε-free rules containing only variables. Definition 7 (Rule instantiation) ) → Let G = (N, T, V, P, S) be an ε-free monotone LCFRS. For a given rule r = A(α A1 (x1 ) · · · Am (xm ) ∈ P (0 < m) that does not contain any terminals, 1. an instantiation with respect to a string w = t1 . . . tn consists of a function f : V → {i, j |1 ≤ i ≤ j ≤ |w|} such that for all x, y adjacent in one of the  , f (x) · f (y)"
J13-1006,W07-1506,0,0.490627,"Missing"
J13-1006,W08-1004,0,0.0245996,"ight be interesting as well, given that non-projectivity is the dependency-counterpart to discontinuity in constituency parsing. A meaningful comparison is difficult to do for the following reasons, however. Firstly, dependency parsing deals with relations between words, whereas in our case words are not considered in the parsing task. Our grammars take POS tags for a given and construct syntactic trees. Also, dependency conversion algorithms generally depend on the correct identification of linguistic head words (Lin 1995). We cannot rely on grammatical function labels, such as, for example, Boyd and Meurers (2008). Therefore we would have to use heuristics for the dependency conversion of the parser output. This would introduce additional noise. Secondly, the resources one obtains from our PLCFRS parser and from dependency parsers (the probabilistic LCFRS and the trained dependency parser) are quite different because the former contains non-lexicalized internal phrase structure identifying meaningful syntactic categories such as VP or NP while the latter is only concerned with relations between lexical items. A comparison would concentrate only on relations between lexical items and the rich phrase str"
J13-1006,P11-2037,0,0.077393,"Missing"
J13-1006,W10-1409,0,0.133075,"Missing"
J13-1006,J98-2004,0,0.103572,"y the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concernin"
J13-1006,N06-1022,0,0.0313949,"ures 26 and 27 show the average number of items produced by the parser and the parsing times for different sentence lengths. The results indicate that the estimates have the desired effect of preventing unnecessary items from being produced. This is reflected in a significantly lower parsing time. The different behavior of the LR and the LN estimate raises the question of the trade-off between maintaining optimality and obtaining a higher parsing speed. In 112 Kallmeyer and Maier PLCFRS Parsing other words, it raises the question of whether techniques such as pruning or coarseto-fine parsing (Charniak et al. 2006) would probably be superior to A∗ parsing. A first implementation of a coarse-to-fine approach has been presented by van Cranenburgh (2012). He generates a CFG from the treebank PLCFRS, based on the idea of Barth´elemy et al. (2001). This grammar, which can be seen as a coarser version of the actual PLCFRS, is then used for pruning of the search space. The problem that van Cranenburgh tackles is specific to PLCFRS: His PCFG stage generalizes over the distinction of labels by their fan-out. The merit of his work is an enormous increase in efficiency: Sentences with a length of up to 40 words ca"
J13-1006,W10-4407,0,0.0225309,"and yields output of competitive quality. There are three main directions for future work on this subject. r r On the symbolic side, LCFRS seems to offer more power than necessary. By removing symbolic expressivity, a lower parsing complexity can be achieved. One possibility is to disallow the use of so-called ill-nested LCFRS rules. These are rules where, roughly, the spans of two right-hand side non-terminals interleave in a cross-serial way. See the parsing ´ algorithm in Gomez-Rodr´ ıguez, Kuhlmann, and Satta (2010). Nevertheless, this seems to be too restrictive for linguistic modeling (Chen-Main and Joshi 2010; Maier and Lichte 2011). Our goal for future work is therefore to define reduced forms of ill-nested rules with which we get a lower parsing complexity. Another possibility is to reduce the fan-out of the extracted grammar. We have pursued the question whether the fan-out of the trees in the treebank can be reduced in a linguistically meaningful way in Maier, Kaeshammer, and Kallmeyer (2012). On the side of the probabilistic model, there are certain independence assumptions made in our model that are too strong. The main problem in respect is that, due to the definition of LCFRS, we have to d"
J13-1006,W11-2913,1,0.83313,"66.93 60.79 63.71 69.23 70.41 69.81 58.52 57.63 58.07 67.06 58.07 65.18 As for the work that aims to create crossing branches, Plaehn (2004) obtains 73.16 Labeled F1 using Probabilistic Discontinuous Phrase Structure Grammar (DPSG), albeit only on sentences with a length of up to 15 words. On those sentences, we obtain 83.97. The crucial difference between DPSG rules and LCFRS rules is that the former explicitly specify the material that can occur in gaps whereas LCFRS does not. Levy (2005), like us, proposes to use LCFRS but does not provide any evaluation results of his work. Very recently, Evang and Kallmeyer (2011) followed up on our work. They transform the Penn Treebank such that the trace nodes and co-indexations are converted into crossing branches and parse them with the parser presented in this article, obtaining promising results. Furthermore, van Cranenburgh, Scha, and Sangati (2011) and van Cranenburgh (2012) have also followed up on our work, introducing an integration of our approach with Data-Oriented Parsing (DOP). The former article introduces an LCFRS adaption of Goodman’s PCFG-DOP (Goodman 2003). For their evaluation, the authors use the same data as we do in Maier (2010), and obtain an"
J13-1006,N09-1061,0,0.435818,"re 6. The analysis (a) displaying nested dependencies has probability 0.16 and (b) (right-linear dependencies) has probability 0.042. 3. Parsing PLCFRS 3.1 Binarization Similarly to the transformation of a CFG into Chomsky normal form, an LCFRS can be binarized, resulting in an LCFRS of rank 2. As in the CFG case, in the transformation, we introduce a non-terminal for each right-hand side longer than 2 and split the rule into two rules, using this new intermediate non-terminal. This is repeated until all ´ right-hand sides are of length 2. The transformation algorithm is inspired by GomezRodr´ıguez et al. (2009) and it is also specified in Kallmeyer (2010). 3.1.1 General Binarization. In order to give the algorithm for this transformation, we  ∈ [(T ∪ V)∗ ]i by a vector x ∈ V j where all need the notion of a reduction of a vector α  . A reduction is, roughly, obtained by keeping all variables in α  variables in x occur in α that are not in x. This is defined as follows: Definition 5 (Reduction)  ∈ [(T ∪ V)∗ ]i and x ∈ V j for some i, j ∈ IN. Let N, T, V, P, S be an LCFRS, α  1 $ . . . $α  i be the string obtained from concatenating the components of α , Let w = α / (V ∪ T). separated by"
J13-1006,W08-1007,0,0.493997,"y mentioned, note that the full SX estimate and the SX estimate with span and sentence length are monotonic and allow for A∗ parsing. The other two estimates, which are both not monotonic, act as FOMs in a best-first parsing context. Consequently, they contribute to speeding up parsing but they decrease the quality of the parsing output. For further evaluation details see Section 6. 5. Grammars for Discontinuous Constituents 5.1 Grammar Extraction The algorithm we use for extracting an LCFRS from a constituency treebank with crossing branches has originally been presented in Maier and Søgaard (2008). It interprets the treebank trees as LCFRS derivation trees. Consider for instance the tree in Figure 22. The S node has two daughters, a VMFIN node and a VP node. This yields a rule S → VP VMFIN. The VP is discontinuous with two components that wrap around the yield of the VMFIN. Consequently, the LCFRS rule is S(XYZ) → VP(X, Z) VMFIN(Y). The extraction of an LCFRS from treebanks with crossing branches is almost immediate, except for the fan-out of the non-terminal categories: In the treebank, we can have the same non-terminal with different fan-outs, for instance a VP without a gap (fan-out"
J13-1006,E95-1034,0,0.205922,"Candito and Seddah 2010). A rather indirect effect is that morphological richness often relaxes word order constraints. The principal intuition is that a rich morphology encodes information that otherwise has to be conveyed by a particular word order. If, for instance, the case of a nominal complement is not provided by morphology, it has to be provided by the position of the complement relative to other complements in the sentence. Example (1) provides an example of case marking and free word order in German. In turn, in free word order languages, word order can encode information structure (Hoffman 1995). (1) a. der kleine Jungenom schickt seiner Schwesterdat den Briefacc sends his sister the letter the little boy b. Other possible word orders: (i) der kleine Jungenom schickt den Briefacc seiner Schwesterdat (ii) seiner Schwesterdat schickt der kleine Jungenom den Briefacc (iii) den Briefacc schickt der kleine Jungenom seiner Schwesterdat ¨ Sprache und Information, Universit¨atsstr. 1, D-40225 Dusseldorf, ¨ ∗ Institut fur Germany. E-mail: kallmeyer@phil.uni-duesseldorf.de. ¨ Sprache und Information, Universit¨atsstr. 1, D-40225 Dusseldorf, ¨ ∗∗ Institut fur Germany. E-mail: maierw@hhu.de. Sub"
J13-1006,C10-1061,1,0.247685,"that has successfully been used for data-driven parsing.1 The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sections 3 and 4 present the binarization algorithm, the parser, and the outside estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches. 1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG"
J13-1006,W06-1508,0,0.387499,"Missing"
J13-1006,N03-1016,0,0.245933,"-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We have implemented a CYK parser and we prese"
J13-1006,P03-1054,0,0.440556,"-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We have implemented a CYK parser and we prese"
J13-1006,E09-1055,0,0.0646792,"ation in a post- or preprocessing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004; Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter approach. Our work is motivated by the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfat"
J13-1006,P04-1042,0,0.166162,"which produce trees without crossing branches but provide a larger domain of locality than CFG— for instance, through complex labels (Hockenmaier 2003) or through the derivation 90 Kallmeyer and Maier CFG: PLCFRS Parsing • LCFRS: A A γ1 • γ2 • γ3 γ Figure 3 Different domains of locality. mechanism (Chiang 2003). The second class, to which we contribute in this paper, consists of approaches that aim at producing trees which contain non-local information. Some methods realize the reconstruction of non-local information in a post- or preprocessing step to PCFG parsing (Johnson 2002; Dienes 2003; Levy and Manning 2004; Cai, Chiang, and Goldberg 2011). Other work uses formalisms that accommodate the direct encoding of non-local information (Plaehn 2004; Levy 2005). We pursue the latter approach. Our work is motivated by the following recent developments. Linear Context-Free Rewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-te"
J13-1006,W10-1407,1,0.918098,"ssfully been used for data-driven parsing.1 The paper is structured as follows. Section 2 introduces probabilistic LCFRS. Sections 3 and 4 present the binarization algorithm, the parser, and the outside estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches. 1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG"
J13-1006,W12-4615,1,0.853416,"Missing"
J13-1006,W10-4415,1,0.866819,"side estimates which we use to speed up parsing. In Section 5 we explain how to extract an LCFRS from a treebank and we present grammar refinement methods for these specific treebank grammars. Finally, Section 6 presents evaluation results and Section 7 compares our work to other approaches. 1 Parts of the results presented in this paper have been presented earlier. More precisely, in Kallmeyer and Maier (2010), we presented the general architecture of the parser and all outside estimates except the LN estimate from Section 4.4 which is presented in Maier, Kaeshammer, and Kallmeyer (2012). In Maier and Kallmeyer (2010) we have presented experiments with the relative clause split from Section 3.2. Finally, Maier (2010) contains the evaluation of the baseline (together with an evaluation using other metrics). 91 Computational Linguistics Volume 39, Number 1 2. Probabilistic Linear Context-Free Rewriting Systems 2.1 Definition of PLCFRS LCFRS (Vijay-Shanker, Weir, and Joshi 1987) is an extension of CFG in which a nonterminal can span not only a single string but a tuple of strings of size k ≥ 1. k is thereby called its fan-out. We will notate LCFRS with the syntax of Simple Range Concatenation Grammars (SRCG)"
J13-1006,H94-1020,0,0.279531,"pokriva. Of house-DET he repaired roof. “It is the roof of the house he repairs.” b. Gwon.han-ul ˘ nu.ga ka.ji.go iss.ji? Authority-OBJ who has not? “Who has no authority?” Discontinuous constituents are by no means limited to languages with freedom in word order. They also occur in languages with a rather fixed word order such as English, resulting from, for instance, long-distance movements. Examples (4a) and (4b) are examples from the Penn Treebank for long extractions resulting in discontinuous S categories and for discontinuous NPs arising from extraposed relative clauses, respectively (Marcus et al. 1994). (4) a. b. Long Extraction in English: (i) Those chains include Bloomingdale’s, which Campeau recently said it will sell. (ii) What should I do. Extraposed nominal modifiers (relative clauses and PPs) in English: (i) They sow a row of male-fertile plants nearby, which then pollinate the malesterile plants. (ii) Prices fell marginally for fuel and electricity. 1.2 Treebank Annotation and Data-Driven Parsing Most constituency treebanks rely on an annotation backbone based on Context-Free Grammar (CFG). Discontinuities cannot be modeled with CFG, because they require a larger domain of locality"
J13-1006,H05-1066,0,0.164117,"Missing"
J13-1006,J03-1006,0,0.613405,"ewriting Systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987) have been established as a candidate for modeling both discontinuous constituents and non-projective dependency trees as they occur in treebanks (Maier and Søgaard 2008; Kuhlmann and Satta 2009; Maier and Lichte 2011). LCFRSs are a natural extension of CFGs where the non-terminals can span tuples of possibly non-adjacent strings (see Figure 3). Because LCFRSs allow for binarization and CYK chart parsing in a way similar to CFGs, PCFG techniques, such as best-first parsing (Caraballo and Charniak 1998), weighted deductive parsing (Nederhof 2003), and A∗ parsing (Klein and Manning 2003a) can be transferred to LCFRS. Finally, as mentioned before, languages such as German ¨ have recently attracted the interest of the parsing community (Kubler and Penn 2008; ¨ Seddah, Kubler, and Tsarfaty 2010). We bring together these developments by presenting a parser for Probabilistic LCFRS (PLCFRS), continuing the promising work of Levy (2005). Our parser produces trees with crossing branches and thereby accounts for syntactic long-distance dependencies while not making any additional assumptions concerning the position of hypothetical traces. We ha"
J13-1006,P83-1021,0,0.722363,"ot contain any terminals, 1. an instantiation with respect to a string w = t1 . . . tn consists of a function f : V → {i, j |1 ≤ i ≤ j ≤ |w|} such that for all x, y adjacent in one of the  , f (x) · f (y) must be defined; we then define f (xy) = f (x) · f (y), elements of α 2.  )) → A1 ( f (x1 )) · · · Am ( f (xm )) is an if f is an instantiation of r, then A( f (α instantiated rule where f (x1 , . . . , xk  ) =  f (x1 ), . . . , f (xk ). We use a probabilistic version of the CYK parser from Seki et al. (1991). The algorithm is formulated using the framework of parsing as deduction (Pereira and Warren 1983; Shieber, Schabes, and Pereira 1995; Sikkel 1997), extended with weights (Nederhof 2003). In this framework, a set of weighted items representing partial parsing results is characterized via a set of deduction rules, and certain items (the goal items) represent successful parses. During parsing, we have to match components in the rules we use with portions of ] where A ∈ N and ρ  the input string. For a given input w, our items have the form [A, ρ is a range vector that characterizes the span of A. Each item has a weight in that encodes the Viterbi inside score of its best parse tree. More"
J13-1006,W08-1005,0,0.0155553,"lthough the results for LoPar are no surprise, given the similarity of the models implemented by our parser and the Stanford parser, it remains to be investigated why the lexicalization component of the Stanford parser does not lead to better results. In any case the comparison shows that on a data set without crossing branches, our parser obtains the results one would expect. A further data set to which we can provide a ¨ comparison is the PaGe workshop experimental data (Kubler and Penn 2008).7 Table 4 ¨ lists the results of some of the papers in Kubler and Penn (2008) on TIGER, namely, for Petrov and Klein (2008) (P&K), who use the Berkeley Parser (Petrov and Klein 2007); Rafferty and Manning (2008) (R&M), who use the Stanford parser (see above); and Hall and Nivre (2008) (H&N), who use a dependency-based approach (see next paragraph). The comparison again shows that our system produces good results. Again the performance gap between the Stanford parser and our parser warrants further investigation. 6 We have obtained the former parser from http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/ LoPar.html and the latter (Version 2.0.1) from http://nlp.stanford.edu/software/lex-parser.shtml. ¨ 7 Thanks to Sandr"
J13-1006,W08-1006,0,0.181191,"est data from which all category splits have been removed. This metric is equivalent to the corresponding PCFG metric for dim(A) = 1. Despite the shortcomings of such a measure (Rehbein and van Genabith 2007), it still allows to some extent a comparison to previous work in PCFG parsing (see also Section 7). Note that we provide the parser with gold POS tags in all experiments. 6.4 Markovization and Binarization We use the markovization settings v = 1 and h = 2 for all further experiments. The setting which has been reported to yield the best results for PCFG parsing of NeGra, v = 2 and h = 1 (Rafferty and Manning 2008), required a parsing time which was too high.4 Table 2 contains the parsing results for NeGraLCFRS using five different binarizations: Head-driven and KM are the two head-outward binarizations that use a head chosen on linguistic grounds (described in Section 5.2); L-to-R is another variant in which we always choose the rightmost daughter of a node as its head.5 Optimal reorders the left-hand side such that the fan-out of the binarized rules is optimized (described in Section 3.1.2). Finally, we also try a deterministic binarization (Deterministic) in which we binarize strictly from left to ri"
J13-1006,N10-1049,0,0.0549873,"Missing"
J13-1006,W07-2460,0,0.229117,"Missing"
J13-1006,A97-1014,0,0.936409,"iterature on this discussion. With a rather free word order, constituents and single parts of them can be displaced freely within the sentence. German, for instance, has a rich inflectional system and allows for a free word order, as we have already seen in Example (1): Arguments can be scrambled, and topicalizations and extrapositions underlie few restrictions. Consequently, discontinuous constituents occur frequently. This is challenging for syntactic description in general (Uszkoreit 1986; Becker, Joshi, and Rambow 1991; Bunt 1996; ¨ Muller 2004), and for treebank annotation in particular (Skut et al. 1997). In this paper, we address the problem of data-driven parsing of discontinuous constituents on the basis of German. In this section, we inspect the type of data we have to deal with, and we describe the way such data are annotated in treebanks. We briefly discuss different parsing strategies for the data in question and motivate our own approach. 1.1 Discontinuous Constituents Consider the sentences in Example (2) as examples for discontinuous constituents (taken from the German NeGra [Skut et al. 1997] and TIGER [Brants et al. 2002] treebanks). Example (2a) shows several instances of discont"
J13-1006,D11-1036,0,0.0632198,"Missing"
J13-1006,E12-1047,0,0.479869,"Missing"
J13-1006,W11-3805,0,0.193015,"Missing"
J13-1006,P87-1015,0,0.917852,"Missing"
K17-1043,N16-3003,1,0.913316,"suffixes for each dialect in comparison to MSA. As the tables show, MGR has the most number of prefixes, while GLF has the most number of suffixes. Further, there are certain prefixes and suffixes that are unique to dialects. While the prefix “Al” (the) leads the list of prefixes for all dialects, the prefix H . “b” in LEV and EGY, where it is either a progressive particle or a preposition, is used more frequently than in MSA, where it is used strictly as a preposition. Similarly, the suffix “kn” (your) is more frequent in LEV than any á» 5.1 We used the SVM-based ranking approach proposed by Abdelali et al. (2016), in which they used SVM based ranking to ascertain the best segmentation for Modern Standard Arabic (MSA), which they show to be fast and of high accuracy. The approach involves generating all possible segmentations of a word and then ranking them. The possible segmentations are generated based on possible prefixes and suffixes that are observed during training. For example, if hypothetically we only had the prefixes ð “w” (and) and È “l” (to) other dialect. The Negation suffix  “$” (not) and feminine suffix marker No. 8 11 11 14 19 Top 5 Al,w,l,b,f Al,b,w,m,h Al,b,w,l,E Al,w,b,l,mA Al,w,l,"
K17-1043,habash-etal-2012-conventional,0,0.0821494,"ained for each dialect and the number of words they contain. Dialect Egyptian Levantine Gulf Maghrebi No of Tokens 6,721 6,648 6,844 5,495 Table 1: Dataset size for the different dialects We manually segmented each word in the corpus while preserving the original characters. This decision was made to allow processing real dialectal words in their original form. Table 2 shows segmented examples from the different dialects. 3.1 Segmentation Convention In some research projects, segmentation of DA is done on a CODA’fied version of the text, where CODA is a standardized writing convention for DA (Habash et al., 2012). CODA guidelines provide directions on to how to normalize words, correct spelling and unify writing. Nonetheless, these guidelines are not available for all dialects. In the absence of such guidelines as well as the dynamic nature of the language, we choose to operate directly on the raw text. As in contrast to MSA, where guidelines for spelling are common and standardized, written DA seems to exhibit a lot of diversity, and hence, segmentation systems need to be robust enough to handle all the variants that might be encountered in such texts. Our segmentation convention is closer to stemmin"
K17-1043,W11-4417,1,0.839746,"Missing"
K17-1043,N13-1044,0,0.142459,"val. Though much work has focused on segmenting Modern Standard Arabic (MSA), recent work began to examine dialectal segmentation in some Arabic dialects. Dialectal segmentation is becoming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHO"
K17-1043,W09-0807,0,0.0760766,"Missing"
K17-1043,bouamor-etal-2014-multidialectal,0,0.0679135,"Missing"
K17-1043,N16-1030,0,0.011125,"A+MSA). 5.2 Figure 2: Architecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last sev"
K17-1043,D14-1154,1,0.904598,"Missing"
K17-1043,W16-4828,1,0.82771,"social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects"
K17-1043,P16-1101,0,0.0180724,"itecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last several years, they have ac"
K17-1043,maamouri-etal-2014-developing,0,0.0662827,"Missing"
K17-1043,mohamed-etal-2012-annotating,0,0.12055,"Missing"
K17-1043,P14-2034,0,0.0769493,"Missing"
K17-1043,W14-3601,1,0.936065,"Missing"
K17-1043,pasha-etal-2014-madamira,0,0.10814,"Missing"
K17-1043,W17-1306,1,0.586168,"Missing"
K17-1043,J14-1006,0,0.0520662,"important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sente"
K17-1043,N12-1006,0,0.0492725,"oming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), wh"
kallmeyer-etal-2008-developing,schulte-im-walde-2002-subcategorisation,0,\N,Missing
kallmeyer-etal-2008-developing,C96-2120,0,\N,Missing
kallmeyer-etal-2008-developing,P89-1018,0,\N,Missing
kallmeyer-etal-2008-developing,P92-1010,0,\N,Missing
kallmeyer-etal-2008-developing,W08-2316,1,\N,Missing
L18-1015,N16-3003,1,0.699404,"weet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is related more to the linguistic nature of the language rather than the genre. 4. 4.1. would be effective for dialects also, particularly given the overlap between MSA and dialectal Arabic. We used Farasa to determine stem templates (Abdelali et al., 2016). For all the experiments, we trained on the training and dev parts and tested on the test part. As mentioned earlier, we also randomly selected 350 MSA sentences from Arabic Penn Treebank (ATB) and treated MSA as a language variety. Doing so would allow us to observe the divergence of dialects from MSA and the relative effectiveness of using a small dataset compared to much more data. Experiments and Evaluation Experimental Setup For the experiments that we conducted, we used the CRF++ implementation of a CRF sequence labeler with L2 regularization and default value of 10 for the generalizati"
L18-1015,al-sabbagh-girju-2010-mining,0,0.0566271,"Missing"
L18-1015,bouamor-etal-2014-multidialectal,0,0.0463357,"Missing"
L18-1015,J92-4003,0,0.113623,"Missing"
L18-1015,cotterell-callison-burch-2014-multi,0,0.0466115,"Missing"
L18-1015,D14-1154,1,0.889503,"Missing"
L18-1015,W17-1316,1,0.769801,"ur justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouk"
L18-1015,W05-0708,0,0.819563,"Missing"
L18-1015,P06-1086,0,0.314234,"Missing"
L18-1015,habash-etal-2012-conventional,0,0.027441,"prepositions, numbers, and definite articles appear more frequently in MSA than in dialects, while on the other hand dialects show higher frequency of verbs, pronouns and particles. Our justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for"
L18-1015,N13-1044,0,0.508721,"Missing"
L18-1015,N13-1039,0,0.0374619,"Missing"
L18-1015,P08-2030,0,0.65644,"Missing"
L18-1015,W17-1306,1,0.871249,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,K17-1043,1,0.86893,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,W15-1511,0,0.189752,"Missing"
L18-1015,P11-2007,0,0.0736066,"Missing"
L18-1101,baccianella-etal-2010-sentiwordnet,0,0.0887774,"Missing"
L18-1101,W14-3623,0,0.0518071,"Missing"
L18-1101,C16-1251,0,0.0528171,"Missing"
L18-1101,C14-1008,0,0.11537,"Missing"
L18-1101,esuli-sebastiani-2006-sentiwordnet,0,0.0584634,"Missing"
L18-1101,C04-1121,0,0.206222,"Missing"
L18-1101,steinberger-etal-2017-large,0,0.0333011,"Missing"
L18-1101,D14-1181,0,0.00254269,"of neural nets to automatically capture the underlying factors that lead from the input to the output, eliminating the need for feature engineering. 3. Convolutional Neural Networks Convolutional Neural Networks (CNNs) (LeCun et al., 1995) are a powerful deep learning technique because they preserve the spatial structure of the data. They have been shown to produce state-of-the-art results in image processing, computer vision (Krizhevsky et al., 2012) and speech recognition (Graves et al., 2013). In recent years, CNNs have been successfully applied to NLP and document classification problems (Kim, 2014; Johnson and Zhang, 2014). The input to CNNs is a feature map which corresponds to the pixels in an image or words in a sentence or document, or characters in words. This feature map is scanned in CNNs one area at a time by filters, assuming that filters slide, or convolve, around the feature map. The way CNNs adjust their filter weights is through backpropagation, which means that after the forward pass, the network is able to look at the loss function and make a backward pass to update the weights. The CNN layer is followed by a pooling layer that compresses or generalizes over the CNN repr"
L18-1101,P11-1015,0,0.105261,"Missing"
L18-1101,N13-1090,0,0.026573,"Missing"
L18-1101,W04-3253,0,0.10306,"Missing"
L18-1101,S13-2074,0,0.0361896,"Missing"
L18-1101,W02-1011,0,0.0255213,"Missing"
L18-1101,D14-1162,0,0.0773683,"Missing"
L18-1101,perez-rosas-etal-2012-learning,0,0.0374446,"Missing"
L18-1101,remus-etal-2010-sentiws,0,0.0768353,"Missing"
L18-1101,H05-1044,0,0.16501,"Missing"
N15-1134,E14-1039,0,0.0288793,"Missing"
N15-1134,D13-1034,0,0.0258249,"2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algo"
N15-1134,W05-1502,0,0.019724,"Missing"
N15-1134,W13-0808,0,0.0174264,"terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algorithms for automaton and parse table constructions, and section 3.3 presents the parsing algorithm. Section 4 concludes the a"
N15-1134,W09-3808,1,0.801885,"an instantiated clause, the instantiated LHS non-terminal may be replaced with the sequence of instantiated RHS terminals. The language of the grammar is the set of strings which can be reduced to the empty word, starting with S instantiated to the input string. See figure 1 for a sample LCFRS. 2.2 Thread Automata Thread automata (TA) (Villemonte de la Clergerie, 2002) are a generic automaton model which can be parametrized to recognize different mildly contextsensitive languages. The TA for LCFRS (LCFRSTA) implements a prefix-valid top-down incremental parsing strategy similar to the ones of Kallmeyer and Maier (2009) and Burden and Ljungl¨of (2005). An LCFRS-TA for some LCFRS G = (N, T, V, P, S) works as follows. The processing of a single rule is handled by a single thread which will traverse the LHS arguments of the rule. A thread is given by a pair p : X, where p ∈ {1, . . . , m}∗ with m the rank of G is the address, and X ∈ N ∪ {ret} ∪ C where ret ∈ / N is the content of the thread. An automaton state is given by a tuple hi, p, T i where T is a set of threads, the thread store, p is the address of the active thread, and i ≥ 0 indicates that i tokens have been recognized. We introduce a new start symbo"
N15-1134,J13-1006,1,0.655071,"xt-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the foll"
N15-1134,J13-2004,0,0.0240383,"by an automaton which is compiled offline. LR parsers were first introduced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Au"
N15-1134,W10-4415,1,0.778997,"ced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structu"
N15-1134,P98-2156,0,0.117529,"as Tree-Adjoining Grammar. In this paper, we present the first LRstyle parsing algorithm for Linear ContextFree Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG which has received considerable attention in the last years. 1 Introduction LR parsing is an incremental shift-reduce parsing strategy in which the transitions between parser states are guided by an automaton which is compiled offline. LR parsers were first introduced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative mor"
N15-1134,C08-2026,0,0.0235806,"which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algorithms for automaton and parse table constructions, and section 3.3 presents the parsing algorithm. Sectio"
N15-1134,P84-1073,0,0.75177,"r mildly context-sensitive formalisms, such as Tree-Adjoining Grammar. In this paper, we present the first LRstyle parsing algorithm for Linear ContextFree Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG which has received considerable attention in the last years. 1 Introduction LR parsing is an incremental shift-reduce parsing strategy in which the transitions between parser states are guided by an automaton which is compiled offline. LR parsers were first introduced for deterministic context-free languages (Knuth, 1965) and later generalized to context-free languages (Tomita, 1984) and tree-adjoining languages (Nederhof, 1998; Prolo, 2003). Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987) is an immediate extension of CFG in which each non-terminal can cover more than one continuous span of the input string. LCFRS and equivalent formalisms have been used for the modeling of discontinuous constituents (Maier and Lichte, 2011) and nonprojective dependencies (Kuhlmann, 2013), as well as for data-driven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also"
N15-1134,E12-1047,0,0.23332,"Missing"
N15-1134,C02-1028,0,0.757526,"ven parsing of such structures (Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2013; van Cranenburgh, 2012; Angelov and Ljungl¨of, 2014). They have also been used for modeling non-concatenative morphology (Botha and Blunsom, 2013), for grammar engineering (Ranta, 2011), and for modeling alignments in machine translation (Søgaard, 2008; Kaeshammer, 2013). To our knowledge, so far, no LR strategy for LCFRS has been presented in the literature. In this paper, we present an LR-style parser for LCFRS. It is based on the incremental parsing strategy implemented by Thread Automata (Villemonte de la Clergerie, 2002). The remainder of the article is structured as follows. In the following section, we introduce LCFRS and thread automata. Section 3 presents the algorithm along an example. In particular, section 3.2 gives the algorithms for automaton and parse table constructions, and section 3.3 presents the parsing algorithm. Section 4 concludes the article. 2 Preliminaries 2.1 LCFRS In this paper, we restrict ourselves to string rewriting LCFRS and omit the more general definition (Weir, 1988). In LCFRS, a single non-terminal can span k ≥ 1 continuous blocks of a string. A CFG is simply a special case of"
P09-1112,E91-1005,0,0.224501,"languages generated by TT-MCTAG is included in PTIME. We provide a positive answer to this question, using a new characterization of TTMCTAG. 1 Introduction For a large range of linguistic phenomena, extensions of Tree Adjoining Grammars (Joshi et al., 1975), or TAG for short, have been proposed based on the idea of separating the contribution of a lexical item into several components. Instead of single trees, these grammars contain (multi-)sets of trees. Examples are tree-local and set-local multicomponent TAG (Joshi, 1985; Weir, 1988), MCTAG for short, non-local MCTAG with dominance links (Becker et al., 1991), Vector-TAG with dominance links (Rambow, 1994) and, more recently, Tree-Tuple MCTAG with Shared Nodes (Lichte, 2007)), or TT-MCTAG for short. For some of the above formalisms the word recognition problem is NP-hard. This has been shown for non-local MCTAG (Rambow and Satta, 1992), even in the lexicalized case (Champollion, 2007). Some others generate only polynomial languages but their generative capacity is too limited to deal with all natural language phenomena. This has been argued for tree-local and even set-local MCTAG on the basis of scrambling data from lan994 Proceedings of the 47th"
P09-1112,W08-2310,0,0.021556,"the nature of these phenomena is not sufficiently well-understood. Note that, in contrast to non-local MCTAG, in TT-MCTAG the trees coming from the same instance of a tuple in the grammar are not required to be added at the same time. TT-MCTAGs share this property of ‘non-simultaneity’ with other vector grammars such as Unordered Vector Grammars (Cremers and Mayer, 1973) and VectorTAG (Rambow, 1994), V-TAG for short, and it 997 is crucial for the polynomial parsing algorithm. The non-simultaneity seems to be an advantage when using synchronous grammars to model the syntax-semantics interface (Nesson and Shieber, 2008). The closest formalism to TT-MCTAG is V-TAG. However, there are fundamental differences between the two. Firstly, they make a different use of dominance links: In V-TAG dominance links relate different nodes in the trees of a tree set from the grammar. They present dominance requirements that constrain the derived tree. In TT-MCTAG, there are no dominance links between nodes in elementary trees. Instead, the node of a head tree in the derivation tree must dominate all its arguments. Furthermore, even though TT-MCTAG arguments can adjoin with a delay to their head, their possible adjunction si"
P09-1112,J05-2003,1,0.878631,"all of the remaining trees in the set function as arguments of the head. Furthermore, in a TT-MCTAG derivation the argument trees must either adjoin directly to their head tree, or they must be linked in the derivation tree to an elementary tree that attaches to the head tree, by means of a chain of adjunctions at root nodes. In other words, in the corresponding TAG derivation tree, the head tree must dominate the argument trees in such a way that all positions on the path between them, except the first one, must be labeled by ε. This captures the notion of adjunction under node sharing from (Kallmeyer, 2005).2 For a given argument tree β in Γ, h(β) denotes the head of β in Γ. For a given γ ∈ I∪A, a(γ) denotes the set of argument trees of γ, if there are any, or the empty set otherwise. Furthermore, for a given TT-MCTAG G, H(G) is the set of head trees and A(G) is the set of argument trees. Finally, a node v in a derivation tree for G with Lab(v) = γ is called a γ-node. Definition 3 Let G = (VN , VT , S, I, A, T ) be some TT-MCTAG. A derivation tree D = hV, E, ri in the underlying TAG GT is licensed in G if and only if the following conditions (MC) and (SN-TTL) are both satisfied. • (MC): For all"
P09-1112,W08-2308,1,0.76032,"rees of a tree set from the grammar. They present dominance requirements that constrain the derived tree. In TT-MCTAG, there are no dominance links between nodes in elementary trees. Instead, the node of a head tree in the derivation tree must dominate all its arguments. Furthermore, even though TT-MCTAG arguments can adjoin with a delay to their head, their possible adjunction site is restricted with respect to their head. As a result, one obtains a slight degree of locality that can be exploited for natural language phenomena that are unbounded only in a limited domain. This is proposed in (Lichte and Kallmeyer, 2008) where the fact that substitution nodes block argument adjunction to higher heads is used to model the limited domain of scrambling in German. V-TAG does not have any such notion of locality. Instead, it uses explicit constraints, so-called integrity constraints, to establish islands. 3.2 An alternative characterization of TT-MCTAG The definition of TT-MCTAG in subsection 3.1 is taken from (Lichte, 2007; Kallmeyer and Parmentier, 2008). The condition (SN-TTL) on the TAG derivation tree is formulated in terms of heads and arguments belonging together, i.e., coming from the same tuple instance."
P09-1112,P85-1011,0,0.700769,"Missing"
P09-1112,P87-1015,0,0.481062,"ted by the desire to separate the multicomponent property that TTMCTAG shares with a range of related formalisms (e.g., tree-local and set-local MCTAG, VectorTAG, etc.) from the notion of tree-locality with shared nodes that is peculiar to TT-MCTAG. Figure 2 shows a TT-MCTAG derivation for (1). Here, the NPnom auxiliary tree adjoins directly to versucht (its head) while the NPacc tree adjoins to the root of a tree that adjoins to the root of a tree that adjoins to reparieren. TT-MCTAG can generate languages that, in a strong sense, cannot be generated by Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987; Weir, 1988), or LCFRS for short. An example is the language of all strings π(n[1] . . . n[m] )v [1] . . . v [m] with m ≥ 1, π a permutation, and n[i] = n is a nominal argument of v [i] = v for 1 ≤ i ≤ m, i.e., these occurrences come from the same tree set in the grammar. Such a language has been proposed as an abstract description of the scrambling phenomenon as found in German and other free word order languages, Definition 2 A TT-MCTAG is a tuple G = (VN , VT , S, I, A, T ) where GT = (VN , VT , S, I, A) is an underlying TAG and T is a finite set of tree tuples of the form Γ = hγ, {β1 , ."
P09-1112,H86-1020,0,\N,Missing
P09-2003,W05-1502,0,0.0317567,"Missing"
P09-2003,kallmeyer-etal-2008-developing,1,0.891948,"ible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radz"
P09-2003,J91-3002,0,0.0809759,"2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number names. Parsing algorithms for RCG have been introduced by Boullier (2000), who presents a directional top-down parsing algorithm using pseudocode, and Barth´elemy et al. (2001), who add an oracle to Boullier’s algorithm. The more restricted Definition 1. A RCG G = hN, T, V, P, Si consists of a) a finite set of predicates N with an arity function dim: N → N  {0} where S ∈ N is the start predicate with dim(S) = 1, b) disjoint finite sets of terminals T and variables V"
P09-2003,E91-1005,0,0.091443,"or the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number names. Parsing algorithms for RCG have been introduced by Boullier (2000), who presents a directional top-down parsing algorithm using pseudocode, and Barth´elemy et al. (2001), who add an oracle to Boullier’s algorithm. The more restricted Definition 1. A RCG G = hN, T, V, P, Si consists"
P09-2003,C08-2026,0,0.0192457,"on-terminals as late as possible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht,"
P09-2003,W01-1807,0,0.0343216,"s in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number na"
P09-2003,E99-1008,0,0.0336239,"gaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are non-semilinear constructions such as case stacking in Old Georgian (Michaelis and Kracht, 1996) and Chinese number names (Radzinski, 1991). Boullier (1999) shows that RCGs can describe the permutations occurring with scrambling and the construction of Chinese number names. Parsing algorithms for RCG have been introduced by Boullier (2000), who presents a directional top-down parsing algorithm using pseudocode, and Barth´elemy et al. (2001), who add an oracle to Boullier’s algorithm. The more restricted Definition 1. A RCG G = hN, T, V, P, Si consists of a) a finite set of predicates N with an arity function dim: N → N  {0} where S ∈ N is the start predicate with dim(S) = 1, b) disjoint finite sets of terminals T and variables V , c) a finite se"
P09-2003,2000.iwpt-1.8,0,0.488759,"hat we use a technique of range boundary constraint propagation to compute the yields of non-terminals as late as possible. Experiments show that, compared to previous approaches, the constraint propagation helps to considerably decrease the number of items in the chart. 1 2 Preliminaries The rules (clauses) of RCGs1 rewrite predicates ranging over parts of the input by other predicates. E.g., a clause S(aXb) → S(X) signifies that S is true for a part of the input if this part starts with an a, ends with a b, and if, furthermore, S is also true for the part between a and b. Introduction RCGs (Boullier, 2000) have recently received a growing interest in natural language processing (Søgaard, 2008; Sagot, 2005; Kallmeyer et al., 2008; Maier and Søgaard, 2008). RCGs generate exactly the class of languages parsable in deterministic polynomial time (Bertsch and Nederhof, 2001). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS is unable to describe certain natural language phenomena that RCGs actually can deal with. One example are long-distance scrambling phenomena (Becker et al., 1991; Becker et al., 1992). Other examples are n"
P09-2003,C02-1028,0,\N,Missing
P09-2003,P87-1015,0,\N,Missing
P09-2003,P01-1007,0,\N,Missing
P18-3009,D17-1180,0,0.277617,"té ne suffit pas (“The activity does not suffice”) An auxiliary tree has a foot node (marked with ∗) with the same label as the root node. When adjoining an auxiliary tree to some node n, the daughter nodes of n become daughters of the foot node. A sample TAG derivation is shown in Figure 2, in which the elementary trees for Mary and pizza are substituted to the subject and object slots of the likes tree and the auxiliary tree for absolutely is adjoined at the VP-node. vances show the applicability of recurrent neural networks (RNNs) for supertagging (Lewis et al., 2016; Vaswani et al., 2016; Kasai et al., 2017). RNN-based supertagging with LTAGs can be seen as a standard sequence labeling task, albeit with a large set of labels (i.e., several thousand classes as supertags). Our deep learning pipeline is shown in Figure 3. A similar architecture showed good results for POS tagging across many languages (Plank et al., 2016). S NP S NP NP↓ Mary VP V VP AdvP likes VP* NP↓ NP ⇒ Mary VP AdvP absolutely pizza VP V NP likes pizza absolutely Figure 2: Elementary trees and a derived tree in LTAG In a lexicalized version of TAG (LTAG) every tree is associated with a lexical item and represents the span over wh"
P18-3009,W02-2237,0,0.372827,"encies between the sentence tokens (Kipper et al., 2000). 2.2 RNN-based TAG supertagging A supertagger is a partial parsing model which is used to assign a sequence of LTAG elementary trees to the sequence of words in a sentence (Sarkar, 2007). Supertagging can thus be seen as preparation for further syntactic parsing which improves the efficiency of the TAG parser through reducing syntactic lexical ambiguity and sentence complexity. Figure 1 provides an example of supertagging with an LTAG for French. Several techniques were proposed for supertagging over the years, among which are HMMbased (Bäcker and Harbusch, 2002), n-gram-based (Chen et al., 2002), and Lightweight Dependency Analysis models (Srinivas, 2000). Recent adFigure 3: Supertagging architecture based on Samih (2017); dimensions shown in parentheses. We use two kinds of embeddings: pre-trained word embeddings from the Sketch Engine collection of language models (Jakubíˇcek et al., 2013; Bojanowski et al., 2016), and character embeddings based on the training set data. The pretrained word embeddings encode distributional information from large corpora. The advantage of the character embeddings is that they can additionally encode subtoken informa"
P18-3009,J99-2004,0,0.568881,"ons that are not captured by typical statistical parsers based on context-free grammars or dependency parsing. Each derivation step is triggered by a lexical element and a principled distinction is made between its arguments and modifiers, which is reflected in richer derivations. This has applications in the context of other tasks which can make use of linguistically rich analyses, such as frame semantic parsing or semantic role labeling (Sarkar, 2007). On the other hand, the increased expressiveness of LTAG makes efficient parsing and statistical estimations more challenging. Previous work (Bangalore and Joshi, 1999; Sarkar, 2007) has shown that the task of parsing with LTAGs can be facilitated through the intermediate step of supertagging—a task of assigning possible supertags (i.e. elementary trees) for each word in a given sentence (Chen, 2010). Supertagging has been referred to as “almost pars2 2.1 Neural Supertagging with LTAGs Lexicalized Tree Adjoining Grammar A Tree Adjoining Grammar (TAG; Joshi and Schabes, 1997) is a linguistically and psychologically motivated tree rewriting formalism (Sarkar, 2007). A TAG consists of a finite set of elementary trees, which can be combined to form larger trees"
P18-3009,W00-2021,0,0.112162,"hown in Figure 3. A similar architecture showed good results for POS tagging across many languages (Plank et al., 2016). S NP S NP NP↓ Mary VP V VP AdvP likes VP* NP↓ NP ⇒ Mary VP AdvP absolutely pizza VP V NP likes pizza absolutely Figure 2: Elementary trees and a derived tree in LTAG In a lexicalized version of TAG (LTAG) every tree is associated with a lexical item and represents the span over which this item can specify its syntactic or semantic constraints (for example, subject-verb number agreement or semantic roles) capturing also long-distance dependencies between the sentence tokens (Kipper et al., 2000). 2.2 RNN-based TAG supertagging A supertagger is a partial parsing model which is used to assign a sequence of LTAG elementary trees to the sequence of words in a sentence (Sarkar, 2007). Supertagging can thus be seen as preparation for further syntactic parsing which improves the efficiency of the TAG parser through reducing syntactic lexical ambiguity and sentence complexity. Figure 1 provides an example of supertagging with an LTAG for French. Several techniques were proposed for supertagging over the years, among which are HMMbased (Bäcker and Harbusch, 2002), n-gram-based (Chen et al., 2"
P18-3009,P16-1101,0,0.0155675,"65 36 44168 appr. 20 89.32 Supertags Supertags occur. once POS tags Sentences Avg. sentence length Accuracy Table 1: Supertagging experiments morphological features. The embeddings go through a recurrent layer to capture the influence of tokens in the preceding and subsequent context for each token. For the recurrent layer we use either bidirectional Long Short Term Memory (LSTM) or Gated Recurrent Units (GRU). We use a Convolutional Neural Network (CNN) layer for character embeddings, since it was proved to be one of the best options for extracting morphological information from word tokens (Ma and Hovy, 2016). The results for the word and character models are concatenated and fed through a softmax layer that gives a probability distribution for possible supertags. Dropout layers are added to counter overfitting. We replaced words without an entry in the word embeddings with a randomly instantiated vector of the same dimension (100). Table 2 provides an overview of the hyper-parameters we used for the supertagger architecture. Layer Hyper-parameters Value Characters CNN numb. of filters state size 40 400 Bi-GRU state size initial state 400 0.0 Words embedding vector dim. window size 100 5 Char. emb"
P18-3009,candito-etal-2010-statistical,0,0.0835644,"Missing"
P18-3009,P16-2067,0,0.0197302,"trees for Mary and pizza are substituted to the subject and object slots of the likes tree and the auxiliary tree for absolutely is adjoined at the VP-node. vances show the applicability of recurrent neural networks (RNNs) for supertagging (Lewis et al., 2016; Vaswani et al., 2016; Kasai et al., 2017). RNN-based supertagging with LTAGs can be seen as a standard sequence labeling task, albeit with a large set of labels (i.e., several thousand classes as supertags). Our deep learning pipeline is shown in Figure 3. A similar architecture showed good results for POS tagging across many languages (Plank et al., 2016). S NP S NP NP↓ Mary VP V VP AdvP likes VP* NP↓ NP ⇒ Mary VP AdvP absolutely pizza VP V NP likes pizza absolutely Figure 2: Elementary trees and a derived tree in LTAG In a lexicalized version of TAG (LTAG) every tree is associated with a lexical item and represents the span over which this item can specify its syntactic or semantic constraints (for example, subject-verb number agreement or semantic roles) capturing also long-distance dependencies between the sentence tokens (Kipper et al., 2000). 2.2 RNN-based TAG supertagging A supertagger is a partial parsing model which is used to assign a"
P18-3009,W09-1008,0,0.0172767,". Since the number of sentences in FTB is smaller than in TiGer, we created a sample of the train set of the TiGer treebank with a comparable number of sentences in the train set (18,809). For the supertagging experiments with the French LTAG, we diROOT *SENT SENT NP-SUJ↓ 4.1 pas hnegat.i Figure 5: Left-sister-adjunction Since a modifier can appear on the right or on 62 S der Umsatzminus geht auf 125 Millionen [...] zurück vided FTB in the standard train, development and test sets (19,080, 1235, and 1235 sentences), making our test and dev. sets comparable to the dev. and test set reported in Candito et al. (2009). Tables 3 and 5 show the most frequent erroneous supertags for German and French. The symbol < > in the supertags signifies the spot for the lexical anchor, while * marks the foot node of auxiliary trees and ↓ represents a substitution site. 4.2 arated by 11 tokens (see Table 3). Since the window size of tokens presented to the supertagger is limited, the connection between the tokens can be overlooked by the supertagger. However, increasing the window size leads to greater noise in the data. We experimented with window sizes of 5, 9, and 13 for German and got the best results with a window s"
P18-3009,J95-4002,0,0.150713,"en right- and left-sister-adjoining trees (marked with * on the left or the right side of the root label as shown in Figure 5). A left-sister-adjoining tree γ can only be adjoined to a node η in the tree τ if the root label of γ is the same as the label of η and the anchor of the elementary tree τ comes in the sentence before the anchor of γ. The children of γ are inserted on the right side of the children in η and become the children of η. A right-sister-adjunction is defined in a similar way. The resulting LTAGs with sister-adjunction are basically LTIGs (Lexicalized Tree Insertion Grammar; Schabes and Waters, 1995) in the way that the auxiliary trees do not allow wrapping adjunction or adjunction on the root node but permit multiple simultaneous adjunction on a single node of initial trees. However, since LTIG is a special variant of LTAG, we refer to the extracted grammar as LTAG in the remainder of the paper. 3.2 4 VPinf-A OBJ NP-OBJ VN P a ` to V V D N faire make repartir restart l’ the activit´e activity ⇓ PP VPinf-A OBJ P a ` to NP-OBJ VN V V D N faire make repartir restart l’ the activit´e activity Figure 4: FTB preprocessing: complement raising Left- and right-sister-adjunction Extraction of an L"
P18-3009,W02-2236,0,0.103962,"et al., 2000). 2.2 RNN-based TAG supertagging A supertagger is a partial parsing model which is used to assign a sequence of LTAG elementary trees to the sequence of words in a sentence (Sarkar, 2007). Supertagging can thus be seen as preparation for further syntactic parsing which improves the efficiency of the TAG parser through reducing syntactic lexical ambiguity and sentence complexity. Figure 1 provides an example of supertagging with an LTAG for French. Several techniques were proposed for supertagging over the years, among which are HMMbased (Bäcker and Harbusch, 2002), n-gram-based (Chen et al., 2002), and Lightweight Dependency Analysis models (Srinivas, 2000). Recent adFigure 3: Supertagging architecture based on Samih (2017); dimensions shown in parentheses. We use two kinds of embeddings: pre-trained word embeddings from the Sketch Engine collection of language models (Jakubíˇcek et al., 2013; Bojanowski et al., 2016), and character embeddings based on the training set data. The pretrained word embeddings encode distributional information from large corpora. The advantage of the character embeddings is that they can additionally encode subtoken information such as morphological feature"
P18-3009,N16-1027,0,0.11774,"ench LTAG for L’activité ne suffit pas (“The activity does not suffice”) An auxiliary tree has a foot node (marked with ∗) with the same label as the root node. When adjoining an auxiliary tree to some node n, the daughter nodes of n become daughters of the foot node. A sample TAG derivation is shown in Figure 2, in which the elementary trees for Mary and pizza are substituted to the subject and object slots of the likes tree and the auxiliary tree for absolutely is adjoined at the VP-node. vances show the applicability of recurrent neural networks (RNNs) for supertagging (Lewis et al., 2016; Vaswani et al., 2016; Kasai et al., 2017). RNN-based supertagging with LTAGs can be seen as a standard sequence labeling task, albeit with a large set of labels (i.e., several thousand classes as supertags). Our deep learning pipeline is shown in Figure 3. A similar architecture showed good results for POS tagging across many languages (Plank et al., 2016). S NP S NP NP↓ Mary VP V VP AdvP likes VP* NP↓ NP ⇒ Mary VP AdvP absolutely pizza VP V NP likes pizza absolutely Figure 2: Elementary trees and a derived tree in LTAG In a lexicalized version of TAG (LTAG) every tree is associated with a lexical item and repres"
S16-2024,W07-0607,0,0.0180343,"arseness of observations—see Minsky and Papert (1969, chap. 12)).2 On the one hand, although neural networks are often the top performers for addressing this problem, their usage is costly: they need to be trained, which is often very time-consuming,3 and their performance can vary from one task to another depending on their objective function.4 On the other hand, although methods based on random projections efficiently address the problem of reducing the dimensionality of vectors—such as random indexing (RI) (Kanerva et al., 2000), reflective random indexing (RRI), (Cohen et al., 2010), ISA (Baroni et al., 2007) and random Manhattan indexing (RMI) (Zadeh and Handschuh, 2014)—in effect they retain distances between entities in the original space.5 Moreover, since these methods use asymptotic Gaussian or Cauchy random projection matrices R with E(R) = 0, their resulting vectors cannot be adjusted and transformed using weighting techniques such as PPMI. Consequently, these methods often do not outperform neural embeddings and combinations of PPMI weighting of count-based models followed by matrix factorization—such as the truncation of weighted vectors using singular value decomposition (SVD). To overco"
S16-2024,P14-1023,0,0.171615,"adeh DFG SFB 991 DFG SFB 991 Heinrich-Heine-Universit¨at D¨usseldorf Heinrich-Heine-Universit¨at D¨usseldorf D¨usseldorf, Germany D¨usseldorf, Germany kallmeyer@phil.hhu.de zadeh@phil.hhu.de Abstract posed hypothesis. Harris’s distributional hypothesis (Harris, 1954) is a well-known example of step one that states that meanings of words correlate with the environment in which the words appear. Vector space models and η-normed-based similarity measures are notable examples of steps two and three, respectively (i.e., word space models or word embeddings). However, as pointed out for instance by Baroni et al. (2014), the count-based models resulting from the steps two and three are not discriminative enough to achieve satisfactory results; instead, predictive models are required. To this end, an additional transformation step is often added. Turney and Pantel (2010) describe this extra step as a combination of weighting and dimensionality reduction.1 This transformation from count-based to predictive models can be implemented simply via a collection of rules of thumb (such as frequency threshold to filter out highly frequent and/or rare context elements), and/or it can involve more sophisticated mathemat"
S16-2024,W15-1523,0,0.0618486,"Missing"
S16-2024,W14-1503,0,0.0301275,"the MEN relatedness test set (Bruni et al., 2014) and the UKWaC corpus (Baroni et al., 2009). The dataset consists of 3000 pairs of words (from 751 distinct tagged lemmas). Similar to other ‘relatedness tests’, Spearman’s rank correlation ρ score from the comparison of human-based ranking and system-induced rankings is the figure of merit. We use these resources for evaluation since they are in public domain, both the dataset and corpus are large, and they have been used for evaluating several word space models—for example, see Levy et al. (2015), Tsvetkov et al. (2015), Baroni et al. (2014), Kiela and Clark (2014). In this section, unless otherwise stated, we use cosine for similarity measurements. Figure 1 shows the performance of the simple count-based word space model for lemmatizedcontext-windows that extend symmetrically around lemmas from MEN.10 As expected, up to 8 dimensional space. 10 We use the tokenized preprocessed UKWaC. However, except for using part-of-speech tags for locating lemmas Such as many classic search algorithms that are proposed for solving NP-complete problems in artificial intelligence. 9 As opposed to pairwise correlations in the original high192 a certain context-window si"
S16-2024,J90-1003,0,0.321632,"ing cosine. 3.2 0.5 0.4 50 +5 0 Count-based+Cos RI+Cos PoP+Kendall 25 +2 5 0.3 1+ 1 4+ 4 Spearman’s Correlation ρ 0.55 Context Window Size Figure 1: Performance of the classic count-based a-word-per-dimension model vs. RI vs. Pop in the MEN relatedness test. Note that count-based and RI models show almost an identical performance in this task. its performance is still not satisfying. Transformations based on association measures such as PPMI have been proposed to improve the discriminatory power of context vectors and thus the performance of models in semantic similarity assessment tasks (see Church and Hanks (1990), Turney (2001), Turney (2008), and Levy et al. (2015)). For a given set of vectors, pointwise mutual information (PMI) is interpreted as a measure of information overlap between vectors. As put by Bouma (2009), PMI is a mathematical tool for measuring how much the actual probability of a particular co-occurrence (e.g., two words in a word space) deviate from the expected probability of their individual occurrences (e.g., the probability of occurrences of each word in a words space) under the assumption of independence (i.e., the occurrence of one word does not affect the occurrences of other"
S16-2024,D14-1178,0,0.313113,"91 DFG SFB 991 Heinrich-Heine-Universit¨at D¨usseldorf Heinrich-Heine-Universit¨at D¨usseldorf D¨usseldorf, Germany D¨usseldorf, Germany kallmeyer@phil.hhu.de zadeh@phil.hhu.de Abstract posed hypothesis. Harris’s distributional hypothesis (Harris, 1954) is a well-known example of step one that states that meanings of words correlate with the environment in which the words appear. Vector space models and η-normed-based similarity measures are notable examples of steps two and three, respectively (i.e., word space models or word embeddings). However, as pointed out for instance by Baroni et al. (2014), the count-based models resulting from the steps two and three are not discriminative enough to achieve satisfactory results; instead, predictive models are required. To this end, an additional transformation step is often added. Turney and Pantel (2010) describe this extra step as a combination of weighting and dimensionality reduction.1 This transformation from count-based to predictive models can be implemented simply via a collection of rules of thumb (such as frequency threshold to filter out highly frequent and/or rare context elements), and/or it can involve more sophisticated mathemat"
S16-2024,D15-1243,0,0.127875,"ng PoP and RI For evaluation purposes, we use the MEN relatedness test set (Bruni et al., 2014) and the UKWaC corpus (Baroni et al., 2009). The dataset consists of 3000 pairs of words (from 751 distinct tagged lemmas). Similar to other ‘relatedness tests’, Spearman’s rank correlation ρ score from the comparison of human-based ranking and system-induced rankings is the figure of merit. We use these resources for evaluation since they are in public domain, both the dataset and corpus are large, and they have been used for evaluating several word space models—for example, see Levy et al. (2015), Tsvetkov et al. (2015), Baroni et al. (2014), Kiela and Clark (2014). In this section, unless otherwise stated, we use cosine for similarity measurements. Figure 1 shows the performance of the simple count-based word space model for lemmatizedcontext-windows that extend symmetrically around lemmas from MEN.10 As expected, up to 8 dimensional space. 10 We use the tokenized preprocessed UKWaC. However, except for using part-of-speech tags for locating lemmas Such as many classic search algorithms that are proposed for solving NP-complete problems in artificial intelligence. 9 As opposed to pairwise correlations in th"
S17-2039,S17-2002,0,0.0279337,"tion 4. This paper describes the HHU system that participated in Task 2 of SemEval 2017, Multilingual and Cross-lingual Semantic Word Similarity. We introduce our unsupervised embedding learning technique and describe how it was employed and configured to address the problems of monolingual and multilingual word similarity measurement. This paper reports from empirical evaluations on the benchmark provided by the task’s organizers. 1 Introduction 2 The goal of Task 2 of SemEval-2017 is to provide a reliable benchmark for the evaluation of monolingual and multilingual semantic representations (Camacho-Collados et al., 2017). The proposed evaluation benchmark goes beyond classic semantic relatedness tests by providing both monolingual and cross-lingual data sets that include multiword expressions, domain-specific terms, and named entities for five languages. To measure ‘semantic similarity’ between pairs of lexical items, the HHU system uses the algorithm proposed in (QasemiZadeh et al., 2017), which is based on a derandomization of the ‘random positive-only projections’ method proposed by QasemiZadeh and Kallmeyer (2016). Word embedding techniques (i.e., using distributional frequencies to produce word vectors o"
S17-2039,P15-1072,0,0.0183531,"ficial submissions. For Farsi, for the first run, we built vectors of dimension m = 2000, weighted them using cascaded-PPMI (see Section 2.1) and used Pearson’s r as a similarity measure. Evaluated by the organisers, this resulted in r = 0.541, ρ = 0.585, and the official score of H = 0.562. In the second run, however, we built vectors of dimensionality m = 2500 and after cascaded-PPMI weighting, similarities were computed using simlin . This resulted in scores of r = 0.606, ρ = 0.601, and H = 0.604. To choose these configurations, we relied on the trial data as well as resources introduce in Camacho-Collados et al. (2015). For English, we observed that adding n-gram features deteriorates results; hence, we removed this set of features from our model of dimensionality m = 2500. In both runs, we used cascaded-PPMI. As a similarity measure, we used simlin and Pearson’s r in the first and second run, respectively. This produced a score of r = 0.71, ρ = 0.699, and H = 0.704 for the first run, and r = 0.656, ρ = 0.697, and H = 0.676 over the second run. Note that for both languages, we could build any vectors for a number lexical items since they did not occur in the input corpora (see the last column of Table 2 for"
S17-2039,S16-2024,1,0.75251,"Missing"
S17-2039,2017.jeptalnrecital-long.8,1,0.840856,"ns on the benchmark provided by the task’s organizers. 1 Introduction 2 The goal of Task 2 of SemEval-2017 is to provide a reliable benchmark for the evaluation of monolingual and multilingual semantic representations (Camacho-Collados et al., 2017). The proposed evaluation benchmark goes beyond classic semantic relatedness tests by providing both monolingual and cross-lingual data sets that include multiword expressions, domain-specific terms, and named entities for five languages. To measure ‘semantic similarity’ between pairs of lexical items, the HHU system uses the algorithm proposed in (QasemiZadeh et al., 2017), which is based on a derandomization of the ‘random positive-only projections’ method proposed by QasemiZadeh and Kallmeyer (2016). Word embedding techniques (i.e., using distributional frequencies to produce word vectors of reduced dimensionality) are one of the most popular approaches to semantic word similarity problems. These methods are often rationalized using Harris’ Distributional Hypothesis that words of similar linguistic properties appear with/within a similar set of ‘contexts’ (Harris, 1954). For example, words of related meanings co-occur with similar context words {c1 , . . . cn"
S18-2016,P98-1012,0,0.0186712,"iments and Results Set all dev FT 27 15 lemma while in the remaining instances the frame is evoked by different lemmas, and d) last but not least, the frame types themselves have long-tailed distribution. Table 3 shows examples of frames and verb lemmas that lexicalize them; in the table, the most frequent lemma for each frame type is italicized. 4.2 Evaluation Measures We evaluate our method’s performance on a) clustering input strings to frame types, and b) clustering syntactic arguments to semantic role types. To this end, we report the harmonic mean of BCubed precision and recall (B C F) (Bagga and Baldwin, 1998), and purity (P U), inverse purity (I P U) and their harmonic mean (F P U) (Steinbach et al., 2000) as figures of merit. These measures reflect a notion of similarity between the distribution of instances in the obtained clusters and the gold/evaluation data based on certain criteria and alone may lack sufficient information for a fair understanding of the system’s performance. While P U and I P U are easy to interpret (by establishing an analogy between them and precision and recall in classification tasks), they may be deceiving under certain conditions (as explained by Amig´o et al., 2009,"
S18-2016,P98-1013,0,0.806852,"requency of the usages of lexical items in our experiments in the corpora used to train embeddings (i.e., an English web corpus (Sch¨afer, 2015) and PTB’s WSJ). We derive our data for evaluation from the PTB’s WSJ sections parsed (using Schuster and Manning, 2016) to the enhanced UD format. We augment these sentences with semantic role annotations obtained from Prague Semantic Dependencies (PSD) (Cinkova et al., 2012) from the SDP resource (Oepen et al., 2016). Using EngVallex (Cinkov´a et al., 2014) and SemLink (Bonial et al., 2013), we semi-automatically annotate verbs with FrameNet frames (Baker et al., 1998). We choose 1k random sentences and manually verify the semi-automatic mappings to eventually build our evaluation dataset of approximately 5k instances (all). From this data, we use a random subset of 200 instances (dev) during the development and for parameter tuning (see Table 1 for detailed statistics). FI 5,324 200 V 169 35 AT 13 7 RF 0.94 0.95 V 167 34 GR 56 24 AI 10,893 450 AIG 7,305 277 PA 0.67 0.62 RA 0.76 0.76 Input strings extracted from the UD parses: GR, AIG, RF , RA , and PA denote, respectively, the number of distinct grammatical relations, syntactic arguments that are a semanti"
S18-2016,D13-1178,0,0.0233078,"tic role induction (Carreras and Marquez, 2005; Lang and Lapata, 2010, 2011; Titov and Klementiev, 2012; Swier and Stevenson, 2004), our method differs from them in that we attempt to include frame head grouping information for inducing roles associated to them. In other words, these methods leave out the problem of sense/frame grouping in their models. Our work differs in objective from methods for unsupervised template induction in information extraction (IE) (e.g., MUC-style frames in Chambers and Jurafsky (2009, 2011) and its later refinements such as (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013), and in a broader sense attempts towards ontology learning and population from text (Cimiano et al., 2005)). Our focus is on lexicalized elementary syntactic structures, identifying lexical semantic relationships, and thereby finding salient patterns in syntax–semantic interface. However, in IE BCF 55.43 0.36 76.71 7.79 11.71 74.65 ±1.38 Table 5: Results on clustering of syntactic arguments to semantic roles. many incomplete yet homogeneous clusters (as we discuss below). With respect to roles, however, the method’s performance and its output remains very similar to the syntactic baseline (B"
S18-2016,P09-1068,0,0.0457929,"input). Despite similarities between our method and those proposed previously to address unsupervised semantic role induction (Carreras and Marquez, 2005; Lang and Lapata, 2010, 2011; Titov and Klementiev, 2012; Swier and Stevenson, 2004), our method differs from them in that we attempt to include frame head grouping information for inducing roles associated to them. In other words, these methods leave out the problem of sense/frame grouping in their models. Our work differs in objective from methods for unsupervised template induction in information extraction (IE) (e.g., MUC-style frames in Chambers and Jurafsky (2009, 2011) and its later refinements such as (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013), and in a broader sense attempts towards ontology learning and population from text (Cimiano et al., 2005)). Our focus is on lexicalized elementary syntactic structures, identifying lexical semantic relationships, and thereby finding salient patterns in syntax–semantic interface. However, in IE BCF 55.43 0.36 76.71 7.79 11.71 74.65 ±1.38 Table 5: Results on clustering of syntactic arguments to semantic roles. many incomplete yet homogeneous clusters (as we discuss below). With respect"
S18-2016,P11-1098,0,0.0551436,"Missing"
S18-2016,W16-5901,0,0.0258635,"ally, given finite sets of frames F and of semantic roles R, our underlying CFG G = hN, T, P, Si is as follows: • T = Tv ∪Tn ∪D∪{root, EOS}, where Tv is the set of possible verbal heads, Tn is the set of possible lexicalizations (fillers) for arguments, and D is a finite set of dependency relations; root and EOS are special symbols. f • N = {S} ∪ {Fhf |f ∈ F} ∪ {Frem |f ∈ F} ∪ f r {Fg |f ∈ F, g ∈ D}∪{R |r ∈ R}∪{V f |f ∈ F} ∪ {Dg |g ∈ D}. • P contains the following rules: 3.1.1 As a solution to sparsity of observations, we modify the IO algorithm slightly. We adapt the procedures described in (Eisner, 2016) with the exception that for computing inside and outside probabilities, instead of mapping terminals to nonterminals using an exact matching of the right-handsides of the rules (and respectively their assigned parameters), we use embedding-based similarities. I.e., for computing inside probabilities, given symbol a as input, instead of considering A →θ as rewrite rules and updating the parse chart only by asserting θ in it, we also consider B →θ bs in which instead of θ we assert α × θs in the IO table, where α is the r2 coefficient correlations of embeddings for a and bs. During the outside"
S18-2016,P13-1121,1,0.796956,"he verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that evoke the same frame (e.g., to group instances of pack that evoke the frame P LACING with instances of verbs load, pile, place, and so on when used to evoke the same P LACING frame). The motivation for this work is twofold. On the one hand, the frame induction techniques we propose can be useful in the context of applications such as text summarization (Cheung and Penn, 2013), question answering (Frank et al., 2007; Shen and Lapata, 2007), and so on, for languages where we lack a frame-annotated resource for supervised frame induction, or to expand the coverage of already existing resources. On the other hand, we are interested in theoretical linguistic insights into frame structure. In this sense, our We present a method for unsupervised lexical frame acquisition at the syntax–semantics interface. Given a set of input strings derived from dependency parses, our method generates a set of clusters that resemble lexical frame structures. Our work is motivated not on"
S18-2016,C14-1123,0,0.0202861,"le 4: Results for head groupings: # C denotes the number of induced clusters by each method/baseline; the last two rows reports the average and the standard deviation for the obtained results using the L-PCFG model.The remaining abbreviations are introduced in § 4.2 and 4.3. Method A LL I N 1 1CP ER I 1CP ER G R R24 TK -URL L-PCFG (Avg.) L-PCFG (Std. Dev.) #C 1 7257 32 24 333 24 ±5.29 PU 47.73 100 92.89 47.73 85.7 90.36 ±0.33 IPU 100 0.18 79.83 5.36 15.01 79.25 ±1.31 FPU 64.62 0.36 85.86 9.64 25.54 84.44 ±0.61 5 Related Work Our work differs from most work on word sense induction (WSI), e.g. (Goyal and Hovy, 2014; Lau et al., 2012; Manandhar et al., 2010; Van de Cruys and Apidianaki, 2011), in that not only do we discern different senses of a lexical item but also we group the induced senses into more general meaning categories (i.e., FrameNet’s grouping). Hence, our model must be able to capture lexical relationships other than polysemy, e.g., synonymy, antonymy (opposite verbs), troponymy, etc.. However, our method can be adapted to WSI, too. Firstly, we can assume that word senses are ‘incompatible’ and thus they necessarily evoke different frames; subsequently, the induced frame clusters can be se"
S18-2016,N13-1104,1,0.953969,"dobj and iobj and they precede prepositional and complement dependents (i.e., nmod:* and *comp).1 Consider (1) as an example; the corresponding string is the yield of the tree in Fig. 1. 2 From a Latent Model to L-PCFG (1) We assume that frames and semantic roles are the latent variables of a probabilistic model. Given the probability mass function pmf(F 1 , . . . , F n , R1 . . . Rk , D1 , . . . , Dm ; C, θ) as our model, we denote latent frames F i , 1 ≤ i ≤ n, and roles Ri , 1 ≤ i ≤ k for observations that are annotated syntactically using Di , 1 ≤ i ≤ m in the input corpus C. Inspired by Cheung et al. (2013), we approximate the probability of a specific frame f with head v, semantic roles r1 . . . rk filled by words w1 . . . wk and corresponding syntactic dependencies d1 . . . dk (under Johnsubj offerroot flowersdobj to Marynmod:to Given the fixed structure of input strings, we design a CFG that rewrites them to our expected hierarchical frame structure consisting of elements F , R, D while capturing the conditional probabilities from Eq. (1). The tree assigning a frame F of type x with semantic roles of type a, b, c to (1) is for 1 Phrasal arguments are reduced to their syntactic head given by t"
S18-2016,P04-1048,0,0.0791473,"¨at D¨usseldorf McGill University kallmeyer@hhu.de zadeh@phil.hhu.de jcheung@cs.mcgill.ca Abstract types in FrameNet. This allows us to generalize across frames concerning semantic roles. As part of this, we study possible ways to automatically generate more abstract lexical-semantic representations from lexicalized dependency structures. In our task, grouping verb tokens into frames requires not only distinguishing between different senses of verbs, but also identifying a range of lexical relationships (e.g., synonymy, opposite verbs, troponymy, etc.) among them. Hence (as Modi et al., 2012; Green et al., 2004), our problem definition differs from most work on unsupervised fine-grained frame induction using verb sense disambiguation (e.g., Kawahara et al., 2014; Peng et al., 2017). Similarly, forming role clusters yields generalization from several alternate linkings between semantic roles and their syntactic realization. Given, for instance, an occurrence of the verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that"
S18-2016,J98-4004,0,0.114592,"een verb types and frame types. But similar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is"
S18-2016,E14-1007,0,0.042034,"Missing"
S18-2016,N03-1016,0,0.066084,"ice of the frame f and the semantic roles r of the k fillers (i.e., x, a, b, and c in Fig. 1). The probabilities of the rules correspond to the conditional probabilities in Eq. (1). The probabilf ity of S → Fhf Frem gives p(F = f ), the probf ability of V → v gives p(V = v|F = f ), and so on. During the subsequent inside-outside (IO) split-and-merge training procedure, the inventory of frames and roles and the probabilities corresponding to our rules are estimated so that the overall likelihood of observations is maximized. 3 The IO Algorithm 3.1.2 Split We alter the splitting procedure from (Klein and Manning, 2003a; Petrov et al., 2006) for our application. In (Klein and Manning, 2003b; Petrov et al., 2006), during split, a non-terminal symbol (which represents a random variable in the underlying probabilistic model) is split and its related production rules are duplicated independently of its parent, or sibling nodes. We can apply such a context-independent split only to the Rr nonf s must split dependently w.r.t. terminals but the F... their sibling nodes that define the frame structure. Therefore, to split frame x to two frames y and z, x , Fx → we replace the entire set {S → Fhx Frem h x x x , V x"
S18-2016,W17-3405,0,0.0171902,"aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is that we can adapt and reuse statistical inference techniqu"
S18-2016,P03-1054,0,0.0238273,"ice of the frame f and the semantic roles r of the k fillers (i.e., x, a, b, and c in Fig. 1). The probabilities of the rules correspond to the conditional probabilities in Eq. (1). The probabilf ity of S → Fhf Frem gives p(F = f ), the probf ability of V → v gives p(V = v|F = f ), and so on. During the subsequent inside-outside (IO) split-and-merge training procedure, the inventory of frames and roles and the probabilities corresponding to our rules are estimated so that the overall likelihood of observations is maximized. 3 The IO Algorithm 3.1.2 Split We alter the splitting procedure from (Klein and Manning, 2003a; Petrov et al., 2006) for our application. In (Klein and Manning, 2003b; Petrov et al., 2006), during split, a non-terminal symbol (which represents a random variable in the underlying probabilistic model) is split and its related production rules are duplicated independently of its parent, or sibling nodes. We can apply such a context-independent split only to the Rr nonf s must split dependently w.r.t. terminals but the F... their sibling nodes that define the frame structure. Therefore, to split frame x to two frames y and z, x , Fx → we replace the entire set {S → Fhx Frem h x x x , V x"
S18-2016,N10-1137,0,0.152583,"te verbs), troponymy, etc.. However, our method can be adapted to WSI, too. Firstly, we can assume that word senses are ‘incompatible’ and thus they necessarily evoke different frames; subsequently, the induced frame clusters can be seen directly as clusters of word senses. Otherwise, the proposed method can be adapted for WSI by altering its initialization, e.g., by building one-model-at-a-time for each word form (i.e., simply altering the input). Despite similarities between our method and those proposed previously to address unsupervised semantic role induction (Carreras and Marquez, 2005; Lang and Lapata, 2010, 2011; Titov and Klementiev, 2012; Swier and Stevenson, 2004), our method differs from them in that we attempt to include frame head grouping information for inducing roles associated to them. In other words, these methods leave out the problem of sense/frame grouping in their models. Our work differs in objective from methods for unsupervised template induction in information extraction (IE) (e.g., MUC-style frames in Chambers and Jurafsky (2009, 2011) and its later refinements such as (Chambers, 2013; Cheung et al., 2013; Balasubramanian et al., 2013), and in a broader sense attempts toward"
S18-2016,P11-1112,0,0.0620286,"Missing"
S18-2016,J05-1004,0,0.243132,"rforms several baselines on a portion of the Wall Street Journal sentences that we have newly annotated for evaluation purposes. 1 Introduction We propose a method for building coarse lexical frames automatically from dependency parsed sentences; i.e., without using any explicit semantic information as training data. The task involves grouping verbs that evoke the same frame (i.e., are considered to be the head of this frame) and further clustering their syntactic arguments into latent semantic roles. Hence, our target structures stand between FrameNet (Ruppenhofer et al., 2016) and PropBank (Palmer et al., 2005) frames. Similar to FrameNet and in contrast to PropBank, we assume a many-to-many relationship between verb types and frame types. But similar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations be"
S18-2016,E12-1060,0,0.0314706,"Missing"
S18-2016,K17-1019,0,0.0196706,"mantic roles. As part of this, we study possible ways to automatically generate more abstract lexical-semantic representations from lexicalized dependency structures. In our task, grouping verb tokens into frames requires not only distinguishing between different senses of verbs, but also identifying a range of lexical relationships (e.g., synonymy, opposite verbs, troponymy, etc.) among them. Hence (as Modi et al., 2012; Green et al., 2004), our problem definition differs from most work on unsupervised fine-grained frame induction using verb sense disambiguation (e.g., Kawahara et al., 2014; Peng et al., 2017). Similarly, forming role clusters yields generalization from several alternate linkings between semantic roles and their syntactic realization. Given, for instance, an occurrence of the verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that evoke the same frame (e.g., to group instances of pack that evoke the frame P LACING with instances of verbs load, pile, place, and so on when used to evoke the same P LACI"
S18-2016,N09-1069,0,0.0239812,"large web corpora. Each frame instance is then represented using a (m + 1, n)-tensor, in which m is the total number of argument types/clusters given by our model at its current stage and n is the dimensionality of the embeddings that represent words that fill these arguments. To this, we add the embedding for the verb that lexicalizes the head of the frame, which gives us the final (m + 1, n)-tensor. For two frame-instances represented by tensors a and b, the similarity for their arguments is Parameters for this new merged G are updated through a few IO iterations (in an incremental fashion (Liang and Klein, 2009)), and finally G is used to obtain a new clustering. The process is repeated for this newly obtained clustering until all the resulting cluster-wise sc similarities are less than a threshold β. sim-arg(a, b) = m 1 X 2 a ~i b ~i r ( v , v ), k i=1 in which v~i s are embeddings for the ith argument P filler ( nj=1 v~i j 6= 0), r2 is the coefficient of deP 2 a ~i b ~i termination, and k = [r ( v , v ) 6= 0]. If an i Computing all derivations for each input string is time consuming and makes the merge process computationally expensive, particularly in the first few iterations. We resolve this issu"
S18-2016,D08-1048,0,0.0776969,"Missing"
S18-2016,P06-1055,0,0.330051,"ilar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is that we can adapt and reuse statistical infe"
S18-2016,P05-1010,0,0.0857042,"and frame types. But similar to PropBank, we aim to cluster syntactic arguments into general semantic roles instead of frame-specific slot * Both authors contributed equally to this work. 130 Proceedings of the 7th Joint Conference on Lexical and Computational Semantics (*SEM), pages 130–141 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics work is a step towards an empirical investigation of frames and semantic roles including hierarchical relations between them. S x Frem Fhx We cast the frame induction task as unsupervised learning using an L-PCFG (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Cohen, 2017). As input, our model takes syntactic dependency trees and extracts input strings corresponding to instances of frame expressions, which are subsequently grouped into latent semantic frames and roles using an L-PCFG. We use the insideoutside (i.e., Expectation-Maximization (Dempster et al., 1977; Do and Batzoglou, 2008)) algorithm and a split-merge procedure (Petrov et al., 2006) for dynamically adapting the number of frames and roles to the data, for which we employ new heuristics. As implied, one advantage of the L-PCFGs framework is that we can adapt and r"
S18-2016,S17-2039,1,0.882872,"Missing"
S18-2016,P08-1028,0,0.070302,"and arg mini,j sim(ai , bj ) (sim is given by Eq. 2 below) and calculate their harmonic mean as the similarity sc between cx and cy . Cluster pairs are sorted in a descending order by sc . Given a threshold δ, for all sc (cx , cy ) &gt; δ, their corresponding production rules (i.e., the similar set of rules mentioned in the split procedure) are merged and their parameters are updated to the arithmetic mean of their origin rules. Similarity Between Frame Instances When necessary (such as during merge), we compute embedding-based similarities between frame instances similar to methods proposed in (Mitchell and Lapata, 2008; Clark, 2013). We build a ndimensional embedding for each word appearing in our input strings from large web corpora. Each frame instance is then represented using a (m + 1, n)-tensor, in which m is the total number of argument types/clusters given by our model at its current stage and n is the dimensionality of the embeddings that represent words that fill these arguments. To this, we add the embedding for the verb that lexicalizes the head of the frame, which gives us the final (m + 1, n)-tensor. For two frame-instances represented by tensors a and b, the similarity for their arguments is P"
S18-2016,W12-1901,0,0.706951,"ich-Heine-Universit¨at D¨usseldorf McGill University kallmeyer@hhu.de zadeh@phil.hhu.de jcheung@cs.mcgill.ca Abstract types in FrameNet. This allows us to generalize across frames concerning semantic roles. As part of this, we study possible ways to automatically generate more abstract lexical-semantic representations from lexicalized dependency structures. In our task, grouping verb tokens into frames requires not only distinguishing between different senses of verbs, but also identifying a range of lexical relationships (e.g., synonymy, opposite verbs, troponymy, etc.) among them. Hence (as Modi et al., 2012; Green et al., 2004), our problem definition differs from most work on unsupervised fine-grained frame induction using verb sense disambiguation (e.g., Kawahara et al., 2014; Peng et al., 2017). Similarly, forming role clusters yields generalization from several alternate linkings between semantic roles and their syntactic realization. Given, for instance, an occurrence of the verb pack and its syntactic arguments, not only do we aim to distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’"
S18-2016,L16-1376,0,0.0432275,"Missing"
S18-2016,E12-1003,0,0.563617,"Missing"
S18-2016,P11-1148,0,0.0462252,"Missing"
S18-2016,D07-1002,0,0.353749,"distinguish different senses of the verb pack (e.g., as used to evoke the F ILLING frame, or the P LACING frame), but also to group these instances of ‘pack’ with other verbs that evoke the same frame (e.g., to group instances of pack that evoke the frame P LACING with instances of verbs load, pile, place, and so on when used to evoke the same P LACING frame). The motivation for this work is twofold. On the one hand, the frame induction techniques we propose can be useful in the context of applications such as text summarization (Cheung and Penn, 2013), question answering (Frank et al., 2007; Shen and Lapata, 2007), and so on, for languages where we lack a frame-annotated resource for supervised frame induction, or to expand the coverage of already existing resources. On the other hand, we are interested in theoretical linguistic insights into frame structure. In this sense, our We present a method for unsupervised lexical frame acquisition at the syntax–semantics interface. Given a set of input strings derived from dependency parses, our method generates a set of clusters that resemble lexical frame structures. Our work is motivated not only by its practical applications (e.g., to build, or expand the"
S19-2003,S07-1002,0,0.0531283,"of C OMMERCE SELL, be labeled with the same unsupervised tag.3 The task resembles word sense induction in that it assigns a class (or sense) label to a verb. In word sense induction (WSI), labels are determined and evaluated on word forms (lemma + part-ofspeech e.g., sell.v or auction.n). WSI evaluations assume that the inventory of senses (set Si s) for different word forms f is devised independently. For instance, assuming f1 is labeled with the set of senses S1 and f2 with S2 , then S1 ∩ S2 6= φ only if f1 = f2 ; and, if f1 6= f2 then S1 ∩ S2 = φ (as in other SemEval benchmarks, including Agirre and Soroa (2007); Manandhar et al. (2010); 3 Task B.1: Unsupervised Frame Semantic Argument Labeling Taking the frames as primary and defining roles relative to each frame, the aim of Task B.1 was to cluster prespecified verb-headed argument structures according to the principles of Frame Semantics, where FrameNet served as the reference for evaluation. This task amounted to unsupervised labeling of frames and core FEs (Figure 2b). Because FrameNet defines FEs frame-specifically, Task B.1 entails Task A. Given a set of semantically-unlabelled arguments as input (e.g., Figure 1a), the root nodes (i.e., verbs)"
S19-2003,W13-2322,0,0.0104004,"of the verb as the following graphic shows. T HEME Criticism S OURCE come from Guidelines The annotation guidelines for this task were slightly different from those of FrameNet and various semantic dependency treebanks. In contrast to FN, which annotates a full span of text as an argument filler, or PropBank, which annotates syntactic constituents of arguments of verbs (Palmer et al., 2005), we identified the text spans and only annotated a single word or a multi-word unit (MWU), i.e., the semantic head of the span, like annotations in Oepen et al. (2016) and Abstract Meaning Representation (Banarescu et al., 2013). To illustrate, in Example 1, FN would annotate Criticism of futures as filling the FE E NTITY. We only annotated Criticism, understanding it as the LU that evokes J UDGMENT COMMUNICATION, which in turn represents the meaning of the whole text span. Thus, we assumed that another frame fa fills an argument of a frame. We annotated only the main content word(s) that evoke(s) fa ; these main words are the semantic heads.4 Multi-word unit semantic heads (e.g., named entities, word form combinations) are annotated as if a single word form, such as Wall Street (# 1), excluding modifiers. In contras"
S19-2003,E17-1045,0,0.0303582,"The rest of this paper is organized as follows: Section 2 contextualizes this task; Section 3 offers a detailed task-description; Section 4 describes the data; Section 5 introduces the evaluation metrics and baselines; Section 6 characterizes the participating systems and unsupervised methods that participants used; Section 7 provides evaluation scores and additional insight about the data; and Section 8 presents concluding remarks. 2 best. This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017). Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies. Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet. Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information. Others (Ustalov et al. (2018); Materna (2012)) addr"
S19-2003,W06-3812,0,0.0665453,"vectors with concatenated verb representation vectors and vectors that represent usage context. Task B.2 employed hand crafted features, a method to encode syntactic information, and again an agglomerative clustering method. Ribeiro et al. (2019) also reported results for all subtasks using similar techniques to those reported in the other two submitted papers. Ribeiro et al. (2019) used the bidirectional neural language model BERT, which Arefyev et al. (2019) also used. Task A employed contextualized word representations proposed in (Ustalov et al., 2018), and Biemann’s clustering algorithm (Biemann, 2006). Compared to the two other systems, Ribeiro et al. (2019) exploited input structures, weighted them, and used them elegantly in its algorithm. With the same method but different hyper-parameters for B.2 along with combining results from Task A, Ribeiro et al. (2019) offered a solution to B.1. 7 BCF 70.70 68.10 65.32 65.35 Task A BCF 63.12 49.49 42.75 45.79 BCF 64.09 42.1 45.65 39.03 B.1 B.2 Table 2: Summary of Results. The BASELINE for Task A is 1C P H, and for B.1 and B.2 is 1C P HG. Best results appear in bold face; discarded results are crossed out. Table 6 lists all other baselines. of al"
S19-2003,N06-2015,0,0.0345463,"t. The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation). Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information. Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels. For development purposes, developers received a small labeled development set. 3.1 Jurgens and Klapaftis (2013); Navigli and Vannella (2013)). For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from Ssell are assigned to the lemma sell.v, and one label s0 is assigned to auction.v, knowing that s0 ∈ / Ssell . Typically, lexical semantic relationships among members of Si s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)). In contrast, this task assumes that the sense inventory is defined independent of word forms. This task involves uncovering mapping between word forms f and members of S such that different word forms (i.e., fi 6= fj ) can be mapped to the same me"
S19-2003,W13-5503,0,0.0335597,"Missing"
S19-2003,J05-1004,0,0.358235,"re FEs, as Example 1 shows. (1) 4.2 O RIGIN Criticism O RIGIN come from Wall Street Also, using the set of 32 generic semantic role labels in VerbNet 3.2 and two additional roles, COG NIZER and CONTENT , we annotated arguments of the verb as the following graphic shows. T HEME Criticism S OURCE come from Guidelines The annotation guidelines for this task were slightly different from those of FrameNet and various semantic dependency treebanks. In contrast to FN, which annotates a full span of text as an argument filler, or PropBank, which annotates syntactic constituents of arguments of verbs (Palmer et al., 2005), we identified the text spans and only annotated a single word or a multi-word unit (MWU), i.e., the semantic head of the span, like annotations in Oepen et al. (2016) and Abstract Meaning Representation (Banarescu et al., 2013). To illustrate, in Example 1, FN would annotate Criticism of futures as filling the FE E NTITY. We only annotated Criticism, understanding it as the LU that evokes J UDGMENT COMMUNICATION, which in turn represents the meaning of the whole text span. Thus, we assumed that another frame fa fills an argument of a frame. We annotated only the main content word(s) that evo"
S19-2003,D08-1048,0,0.42248,"Missing"
S19-2003,S12-1012,0,0.0226833,", systems learn and assign semantic labels to test records without appealing to any explicit training labels. For development purposes, developers received a small labeled development set. 3.1 Jurgens and Klapaftis (2013); Navigli and Vannella (2013)). For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from Ssell are assigned to the lemma sell.v, and one label s0 is assigned to auction.v, knowing that s0 ∈ / Ssell . Typically, lexical semantic relationships among members of Si s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)). In contrast, this task assumes that the sense inventory is defined independent of word forms. This task involves uncovering mapping between word forms f and members of S such that different word forms (i.e., fi 6= fj ) can be mapped to the same meaning (label), and the same meaning (label) can be mapped to several word forms. We defined S with respect to FrameNet and assumed that its typed-situation frames are units of meaning. So, C OMMERCE SELL captures the meaning associated with both sell.v and auction.v., as well as other selling-relate"
S19-2003,W12-1901,0,0.250272,"remarks. 2 best. This limitation does not hinder unsupervised methods, which will port and extend the coverage of semantic parsers, a common challenge in semantic parsing (Hartmann et al., 2017). Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies. Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet. Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information. Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments. Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)). This task aimed to benchmark a class of such unsupervised frame induction methods. Background Frame Semant"
S19-2003,L16-1376,0,0.0261748,"Missing"
S19-2003,S13-2035,0,0.0202696,"ore, 1968) and against a set of generic semantic roles, taken primarily from VerbNet. The task was unsupervised in that it forbade the use of any explicit semantic annotation (only permitting morphosyntactic annotation). Instead, we encouraged the use of unsupervised representation learning methods (e.g., word embeddings, brown clusters) to obtain semantic information. Hence, systems learn and assign semantic labels to test records without appealing to any explicit training labels. For development purposes, developers received a small labeled development set. 3.1 Jurgens and Klapaftis (2013); Navigli and Vannella (2013)). For instance, in WSI evaluations based on OntoNotes (Hovy et al., 2006), six different labels from Ssell are assigned to the lemma sell.v, and one label s0 is assigned to auction.v, knowing that s0 ∈ / Ssell . Typically, lexical semantic relationships among members of Si s (e.g., synonymy, antonymy) are then analyzed independently of WSI (e.g., Lenci and Benotto (2012); Girju et al. (2007); McCarthy and Navigli (2007)). In contrast, this task assumes that the sense inventory is defined independent of word forms. This task involves uncovering mapping between word forms f and members of S suc"
S19-2003,D18-1412,0,0.147859,"format and for training data-driven machine learning systems, which is required for tasks such as information extraction, question-answering, text summarization, among others. However, manually developing frame semantic databases and annotating corpus-derived illustrative examples to support analyses of frames are resource-intensive tasks. The most well-known frame semantic (lexical) resource is FrameNet (Ruppenhofer et al., 2016), which only covers a (relatively) small set of the vocabulary of contemporary English. While NLP research has integrated FrameNet data into semantic parsing, e.g., Swayamdipta et al. (2018), these methods cannot extend beyond previously seen training labels, tagging out-of-domain semantics as unknown at 3 Task Description C OMMERCE SELL skyscraper dobj nsubj nmod:to Exxon Mobil sell company (a) Task A - Identifying Semantic Frames: Unsupervised learned labels evaluated against FN’s lexical units C OMMERCE SELL skyscraper Goods Seller dobj nsubj nmod:to Exxon Mobil sell Buyer company (b) Task B.1 - Full Frame Semantic Tagging: Unsupervised labels evaluated against FN’s frames Theme Agent nsubj skyscraper dobj nmod:to Exxon Mobil sell Recipient company (c) Task B.2 – Case Role Lab"
S19-2003,P18-2010,0,0.114883,"in semantic parsing (Hartmann et al., 2017). Unsupervised frame induction methods can serve as an assistive semantic analytic tool, to build language resources and facilitate linguistic studies. Since the focus is usually to build language resources, most systems (Pennacchiotti et al. (2008); Green et al. (2004)) have used a lexical semantic resource like WordNet (Miller, 1995) to extend coverage of a resource like FrameNet. Some methods, e.g., Modi et al. (2012) and Kallmeyer et al. (2018), tried to extract FrameNetlike resources automatically without additional semantic information. Others (Ustalov et al. (2018); Materna (2012)) addressed frame induction only for verbs with two arguments. Lastly, unsupervised frame induction methods can also facilitate linguistic investigations by capturing information about the reciprocal relationships between statistical features and linguistic or extra-linguistic observations (e.g., Reisinger et al. (2015)). This task aimed to benchmark a class of such unsupervised frame induction methods. Background Frame Semantics (Fillmore, 1976) and other theories (Gamerschlag et al., 2014) that adopt typed feature structures for representing knowledge and linguistic structure"
steiner-kallmeyer-2002-viqtorya,W98-1207,0,\N,Missing
steiner-kallmeyer-2002-viqtorya,W00-1324,1,\N,Missing
steiner-kallmeyer-2002-viqtorya,H94-1020,0,\N,Missing
W00-1324,H94-1020,0,0.06028,"ated on the database. 1 1.1 Introduction Syntactic annotation and linguistic research With the increasing availability of large amounts of electronic texts, linguists have access to more and more material for empirically based linguistic research. Furthermore, electronic corpora are more and more richly annotated and thereby more and more detailed and structured information contained in the corpora becomes accessable. Currently many corpora are tagged with morphosyntactic categories (part-of-speech) and there are already several syntactically annotated corpora. Examples are the Penn Treebank (Marcus et al., 1994; Bies et al., 1995) annotated at the University of Pennsylvania, the Negra corpus (Brauts et al., 1999) developed in Saarbriicken, the Verbmobil treebank~ (Hinrichs et al., 2000) annotated in Tiibingen *The work presented here was done as part of a project in SFB 441 &quot;Linguistic Data Structures&quot; at the University of Tiibingen. and the French treebank annotated in Paris (Abeill4 and C14ment, 1999). However, in order to have access to these rich linguistic annotations, adequate query tools are needed. In the following, an example of a linguistically relevant construction is considered that illu"
W00-1324,mengel-lezius-2000-xml,0,0.0588212,"QL, it is possible to search not only for parent but also for dominance relations. However, in order to deal with discontinuous constituents, most syntactically annotated corpora do not contain trees but slightly different data structures. The Penn Treebank for example consists of trees with an additional coindexation relation, Negra allows crossing branches and in Verbmobil, an element (a tree-like structure) in the corpus might contain completely disconnected nodes. In order to express these annotations in XML, one has to encode for example each node and each edge as a single element as in (Mengel and Lezius, 2000). But then a query for a dominance relation can no longer be formulated with a regular path expression. In this paper, I propose a query tool that allows to search for parent, dominance and linear precedence relations even in corpora annotated with structures slightly different from trees. 2 ± - - I I I NX D I I VXFIN I APPR I NE I VAFIN fiber C. habe I I I NX NX I PPER ich I I VXINF I I I ART I NN ein Buch [ VVPP gel~en Figure 1: Annotation of (1) in Verbmobil format that contains approx. 38.000 trees (or rather tree-like annotation structures since, as already mentioned, the structures are n"
W00-2018,E91-1005,0,0.0360909,"Missing"
W00-2018,P95-1021,0,0.0435844,"Missing"
W02-2218,W98-0106,0,0.278959,"ays loves Mary. S derived tree: NP VP VP NP ADV John always V VP  NP John loves NP Mary S VP NP ADV always derivation tree: love VP V NP loves Mary (1)john (22)mary (2)always Figure 1: TAG derivation for (1) 1.2. Compositional semantics with LTAG Because of the localization of the arguments of a lexical item within elementary trees TAG derivation trees express predicate argument dependencies. Therefore it is generally assumed that the proper way to define compositional semantics for LTAG is with respect to the derivation tree, rather than the derived tree (see e.g. Shieber and Schabes, 1990; Candito and Kahane, 1998; Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi 1999, 2002). The overall idea is as follows. Each elementary tree is connected with a semantic representation. The way these semantic representations combine with each other depends on the derivation tree. Following Kallmeyer and Joshi (1999, 2002), in this paper, we will adopt ‘flat’ semantic representations as in, for example, Minimal Recursion Semantics MRS, (Copestake et al., 1999). (2) shows the elementary semantic representations for (1).  c 2002 Laura Kallmeyer. Proceedings of the Sixth International Workshop on Tree Adjoining Gramma"
W02-2218,C90-3045,0,0.115883,"position (2). (1) John always loves Mary. S derived tree: NP VP VP NP ADV John always V VP  NP John loves NP Mary S VP NP ADV always derivation tree: love VP V NP loves Mary (1)john (22)mary (2)always Figure 1: TAG derivation for (1) 1.2. Compositional semantics with LTAG Because of the localization of the arguments of a lexical item within elementary trees TAG derivation trees express predicate argument dependencies. Therefore it is generally assumed that the proper way to define compositional semantics for LTAG is with respect to the derivation tree, rather than the derived tree (see e.g. Shieber and Schabes, 1990; Candito and Kahane, 1998; Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi 1999, 2002). The overall idea is as follows. Each elementary tree is connected with a semantic representation. The way these semantic representations combine with each other depends on the derivation tree. Following Kallmeyer and Joshi (1999, 2002), in this paper, we will adopt ‘flat’ semantic representations as in, for example, Minimal Recursion Semantics MRS, (Copestake et al., 1999). (2) shows the elementary semantic representations for (1).  c 2002 Laura Kallmeyer. Proceedings of the Sixth International Worksho"
W02-2218,C88-2147,0,0.376617,"node of some elementary , the adjoined tree is not only connected to but also to the tree to which was added in some previous derivation step. The enriched derivation structure used for semantics is called e-derivation structure for short. The e-derivation structure of (7) is shown in Fig. 4, the additional link, i.e., the one that does not be part of the derivtaion tree, is depicted as a dotted edge. derived tree: S N VP Det chaque V N aboie e-derivation structure: aboie (1)chien (0)chaque chien Figure 4: Derived tree and e-derivation structure of (7) In a feature-structure based TAG (FTAG, (Vijay-Shanker and Joshi, 1988)), the additional connection between the qunatifier and the verb is even more obvious, since a unification of feature structures from all three trees (here aboie, chaque and chien) takes place. See Fig. 5 for a general FTAG derivation sequence of substitution and adjunction at the root of the tree that was added by substitution. In the derived tree, the root of the adjoined tree carries a top feature structure that results from unifying feature structures from all three trees involved in this derivation. In this sense, the links in a e-derivation structure reflect unifications of feature struc"
W02-2218,W90-0102,0,\N,Missing
W04-3305,W04-3325,1,0.755228,"&& && T ) MAXS 0 3 33 $ 33 33 S &apos; &apos;B -P &quot; .22 (7) , There is only one disambiguation, namely ` P5 Z S ` PS Z  ` P , that leads to the semantics every VMaZ boy V$MXWbZ laugh VMW1W . The feature maximal scope (MAXS) is needed to provide the correct maximal scope of quantifiers. This is important in questions (see below). Furthermore, MAXS is also used to make sure that quantifiers embedded under attitude verbs such as think cannot scope over the embedding verb. This constraint is largely assumed to hold for quantifiers (see Kallmeyer and Romero, 2004, for further discussion). Following Romero et al. (2004), we assume that whoperators are similar to quantifiers in the sense that they also have a separate scope part and they also have a MAXS scope limit. But their scope limit is provided by the S’ node, not the S node. For an analysis of (6), see Fig. 3. The MAXS features together with the semantics of the question verb make sure that all wh-operators have scope over the question proposition (here P ) and all quantifiers scope below this proposition. The minimal nuclear scope of the wh-operator (variable O ) is provided by the question proposition P . MAXS s’ 6   6 54 9 8 # some - ( S’ B wh :"
W04-3305,E03-1030,1,0.895375,"tics of Tree Adjoining Grammar that uses semantic feature structures and variable unification as in Kallmeyer and Romero (2004) can provide the correct variable bindings for both types of questions. The paper proposes elementary trees and semantic representations that allow to account for both constructions, (1) and (2), in a uniform way. (3) 2 LTAG Semantics (a) On the corner of which street does his friend live? (b) A picture of whom does John like? 1 This was pointed out to us by one anonymous reviewer. In approaches to TAG semantics (see e.g. Kallmeyer and Joshi, 2003; Joshi et al., 2003; Gardent and Kallmeyer, 2003) each elementary tree is commonly associated with its appropriate semantic representation. In this paper we TAG+7: Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms. May 20-22, 2004, Vancouver, BC, CA. Pages 32-39. use the framework presented in Kallmeyer and Romero (2004) that follows this line: We use flat semantic representations with unification variables (similar to MRS, Copestake et al., 1999). The semantic representations contain propositional metavariables. Constraints on the relative scope of these metavariables and propositional labels are used to provid"
W04-3305,W04-3321,1,\N,Missing
W04-3305,W00-2016,0,\N,Missing
W04-3305,W02-2201,0,\N,Missing
W04-3306,E91-1005,0,0.558132,"arieren but it precedes der Mechaniker, the subject of the main verb verspricht and it is not part of the embedded VP. It has been argued that in German there is no bound on the number of scrambled elements and no bound on the depth of scrambling (i.e., in terms of movement, the number of VP borders crossed by the moved element). (See for example (Rambow, 1994a; Meurers, 2000; M¨uller, 2002) for descriptions of scrambling data.) (1) ... dass [es]1 der Mechaniker [t1 zu reparieren] verspricht ... that it the mechanic to repair ‘... that the mechanic promises to repair it’ promises As shown in (Becker et al., 1991), TAG are not powerful enough to describe scrambling in German in an adequate way. By this we mean that a TAG analysis of scrambling with the correct predicate-argument structure is not possible, i.e., an analysis with each argument attaching to the verb it depends on. Let us consider the analysis of (1) in order to get an idea of why scrambling poses a problem for TAG. If we leave aside the complementizer dass, elementary trees for verspricht and reparieren might look as shown in Fig. 1. In the derivation, the verspricht-tree adjoins to the root of the reparieren-tree and the NP der Mechanike"
W04-3306,E03-1030,1,0.899233,"Missing"
W04-3306,P94-1036,0,0.853992,"e in languages such as German, Hindi, Japanese and Korean. These languages are therefore often said to have a free word order. Consider for example the German sentence (1). In (1), the accusative NP es is an argument of the embedded infinitive zu reparieren but it precedes der Mechaniker, the subject of the main verb verspricht and it is not part of the embedded VP. It has been argued that in German there is no bound on the number of scrambled elements and no bound on the depth of scrambling (i.e., in terms of movement, the number of VP borders crossed by the moved element). (See for example (Rambow, 1994a; Meurers, 2000; M¨uller, 2002) for descriptions of scrambling data.) (1) ... dass [es]1 der Mechaniker [t1 zu reparieren] verspricht ... that it the mechanic to repair ‘... that the mechanic promises to repair it’ promises As shown in (Becker et al., 1991), TAG are not powerful enough to describe scrambling in German in an adequate way. By this we mean that a TAG analysis of scrambling with the correct predicate-argument structure is not possible, i.e., an analysis with each argument attaching to the verb it depends on. Let us consider the analysis of (1) in order to get an idea of why scram"
W04-3306,C88-2147,0,0.520938,"Missing"
W04-3306,J01-1004,0,\N,Missing
W04-3306,J05-2003,1,\N,Missing
W04-3321,W98-0106,0,0.0364406,"tory of how the elementary trees are put together. A derived tree is the result of carrying out the substitutions and adjunctions. Each edge in the derivation tree stands for an adjunction or a substitution. The edges are equipped with Gorn addresses of the nodes where the substitutions/adjunctions take place.1 See for example the derivation of (1) in Fig. 1. (1) John sometimes laughs Taking into account the minimality of elementary trees and the fact that derivation steps in TAG correspond to predicate-argument applications, it seems appropriate to base LTAG semantics on the derivation tree (Candito and Kahane, 1998; Joshi and Vijay-Shanker, 1999; Kallmeyer and Joshi, 2003). However, it has been observed that in some cases this is problematic since the derivation tree does not provide enough information to correctly construct the desired semantic dependencies. The goal of this paper is to bring together ideas from several recent approaches in order to develop a general framework for LTAG semantics that allows us to compute semantic representations on the derivation tree, overcoming some otherwise problematic cases. Within this framework we then sketch several sample analyses. 2 Previous approaches to LTA"
W04-3321,W02-2218,1,0.881393,"d and derivation tree. Considering that one of the guiding linguistic principles of LTAG is semantic minimality of elementary tree, i.e. that the semantics of elementary trees is non-decomposable, it is more appropriate to link semantic representations to whole elementary trees and to abstract away (at least to a certain degree) from the concrete shape of the elementary trees. This amounts to linking semantic representations to nodes in the derivation tree. An alternative proposal for computing semantics only on the derivation tree is to enrich the derivation tree with additional links as in (Kallmeyer, 2002a; Kallmeyer, 2002b). In this approach, the derived tree needs not be considered for computing semantics. The problem with this proposal is that sometimes it is not clear which link one has to follow in order to find the value for some semantic variable. Therefore additional rules for ordering the links for semantic computation are needed. The result is a rather complex machinery in order to obtain the dependencies needed for semantics. More recently, (Gardent and Kallmeyer, 2003) propose to use the feature unification mechanism in the syntax, i.e., in the derived tree, in order to determine t"
W04-3321,P95-1021,0,0.134166,"Missing"
W04-3321,W02-2217,0,0.013648,". Consequently, one has to adjoin the scope part to the like S node. P values of the node to which they adjoin. They scope over the lower proposition. By unification, the proposition introduced by the topmost adverb/raising verb is the P value of the root of the verb tree which is below the MAXS proposition. Therefore, in (11b), the attitude verb claim takes scope over the adverb. Furthermore, the problem of multiple modifiers as in (12) is also often discussed as an example where the TAG derivation tree does not give the semantic dependencies one needs (see, e.g., (Schabes and Shieber, 1994; Rogers, 2002)). These cases are difficult for a derivation tree based semantics because only the adjective that is closest to the modified noun attaches to the noun, all adjectives that are further to the left attach to the adjective on their right. However, all adjectives equally take the variable provided by the noun as their argument. (12) roasted red pepper 6.3 Problems for derivation based semantics Now let us come back to the examples (2) and (3) mentioned in the beginning, repeated here as (10) and (11): As shown in Fig. 11, in our approach the arguments of the three predicates, pepper, red and roas"
W04-3321,W04-3325,1,0.580437,"Missing"
W04-3321,J94-1004,0,0.369183,"bility does not work either. Consequently, one has to adjoin the scope part to the like S node. P values of the node to which they adjoin. They scope over the lower proposition. By unification, the proposition introduced by the topmost adverb/raising verb is the P value of the root of the verb tree which is below the MAXS proposition. Therefore, in (11b), the attitude verb claim takes scope over the adverb. Furthermore, the problem of multiple modifiers as in (12) is also often discussed as an example where the TAG derivation tree does not give the semantic dependencies one needs (see, e.g., (Schabes and Shieber, 1994; Rogers, 2002)). These cases are difficult for a derivation tree based semantics because only the adjective that is closest to the modified noun attaches to the noun, all adjectives that are further to the left attach to the adjective on their right. However, all adjectives equally take the variable provided by the noun as their argument. (12) roasted red pepper 6.3 Problems for derivation based semantics Now let us come back to the examples (2) and (3) mentioned in the beginning, repeated here as (10) and (11): As shown in Fig. 11, in our approach the arguments of the three predicates, peppe"
W04-3321,P97-1026,0,0.0121323,"ns p in γ such that there is no edge from γ to some other tree with position p: the T and B features of γ.p are identified. By these unifications, some of the variables in the semantic representations get values. In the end, after having performed these unifications, the union of all semantic representations is built. The result is an underspecified representation.4 3.3 A sample derivation As an example consider the analysis of (1): Fig. 3 shows the semantic representations and the semantic feature structures of the three elementary trees involved in the derivation. 4 3 A similar approach is (Stone and Doran, 1997) where, as in (Gardent and Kallmeyer, 2003), each elementary tree has a flat semantic representation, the semantic representations are conjoined when combining them and variable assignments are done by unification in the feature structures on the derived tree. But there is no underspecification, and the approach is less explicit than (Gardent and Kallmeyer, 2003). For combining feature structure, we adopt an operational way in this paper because this is general practice in LTAG. I.e., unification is an operation on actual structures. Viewing feature structures as descriptions and thinking of u"
W04-3321,C88-2147,0,0.486558,"using a well-defined operation, unification, for semantic computation. But it has the disadvantage of using the derived tree for semantics even though semantic representations are assigned to whole elementary trees (i.e., to nodes in the derivation tree) and not to nodes in the derived tree. Furthermore, the feature structures needed for semantics are slightly different form those used for syntax since they contain semantic variables and labels as possible feature values. Consequently, the number of feature structures is no longer finite (in contrast to feature-based TAG (FTAG) as defined in (Vijay-Shanker and Joshi, 1988)) and therefore the generative capacity of the formalism is extended. In other words, a more powerful formalism is used for syntax just because it is needed for the specific 156 semantic features.3 In order to separate more neatly between syntax with feature structures linked to nodes in the derived tree and semantics where semantic representations are linked to nodes in the derivation tree, we propose in the following to incorporate semantic feature structures in the derivation tree. Formally, this means just extracting the semantic features used in (Gardent and Kallmeyer, 2003) from the deri"
W04-3321,E03-1030,1,\N,Missing
W04-3325,W04-3321,1,0.72482,"In the end, a sentence like (11) must receive the truth-conditions in (12). The expression     in (12) stands for the set of doxastic alternatives of John in , that is, for the set of possible situa0  tions that conform to John’s beliefs  in . The formula (12) states that we are in a situation such that, for all of 0  John’s belief alternatives in and for all propositions   0      who called  iff  who called  .  ,     (11) John knows who called.      2 Semantic unification For LTAG semantics, we use the semantic unification framework described in (Kallmeyer and Romero, 2004) that is very close to (Gardent and Kallmeyer, 2003): We do compositional semantics on the derivation tree, i.e., each elementary tree has a semantic representation and the derivation tree indicates how to do semantic computation. Semantic representations are equipped with semantic feature structures. Semantic representations are sets of formulas (typed  -expressions with labels) and scope ( constraints. A scope constraint is an expression  where and ( are propositional labels or propositional variables. Semantic feature structures have features P for all node positions  that can occur in"
W04-3325,E03-1030,1,0.770748,"ruth-conditions in (12). The expression     in (12) stands for the set of doxastic alternatives of John in , that is, for the set of possible situa0  tions that conform to John’s beliefs  in . The formula (12) states that we are in a situation such that, for all of 0  John’s belief alternatives in and for all propositions   0      who called  iff  who called  .  ,     (11) John knows who called.      2 Semantic unification For LTAG semantics, we use the semantic unification framework described in (Kallmeyer and Romero, 2004) that is very close to (Gardent and Kallmeyer, 2003): We do compositional semantics on the derivation tree, i.e., each elementary tree has a semantic representation and the derivation tree indicates how to do semantic computation. Semantic representations are equipped with semantic feature structures. Semantic representations are sets of formulas (typed  -expressions with labels) and scope ( constraints. A scope constraint is an expression  where and ( are propositional labels or propositional variables. Semantic feature structures have features P for all node positions  that can occur in elementary trees.2 The values of these features are"
W06-1510,E91-1005,0,0.197613,"Missing"
W06-1510,C88-2147,0,\N,Missing
W06-1510,J05-2003,1,\N,Missing
W06-1510,W04-3321,1,\N,Missing
W06-1511,W02-2234,0,0.0101661,"icted with dotted lines. They yield in particular 9 = 7 , therefore, with constraint 7 ≥ l1 , l1 is in the scope of the negation. The presence of a negation is indicated by a global NEG = yes. In case there is no negation, we have to make sure we obtain NEG = no and not just an unspecified NEG value. Therefore, the VP spine is articulated with non-global NEG features that switch from no to yes once a negation occurs. Here this is the case at node position V, consequently 6 = 5 = 4 = 3 = yes. The topmost Peter ruft Hans nicht an Peter calls Hans not PART (‘Peter does not call Hans’) Similar to Gerdes (2002), the VP nodes carry features VF (‘Vorfeld’), LK (‘Linke Satzklammer’), MF (‘Mittelfeld’), and RK (‘Rechte Satzklammer’) for the topological fields. In German, the vorfeld, the position preceding the left satzklammer, must be filled by exactly one constituent. We guarantee this with the feature VF: The different VF features at the highest VP node in the tree for ruft an make sure that adjunction to the vorfeld is obligatory. At the same time, elements adjoining to any of the topological fields (see the tree for Peter) have a foot node feature VF = − and have equal top and bottom features VF at"
W06-1511,W06-1510,1,0.872717,"Missing"
W06-1511,C88-2147,0,0.790227,"gure 1: LTAG semantics of (9) The meta-variables from the semantic representations can occur in the feature structure descriptions. In this case they can receive values following from the feature value equations performed on the derivation tree. As an example see Fig. 1 showing the derivation tree for (9) with semantic representations and semantic feature structure descriptions as node labels. (9) John always laughs The additional feature equations in this example are depicted with dotted links. They arise from top-bottom feature identifications parallel to the unifications performed in FTAG (Vijay-Shanker and Joshi, 1988) and from identifications of global features. They yield 1 = x and 4 = l1 . Applying these identities to the semantic representations after having built their union leads to (10). The constraint 3 ≥ l1 states that l1 : laugh(x) is a component of 3 . Hans glaubt nicht, dass Peter Hans believes not that Peter sonderlich gl¨ucklich sein wird. very happy be will (‘Hans does not believe that Peter will be very happy.’) 2 The LTAG Semantics Framework We use the Kallmeyer and Romero (2005) framework for semantics. Each elementary tree is linked to a semantic representation containing Ty2 terms and sc"
W06-1515,W06-1511,1,0.788956,"wo frameworks differ substantially in their treatment of underspecification: 1. LRS employs partial descriptions of fully specified models, whereas LTAG generates underspecified representations in the style of (Bos, 1995) that require the definition of a disambiguation (a “plugging” in the terminology of Bos). 2. LRS constraints contain not Ty2 terms but descriptions of Ty2 terms. Therefore, in contrast to LTAG, two descriptions can denote the same formula. Here, LTAG is more limited compared to LRS. On the other hand, the way semantic representations are defined in LTAG guarantees 3 NEG See (Lichte and Kallmeyer, 2006) for a discussion of and N - SCOPE in the context of NPI-licensing. that they almost correspond to normal dominance constraints, which are known to be polynomially parsable. The difference in the use of underspecification techniques reflects the more general difference between a generative rewriting system such as LTAG, in which the elements of the grammar are objects, and a purely description-based formalism such as HPSG, in which token identities between different components of linguistic structures are natural and frequently employed. 7 Summary and Conclusion LTAG and LRS have several commo"
W06-1515,C88-2147,0,0.228855,"Missing"
W08-1701,P07-2004,1,0.842421,"has been used to implement efficient parsing algorithms for several formalisms, including TAG and RCG. Unfortunately, it does not include any built-in GUI and requires a good knowledge of the GNU build tools to compile parsers. This makes it relatively difficult to use. DyALog’s main quality lies in its efficiency in terms of parsing time and its capacity to handle very large resources. Unlike TuLiPA, it does not compute semantic representations. The closest approach to TuLiPA corresponds to the SemTAG system13 , which extends TAG parsers compiled with DyALog with a semantic calculus module (Gardent and Parmentier, 2007). Unlike TuLiPA, this system only supports TAG, and does not provide any graphical output allowing to easily check the result of parsing. Note that, for grammar designers mainly interested in TAG, SemTAG and TuLiPA can be seen as complementary tools. Indeed, one may use TuLiPA to develop the grammar and check specific syntactic structures thanks to its intuitive parsing environment. Once the grammar is stable, one may use SemTAG in batch processing to parse corpuses and build semantic representations using large grammars. This combination of these 2 systems is made easier by the fact that both"
W08-1701,C04-1044,0,0.144272,"having a frontier node marked for anchoring (i.e., lexicalization). At parsing time, the tree schemata are anchored according to the input string. This anchoring selects a subgrammar supposed to cover the input string. Unfortunately, this subgrammar may contain many trees that either do not lead to a parse or for which we know a priori that they cannot be combined within the same derivation (so we should not predict a derivation from one of these trees to another during parsing). As a result, the parser could have poor performance because of the many derivation paths that have to be explored. Bonfante et al. (2004) proposed to polarize the structures of the grammar, and to apply an automaton-based filtering of the compatible structures. The idea is the following. One compute polarities representing the needs/resources brought by a given tree (or tree tuple for TT-MCTAG). A substitution or foot node with category NP reflects a need for an NP (written NP-). In the same way, an NP root node reflects a resource of type NP (written NP+). Then you build an automaton whose edges correspond to trees, and states to polarities brought by trees along the path. The automaton is then traversed to extract all paths l"
W08-1701,C96-2120,0,0.0602791,"This environment is being used (i) at the University of T¨ubingen, in the context of the development of a TT-MCTAG for German describing both syntax and semantics, and (ii) at LORIA Nancy, in the development of an XTAG-based metagrammar for English. The German grammar, called GerTT (for German Tree Tuples), is released under a LGPL license for Linguistic Resources11 and is presented in (Kallmeyer et al., 2008). The test-suite currently used to check the grammar is hand-crafted. A more systematic evaluation of the grammar is in preparation, using the Test Suite for Natural Language Processing (Lehmann et al., 1996). Unfortunately, as mentioned above, the linguist has to move back-and-forth from the grammar/lexicon descriptions to the parser, i.e., each time the parser reports grammar errors, the linguist fixes these and then recomputes the XML files and then parses again. To avoid this tedious task of resources re-compilation, we started developing an Eclipse8 plug-in for the TuLiPA system. Thus, the linguist will be able to manage all these resources, and to call the parser, the metagrammar compiler, and the lexConverter from a common interface (see Fig. 6). Figure 6: TuLiPA’s eclipse plug-in. The moti"
W08-1701,2000.iwpt-1.8,0,0.68321,"t between formalisms (e.g., in terms of parsing complexity in practice), and would allow for a better sharing of resources (e.g., having a common lexicon, from which different features would be extracted depending on the target formalism). In this context, we present a parsing environment relying on a general architecture that can be used for parsing with mildly context-sensitive (MCS) formalisms1 (Joshi, 1987). Its underlying idea is to use Range Concatenation Grammar (RCG) as a pivot formalism, for RCG has been shown to strictly include MCS languages while being parsable in polynomial time (Boullier, 2000). Currently, this architecture supports tree-based grammars (Tree-Adjoining Grammars and MultiComponent Tree-Adjoining Grammars with Tree Tuples (Lichte, 2007)). More precisely, treebased grammars are first converted into equivalent RCGs, which are then used for parsing. The result of RCG parsing is finally interpreted to extract a derivation structure for the input grammar, as well as to perform additional processings (e.g., semantic calculus, extraction of dependency views). The paper is structured as follows. In section 2, we present the architecture of the TuLiPA parsing environment and sh"
W08-1701,C00-2087,0,0.0343331,"can be seen as complementary tools. Indeed, one may use TuLiPA to develop the grammar and check specific syntactic structures thanks to its intuitive parsing environment. Once the grammar is stable, one may use SemTAG in batch processing to parse corpuses and build semantic representations using large grammars. This combination of these 2 systems is made easier by the fact that both use the same input formats (a metagrammar in the XMG language and a text-based lexicon). This approach is the one being adopted for the development of a French TAG equipped with semantics. For Interaction Grammar (Perrier, 2000), there exists an engineering environment gathering the XMG metagrammar compiler and an eLEtrOstatic PARser (LEOPAR).14 This environment is being used to develop an Interaction Grammar for French. TuLiPA’s lexical disambiguation module For other formalisms, there exist state-of-the-art grammar engineering environments that have been used for many years to design large deep grammars for several languages. For Lexical Functional Grammar, one may cite the Xerox Linguistic Environment (XLE).15 For Head-driven Phrase Structure Grammar, the main available systems are the Linguistic Knowledge Base (L"
W08-1701,A92-1039,0,0.387077,"ars (namely Tree-Adjoining Grammars (TAG) and Multi-Component TreeAdjoining Grammars with Tree Tuples (TT-MCTAG)) and allows computation not only of syntactic structures, but also of the corresponding semantic representations. It is used for the development of a tree-based grammar for German. 1 Introduction Grammars and lexicons represent important linguistic resources for many NLP applications, among which one may cite dialog systems, automatic summarization or machine translation. Developing such resources is known to be a complex task that needs useful tools such as parsers and generators (Erbach, 1992). Furthermore, there is a lack of a common framework allowing for multi-formalism grammar engineering. Thus, many formalisms have been proposed to model natural language, each coming with specific implementations. Having a common framework would facilitate the comparison 1 A formalism is said to be mildly context sensitive (MCS) iff (i) it generates limited cross-serial dependencies, (ii) it is polynomially parsable, and (iii) the string languages generated by the formalism have the constant growth property (e.g., n {a2 |n ≥ 0} does not have this property). Examples of MCS formalisms include T"
W08-1701,E03-1030,1,0.818722,"with respect to the input grammar. The use of RCG as a pivot formalism, and thus of an RCG parser as a core component of the system, leads to a modular architecture. In turns, this makes TuLiPA more easily extensible, either in terms of functionalities, or in terms of formalisms. 2.1 NPj name(j,john) 2 Range Concatenation Grammar as a pivot formalism VP Adding functionalities to the parsing environment As an illustration of TuLiPA’s extensibility, one may consider two extensions applied to the system recently. First, a semantic calculus using the syntax/semantics interface for TAG proposed by Gardent and Kallmeyer (2003) has been added. This interface associates each tree with flat semantic formulas. The arguments of these formulas are unification variables, which are co-indexed with features labelling the nodes of the syntactic tree. During classical TAG derivation, trees are combined, triggering unifications of the feature structures labelling nodes. As a result of these unifications, the arguments of the semantic formulas are unified (see Fig. 1). 2 7 is the only valid state and {proper., trans., det., noun.} the only compatible set of trees). 0 John 1 1 eats 2 intrans. 0 proper. 1 NP+ a3 2 2 det. S+ 3 4 c"
W08-1701,kallmeyer-etal-2008-developing,1,\N,Missing
W08-2308,E91-1005,0,0.204436,"http://www.sfb441.uni-tuebingen.de/ emmy-noether-kallmeyer/gertt/ 2 http://sourcesup.cru.fr/tulipa/ TT-MCTAG lets one abstract away from the relative order of co-complements in the final derived tree, which is more appropriate than classic TAG when dealing with flexible word order in German. In this paper, we present the analyses for sentential complements, i.e., wh-extraction, thatcomplementation and bridging, and we work out the crucial differences between these and respective accounts in XTAG (for English) and V-TAG (for German). 1 Introduction Classic TAG is known to offer rather limited (Becker et al., 1991) and unsatisfying ways to account for flexible word order in languages such as German. The descriptive overhead is immediately evident: Every possible relative order of cocomplements of a verb, has to be covered by an extra elementary tree. To give an example from German, the verb vergisst (forgets) with two complements would receive two elementary trees in order to license the verb final configurations in (1), not mentioning the other extra elementary trees that are necessary for verb-second position. Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Forma"
W08-2308,W07-1201,0,0.0391423,"Missing"
W08-2308,J05-2003,1,0.9136,"esponding strings (node sharing relations are depicted as dotted edges): v0 v0 v0 0 0 0 v1 v1 v1 0 0 0 n1 v2 v2 0 0 0 v2 n2 n1 0 0 0 n2 n1 n2 n2 v2 n1 v1 v0 n1 n2 v2 v1 v0 n2 n1 v2 v1 v0 Figure 1: Sample TT-MCTAG 2 k-TT-MCTAG In TT-MCTAG, elementary structures are made of tuples of the form hγ, {β1 , ..., βn }i, where γ, β1 , ..., βn are elementary trees in terms of TAG (Joshi and Schabes, 1997). More precisely, γ is a lexicalized elementary tree while β1 , ..., βn are auxiliary trees. During derivation, the β-trees have to attach to the γ-tree, either directly or indirectly via node sharing (Kallmeyer, 2005). Roughly speaking, node sharing terms an extended locality, that allows β-trees to also adjoin at the roots of trees that either adjoin to γ themselves, or that are again in a node sharing relation to γ. In other words, an argument β must be linked by a chain of root adjunction to an elementary tree that adjoins to β’s head γ. As an example, consider the TT-MCTAG in Fig. 1. A derivation in this grammar necessarily starts with γv . We can adjoin arbitrarily many copies of γv1 or γv2 , always to the root of the already derived tree. Concerning the respective argument trees βn1 and βn2 , they mu"
W08-2308,W08-2316,1,0.813591,"d order flexibility and the size of the lexicon. Another comparison is dedicated to V-TAG (Rambow, 1994), which follows a strategy similar to TT-MCTAG, but chooses a different path to constrain locality. The effects of this choice can be clearly observed with bridging constructions. We thus restrict ourselves to sentential complements, namely wh-extraction, thatcomplementation and bridging. The assigned analyses are parts of an extensive grammar for German, GerTT (German TT-MCTAG), that is currently being implemented using TT-MCTAG.1 A parser is also available as part of the TuLiPA framework (Parmentier et al., 2008).2 While classic TAG seems to be appropriate for dealing with fixed word order languages and structural case (i.e., rudimentary case inflection), it is 1 http://www.sfb441.uni-tuebingen.de/ emmy-noether-kallmeyer/gertt/ 2 http://sourcesup.cru.fr/tulipa/ TT-MCTAG lets one abstract away from the relative order of co-complements in the final derived tree, which is more appropriate than classic TAG when dealing with flexible word order in German. In this paper, we present the analyses for sentential complements, i.e., wh-extraction, thatcomplementation and bridging, and we work out the crucial dif"
W08-2316,W08-2316,1,0.0523041,"CG). Furthermore, for tree-based grammars, the parser computes not only syntactic analyses but also the corresponding semantic representations. 1 Introduction The starting point of the work presented here is the aim to implement a parser for a German TAG-based grammar that computes syntax and semantics. As a grammar formalism for German we chose a multicomponent extension of TAG called TT-MCTAG (Multicomponent TAG with Tree Tuples) which has been first introduced by Lichte (2007). With some additional constraints, TT-MCTAG is mildly context-sensitive (MCS) as shown by Kallmeyer and Parmentier (2008). Instead of implementing a specific TT-MCTAG parser we follow a more general approach by using Range Concatenation Grammars (RCG) as a pivot formalism for parsing MCS languages. Indeed the generative capacity of RCGs lies beyond MCS, while they stay parsable in polynomial time (Boullier, 1999). In this context, the TT-MCTAG (or TAG) is transformed into a strongly equivalent RCG that is then used for parsing. We have implemented the conversion into RCG, the RCG parser and the retrieval of the corresponding TTMCTAG analyses. The parsing architecture comes with graphical input and output interfa"
W08-2316,P89-1018,0,0.504997,"Missing"
W08-2316,C04-1044,0,0.262645,"Missing"
W08-2316,2000.iwpt-1.8,0,0.564791,"Missing"
W08-2316,E03-1030,1,0.897416,"Missing"
W08-2316,kallmeyer-etal-2008-developing,1,0.890865,"Missing"
W08-2316,W08-2308,1,0.773959,"0 , ni,1 i ∈ PD and for 1 ≤ j ≤ k − 1: hni,j , ni,j+1 i ∈ PD where this edge is labelled with ǫ. TT-MCTAG has been proposed to deal with free word order languages. An example from German is shown in Fig. 1. Here, the NPnom auxiliary tree 2 For a tree γ, Pγ is the parent relation on the nodes, i.e., hx, yi ∈ Pγ for nodes x, y in γ iff x is the mother of y. adjoins directly to verspricht (its head) while the NPacc tree adjoins to the root of a tree that adjoins to the root of a tree that adjoins to reparieren. For a more extended account of German word order using TT-MCTAG see Lichte (2007) and Lichte and Kallmeyer (2008). TT-MCTAG can be further restricted, such that at each point of the derivation the number of pending β-trees is at most k. This subclass is also called k-TT-MCTAG. Definition 2 (k-TT-MCTAG) A TT-MCTAG G = hI, A, N, T, Ai is of rank k (or a k-TT-MCTAG for short) iff for each derivation tree D licensed in G: (TT-k) There are no nodes n, h0 , . . . , hk , a0 , . . . , ak in D such that the label of ai is an argument tree of the label of hi and hhi , ni, hn, ai i ∈ + for 0 ≤ i ≤ k. PD TT-MCTAG in general are NP-complete (Søgaard et al., 2007) while k-TT-MCTAG are MCS (Kallmeyer and Parmentier, 20"
W09-3808,W05-1502,0,0.223393,"Missing"
W09-3808,W05-1506,0,0.0331572,"minal filter iff we can find an injec~ tive mapping fT : Term = {hk, li |φ(k)(l) ∈T and either k > i or (k = i and l > j)} → {pos + 1, . . . , n} such that We have presented an Earley-style algorithm for simple range concatenation grammar, formulated as deduction system. Furthermore, we have presented a set of filters on the chart reducing the number of items. An implementation and a test with grammars extracted from treebanks showed that reasonable parsing times can be achieved. We are currently working on a probabilistic k-best extension of our parser which resumes comparable work for PCFG (Huang and Chiang, 2005). Unfortunately, experiments with the Earley algorithm have shown that with grammars of a reasonable size for data-driven parsing (> 15, 000 clauses), an exhaustive parsing is no longer efficient, due to the highly ambiguous grammars. Algorithms using only passive items seem more promising in this context since they facilitate the application of A∗ parsing techniques. ~ 1. wfT (hk,li) = φ(k)(l) for all hk, li ∈ Term; 2. for all hk1 , l1 i, hk2 , l2 i ∈ Term with k1 = k2 and l1 < l2 : fT (hk2 , l2 i) ≥ fT (hk1 , l1 i) + (l2 − l1 ); 3. for all hk1 , l1 i, hk2 , l2 i ∈ Term with k1 < ~ 1 ) |− k2"
W09-3808,W06-1508,0,0.239538,"ar context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987), the equivalent multiple context-free grammars (MCFG) (Seki et al., 1991) and simple range concatenation grammars (sRCG) (Boullier, 1998) have recently attracted an increasing interest in the context of natural language processing. For example, Maier and Søgaard (2008) propose to extract simple RCGs from constituency treebanks with crossing branches while Kuhlmann and Satta (2009) propose to extract LCFRS from non-projective dependency treebanks. Another application area of this class of formalisms is biological computing (Kato et al., 2006). This paper addresses the symbolic parsing of sRCG/LCFRS. Starting from the parsing algorithms presented in Burden and Ljungl¨of (2005) and Villemonte de la Clergerie (2002), we propose an incremental Earley algorithm for simple RCG. The strategy is roughly like the one pursued in Villemonte de la Clergerie (2002). However, instead of the automaton-based formalization in Villemonte de la Clergerie’s work, we give a general formulation of an incremental Earley algorithm, using the framework of parsing as deduction. In order to reduce the search space, we introduce different types of filters on"
W09-3808,N03-1016,0,0.143096,"gument, we convert the item into a passive one: ~ → Ψ, ~ pos, hi, ji, ρ [B(ψ) ~B ] [B, ρ] ~ ~ = i, |ψ(i)| = j, |ψ| ~ ρ ~B (ψ) = ρ Complete: Whenever we have a passive B item we can use it to move the dot over the variable of the last argument of B in a parent A-clause that was used to predict it. (n − pos) dim(A) ~ ~ + (dim(A) − i) ≥ (|φ(i)| − j) + Σk=i+1 |φ(k)| ~ → . . . B(ξ) ~ . . . , pos, hk, li, ρ [B, ρ ~B ], [A(φ) ~A ] ~ → . . . B(ξ) ~ . . . , pos′ , hk, l + 1i, ρ [A(φ) ~] The length filter is applied to results of predict, resume, suspend and complete. A second filter, first proposed in Klein and Manning (2003), checks for the presence of required preterminals. In our case, we assume the where the dot in the antecedent A-item precedes ~ ρB |), the last range in ρ~B is the variable ξ(|~ ′ hpos, pos i, and for all 1 ≤ m < |~ ρB |: ρ~B (m) = 63 5 Conclusion and Future Work preterminals to be treated as terminals, so this filter amounts to checking for the presence of all terminals in the predicted part of a clause (the part to the right of the dot) in the remaining input. Furthermore, we check that the terminals appear in the predicted order and that the distance between two of them is at least the num"
W09-3808,E09-1055,0,0.0624657,"exactly one rule S(ε) → ε and S does not appear in any of the righthand sides of the rules in the grammar. A rule is an ε-rule if one of the arguments 1 Introduction Linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987), the equivalent multiple context-free grammars (MCFG) (Seki et al., 1991) and simple range concatenation grammars (sRCG) (Boullier, 1998) have recently attracted an increasing interest in the context of natural language processing. For example, Maier and Søgaard (2008) propose to extract simple RCGs from constituency treebanks with crossing branches while Kuhlmann and Satta (2009) propose to extract LCFRS from non-projective dependency treebanks. Another application area of this class of formalisms is biological computing (Kato et al., 2006). This paper addresses the symbolic parsing of sRCG/LCFRS. Starting from the parsing algorithms presented in Burden and Ljungl¨of (2005) and Villemonte de la Clergerie (2002), we propose an incremental Earley algorithm for simple RCG. The strategy is roughly like the one pursued in Villemonte de la Clergerie (2002). However, instead of the automaton-based formalization in Villemonte de la Clergerie’s work, we give a general formulat"
W09-3808,P87-1015,0,\N,Missing
W09-3808,C02-1028,0,\N,Missing
W09-3810,2000.iwpt-1.8,0,0.874802,". Furthermore, we transfer this property to grammars extracted from treebanks. 1 Introduction Discontinuous phrases are frequent in natural language, particularly in languages with a relatively free word order. Several formalisms have been proposed in the literature for modeling trees containing such phrases. These include nonprojective dependency grammar (Nivre, 2006), discontinuous phrase structure grammar (DPSG) (Bunt et al., 1987), as well as linear contextfree rewriting systems (LCFRS) (Vijay-Shanker et al., 1987) and the equivalent formalism of simple range concatenation grammar (sRCG) (Boullier, 2000). Kuhlmann (2007) uses LCFRS for non-projective dependency trees. DPSG have been used in Plaehn (2004) for data-driven parsing of treebanks with discontinuous constituent annotation. Maier and Søgaard (2008) extract sRCGs from treebanks with discontinuous constituent structures. Both LCFRS and sRCG can model discontinuities and allow for synchronous rewriting as well. We speak of synchronous rewriting when two or 2 Synchronous Rewriting Trees in German treebanks By synchronous rewriting we indicate the synchronous instantiation of two or more context-free derivation processes. As an example, c"
W09-3810,E87-1034,0,0.536484,"Missing"
W09-3810,P87-1015,0,\N,Missing
W09-3810,J07-2003,0,\N,Missing
W10-4412,J94-1004,0,0.0396161,"ording to which A -daughters of de-anchored nodes must be deanchored as well. A solution could be to enable the optionality of the de-anchoring of adverbs. This would make it necessary to have access to a distinctive feature of adverbs and verbs in the process of de-anchoring, e.g., by having access to the feature structure used for implementing the external condition below. The distinction could be a distinction between modifier auxiliary trees (adverbs, adjectives, etc.) and predicative auxiliary trees (verbs selecting for a sentential complement). This distinction already has been used in (Schabes and Shieber, 1994). A further example of an adverb that is not deanchored is (15). The derivation tree is shown in Fig. 9. (16) Larry thinks Sue is nice. nice S Sue A A thinks is S Larry Figure 10: Derivation trees for “Larry thinks Sue is nice”. Since both the bridging verb thinks and the embedded finite auxiliary verb is directly adjoin to the small clause anchored by nice, the derivation tree contains exactly two A-branches. Taken the internal condition for granted, the acceptability of the following gaps is predicted: (15) Mary is always fond of cheese cake, but Peter is only sometimes fond of cheese cake."
W10-4412,W08-2311,0,0.123856,"tly, de-anchored trees must be linked to the root of the derivation tree by a chain of adjunctions, and the sub-graph of de-anchored nodes in a derivation tree must satisfy certain internal constraints. Secondly, de-anchoring must be licensed by the presence of a homomorphic antecedent derivation tree. 1 Introduction Existing TAG-accounts of gapping propose the contraction of nodes (Sarkar and Joshi, 1997) or adopt elementary trees with a gap that lack the verbal anchor (Babko-Malaya, 2006) or combine the gapped elementary tree with its antecedent site into a tree set within an MCTAG-account (Seddah, 2008). This work breaks new ground in that it uses deanchoring, a deletion-like operation, for the modelling of gapping, which applies to elementary trees while being licensed by the derivation tree of licit TAG-derivations. De-anchoring removes the anchors of an elementary tree and can be seen to parallel PF-deletion in generative grammar (Hartmann, 2000; Merchant, 2001). ∗ We are grateful to Maribel Romero and Andreas Konietzko; the paper has benefitted a lot from discussions with them. Furthermore, we would like to thank three anonymous reviewers for their valuable comments. 93 Timm Lichte, Laur"
W10-4412,W06-1512,0,\N,Missing
W10-4415,P99-1065,0,0.0762209,"Missing"
W10-4415,N09-1061,0,0.274568,"Missing"
W10-4415,P02-1018,0,0.147066,"constituency parsing is comparable to the output quality of other state-of-the-art results, all while yielding structures that display discontinuous dependencies. 1 Introduction It is a well-known fact that Context-Free Grammar (CFG) does not provide enough expressivity to describe natural languages. For data-driven probabilistic CFG parsing, some of the information present in constituency treebanks, namely the annotation of non-local dependencies, cannot be captured by a CFG. It is therefore removed before learning a PCFG from the treebank and must be re-introduced in a post-processing step (Johnson, 2002; Levy and Manning, 2004). Non-projective dependencies also lie beyond the expressivity of CFG. Current dependency parsers are able to parse them (McDonald et al., 2005; Nivre et al., 2007). However, the corresponding parsing algorithms are not grammar-based. We propose to use a grammar formalism with an extended domain of locality that is able to capture the non-local dependencies both in constituency and dependency treebanks. We chose Linear Context-Free Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG that allows non-terminals to span tuples of discontinuous strings. T"
W10-4415,C10-1061,1,0.880244,"ntinuity and Non-Projectivity: Using Mildly Context-Sensitive Formalisms for Data-Driven Parsing where A ∈ N , ρ ~ ∈ (P os(w) × P os(w))dim(A) the vector of ranges characterizing all components of the span of A. We specify the set of weighted parse items via the deduction rules in Fig. 2. An instantiated rule is a rule where variables have been replaced with corresponding vectors of ranges. Our parser performs weighted deductive parsing, based on this deduction system. We use a chart C and an agenda A, both initially empty, and we proceed as in Fig. 3. For more details of the parser, see also Kallmeyer and Maier (2010). length of 25 words.1 This leads to a size of 14,858, resp. 1,651 sentences for the NeGra training, resp. test sets and to 13,935, resp. 1,300 sentences for the PDT training, resp. test set. 3.2 Grammar Extraction From all of our data sets, we extract PLCFRSs. For the constituent sets, we use the algorithm from Maier and Søgaard (2008), for the dependencies the algorithm from Kuhlmann and Satta (2009). For reasons of space, we restrict ourselves here to the examples in Fig. 4–6. 3 Experimental Setup 3.1 S Data VP Our data sources are the NeGra treebank (Skut et al., 1997) and the Prague Depen"
W10-4415,W09-3810,1,0.8051,"ranches as in the German Negra treebank) allow a straight-forward interpretation of the trees as LCFRS derivation structures, without the necessity of inducing linguistic knowledge.1 This considerably facilitates the extraction of probabilistic LCFRSs (Maier and Søgaard, 2008). The same holds for non-projective dependency structures, which can also straight-forwardly be interpreted as LCFRS derivation structures (Kuhlmann and Satta, 2009). Previous approaches that have used non-context-free formalisms for data-driven constituency parsing (Plaehn, 2004; Chiang, 2003) are either too restricted (Kallmeyer et al., 2009) or do not allow for an immediate interpretation of the treebank trees as derivation structures. Grammarbased non-projective dependency parsing has, to our knowledge, not been attempted at all. First results for PLCFRS constituency parsing with a detailed evaluation have been presented in Maier (2010). The contribution of this article is to present the first results for data-driven dependency parsing on the dependency version of the German NeGra treebank and on the Prague Dependency Treebank. Furthermore, we give greater detail on the parser and the experimental setup. We also additionally inv"
W10-4415,W06-1508,0,0.560149,"2. Here, the superscript is the vertical context and the subscript the horizontal context of the new non-terminal X. The probabilities are then computed based on the 123 Wolfgang Maier, Laura Kallmeyer S 1000 XS NN 900 XS VP ,NN Number of items (in 1000) 800 VP XVP PDS XVP ADV ,PDS XSVMFIN ,VP PDS NeGra LCFRS NeGra combined splits NeGra PCFG NeGra Dep PDT VMFIN XVP VAINF ,ADV NN ADV VAINF 700 600 500 400 300 200 100 Figure 10: Sample Markovization with v = 1, h = 2 0 6 8 frequencies of rules in the treebank, using a Maximum Likelihood estimator (MLE). Such an estimation has been used before (Kato et al., 2006). 3.6 NeGra LCFRS S sp. VP sp. S ◦ VP sp. NeGra PCFG NeGra Dep. PDT bin. 16,904 17,033 18,362 18,503 15,563 68,847 38,312 18 20 22 24 Figure 11: Number of items bin. lab. 4,142 4,179 4,952 4,995 3,898 49,085 24,119 Table 1: PLCFRSs extracted from training sets LP LR LF1 UP UR UF1 Tab. 1 contains the properties of the grammars: The number of rules in the unbinarized grammar, the number of rules in the binarized and markovized grammar and the number of labels (including POS tags) in the binarized and markovized grammar. 4 Experiments LCFRS 73.24 73.56 73.40 77.12 77.46 77.29 w/ category splits V"
W10-4415,P03-1054,0,0.0662375,"Missing"
W10-4415,E09-1055,0,0.469165,"span tuples of discontinuous strings. The reason why we think LCFRS particularly well-suited is that treebanks with a direct annotation of discontinuous constituents (with crossing branches as in the German Negra treebank) allow a straight-forward interpretation of the trees as LCFRS derivation structures, without the necessity of inducing linguistic knowledge.1 This considerably facilitates the extraction of probabilistic LCFRSs (Maier and Søgaard, 2008). The same holds for non-projective dependency structures, which can also straight-forwardly be interpreted as LCFRS derivation structures (Kuhlmann and Satta, 2009). Previous approaches that have used non-context-free formalisms for data-driven constituency parsing (Plaehn, 2004; Chiang, 2003) are either too restricted (Kallmeyer et al., 2009) or do not allow for an immediate interpretation of the treebank trees as derivation structures. Grammarbased non-projective dependency parsing has, to our knowledge, not been attempted at all. First results for PLCFRS constituency parsing with a detailed evaluation have been presented in Maier (2010). The contribution of this article is to present the first results for data-driven dependency parsing on the dependen"
W10-4415,P04-1042,0,0.371165,"rsing is comparable to the output quality of other state-of-the-art results, all while yielding structures that display discontinuous dependencies. 1 Introduction It is a well-known fact that Context-Free Grammar (CFG) does not provide enough expressivity to describe natural languages. For data-driven probabilistic CFG parsing, some of the information present in constituency treebanks, namely the annotation of non-local dependencies, cannot be captured by a CFG. It is therefore removed before learning a PCFG from the treebank and must be re-introduced in a post-processing step (Johnson, 2002; Levy and Manning, 2004). Non-projective dependencies also lie beyond the expressivity of CFG. Current dependency parsers are able to parse them (McDonald et al., 2005; Nivre et al., 2007). However, the corresponding parsing algorithms are not grammar-based. We propose to use a grammar formalism with an extended domain of locality that is able to capture the non-local dependencies both in constituency and dependency treebanks. We chose Linear Context-Free Rewriting Systems (LCFRS), a mildly context-sensitive extension of CFG that allows non-terminals to span tuples of discontinuous strings. The reason why we think LC"
W10-4415,W10-1407,1,0.852723,"dependency structures, which can also straight-forwardly be interpreted as LCFRS derivation structures (Kuhlmann and Satta, 2009). Previous approaches that have used non-context-free formalisms for data-driven constituency parsing (Plaehn, 2004; Chiang, 2003) are either too restricted (Kallmeyer et al., 2009) or do not allow for an immediate interpretation of the treebank trees as derivation structures. Grammarbased non-projective dependency parsing has, to our knowledge, not been attempted at all. First results for PLCFRS constituency parsing with a detailed evaluation have been presented in Maier (2010). The contribution of this article is to present the first results for data-driven dependency parsing on the dependency version of the German NeGra treebank and on the Prague Dependency Treebank. Furthermore, we give greater detail on the parser and the experimental setup. We also additionally investigate the effect on manually introduced category splits for PLCFRS constituency parsing. 1 Treebank trees in which non-local dependencies are annotated differently, such as with trace nodes in the Penn Treebank, could also be interpreted as LCFRS derivations given an appropriate transformation algo"
W10-4415,W07-2216,0,0.018926,"our result, we still lie in the same range. Plaehn (2004) also reports results for direct parsing of the discontinuous constituents using Probabilistic Discontinuous Phrase Structure Grammar (DPSG). See Maier (2010) for details. 4.2 Dependency Parsing In this section, we present the first grammar-based non-projective dependency parsing results. As Kuhlmann and Satta (2009) note, the principal advantage of grammar-based non-projective dependency parsing is that edge probabilities can be finetuned while staying polynomially parseable. This is not possible in the Maximum Spanning Tree approach (McDonald and Satta, 2007). For compar6 The results from the literature were obtained on sentences longer than 25 words and would most likely be better for our sentence length. ison of our dependency parser output, we report labeled and unlabeled attachment score and completely correct graphs (punctuation included). As markovization setting for the PDT set, we choose v = 2 and h = ∞. UAS LAS UComp LComp NeGra Grammar MST 78.98 87.96 71.84 82.62 32.65 42.16 25.03 29.56 PDT Grammar 51.44 67.09 14.92 9.46 MST 76.01 40.54 28.92 17.23 Table 4: Dependency parsing Tab. 4 contain the dependency parsing results for our parser a"
W10-4415,H05-1066,0,0.411279,"Missing"
W10-4415,J03-1006,0,0.363351,"tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules (1) (1) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) (m) · · · Am (X1 LCFRS and p : P → [0..1] a function such that ~ = 1. for all A ∈ N : ΣA(~x)→Φ∈P p(A(~x) → Φ) ~ 2.2 PLCFRS Parsing Our parser is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). We assume without loss of generality that our LCFRSs are binary (i.e., have rank 2) (G´omez-Rodr´ıguez et al., 2009) and do not contain rules where some of the LHS components are ε (Boullier, 1998; Seki et al., 1991). Our binarization algorithm is given in Section 3.4. Furthermore, we make the assumption that POS tagging is done before parsing. The POS tags are special non-terminals of fan-out 1. Scan: 0 : [A, hhi, i + 1ii] Unary: A POS tag of wi+1 in : [B, ρ~] in + |log(p) |: [A, ρ~] p : A(~ α) → B(~ α) ∈ P inB : [B, ρ~B ], inC : [C, ρ~C ] inB + inC + log(p) : [A, ρ~A ] where p : A(ρ~A ) →"
W10-4415,N07-1051,0,0.20918,"AINF(werden) → S1 (X1 X2 X3 ) → VP2 (X1 , X2 X3 ) → VP2 (X1 , X2 ) → → ε → ε ε ε VP2 (X1 , X3 ) VMFIN(X2 ) VP2 (X1 , X2 ) VAINF(X3 ) PROAV(X1 ) VVPP(X2 ) Figure 5: LCFRS rules for the tree in Fig. 4 3.3 Grammar Annotation Grammar annotation (i.e., manual enhancement of annotation information through category splitting) has previously been successfully employed in parsing German (Versley, 2005). In order to see if such modifications can have a beneficial effect in PLCFRS parsing, we will apply the following category splits to the Negra constituency data sets with unmodified labels (inspired by Petrov and Klein (2007)): We split the category S (“sentence”) into SRC (“relative clause”) and S (all other categories S). Relative clauses mostly occur in a very specific 1 This length restriction can be greatly alleviated by using an estimate of outside probabilities of parse items which speeds up parsing (Kallmeyer and Maier, 2010) 121 Wolfgang Maier, Laura Kallmeyer for all rules r = A(~ α) → A0 (α~0 ) . . . Am (α~m ) in P with m > 1 do remove r from P R := ∅ pick new non-terminals C1 , . . . , Cm−1 add the rule A(~ α) → A0 (α~0 )C1 (γ~1 ) to R where γ~1 is obtained by reducing α ~ with α~0 for all i, 1 ≤ i ≤ m"
W10-4415,W08-1006,0,0.0561134,".94 here 74.46 R&M 08 77.20 P&K 07 80.1 Table 3: Previous NeGra PCFG parsing Tab. 2 presents the constituent parsing results for both data sets with (LCFRS) and without (PCFG) crossing branches. For the sake of comparison, we 5 Note that our metric is equivalent to the corresponding PCFG metric for dim(A) = 1. 124 Discontinuity and Non-Projectivity: Using Mildly Context-Sensitive Formalisms for Data-Driven Parsing report PCFG parsing results from the literature6 in Tab. 3, namely for PCFG parsing with a plain vanilla treebank grammar (K¨ubler, 2005), for PCFG parsing with the Stanford parser (Rafferty and Manning, 2008) (markovization as in our parser), and for the current state-of-the-art, namely PCFG parsing with a latent variable model (Petrov and Klein, 2007). We see that the LCFRS parser output (which contains more information than the output of a PCFG parser) is competitive. The PCFG (1-LCFRS) parsing results are even closer to the ones of current systems. Recall that these are just first results, much optimization potential is left. Before we evaluate the experiments with category splits, we replace all split labels in the parser output with the corresponding original labels. The results show that the"
W10-4415,W07-2460,0,0.210488,"Missing"
W10-4415,A97-1014,0,0.873466,"Missing"
W10-4415,P87-1015,0,0.914137,"could also be interpreted as LCFRS derivations given an appropriate transformation algorithm. 119 Wolfgang Maier, Laura Kallmeyer The remainder of this paper is structured as follows. In the following section, we present the formalism and our parser. Sect. 3 is dedicated the experimental setup, Sect. 4 contains the experimental results. In Sect. 5, we present a conclusion. 2 A Parser for Probabilistic Linear Context-Free Rewriting Systems We notate LCFRS with the syntax of simple Range Concatenation Grammars (SRCG) (Boullier, 1998), a formalism that is equivalent to LCFRS. 2.1 PLCFRS A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, S) where a) N is a finite set of non-terminals with a function dim: N → N that determines the fan-out of each A ∈ N ; b) T and V are disjoint finite sets of terminals and variables; c) S ∈ N is the start symbol with dim(S) = 1; d) P is a finite set of rewriting rules (1) (1) A(α1 , . . . , αdim(A) ) → A1 (X1 , . . . , Xdim(A1 ) ) (m) · · · Am (X1 LCFRS and p : P → [0..1] a function such that ~ = 1. for all A ∈ N : ΣA(~x)→Φ∈P p(A(~x) → Φ) ~ 2.2 PLCFRS Parsing Our parser is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive pars"
W10-4415,J03-4003,0,\N,Missing
W10-4415,P00-1058,0,\N,Missing
W11-2913,2000.iwpt-1.8,0,0.123114,"art C and an agenda A, both initially empty, and proceeds as in Fig. 3. 3 Treebank Transformation The PTB annotation guidelines (Bies et al., 1995, Section 1.1) specify a set of rules that determine where arguments and adjuncts are attached with respect to their head words. For example, subjects are attached at clause level, most other arguments and adjuncts of verbs are attached at VP level, and phrases modifying nouns such as PPs and relative clauses are adjoined at NP level. Knowing these 1 This corresponds to the instantiated clauses in simple Range Concatenation Grammars (Boullier, 1998; Boullier, 2000). 106 rules, head-argument and head-adjunct dependencies can be read off the trees easily, e.g. for semantic interpretation. type *T* *T*-PRN *ICH* *EXP* *RNR* any reattachment no reattachment total Non-local head-argument and head-adjunct dependencies constitute exceptions to these rules. Following the rules would lead to discontinuous constituents with crossing branches, containing the head and the argument or adjunct, but not containing some intervening tokens. Examples of non-locally dependent arguments and adjuncts include wh-moved phrases, fronted phrases, extraposed modifiers, it-extrap"
W11-2913,W98-0503,0,0.0934361,"5(d)). The other special case concerns phrases, typically quotations, that surround the matrix phrase containing the head on which they depend. In the PTB annotation, the matrix phrase is embedded into such arguments under a node labeled PRN for parenthesis (Fig. 4(e)). To avoid cycles after the transformation, such matrix phrases are detached from within the argument and reattached to the node where the argument was originally attached, if any (Fig. 5(e)). Table 1 gives an overview of the tendency of each type of null element2 to introduce gaps when so transformed as indicated by gap-degree (Holan et al., 1998; Maier and Lichte, 2009), i.e. the maximal number of gaps in any constituent of the resulting trees. Most typically, one gap is introduced since there is a single phrase non-adjacent to the rest of the phrase to which it is attached. No gap at all is introduced by the reattachment of most whmoved subjects and *EXP*-type phrases in object position. Gap degrees of 2 are almost exclusively accounted for by surrounding phrases where the We use the approach proposed and tested on the German treebanks NEGRA and TIGER in Maier and Kallmeyer (2010): permit discontinuous constituents, attach non-local"
W11-2913,P04-1041,0,0.0612281,"Missing"
W11-2913,P04-1040,0,0.0590956,"Missing"
W11-2913,P04-1082,0,0.297006,"s are often discarded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite was used is separated into two non-adjacent parts. (1) is an example from the Penn Treebank (PTB). More generally, all constructions where head-argument or headmodifier dependencies are non-local, such as whmovement, can be seen as instances of discontin104 Proceedings of the 12th International Co"
W11-2913,P02-1018,0,0.217558,"banks with PTB-like annotation, labeling conventions and trace nodes are often discarded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite was used is separated into two non-adjacent parts. (1) is an example from the Penn Treebank (PTB). More generally, all constructions where head-argument or headmodifier dependencies are non-local, such as whmovement, can be seen a"
W11-2913,P03-1055,0,0.0604045,"non-local, such as whmovement, can be seen as instances of discontin104 Proceedings of the 12th International Conference on Parsing Technologies, pages 104–116, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. CFG: LCFRS: • section 5 reports the results or our parsing experiments with a detailed evaluation of the way the different types of long-distance dependencies are captured. Section 6 concludes. A A • γ1 • γ2 γ3 2 PLCFRS Parsing γ Figure 1: Different domains of locality 2.1 PLCFRS rated into the PCFG model (Collins, 1999) or into complex labels (Dienes and Dubey, 2003; Hockenmaier, 2003; Cahill et al., 2004). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004; Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2010). This paper pursues the third approach. Our work is based on recent research in using Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) for data driven parsing. LCFRSs extend CFGs such that nonterminals can span tuples of possibly non-adjacent strings (see Fig. 1). This enables them to describe discontinuous constituents and non-projective dependencies (Kuhlmann and Sat"
W11-2913,C10-1061,1,0.911542,"Linguistics October 5-7, 2011, Dublin City University. CFG: LCFRS: • section 5 reports the results or our parsing experiments with a detailed evaluation of the way the different types of long-distance dependencies are captured. Section 6 concludes. A A • γ1 • γ2 γ3 2 PLCFRS Parsing γ Figure 1: Different domains of locality 2.1 PLCFRS rated into the PCFG model (Collins, 1999) or into complex labels (Dienes and Dubey, 2003; Hockenmaier, 2003; Cahill et al., 2004). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004; Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2010). This paper pursues the third approach. Our work is based on recent research in using Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) for data driven parsing. LCFRSs extend CFGs such that nonterminals can span tuples of possibly non-adjacent strings (see Fig. 1). This enables them to describe discontinuous constituents and non-projective dependencies (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). Furthermore, they are able to capture synchronous derivations, something that is empirically attested in treebanks (Kallmeyer et al., 2009). In order to parse German,"
W11-2913,N06-1024,0,0.196628,"arded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite was used is separated into two non-adjacent parts. (1) is an example from the Penn Treebank (PTB). More generally, all constructions where head-argument or headmodifier dependencies are non-local, such as whmovement, can be seen as instances of discontin104 Proceedings of the 12th International Conference on Parsing Tec"
W11-2913,W09-3810,1,0.855365,"ier and Kallmeyer, 2010; Kallmeyer and Maier, 2010). This paper pursues the third approach. Our work is based on recent research in using Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) for data driven parsing. LCFRSs extend CFGs such that nonterminals can span tuples of possibly non-adjacent strings (see Fig. 1). This enables them to describe discontinuous constituents and non-projective dependencies (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). Furthermore, they are able to capture synchronous derivations, something that is empirically attested in treebanks (Kallmeyer et al., 2009). In order to parse German, a language where discontinuities are particularly frequent, Kallmeyer and Maier (2010); Maier and Kallmeyer (2010) use probabilistic LCFRSs (PLCFRSs). As a data source, they use the German NEGRA and TIGER treebanks that annotate discontinuous constituents by using crossing branches. We adapt this approach for German to English, using the PTB. For this, we first need to transform the trace-based annotation of discontinuous constituents into an annotation with crossing branches which requires a careful treatment of the different types of traces that occur in the PTB."
W11-2913,N09-1061,0,0.340984,"Missing"
W11-2913,W06-1508,0,0.195792,"composition function of the rule. c) Nothing else is in yield(A). The language is then {w |hwi ∈ yield(S)}. The fan-out of an LCFRS G is the maximal fanout of all non-terminals in G. An LCFRS with a fan-out of n is called an n-LCFRS. Furthermore, the RHS length of a rewriting rules r ∈ P is called the rank of r and the maximal rank of all rules in P is called the rank of G. We call a LCFRS monotone if for every r ∈ P and every RHS nonterminal A in r and each pair X1 , X2 of arguments of A in the RHS of r, X1 precedes X2 in the RHS iff X1 precedes X2 in the LHS. A probabilistic LCFRS (PLCFRS) (Kato et al., 2006) is a tuple hN, T, V, P, S, pi such that hN, T, V, P, Si is a LCFRS and p : P → [0..1] a function such that for all A ∈ N : ~ = 1. ΣA(~x)→Φ∈P p(A(~x) → Φ) ~ 2.2 CYK Parsing We use the parser from Kallmeyer and Maier (2010); Maier (2010), Maier and Kallmeyer (2010) which is a probabilistic version of the CYK parser from Seki et al. (1991), applying techniques of weighted deductive parsing (Nederhof, 2003). LCFRSs can be binarized (G´omez-Rodr´ıguez et al., 2009) and ε-components in the LHS of rules can be removed (Boullier, 1998). We can therefore assume that all rules are of rank 2 (in section"
W11-2913,H94-1020,0,0.426269,"Penn Treebank. Even the evaluation results concerning local dependencies, which can in principle be captured by a PCFG-based model, are better with our PLCFRS model. This demonstrates that by discarding information on non-local dependencies the PCFG model loses important information on syntactic dependencies in general. (1) Areas of the factory were particularly dusty where the crocidolite was used. In the past, data-driven parsing has largely been dominated by Probabilistic Context-Free Grammar (PCFG). This is partly due to the annotation formats of treebanks such as the Penn Treebank (PTB) (Marcus et al., 1994), which are used as a data source for grammar extraction. Their annotation generally relies on the use of trees without crossing branches, augmented with a mechanism that accounts for non-local dependencies. In the PTB, e.g., labeling conventions and trace nodes are used which establish additional implicit edges in the tree beyond the overt phrase structure. However, given the expressivity restrictions of PCFG, work on data-driven parsing has mostly excluded non-local dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded. Some wor"
W11-2913,E09-1055,0,0.182328,"and Dubey, 2003; Hockenmaier, 2003; Cahill et al., 2004). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004; Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2010). This paper pursues the third approach. Our work is based on recent research in using Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) for data driven parsing. LCFRSs extend CFGs such that nonterminals can span tuples of possibly non-adjacent strings (see Fig. 1). This enables them to describe discontinuous constituents and non-projective dependencies (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). Furthermore, they are able to capture synchronous derivations, something that is empirically attested in treebanks (Kallmeyer et al., 2009). In order to parse German, a language where discontinuities are particularly frequent, Kallmeyer and Maier (2010); Maier and Kallmeyer (2010) use probabilistic LCFRSs (PLCFRSs). As a data source, they use the German NEGRA and TIGER treebanks that annotate discontinuous constituents by using crossing branches. We adapt this approach for German to English, using the PTB. For this, we first need to transform the trace-based annotati"
W11-2913,P04-1042,0,0.407251,"like annotation, labeling conventions and trace nodes are often discarded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite was used is separated into two non-adjacent parts. (1) is an example from the Penn Treebank (PTB). More generally, all constructions where head-argument or headmodifier dependencies are non-local, such as whmovement, can be seen as instances of discontin"
W11-2913,E06-1011,0,0.0532186,"ontinuous parsing results. Table 4 shows that discontinuous parsing as compared to context-free parsing boosts the unlabeled attachment score (i.e. recall on word-word dependencies) slightly for local dependencies and considerably for non-local dependencies. The latFigure 8: Failure to recognize the discontinuous NP phone calls from nervous shareholders Figure 9: Correct parse of a deeply embedded moved wh-phrase ter are broken down by type as in Section 3. Dependency evaluation also allows a direct comparison with state-of-the-art dependency parsers. In Table 4 we give results for MSTParser (McDonald and Pereira, 2006) trained on two dependency versions of Tr ′ , converted from constituents to dependencies once without dependency labels and once with dependency labels using the method of Hall and Nivre (2008). It can be seen that MSTParser recognizes a fair percentage of even the difficult *ICH* and *EXP* type dependencies (cf. Section 5.3) and that it has a considerably better overall score. We expect that this gap can be bridged by optimizing Maier and Kallmeyer’s parser with techniques successfully used for context-free constituent parsing as outlined above, but this remains to be proven experimentally."
W11-2913,H05-1066,0,0.123007,"are used which establish additional implicit edges in the tree beyond the overt phrase structure. However, given the expressivity restrictions of PCFG, work on data-driven parsing has mostly excluded non-local dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite"
W11-2913,W10-4415,1,0.907067,"ociation for Computational Linguistics October 5-7, 2011, Dublin City University. CFG: LCFRS: • section 5 reports the results or our parsing experiments with a detailed evaluation of the way the different types of long-distance dependencies are captured. Section 6 concludes. A A • γ1 • γ2 γ3 2 PLCFRS Parsing γ Figure 1: Different domains of locality 2.1 PLCFRS rated into the PCFG model (Collins, 1999) or into complex labels (Dienes and Dubey, 2003; Hockenmaier, 2003; Cahill et al., 2004). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004; Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2010). This paper pursues the third approach. Our work is based on recent research in using Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) for data driven parsing. LCFRSs extend CFGs such that nonterminals can span tuples of possibly non-adjacent strings (see Fig. 1). This enables them to describe discontinuous constituents and non-projective dependencies (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). Furthermore, they are able to capture synchronous derivations, something that is empirically attested in treebanks (Kallmeyer et al., 2009)"
W11-2913,J03-1006,0,0.561222,"very r ∈ P and every RHS nonterminal A in r and each pair X1 , X2 of arguments of A in the RHS of r, X1 precedes X2 in the RHS iff X1 precedes X2 in the LHS. A probabilistic LCFRS (PLCFRS) (Kato et al., 2006) is a tuple hN, T, V, P, S, pi such that hN, T, V, P, Si is a LCFRS and p : P → [0..1] a function such that for all A ∈ N : ~ = 1. ΣA(~x)→Φ∈P p(A(~x) → Φ) ~ 2.2 CYK Parsing We use the parser from Kallmeyer and Maier (2010); Maier (2010), Maier and Kallmeyer (2010) which is a probabilistic version of the CYK parser from Seki et al. (1991), applying techniques of weighted deductive parsing (Nederhof, 2003). LCFRSs can be binarized (G´omez-Rodr´ıguez et al., 2009) and ε-components in the LHS of rules can be removed (Boullier, 1998). We can therefore assume that all rules are of rank 2 (in section 4.1, we explain our binarization technique) and do not contain ε components in their LHSs. Furthermore, we assume POS tagging to be done before parsing. POS tags are non-terminals of fan-out 1. The rules are then either of the form A(a) → ε with A a POS tag and a ∈ T or of the form A(~ α) → B(~x) or A(~ α) → B(~x)C(~y ) where α ~ ∈ (V + )dim(A) , i.e., only the rules for POS tags contain terminals in th"
W11-2913,E06-1010,0,0.0168327,"trictions of PCFG, work on data-driven parsing has mostly excluded non-local dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite was used is separated into two non-adjacent parts. (1) is an example from the Penn Treebank (PTB). More generally, all cons"
W11-2913,P09-1040,0,0.06299,"dditional implicit edges in the tree beyond the overt phrase structure. However, given the expressivity restrictions of PCFG, work on data-driven parsing has mostly excluded non-local dependencies. When using treebanks with PTB-like annotation, labeling conventions and trace nodes are often discarded. Some work has however been done towards incorporating non-local information into data-driven parsing. One general way to do this is (nonprojective) dependency parsing where parsers are not grammar-based and the notion of constituents or phrases is not employed, see e.g. McDonald et al. (2005) or Nivre (2009). Within the domain of grammar-based constituent parsing, we can distinguish three approaches (Nivre, 2006): 1. Non-local information can be reconstructed in a post-processing step after PCFG parsing (Johnson, 2002; Levy and Manning, 2004; Jijkoun and de Rijke, 2004; Campbell, 2004; Gabbard et al., 2006). 2. Non-local information can be incorpo1 Introduction Discontinuous constituents as exemplified in (1) are more frequent than generally assumed, even in languages such as English that display a rather rigid word order. In (1), the NP areas of the factory where the crocidolite was used is sepa"
W11-2913,W10-1407,0,0.644007,"tober 5-7, 2011, Dublin City University. CFG: LCFRS: • section 5 reports the results or our parsing experiments with a detailed evaluation of the way the different types of long-distance dependencies are captured. Section 6 concludes. A A • γ1 • γ2 γ3 2 PLCFRS Parsing γ Figure 1: Different domains of locality 2.1 PLCFRS rated into the PCFG model (Collins, 1999) or into complex labels (Dienes and Dubey, 2003; Hockenmaier, 2003; Cahill et al., 2004). 3. A formalism can be used which accommodates the direct encoding of non-local information (Plaehn, 2004; Maier and Kallmeyer, 2010; Kallmeyer and Maier, 2010). This paper pursues the third approach. Our work is based on recent research in using Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987) for data driven parsing. LCFRSs extend CFGs such that nonterminals can span tuples of possibly non-adjacent strings (see Fig. 1). This enables them to describe discontinuous constituents and non-projective dependencies (Kuhlmann and Satta, 2009; Maier and Lichte, 2009). Furthermore, they are able to capture synchronous derivations, something that is empirically attested in treebanks (Kallmeyer et al., 2009). In order to parse German,"
W11-2913,P06-1055,0,0.0325398,"eate discontinuous versions of the training and test set by carrying out the reattachment operations described in Section 3 while also keeping context-free versions. All four sets are then preprocessed by removing all (remaining) indices, null elements and empty constituents. We call the resulting context-free training and test set Tr and Te, and the resulting discontinuous training and test set Tr ′ and Te ′ . 5.1 ent parsers. We compare Maier and Kallmeyer’s parser trained on Tr ′ (resulting in a 3-PLCFRS) with three parsers that do not produce discontinuous structures: the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) trained on Tr using our manual category splits but no automatic splitting/merging/smoothing, the Berkeley parser trained on Tr using its default setting of six iterations of split/merge/smooth, and Maier and Kallmeyer’s parser with a grammar extracted from Tr (a 1-PLCFRS, i.e. a PCFG). The upper half of Table 2 shows the results. For comparison, we also evaluated the three context-free parsers on the untransformed context-free test set Te. These figures are given in the lower half of the table. For Maier and Kallmeyer’s parser, the number of rules in the grammar befor"
W11-2913,P87-1015,0,0.780582,"Missing"
W11-2913,N07-1051,0,\N,Missing
W11-2913,J03-4003,0,\N,Missing
W11-2913,P03-1054,0,\N,Missing
W12-4604,E03-1030,1,0.71958,"perfective verb, and c) no holistic effect in other cases. We also aim at providing an explanation of why some verbs allow locative alternation and some do not and how the addition of a prefix to a Russian verb changes the set of constructions it can participate in. 3 LTAG and Frame Semantics Following (Kallmeyer and Osswald, 2012a), we adopt a syntax-semantics interface that links a single semantic representation (in our case, a semantic frame) to an entire elementary tree and that models semantic composition by unifications triggered by substitution and adjunction. In this we partly follow (Gardent and Kallmeyer, 2003; Kallmeyer and Romero, 2008), except that our focus is on event semantics and the decomposition of lexical meaning and we therefore use frames. Formally, frames are taken to be typed feature structures. Each elementary tree is linked to a feature structure and unification is triggerd via the feature unifications in the syntax. For this purpose, some of the nodes in the elementary trees have semantic features such as I (for inidividual) and E (for event). Their unifications cause equations between metavariables. As a result, the corresponding semantic feature structures are unified as well. A"
W12-4613,W98-0106,0,0.208008,"TAG), the operations of substitution and adjunction establish an asymmetric relation between two lexical items: The elementary tree for one item is substituted or adjoined into the elementary tree for another one. In many cases, this relation can be interpreted as a relation of syntactic dependency (complementation or adjunction), and it is natural then to try to interpret LTAG derivations as dependency trees. However, as several authors have pointed out, the correspondence between derivation trees and dependency structures is not as direct as it may seem at first glance (Rambow et al., 1995; Candito and Kahane, 1998; Frank and van Genabith, 2001). Examples of mismatches are in particular those where predicates adjoin into their arguments, an analysis that is chosen whenever an argument allows for long extractions. In these cases, the edge The paper is structured as follows. We start by reviewing the divergence between derivation trees and dependency trees in Section 2. In Section 3 we present the basic ideas behind our transformation. A formalization of this transformation is given in Section 4. In Section 5 we examine the structural properties of the dependency trees that can be induced using our transf"
W12-4613,W08-2303,0,0.0287142,"whenever different parts of an elementary tree can be split. Arguments are added by substitution but an ‘extracted’ part of an argument, linked to the lower part by a dominance link, can be separated from this lower part and end up much higher. This gives more flexibility concerning the modeling of discontinuities and non-projective dependencies. Another LTAG variant that has been discussed a lot recently in the context of the ‘missing link problem’ is tree-local MCTAG with flexible composition (Joshi et al., 2007; Chen-Main and Joshi, 2012), formalized by the notion of delayed treelocality (Chiang and Scheffler, 2008). The idea is roughly to perform the reversal of complementtaking adjunctions not on the derivation tree, but already during derivation. More precisely, instead of considering such an operation as a standard adjunction, it is considered as a wrapping operation directed from the adjunction site to the auxiliary tree. Consider for instance the derivation in Fig. 3. If we take the adjunction of the think tree into the prefer tree to be a wrapping of prefer, split at its internal S node, around the think tree. If this is reflected in the derivation tree by an edge from prefer to think, then one ob"
W12-4613,E03-1030,1,0.90113,"s is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. Different strategies have been adopted to obtain linguistically plausible dependency structures using LTAG. Some proposals adopt variants of the formalism with different derivation operations (Rambow et al., 2001; Chen-Main and Joshi, 2012); others retrieve the missing dependencies from the derivation tree and the derived tree (Kallmeyer, 2002; Gardent and Kallmeyer, 2003). In this paper we follow the second line of work in that we take a two-step approach: To get a dependency tree, we first construct a derivation tree, and then obtain the dependencies in a postprocessing step. However, in contrast to previous work we retain the property that dependencies should form a tree structure: We do not regard the missing dependencies as additions to the derivation tree, but view the correspondence between derivation trees and dependency structures as a tree-to-tree mapping. The crucial feature of this mapping is that it can reverse some of the edges in the derivation t"
W12-4613,W02-2218,1,0.885941,"the original edges is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. Different strategies have been adopted to obtain linguistically plausible dependency structures using LTAG. Some proposals adopt variants of the formalism with different derivation operations (Rambow et al., 2001; Chen-Main and Joshi, 2012); others retrieve the missing dependencies from the derivation tree and the derived tree (Kallmeyer, 2002; Gardent and Kallmeyer, 2003). In this paper we follow the second line of work in that we take a two-step approach: To get a dependency tree, we first construct a derivation tree, and then obtain the dependencies in a postprocessing step. However, in contrast to previous work we retain the property that dependencies should form a tree structure: We do not regard the missing dependencies as additions to the derivation tree, but view the correspondence between derivation trees and dependency structures as a tree-to-tree mapping. The crucial feature of this mapping is that it can reverse some of"
W12-4613,J13-2004,1,0.816552,"side. We call this tuple the template of f , and use it as a unique name for f . This means that we can talk about e.g. the standard binary concatenation function as ‘the yield function ""x11 x21 #’. The yield function associated with an operation symbol in a derivation tree can be extracted in a systematic way; see Boullier (1999) for a procedure that performs this extraction in the formalism of range concatenation grammars. 4.3 Direct Interpretation The direct interpretation of a derivation tree as a dependency tree can be formalized by interpreting symbols as operations on dependency trees (Kuhlmann, 2013). Continuing our running example, suppose that for each subtree of t0 we are not only given a string or a pair of strings as before, but also a corresponding dependency tree. Then the symbol to_love has a straightforward reading as constructing a new dependency tree for the full sentence (1): Preserve all the old dependencies, 111 and add new edges from to_love to the roots of the dependency trees associated with the subterms. 4.4 Transformation v1 v4 w1 w2 We illustrate the formal transformation by means of our running example. In order to convey the intuitive idea, we first present the trans"
W12-4613,P95-1021,0,0.686923,"adjoining grammar (LTAG), the operations of substitution and adjunction establish an asymmetric relation between two lexical items: The elementary tree for one item is substituted or adjoined into the elementary tree for another one. In many cases, this relation can be interpreted as a relation of syntactic dependency (complementation or adjunction), and it is natural then to try to interpret LTAG derivations as dependency trees. However, as several authors have pointed out, the correspondence between derivation trees and dependency structures is not as direct as it may seem at first glance (Rambow et al., 1995; Candito and Kahane, 1998; Frank and van Genabith, 2001). Examples of mismatches are in particular those where predicates adjoin into their arguments, an analysis that is chosen whenever an argument allows for long extractions. In these cases, the edge The paper is structured as follows. We start by reviewing the divergence between derivation trees and dependency trees in Section 2. In Section 3 we present the basic ideas behind our transformation. A formalization of this transformation is given in Section 4. In Section 5 we examine the structural properties of the dependency trees that can b"
W12-4613,J01-1004,0,0.195329,"iew the correspondence between derivation trees and dependency structures as a tree transformation during which the direction of some of the original edges is reversed. We show that, under this transformation, LTAG is able to induce both ill-nested dependency trees and dependency trees with gap-degree greater than 1, which is not possible under the direct reading of derivation trees as dependency trees. Different strategies have been adopted to obtain linguistically plausible dependency structures using LTAG. Some proposals adopt variants of the formalism with different derivation operations (Rambow et al., 2001; Chen-Main and Joshi, 2012); others retrieve the missing dependencies from the derivation tree and the derived tree (Kallmeyer, 2002; Gardent and Kallmeyer, 2003). In this paper we follow the second line of work in that we take a two-step approach: To get a dependency tree, we first construct a derivation tree, and then obtain the dependencies in a postprocessing step. However, in contrast to previous work we retain the property that dependencies should form a tree structure: We do not regard the missing dependencies as additions to the derivation tree, but view the correspondence between der"
W12-4613,P87-1015,0,0.21392,"John claim Mary seems 1 Bill claim claim 1 Bill ! to_love 1 21 22 John Mary seems Bill ! seems to_love John Mary Figure 4: Transformation of the derivation tree for (1) 2 Kallmeyer and Romero (2008) use specific features on the head projection line of verbs (the verbal spine in their terminology) in order to retrieve the missing dependency links. 110 4 Formal Version of the Transformation In this section we give a formal account of the transformation sketched above. 4.1 Derivation Trees as Terms We represent derivation trees as terms, formal expressions over a signature of operation symbols (Vijay-Shanker et al., 1987). For example, we represent the derivation in Fig. 2 by the term t0 : respectively. The yield of the derived tree corresponding to a derivation tree t can be obtained by associating with each operation symbol in t a yield function (Weir, 1988). To illustrate the idea, suppose that the yields of the four subterms of t0 are given as ""John#, ""Bill claims, ε#, ""Mary#, and ""seems, ε#, respectively. Then the full yield (1) is obtained by assigning the following yield function to ‘to_love’: to_love(""x11 #, ""x21 , x22 #, ""x31 #, ""x41 , x42 #) = t0 = to_love(John, claims(Bill), Mary, seems) Here ‘to_lo"
W12-4615,W07-1506,0,0.3462,"Missing"
W12-4615,W11-2913,1,0.869716,"ation labels is used in order to establish implicit edges. In other treebanks, e.g., the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks, crossing branches are allowed.1 This way, all parts of a discontinuous constituent can 1 The annotation differences between TIGER and NeGra are minor and can be neglected for the purpose of this work. be grouped under a single node. There is no fundamental difference between both representations: PTB-style annotation can be converted into a NeGra/TIGER-style annotation. This has been done in the Discontinuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011). For data-driven parsing with Probabilistic CFG (PCFG), the annotation information concerning discontinuities must be discarded, because it exceeds the expressivity of CFG. For NeGra, there exist two methods, namely (i) attaching nonhead daughters of discontinuous constituents to higher positions in the tree, such that the crossing branches disappear (the NeGra distribution contains a version of the treebank in which this transformation is readily carried out), or (ii) introducing an additional non-terminal node for each continuous part of a discontinuous constituent (Boyd, 2007). As an examp"
W12-4615,N10-1035,0,0.0586414,"Missing"
W12-4615,C10-1061,1,0.852132,"ssing branches removal for NeGra. Note that the argument structure is changed as a result of the removal of the crossing branches. discontinuities (Maier and Lichte, 2011). In LCFRS, a single non-terminal can span k ≥ 1 continuous blocks of a string. A CFG is simply a special case of an LCFRS in which k = 1. k is called the fan-out of the non-terminal, and a corresponding constituent is said to have block degree k. It has been shown that probabilistic data-driven parsing on the basis of Probabilistic LCFRS (PLCFRS) is feasible and gives good results while preserving discontinuity information (Kallmeyer and Maier, 2010; Maier, 2010; van Cranenburgh et al., 2011; Evang and Kallmeyer, 2011; van Cranenburgh, 2012; Maier, 2012). The major problem of PLCFRS parsing is its high computational complexity. A binarized PCFG can be parsed in O(n3 ), parsing a binarized LCFRS takes O(n3k ) (Seki et al., 1991), where k is the fan-out of the grammar (the maximal fan-out of any of its non-terminals). The parsers from the literature allow for an unbounded k. This leads to parsing times beyond practically acceptable values for sentences longer than 25 to 30 words. In this paper, our goal is to show that by restricting the b"
W12-4615,W06-1508,0,0.908376,". In the specialized deduction system, unary items now take the form [A, i, j] and binary items take the form [A, i, j, k, l], where A ∈ N and i, j, resp. k, l are spans dominated by A with 0 ≤ i &lt; j &lt; k &lt; l ≤ n. The goal item is [S, 0, n]. We replace the old Unary and Binary deduction rules in figure 4 with 14 new rules, one per production type. Figure 6 shows the new scan rule and the complete rules for type 1, type 6 and type 10, which should make the basic idea clear. Note that there is no need to refer to instantiations anymore. Our case-by-case strategy is similar to the one employed by Kato et al. (2006). As in our previous work, we specify the set 0 : [A, i, i + 1] A POS tag of wi+1 in : [B, i, j] in + |log(p) |: [A, i, j] where p : A(X) → B(X) ∈ P . Complete1: inB : [B, i, j, k, l], inC : [C, l, u] inB + inC + |log(p) |: [A, i, j, k, u] where p : A(X, Y Z) → B(X, Y )C(Z) ∈ P . Complete6: inB : [B, i, x, k, y], inC : [C, x, j, y, l] inB + inC + |log(p) |: [A, i, j, k, l] where p : A(XY, ZU ) → B(X, Z)C(Y, U ) ∈ P Complete10: Figure 6: Weighted CYK deduction rules for 2-LCFRS of parse items using the algorithm of weighted deductive parsing (WDP) (Nederhof, 2003). In WDP, one maintains a prior"
W12-4615,N03-1016,0,0.677656,"CYK deduction rules for 2-LCFRS of parse items using the algorithm of weighted deductive parsing (WDP) (Nederhof, 2003). In WDP, one maintains a priority queue of items, sorted by the resp. Viterbi inside scores. The topmost item is always processed first. WDP guarantees optimality, i.e., that the best parse is found. 5.2 A Novel Outside Estimate One can speed up parsing by adding to the Viterbi inside score of an item an estimate of its Viterbi outside score, in other words, an estimate of the cost of completion of the item to a complete parse. This has proven to be successful for both PCFG (Klein and Manning, 2003) and PLCFRS (Kallmeyer and Maier, 2010). As outside estimate, one uses the outside probability of a summary of the item, i.e., of an equivalence class of parse items. The difficulty for PLCFRS is to choose the summary such that optimality is maintained through the two estimate properties admissibility and monotonicity (Klein and Manning, 2003). Here, we present the novel LN estimate, which is based on a summary that records only the sum of the span lengths and the length of the entire sentence. It is the first practically computable estimate which allows for maintaining optimality. The estimat"
W12-4615,W10-1407,1,0.742211,"removal for NeGra. Note that the argument structure is changed as a result of the removal of the crossing branches. discontinuities (Maier and Lichte, 2011). In LCFRS, a single non-terminal can span k ≥ 1 continuous blocks of a string. A CFG is simply a special case of an LCFRS in which k = 1. k is called the fan-out of the non-terminal, and a corresponding constituent is said to have block degree k. It has been shown that probabilistic data-driven parsing on the basis of Probabilistic LCFRS (PLCFRS) is feasible and gives good results while preserving discontinuity information (Kallmeyer and Maier, 2010; Maier, 2010; van Cranenburgh et al., 2011; Evang and Kallmeyer, 2011; van Cranenburgh, 2012; Maier, 2012). The major problem of PLCFRS parsing is its high computational complexity. A binarized PCFG can be parsed in O(n3 ), parsing a binarized LCFRS takes O(n3k ) (Seki et al., 1991), where k is the fan-out of the grammar (the maximal fan-out of any of its non-terminals). The parsers from the literature allow for an unbounded k. This leads to parsing times beyond practically acceptable values for sentences longer than 25 to 30 words. In this paper, our goal is to show that by restricting the b"
W12-4615,J93-2004,0,0.0406121,"man NeGra treebank and the Discontinuous Penn Treebank in which all trees have block degree two. The experiments show that compared to previous work, our approach provides an enormous speed-up while delivering an output of comparable richness. 1 Introduction In many constituency treebanks, the syntactic annotation takes the form of Context-Free Grammar (CFG) derivation trees, i.e., of trees with no crossing branches. Discontinuous structures (Huck and Ojeda, 1987) cannot be modeled with CFG and are therefore handled by an additional mechanism in such an annotation. In the Penn Treebank (PTB) (Marcus et al., 1993), for instance, a combination of trace nodes and co-indexation labels is used in order to establish implicit edges. In other treebanks, e.g., the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks, crossing branches are allowed.1 This way, all parts of a discontinuous constituent can 1 The annotation differences between TIGER and NeGra are minor and can be neglected for the purpose of this work. be grouped under a single node. There is no fundamental difference between both representations: PTB-style annotation can be converted into a NeGra/TIGER-style annotation. This"
W12-4615,J03-1006,0,0.356855,"ilar to the one employed by Kato et al. (2006). As in our previous work, we specify the set 0 : [A, i, i + 1] A POS tag of wi+1 in : [B, i, j] in + |log(p) |: [A, i, j] where p : A(X) → B(X) ∈ P . Complete1: inB : [B, i, j, k, l], inC : [C, l, u] inB + inC + |log(p) |: [A, i, j, k, u] where p : A(X, Y Z) → B(X, Y )C(Z) ∈ P . Complete6: inB : [B, i, x, k, y], inC : [C, x, j, y, l] inB + inC + |log(p) |: [A, i, j, k, l] where p : A(XY, ZU ) → B(X, Z)C(Y, U ) ∈ P Complete10: Figure 6: Weighted CYK deduction rules for 2-LCFRS of parse items using the algorithm of weighted deductive parsing (WDP) (Nederhof, 2003). In WDP, one maintains a priority queue of items, sorted by the resp. Viterbi inside scores. The topmost item is always processed first. WDP guarantees optimality, i.e., that the best parse is found. 5.2 A Novel Outside Estimate One can speed up parsing by adding to the Viterbi inside score of an item an estimate of its Viterbi outside score, in other words, an estimate of the cost of completion of the item to a complete parse. This has proven to be successful for both PCFG (Klein and Manning, 2003) and PLCFRS (Kallmeyer and Maier, 2010). As outside estimate, one uses the outside probability"
W12-4615,W07-2460,0,0.493969,"Missing"
W12-4615,A97-1014,0,0.386495,"ous speed-up while delivering an output of comparable richness. 1 Introduction In many constituency treebanks, the syntactic annotation takes the form of Context-Free Grammar (CFG) derivation trees, i.e., of trees with no crossing branches. Discontinuous structures (Huck and Ojeda, 1987) cannot be modeled with CFG and are therefore handled by an additional mechanism in such an annotation. In the Penn Treebank (PTB) (Marcus et al., 1993), for instance, a combination of trace nodes and co-indexation labels is used in order to establish implicit edges. In other treebanks, e.g., the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks, crossing branches are allowed.1 This way, all parts of a discontinuous constituent can 1 The annotation differences between TIGER and NeGra are minor and can be neglected for the purpose of this work. be grouped under a single node. There is no fundamental difference between both representations: PTB-style annotation can be converted into a NeGra/TIGER-style annotation. This has been done in the Discontinuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011). For data-driven parsing with Probabilistic CFG (PCFG), the annotation information concerning"
W12-4615,W11-3805,0,0.240245,"Missing"
W12-4615,E12-1047,0,0.547251,"Missing"
W16-3305,E03-1030,1,0.686818,"face of Lexicalized Tree Adjoining Grammar (LTAG) builds on the assumptions that (i) the elementary tree of a predicate contains slots for the arguments of the predicate, (ii) this elementary tree is paired with a semantic representation with semantic arguments, (iii) there is a linking between syntactic argument slots and semantic arguments that makes sure that the filling of an argument node in the syntax triggers the insertion of a corresponding semantic representation into the linked semantic argument position. This holds for the unification-based approach from Kallmeyer and Joshi (2003), Gardent and Kallmeyer (2003) and Kallmeyer and Romero (2008) using predicate logic, for the approach based on synchronous TAG (Shieber, 1994; Nesson and Shieber, 2006; Nesson and Shieber, 2008) and also for the frame-based approach of Kallmeyer and Osswald (2013). However, none of these approaches has implemented a theory which explains why only certain patterns of argument linking are allowed. In Fig. 1, for instance, the elementary tree for ate is paired with the upper frame while the lower frame is not grammatical in combination with this tree because of the incorrect linking. The AGENT has to be contributed by the su"
W16-3305,W08-2310,0,0.019426,"e, (ii) this elementary tree is paired with a semantic representation with semantic arguments, (iii) there is a linking between syntactic argument slots and semantic arguments that makes sure that the filling of an argument node in the syntax triggers the insertion of a corresponding semantic representation into the linked semantic argument position. This holds for the unification-based approach from Kallmeyer and Joshi (2003), Gardent and Kallmeyer (2003) and Kallmeyer and Romero (2008) using predicate logic, for the approach based on synchronous TAG (Shieber, 1994; Nesson and Shieber, 2006; Nesson and Shieber, 2008) and also for the frame-based approach of Kallmeyer and Osswald (2013). However, none of these approaches has implemented a theory which explains why only certain patterns of argument linking are allowed. In Fig. 1, for instance, the elementary tree for ate is paired with the upper frame while the lower frame is not grammatical in combination with this tree because of the incorrect linking. The AGENT has to be contributed by the subject while the THEME slot is filled via the object substitution node. The principles and constraints underlying linking have been extensively investigated (Levin an"
W16-3305,C88-2147,0,0.664475,"4; Duchier et al., 2004). We will use the metagrammar compiler XMG (Crabb´e et al., 2013; Lichte and Petitjean, 2015) with frames as semantic representations (Kallmeyer and Osswald, 2013). Since argument linking is independent from the choice of the semantic representation, our analysis could also be applied to an LTAG syntax-semantics interface using one of the other frameworks mentioned above. 2 The syntax-semantics interface We assume familiarity with the basic notions of LTAG (Joshi and Schabes, 1997; Abeill´e and Rambow, 2000), enriched with syntactic feature structures in the usual way (Vijay-Shanker and Joshi, 1988). Following Kallmeyer and Osswald (2013), we pair syntactic trees with frame-semantic representations, which instantiate a slightly extended variant of typed feature structure. An example is given in Fig. 2. We use interface features on the syntactic nodes that are responsible for triggering semantic composition (i.e., frame unification) via the syntactic feature unifications during substitution and adjunction. These features here are I (for “individual”) and E (for “event”), whose values are variables that also occur in the frames. Upon substituting the elementary tree of John into the subjec"
W16-3305,J13-3005,0,\N,Missing
W16-5311,S15-2151,0,0.0148292,"cation of Semantic Relations. We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al."
W16-5311,S16-1168,0,0.0235916,"classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently wi"
W16-5311,S07-1081,0,0.0372437,"antic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2"
W16-5311,N16-2002,0,0.0163109,"many. For example, while it is relatively easy to predict ‘queen’ as the answer to this query x = king − man + woman, you cannot expect ‘contract’ as the answer to the query x = shoe − boot + lease with the same level of confidence if the relationship is expected to be either synonymy, antonymy, hyponymy, or hypernymy. In this paper we try three different methods for handling semantic classification in the shared task: word analogy, linear regression and multi-task CNN. Using word analogy for identifying semantic relations has been discussed in a number of papers including (Levy et al., 2015; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and Mc"
W16-5311,P05-1053,0,0.0185926,"linear regression and multi-task CNN. Using word analogy for identifying semantic relations has been discussed in a number of papers including (Levy et al., 2015; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we"
W16-5311,P97-1023,0,0.558863,"; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to e"
W16-5311,S12-1047,0,0.0318201,"tend to have similar contextual embeddings. This paper describes our system for the CogALex-V Shared Task on Corpus-Based Identification of Semantic Relations. We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about thr"
W16-5311,S14-2003,0,0.0171965,"s our system for the CogALex-V Shared Task on Corpus-Based Identification of Semantic Relations. We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman,"
W16-5311,W14-1618,0,0.0310541,"ord 54 . This is built with the GloVe architecture from a corpus of 6B words (400K vocabulary entries) with 300 dimensions, and applying AdaGrad with context size of 20. 4 Experiments and Results In this section we outline the experiments and report the results for the three approaches we tested: word analogy, linear regression and multi-task CNN. The results reported in this section are on the training set for all labels including “FALSE” for Task-1 and “RANDOM” for Task-2. Results on the test set of our selected systems are reported in Section 5. 4.1 Word Analogy In word analogy, similar to Levy et al. (2014), we query the word vector directly to obtain the closest match to the given example using the formula: predicted_word = example_word1 − example_word2 + target_word. We iterate the query over all the examples in the training set and limit the search scope to the vocabulary items within the set (a set is the target word and all potentially related words). Then we take the average of the responses. The results in Table 2 show that this approach does not work as well for this current task. As we will show, the scores are much lower than those of the other approaches we explored here. 4.2 Linear R"
W16-5311,Q15-1016,0,0.0391388,"en words is one-to-many. For example, while it is relatively easy to predict ‘queen’ as the answer to this query x = king − man + woman, you cannot expect ‘contract’ as the answer to the query x = shoe − boot + lease with the same level of confidence if the relationship is expected to be either synonymy, antonymy, hyponymy, or hypernymy. In this paper we try three different methods for handling semantic classification in the shared task: word analogy, linear regression and multi-task CNN. Using word analogy for identifying semantic relations has been discussed in a number of papers including (Levy et al., 2015; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge."
W16-5311,N13-1090,0,0.367689,"et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently with the introduction of the new log linear architecture of Mikolov et al. (2013a). This architecture provided an efficient and simplified training methodology that minimizes computational complexity by doing away with the non-linear hidden layer, enabling training on much larger data than were previously possible. The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality siz"
W16-5311,D14-1162,0,0.0899215,", 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently with the introduction of the new log linear architecture of Mikolov et al. (2013a). This architecture provided an efficient and simplified training methodology that minimizes computational complexity by doing away with the non-linear hidden layer, enabling training on much larger data than were previously possible. The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes. The evaluation data1 used in the development of the Google Continuous Bag of Words (CBOW) and skip-gram vectors (Mikolov et al., 2013a) focused on semantic similarities and coarse-grained semantic relations in the form of deterministic answers by analogy. These relationships were one-to-one including, for example, capitals (Athens: Greece - Baghdad: Iraq), currencies (India: rupee - Iran: rial), gender (king: queen - man: woman), derivation (amazing: amazingly - safe: safely), and i"
W16-5311,W14-4701,0,0.0286264,"egression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently with the introduction of the new log linear architecture of Mikolo"
W16-5311,P15-1061,0,0.025986,"Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2015) and (Xu et al., 2015). 3 Data Description 3.1 Shared Task Data The shared task organizers provide a training set of 3,054 word pairs for 318 target words. In Task-1, we are given a pair of words and we need to determine if the words are semantically related or not. Some examples of Task-1 are shown in 1. In Task-2 participants are required to detect the type of the relationship: HYPER, PART_OF, SYN, ANT, or RANDOM. 3.2 Pre-Trained Word Vectors In our experiments we experimented with three large-scale, publicly available pre-trained word vectors: 1 http://www.fit.vutbr.cz/ imikolov/rnnlm/word-"
W16-5311,D15-1062,0,0.0222394,"e Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2015) and (Xu et al., 2015). 3 Data Description 3.1 Shared Task Data The shared task organizers provide a training set of 3,054 word pairs for 318 target words. In Task-1, we are given a pair of words and we need to determine if the words are semantically related or not. Some examples of Task-1 are shown in 1. In Task-2 participants are required to detect the type of the relationship: HYPER, PART_OF, SYN, ANT, or RANDOM. 3.2 Pre-Trained Word Vectors In our experiments we experimented with three large-scale, publicly available pre-trained word vectors: 1 http://www.fit.vutbr.cz/ imikolov/rnnlm/word-test.v1.txt 87 Word 1"
W16-5311,C14-1220,0,0.030602,"yntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2015) and (Xu et al., 2015). 3 Data Description 3.1 Shared Task Data The shared task organizers provide a training set of 3,054 word pairs for 318 target words. In Task-1, we are given a pair of words and we need to determine if the words are semantically related or not. Some examples of Task-1 are shown in 1. In Task-2 participants are required to detect"
W16-5311,P16-1158,0,\N,Missing
W16-5806,P13-2037,0,0.158762,"Missing"
W16-5806,K15-1005,0,0.0606159,"Missing"
W16-5806,W14-3902,0,0.11852,"nguage is sometimes referred to as the ‘host language’, and the embedded language as the ‘guest language’ (Yeh et al., 2013). Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limite"
W16-5806,W14-5152,0,0.0188575,"lmeyer Mohammed Attia Dept. of Computational Linguistics Dept. of Computer Science Google Inc. University of Houston Heinrich Heine University, New York City Houston, TX, 77004 Düsseldorf, Germany NY, 10011 solorio@cs.uh.edu kallmeyer@phil.hhu.de attia@google.com Abstract tweets, SMS messages, user comments on the articles, blogs, etc., this phenomenon is becoming more pervasive. Code-switching does not only occur across sentences (inter-sentential) but also within the same sentence (intra-sentential), adding a substantial complexity dimension to the automatic processing of natural languages (Das and Gambäck, 2014). This phenomenon is particularly dominant in multilingual societies (Milroy and Muysken, 1995), migrant communities (Papalexakis et al., 2014), and in other environments due to social changes through education and globalization (Milroy and Muysken, 1995). There are also some social, pragmatic and linguistic motivations for code-switching, such as the the intent to express group solidarity, establish authority (Chang and Lin, 2014), lend credibility, or make up for lexical gaps. This paper describes the HHU-UH-G system submitted to the EMNLP 2016 Second Workshop on Computational Approaches to"
W16-5806,W15-3904,0,0.0617735,"Missing"
W16-5806,elfardy-diab-2012-simplified,0,0.0155933,"ter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect. With the massive increase in code-switched writings in user-generated content, it has become imperative to develop tools and methods to handle and process this type of data."
W16-5806,C82-1023,0,0.183927,"Missing"
W16-5806,P16-1101,0,0.00704538,"at of Chang and Lin (2014) in that we use RNNs and word embeddings. The difference is that we use long-shortterm memory (LSTM) with the added advantage of the memory cells that efficiently capture longdistance dependencies. We also combine wordlevel with character-level representation to obtain morphology-like information on words. 3 Model In this section, we will provide a brief description of LSTM, and introduce the different components of our code-switching detection model. The architecture of our system, shown in Figure 1, bears resemblance to the models introduced by Huang et al. (2015), Ma and Hovy (2016), and Collobert et al. (2011). 3.1 Long Short-term Memory A recurrent neural network (RNN) belongs to a family of neural networks suited for modeling sequential data. Given an input sequence x = (x1 , ..., xn ), an RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n: Figure 1: System Architecture. where ht is the hidden states vector, W denotes weight matrix, b denotes bias vector and f is the activation function of the hidden layer. Theoretically RNN can learn long distance dependencies, still in practice they fail due the vanishing/exploding"
W16-5806,W15-1608,1,0.900866,"Missing"
W16-5806,N13-1039,0,0.0355151,"Missing"
W16-5806,W14-3905,0,0.049232,"Missing"
W16-5806,P08-2030,0,0.0602839,"Missing"
W16-5806,L16-1658,1,0.835896,"ges, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect. With the massive increase in code-switched writings in user-generated content, it has become imperative to develop tools and methods to handle and process this type of data. Identification of languages used in the sentence is the first step in doing any kind of text analysis. For example, most data found in social media produced by bilingual people is a mixture of two languages. In order to process or translate this data t"
W16-5806,D08-1102,1,0.820571,"rmance. 1 Introduction Code-switching can be defined as the act of alternating between elements of two or more languages or language varieties within the same utterance. The main language is sometimes referred to as the ‘host language’, and the embedded language as the ‘guest language’ (Yeh et al., 2013). Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approach"
W16-5806,W14-3903,0,0.0180518,"switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect."
W16-5806,W14-3904,0,0.050393,"Missing"
W16-5806,L16-1667,0,0.022612,"ween elements of two or more languages or language varieties within the same utterance. The main language is sometimes referred to as the ‘host language’, and the embedded language as the ‘guest language’ (Yeh et al., 2013). Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Asso"
W16-5808,W15-3206,0,0.105,"ut requiring a local installation of software; • On server side, there should be safe storage; furthermore, the administration overhead should be kept minimal and there should only be minimal software requirements for the server side. 1 http://getbootstrap.com 65 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 65–70, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics Even though several annotation interfaces for similar tasks have been presented, such as COLANN (Benajiba and Diab, 2010), COLABA (Diab et al., 2010), and DIWAN (Al-Shargi and Rambow, 2015), they were either not available or did not match our needs. We therefore built SAWT. SAWT has been successfully used to create a code-switched corpus of 223k tokens with three annotators (Samih and Maier, 2016). It is currently used for Part-of-Speech annotation of Moroccan Arabic dialect data. The remainder of the article is structured as follows. In section 2 we present the different aspects of SWAT, namely, its data storage model, its server side structure and its client side structure. In section 3, we review related work, and in section 4, we conclude the article. 2 SAWT SAWT is a web ap"
W16-5808,P02-1022,0,0.101638,"ws for a very high speed, since only one click per token is required. 4 Figure 1: Screenshot of SAWT: Annotation on Android device shows a screenshot of code-switching annotation done in the context of our earlier work (Samih and Maier, 2016). Finally, figure 1 shows a screenshot of the POS annotation interface used on the Asus Nexus 7 2013 tablet running Google Chrome on Android 6. 3 Related Work As mentioned above, we are not aware of a software which would have fulfilled our needs exactly. Previously released annotation software can be grouped into several categories. Systems such as GATE (Cunningham et al., 2002), CLaRK (Simov et al., 2003) and MMAX2 (M¨uller and Strube, 2006) are desktop-based software. They offer a large range of functions, and are in general oriented towards more complex annotation tasks, such as syntactic treebank annotation. In the context of Arabic dialect annotation, several systems have been created. COLANN GUI (Benajiba and Diab, 2010), which unfortunately was not available to us, is a web application that specialized on dialect annotation. DIWAN (Al-Shargi and Rambow, 2015) is a desktop application for dialect annotation which can be used online. The systems that came closes"
W16-5808,pasha-etal-2014-madamira,0,0.045052,"Missing"
W16-5808,petrov-etal-2012-universal,0,0.149276,"model runs on the web server. If suggestions are desired, then in the configuration file, the corresponding path and parameters for the software must be given. If the parameter is left blank, no suggestions are shown. • Search box activation: A boolean parameter indicating if a search box is desired. In the search box, the annotator can look up his previous annotations for a certain token. • Utility links: The top border of the user interface consists of a link bar, the links for which can be freely configured. In our project, e.g., they are used for linking to the list of Universal POS tags (Petrov et al., 2012), to a list of Arabic function words, to an Arabic Morphological Analyzer (MADAMIRA) (Pasha et al., 2014), and to an Arabic screen keyboard, as can be seen in figures 2 and 3. Once the configuration script has been run, the web application code must be copied to a suitable place within a web server installation. In order to upload a text which is to be annotated by a certain user, the second script must be used. It takes the following command line parameters. • Input data: The name of the file containing the data to be annotated. The text must be pretokenized (one space between each token), an"
W16-5808,L16-1658,1,0.930223,"it is a common phenomenon. Both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) variants co-exist, MSA and DA being used for formal and informal communication, respectively (Ferguson, 1959). Particularly recently, the computational treatment of code-switching has received attention (Solorio et al., 2014). Within a project concerned with the processing of code-switched data of an under-resourced Arabic dialect, Moroccan Darija, a large code-switched corpus had to be annotated token-wise with the extended label set from the EMNLP 2014 Shared Task on Code-Switching (Solorio et al., 2014; Samih and Maier, 2016). The label set contains three labels that mark MSA and DA tokens, as well as tokens in another language (English, French, Spanish, Berber). Furthermore, it contains labels for tokens which mix two languages (e.g., for French words to • It should not be bound to a particular label set, since within the project, not only codeswitching annotation, but also the annotation of Part-of-Speech was envisaged; • It should allow for post-editing of tokenization during the annotation; • It should be web-based, due to the annotators being at different physical locations; • On client side, it should be pla"
W16-5808,W14-3907,0,0.0135575,"ided, furthermore it should be as simple as possible to use for the annotators, allowing for a high annotation speed; Introduction Code-switching (Bullock and Toribio, 2009) occurs when speakers switch between different languages or language variants within the same context. In the Arab world, for instance, it is a common phenomenon. Both Modern Standard Arabic (MSA) and Dialectal Arabic (DA) variants co-exist, MSA and DA being used for formal and informal communication, respectively (Ferguson, 1959). Particularly recently, the computational treatment of code-switching has received attention (Solorio et al., 2014). Within a project concerned with the processing of code-switched data of an under-resourced Arabic dialect, Moroccan Darija, a large code-switched corpus had to be annotated token-wise with the extended label set from the EMNLP 2014 Shared Task on Code-Switching (Solorio et al., 2014; Samih and Maier, 2016). The label set contains three labels that mark MSA and DA tokens, as well as tokens in another language (English, French, Spanish, Berber). Furthermore, it contains labels for tokens which mix two languages (e.g., for French words to • It should not be bound to a particular label set, sinc"
W16-5808,E12-2021,0,0.124532,"Missing"
W16-5808,P13-4001,0,0.27822,"Missing"
W17-1306,N16-3003,1,0.794973,"or testing, 75 for development and the remaining 200 for training. The concept We followed in LSTM sequence labeling is that segmentation is one-to-one mapping at the character level where each character is annotated as either beginning a segment (B), continues a previous segment (M), ends a segment (E), or is a segment by itself (S). After the labeling is complete we merge the characters and labels  together, for example @ñËñ®J K. byqwlwA is labeled as “SBMMEBE”, which means that the word is segmented as b+yqwl+wA. We compar results of our two LSTM models (BiLSTM and BiLSTMCRF) with Farasa (Abdelali et al., 2016), an open source segementer for MSA3 , and MADAMIRA for Egyptian dialect. Table 3 shows accuracy for Farasa, MADAMIRA, and both of our models. • ËQK @ñË@ AlwAyrls “the  AlHA$tAj “the hashtag”. h. AJAêË@ AlgTY “the Spelling variation: e.g. ù¢ªË@ cover”, úÎë B l&gt;hly “to Ahly”. wireless”, • Morphological inflection (imperative): e.g.  ¯ fwqwA “wake up”. ø Y $dy “pull”, @ñ¯ñ • Segmentation ambiguity: e.g. éJ Ë lyh meaning mAlnA meaning “our “why” or “to him”, AJËAÓ money” or “what we have”. • Combinations not known to MADAMIRA:  ® ® JÓ mtqflwhA$ “don’t close it”, e.g. AëñÊ @ &gt;wSflkwA “I"
W17-1306,W09-0807,0,0.049808,"instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he"
W17-1306,bouamor-etal-2014-multidialectal,0,0.121902,"tching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of “mA byjy lhA$”. • Some affixes are altered in form from their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emp"
W17-1306,D14-1154,1,0.858589,"cters embedding and stacks them to build a matrix. This latter is then used as the input to the Bi-directional LSTM. On the last layer, an affine transformation function followed by a CRF computes the probability distribution over all labels Early Stopping We also employ early stopping (Caruana et al., 2000; Graves et al., 2013b) to mitigate overfitting by monitoring the model’s performance on development set. 4 ary respectively. The architecture of our segmentation model, shown in Figure 2, is straightforward. It comprises the following three layers: Dataset We used the dataset described in (Darwish et al., 2014). The data was used in a dialect identification task to distinguish between dialectal Egyptian and MSA. It contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs. We manually annotated each word in this corpus to provide: CODA-compliant writing (Habash et al., 2012), segmentation, stem, lemma, and POS, also the corr"
W17-1306,W15-3904,0,0.0407118,"Missing"
W17-1306,W16-4828,1,0.865893,"is paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segment"
W17-1306,N16-1030,0,0.164915,"− x + W← −← − h t−1 + b← −) ht = σ(Wx← h t h h h → − ← − → h + W← − h + by yt = W− hy t hy t set of labels. In our case S ={B, M, E, S, WB}, where B is the beginning of a token, M is the middle of a token, E is the end of a token, S is a single character token, and W B is the word boundary. w ~ is the weight vector for weighting the feature vec~ Training and decoding are performed by the tor Φ. Viterbi algorithm. Note that replacing the softmax with CRF at the output layer in neural networks has proved to be very fruitful in many sequence labeling tasks (Ma and Hovy, 2016; Huang et al., 2015; Lample et al., 2016; Samih et al., 2016) More interpretations about these formulas are found in Graves et al. (2013a). A very important element of the recent success of many NLP applications, is the use of characterlevel representations in deep neural networks. This has shown to be effective for numerous NLP tasks (Collobert et al., 2011; dos Santos et al., 2015) as it can capture word morphology and reduce out-of-vocabulary. This approach has also been especially useful for handling languages with rich morphology and large character sets (Kim et al., 2016). We use pre-trained character embeddings to initialize"
W17-1306,P16-1101,0,0.269252,"(ct ) where σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Lipton et al., 2015). Figure 1 illustrates a single LSTM memory cell (Graves and Schmidhuber, 2005) Arabic Segmentation Model In this section, we will provide a brief description of LSTM, and introduce the different components of our Arabic segmentation model. For all our work, we used the Keras toolkit (Chollet, 2015). The architecture of our model, shown in Figure 2 is similar to Ma and Hovy (2016), Huang et al. (2015), and Collobert et al. (2011) 3.1 Long Short-term Memory A recurrent neural network (RNN) belongs to a family of neural networks suited for modeling sequential data. Given an input sequence x = (x1 , ..., xn ), an RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n: 1 Figure 1: A Long Short-Term Memory Cell. 3.2 Bi-directional LSTM Bi-LSTM networks (Schuster and Paliwal, 1997) are extensions to the single LSTM networks. They MADAMIRA release 20160516 2.1 48 are capable of learning long-term dependencies and maintain contex"
W17-1306,maamouri-etal-2014-developing,0,0.0957441,"Missing"
W17-1306,habash-etal-2012-conventional,0,0.430983,"their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emphasis, e.g. úÍððððððñ«X@ AdEwwwwwwwliy “pray for me”. tAtuw “tattoo”, or coinage, such as the negative particles Ég. @P rAjil “man” Ég. P rajul, and vowel shortening, such as AÖß X dayomA “always” from AÖß @X dAyomA. from • Lack of standard orthography. Many of the words in DA do not follow a standard orthographic system (Habash et al., 2012). “cafe” and H t or  s as in Q J» kvyr Õç' tm → ñK tw. • Some morphological patterns that do not exist in MSA, such as the passive pattern AitofaEal, such as QåºK@ Aitokasar “it broke”. 47 For segmentation, Yao and Huang (2016) successfully used a bi-directional LSTM model for segmenting Chinese text. In this paper, we build on their work and extend it in two ways, namely combining bi-LSTM with CRF and applying on Arabic, which is an alphabetic language. Mohamed et al. (2012) built a segmenter based on memory-based learning. The segmenter has been trained on a small corpus of Egyptian"
W17-1306,mohamed-etal-2012-annotating,0,0.411081,"Missing"
W17-1306,N13-1044,0,0.282692,"neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction"
W17-1306,P14-2034,0,0.310795,"Missing"
W17-1306,pasha-etal-2014-madamira,0,0.190181,"Missing"
W17-1306,P13-2001,1,0.858088,"tures or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object pronoun “hA”, and the post"
W17-1306,W16-5806,1,0.909334,"under lenition, softening of a consonant, or fortition, hardening of a consonant. • Vowel elongation, such as • The use of masculine plural or singular noun forms instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple"
W17-1306,P16-1162,0,0.049716,"reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object prono"
W17-1306,J14-1006,0,0.025968,"ing some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of"
W17-1306,N12-1006,0,0.0419555,"inine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her),"
W17-6203,W08-2313,0,0.0173945,"g of two trees: one tree anchored by the depitive word, namely roughly the auxiliary tree that was used above in the presented LTAG analyses, and one degenerated elementary tree, the scope taking part. The degenerated elementary tree possibly only consists of a single node and is supposed to either adjoin to the root node of the target NP or to substitute into the target NP slot of the verbal tree, yielding again a substitution node. Similar tree sets can be found in works that deal, for example, with reflexives (Ryant and Scheffler, 2006; Kallmeyer and Romero, 2007; Storoshenko et al., 2008; Frank, 2008) and extraposed relative clauses (Kroch and Joshi, 1987; Chen-Main and Joshi, 2014). The approach in Frank (2008) adopts a tree-local solution where reflexives introduce on the syntactic side, in addition to the initial tree of the reflexive, an initial single-node NP tree that substitutes into the antecedent slot. A similar solution might be possible for depicitves. The approach in Chen-Main and Joshi (2014) is interesting because it remains tree-local even for illnested dependency structures, thanks to flexible composition and multiple adjunction. As was shown with (4-b), depictives can give"
W17-6203,W16-3305,1,0.850617,"gets for depictive modification. The question is whether we can generalize this in some way. One abstraction over semantic roles is provided by the MACROROLES actor and undergoer introduced in Van Valin, Jr. (2005) (see also the similar concepts of proto-agent and proto-patient in Dowty, 1991). Van Valin, Jr. (2005) explains how to determine actor and undergoer based on the semantic characterization of an event, more specifically based on the semantic roles of its participants. A constraint-based LTAG implementation of his linking theory within the metagrammar (using XMG) has been proposed in Kallmeyer et al. (2016). With the additional linking constraints in the metagrammar, the frame for eating for instance gets enriched with marcorole information, as shown in Figure 5. eating  AGENT   0  ACTOR  THEME  AdjP Figure 6: Revised elementary tree for raw A semantic approach with actor-undergoer linking  6] UNDERGOER 2 Figure 5: eating frame with macroroles The hypothesis we want to pursue in the following is that depictives target either the actor or the undergoer of the event that they modify (see Figure 6 for the revised elementary tree for raw). This hypothesis is not just a generalization over 2"
W17-6203,J13-3005,0,0.0220401,"Missing"
W17-6203,J01-1004,0,0.181252,"Missing"
W17-6203,W06-1509,0,0.0241521,"s target. Within an MCTAG, depictives could be represented as tree sets consisting of two trees: one tree anchored by the depitive word, namely roughly the auxiliary tree that was used above in the presented LTAG analyses, and one degenerated elementary tree, the scope taking part. The degenerated elementary tree possibly only consists of a single node and is supposed to either adjoin to the root node of the target NP or to substitute into the target NP slot of the verbal tree, yielding again a substitution node. Similar tree sets can be found in works that deal, for example, with reflexives (Ryant and Scheffler, 2006; Kallmeyer and Romero, 2007; Storoshenko et al., 2008; Frank, 2008) and extraposed relative clauses (Kroch and Joshi, 1987; Chen-Main and Joshi, 2014). The approach in Frank (2008) adopts a tree-local solution where reflexives introduce on the syntactic side, in addition to the initial tree of the reflexive, an initial single-node NP tree that substitutes into the antecedent slot. A similar solution might be possible for depicitves. The approach in Chen-Main and Joshi (2014) is interesting because it remains tree-local even for illnested dependency structures, thanks to flexible composition a"
W17-6203,J94-1004,0,0.075246,"Missing"
W17-6203,W08-2320,0,0.0227154,"ted as tree sets consisting of two trees: one tree anchored by the depitive word, namely roughly the auxiliary tree that was used above in the presented LTAG analyses, and one degenerated elementary tree, the scope taking part. The degenerated elementary tree possibly only consists of a single node and is supposed to either adjoin to the root node of the target NP or to substitute into the target NP slot of the verbal tree, yielding again a substitution node. Similar tree sets can be found in works that deal, for example, with reflexives (Ryant and Scheffler, 2006; Kallmeyer and Romero, 2007; Storoshenko et al., 2008; Frank, 2008) and extraposed relative clauses (Kroch and Joshi, 1987; Chen-Main and Joshi, 2014). The approach in Frank (2008) adopts a tree-local solution where reflexives introduce on the syntactic side, in addition to the initial tree of the reflexive, an initial single-node NP tree that substitutes into the antecedent slot. A similar solution might be possible for depicitves. The approach in Chen-Main and Joshi (2014) is interesting because it remains tree-local even for illnested dependency structures, thanks to flexible composition and multiple adjunction. As was shown with (4-b), depic"
W17-6207,J13-3005,0,0.0547571,"Missing"
W17-6207,N10-1035,0,0.0209964,"expressions. Section 3 presents the core idea of this paper which is to add feature structures to edges (as well as to nodes) for bookkeeping purposes. The approach is then applied in Section 4 for the seamless alignment of the operator projection of RRG with the constituent projection. Section 5 shows how the proposed representation of the operator projection behaves in the case of complex sentences, in which the structure of the clausal skeleton interacts with the scope62 well-nested Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987; Seki et al., 1991; Kanazawa, 2009; Gómez-Rodríguez et al., 2010) of fan-out k + 1 (see Kallmeyer, 2016, for more details on this equivalence). (1) What does John think Bill smashed? CL CO CL TNS PrCS CL RP CO RP CL RP NUC 2.2 V Sister adjunction for modification So-called peripheral elements in RRG are added via sister adjunction. An example is the modifier quickly in the example in (2). The corresponding derivation is given in Fig. 4. The root of the modifier tree merges with the target node n of the adjunction and the (necessarily unique) daughter is inserted as a new daughter of n. The categories of the root and n have to be the same. does John think NU"
W17-6207,P87-1015,0,0.78701,"ong distance dependencies, and the flat adjunction of modifier expressions. Section 3 presents the core idea of this paper which is to add feature structures to edges (as well as to nodes) for bookkeeping purposes. The approach is then applied in Section 4 for the seamless alignment of the operator projection of RRG with the constituent projection. Section 5 shows how the proposed representation of the operator projection behaves in the case of complex sentences, in which the structure of the clausal skeleton interacts with the scope62 well-nested Linear Context-Free Rewriting System (LCFRS) (Vijay-Shanker et al., 1987; Seki et al., 1991; Kanazawa, 2009; Gómez-Rodríguez et al., 2010) of fan-out k + 1 (see Kallmeyer, 2016, for more details on this equivalence). (1) What does John think Bill smashed? CL CO CL TNS PrCS CL RP CO RP CL RP NUC 2.2 V Sister adjunction for modification So-called peripheral elements in RRG are added via sister adjunction. An example is the modifier quickly in the example in (2). The corresponding derivation is given in Fig. 4. The root of the modifier tree merges with the target node n of the adjunction and the (necessarily unique) daughter is inserted as a new daughter of n. The ca"
W17-6207,P95-1021,0,0.359822,"titution (2) Mary quickly entered the room CL PrCS RP TNS CO CL RP NUC CO V RP CL CO RP NUC NUC CO V Mary what does John think Bill smashed Figure 3: Derived tree for (1) ADV quickly RP V entered the room ; CL Note that in a first proposal of how to apply wrapping substitution to tree composition in RRG, Kallmeyer et al. (2013) assumed a more binary structure. The version sketched above goes back to Osswald and Kallmeyer (in press) and is more in line with the flat structures used in RRG. The idea of using wrapping substitution is partly inspired by the operations subsertion in DTree Grammar (Rambow et al., 1995) and generalized substitution in D-Tree Substitution Grammar (Rambow et al., 2001), which, however, are more general. Wrapping substitution shares with subsertion the non-locality: the two nodes targeted by the wrapping substitution (i.e., the substitution node and the root node of the target tree) need not come from the same elementary tree and can be far apart from each other. If the number of wrapping substitutions that stretch across a node in the derived tree is limited by some constant k, it can be shown that an equivalent simple Context-Free Tree Grammar (CFTG) (Kanazawa, 2016) of rank"
W17-6207,C88-2147,0,\N,Missing
W17-6914,E03-1030,1,0.58237,"oot of the adjoining tree unifies with the bottom of the adjunction site. Furthermore, in the final derived tree, top and bottom unify in all nodes. In line with previous proposals of how to add semantics to LTAG, we pair each elementary tree with a semantic representation that consists of a set of HL formulas which can contain holes and which can be labeled. In other words, hole semantics (Bos, 1995) is applied to HL and these underspecified formulas are linked to the elementary trees. Composition is triggered by the syntactic unifications using interface features on the syntactic trees (cf. Gardent and Kallmeyer 2003; Kallmeyer and Romero 2008). As an example consider the derivation in Fig. 2 where the two NPs are substituted into the two argument slots A S l1 : Eeating ∧ hAGENTi 1 VP ∧ hTHEMEi 2 NP[I= 1 , MINS=l1 ] NP[I=i] @i (person ∧hNAMEiJohn) ‘John’ NP[I=x, MINS= 4 ] V NP[I= 2 , MINS=l1 ] ‘ate’ ‘pizza’ E(↓x.pizza ∧ 3 ), 3 ∗ 4 Figure 2: Derivation of ‘John ate pizza’ in the ‘ate’ tree. The interface features I on the NP nodes make sure that the contributions of the two arguments feed into the AGENT and THEME nodes of the frame structure. Furthermore, an interface feature MINS is used for providing th"
W17-6914,C88-2147,0,0.59854,"and Rambow 2000) consists of a finite set of elementary trees. Larger trees are derived via substitution (replacing a leaf with a tree) and adjunction (replacing an internal node with a tree). An adjoining tree has a unique foot node (marked with an asterisk), which is a non-terminal leaf labeled with the same category as the root of the tree. When adjoining such a tree to some node n of another tree, in the resulting tree, the subtree with root n from the original tree is attached at the foot node of the adjoining tree. Non-terminal nodes in LTAG are usually enriched with feature structures (Vijay-Shanker and Joshi, 1988): Each node has a top and a bottom feature structure (except substitution nodes, which have only a top). Features can be shared within elementary trees. In a substitution step, the top of the root of the new tree unifies with the top of the substitution node; in an adjunction, the top of the root of the adjoining tree unifies with the top of the adjunction site and the bottom of the foot of the adjoining tree unifies with the bottom of the adjunction site. Furthermore, in the final derived tree, top and bottom unify in all nodes. In line with previous proposals of how to add semantics to LTAG,"
W18-4930,D14-1082,0,0.0398146,"st and reliable choice (Fan et al., 2008) but still overfitting and poor generalization is possible. Given the availability of GPU accelerated computing and in light of the recent advances in deep learning, we replace a classic classifier such as linear SVM—as used by Saied et al. (2017)—or a Perceptron classifier—as used by Constant and Nivre (2016)—with a simple multi-layer CNN. Usually, when using a deep neural network, the above-stated feature extraction process is altered by using embeddings and particularly pre-trained word embeddings instead of binary-encoded lexical features—e.g., see Chen and Manning (2014). Since the use of pre-trained word embeddings is not an option for systems that participate in the closed track of the shared task2 , we use a data-independent dimensionality reduction method based on random projections. The usage of this dimension reduction method removes engineering issues associated with the use of high-dimensional sparse vectors as input for deep learning methods, e.g., by reducing the memory required to fit the batch of vectors used during training and by decreasing the training time (specially for large corpora). Our dimension reduction method is based on the positive-o"
W18-4930,P16-1016,0,0.342485,"ication in a highly multilingual context. To foster this initiative, the 2018 shared task provides test and training corpora, annotated with VMWEs in 19 different languages1 , and a framework to evaluate supervised methods for identifying VMWEs (Ramisch et al., 2018). In this paper, we present T RA PACC and its variant T RA PACC S that address the VMWE identification task using a neural transition system. The general idea behind T RA PACC has been motivated by observations from the first edition of the shared task: Saied et al. (2017) showed that the modified arc-standard transition method by Constant and Nivre (2016) (hereinafter MAST) can be employed to outperform other systems in which VMWE identification is modeled as a sequence labeling task disregarding of their employed learning model, e.g., conditional random fields (CRFs) (Maldonado et al., 2017), and bidirectional recurrent neural networks (Klyueva et al., 2017). Accordingly, we adapt MAST for the purpose of the PARSEME shared task; but, in contrast to Saied et al. (2017), instead of using a linear SVM for learning and predicting transitions, we use a CNN preceded by a dimension reduction proposed in (QasemiZadeh and Kallmeyer, 2016; QasemiZadeh"
W18-4930,W17-1707,0,0.0348023,"ariant T RA PACC S that address the VMWE identification task using a neural transition system. The general idea behind T RA PACC has been motivated by observations from the first edition of the shared task: Saied et al. (2017) showed that the modified arc-standard transition method by Constant and Nivre (2016) (hereinafter MAST) can be employed to outperform other systems in which VMWE identification is modeled as a sequence labeling task disregarding of their employed learning model, e.g., conditional random fields (CRFs) (Maldonado et al., 2017), and bidirectional recurrent neural networks (Klyueva et al., 2017). Accordingly, we adapt MAST for the purpose of the PARSEME shared task; but, in contrast to Saied et al. (2017), instead of using a linear SVM for learning and predicting transitions, we use a CNN preceded by a dimension reduction proposed in (QasemiZadeh and Kallmeyer, 2016; QasemiZadeh et al., 2017). Furthermore, T RA PACC S extends the T RA PACC system by replacing the output softmax layer of the CNN with a kernel SVM. The latter is motivated by research, such as Razavian et al. (2014) and Poria et al. (2015), that suggests using CNN combined with a more elaborate classifier (i.e., to use"
W18-4930,W17-1715,0,0.030686,"(Ramisch et al., 2018). In this paper, we present T RA PACC and its variant T RA PACC S that address the VMWE identification task using a neural transition system. The general idea behind T RA PACC has been motivated by observations from the first edition of the shared task: Saied et al. (2017) showed that the modified arc-standard transition method by Constant and Nivre (2016) (hereinafter MAST) can be employed to outperform other systems in which VMWE identification is modeled as a sequence labeling task disregarding of their employed learning model, e.g., conditional random fields (CRFs) (Maldonado et al., 2017), and bidirectional recurrent neural networks (Klyueva et al., 2017). Accordingly, we adapt MAST for the purpose of the PARSEME shared task; but, in contrast to Saied et al. (2017), instead of using a linear SVM for learning and predicting transitions, we use a CNN preceded by a dimension reduction proposed in (QasemiZadeh and Kallmeyer, 2016; QasemiZadeh et al., 2017). Furthermore, T RA PACC S extends the T RA PACC system by replacing the output softmax layer of the CNN with a kernel SVM. The latter is motivated by research, such as Razavian et al. (2014) and Poria et al. (2015), that suggest"
W18-4930,W04-0308,0,0.0300878,"nt and Nivre (2016). MAST jointly predicts syntactic dependencies and MWEs in a sentence by 1 Besides Arabic. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/ License details: 268 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 268–274 Santa Fe, New Mexico, USA, August 25-26, 2018. introducing a new lexical stack (and, accordingly a number of transitions specific to this lexical stack) to the arc-standard incremental dependency parsing (Nivre, 2004; Kubler et al., 2009). Since we are only concerned with the identification of MWEs, the quintuple model of MAST is reduced to a triple consisting of a) a buffer B of input tokens to be parsed, b) a stack S of lexical units that are partially processed, and c) a set of processed lexical units P . In effect, the identification of VMWEs becomes very simple with only four general types of transitions (apart from initialize and terminate): • Shift: pushes the head of the buffer B to the stack S; • Merge: reduces the top two elements of the stack S to one element in the stack; • Reduce: is a pop op"
W18-4930,D15-1303,0,0.0320776,"s (CRFs) (Maldonado et al., 2017), and bidirectional recurrent neural networks (Klyueva et al., 2017). Accordingly, we adapt MAST for the purpose of the PARSEME shared task; but, in contrast to Saied et al. (2017), instead of using a linear SVM for learning and predicting transitions, we use a CNN preceded by a dimension reduction proposed in (QasemiZadeh and Kallmeyer, 2016; QasemiZadeh et al., 2017). Furthermore, T RA PACC S extends the T RA PACC system by replacing the output softmax layer of the CNN with a kernel SVM. The latter is motivated by research, such as Razavian et al. (2014) and Poria et al. (2015), that suggests using CNN combined with a more elaborate classifier (i.e., to use CNN only as a feature selection method) is likely to improve prediction performance. The remainder of this paper is structured as follows: We describe our method in Section 2, we report results from the experiment and discuss them in Section 3. Section 4 concludes this paper. 2 Method and System Description The backbone of T RA PACC (and subsequently T RA PACC S ) is the idea behind the MAST proposed by Constant and Nivre (2016). MAST jointly predicts syntactic dependencies and MWEs in a sentence by 1 Besides Ara"
W18-4930,S16-2024,1,0.865433,"ransition method by Constant and Nivre (2016) (hereinafter MAST) can be employed to outperform other systems in which VMWE identification is modeled as a sequence labeling task disregarding of their employed learning model, e.g., conditional random fields (CRFs) (Maldonado et al., 2017), and bidirectional recurrent neural networks (Klyueva et al., 2017). Accordingly, we adapt MAST for the purpose of the PARSEME shared task; but, in contrast to Saied et al. (2017), instead of using a linear SVM for learning and predicting transitions, we use a CNN preceded by a dimension reduction proposed in (QasemiZadeh and Kallmeyer, 2016; QasemiZadeh et al., 2017). Furthermore, T RA PACC S extends the T RA PACC system by replacing the output softmax layer of the CNN with a kernel SVM. The latter is motivated by research, such as Razavian et al. (2014) and Poria et al. (2015), that suggests using CNN combined with a more elaborate classifier (i.e., to use CNN only as a feature selection method) is likely to improve prediction performance. The remainder of this paper is structured as follows: We describe our method in Section 2, we report results from the experiment and discuss them in Section 3. Section 4 concludes this paper."
W18-4930,S17-2039,1,0.854121,"Missing"
W18-4930,W17-1717,0,0.267513,", 2017) is an initiative aiming at improving automatic methods for VMWE identification in a highly multilingual context. To foster this initiative, the 2018 shared task provides test and training corpora, annotated with VMWEs in 19 different languages1 , and a framework to evaluate supervised methods for identifying VMWEs (Ramisch et al., 2018). In this paper, we present T RA PACC and its variant T RA PACC S that address the VMWE identification task using a neural transition system. The general idea behind T RA PACC has been motivated by observations from the first edition of the shared task: Saied et al. (2017) showed that the modified arc-standard transition method by Constant and Nivre (2016) (hereinafter MAST) can be employed to outperform other systems in which VMWE identification is modeled as a sequence labeling task disregarding of their employed learning model, e.g., conditional random fields (CRFs) (Maldonado et al., 2017), and bidirectional recurrent neural networks (Klyueva et al., 2017). Accordingly, we adapt MAST for the purpose of the PARSEME shared task; but, in contrast to Saied et al. (2017), instead of using a linear SVM for learning and predicting transitions, we use a CNN precede"
W19-0407,2017.jeptalnrecital-court.12,1,0.730096,"ctions described using logic and constraints, is compiled with XMG-2 to obtain the non-factorized grammar. Such a grammar can be used for parsing, thereby allowing to check that automatic semantic analyses are consistent with the ones that we presented. To do so, we used the parser TuLiPA for LTAG and semantic frames (Arps 6 We ignore here the fact that Fertigstellung is derived from the verb fertigstellen via -ung nominalization and that this also should be modeled in a principled way within the syntax-semantics interface. For possible analyses of such phenomena using frames see for instance Andreou and Petitjean (2017). 7 Our implementation and the instructions to experiment with it are available online: https://github.com/spetitjean/XMG-2/tree/master/MetaGrammars/synframe/LVC. and Petitjean, 2018), giving as parameters the toy grammar and its type hierarchy. TuLiPA was able to compute the expected derivations for all the examples. 5 Conclusion and future work In this paper, we develop a compositional analysis of the semantics of German LVCs involving the posture verb stehen (‘stand’) and a vor-PP. The chosen framework combines LTAG with frames, which comes with constructionist elements (LTAG) and with the"
W19-0407,L18-1351,1,0.881501,"Missing"
W19-0407,W14-5816,0,0.0169946,"] P "" 7 Peter NP[I= 8 ] # person NAME NP[I= 5 ] Peter 8 vor h i house dem Haus Figure 2: LTAG-frame derivation for (3), leading to the frame from Fig. 1 Generalizations over elementary tree frame pairs and constraints over frame types are captured in the so-called metagrammar in form of a principled and factorized description of syntactic and semantic building blocks. We will use this in the next sections when developing our analysis of LVCs with stehen. There has been only very little work on computational implementations of LVCs so far that take both syntax and semantics into consideration. Vaidya et al. (2014) propose an LTAG analysis for certain LVCs in Hindi but, in contrast to our paper, do not deal with semantics. Their syntactic analysis is such that the light verb adjoins into the noun, i.e., the noun spans the entire subcategorization frame. This requires separate and largely unrelated analyses for the literal uses and the light verb uses of verbs such as stehen. Our analysis is more factorized and thereby more unified, and it establishes links between the subject NP and semantic arguments of the embedded event NP via appropriate frame unifications. 4 The analysis In the following, we develo"
W19-0407,C88-2147,0,0.787207,"; Abeill´e and Rambow, 2000). A LTAG consists of a finite set of elementary trees. Larger trees can be derived via the composition operations substitution (replacing a leaf with a new tree) and adjunction (replacing an internal node with a new tree). An adjoining tree has a unique non-terminal leaf that is its foot node (marked with an asterisk). When adjoining such a tree to some node v, in the resulting tree, the subtree with root v from the old tree ends up below the foot node. In order to capture syntactic generalizations, the non-terminal node labels are enriched with feature structures (Vijay-Shanker and Joshi, 1988). Each node has a top and a bottom feature structure (except substitution nodes, which have only a top). Nodes in the same elementary tree can share features. Substitutions and adjunctions trigger unifications: In a substitution step, the top of the root of the new tree unifies with the top of the substitution node. In an adjunction step, the top of the root of the adjoining tree unifies with the top of the adjunction site and the bottom of the foot of the adjoining tree unifies with the bottom of the adjunction site. Furthermore, in the final derived tree, top and bottom must unify in all nod"
W19-5113,W16-5901,0,0.0256115,"tly using the inside-outside algorithm. Backpropagation. In order to use P (`(i, j) | w, G) as a part of the training objective, the inside-outside algorithm needs to be specified in a backpropagation-enabled way. We achieve this by using a library11 which automatically determines the way to backpropagate the gradient from the output to the input of the inside-outside algorithm. Conveniently, this requires no changes in the structure of the algorithm itself. This is similar to how the inside algorithm can be extended with its outside counterpart automatically, using automatic differentiation (Eisner, 2016). Optimization. We used stochastic gradient descent (SGD) to train the models for the individual datasets and VMWE categories. We used miniAs a benchmark, we use the system of Al Saied et al. (2018), henceforth called ATILF, a transition-based tagger relying on support vector machines and hand-crafted features for classification. The hand-crafted features are separately specified for each language. ATILF addresses several VMWE challenges at the same time – it is able to handle single-token, discontinuous, nested, and (some forms of) overlapping VMWEs. Without relying on word embeddings or any"
W19-5113,S17-1006,0,0.0229956,"s, it should be straightforward to extend it to a fully joint solution, notably due to its similarities with the graph-based, arc-factored, neural dependency parsing architecture of (Dozat and Manning, 2017). Related Work The MWE identification strategies can be broadly divided into approaches based on deep learning, sequence labelling, and parsing-based methods. Because all of these have different advantages (and are not necessarily mutually exclusive), some systems also pursue a mix of these approaches, e.g., the system proposed here uses both (graphbased) parsing and deep learning methods. Gharbieh et al. (2017) were one of the first to apply deep learning to MWE identification. They tested different network architectures, e.g., a layered feed-forward network and a recurrent neural network, and all of them outperformed more traditional MWE identification methods. The approaches based on deep learning have the advantage that they can easily leverage pre-trained word vectors as features (Constant et al., 2017; Taslimipoor and Rohanian, 2018; Ehren et al., 2018). The method described in this work also relies on pretrained word vectors. Schneider et al. (2014) addressed the task of MWE identification as"
W19-5113,C12-1015,0,0.0551172,"Missing"
W19-5113,W18-4928,0,0.066876,"Missing"
W19-5113,C14-1177,0,0.408543,"Missing"
W19-5113,J17-4005,0,0.080066,"Missing"
W19-5113,L18-1008,0,0.0318323,"for the German dataset, in which some of the dependency structures did not satisfy this property. The two previous steps are carried out automatically by our VMWE identification systems. However, we also performed one full-fledged preprocessing operation of adding the missing lemmas9 in the French test set. Even though having no impact on the results of the proposed systems, this step was necessary to obtain reliable comparison with the benchmark system (see Sec. 4.3), configured to use lemma information in case of French. of the concatenation of the corresponding (i) FastText word embedding (Mikolov et al., 2018), (ii) POS embedding, and (iii) dependency label embedding. The latter correspond to the dependency label of the arc connecting the word with its dependency head. The size of the FastText word embeddings is 300. We chose the size of 25 for both POS and dependency embeddings, which should be sufficient given the small number of values they can take. The POS and dependency label embedding vectors are both learned during training, while the FastText embeddings are kept intact. 4.4.2 In both labelling models, the size of the network’s input layer is determined by the size of the input vector repre"
W19-5113,P16-1016,0,0.0824633,"he encoding schemata and the labelling models. In Sec. 4, we summarize the experiments we performed to evaluate our approach. Finally, we conclude and mention possible future work in Sec. 5. 2 lem. They proposed a sequence labeling scheme (IiOoBb) which allows to represent discontinuous MWEs as well as nested MWEs. The encoding methods we propose also allow to handle discontinuous and, to a certain degree, nested MWEs, with the important difference that they apply to trees rather than sequences. Previous work on applying parsing-based techniques to MWE identification includes transitionbased (Constant and Nivre, 2016; Al Saied et al., 2018; Stodden et al., 2018) and graph-based (Waszczuk, 2018; Boros¸ and Burtica, 2018) approaches. As shown by the two PARSEME shared tasks (Savary et al., 2017; Ramisch et al., 2018a), both strategies can be very effective, even without relying on pre-trained word vectors. The method we propose is graph-based and it resembles the one of Waszczuk (2018) in that it relies on global modelling and restricts the labelling decisions to dependency fragments, and the one of Boros¸ and Burtica (2018) in that it relies on a neural architecture. In comparison with the former, the enco"
W19-5113,P15-1108,0,0.134581,"relies on a neural architecture. In comparison with the former, the encoding schemata we propose allow to deal with two important phenomena the method of (Waszczuk, 2018) could not handle – adjacent and disconnected MWE occurrences. Another way to classify MWE identification approaches is based on whether the process of MWE prediction takes place before, during, or after (syntactic and/or semantic) parsing. The joint solution is typically considered as most promising in that it can potentially improve both MWE identification and parsing results (Constant and Nivre, 2016; Le Roux et al., 2014; Nasr et al., 2015; Simk´o et al., 2018). On this scale, our method clearly fits into the family of post-processing approaches, since it requires the dependency trees on input. Nevertheless, it should be straightforward to extend it to a fully joint solution, notably due to its similarities with the graph-based, arc-factored, neural dependency parsing architecture of (Dozat and Manning, 2017). Related Work The MWE identification strategies can be broadly divided into approaches based on deep learning, sequence labelling, and parsing-based methods. Because all of these have different advantages (and are not nece"
W19-5113,W18-4929,1,0.891006,"Missing"
W19-5113,Q14-1016,0,0.0971593,"raphbased) parsing and deep learning methods. Gharbieh et al. (2017) were one of the first to apply deep learning to MWE identification. They tested different network architectures, e.g., a layered feed-forward network and a recurrent neural network, and all of them outperformed more traditional MWE identification methods. The approaches based on deep learning have the advantage that they can easily leverage pre-trained word vectors as features (Constant et al., 2017; Taslimipoor and Rohanian, 2018; Ehren et al., 2018). The method described in this work also relies on pretrained word vectors. Schneider et al. (2014) addressed the task of MWE identification as a sequence labeling prob3 Methods In this section we describe the methods and models used in the proposed MWE identification approach. In Sec. 3.1, we introduce some basic definitions. In Sec. 3.2, we detail the methods of encoding MWE occurrences as tree labellings. This allows to reduce the problem of MWE identification to the problem of determining the best la2 Implementation of the methods presented in this work can be found at https://github.com/kawu/vine. 115 belling of the given (dependency) tree. We propose two solutions to the latter proble"
W19-5113,W18-4930,1,0.820587,"Sec. 4, we summarize the experiments we performed to evaluate our approach. Finally, we conclude and mention possible future work in Sec. 5. 2 lem. They proposed a sequence labeling scheme (IiOoBb) which allows to represent discontinuous MWEs as well as nested MWEs. The encoding methods we propose also allow to handle discontinuous and, to a certain degree, nested MWEs, with the important difference that they apply to trees rather than sequences. Previous work on applying parsing-based techniques to MWE identification includes transitionbased (Constant and Nivre, 2016; Al Saied et al., 2018; Stodden et al., 2018) and graph-based (Waszczuk, 2018; Boros¸ and Burtica, 2018) approaches. As shown by the two PARSEME shared tasks (Savary et al., 2017; Ramisch et al., 2018a), both strategies can be very effective, even without relying on pre-trained word vectors. The method we propose is graph-based and it resembles the one of Waszczuk (2018) in that it relies on global modelling and restricts the labelling decisions to dependency fragments, and the one of Boros¸ and Burtica (2018) in that it relies on a neural architecture. In comparison with the former, the encoding schemata we propose allow to deal with tw"
W19-5113,W18-4931,1,0.908787,"e Quartier Latin.’1 In this paper, we propose a method which identifies VMWE occurrences directly over dependency structures. Relying on existing dependency trees greatly simplifies the task, since VMWEs 1 From the German set of the PARSEME Shared Task 1.1. 114 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 114–124 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics are usually connected in such trees (Bejˇcek et al., 2012), even if they are discontinuous on the surface of word sequences as in (1). In the same vein as (Waszczuk, 2018), our method is conceptually divided into two layers. The first is concerned with encoding VMWE occurrences as tree labellings, as well as the inverse process of decoding the labellings into VMWE annotations. In the second layer, a probability model which allows to discriminate between different VMWE labellings is used. We propose two probability models, both based on dense feature representations (i.e. pre-trained word embeddings) of input words. Relying on dense features allows to easier generalize beyond training data, on the one hand, and to possibly capture helpful syntactic and semantic"
W98-0120,P95-1021,0,0.0356319,"Missing"
W98-0120,J92-4004,0,0.0837655,"Missing"
