2020.acl-main.376,P18-1249,0,0.106964,"Missing"
2020.acl-main.376,W17-6315,0,0.0609843,"TPARSE Lab, LyS Group Depto. de Ciencias de la Computaci´on y Tecnolog´ıas de la Informaci´on Campus de Elvi˜na, s/n, 15071 A Coru˜na, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract Most attempts to improve this approach focused on modifying the neural network architecture, while keeping the top-down linearization strategy. As exceptions, Ma et al. (2017) and Liu and Zhang (2017a) proposed linearizations based on sequences of transition-based parsing actions instead of brackets. Ma et al. (2017) tried a bottom-up linearization, but they obtained worse results than topdown approaches.1 Liu and Zhang (2017a) kept the top-down strategy, but using transitions of the top-down transition system of Dyer et al. (2016) instead of a bracketed linearization, achieving a higher performance. In transition-based constituent parsing, an inorder algorithm has recently proved superior to the bottom-up and top-down approaches (Liu and Zhang, 2017b), but we know of no applications of this approach in seq2seq parsing. Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieve"
2020.acl-main.376,Q17-1029,0,0.0741314,"TPARSE Lab, LyS Group Depto. de Ciencias de la Computaci´on y Tecnolog´ıas de la Informaci´on Campus de Elvi˜na, s/n, 15071 A Coru˜na, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract Most attempts to improve this approach focused on modifying the neural network architecture, while keeping the top-down linearization strategy. As exceptions, Ma et al. (2017) and Liu and Zhang (2017a) proposed linearizations based on sequences of transition-based parsing actions instead of brackets. Ma et al. (2017) tried a bottom-up linearization, but they obtained worse results than topdown approaches.1 Liu and Zhang (2017a) kept the top-down strategy, but using transitions of the top-down transition system of Dyer et al. (2016) instead of a bracketed linearization, achieving a higher performance. In transition-based constituent parsing, an inorder algorithm has recently proved superior to the bottom-up and top-down approaches (Liu and Zhang, 2017b), but we know of no applications of this approach in seq2seq parsing. Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieve"
2020.acl-main.376,Q17-1004,0,0.0666266,"TPARSE Lab, LyS Group Depto. de Ciencias de la Computaci´on y Tecnolog´ıas de la Informaci´on Campus de Elvi˜na, s/n, 15071 A Coru˜na, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract Most attempts to improve this approach focused on modifying the neural network architecture, while keeping the top-down linearization strategy. As exceptions, Ma et al. (2017) and Liu and Zhang (2017a) proposed linearizations based on sequences of transition-based parsing actions instead of brackets. Ma et al. (2017) tried a bottom-up linearization, but they obtained worse results than topdown approaches.1 Liu and Zhang (2017a) kept the top-down strategy, but using transitions of the top-down transition system of Dyer et al. (2016) instead of a bracketed linearization, achieving a higher performance. In transition-based constituent parsing, an inorder algorithm has recently proved superior to the bottom-up and top-down approaches (Liu and Zhang, 2017b), but we know of no applications of this approach in seq2seq parsing. Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieve"
2020.acl-main.376,J93-2004,0,0.0701532,"Missing"
2020.acl-main.376,P80-1024,0,0.714509,"ting subtree (Reduce-X). In that way, we can one-to-one map opening brackets to Non-Terminal-X transitions, closing brackets to Reduce-X actions and XX-tags to Shift transitions as shown in example c of Figure 1 . This enriched version will enlarge the vocabulary, but will also add some extra informa2 Source code available at https://github.com/ danifg/InOrderSeq2seq. tion that, as we will see below, improves parsing accuracy. As an alternative to the top-down parser of (Dyer et al., 2016), Liu and Zhang (2017b) define a transition system based on in-order traversal, as in leftcorner parsing (Rosenkrantz and Lewis, 1970): the non-terminal node on top of the tree being built is only considered after the first child is completed in the stack, building each subtree in a bottom-up manner, but choosing the non-terminal node on top before the new constituent is reduced. Transitions are the same as in the top-down algorithm (plus a Finish transition to terminate the parsing process), but the effect of applying a Reduce transition is different: it pops all elements from the stack until the first non-terminal node is found, which is also popped together with the preceding element in the stack to build a new constituen"
2020.acl-main.376,P17-1076,0,0.175636,"Missing"
2020.acl-main.376,P18-2097,0,0.466769,"ther with the preceding element in the stack to build a new constituent with all of them as children of the non-terminal node.3 This algorithm pushed state-of-the-art accuracies in shift-reduce constituent parsing; and, as we show in Section 4, it can be succesfully applied as a linearization method for seq2seq constituent parsing. Sequence d in Figure 1 exemplifies in-order linearization. Similarly to the enriched top-down variant, we also extend the in-order shift-reduce linearization by parametrizing Reduce transitions. Additionally, we can also add extra information to Shift transitions. (Suzuki et al., 2018) leaves POS tags of punctuation symbols out of the normalization proposed by Vinyals et al. (2015) without further explanation, but possibly they consider it can help seq2seq models. We adapt this idea to our novel enriched in-order linearization and lexicalize Shift transitions when a “.” or a “,” are pushed into the stack as “Shift.” and “Shift,”, respectively.4 In our experiments, we see that lexicalizing Shift transitions has indeed an impact on parsing performance. In Figure 1 and sequence e, we include an example of this linearization technique. Note that, although we use a transition-ba"
2020.acl-main.376,P15-1113,0,0.0565043,"Missing"
2020.acl-main.376,P13-1043,0,0.356498,"Missing"
2020.acl-main.629,P13-1023,0,0.0617804,"to the best of our knowledge, first to explore DAG parsing with Pointer Networks, proposing a purely transition-based algorithm that can be a competitive alternative to graph-based SDP models. Finally, during the reviewing process of this work, the proceedings of the CoNLL 2019 shared task (Oepen et al., 2019) were released. In that event, SDP parsers were evaluated on updated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019). Also in parallel to the development of this research, Zhang et al. (2019) propo"
2020.acl-main.629,C18-1139,0,0.152467,"irstly, Wang et al. (2019) manage to add second-order information for score computation and then apply either mean field variational inference or loopy belief propagation information to decode the highestscoring SDP graph. While significantly boosting parsing accuracy, the original O(n2 ) runtime complexity is modified to O(n3 ) in the resulting SDP system. Secondly, He and Choi (2019) significantly improve the original parser’s accuracy by not only using contextualized word embeddings extracted from BERT (Devlin et al., 2019), but also introducing contextual string embeddings (called Flair) (Akbik et al., 2018), which consist in a novel type of word vector representations based on character7036 level language modeling. Both extensions, (Wang et al., 2019) and (He and Choi, 2019), are currently the state of the art on the SemEval 2015 Task 18 in the fully-supervised and semi-supervised scenarios, respectively. Kurita and Søgaard (2019) have also recently proposed a complex approach that iteratively applies the syntactic dependency parser by Zhang et al. (2017), sequentially building a DAG structure. At each iteration, the graph-based parser selects the highest-scoring arcs, keeping the single-head co"
2020.acl-main.629,S15-2162,0,0.0376054,"Missing"
2020.acl-main.629,K19-2008,0,0.0168668,"e released. In that event, SDP parsers were evaluated on updated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019). Also in parallel to the development of this research, Zhang et al. (2019) proposed a transition-based parser that, while it can be applied for SDP, was specifically designed for AMR and UCCA parsing (where graph nodes do not correspond with words and must be generated during the parsing process). In particular, this approach incrementally builds a graph by predicting at each step a se"
2020.acl-main.629,W13-2322,0,0.113624,", 2015), enhanced with BiLSTMs and Tree-LSTMs for feature extraction. We are, to the best of our knowledge, first to explore DAG parsing with Pointer Networks, proposing a purely transition-based algorithm that can be a competitive alternative to graph-based SDP models. Finally, during the reviewing process of this work, the proceedings of the CoNLL 2019 shared task (Oepen et al., 2019) were released. In that event, SDP parsers were evaluated on updated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019)"
2020.acl-main.629,P12-1000,0,0.195548,"Missing"
2020.acl-main.629,N19-1423,0,0.547186,"15) to predict transitions that can attach multiple heads to the same word and incrementally build a labelled DAG. This kind of neural networks provide an encoder-decoder architecture that is capable of capturing information from the whole sentence and previously created arcs, alleviating the impact of error propagation and already showing remarkable results in transition-based dependency parsing (Ma et al., 2018; Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2019). We further enhance our neural network with deep contextualized word embeddings extracted from the pre-trained language model BERT (Devlin et al., 2019). The proposed SDP parser1 can process sentences in SDP treebanks (where structures are sparse DAGs with a low in-degree) in O(n2 log n) time, or O(n2 ) without cycle detection. This is more efficient than the current fully-supervised state-of-theart system by Wang et al. (2019) (O(n3 ) without cycle detection), while matching its accuracy on the SemEval 2015 Task 18 datasets (Oepen et al., 2015). In addition, we also prove that our novel transition-based model provides promising accuracies in the semi-supervised scenario, achieving some state-of-the-art results. 2 Related Work An early approa"
2020.acl-main.629,P18-2077,0,0.325218,"ask 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes higher-order information with hand-crafted features and employs the AD3 algorithm (Martins et al., 2011) to find valid DAGs during decoding. This was extended by Peng et al. (2017) with BiLSTM-based feature extraction and multitask learning: the three formalisms considered in the shared task were jointly learned to improve final accuracy. After the success of Dozat et al. (2017) in graphbased dependency parsing, Dozat and Manning (2018) proposed minor adaptations to use this biaffine neural architecture to produce SDP graphs. To that end, they removed the maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967) necessary for decoding well-formed dependency trees and simply kept those edges with a positive score. In addition, they trained the unlabelled parser with a sigmoid cross-entropy (instead of the original softmax one) in order to accept multiple heads. The parser by Dozat and Manning (2018) was recently improved by two contributions. Firstly, Wang et al. (2019) manage to add second-order information for scor"
2020.acl-main.629,K17-3002,0,0.0189824,"purely graph-based DAG models since the SemEval 2015 Task 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes higher-order information with hand-crafted features and employs the AD3 algorithm (Martins et al., 2011) to find valid DAGs during decoding. This was extended by Peng et al. (2017) with BiLSTM-based feature extraction and multitask learning: the three formalisms considered in the shared task were jointly learned to improve final accuracy. After the success of Dozat et al. (2017) in graphbased dependency parsing, Dozat and Manning (2018) proposed minor adaptations to use this biaffine neural architecture to produce SDP graphs. To that end, they removed the maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967) necessary for decoding well-formed dependency trees and simply kept those edges with a positive score. In addition, they trained the unlabelled parser with a sigmoid cross-entropy (instead of the original softmax one) in order to accept multiple heads. The parser by Dozat and Manning (2018) was recently improved by two contributions. Firstly, Wang et"
2020.acl-main.629,S15-2154,0,0.324061,"6 c July 5 - 10, 2020. 2020 Association for Computational Linguistics gation: i.e., since transitions are sequentially and locally predicted, an erroneous action can affect future predictions, having a significant impact in long sentences and being, to date, less appealing for SDP. In fact, in recent years only a few contributions, such as the system developed by Wang et al. (2018), present a purely transition-based SDP parser. It is more common to find hybrid systems that combine transition-based approaches with graph-based techniques to alleviate the impact of error propagation in accuracy (Du et al., 2015), but this penalizes the efficiency provided by transition-based algorithms. Away from the current mainstream, we present a purely transition-based parser that directly generates SDP graphs without the need of any additional techniques. We rely on Pointer Networks (Vinyals et al., 2015) to predict transitions that can attach multiple heads to the same word and incrementally build a labelled DAG. This kind of neural networks provide an encoder-decoder architecture that is capable of capturing information from the whole sentence and previously created arcs, alleviating the impact of error propag"
2020.acl-main.629,K19-2007,0,0.278701,"during the reviewing process of this work, the proceedings of the CoNLL 2019 shared task (Oepen et al., 2019) were released. In that event, SDP parsers were evaluated on updated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019). Also in parallel to the development of this research, Zhang et al. (2019) proposed a transition-based parser that, while it can be applied for SDP, was specifically designed for AMR and UCCA parsing (where graph nodes do not correspond with words and must be generated dur"
2020.acl-main.629,P15-1033,0,0.0236803,"nsition-based algorithms, assigning, during voting, higher weights to them. Among the transition systems used, we can find the one developed by Titov et al. (2009), which is not able to cover all SDP graphs. We have to wait until the work by Wang et al. (2018) to see that a purely transition-based SDP parser (enhanced with a simple model ensemble technique) can achieve competitive results. They simply modified the preconditions of the complex transition system by Choi and McCallum (2013) to produce unrestricted DAG structures. In addition, their system was implemented by means of stack-LSTMs (Dyer et al., 2015), enhanced with BiLSTMs and Tree-LSTMs for feature extraction. We are, to the best of our knowledge, first to explore DAG parsing with Pointer Networks, proposing a purely transition-based algorithm that can be a competitive alternative to graph-based SDP models. Finally, during the reviewing process of this work, the proceedings of the CoNLL 2019 shared task (Oepen et al., 2019) were released. In that event, SDP parsers were evaluated on updated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banares"
2020.acl-main.629,N19-1076,1,0.909894,"Missing"
2020.acl-main.629,P10-1151,1,0.805612,"Missing"
2020.acl-main.629,hajic-etal-2012-announcing,0,0.155187,"Missing"
2020.acl-main.629,P17-1104,0,0.0326911,"dated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019). Also in parallel to the development of this research, Zhang et al. (2019) proposed a transition-based parser that, while it can be applied for SDP, was specifically designed for AMR and UCCA parsing (where graph nodes do not correspond with words and must be generated during the parsing process). In particular, this approach incrementally builds a graph by predicting at each step a semantic relation composed of the target and source nodes plus the"
2020.acl-main.629,K19-2002,0,0.0891785,"task (Oepen et al., 2019) were released. In that event, SDP parsers were evaluated on updated versions of SemEval 2015 Task 18 datasets, as well as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019). Also in parallel to the development of this research, Zhang et al. (2019) proposed a transition-based parser that, while it can be applied for SDP, was specifically designed for AMR and UCCA parsing (where graph nodes do not correspond with words and must be generated during the parsing process). In particular, this approach incrementally builds a graph by predict"
2020.acl-main.629,P19-1232,0,0.674374,"O(n3 ) in the resulting SDP system. Secondly, He and Choi (2019) significantly improve the original parser’s accuracy by not only using contextualized word embeddings extracted from BERT (Devlin et al., 2019), but also introducing contextual string embeddings (called Flair) (Akbik et al., 2018), which consist in a novel type of word vector representations based on character7036 level language modeling. Both extensions, (Wang et al., 2019) and (He and Choi, 2019), are currently the state of the art on the SemEval 2015 Task 18 in the fully-supervised and semi-supervised scenarios, respectively. Kurita and Søgaard (2019) have also recently proposed a complex approach that iteratively applies the syntactic dependency parser by Zhang et al. (2017), sequentially building a DAG structure. At each iteration, the graph-based parser selects the highest-scoring arcs, keeping the single-head constraint. The process ends when no arcs are added in the last iteration. The combination of partial parses results in an SDP graph. Since the graph is built in a sequential process, they use reinforcement learning to guide the model through more optimal paths. Following Peng et al. (2017), multi-task learning is also added to bo"
2020.acl-main.629,K19-2010,0,0.0112909,"ll as on datasets in other semantic formalisms such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013) and Universal Cognitive Conceptual Annotation (UCCA) (Abend and Rappoport, 2013). Although graph-based parsers achieved better accuracy in the SDP track, several BERT-enhanced transition-based approaches were proposed. Among them we can find an extension (Che et al., 2019) of the system by Wang et al. (2018), several adaptations for SDP (Hershcovich and Arviv, 2019; Bai and Zhao, 2019) of the transition-based UCCA parser by Hershcovich et al. (2017), as well as an SDP variant (Lai et al., 2019) of the constituent transition system introduced by Fern´andez-Gonz´alez and G´omezRodr´ıguez (2019). Also in parallel to the development of this research, Zhang et al. (2019) proposed a transition-based parser that, while it can be applied for SDP, was specifically designed for AMR and UCCA parsing (where graph nodes do not correspond with words and must be generated during the parsing process). In particular, this approach incrementally builds a graph by predicting at each step a semantic relation composed of the target and source nodes plus the arc label. While this can be seen as an extens"
2020.acl-main.629,N15-1142,0,0.0245826,"elled F-measure scores (LF1) (including ROOT arcs) on the in-domain (ID) and out-of-domain (OOD) test sets for each formalism as well as the macroaverage over the three of them. 5.2 Settings We use the Adam optimizer (Kingma and Ba, 2014) and follow (Ma et al., 2018; Dozat and Manning, 2017) for parameter optimization. We do not specifically perform hyper-parameter selection for SDP and just adopt those proposed by Ma et al. (2018) for syntactic dependency parsing (detailed in Table 2). For initializing word and lemma vectors, we use the pre-trained structured-skipgram embeddings developed by Ling et al. (2015). POS tag and character embeddings are randomly initialized and all embeddings (except the deep contextualized ones) are fine-tuned during training. Due to random initializations, we report average accuracy over 5 repetitions for each experiment. In addition, during a 500-epoch training, the model with the highest labelled F-score on the development set is chosen. Finally, while further beam-size exploration might improve accuracy, we use beam-search decoding with beam size 5 in all experiments. 2 https://github.com/ semantic-dependency-parsing/toolkit Results and Discussion Table 3 reports th"
2020.acl-main.629,P16-1101,0,0.041496,"au et al., 2014) to select positions from the input sequence, without requiring a fixed size of the output dictionary. This allows Pointer Networks to easily address those problems where the target classes considered at each step are variable and depend on the length of the input sequence. We prove that implementing the transition system previously defined on this neural network results in an accurate SDP system. We follow previous work in dependency parsing (Ma et al., 2018; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2019) to design our neural architecture: Encoder A BiLSTM-CNN architecture (Ma and Hovy, 2016) is used to encode the input sentence 7038 w1 , . . . , wn , word by word, into a sequence of encoder hidden states h1 , . . . , hn . CNNs with max pooling are used for extracting character-level representations of words and, then, each word wi is represented by the concatenation of character (eci ), p l word (ew i ), lemma (ei ) and POS tag (ei ) embeddings: p l xi = eci ⊕ ew i ⊕ ei ⊕ ei After that, the xi of each word wi is fed one-by-one into a BiLSTM that captures context information in both directions and generates a vector representation hi : hi = hli ⊕ hri = BiLSTM(xi ) In addition, a s"
2020.acl-main.629,P18-1130,0,0.0918927,"m the current mainstream, we present a purely transition-based parser that directly generates SDP graphs without the need of any additional techniques. We rely on Pointer Networks (Vinyals et al., 2015) to predict transitions that can attach multiple heads to the same word and incrementally build a labelled DAG. This kind of neural networks provide an encoder-decoder architecture that is capable of capturing information from the whole sentence and previously created arcs, alleviating the impact of error propagation and already showing remarkable results in transition-based dependency parsing (Ma et al., 2018; Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2019). We further enhance our neural network with deep contextualized word embeddings extracted from the pre-trained language model BERT (Devlin et al., 2019). The proposed SDP parser1 can process sentences in SDP treebanks (where structures are sparse DAGs with a low in-degree) in O(n2 log n) time, or O(n2 ) without cycle detection. This is more efficient than the current fully-supervised state-of-theart system by Wang et al. (2019) (O(n3 ) without cycle detection), while matching its accuracy on the SemEval 2015 Task 18 datasets (Oepen et al., 20"
2020.acl-main.629,J93-2004,0,0.0761692,"Missing"
2020.acl-main.629,P13-2109,0,0.0296791,"erformed by the first transition-based DAG parser by Sagae and Tsujii (2008). They extended the existing transition system by Nivre (2003) to allow multiple heads per token. The resulting algorithm was not able to produce DAGs with crossing dependencies, requiring the pseudo-projective transformation by Nivre and Nilsson (2005) (plus a cycle removal procedure) as a post-processing stage. More recently, there has been a predominance of purely graph-based DAG models since the SemEval 2015 Task 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes higher-order information with hand-crafted features and employs the AD3 algorithm (Martins et al., 2011) to find valid DAGs during decoding. This was extended by Peng et al. (2017) with BiLSTM-based feature extraction and multitask learning: the three formalisms considered in the shared task were jointly learned to improve final accuracy. After the success of Dozat et al. (2017) in graphbased dependency parsing, Dozat and Manning (2018) proposed minor adaptations to use this biaffine neural architecture to produce SDP graphs. To that end,"
2020.acl-main.629,D11-1022,0,0.0335487,"ds per token. The resulting algorithm was not able to produce DAGs with crossing dependencies, requiring the pseudo-projective transformation by Nivre and Nilsson (2005) (plus a cycle removal procedure) as a post-processing stage. More recently, there has been a predominance of purely graph-based DAG models since the SemEval 2015 Task 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes higher-order information with hand-crafted features and employs the AD3 algorithm (Martins et al., 2011) to find valid DAGs during decoding. This was extended by Peng et al. (2017) with BiLSTM-based feature extraction and multitask learning: the three formalisms considered in the shared task were jointly learned to improve final accuracy. After the success of Dozat et al. (2017) in graphbased dependency parsing, Dozat and Manning (2018) proposed minor adaptations to use this biaffine neural architecture to produce SDP graphs. To that end, they removed the maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967) necessary for decoding well-formed dependency trees and simply kept those e"
2020.acl-main.629,P17-1186,0,0.127384,"Missing"
2020.acl-main.629,E06-1011,0,0.476709,"ch has focused on adapting them to deal with the absence of singlehead and connectedness constraints and to produce an SDP graph instead. As in dependency parsing, we can find two main families of approaches to efficiently generate accurate SDP graphs. On the one hand, graph-based algorithms have drawn more attention since adapting them to this task is relatively straightforward. In particular, these globally optimized methods independently score arcs (or sets of them) and then search for a high-scoring graph by combining these scores. From one of the first graph-based DAG parsers proposed by McDonald and Pereira (2006) to the current state-of-the-art models (Wang et al., 2019; He and Choi, 2019), different graph-based SDP approaches have been presented, providing accuracies above their main competitors: transitionbased DAG algorithms. A transition-based parser generates a sequence of actions to incrementally build a valid graph (usually from left to right). This is typically done by local, greedy prediction and can efficiently parse a sentence in a linear or quadratic number of actions (transitions); however, the lack of global inference makes them more prone to suffer from error propa7035 Proceedings of th"
2020.acl-main.629,C08-1095,0,0.268356,"at our novel transition-based model provides promising accuracies in the semi-supervised scenario, achieving some state-of-the-art results. 2 Related Work An early approach to DAG parsing was implemented as a modification to a graph-based parser by McDonald and Pereira (2006). This produced DAGs using approximate inference by first finding a dependency tree, and then adding extra edges that would increase the graph’s overall score. A 1 Source code available at https://github.com/ danifg/SemanticPointer. few years later, this attempt was outperformed by the first transition-based DAG parser by Sagae and Tsujii (2008). They extended the existing transition system by Nivre (2003) to allow multiple heads per token. The resulting algorithm was not able to produce DAGs with crossing dependencies, requiring the pseudo-projective transformation by Nivre and Nilsson (2005) (plus a cycle removal procedure) as a post-processing stage. More recently, there has been a predominance of purely graph-based DAG models since the SemEval 2015 Task 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes"
2020.acl-main.629,C04-1204,0,0.219433,"Missing"
2020.acl-main.629,W03-3017,0,0.233018,"semi-supervised scenario, achieving some state-of-the-art results. 2 Related Work An early approach to DAG parsing was implemented as a modification to a graph-based parser by McDonald and Pereira (2006). This produced DAGs using approximate inference by first finding a dependency tree, and then adding extra edges that would increase the graph’s overall score. A 1 Source code available at https://github.com/ danifg/SemanticPointer. few years later, this attempt was outperformed by the first transition-based DAG parser by Sagae and Tsujii (2008). They extended the existing transition system by Nivre (2003) to allow multiple heads per token. The resulting algorithm was not able to produce DAGs with crossing dependencies, requiring the pseudo-projective transformation by Nivre and Nilsson (2005) (plus a cycle removal procedure) as a post-processing stage. More recently, there has been a predominance of purely graph-based DAG models since the SemEval 2015 Task 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes higher-order information with hand-crafted features and emplo"
2020.acl-main.629,J08-4003,0,0.0423972,"use during training. In particular, (Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2019) proved to be more suitable for dependency parsing than (Ma et al., 2018) since it requires half as many steps to produce the same dependency parse, being not only faster, but also more accurate (as this mitigates the impact of error propagation). Inspired by Fern´andez-Gonz´alez and G´omez7037 Rodr´ıguez (2019), we train a Pointer Network to point to the head of a given word and propose an algorithm that does not use any kind of data structures (stack or buffer, required in classic transitionbased parsers (Nivre, 2008)), but just a focus word pointer i for marking the word currently being processed. More in detail, given an input sentence of n words w1 , . . . , wn , the parsing process starts with i pointing at the first word w1 . At each time step, the current focus word wi is used by the Pointer Network to return a position p from the input sentence (or 0, where the ROOT node is located). This information is used to choose between the two available transitions: • If p 6= i, then the pointed word wp is considered as a semantic head word (predicate) of wi and an Attach-p transition is applied, creating the"
2020.acl-main.629,P05-1013,0,0.110415,"onald and Pereira (2006). This produced DAGs using approximate inference by first finding a dependency tree, and then adding extra edges that would increase the graph’s overall score. A 1 Source code available at https://github.com/ danifg/SemanticPointer. few years later, this attempt was outperformed by the first transition-based DAG parser by Sagae and Tsujii (2008). They extended the existing transition system by Nivre (2003) to allow multiple heads per token. The resulting algorithm was not able to produce DAGs with crossing dependencies, requiring the pseudo-projective transformation by Nivre and Nilsson (2005) (plus a cycle removal procedure) as a post-processing stage. More recently, there has been a predominance of purely graph-based DAG models since the SemEval 2015 Task 18 (Oepen et al., 2015). Almeida and Martins (2015) adapted the pre-deep-learning dependency parser by Martins et al. (2013) to produce SDP graphs. This graph-based parser encodes higher-order information with hand-crafted features and employs the AD3 algorithm (Martins et al., 2011) to find valid DAGs during decoding. This was extended by Peng et al. (2017) with BiLSTM-based feature extraction and multitask learning: the three"
2020.acl-main.629,K19-2001,0,0.150618,"Missing"
2020.acl-main.629,S15-2153,0,0.242538,"Missing"
2020.acl-main.629,P19-1454,0,0.056719,"ad and connectedness constraints and to produce an SDP graph instead. As in dependency parsing, we can find two main families of approaches to efficiently generate accurate SDP graphs. On the one hand, graph-based algorithms have drawn more attention since adapting them to this task is relatively straightforward. In particular, these globally optimized methods independently score arcs (or sets of them) and then search for a high-scoring graph by combining these scores. From one of the first graph-based DAG parsers proposed by McDonald and Pereira (2006) to the current state-of-the-art models (Wang et al., 2019; He and Choi, 2019), different graph-based SDP approaches have been presented, providing accuracies above their main competitors: transitionbased DAG algorithms. A transition-based parser generates a sequence of actions to incrementally build a valid graph (usually from left to right). This is typically done by local, greedy prediction and can efficiently parse a sentence in a linear or quadratic number of actions (transitions); however, the lack of global inference makes them more prone to suffer from error propa7035 Proceedings of the 58th Annual Meeting of the Association for Computational"
2020.acl-main.629,D19-1392,0,0.0644535,"Missing"
2020.acl-main.629,E17-1063,0,0.0454773,"ng contextualized word embeddings extracted from BERT (Devlin et al., 2019), but also introducing contextual string embeddings (called Flair) (Akbik et al., 2018), which consist in a novel type of word vector representations based on character7036 level language modeling. Both extensions, (Wang et al., 2019) and (He and Choi, 2019), are currently the state of the art on the SemEval 2015 Task 18 in the fully-supervised and semi-supervised scenarios, respectively. Kurita and Søgaard (2019) have also recently proposed a complex approach that iteratively applies the syntactic dependency parser by Zhang et al. (2017), sequentially building a DAG structure. At each iteration, the graph-based parser selects the highest-scoring arcs, keeping the single-head constraint. The process ends when no arcs are added in the last iteration. The combination of partial parses results in an SDP graph. Since the graph is built in a sequential process, they use reinforcement learning to guide the model through more optimal paths. Following Peng et al. (2017), multi-task learning is also added to boost final accuracy. On the other hand, the use of transition-based algorithms in the SDP task had been less explored until very"
2020.acl-main.629,S14-2008,0,0.112086,"Missing"
2020.coling-main.223,2020.conll-1.6,1,0.814699,"Missing"
2020.coling-main.223,K17-3022,0,0.0539031,"Missing"
2020.coling-main.223,N18-2062,1,0.485305,"Missing"
2020.coling-main.223,P10-1151,1,0.859226,"Missing"
2020.coling-main.223,J13-4002,1,0.895316,"Missing"
2020.coling-main.223,D18-1162,1,0.813778,"Missing"
2020.coling-main.223,P18-1249,0,0.0239044,"g of Spoustov´a and Spousta (2010) and a new encoding based on balanced brackets, inspired by Yli-Jyr¨a and G´omez-Rodr´ıguez (2017). While the encoding of Spoustov´a and Spousta (2010) achieved a good accuracy, and it has full coverage of non-projective dependency trees, it requires PoS tags to encode the dependency arcs. This can be seen as a weakness, not just because computing and feeding PoS tags increases the latency, but also because the traditional assumption that PoS tagging is needed for parsing is being increasingly called into question (de Lhoneux et al., 2017; Smith et al., 2018; Kitaev and Klein, 2018; Anderson and G´omez-Rodr´ıguez, 2020). Low-frequency PoS tags can cause sparsity in the encoding, and low-quality PoS tags could be a potential source of errors in low-resource languages. For this reason, Lacroix (2019) proposed two alternative encodings with the same relative indexing philosophy, but without using PoS tags. However, these encodings require a composition of two sequence labeling processes instead of one. On the other hand, the bracketing encoding inspired in (Yli-Jyr¨a and G´omez-Rodr´ıguez, 2017) represents the trees independently of PoS tags or any other previous tagging s"
2020.coling-main.223,Q15-1040,0,0.024992,"separate stacks for each kind of bracket). The paper by Strzyz et al. (2019b) erroneously describes the encoding as only supporting projective trees. In fact, the implementation in that paper is supporting this mild extension of projectivity where crossing arcs in opposite directions are allowed. 2.2 2-Planarity A dependency graph (V, E) is said to be k-planar, for k ≥ 1, if there is a partition of the edges into sets E1 , . . . , Ek , called planes, in such a way that edges that are in the same plane do not cross. For k = 1, this corresponds to the concept of a noncrossing dependency graph (Kuhlmann and Jonsson, 2015) or planar linear arrangement (Chao and Sha, 1992) (not to be confused with a planar graph). Under the assumption of trees rooted at the dummy root node 0, 1-planar trees are equivalent to the well-known projective trees. For k ≥ 2, this means that the dependency graph (together with the linear order of the words) is a k-page book embedding of a graph (see (Pitler et al., 2013)). Intuitively, a k-planar graph is one where each arc can be assigned one out of k colors in such a way that arcs with the same color do not cross (see Figure 1). 2-planarity has been shown to be particularly relevant f"
2020.coling-main.223,W19-7716,0,0.0128394,"overage of non-projective dependency trees, it requires PoS tags to encode the dependency arcs. This can be seen as a weakness, not just because computing and feeding PoS tags increases the latency, but also because the traditional assumption that PoS tagging is needed for parsing is being increasingly called into question (de Lhoneux et al., 2017; Smith et al., 2018; Kitaev and Klein, 2018; Anderson and G´omez-Rodr´ıguez, 2020). Low-frequency PoS tags can cause sparsity in the encoding, and low-quality PoS tags could be a potential source of errors in low-resource languages. For this reason, Lacroix (2019) proposed two alternative encodings with the same relative indexing philosophy, but without using PoS tags. However, these encodings require a composition of two sequence labeling processes instead of one. On the other hand, the bracketing encoding inspired in (Yli-Jyr¨a and G´omez-Rodr´ıguez, 2017) represents the trees independently of PoS tags or any other previous tagging step, but it has the limitation of being restricted to a very mild extension of projective trees. Contribution. In this paper, we extend the idea of the bracketing-based encoding to non-projective parsing by defining a var"
2020.coling-main.223,C18-1271,0,0.197643,"Missing"
2020.coling-main.223,P19-1531,1,0.892034,"Missing"
2020.coling-main.223,N19-1077,1,0.889861,"Missing"
2020.coling-main.223,P18-4013,0,0.0537567,"Missing"
2020.coling-main.223,P17-1160,1,0.903182,"Missing"
2020.coling-main.336,K19-2008,0,0.0239535,"Eryiˇgit (2015) Artsymenia et al. (2016) Yes No Yes Yes SH 1 SH SH 1 1 Online reordering (Zhang et al., 2016) Two-stack (Zhang et al., 2016) Hershcovich et al. (2017) Wang et al. (2018) DM, PSD (Che et al., 2019) Yes No Yes Yes Read t. k SH 1 No-SH, R-SH SH 1 1 Table 4: Transition-based semantic dependency parsers, whether they are left-to-right (L2R?) or not, read transitions in case they are, and value of the constant k. For simplicity we only include semantic dependency parsers and exclude parsers for formalisms that go beyond dependency graphs, like AMR, and cross-framework parsers (e.g. Bai and Zhao (2019)). Table 4 lists transition-based semantic dependency parsers with analogous information to Tables 2 and 3. Once again, parsers that do not fall into our definition of left-to-right are mostly those that define a swap transition. Note that for this coverage analysis we exclude semantic formalisms that go beyond 3781 dependency graphs, such as AMR (Banarescu et al., 2013), where pure transition-based models (e.g. Damonte et al. (2017), Ballesteros and Al-Onaizan (2017),Vilares and G´omez-Rodr´ıguez (2018)) need to remove tokens and also create (multiple) concepts from words, and therefore inclu"
2020.coling-main.336,D17-1130,0,0.0188211,"semantic dependency parsers and exclude parsers for formalisms that go beyond dependency graphs, like AMR, and cross-framework parsers (e.g. Bai and Zhao (2019)). Table 4 lists transition-based semantic dependency parsers with analogous information to Tables 2 and 3. Once again, parsers that do not fall into our definition of left-to-right are mostly those that define a swap transition. Note that for this coverage analysis we exclude semantic formalisms that go beyond 3781 dependency graphs, such as AMR (Banarescu et al., 2013), where pure transition-based models (e.g. Damonte et al. (2017), Ballesteros and Al-Onaizan (2017),Vilares and G´omez-Rodr´ıguez (2018)) need to remove tokens and also create (multiple) concepts from words, and therefore include specific transitions to do so. Removing tokens can be seen as a read transition, but creating many concept nodes from a single word breaks the left-to-right condition and does not ensure that the system will have n read transitions, where n is the length of the input sentence. Although outside the scope of this paper, note that a hybrid system that first computed the concepts from words (e.g. with a seq2seq architecture), to then apply a transition-based graph pars"
2020.coling-main.336,K15-1029,0,0.0508353,"Missing"
2020.coling-main.336,W13-2322,0,0.0292951,"transitions in case they are, and value of the constant k. For simplicity we only include semantic dependency parsers and exclude parsers for formalisms that go beyond dependency graphs, like AMR, and cross-framework parsers (e.g. Bai and Zhao (2019)). Table 4 lists transition-based semantic dependency parsers with analogous information to Tables 2 and 3. Once again, parsers that do not fall into our definition of left-to-right are mostly those that define a swap transition. Note that for this coverage analysis we exclude semantic formalisms that go beyond 3781 dependency graphs, such as AMR (Banarescu et al., 2013), where pure transition-based models (e.g. Damonte et al. (2017), Ballesteros and Al-Onaizan (2017),Vilares and G´omez-Rodr´ıguez (2018)) need to remove tokens and also create (multiple) concepts from words, and therefore include specific transitions to do so. Removing tokens can be seen as a read transition, but creating many concept nodes from a single word breaks the left-to-right condition and does not ensure that the system will have n read transitions, where n is the length of the input sentence. Although outside the scope of this paper, note that a hybrid system that first computed the"
2020.coling-main.336,K19-2007,0,0.0318673,"Missing"
2020.coling-main.336,P13-1104,0,0.0618053,"Missing"
2020.coling-main.336,P11-2121,0,0.0571815,"Missing"
2020.coling-main.336,N19-1018,0,0.19185,"ive positional encoding used in Li et al. (2018), which has been shown useful under strong machine learning models (Vacareanu et al., 2020). Table 3 shows transition-based constituency parsers with analogous information to Table 2. In this case, all of the listed parsers that do not fall into our definition of left-to-right are discontinuous constituent parsers that use swap transitions (Versley, 2014; Maier, 2015). Every continuous constituent parser that we found is covered, as well as discontinuous parsers that use other devices, like gap transitions (Coavoux et al., 2019) or set handling (Coavoux and Cohen, 2019). All of the supported constituent parsers have k = 0, following the traditional shift-reduce paradigm that only operates on nodes after reading them from the input buffer. Algorithm L2R? Read t. k Algorithm L2R? Sagae and Tsujii (2008) Yes SH 1 No Titov et al. (2009) Ribeyre et al. (2014) Tokg¨oz and Eryiˇgit (2015) Artsymenia et al. (2016) Yes No Yes Yes SH 1 SH SH 1 1 Online reordering (Zhang et al., 2016) Two-stack (Zhang et al., 2016) Hershcovich et al. (2017) Wang et al. (2018) DM, PSD (Che et al., 2019) Yes No Yes Yes Read t. k SH 1 No-SH, R-SH SH 1 1 Table 4: Transition-based semantic"
2020.coling-main.336,P16-1017,0,0.305013,"Missing"
2020.coling-main.336,E17-1118,0,0.0301933,"Missing"
2020.coling-main.336,Q19-1005,0,0.0204515,"Missing"
2020.coling-main.336,C14-1052,0,0.0664181,"Missing"
2020.coling-main.336,D16-1001,0,0.251198,"Missing"
2020.coling-main.336,E17-1051,0,0.0209583,"plicity we only include semantic dependency parsers and exclude parsers for formalisms that go beyond dependency graphs, like AMR, and cross-framework parsers (e.g. Bai and Zhao (2019)). Table 4 lists transition-based semantic dependency parsers with analogous information to Tables 2 and 3. Once again, parsers that do not fall into our definition of left-to-right are mostly those that define a swap transition. Note that for this coverage analysis we exclude semantic formalisms that go beyond 3781 dependency graphs, such as AMR (Banarescu et al., 2013), where pure transition-based models (e.g. Damonte et al. (2017), Ballesteros and Al-Onaizan (2017),Vilares and G´omez-Rodr´ıguez (2018)) need to remove tokens and also create (multiple) concepts from words, and therefore include specific transitions to do so. Removing tokens can be seen as a read transition, but creating many concept nodes from a single word breaks the left-to-right condition and does not ensure that the system will have n read transitions, where n is the length of the input sentence. Although outside the scope of this paper, note that a hybrid system that first computed the concepts from words (e.g. with a seq2seq architecture), to then"
2020.coling-main.336,K17-3022,0,0.0417919,"Missing"
2020.coling-main.336,W17-6314,0,0.361965,"Missing"
2020.coling-main.336,D15-1162,0,0.154503,"Arc-hybrid (Kuhlmann et al., 2011) Covington projective (Covington, 2001; Nivre, 2008) Covington non-projective (Covington, 2001; Nivre, 2008) Easy-first (Goldberg and Elhadad, 2010) Yes SH 0 Yes Yes Yes SH, RA 1 SH 1 SH 0 Spinal arc-eager (Ballesteros and Car- Yes reras, 2015) Yamada and Matsumoto (2003) No Choi and Palmer (2011) Yes Choi and McCallum (2013) Yes Yes SH Attardi (Attardi, 2006) Yes SH Planar (G´omez-Rodr´ıguez and Nivre, Yes 2010) 2-Planar (G´omez-Rodr´ıguez and Nivre, Yes 2010) SH No SH 0 Non-monotonic arc-eager (Honnibal et Yes al., 2013) Improved non-monotonic arc-eager No (Honnibal and Johnson, 2015) 0 Non-monotonic Covington (Fern´andez- Yes Gonz´alez and G´omez-Rodr´ıguez, 2017) 1 Tree-constrained arc-eager (Nivre and No Fern´andez-Gonz´alez, 2014) 1 Non-local Covington (Fern´andez- Yes Gonz´alez and G´omez-Rodr´ıguez, 2018) 2,3 Two-register (Pitler and McDonald, No 2015) Arc-eager with buffer transitions Yes (Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2012) Swap (Nivre, 2009) No Swap-hybrid (de Lhoneux et al., 2017b) No SH Arc-swift (Qi and Manning, 2017) SH, RAk 1 Yes L2R? Read t. SH, RA k 1 SH 1 No-SH, R-SH 1 SH, RA 1 SH 1 SH 1 Stack-pointer (Ma et al., 2018) No Left-to-right pointer"
2020.coling-main.336,W13-3518,0,0.0390811,"Missing"
2020.coling-main.336,Q16-1023,0,0.029818,"ch state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang and Nivre, 2011). With the adoption of deep learning, which can globally contextualize word representations, the dependency on hand-crafted features has been drastically reduced (Kiperwasser and Goldberg, 2016; Shi et al., 2017); and it has also been shown that alternative ways to attack the problem can be practical. More particularly, several parsing problems have been cast as a machine translation task, where a sequence-to-sequence (seq2seq) network maps the sentence into a string of arbitrary length that encodes a linearized graph (Vinyals et al., 2015; Li et al., 2018; Konstas et al., 2017). To a certain extent, the attention mechanism in these seq2seq models can be seen as an abstraction of the stack and buffer in transition-based parsers, where the attention weights mark the relevant words to"
2020.coling-main.336,P17-1014,0,0.0191251,"resentations (Zhang and Nivre, 2011). With the adoption of deep learning, which can globally contextualize word representations, the dependency on hand-crafted features has been drastically reduced (Kiperwasser and Goldberg, 2016; Shi et al., 2017); and it has also been shown that alternative ways to attack the problem can be practical. More particularly, several parsing problems have been cast as a machine translation task, where a sequence-to-sequence (seq2seq) network maps the sentence into a string of arbitrary length that encodes a linearized graph (Vinyals et al., 2015; Li et al., 2018; Konstas et al., 2017). To a certain extent, the attention mechanism in these seq2seq models can be seen as an abstraction of the stack and buffer in transition-based parsers, where the attention weights mark the relevant words to generate the next component of the output string. Alternatively, some authors have reduced constituent and dependency parsing to sequence labeling, where given an input sentence of length n, the output has length n too, assigning one label to each word (G´omez-Rodr´ıguez and Vilares, 2018; Strzyz et al., 2019). However, these reductions have consisted in defining custom encodings for the"
2020.coling-main.336,P11-1068,1,0.728952,"Missing"
2020.coling-main.336,C18-1271,0,0.0754417,"rich feature representations (Zhang and Nivre, 2011). With the adoption of deep learning, which can globally contextualize word representations, the dependency on hand-crafted features has been drastically reduced (Kiperwasser and Goldberg, 2016; Shi et al., 2017); and it has also been shown that alternative ways to attack the problem can be practical. More particularly, several parsing problems have been cast as a machine translation task, where a sequence-to-sequence (seq2seq) network maps the sentence into a string of arbitrary length that encodes a linearized graph (Vinyals et al., 2015; Li et al., 2018; Konstas et al., 2017). To a certain extent, the attention mechanism in these seq2seq models can be seen as an abstraction of the stack and buffer in transition-based parsers, where the attention weights mark the relevant words to generate the next component of the output string. Alternatively, some authors have reduced constituent and dependency parsing to sequence labeling, where given an input sentence of length n, the output has length n too, assigning one label to each word (G´omez-Rodr´ıguez and Vilares, 2018; Strzyz et al., 2019). However, these reductions have consisted in defining cu"
2020.coling-main.336,W17-6315,0,0.0177312,"ransitions can be learned with only one positional feature (our setup just assigns a sequence of labels beginning with a SH to each word shifted from the input, i.e., it only uses the first buffer word b0 , and has access to no explicit representation of stack elements at all). Thus, this shows that our multi-transition labels seem to be learnable with less data than individual transitions in transition systems, although the latter (with suitable features) are still ahead in terms of raw accuracy. While similar effects had been observed in seq2seq transition-based parsers (Zhang et al., 2017; Liu and Zhang, 2017a), these use more complex neural architectures than transition-based implementations, including attention weights that can focus on words in prominent stack positions (Liu and Zhang, 2017a). Here, we are just using plain BiLSTMs as in the standard transition-based implementation of Shi et al. (2017). 3784 Model Shi et al. Our model Features Arc-standard Arc-eager Arc-hybrid {s2 , s1 , s0 , b0 } {s1 , s0 , b0 } {s0 , b0 } {b0 } 93.95±0.12 94.13±0.06 54.47±0.36 47.11±0.44 93.92±0.04 93.91±0.07 93.92±0.07 79.15±0.06 94.08±0.13 94.08±0.05 94.03±0.12 52.39±0.23 92.13 93.22 92.66 {b0 } Table 7: Per"
2020.coling-main.336,W03-3017,0,0.611552,"dberg and Elhadad (2010). The algorithm runs n transitions in total that attach each input word to a head, so if we ignore the second condition, all its transitions could be considered read transitions, and we would obtain a compact sequence labeling encoding where each label contains a single transition. However, the problem is that the label for a given word is not semantically linked to that word, as the parser can create arcs in any order, so the encoding can hardly be considered practical. 3.3 Coverage Algorithm L2R? Read t. k Algorithm Arc-standard (Fraser, 1989; Nivre, 2004) Arc-eager (Nivre, 2003) Arc-hybrid (Kuhlmann et al., 2011) Covington projective (Covington, 2001; Nivre, 2008) Covington non-projective (Covington, 2001; Nivre, 2008) Easy-first (Goldberg and Elhadad, 2010) Yes SH 0 Yes Yes Yes SH, RA 1 SH 1 SH 0 Spinal arc-eager (Ballesteros and Car- Yes reras, 2015) Yamada and Matsumoto (2003) No Choi and Palmer (2011) Yes Choi and McCallum (2013) Yes Yes SH Attardi (Attardi, 2006) Yes SH Planar (G´omez-Rodr´ıguez and Nivre, Yes 2010) 2-Planar (G´omez-Rodr´ıguez and Nivre, Yes 2010) SH No SH 0 Non-monotonic arc-eager (Honnibal et Yes al., 2013) Improved non-monotonic arc-eager No"
2020.coling-main.336,N15-1068,0,0.0574932,"Missing"
2020.coling-main.336,P17-2018,0,0.0373686,"Missing"
2020.coling-main.336,W05-1513,0,0.206578,"labeling parsing, but also provides a method to obtain new encodings for fast and simple sequence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings. 1 Introduction Transition-based parsing algorithms compute the syntax or semantics of sentences as graphs; for instance in the form of dependency (Fraser, 1989; Yamada and Matsumoto, 2003; Nivre et al., 2004), phrasestructure (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature r"
2020.coling-main.336,C08-1095,0,0.0595897,"Missing"
2020.coling-main.336,D17-1002,0,0.101562,"structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang and Nivre, 2011). With the adoption of deep learning, which can globally contextualize word representations, the dependency on hand-crafted features has been drastically reduced (Kiperwasser and Goldberg, 2016; Shi et al., 2017); and it has also been shown that alternative ways to attack the problem can be practical. More particularly, several parsing problems have been cast as a machine translation task, where a sequence-to-sequence (seq2seq) network maps the sentence into a string of arbitrary length that encodes a linearized graph (Vinyals et al., 2015; Li et al., 2018; Konstas et al., 2017). To a certain extent, the attention mechanism in these seq2seq models can be seen as an abstraction of the stack and buffer in transition-based parsers, where the attention weights mark the relevant words to generate the next"
2020.coling-main.336,D18-1291,0,0.0275343,"Missing"
2020.coling-main.336,D17-1174,0,0.0241769,"Missing"
2020.coling-main.336,N19-1077,1,0.891157,"Missing"
2020.coling-main.336,K16-1019,0,0.0194813,"ing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings. 1 Introduction Transition-based parsing algorithms compute the syntax or semantics of sentences as graphs; for instance in the form of dependency (Fraser, 1989; Yamada and Matsumoto, 2003; Nivre et al., 2004), phrasestructure (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang and Nivre, 2011). With the adoption of deep learning, which can globally contextualize word"
2020.coling-main.336,P15-3004,0,0.0605056,"Missing"
2020.coling-main.336,2020.lrec-1.643,0,0.0556463,"Missing"
2020.coling-main.336,W14-6104,0,0.0258972,"the right as heads (in spite of being purely left-to-right in terms of the order in which it considers dependents); and if we still apply our transformation to it, we obtain an encoding isomorphic to the relative positional encoding used in Li et al. (2018), which has been shown useful under strong machine learning models (Vacareanu et al., 2020). Table 3 shows transition-based constituency parsers with analogous information to Table 2. In this case, all of the listed parsers that do not fall into our definition of left-to-right are discontinuous constituent parsers that use swap transitions (Versley, 2014; Maier, 2015). Every continuous constituent parser that we found is covered, as well as discontinuous parsers that use other devices, like gap transitions (Coavoux et al., 2019) or set handling (Coavoux and Cohen, 2019). All of the supported constituent parsers have k = 0, following the traditional shift-reduce paradigm that only operates on nodes after reading them from the input buffer. Algorithm L2R? Read t. k Algorithm L2R? Sagae and Tsujii (2008) Yes SH 1 No Titov et al. (2009) Ribeyre et al. (2014) Tokg¨oz and Eryiˇgit (2015) Artsymenia et al. (2016) Yes No Yes Yes SH 1 SH SH 1 1 Online"
2020.coling-main.336,W18-6019,1,0.874484,"Missing"
2020.coling-main.336,N15-1040,0,0.0215074,"uence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings. 1 Introduction Transition-based parsing algorithms compute the syntax or semantics of sentences as graphs; for instance in the form of dependency (Fraser, 1989; Yamada and Matsumoto, 2003; Nivre et al., 2004), phrasestructure (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang and Nivre, 2011). With the adoption of deep learning, which can gl"
2020.coling-main.336,W03-3023,0,0.069608,"heoretical relation between transition-based parsing and sequence-labeling parsing, but also provides a method to obtain new encodings for fast and simple sequence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings. 1 Introduction Transition-based parsing algorithms compute the syntax or semantics of sentences as graphs; for instance in the form of dependency (Fraser, 1989; Yamada and Matsumoto, 2003; Nivre et al., 2004), phrasestructure (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-d"
2020.coling-main.336,P18-4013,0,0.0140013,"hineseGSD , EnglishEWT , FinnishTDT , HebrewHTB , RussianGSD , TamilTTB , UyghurUDT and WolofWTB . Appendix A shows the number of labels that our approach generates for each treebank. We also use UDpipe (Straka, 2018) to obtain data with predicted segmentation, tokenization and PoS tags. 4.2 Sequence labeling models For training, we consider two sequence labeling encoders (see Appendix B for hyper-parameters), which will produce n hidden contextualized representations hi to generate the labels: BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) We use the NCRF++ framework (Yang and Zhang, 2018) to train the models and consider two different setups: 1. A setup where the input vectors to the models are a concatenation of the pre-trained word embeddings by Ginter et al. (2017), a second word vector generated by a char-LSTM layer, which is trained together with the rest of the network, and PoS tag vectors. 2. Same as 1, but without PoS tag vectors. 2 With buffer-based we refer to a swap transition that puts back a word from the stack to the buffer, contrary to stack-based swap that changes the order of the words in the stack, respecting the left-to-right condition. 3782 Split Encoding d"
2020.coling-main.336,P17-1160,1,0.898923,"Missing"
2020.coling-main.336,W09-3825,0,0.190533,"lso provides a method to obtain new encodings for fast and simple sequence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings. 1 Introduction Transition-based parsing algorithms compute the syntax or semantics of sentences as graphs; for instance in the form of dependency (Fraser, 1989; Yamada and Matsumoto, 2003; Nivre et al., 2004), phrasestructure (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang a"
2020.coling-main.336,P11-2033,0,0.0471351,"k, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang and Nivre, 2011). With the adoption of deep learning, which can globally contextualize word representations, the dependency on hand-crafted features has been drastically reduced (Kiperwasser and Goldberg, 2016; Shi et al., 2017); and it has also been shown that alternative ways to attack the problem can be practical. More particularly, several parsing problems have been cast as a machine translation task, where a sequence-to-sequence (seq2seq) network maps the sentence into a string of arbitrary length that encodes a linearized graph (Vinyals et al., 2015; Li et al., 2018; Konstas et al., 2017). To a certain"
2020.coling-main.336,P16-1040,0,0.0211702,"Missing"
2020.coling-main.336,D17-1175,0,0.0216752,"ls of sequences of transitions can be learned with only one positional feature (our setup just assigns a sequence of labels beginning with a SH to each word shifted from the input, i.e., it only uses the first buffer word b0 , and has access to no explicit representation of stack elements at all). Thus, this shows that our multi-transition labels seem to be learnable with less data than individual transitions in transition systems, although the latter (with suitable features) are still ahead in terms of raw accuracy. While similar effects had been observed in seq2seq transition-based parsers (Zhang et al., 2017; Liu and Zhang, 2017a), these use more complex neural architectures than transition-based implementations, including attention weights that can focus on words in prominent stack positions (Liu and Zhang, 2017a). Here, we are just using plain BiLSTMs as in the standard transition-based implementation of Shi et al. (2017). 3784 Model Shi et al. Our model Features Arc-standard Arc-eager Arc-hybrid {s2 , s1 , s0 , b0 } {s1 , s0 , b0 } {s0 , b0 } {b0 } 93.95±0.12 94.13±0.06 54.47±0.36 47.11±0.44 93.92±0.04 93.91±0.07 93.92±0.07 79.15±0.06 94.08±0.13 94.08±0.05 94.03±0.12 52.39±0.23 92.13 93.22 92."
2020.coling-main.336,P13-1043,0,0.144278,"o obtain new encodings for fast and simple sequence labeling parsing from the many existing transition-based parsers for different formalisms. Applying it to dependency parsing, we implement sequence labeling versions of four algorithms, showing that they are learnable and obtain comparable performance to existing encodings. 1 Introduction Transition-based parsing algorithms compute the syntax or semantics of sentences as graphs; for instance in the form of dependency (Fraser, 1989; Yamada and Matsumoto, 2003; Nivre et al., 2004), phrasestructure (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013) or meaning representations (Wang et al., 2015; Swayamdipta et al., 2016; Hershcovich et al., 2018). These algorithms define an abstract state machine where each state (configuration) holds a structured representation, as well as auxiliary data structures (often, but not always, a buffer and a stack of tokens). Shift-reduce actions (transitions) are defined to move the system between states until a full parse is found. The transition to take at each state was traditionally predicted by data-driven classifiers based on local decisions and rich feature representations (Zhang and Nivre, 2011). Wi"
2020.coling-main.339,C16-1012,0,0.0190651,"y et al. (2006) used a combination of a k-best parser and a discriminative reranker in order to increase their training set size with automatically parsed sentences. Later McDonald et al. (2011) and Wróblewska and Przepiórkowski (2012) proposed various projection techniques to create annotated data for languages that did not have any, relying to different extents on parallel corpora. Sentence morphing itself is not new either. Wang and Eisner (2018) and Rasooli and Collins (2019) proposed to reorder sentences from a source language in order to better match the word order of a target language. Aufrant et al. (2016) proposed not only to reorder words but also to delete some such as determiners when they are irrelevant for the target language. However, these methods rely on external resources (parse trees in other languages and/or unparsed sentences from the same language) to create new data. And creating new examples this way introduces noise in the training data. In order to avoid this kind of problems, one can directly try to expand the little annotated data available. Şahin and Steedman (2018) proposed to transform gold dependency trees directly by rotating their core arguments and deleting subtrees i"
2020.coling-main.339,K17-3002,0,0.0153286,"trong and therefore limit the number of sentences that can be generated, in a second experiment, we look at the impact of relaxing those constraints. Eventually, as the number of generated sentences gets big enough, it becomes possible to generate a separated development set for validating the trained models, so we also look at the impact of using an artificial development set. We run each experiment 8 times (one for each sample). The reported results are LAS scores on the original dev sets averaged over those 8 runs. For all the experiments, we use an implementation of the biaffine parser of Dozat et al. (2017) available on Github4 . We do not use pretrained word embeddings, so word and character representations are learned from scratch alongside the rest of the model. In this reduced data setting, we use 100 dimensions for word embeddings and 50 for characters. We only consider the task of parsing and use gold segmentation and UPOS tags. 5.2.1 Crop, Rotate, Nonce and Swap Table 4 reports the average parsing results obtained on the development set using the base sampled training sets and augmented sets via cropping, rotating, nonce sentences and swapping. For this experiment, we use the maximum numb"
2020.coling-main.339,N18-1108,0,0.195576,"n POS tagging. It was thus unclear how their data augmentation method would behave on dependency parsing, where the data generation process and the success measure both look at the same structure. Later Vania et al. (2019) evaluated the data augmentation technique of Şahin and Steedman (2018) directly on dependency parsing as part of a wider investigation on methods for parsing low-resourced languages. Their results showed that creating sentences by morphing trees indeed helps parsers. They also considered replacing words by words with agreeing morphology to create new “nonce” sentences after Gulordava et al. (2018) and looked at cross-lingual training, which is also beneficial. In this paper, we depart from their work by focusing on gold data morphing only. We propose a new data augmentation method that creates new “gold” parses from existing ones by swapping compatible subtrees. We also propose an experimental method for mimicking a low-resource setting using high resource languages. 3 Data Augmentation In this section we present a language-agnostic method to automatically generate high-quality dependency annotated data from a small amount of (manually) annotated sentences. Beside being as languageagno"
2020.coling-main.339,N16-1121,0,0.0193385,"he number of sentences. This is important since contrary to previous tree morphing techniques, the number of sentences created by tree swapping is potentially unbounded. Then, we have shown that relaxing the strong swapping constraints on a per language basis further improves the results. But that the language/constraints relation is not necessarily clear. Finally, we saw that despite being useful for training parsers, the created sentences are not diverse enough to be useful for model validation. Previous works have demonstrated the possibility of training parsers with incomplete annotation (Lacroix et al., 2016). As a few generated sentences may be odd sounding or slightly ungrammatical, it would be interesting to see how parsers fare when trained with sound trees over ungrammatical sentences. We keep it as a future work. We also need to further investigate the three-way interaction between languages, augmentation techniques and parsing algorithms, as apparently not all augmentation techniques fare as well for all languages. Mixing data augmentation policies might also have a positive impact. More generally, it would be interesting to see how far a parser can go with only a handful of annotated sente"
2020.coling-main.339,N06-1020,0,0.351194,"same basic inputs. 1 Introduction Data sparsity has been a problem since the beginning of natural language processing. Neural networks have not solved it and have made it even more visible with their hunger for data. Hence a revived interest in data augmentation, since artificially created annotation can prove very useful to tackle the lack of manually annotated training data. Many approaches have been proposed to perform data augmentation. Some rely on external resources, such as unannotated raw text in order to iteratively increase their training data with automatically annotated examples (McClosky et al., 2006; Yu and Bohnet, 2015). However, this is an error-prone method and useful annotations must be separated from harmful ones. Other proposals instead rely solely on available annotated data in order to generate new examples (Şahin and Steedman, 2018). In this paper, we present a language-agnostic approach to the automatic generation of dependencyannotated sentences. Our method essentially swaps compatible subtrees from different sentences in order to generate new annotated sentences. By enforcing a number of constraints on the subtrees to be swapped, we avoid to generate too ungrammatical sentenc"
2020.coling-main.339,D11-1006,0,0.0379991,"8–3830 Barcelona, Spain (Online), December 8-13, 2020 How to analyse a language that at the same time is an isolate and uses its own writing system (e.g. Japanese, Korean) at least in the data if we do not have typological information? And, how much can we learn from a very limited amount of training data? 2 Related Work Automatically generated annotation has been used for dependency parsing for at least two decades. McClosky et al. (2006) used a combination of a k-best parser and a discriminative reranker in order to increase their training set size with automatically parsed sentences. Later McDonald et al. (2011) and Wróblewska and Przepiórkowski (2012) proposed various projection techniques to create annotated data for languages that did not have any, relying to different extents on parallel corpora. Sentence morphing itself is not new either. Wang and Eisner (2018) and Rasooli and Collins (2019) proposed to reorder sentences from a source language in order to better match the word order of a target language. Aufrant et al. (2016) proposed not only to reorder words but also to delete some such as determiners when they are irrelevant for the target language. However, these methods rely on external res"
2020.coling-main.339,N19-1385,0,0.0164278,"unt of training data? 2 Related Work Automatically generated annotation has been used for dependency parsing for at least two decades. McClosky et al. (2006) used a combination of a k-best parser and a discriminative reranker in order to increase their training set size with automatically parsed sentences. Later McDonald et al. (2011) and Wróblewska and Przepiórkowski (2012) proposed various projection techniques to create annotated data for languages that did not have any, relying to different extents on parallel corpora. Sentence morphing itself is not new either. Wang and Eisner (2018) and Rasooli and Collins (2019) proposed to reorder sentences from a source language in order to better match the word order of a target language. Aufrant et al. (2016) proposed not only to reorder words but also to delete some such as determiners when they are irrelevant for the target language. However, these methods rely on external resources (parse trees in other languages and/or unparsed sentences from the same language) to create new data. And creating new examples this way introduces noise in the training data. In order to avoid this kind of problems, one can directly try to expand the little annotated data available"
2020.coling-main.339,D18-1545,0,0.22359,"in data augmentation, since artificially created annotation can prove very useful to tackle the lack of manually annotated training data. Many approaches have been proposed to perform data augmentation. Some rely on external resources, such as unannotated raw text in order to iteratively increase their training data with automatically annotated examples (McClosky et al., 2006; Yu and Bohnet, 2015). However, this is an error-prone method and useful annotations must be separated from harmful ones. Other proposals instead rely solely on available annotated data in order to generate new examples (Şahin and Steedman, 2018). In this paper, we present a language-agnostic approach to the automatic generation of dependencyannotated sentences. Our method essentially swaps compatible subtrees from different sentences in order to generate new annotated sentences. By enforcing a number of constraints on the subtrees to be swapped, we avoid to generate too ungrammatical sentences. Furthermore, contrary to previous work that kept the syntactic structure of sentences or impoverished it, our method injects structures from other sentences, so it can introduce more syntactic complexity in the generated sentences. In order to"
2020.coling-main.339,D19-1102,0,0.0176914,"raining POS taggers for low-resource languages. While similar in idea to their work, justifying experimental comparison, our work is different in two important aspects. First, their data generation methods and ours differ both in the grammaticality constraints and in the shape of the generated sentences. In addition, while the data generation process morphs dependency trees, they evaluated it on POS tagging. It was thus unclear how their data augmentation method would behave on dependency parsing, where the data generation process and the success measure both look at the same structure. Later Vania et al. (2019) evaluated the data augmentation technique of Şahin and Steedman (2018) directly on dependency parsing as part of a wider investigation on methods for parsing low-resourced languages. Their results showed that creating sentences by morphing trees indeed helps parsers. They also considered replacing words by words with agreeing morphology to create new “nonce” sentences after Gulordava et al. (2018) and looked at cross-lingual training, which is also beneficial. In this paper, we depart from their work by focusing on gold data morphing only. We propose a new data augmentation method that create"
2020.coling-main.339,D18-1163,0,0.0141374,"arn from a very limited amount of training data? 2 Related Work Automatically generated annotation has been used for dependency parsing for at least two decades. McClosky et al. (2006) used a combination of a k-best parser and a discriminative reranker in order to increase their training set size with automatically parsed sentences. Later McDonald et al. (2011) and Wróblewska and Przepiórkowski (2012) proposed various projection techniques to create annotated data for languages that did not have any, relying to different extents on parallel corpora. Sentence morphing itself is not new either. Wang and Eisner (2018) and Rasooli and Collins (2019) proposed to reorder sentences from a source language in order to better match the word order of a target language. Aufrant et al. (2016) proposed not only to reorder words but also to delete some such as determiners when they are irrelevant for the target language. However, these methods rely on external resources (parse trees in other languages and/or unparsed sentences from the same language) to create new data. And creating new examples this way introduces noise in the training data. In order to avoid this kind of problems, one can directly try to expand the"
2020.coling-main.339,W15-2138,0,0.0235315,"ntroduction Data sparsity has been a problem since the beginning of natural language processing. Neural networks have not solved it and have made it even more visible with their hunger for data. Hence a revived interest in data augmentation, since artificially created annotation can prove very useful to tackle the lack of manually annotated training data. Many approaches have been proposed to perform data augmentation. Some rely on external resources, such as unannotated raw text in order to iteratively increase their training data with automatically annotated examples (McClosky et al., 2006; Yu and Bohnet, 2015). However, this is an error-prone method and useful annotations must be separated from harmful ones. Other proposals instead rely solely on available annotated data in order to generate new examples (Şahin and Steedman, 2018). In this paper, we present a language-agnostic approach to the automatic generation of dependencyannotated sentences. Our method essentially swaps compatible subtrees from different sentences in order to generate new annotated sentences. By enforcing a number of constraints on the subtrees to be swapped, we avoid to generate too ungrammatical sentences. Furthermore, contr"
2020.conll-1.6,2020.iwpt-1.2,1,0.778961,"Missing"
2020.conll-1.6,D15-1041,0,0.0892167,"uracy the most, highlighting some potentially meaningful linguistic facets of the problem. 1 Introduction Part-of-speech (POS) tags and dependency parsing have formed a long-standing union in NLP. But equally long-standing has been the question of its efficacy. Prior to the prevalence of deep learning in NLP, they were shown to be useful for syntactic disambiguation in certain contexts (Voutilainen, 1998; Dalrymple, 2006; Alfared and Béchet, 2012). However, for neural network implementations, especially those which utilise character embeddings, POS tags have been shown to be much less useful (Ballesteros et al., 2015; de Lhoneux et al., 2017). Others have found that POS tags can still have a positive impact when using character representations given that the accuracy of the predicted POS tags used is sufficiently high (Dozat et al., 2017). Smith et al. (2018) undertook a systematic study of the impact of features for Universal Dependency (UD) parsing and found that using universal POS (UPOS) tags does still offer a marginal improvement for their transition-based neural parser. The use of fine-grained POS tags still seems to garner noticeable improvements (Ammar et al., 2016). Latterly, POS tags have been"
2020.conll-1.6,W19-8003,0,0.0586982,"Missing"
2020.conll-1.6,K17-3002,0,0.138519,"the question of its efficacy. Prior to the prevalence of deep learning in NLP, they were shown to be useful for syntactic disambiguation in certain contexts (Voutilainen, 1998; Dalrymple, 2006; Alfared and Béchet, 2012). However, for neural network implementations, especially those which utilise character embeddings, POS tags have been shown to be much less useful (Ballesteros et al., 2015; de Lhoneux et al., 2017). Others have found that POS tags can still have a positive impact when using character representations given that the accuracy of the predicted POS tags used is sufficiently high (Dozat et al., 2017). Smith et al. (2018) undertook a systematic study of the impact of features for Universal Dependency (UD) parsing and found that using universal POS (UPOS) tags does still offer a marginal improvement for their transition-based neural parser. The use of fine-grained POS tags still seems to garner noticeable improvements (Ammar et al., 2016). Latterly, POS tags have been commonly utilised implicitly for neural network parsers in multilearning frameworks where they can be leveraged without the cost of error-propagation (Zhang and Contribution We analyse the effect UPOS accuracy has on two depen"
2020.conll-1.6,L18-1550,0,0.0157781,"versal POS tags as POS tags rather than UPOS tags for sake of efficiency. 69 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 69–96 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Data We use the same subset of UD v2.4 treebanks (Nivre et al., 2019) as Anderson and GómezRodríguez (2020): Ancient Greek Perseus, Chinese GSD, English EWT, Finnish TDT, Hebrew HTB, Russian GSD, Tamil TTB, Uyghur UDT, and Wolof WTB. We used fastText word embeddings for each language except for Ancient Greek and Wolof (Grave et al., 2018). For Ancient Greek we use embeddings from Ginter et al. (2017) and for Wolof those from Heinzerling and Strube (2018). When necessary, we reduced the dimensions to 100 using the algorithm of Raunak (2017). 2.1 chosen to control the contribution from each feature, but it is obviously feasible that optimising these contributions could result in different absolute results. However, keeping these static unless purposefully changing them for controlled input means we can make relative comparisons. Experiment 1 We trained parsers for each treebank with gold tags and with predicted tags using a subs"
2020.conll-1.6,L18-1473,0,0.110469,"Anderson Carlos Gómez-Rodríguez Universidade da Coruña, CITIC FASTPARSE Lab, LyS Research Group, Departamento de Ciencias de la Computación y Tecnologías de la Información Campus Elviña, s/n, 15071 A Coruña, Spain {m.anderson,carlos.gomez}@udc.es Abstract Weiss, 2016; Yang et al., 2017; Li et al., 2018; Nguyen and Verspoor, 2018; Zhang et al., 2020). Beyond multi-learning systems, Strzyz et al. (2019) introduced dependency parsing as sequence labelling by encoding dependencies using relative positions of UPOS tags, thus explicitly requiring them at runtime. We follow the work of Smith et al. (2018) and evaluate the interplay of word embeddings, character embeddings, and POS tags as features for two modern parsers, one a graph-based parser, Biaffine, and the other a transition-based parser, UUParser (Dozat and Manning, 2017; Smith et al., 2018). Similar to Zhang et al. (2020), we focus on the contribution of POS tags but evaluate UPOS tags. We present an analysis on the effect UPOS accuracy has on parsing performance. Results suggest that leveraging UPOS tags as features for neural parsers requires a prohibitively high tagging accuracy and that the use of gold tags offers a non-linear in"
2020.conll-1.6,K17-3022,0,0.326695,"Missing"
2020.conll-1.6,K18-2006,0,0.0450224,"Missing"
2020.conll-1.6,P16-1147,0,0.0239236,"Missing"
2020.conll-1.6,K18-2008,0,0.034902,"Missing"
2020.conll-1.6,K18-2011,0,0.0973524,"Missing"
2020.conll-1.6,D18-1291,0,0.597272,"Missing"
2020.conll-1.6,N19-1077,1,0.905193,"Missing"
2020.conll-1.6,W98-1303,0,0.632124,"igh tagging accuracy and that the use of gold tags offers a non-linear increase in performance, suggesting some sort of exceptionality. We also investigate what aspects of predicted UPOS tags impact parsing accuracy the most, highlighting some potentially meaningful linguistic facets of the problem. 1 Introduction Part-of-speech (POS) tags and dependency parsing have formed a long-standing union in NLP. But equally long-standing has been the question of its efficacy. Prior to the prevalence of deep learning in NLP, they were shown to be useful for syntactic disambiguation in certain contexts (Voutilainen, 1998; Dalrymple, 2006; Alfared and Béchet, 2012). However, for neural network implementations, especially those which utilise character embeddings, POS tags have been shown to be much less useful (Ballesteros et al., 2015; de Lhoneux et al., 2017). Others have found that POS tags can still have a positive impact when using character representations given that the accuracy of the predicted POS tags used is sufficiently high (Dozat et al., 2017). Smith et al. (2018) undertook a systematic study of the impact of features for Universal Dependency (UD) parsing and found that using universal POS (UPOS)"
2020.conll-1.6,P18-4013,0,0.0483422,"Missing"
2020.emnlp-main.221,N19-1018,0,0.529003,"nuities happen in languages that exhibit free word order such as German or Guugu Yimidhirr (Haviland, 1979; Johnson, 1985), but also in those with high rigidity, e.g. English, whose grammar allows certain discontinuous expressions, such as wh-movement or extraposition (Evang and Kallmeyer, 2011). This makes discontinuous parsing a core computational linguistics problem that affects a wide spectrum of languages. There are different paradigms for discontinuous phrase-structure parsing, such as chart-based parsers (Maier, 2010; Corro, 2020), transitionbased algorithms (Coavoux and Crabb´e, 2017; Coavoux and Cohen, 2019) or reductions to a problem of a different nature, such as dependency parsing (Hall and Nivre, 2008; Fern´andez-Gonz´alez and Martins, 2015). However, many of these approaches come either at a high complexity or low https://github.com/aghie/disco2labels ADV ADV Noch nie VAFIN PPER ADV ADV habe ich so viel VVPP $. gewählt . (Yet) (never) (have) (chosen) . (I) (so) (much) Figure 1: An example of a German sentence exhibiting discontinuous structures, extracted from the NEGRA treebank (Skut et al., 1997). A valid English translation is: ‘Never before I have chosen so much.’ Introduction 1 AVP AVP"
2020.emnlp-main.221,E17-1118,0,0.145435,"Missing"
2020.emnlp-main.221,Q19-1005,0,0.239929,"Missing"
2020.emnlp-main.221,W16-0906,0,0.385135,"Missing"
2020.emnlp-main.221,P09-1040,0,0.0675671,"Missing"
2020.emnlp-main.221,D17-1174,0,0.547677,"Missing"
2020.emnlp-main.221,2020.iwpt-1.12,0,0.572411,"Missing"
2020.emnlp-main.221,N19-1077,1,0.885532,"Missing"
2020.emnlp-main.221,W16-0907,0,0.34734,"to build a tree with in 4n-2 transitions, given a sentence of length n. A middle ground between explicit constituent parsing algorithms and this paper is the work based on transformations. For instance, Hall and Nivre (2008) convert constituent trees into a nonlinguistic dependency representation that is learned by a transition-based dependency parser, to then map its output back to a constituent tree. A similar approach is taken by Fern´andez-Gonz´alez and Martins (2015), but they proposed a more compact representation that leads to a much reduced set of output labels. Other authors such as Versley (2016) propose a two-step approach that approximates discontinuous structure trees by parsing context-free grammars with generative probabilistic models and transforming them to discontinuous ones. Corro et al. (2017) cast discontinuous phrase-structure parsing into a framework that jointly performs supertagging and non-projective dependency parsing by a reduction to the Generalized Maximum Spanning Arborescence problem (Myung et al., 1995). The recent work by Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020a) can be also framed within this paradigm. They essentially adapt the work by Fern´andez-Gon"
2020.emnlp-main.221,P87-1015,0,0.754621,"Missing"
2020.emnlp-main.221,N19-1341,1,0.82105,"nt pi that encodes τ (i). However, as illustrated in Appendix A.3, an analysis on the corpora used in this work shows that the presence of unseen labels in the test set is virtually zero. 5 Sequence labeling frameworks To test whether these encoding functions are learnable by parametrizable functions, we consider different sequence labeling architectures. We will be denoting by ENCODER a generic, contextualized encoder that for every word wi generates a hidden vector hi conditioned on the sentence, i.e. ENCODER (wi |w)=hi . We use a hard-sharing multitask learning architecture (Caruana, 1997; Vilares et al., 2019) to map every hi to four 1-layered feedforward networks, followed by softmaxes, that predict each of the components of li0 . Each task’s loss is optimized using categorical cross-entropy P 0 Lt = − log(P P(li |hi )) and the final loss computed as L = t∈T asks Lt . We test four EN CODERs, which we briefly review but treat as black boxes. Their number of parameters and the training hyper-parameters are listed in Appendix A.4. Transducers without pretraining We try (i) a 2-stacked BiLSTM (Hochreiter and Schmidhuber, 1997; Yang and Zhang, 2018) where the generation of hi is conditioned on the left"
2020.emnlp-main.221,P19-1529,0,0.0260326,"GHz. For GPU experiments, we relied on a single GeForce GTX 1080, except for the BERT-large experiments, where due to memory requirements we required a Tesla P40. (e.g. chart-based versus sequence labeling). Finally, Table 4 details the discontinuous performance of our best performing models. Discussion on other applications It is worth noting that while we focused on parsing as sequence labeling, encoding syntactic trees as labels is useful to straightforwardly feed syntactic information to downstream models, even if the trees themselves come from a non-sequence-labeling parser. For example, Wang et al. (2019) use the sequence labeling encoding of G´omez-Rodr´ıguez and Vilares (2018) to provide syntactic information to a semantic role labeling model. Apart from providing fast and accurate parsers, our encodings can be used to do the same with discontinuous syntax. 7 Conclusion We reduced discontinuous parsing to sequence labeling. The key contribution consisted in predicting a continuous tree with a rearrangement of the leaf nodes to shape discontinuities, and defining various ways to encode such a rearrangement as a sequence of labels associated to each word, taking advantage of the fact that in p"
2020.emnlp-main.221,P18-4013,0,0.0221966,"aring multitask learning architecture (Caruana, 1997; Vilares et al., 2019) to map every hi to four 1-layered feedforward networks, followed by softmaxes, that predict each of the components of li0 . Each task’s loss is optimized using categorical cross-entropy P 0 Lt = − log(P P(li |hi )) and the final loss computed as L = t∈T asks Lt . We test four EN CODERs, which we briefly review but treat as black boxes. Their number of parameters and the training hyper-parameters are listed in Appendix A.4. Transducers without pretraining We try (i) a 2-stacked BiLSTM (Hochreiter and Schmidhuber, 1997; Yang and Zhang, 2018) where the generation of hi is conditioned on the left and right context. (ii) We also explore a Transformer encoder (Vaswani et al., 2017) with 6 layers and 8 heads. The motivation is that we believe that the multihead attention mechanism, in which a word attends to every other word in the sentence, together with positional embeddings, could be beneficial to detect discontinuities. In practice, we found training these transformer encoders harder than training BiLSTMs, and that obtaining a competitive performance required larger models, smaller learning rates, and more epochs (see also Appendi"
2020.emnlp-main.221,P13-1043,0,0.0771268,"Missing"
2020.gamnlp-1.9,reese-etal-2010-wikicorpus,0,0.0528107,"Missing"
2020.iwpt-1.2,K16-1029,0,0.053732,"Missing"
2020.iwpt-1.2,P19-1580,0,0.0618449,"Missing"
2020.iwpt-1.2,K18-2011,0,0.120064,"Missing"
2020.iwpt-1.2,D18-1291,0,0.14095,"Missing"
2020.iwpt-1.2,C18-2028,0,0.0384436,"Missing"
2020.iwpt-1.2,P19-1355,0,0.0648367,"Missing"
2020.iwpt-1.2,N19-1077,1,0.898374,"Missing"
2020.iwpt-1.2,D18-1244,0,0.0299684,"nt of anthropogenic climate change. In conjunction with this push for greener AI, NLP practitioners have turned to the problem of developing models that are not only accurate but also efficient, so as to make them more readily 2 Dependency parsing Dependency parsing is a core NLP task where the syntactic relations of words in a sentence are encoded as a well-formed tree with each word attached to a head via a labelled arc. Figure 1 shows an example of such a tree. The syntactic information attained from parsers has been shown to benefit a number of other NLP tasks such as relation extraction (Zhang et al., 2018), machine translation (Chen et al., 2018), and sentiment analysis (Poria et al., 2014; Vilares et al., 2017). 2 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 2–13 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics ROOT NSUBJ a fast and accurate parser, as described above, and is used as the parser architecture for our experiments. More details of the system can be found in Dozat and Manning (2017). NMOD CASE DET The OBJ DET son of the DET cat hunts the rat 3 Model compression has been under considerati"
2020.iwpt-1.2,P19-1159,0,0.0356663,"Missing"
2020.iwpt-1.2,P19-1230,0,0.017587,"dent. Teacherstudent distillation has successfully been exploited in NLP for machine translation, language modelling, and speech recognition (Kim and Rush, Figure 1: Dependency tree example. 2.1 Network compression Current parser performance Table 1 shows performance details of current stateof-the-art dependency parsers on the English Penn Treebank (PTB) with predicted POS tags from the Stanford POS tagger (Marcus and Marcinkiewicz, 1993; Toutanova et al., 2003). The Biaffine parser of Dozat and Manning (2017) offers the best tradeoff between accuracy and parsing speed with the HPSG parser of Zhou and Zhao (2019) achieving the absolute best reported accuracy but with a reported parsing speed of roughly one third of the Biaffine’s parsing speed. It is important to note that direct comparisons between systems with respect to parsing speed are wrought with compounding variables, e.g. different GPUs or CPUs used, different number of CPU cores, different batch sizes, and often hardware is not even reported. We therefore run a subset of parsers locally to achieve speed measurements in a controlled environment, also shown in Table 1: we compare a PyTorch implentation of the Biaffine parser (which runs more t"
2020.iwpt-1.2,N03-1033,0,0.217611,"t al., 2016). Teacherstudent distillation is the process of taking a large network, the teacher, and transferring its knowledge to a smaller network, the student. Teacherstudent distillation has successfully been exploited in NLP for machine translation, language modelling, and speech recognition (Kim and Rush, Figure 1: Dependency tree example. 2.1 Network compression Current parser performance Table 1 shows performance details of current stateof-the-art dependency parsers on the English Penn Treebank (PTB) with predicted POS tags from the Stanford POS tagger (Marcus and Marcinkiewicz, 1993; Toutanova et al., 2003). The Biaffine parser of Dozat and Manning (2017) offers the best tradeoff between accuracy and parsing speed with the HPSG parser of Zhou and Zhao (2019) achieving the absolute best reported accuracy but with a reported parsing speed of roughly one third of the Biaffine’s parsing speed. It is important to note that direct comparisons between systems with respect to parsing speed are wrought with compounding variables, e.g. different GPUs or CPUs used, different number of CPU cores, different batch sizes, and often hardware is not even reported. We therefore run a subset of parsers locally to"
2020.iwpt-1.2,N19-1341,0,0.054362,"Missing"
2020.iwpt-1.20,D16-1139,0,0.0557009,"Missing"
2020.iwpt-1.20,D16-1180,0,0.0606544,"Missing"
2020.iwpt-1.20,C18-2028,0,0.0623222,"Missing"
2020.lrec-1.499,E17-1088,0,0.305083,"ywords: Less-Resourced/Endangered Languages, Multilinguality 1. Introduction Cross-lingual embeddings are continuous encodings of words or tokens from many languages into the same vector space. This implies, in principle, that words with similar semantics are represented by similar vectors irrespective of the language they come from. Previous work has shown that these continuous representations are useful to transfer knowledge from resource-rich languages, mainly English, to many other low-resource languages both in widespread use (Ruder et al., 2018; Doval et al., 2019b) and also threatened (Adams et al., 2017). An NLP task where knowledge transfer has been of particular interest is machine translation (Zoph et al., 2016; Gu et al., 2018), but there is also work covering many other of the main tasks in the field: PoS tagging (Fang and Cohn, 2017), dependency parsing (Kulmizev, 2018), or sentiment analysis (Le et al., 2016). In this context, to our knowledge, no previous work has covered the case of Turkic languages, a family of languages estimated to be spoken by over 170 million speakers (Menz, 2016) with little to no NLP resources available. Even though the family comprises dozens of languages, of"
2020.lrec-1.499,Q19-1036,0,0.038961,"Missing"
2020.lrec-1.499,D16-1250,0,0.0290858,"to English language in our case, that remains fixed throughout the process. The most widely-used algorithms for this, such as VecMap (Artetxe et al., 2018) and MUSE (Conneau et al., 2018), rely on orthogonal transformations learned on bilingual dictionaries that preserve the internal structure of the monolingual spaces while they become integrated into the same cross-lingual space (that of the reference language). Although several authors show that maintaining the internal monolingual structures avoids overfitting to the crosslingual objective, which would degrade the monolingual performance (Artetxe et al., 2016), Doval et al. (2019a) have found that applying a simple post-processing step that modifies those structures improves the cross-lingual performance of the models without compromising on the monolingual side; quite the contrary, in fact, as this step improves the monolingual performance. This post-processing step involves taking already aligned vector spaces using alignment models that keep the internal structure of the monolingual spaces, then applies an additional transformation that map the vector representations of both word and its translation onto their average, thereby creating a cross-l"
2020.lrec-1.499,J08-3003,0,0.129541,"Missing"
2020.lrec-1.499,eryigit-2012-impact,0,0.0248297,"nsight that low-resource languages can benefit more when the reference language is a related one and resource-richer. Furthermore, our sentiment analysis experiment shows that the best aligned word embeddings for Uzbek outperform their initial counterpart. The resources obtained in the course of the present work are freely available online.1 2. Related Work Most of the previous work on Turkic languages has been done exclusively on Turkish. In this case, it has mainly focused on its complex morphology, giving rise to analyzers and disambiguation models (Yuret and T¨ure, 2006; Sak et al., 2011; Eryigit, 2012; Aky¨urek et al., 2019). Nonetheless, other common NLP tasks have been also tackled for Turkish: PoS tagging (Dincer et al., 2008; Can et al., 2016), dependency parsing (Eryi˘git et al., 2008; Eryi˘git et al., 2018), or named entity recognition (Yeniterzi, 2011; Seker and Eryigit, 2017). For a more general overview of the matter, Oflazer and Sarac¸lar (2018) review the state of Turkish NLP. Although not many, there have been works for other languages rather than Turkish: for Uzbek (Li et al., 2016; Matlatipov and Vetulani, 2009; Kuriyozov et al., 2019), for Azeri (Fatullayev et al., 2008a; Fa"
2020.lrec-1.499,P17-2093,0,0.1487,"imilar semantics are represented by similar vectors irrespective of the language they come from. Previous work has shown that these continuous representations are useful to transfer knowledge from resource-rich languages, mainly English, to many other low-resource languages both in widespread use (Ruder et al., 2018; Doval et al., 2019b) and also threatened (Adams et al., 2017). An NLP task where knowledge transfer has been of particular interest is machine translation (Zoph et al., 2016; Gu et al., 2018), but there is also work covering many other of the main tasks in the field: PoS tagging (Fang and Cohn, 2017), dependency parsing (Kulmizev, 2018), or sentiment analysis (Le et al., 2016). In this context, to our knowledge, no previous work has covered the case of Turkic languages, a family of languages estimated to be spoken by over 170 million speakers (Menz, 2016) with little to no NLP resources available. Even though the family comprises dozens of languages, of which at least five have over ten million speakers (Menz, 2016), only Turkish has a reasonable amount of publicly available resources that had been already used in low-resource scenarios. For the rest of them, we had to resort to extractio"
2020.lrec-1.499,E14-1049,0,0.031622,"he low-resource language to integrate it into the model without needing full retraining. Since word embeddings follow the same narrative as those internal representations (in fact, they are usually modelled as such), we can think of multilingual, or cross-lingual, embeddings as a vehicle to transfer knowledge between languages. In this case, we consider the procedure that first obtains monolingual embeddings and then aligns them into a shared space. Starting from the work of Mikolov et al. 1 https://github.com/elmurod1202/ crosLingWordEmbTurk (2013), this approach has seen many contributions (Faruqui and Dyer, 2014; Lazaridou et al., 2015; Zhang et al., 2016; Smith et al., 2017). Notable methods recently developed are VecMap (Artetxe et al., 2018), MUSE (Conneau et al., 2018), and then Meemi (Doval et al., 2019a) as a way to further improve the integration of the cross-lingual space. Regarding low-resource languages, some analysis papers such as (Ruder et al., 2018; Søgaard et al., 2018; Doval et al., 2019b; Glavas et al., 2019) study the viability of these methods in adverse scenarios, and others even resort to endangered languages, such as (Adams et al., 2017). 3. 3.1. Methodology Cross-lingual embedd"
2020.lrec-1.499,2008.eamt-1.7,0,0.0474562,"et al., 2011; Eryigit, 2012; Aky¨urek et al., 2019). Nonetheless, other common NLP tasks have been also tackled for Turkish: PoS tagging (Dincer et al., 2008; Can et al., 2016), dependency parsing (Eryi˘git et al., 2008; Eryi˘git et al., 2018), or named entity recognition (Yeniterzi, 2011; Seker and Eryigit, 2017). For a more general overview of the matter, Oflazer and Sarac¸lar (2018) review the state of Turkish NLP. Although not many, there have been works for other languages rather than Turkish: for Uzbek (Li et al., 2016; Matlatipov and Vetulani, 2009; Kuriyozov et al., 2019), for Azeri (Fatullayev et al., 2008a; Fatullayev et al., 2008b; Abbasov et al., 2010), for Kazakh (Salimzyanov et al., 2013; Sakenovich and Zharmagambetov, 2016; Yergesh et al., 2017) and for Kyrgyz (Washington et al., 2012; G¨ormez et al., 2011). Knowledge transfer between languages, and mostly between resource-rich and resource-scarce languages, has drawn much attention in the past years. This might be attributed to the rise of neural networks as the chosen machine learning technique to tackle all sorts of tasks in NLP: machine translation (Zoph et al., 2016; Gu et al., 2018), PoS tagging (Fang and Cohn, 2017), dependency par"
2020.lrec-1.499,P19-1070,0,0.0380949,"Missing"
2020.lrec-1.499,L18-1550,0,0.0795785,"n one token (word). The last step was to split the resulting dictionaries into training and test sets, by randomly choosing 500 words from each language for the test sets, and using the rest as training sets. Table 3 reports statistics about the size of the dictionaries obtained in this way. embeddings, we decided to focus on the scripts that are prevailing in recent texts: Latin for Turkish, Uzbek and Azerbaijani, and Cyrillic (with extensions) for Kazakh and Kyrgyz. 3.2.1. Obtaining Word Embeddings First, we obtained preexisting fastText pre-trained word embeddings for our target languages (Grave et al., 2018). Apart from that, considering the fact that all target languages are low-resource and there is always room for improvement, we also decided to train our own word embeddings and use them for experiments. For that purpose, we obtained a collection of large corpora for Turkic languages (Baisa et al., 2012) and trained new fastText embeddings. Size statistics for our obtained word embeddings are shown in Table 1. We shall be using both embeddings in our experiments to evaluate their performance. 3.2.2. Obtaining Dictionaries Proper dictionaries that are both free to use and adequate in terms of s"
2020.lrec-1.499,N18-1032,0,0.0694743,"ds or tokens from many languages into the same vector space. This implies, in principle, that words with similar semantics are represented by similar vectors irrespective of the language they come from. Previous work has shown that these continuous representations are useful to transfer knowledge from resource-rich languages, mainly English, to many other low-resource languages both in widespread use (Ruder et al., 2018; Doval et al., 2019b) and also threatened (Adams et al., 2017). An NLP task where knowledge transfer has been of particular interest is machine translation (Zoph et al., 2016; Gu et al., 2018), but there is also work covering many other of the main tasks in the field: PoS tagging (Fang and Cohn, 2017), dependency parsing (Kulmizev, 2018), or sentiment analysis (Le et al., 2016). In this context, to our knowledge, no previous work has covered the case of Turkic languages, a family of languages estimated to be spoken by over 170 million speakers (Menz, 2016) with little to no NLP resources available. Even though the family comprises dozens of languages, of which at least five have over ten million speakers (Menz, 2016), only Turkish has a reasonable amount of publicly available resou"
2020.lrec-1.499,P15-1027,0,0.0232342,"to integrate it into the model without needing full retraining. Since word embeddings follow the same narrative as those internal representations (in fact, they are usually modelled as such), we can think of multilingual, or cross-lingual, embeddings as a vehicle to transfer knowledge between languages. In this case, we consider the procedure that first obtains monolingual embeddings and then aligns them into a shared space. Starting from the work of Mikolov et al. 1 https://github.com/elmurod1202/ crosLingWordEmbTurk (2013), this approach has seen many contributions (Faruqui and Dyer, 2014; Lazaridou et al., 2015; Zhang et al., 2016; Smith et al., 2017). Notable methods recently developed are VecMap (Artetxe et al., 2018), MUSE (Conneau et al., 2018), and then Meemi (Doval et al., 2019a) as a way to further improve the integration of the cross-lingual space. Regarding low-resource languages, some analysis papers such as (Ruder et al., 2018; Søgaard et al., 2018; Doval et al., 2019b; Glavas et al., 2019) study the viability of these methods in adverse scenarios, and others even resort to endangered languages, such as (Adams et al., 2017). 3. 3.1. Methodology Cross-lingual embeddings We start by buildin"
2020.lrec-1.499,W16-5415,0,0.0667871,"Missing"
2020.lrec-1.499,L16-1467,0,0.0600698,"Missing"
2020.lrec-1.499,D14-1162,0,0.0963055,"Missing"
2020.lrec-1.499,2013.mtsummit-papers.22,0,0.0382877,"ks have been also tackled for Turkish: PoS tagging (Dincer et al., 2008; Can et al., 2016), dependency parsing (Eryi˘git et al., 2008; Eryi˘git et al., 2018), or named entity recognition (Yeniterzi, 2011; Seker and Eryigit, 2017). For a more general overview of the matter, Oflazer and Sarac¸lar (2018) review the state of Turkish NLP. Although not many, there have been works for other languages rather than Turkish: for Uzbek (Li et al., 2016; Matlatipov and Vetulani, 2009; Kuriyozov et al., 2019), for Azeri (Fatullayev et al., 2008a; Fatullayev et al., 2008b; Abbasov et al., 2010), for Kazakh (Salimzyanov et al., 2013; Sakenovich and Zharmagambetov, 2016; Yergesh et al., 2017) and for Kyrgyz (Washington et al., 2012; G¨ormez et al., 2011). Knowledge transfer between languages, and mostly between resource-rich and resource-scarce languages, has drawn much attention in the past years. This might be attributed to the rise of neural networks as the chosen machine learning technique to tackle all sorts of tasks in NLP: machine translation (Zoph et al., 2016; Gu et al., 2018), PoS tagging (Fang and Cohn, 2017), dependency parsing (Kulmizev, 2018), or sentiment analysis (Le et al., 2016). In general, the internal"
2020.lrec-1.499,P18-1072,0,0.0414465,"Missing"
2020.lrec-1.499,washington-etal-2012-finite,0,0.0276153,"y parsing (Eryi˘git et al., 2008; Eryi˘git et al., 2018), or named entity recognition (Yeniterzi, 2011; Seker and Eryigit, 2017). For a more general overview of the matter, Oflazer and Sarac¸lar (2018) review the state of Turkish NLP. Although not many, there have been works for other languages rather than Turkish: for Uzbek (Li et al., 2016; Matlatipov and Vetulani, 2009; Kuriyozov et al., 2019), for Azeri (Fatullayev et al., 2008a; Fatullayev et al., 2008b; Abbasov et al., 2010), for Kazakh (Salimzyanov et al., 2013; Sakenovich and Zharmagambetov, 2016; Yergesh et al., 2017) and for Kyrgyz (Washington et al., 2012; G¨ormez et al., 2011). Knowledge transfer between languages, and mostly between resource-rich and resource-scarce languages, has drawn much attention in the past years. This might be attributed to the rise of neural networks as the chosen machine learning technique to tackle all sorts of tasks in NLP: machine translation (Zoph et al., 2016; Gu et al., 2018), PoS tagging (Fang and Cohn, 2017), dependency parsing (Kulmizev, 2018), or sentiment analysis (Le et al., 2016). In general, the internal representations of the linguistic tokens (most of the times, real-valued vectors) obtained by the n"
2020.lrec-1.499,P11-3019,0,0.0310104,"resources obtained in the course of the present work are freely available online.1 2. Related Work Most of the previous work on Turkic languages has been done exclusively on Turkish. In this case, it has mainly focused on its complex morphology, giving rise to analyzers and disambiguation models (Yuret and T¨ure, 2006; Sak et al., 2011; Eryigit, 2012; Aky¨urek et al., 2019). Nonetheless, other common NLP tasks have been also tackled for Turkish: PoS tagging (Dincer et al., 2008; Can et al., 2016), dependency parsing (Eryi˘git et al., 2008; Eryi˘git et al., 2018), or named entity recognition (Yeniterzi, 2011; Seker and Eryigit, 2017). For a more general overview of the matter, Oflazer and Sarac¸lar (2018) review the state of Turkish NLP. Although not many, there have been works for other languages rather than Turkish: for Uzbek (Li et al., 2016; Matlatipov and Vetulani, 2009; Kuriyozov et al., 2019), for Azeri (Fatullayev et al., 2008a; Fatullayev et al., 2008b; Abbasov et al., 2010), for Kazakh (Salimzyanov et al., 2013; Sakenovich and Zharmagambetov, 2016; Yergesh et al., 2017) and for Kyrgyz (Washington et al., 2012; G¨ormez et al., 2011). Knowledge transfer between languages, and mostly betwe"
2020.lrec-1.499,N06-1042,0,0.0573235,"Missing"
2020.lrec-1.499,N16-1156,0,0.033249,"Missing"
2020.lrec-1.499,D16-1163,0,0.0904992,"us encodings of words or tokens from many languages into the same vector space. This implies, in principle, that words with similar semantics are represented by similar vectors irrespective of the language they come from. Previous work has shown that these continuous representations are useful to transfer knowledge from resource-rich languages, mainly English, to many other low-resource languages both in widespread use (Ruder et al., 2018; Doval et al., 2019b) and also threatened (Adams et al., 2017). An NLP task where knowledge transfer has been of particular interest is machine translation (Zoph et al., 2016; Gu et al., 2018), but there is also work covering many other of the main tasks in the field: PoS tagging (Fang and Cohn, 2017), dependency parsing (Kulmizev, 2018), or sentiment analysis (Le et al., 2016). In this context, to our knowledge, no previous work has covered the case of Turkic languages, a family of languages estimated to be spoken by over 170 million speakers (Menz, 2016) with little to no NLP resources available. Even though the family comprises dozens of languages, of which at least five have over ten million speakers (Menz, 2016), only Turkish has a reasonable amount of public"
2020.lrec-1.499,J08-4010,0,\N,Missing
2020.lrec-1.499,Q17-1010,0,\N,Missing
2020.lrec-1.633,P10-1075,0,0.0339954,"n with respect to arc direction entropy. This is somewhat corroborated by a previous analysis undertaken by Gulordava and Merlo (2016), who modified treebanks to minimise dependency length and the arc direction entropy and obtained an increase in parser performance using these treebanks. Additionally, Kirilin and Versley (2015) evaluated error patterns (over- and underproduction of certain structures); Goldberg and Elhadad (2010) made a similar analysis by classifying parser output and test sentences based on underproduced (indicating test) and overproduced (indicating prediction) structures; Dickinson (2010) also showed that algorithms generate idiosyncratic structures not found in the training data; and Schwartz et al. (2012) found that certain structures work better with certain heads. Together these findings suggest that algorithms have some sort of bias towards generating certain types of structures. 2.3. Dependency distance There has also been more linguistic-specific work undertaken with respect to dependency distance, which can be relevant for parsing. For example, Gibson (2000) introduced dependency locality theory (DLT) which postulates that dependency distances are minimised in order to"
2020.lrec-1.633,N18-2109,1,0.906186,"Missing"
2020.lrec-1.633,N10-1115,0,0.0440457,"arc direction entropy. They, however, did not observe a correlation in parsing accuracy for different dependency lengths resulting from different encoding styles, but they did find a correlation with respect to arc direction entropy. This is somewhat corroborated by a previous analysis undertaken by Gulordava and Merlo (2016), who modified treebanks to minimise dependency length and the arc direction entropy and obtained an increase in parser performance using these treebanks. Additionally, Kirilin and Versley (2015) evaluated error patterns (over- and underproduction of certain structures); Goldberg and Elhadad (2010) made a similar analysis by classifying parser output and test sentences based on underproduced (indicating test) and overproduced (indicating prediction) structures; Dickinson (2010) also showed that algorithms generate idiosyncratic structures not found in the training data; and Schwartz et al. (2012) found that certain structures work better with certain heads. Together these findings suggest that algorithms have some sort of bias towards generating certain types of structures. 2.3. Dependency distance There has also been more linguistic-specific work undertaken with respect to dependency d"
2020.lrec-1.633,W15-2115,0,0.0209245,"n. Many analyses have observed this tendency to minimise dependency distance, further corroborating this hypothesis (Ferrer-i-Cancho, 2004; Liu, 2008; Liu, 2007; BuchKromann, 2006; Futrell et al., 2015; Temperley and Gildea, 2018). However, the extent to which this restriction is adhered has been observed to vary significantly across languages (Jiang and Liu, 2015; Gildea and Temperley, 2010). Other works have investigated different syntactic traits of languages associated with dependency length such as highlighting a correlation with an increase in dependency length and free-order languages (Gulordava and Merlo, 2015) and with an increase in non-projective dependencies (Ferrer-iCancho and G´omez-Rodr´ıguez, 2016; G´omez-Rodr´ıguez and Ferrer-i-Cancho, 2017). In response to Liu et al. (2017), G´omez-Rodr´ıguez (2017) hypothesised that the good practical results achieved by transition-based algorithms could be because they are biased towards short dependencies. This is substantiated by Eisner and Smith (2010) who improved parser performance by imposing upper bounds on dependency length and using dependency lengths as a parsing feature. In another response to Liu et al. (2017), Hudson (2017) highlighted mean"
2020.lrec-1.633,Q16-1025,0,0.311064,"ved that the use of contextualised word embedding off-set the typical error-propagation found when using transition-based parsers. Rehbein et al. (2017) investigated what made certain corpora harder to parse than others by looking at how the dependency encoding scheme affects dependency length and arc direction entropy. They, however, did not observe a correlation in parsing accuracy for different dependency lengths resulting from different encoding styles, but they did find a correlation with respect to arc direction entropy. This is somewhat corroborated by a previous analysis undertaken by Gulordava and Merlo (2016), who modified treebanks to minimise dependency length and the arc direction entropy and obtained an increase in parser performance using these treebanks. Additionally, Kirilin and Versley (2015) evaluated error patterns (over- and underproduction of certain structures); Goldberg and Elhadad (2010) made a similar analysis by classifying parser output and test sentences based on underproduced (indicating test) and overproduced (indicating prediction) structures; Dickinson (2010) also showed that algorithms generate idiosyncratic structures not found in the training data; and Schwartz et al. (20"
2020.lrec-1.633,D19-1277,0,0.0526287,"Missing"
2020.lrec-1.633,J11-1007,0,0.0490525,"c-Eager. 5147 The differences between algorithms have also been observed with different architectural implementations, namely neural networks. Chen and Manning (2014) found that for some treebanks Arc-Eager performed better whereas for others Arc-Standard performed better. 2.2. Analysis of parsing errors There have been analyses on the strengths and weaknesses of different parsers by comparing the errors they make on different kinds of dependencies, with dependency distance and direction often being considered. The most comprehensive analysis on the performance of dependency parsers came from McDonald and Nivre (2011), who compared a graph-based parser with a transition-based parser. They investigated the relative strengths of each of the parsing paradigms with respect to factors like dependency length, distance to root, and sentence length. They found that the transition-based parser performed worse for longer dependencies and those closer to the root, due to error propagation, and better in the converse cases. However, while this analytical methodology gave interesting insights when comparing parsers from very different paradigms, it is not fine-grained enough to draw conclusions when the comparison is b"
2020.lrec-1.633,W07-2216,0,0.0645201,"for sentences of length 10 to 12 in version 2.1 of the Universal Dependency treebanks and the inherent displacement distributions of two algorithms: Arc-Standard (std, green) and Arc-Eager (eager, purple). The corresponding UAS and EMD values are displayed. ent distribution is closer (as measured by the EMD) to the actual observed displacements in that language or corpus. While the inherent distribution of an algorithm for sentences of length k would be difficult to obtain analytically, especially for the algorithms that support arbitrary nonprojectivity where exact inference is intractable (McDonald and Satta, 2007), in practice we will approximate it by running a number of simulations of the above stochastic process. The above definition can be extended to corpora (or subsets of them, such as the sentence-length bins we use in this paper). Let S be a set of n sentences containing nk sentences of length k, for a range of values of k. Then, the inherent distribution of P with respect to S is the discrete probability distribution of the random variable generated by taking a random sentence length from S (where each length k is taken with probability nk /n), and then taking a random displacement using the p"
2020.lrec-1.633,P19-1491,0,0.0386371,"Missing"
2020.lrec-1.633,J08-4003,0,0.57115,"duction Dependency parsing, and in particular the transition-based family of parsers, has a large variety of parsing algorithms to choose from. When comparing different algorithms, empirical results on collections of corpora often show differences in accuracy that can heavily vary across different languages or treebanks, so that a given algorithm can be the best choice for one corpus while being outperformed in another. However, despite these nontrivial patterns in accuracy variations having been observed in many experiments in the last decade, both with non-neural and neural implementations (Nivre, 2008; Ballesteros and Nivre, 2013; Chen and Manning, 2014; Fern´andez-Gonz´alez et al., 2016), very little is known about what makes an algorithm more fitting for a corpus beyond obvious facts (like non-projective algorithms being better for highly non-projective treebanks). This makes the practical choice of a particular algorithm for a language a matter of trial and error. For example, MaltOptimizer chooses between projective and non-projective algorithm according to the amount of non-projective dependencies observed in the treebank, but then the choice of a specific algorithm among projective ("
2020.lrec-1.633,C12-1147,0,0.552171,"Missing"
2020.lrec-1.633,K17-3009,0,0.0615328,"Missing"
2020.lrec-1.633,E12-2012,0,\N,Missing
2020.lrec-1.633,J13-1002,0,\N,Missing
2020.lrec-1.633,D14-1082,0,\N,Missing
2020.lrec-1.633,W17-6525,0,\N,Missing
2021.acl-short.138,2020.udw-1.6,0,0.0630061,"Missing"
2021.acl-short.138,2020.iwpt-1.20,1,0.828267,"Missing"
2021.acl-short.138,D18-1312,0,0.0212604,"nce is well attested (Sagae et al., 2008; Falenska and C¸etino˘glu, 2017; Strzyz et al., 2019; Dehouck et al., 2020). Sentence length has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 2020), part-of-speech bigram perplexity (Berdicevskis et al., 2018), and word order freedom (Gulordava and Merlo, 2016). The history of reproduction and replication in NLP is not so well established, with only a few studies in recent years, e.g. on Universal Dependency (UD) parsing (C¸o¨ ltekin, 2020) and on automatic essay scoring systems (Huber and C¸o¨ ltekin, 2020). Linear techniques, linear regression models or evaluating correlation coefficients are commonly used for statistical analyses of NLP systems. They have been used to model constituency parser performance (Ravi et"
2021.acl-short.138,2020.iwpt-1.4,0,0.0805913,"Missing"
2021.acl-short.138,Q16-1025,0,0.0219691,"th has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 2020), part-of-speech bigram perplexity (Berdicevskis et al., 2018), and word order freedom (Gulordava and Merlo, 2016). The history of reproduction and replication in NLP is not so well established, with only a few studies in recent years, e.g. on Universal Dependency (UD) parsing (C¸o¨ ltekin, 2020) and on automatic essay scoring systems (Huber and C¸o¨ ltekin, 2020). Linear techniques, linear regression models or evaluating correlation coefficients are commonly used for statistical analyses of NLP systems. They have been used to model constituency parser performance (Ravi et al., 2008), to evaluate what affects annotation agreement (Bayerl and Paul, 2011), to investigate what impacts statistical MT systems"
2021.acl-short.138,C12-1063,0,0.0302733,"The history of reproduction and replication in NLP is not so well established, with only a few studies in recent years, e.g. on Universal Dependency (UD) parsing (C¸o¨ ltekin, 2020) and on automatic essay scoring systems (Huber and C¸o¨ ltekin, 2020). Linear techniques, linear regression models or evaluating correlation coefficients are commonly used for statistical analyses of NLP systems. They have been used to model constituency parser performance (Ravi et al., 2008), to evaluate what affects annotation agreement (Bayerl and Paul, 2011), to investigate what impacts statistical MT systems (Guzman and Vogel, 2012), what impacts performance on span identifying tasks (Papay et al., 2020), and many other examples. Therefore, it is likely that lessons drawn from this replication 1090 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1090–1098 August 1–6, 2021. ©2021 Association for Computational Linguistics Training size + DUG + hLtest i All CoNLL18 Original UDPipe 1.2 UDPipe 2.0 CoNLL18 10 seeds UDPipe 1.2 UDPipe 2.0 0.014 0.228 0.195 0.100 0.061 0.169 0.060 0.097 0.146 -0"
2021.acl-short.138,2020.lrec-1.688,0,0.033096,"Missing"
2021.acl-short.138,J11-1007,0,0.033717,"be many crossovers between training and test data. DUG is also tied to the mean sentence length in the test data: smaller sentences are much more likely to 1 Note that in the treebanks used in this paper, namely Universal Dependencies, well-formed trees are enforced. Related Work There is a long history of investigating the causes of variance in parser performance. The effect of training data size on parser performance is well attested (Sagae et al., 2008; Falenska and C¸etino˘glu, 2017; Strzyz et al., 2019; Dehouck et al., 2020). Sentence length has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 2020), part-of-speech bigram perplexity (Berdicevskis et al., 2018), and word order freedom (Gulordava and Merlo, 2016). The history of reproduction and replication in"
2021.acl-short.138,2020.emnlp-main.396,0,0.091548,"Missing"
2021.acl-short.138,D08-1093,0,0.144821,"Missing"
2021.acl-short.138,W08-0504,0,0.0298886,"erformance. However, DUG has two important covariants. The size of the training data impacts DUG because the smaller a treebank is, the less likely there will be many crossovers between training and test data. DUG is also tied to the mean sentence length in the test data: smaller sentences are much more likely to 1 Note that in the treebanks used in this paper, namely Universal Dependencies, well-formed trees are enforced. Related Work There is a long history of investigating the causes of variance in parser performance. The effect of training data size on parser performance is well attested (Sagae et al., 2008; Falenska and C¸etino˘glu, 2017; Strzyz et al., 2019; Dehouck et al., 2020). Sentence length has also been observed to impact performance (McDonald and Nivre, 2011). One likely factor behind this is different sentence lengths having difference dependency distance distributions (Ferrer-i-Cancho and Liu, 2014) which in turn affects parsing as longer dependencies are typically harder to parse (Anderson and G´omez-Rodr´ıguez, 2020; Falenska et al., 2020). Others have offered explanations based on linguistic characteristics such as morphological complexity (Dehouck and Denis, 2018; C¸o¨ ltekin, 20"
2021.acl-short.138,2020.emnlp-main.220,1,0.935198,"entences by length and find that only a small subset of sentences vary in performance with respect to graph isomorphism. Further, the correlation observed between parser performance and graph isomorphism in the wild disappears when controlling for covariants. However, in a controlled experiment, where covariants are kept fixed, we do observe a strong correlation. We suggest that conclusions drawn from statistical analyses like this need to be tempered and that controlled experiments can complement them by more readily teasing factors apart. 1 2 Introduction We undertake a replication study of Søgaard (2020) which introduced graph isomorphism (DUG - directed unlabelled graph isomorphism) as a means of explaining differences in parser performance across different treebanks. It measures the ratio of graphs1 in the test set that were also observed in the training data. It is intuitive that this would likely be related to parser performance. However, DUG has two important covariants. The size of the training data impacts DUG because the smaller a treebank is, the less likely there will be many crossovers between training and test data. DUG is also tied to the mean sentence length in the test data: sm"
2021.acl-short.138,L16-1680,0,0.0608886,"Missing"
2021.acl-short.138,N19-1077,1,0.891052,"Missing"
2021.acl-short.138,K18-2001,0,0.0211496,"Missing"
2021.emnlp-main.825,P19-1340,0,0.0276709,"f the proposed encodings. In this paper, we study the (fully reversible) discontinuous-to-continuous conversion by token reordering and how any off-the-shelf continuous parser can be directly applied without any further adaptation or extended label set. To undertake the independent token reordering, we rely on a Pointer Network architecture (Vinyals et al., 2015) that can accurately relocate those tokens causing discontinuities in the sentence to new positions, generating new sentences that can be directly parsed by any continuous parser. We test our approach 1 with two continuous algorithms (Kitaev et al., 2019; Yang and Deng, 2020) on three widely-used discontinuous treebanks, obtaining remarkable accuracies and outperforming current state-of-the-art discontinuous parsers in terms of speed. given by an in-order traversal of t. This permutation defines a bijective function, f : {0, . . . , n − 1} → {0, . . . , n − 1}, mapping each token at position i in w to its new CCA position j in w0 . Then, w0 can be parsed by a continuous parser and, by keeping track of f (i.e., storing original token positions), it is trivial to recover the discontinuous tree by applying its inverse f −1 . The challenge is in"
2021.emnlp-main.825,N15-1142,0,0.0675541,"Missing"
2021.emnlp-main.825,P16-1101,0,0.0694216,"Missing"
2021.emnlp-main.825,P18-1130,0,0.0134362,"positions. Additionally, a binary biaffine classifier (Dozat and Manning, 2017) that identifies relocated tokens is jointly trained by summing the pointer and labeller losses. Since the decoding process requires n − 1 steps to assign the CCA position to each token and at each step the attention vector at is computed over the whole input, the proposed neural model can process a sentence in O(n2 ) time complexity. Figure 2 depicts the neural architecture and the decoding procedure for reordering the sentence in Figure 1(a). 2.3 3 Neural Architecture Following other pointer-network-based models (Ma et al., 2018; Fernández-González and GómezRodríguez, 2019), we design a specific neural architecture for our problem: Decoder An LSTM is used to model the decoding process. At each time step t, the decoder is fed the encoder hidden state hi of the current token wi to be relocated and generates a decoder hidden state st that will be used for computing the probability distribution over all available CCA positions from the input (i.e., j ∈ [0, n−1]A, with A being the set of already-assigned CCA positions). A biaffine scoring function (Dozat and Manning, 2017) is used for computing this probability distribut"
2021.emnlp-main.825,D17-1174,0,0.0295299,"Missing"
2021.emnlp-main.825,P15-1116,0,0.024914,"ntinuous Parsing with Pointer Network Reordering Daniel Fernández-González and Carlos Gómez-Rodríguez Universidade da Coruña, CITIC FASTPARSE Lab, LyS Group Depto. de Ciencias de la Computación y Tecnologías de la Información Campus de Elviña, s/n, 15071 A Coruña, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract (Stanojevi´c and Steedman, 2020; Corro, 2020); (3) transition-based parsers that deal with discontinuDiscontinuous constituent parsers have always ities by adding a specific transition in charge of lagged behind continuous approaches in terms changing token order (Versley, 2014; Maier, 2015; of accuracy and speed, as the presence of conMaier and Lichte, 2016; Stanojevi´c and Alhama, stituents with discontinuous yield introduces extra complexity to the task. However, a dis2017; Coavoux and Crabbé, 2017) or by designing continuous tree can be converted into a continnew data structures that allow interactions between uous variant by reordering tokens. Based on already-created non-adjacent subtrees (Coavoux that, we propose to reduce discontinuous parset al., 2019; Coavoux and Cohen, 2019); and, fiing to a continuous problem, which can then nally, (4) several approaches that reduce"
2021.emnlp-main.825,2020.iwpt-1.12,0,0.0486745,"Missing"
2021.emnlp-main.825,W16-0906,0,0.0216861,"rnández-González and Carlos Gómez-Rodríguez Universidade da Coruña, CITIC FASTPARSE Lab, LyS Group Depto. de Ciencias de la Computación y Tecnologías de la Información Campus de Elviña, s/n, 15071 A Coruña, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract (Stanojevi´c and Steedman, 2020; Corro, 2020); (3) transition-based parsers that deal with discontinuDiscontinuous constituent parsers have always ities by adding a specific transition in charge of lagged behind continuous approaches in terms changing token order (Versley, 2014; Maier, 2015; of accuracy and speed, as the presence of conMaier and Lichte, 2016; Stanojevi´c and Alhama, stituents with discontinuous yield introduces extra complexity to the task. However, a dis2017; Coavoux and Crabbé, 2017) or by designing continuous tree can be converted into a continnew data structures that allow interactions between uous variant by reordering tokens. Based on already-created non-adjacent subtrees (Coavoux that, we propose to reduce discontinuous parset al., 2019; Coavoux and Cohen, 2019); and, fiing to a continuous problem, which can then nally, (4) several approaches that reduce disconbe directly solved by any off-the-shelf contintinuous constitue"
2021.emnlp-main.825,J93-2004,0,0.0740487,"representing the wide range of syntac2005)), to then be processed by continuous parstic phenomena present in human languages such ing models and discontinuities recovered in a postas long-distance extractions, dislocations or crossprocessing step. serial dependencies, among others. It is well known that discontinuities are inherAlthough continuous approaches ignore these ently related to the order of tokens in the sentence, linguistic phenomena by, for instance, removing them from the original treebank (a common prac- and a discontinuous tree can be transformed into tice in the Penn Treebank (Marcus et al., 1993)), a continuous one by just reordering the words and there exist different algorithms that can handle dis- without including additional structures, an idea continuous parsing. Currently, we can highlight (1) that has been exploited in practically all transitionthose based in Linear Context-Free Rewriting Sys- based parsers and other approaches (Vilares and Gómez-Rodríguez, 2020). However, in these modtems (LCFRS) (Vijay-Shanker et al., 1987), which els the reordering process is tightly integrated and allow exact CKY-style parsing of discontinuous structures at a high computational cost (Gebhar"
2021.emnlp-main.825,P17-1076,0,0.0517371,"Missing"
2021.emnlp-main.825,P05-1013,0,0.222518,"Missing"
2021.emnlp-main.825,W14-6104,0,0.0286374,"ontinuous to Continuous Parsing with Pointer Network Reordering Daniel Fernández-González and Carlos Gómez-Rodríguez Universidade da Coruña, CITIC FASTPARSE Lab, LyS Group Depto. de Ciencias de la Computación y Tecnologías de la Información Campus de Elviña, s/n, 15071 A Coruña, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract (Stanojevi´c and Steedman, 2020; Corro, 2020); (3) transition-based parsers that deal with discontinuDiscontinuous constituent parsers have always ities by adding a specific transition in charge of lagged behind continuous approaches in terms changing token order (Versley, 2014; Maier, 2015; of accuracy and speed, as the presence of conMaier and Lichte, 2016; Stanojevi´c and Alhama, stituents with discontinuous yield introduces extra complexity to the task. However, a dis2017; Coavoux and Crabbé, 2017) or by designing continuous tree can be converted into a continnew data structures that allow interactions between uous variant by reordering tokens. Based on already-created non-adjacent subtrees (Coavoux that, we propose to reduce discontinuous parset al., 2019; Coavoux and Cohen, 2019); and, fiing to a continuous problem, which can then nally, (4) several approaches"
2021.emnlp-main.825,2021.naacl-main.232,0,0.0525464,"Missing"
2021.emnlp-main.825,W16-0907,0,0.107815,"nce, into a non-projective Network capable of accurately generating the dependency parsing task (Fernández-González and continuous token arrangement for a given input sentence and define a bijective function to Martins, 2015; Fernández-González and Gómezrecover the original order. Experiments on the Rodríguez, 2020a) or into a sequence labelling main benchmarks with two continuous parsers problem (Vilares and Gómez-Rodríguez, 2020). In prove that our approach is on par in accuracy (4), we can also include the solutions proposed by with purely discontinuous state-of-the-art algoBoyd (2007) and Versley (2016), which transform rithms, but considerably faster. discontinuous treebanks into continuous variants 1 Introduction where discontinuous constituents are encoded by creating additional constituent nodes and extendDiscontinuous phrase-structure trees (with crossing the original non-terminal label set (following ing branches like the one in Figure 1(a)) are crucial a pseudo-projective technique (Nivre and Nilsson, for fully representing the wide range of syntac2005)), to then be processed by continuous parstic phenomena present in human languages such ing models and discontinuities recovered in a"
2021.emnlp-main.825,A97-1014,0,0.466918,"urrent token wi to be relocated and generates a decoder hidden state st that will be used for computing the probability distribution over all available CCA positions from the input (i.e., j ∈ [0, n−1]A, with A being the set of already-assigned CCA positions). A biaffine scoring function (Dozat and Manning, 2017) is used for computing this probability distribution that will implement the attention mechanism: vtj = score(st , hj ) = g1 (st )T Wg2 (hj ) +UT g1 (st ) + VT g2 (hj ) + b; at = softmax(vt ) 3.1 Experiments Setup Data We test our approach on two German discontinuous treebanks, NEGRA (Skut et al., 1997) and TIGER (Brants et al., 2002), and the disconEncoder Each input sentence w is encoded, to- tinuous English Penn Treebank (DPTB) (Evang ken by token, by a BiLSTM-CNN architecture (Ma and Kallmeyer, 2011) with standard splits as deand Hovy, 2016) into a sequence of encoder hidden scribed in Appendix A.1, discarding PoS tags in all 10572 Figure 2: Simplified sketch of the Pointer Network architecture and decoding steps to reorder the sentence in Figure 1(a). label. no yes NEGRA no yes DPTB no yes TIGER UAS 94.16 94.19 94.56 94.82 97.69 97.88 Rec. 76.11 77.66 79.44 81.58 78.63 82.20 Prec. 76.20"
2021.emnlp-main.825,2020.emnlp-main.221,1,0.847843,"Missing"
2021.iwpt-1.12,L18-1347,0,0.0392919,"Missing"
2021.iwpt-1.12,D18-1217,0,0.0160084,"raged over 5 runs) and with a batch size of 256 (excluding UUParser which doesn’t support batching) with GloVe 100 dimension embeddings. Table is extended from one in Anderson and G´omez-Rodr´ıguez (2020a). most comprehensive source of results provided in the literature under a consistent context (at least in terms of data and splits, although not hardware), so they are useful to see high-level trends and as a starting point to choose parsers for our experiment. In Table 1 we report performance of modern parsing systems for which speeds have been reported. We couldn’t find a reported speed of Clark et al. (2018) which currently has the highest reported performance on PTB (UAS 96.61 and LAS 95.02) when not using BERT. However, its main contribution is semi-supervised augmentations that could be utilised by any parsing system, with their core parser being the Biaffine parser. Zhou and Zhao (2019)’s system leverages constituency and dependency parsing and when not using training data with both constituency and dependency annotations (often not available) the system achieves UAS 95.82 LAS 94.43 (i.e. very similar in LAS to the other top-performing sytems). Zhang et al. (2020a) use a Biaffine parser but w"
2021.iwpt-1.12,K18-2011,0,0.0339359,"Missing"
2021.iwpt-1.12,D18-1291,0,0.0319079,"Missing"
2021.iwpt-1.12,P19-1355,0,0.0300662,"s of a representative sample of modern parsing systems on linguistically diverse data. This analysis runs the systems in a consistent way with respect to software, hardware, and network settings. We also offer a brief overview of self-reported performance on PTB for systems that have a published speed. We add to this measurements for a subset of these systems which we ran locally for a more consistent comparison, i.e. something of a reproducibility effort. Introduction The inefficiency of modern NLP systems has recently come under scrutiny, especially regarding their large energy consumption (Strubell et al., 2019). This hasn’t started a revolution, but there is some NLP work where efficiency is considered. Zhang and Duh (2020) studied different settings for neural machine translation systems, evaluating not only accuracy but also certain costs such as inference time, training time, and model size. Zhou et al. (2021) analysed the fine-tuning and inference time for pretrained LMs, and estimated the cost of pretraining. Jacobsen et al. (2021) presented a Pareto optimisation analysis for POS taggers, considering accuracy and model size. In parsing in particular, Strzyz et al. (2019) evaluated dependency pa"
2021.iwpt-1.17,2021.iwpt-1.12,1,0.783035,"Missing"
2021.iwpt-1.17,N19-1077,1,0.90216,"Missing"
2021.iwpt-1.17,2020.coling-main.223,1,0.762705,"Missing"
2021.iwpt-1.17,P17-1077,0,0.0181394,"rse a constant amount of subgraphs (typically, two) whose union provides the final output. This has been applied to go beyond noncrossing dependency trees in transitionbased dependency parsing by splitting trees into two subsets of arcs (planes) such that there cannot be crossings within each of them, but their union (the final output) can have crossing arcs (G´omez-Rodr´ıguez and Nivre, 2010; G´omezRodr´ıguez and Nivre, 2013; Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2018). In semantic parsing, it has also be used to extend the search space from noncrossing graphs to pagenumber-2 graphs by Sun et al. (2017), who use graph-based parsing to obtain two noncrossing graphs that are combined by Lagrangian relaxation. In the context of sequence labeling, this approach was recently applied by Strzyz et al. (2020) with similar goals and methods as the transition-based parsers above. We present the system submission from the FASTPARSE team for the EUD Shared Task at IWPT 2021. We engaged in the task last year by focusing on efficiency. This year we have focused on experimenting with new ideas on a limited time budget. Our system is based on splitting the EUD graph into several trees, based on linguistic c"
2021.iwpt-1.17,N18-2062,1,0.904632,"Missing"
2021.iwpt-1.17,2020.findings-emnlp.398,0,0.0590362,"Missing"
2021.iwpt-1.17,P10-1151,1,0.776416,"Missing"
2021.iwpt-1.17,J13-4002,1,0.860907,"Missing"
2021.iwpt-1.17,D18-1162,1,0.893897,"Missing"
2021.iwpt-1.17,J13-4006,0,0.0527188,"Missing"
2021.iwpt-1.17,C18-1271,0,0.0151689,"COP NSUBJ They look AUX : PASS COMPOUND like they were doberman pinchers who were shrunk REF NSUBJ : PASS ADVCL :-4 ROOT MARK ACL : RELCL NSUBJ COP NSUBJ They look AUX : PASS COMPOUND like they were doberman pinchers who were shrunk ROOT ADVCL :-4 ROOT MARK ACL : RELCL NSUBJ COP NSUBJ They look AUX : PASS COMPOUND like they were doberman pinchers who were shrunk REF Figure 3: Relative tree split. Top full EUD graph, middle basic tree, bottom relative tree. quence of one label per token in a sentence, so parsing is reduced to a standard sequence labelling problem (Spoustov´a and Spousta, 2010; Li et al., 2018; Strzyz et al., 2019b).1 We choose to use the original bracketing encoding from Strzyz et al. (2019b), as it does not require UPOS tags on decoding (the other leading encoding does). While there is a more recent bracketing encoding that covers more non-projectivity (Strzyz et al., 2020), this also involves splitting trees which we assumed would add too much complexity on top of our linguistic-based splitting. Our chosen encoding represents a tree as sequence of tags composed of left and right brackets representing each word’s incoming and outgoing arcs. Namely, the encoding for wi is based on"
2021.iwpt-1.8,K17-3002,0,0.0120508,"nt of CS & IT Universit´e Sorbonne Nouvelle Department of CS & IT m.anderson@udc mathieu.dehouck@udc.es carlos.gomez@udc.es Abstract are for neural dependency parsers, especially when utilising character embeddings (Ballesteros et al., 2015; de Lhoneux et al., 2017). Work investigating the utility of POS tags typically observe a small increase in performance or no impact when used as features for neural dependency parsers. Smith et al. (2018) found that universal POS (UPOS) tags offer a marginal improvement for their transition based parser for multi-lingual universal dependency (UD) parsing. Dozat et al. (2017) also observed an improvement in parsing performance for graph-based parsers when the predicted UPOS tags came from sufficiently accurate taggers. Zhang et al. (2020) only found POS tags to be useful for English and Chinese when utilising them as an auxiliary task in a multi-task system. Anderson and G´omez-Rodr´ıguez (2020) found that a prohibitively high accuracy was needed to utilise predicted UPOS tags for both graph- and transitionbased parsers for UD parsing. They also obtained results that suggested smaller treebanks might be able to directly utilise less accurate UPOS tags. We evaluate"
2021.iwpt-1.8,P09-1042,0,0.0568667,"g performance. We do this for real low resource universal dependency treebanks, artificially low resource data with varying treebank sizes, and for very small treebanks with varying amounts of augmented data. We find that predicted UPOS tags are somewhat helpful for low resource treebanks, especially when fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases. 1 Introduction Low resource parsing is a long-standing problem in NLP and many techniques have been introduced to tackle it (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Agi´c et al., 2016). For an extensive review and comparison of techniques see Vania et al. (2019). Here we focus on the utility of part-of-speech (POS) tags as features for low resource dependency parsers. POS tags are a common feature for dependency parsers. Tiedemann (2015) highlighted the unrealistic performance of low resource parsers when using gold POS tags in a simulated low resource setting. The performance difference was stark despite using fairly accurate taggers, which is not a reasonable assumption for low resource languages. Tagging performance in low reso"
2021.iwpt-1.8,P13-1057,0,0.03111,"Missing"
2021.iwpt-1.8,2020.conll-1.6,1,0.824807,"Missing"
2021.iwpt-1.8,P16-2091,0,0.0290842,"Missing"
2021.iwpt-1.8,D09-1031,0,0.127086,"Missing"
2021.iwpt-1.8,D15-1041,0,0.0740894,"Missing"
2021.iwpt-1.8,P11-1061,0,0.0982135,"Missing"
2021.iwpt-1.8,D11-1006,0,0.10844,"Missing"
2021.iwpt-1.8,D18-1291,0,0.0300804,"Missing"
2021.iwpt-1.8,Q13-1001,0,0.0837091,"Missing"
2021.iwpt-1.8,W15-2137,0,0.020225,"n fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases. 1 Introduction Low resource parsing is a long-standing problem in NLP and many techniques have been introduced to tackle it (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Agi´c et al., 2016). For an extensive review and comparison of techniques see Vania et al. (2019). Here we focus on the utility of part-of-speech (POS) tags as features for low resource dependency parsers. POS tags are a common feature for dependency parsers. Tiedemann (2015) highlighted the unrealistic performance of low resource parsers when using gold POS tags in a simulated low resource setting. The performance difference was stark despite using fairly accurate taggers, which is not a reasonable assumption for low resource languages. Tagging performance in low resource settings is still very weak even when utilising cross-lingual techniques and other forms of weak supervision (Kann et al., 2020). Even when more annotated data is available, it isn’t clear how useful POS tags 2 Methodology We performed three experiments. The first is an evaluation of predicted t"
2021.iwpt-1.8,D19-1102,0,0.0211411,"treebank sizes, and for very small treebanks with varying amounts of augmented data. We find that predicted UPOS tags are somewhat helpful for low resource treebanks, especially when fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases. 1 Introduction Low resource parsing is a long-standing problem in NLP and many techniques have been introduced to tackle it (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Agi´c et al., 2016). For an extensive review and comparison of techniques see Vania et al. (2019). Here we focus on the utility of part-of-speech (POS) tags as features for low resource dependency parsers. POS tags are a common feature for dependency parsers. Tiedemann (2015) highlighted the unrealistic performance of low resource parsers when using gold POS tags in a simulated low resource setting. The performance difference was stark despite using fairly accurate taggers, which is not a reasonable assumption for low resource languages. Tagging performance in low resource settings is still very weak even when utilising cross-lingual techniques and other forms of weak supervision (Kann et"
2021.iwpt-1.8,I08-3008,0,0.0561812,"g accuracy has on parsing performance. We do this for real low resource universal dependency treebanks, artificially low resource data with varying treebank sizes, and for very small treebanks with varying amounts of augmented data. We find that predicted UPOS tags are somewhat helpful for low resource treebanks, especially when fewer fully-annotated trees are available. We also find that this positive impact diminishes as the amount of data increases. 1 Introduction Low resource parsing is a long-standing problem in NLP and many techniques have been introduced to tackle it (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Agi´c et al., 2016). For an extensive review and comparison of techniques see Vania et al. (2019). Here we focus on the utility of part-of-speech (POS) tags as features for low resource dependency parsers. POS tags are a common feature for dependency parsers. Tiedemann (2015) highlighted the unrealistic performance of low resource parsers when using gold POS tags in a simulated low resource setting. The performance difference was stark despite using fairly accurate taggers, which is not a reasonable assumption for low resource languages. Tagging p"
2021.nodalida-main.31,2020.conll-1.6,1,0.831508,"Missing"
2021.nodalida-main.31,D15-1041,0,0.0303118,"result in reductions in tagging performance. We then mask UPOS tags based on errors made by taggers to tease away the contribution of UPOS tags which taggers succeed and fail to classify correctly and the impact of tagging errors. 1 Introduction Part-of-speech (POS) tags have commonly been used as input features for dependency parsers. They were especially useful for non-neural implementations (Voutilainen, 1998; Dalrymple, 2006; Alfared and B´echet, 2012). However, the efficacy of POS tags for neural network dependency parsers is less apparent especially when utilising character embeddings (Ballesteros et al., 2015; de Lhoneux et al., 2017). Universal POS (UPOS) tags have still been seen to improve parsing performance but only if the predicted tags come from a sufficiently accurate tagger (Dozat et al., 2017). Typically using predicted POS tags has offered a nominal increase in performance or has had no impact at all. Smith et al. (2018) undertook a thorough systematic analysis of the interplay of UPOS tags, character embeddings, and pre-trained word embeddings for multi-lingual Universal Dependency (UD) parsing and found that tags offer a marginal improvement for their transition based parser. However,"
2021.nodalida-main.31,Q17-1010,0,0.0308665,"the impact tagging errors have by either masking errors or using the gold standard tags for erroneously predicted tags while masking all other tags. Data We took a subset of UD v2.6 treebanks consisting of 11 languages, all of which are from different language families (Zeman et al., 2020): Arabic PADT (ar), Basque BDT (eu), Finnish TDT (fi), Indonesian GSD (id), Irish IDT (ga), Japanese GSD (ja), Korean Kaist (ko), Tamil TTB (ta), Turkish IMST (tr), Vietnamese VTB (vi), and Wolof WTB (wo). We used pre-trained word embeddings from fastText (for Wolof we had to use the previous Wiki version) (Bojanowski et al., 2017; Grave et al., 2018). We compressed the word embeddings to 100 dimensions with PCA. Arabic Basque Finnish Indonesian Irish Japanese Korean Tamil Turkey Vietnamese Wolof Tagger Tagger-FT Parser 96.71 95.35 96.92 93.72 92.84 97.94 95.09 89.29 95.10 87.85 93.85 96.52 95.18 96.62 93.79 92.80 97.85 94.26 87.28 94.98 87.63 93.79 93.73 88.09 92.24 91.98 88.24 92.80 86.93 75.41 86.14 83.40 85.81 Table 1: Tagging accuracies for tagger trained normally (Tagger), “fine-tuning” a newly initialised MLP for the trained taggers (Tagger-FT), and for parsers fine-tuned to predict tags (Parser). Experiment 1:"
2021.nodalida-main.31,K17-3002,0,0.0194572,"he impact of tagging errors. 1 Introduction Part-of-speech (POS) tags have commonly been used as input features for dependency parsers. They were especially useful for non-neural implementations (Voutilainen, 1998; Dalrymple, 2006; Alfared and B´echet, 2012). However, the efficacy of POS tags for neural network dependency parsers is less apparent especially when utilising character embeddings (Ballesteros et al., 2015; de Lhoneux et al., 2017). Universal POS (UPOS) tags have still been seen to improve parsing performance but only if the predicted tags come from a sufficiently accurate tagger (Dozat et al., 2017). Typically using predicted POS tags has offered a nominal increase in performance or has had no impact at all. Smith et al. (2018) undertook a thorough systematic analysis of the interplay of UPOS tags, character embeddings, and pre-trained word embeddings for multi-lingual Universal Dependency (UD) parsing and found that tags offer a marginal improvement for their transition based parser. However, Zhang et al. (2020) found that the only way to leverage POS tags (both coarse and fine-grained) for English and Chinese dependency parsing was to utilise them as an auxiliary task in a multi-task f"
2021.nodalida-main.31,L18-1550,0,0.0164912,"s have by either masking errors or using the gold standard tags for erroneously predicted tags while masking all other tags. Data We took a subset of UD v2.6 treebanks consisting of 11 languages, all of which are from different language families (Zeman et al., 2020): Arabic PADT (ar), Basque BDT (eu), Finnish TDT (fi), Indonesian GSD (id), Irish IDT (ga), Japanese GSD (ja), Korean Kaist (ko), Tamil TTB (ta), Turkish IMST (tr), Vietnamese VTB (vi), and Wolof WTB (wo). We used pre-trained word embeddings from fastText (for Wolof we had to use the previous Wiki version) (Bojanowski et al., 2017; Grave et al., 2018). We compressed the word embeddings to 100 dimensions with PCA. Arabic Basque Finnish Indonesian Irish Japanese Korean Tamil Turkey Vietnamese Wolof Tagger Tagger-FT Parser 96.71 95.35 96.92 93.72 92.84 97.94 95.09 89.29 95.10 87.85 93.85 96.52 95.18 96.62 93.79 92.80 97.85 94.26 87.28 94.98 87.63 93.79 93.73 88.09 92.24 91.98 88.24 92.80 86.93 75.41 86.14 83.40 85.81 Table 1: Tagging accuracies for tagger trained normally (Tagger), “fine-tuning” a newly initialised MLP for the trained taggers (Tagger-FT), and for parsers fine-tuned to predict tags (Parser). Experiment 1: Error crossover We tr"
2021.nodalida-main.31,K17-3022,0,0.0375164,"Missing"
2021.nodalida-main.31,D18-1291,0,0.0348572,"Missing"
2021.nodalida-main.31,D19-1102,0,0.0173409,"88.24 92.80 86.93 75.41 86.14 83.40 85.81 Table 1: Tagging accuracies for tagger trained normally (Tagger), “fine-tuning” a newly initialised MLP for the trained taggers (Tagger-FT), and for parsers fine-tuned to predict tags (Parser). Experiment 1: Error crossover We trained parsers and taggers on the subset of UD treebanks described above. We then took the parser network and replaced the biaffine structure with a multilayer perceptron (MLP) to predict UPOS tags. We froze the network except for the MLP and finetuned the MLP with one epoch of learning, which is similar to the process used in Vania et al. (2019). We train for only one epoch to balance training the MLP to decode what the system already has encoded without giving it the opportunity to encode more information. We repeated this for the tagger networks (replacing their MLP with a randomly initialised MLP) to validate this fine-tuning procedure. We then compared the tagging errors of both the parsers fine-tuned for tagging and the original taggers. We also undertook an analysis of the errors from the normal taggers which included looking at the impact out-of-vocabulary, POS tag context, and a narrow syntactic context. We define the context"
2021.nodalida-main.31,W98-1303,0,0.378258,"rs implicitly learn about word types and how this relates to the errors taggers make to explain the minimal impact using predicted tags has on parsers. We also present a short analysis on what contexts result in reductions in tagging performance. We then mask UPOS tags based on errors made by taggers to tease away the contribution of UPOS tags which taggers succeed and fail to classify correctly and the impact of tagging errors. 1 Introduction Part-of-speech (POS) tags have commonly been used as input features for dependency parsers. They were especially useful for non-neural implementations (Voutilainen, 1998; Dalrymple, 2006; Alfared and B´echet, 2012). However, the efficacy of POS tags for neural network dependency parsers is less apparent especially when utilising character embeddings (Ballesteros et al., 2015; de Lhoneux et al., 2017). Universal POS (UPOS) tags have still been seen to improve parsing performance but only if the predicted tags come from a sufficiently accurate tagger (Dozat et al., 2017). Typically using predicted POS tags has offered a nominal increase in performance or has had no impact at all. Smith et al. (2018) undertook a thorough systematic analysis of the interplay of U"
C10-1094,H91-1060,0,0.288184,"Missing"
C10-1094,P06-4020,0,0.0683923,"Missing"
C10-1094,W06-2920,0,0.0649395,"Missing"
C10-1094,P04-1041,0,0.0138708,"Missing"
C10-1094,W08-2102,0,0.0219878,"Missing"
C10-1094,P05-1022,0,0.052948,"state-of-the-art parsers were evaluated for their recall on the goldstandard dependencies. Three of the parsers were based on grammars automatically extracted from the PTB: the C&C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), and the Stanford parser (Klein and Manning, 2003). The two remaining systems were the 834 RASP parser (Briscoe et al., 2006), using a manually constructed grammar and a statistical parse selection component, and the DCU post-processor of PTB parsers (Cahill et al., 2004) using the output of the Charniak and Johnson reranking parser (Charniak and Johnson, 2005). Because of the wide variation in parser output representations, a mostly manual evaluation was performed to ensure that each parser got credit for the constructions it recovered correctly. The parsers were run essentially “out of the box”, meaning that the development set was used to confirm input and output formats, but no real tuning was performed. In addition, since a separate question model is available for C&C, this was also evaluated on ObQ sentences. The best overall performers were C&C and Enju, which is unsurprising since they are deep parsers based on grammar formalisms designed to"
C10-1094,J07-4004,0,0.224176,"Missing"
C10-1094,de-marneffe-etal-2006-generating,0,0.0641288,"Missing"
C10-1094,gimenez-marquez-2004-svmtool,0,0.0865987,"Missing"
C10-1094,E06-1011,1,0.313697,"-based parsers typically rely on global training and inference algorithms, where the goal is to learn models in which the weight/probability of correct trees is higher than that of incorrect trees. At inference time a global search is run to find the 1 highest weighted dependency tree. Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). MaltParser2 is a freely available implementation of the parsing models described in Nivre et al. (2006a) and Nivre et al. (2006b). MaltParser is categorized as a transition-based parsing system, characterized by parsing algorithms that produce dependency trees by transitioning through abstract state machines (K¨ubler et al., 2008). Transitionbased parsers learn models that predict the next state given the current state of the system as well as features over the history of parsing decisions and the input sentence. At inference time, the parser starts in an initial state, then greedily moves t"
C10-1094,W07-2216,1,0.608607,"graph-based parsing system in that core parsing algorithms can be equated to finding directed maximum spanning trees (either projective or non-projective) from a dense graph representation of the sentence. Graph-based parsers typically rely on global training and inference algorithms, where the goal is to learn models in which the weight/probability of correct trees is higher than that of incorrect trees. At inference time a global search is run to find the 1 highest weighted dependency tree. Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). MaltParser2 is a freely available implementation of the parsing models described in Nivre et al. (2006a) and Nivre et al. (2006b). MaltParser is categorized as a transition-based parsing system, characterized by parsing algorithms that produce dependency trees by transitioning through abstract state machines (K¨ubler et al., 2008). Transitionbased parsers learn models that predict t"
C10-1094,P05-1012,1,0.0924636,"Missing"
C10-1094,W06-2932,1,0.848475,"r the evaluation, including parser training, post-processing, and evaluation.3 4.1 Parser Training One important difference between MSTParser and MaltParser, on the one hand, and the best performing parsers evaluated in Rimell et al. (2009), on the other, is that the former were never developed specifically as parsers for English. Instead, they are best understood as data-driven parser generators, that is, tools for generating a parser given a training set of sentences annotated with dependency structures. Over the years, both systems have been applied to a wide range of languages (see, e.g., McDonald et al. (2006), McDonald (2006), Nivre et al. (2006b), Hall et al. (2007), Nivre et al. (2007)), but they come with no language-specific enhancements and are not equipped specifically to deal with unbounded dependencies. Since the dependency representation used in the evaluation corpus is based on the Stanford typed dependency scheme (de Marneffe et al., 2006), we opted for using the WSJ section of the PTB, converted to Stanford dependencies, as our primary source of training data. Thus, both parsers were trained on section 2–21 of the WSJ data, which we converted to Stanford dependencies using the Stanford"
C10-1094,P05-1011,0,0.0881372,"Missing"
C10-1094,nivre-etal-2006-maltparser,1,0.308576,"hich the weight/probability of correct trees is higher than that of incorrect trees. At inference time a global search is run to find the 1 highest weighted dependency tree. Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). MaltParser2 is a freely available implementation of the parsing models described in Nivre et al. (2006a) and Nivre et al. (2006b). MaltParser is categorized as a transition-based parsing system, characterized by parsing algorithms that produce dependency trees by transitioning through abstract state machines (K¨ubler et al., 2008). Transitionbased parsers learn models that predict the next state given the current state of the system as well as features over the history of parsing decisions and the input sentence. At inference time, the parser starts in an initial state, then greedily moves to subsequent states – based on the predictions of the model – until a termination state is reached. Tran"
C10-1094,W06-2933,1,0.871166,"Missing"
C10-1094,P06-2041,1,0.881979,"Missing"
C10-1094,D07-1097,1,0.623321,"Missing"
C10-1094,P09-1040,1,0.856395,"ntially produces the same kind of dependency structures as output but uses the original phrase structure trees from the PTB as input to training. For our experiments we used MSTParser with the same parsing algorithms and features as reported in McDonald et al. (2006). However, unlike that work we used an atomic maximum entropy model as the second stage arc predictor as opposed to the more time consuming sequence labeler. McDonald et al. (2006) showed that there is negligible accuracy loss when using atomic rather than structured labeling. For MaltParser we used the projective Stack algorithm (Nivre, 2009) with default settings and a slightly enriched feature model. All parsing was projective because the Stanford dependency trees are strictly projective. 4 QB contains 4000 questions, but we removed all questions that also occurred in the test or development set of Rimell et al. (2009), who sampled their questions from the same TREC QA test sets. 836 4.2 Post-Processing All the development and test sets in the corpus of Rimell et al. (2009) were parsed using MSTParser and MaltParser after part-of-speech tagging the input using SVMTool (Gim´enez and M`arquez, 2004) trained on section 2–21 of the"
C10-1094,P08-1067,0,0.062612,"Missing"
C10-1094,D09-1085,1,0.255417,"Sagae and Lavie, 2006; Huang, 2008; Carreras et al., 2008), broad-coverage parsing is still far from being a solved problem. In particular, metrics like attachment score for dependency parsers (Buchholz and Marsi, 2006) and Parseval for constituency parsers (Black et al., 1991) suffer from being an average over a highly skewed distribution of different grammatical constructions. As a result, infrequent yet semantically important construction types could be parsed with accuracies far below what one might expect. This shortcoming of aggregate parsing metrics was highlighted in a recent study by Rimell et al. (2009), introducing a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies in seven different grammatical constructions. This corpus was used to evaluate five state-of-the-art parsers cgomezr@udc.es for English, focusing on grammar-based and statistical phrase structure parsers. For example, in the sentence By Monday, they hope to have a sheaf of documents both sides can trust., parsers should recognize that there is a dependency between trust and documents, an instance of object extraction out of a (reduced) relative clause. In the evaluation, the recal"
C10-1094,P06-1063,0,0.0324415,"Missing"
C10-1094,N06-2033,0,0.0764608,"Missing"
C10-1094,P03-1054,0,0.00493055,"d (2006), Nivre et al. (2006b), Hall et al. (2007), Nivre et al. (2007)), but they come with no language-specific enhancements and are not equipped specifically to deal with unbounded dependencies. Since the dependency representation used in the evaluation corpus is based on the Stanford typed dependency scheme (de Marneffe et al., 2006), we opted for using the WSJ section of the PTB, converted to Stanford dependencies, as our primary source of training data. Thus, both parsers were trained on section 2–21 of the WSJ data, which we converted to Stanford dependencies using the Stanford parser (Klein and Manning, 2003). The Stanford scheme comes in several varieties, but because both parsers require the dependency structure for each sentence to be a tree, we had to use the so-called basic variety (de Marneffe et al., 2006). It is well known that questions are very rare in the WSJ data, and Rimell et al. (2009) found that parsers trained only on WSJ data generally performed badly on the questions included in the 3 To ensure replicability, we provide all experimental settings, post-processing scripts and additional information about the evaluation at http://stp.ling.uu.se/∼nivre/exp/. evaluation corpus, while"
C10-1094,D07-1013,1,0.699906,"bounded dependency constructions (a–g). Arcs drawn below each sentence represent the dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation, with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets. (2009) and considerably better than the other statistical parsers in that evaluation. Interestingly, though the two systems have similar accuracies overall, there is a clear distinction between the kinds of errors each system makes, which we argue is consistent with observations by McDonald and Nivre (2007). 2 Unbounded Dependency Evaluation An unbounded dependency involves a word or phrase interpreted at a distance from its surface position, where an unlimited number of clause boundaries may in principle intervene. The unbounded dependency corpus of Rimell et al. (2009) includes seven grammatical constructions: object extraction from a relative clause (ObRC), object extraction from a reduced relative clause (ObRed), subject extraction from a relative clause (SbRC), free relatives (Free), object questions (ObQ), right node raising (RNR), and subject extraction from an embedded clause (SbEm), all"
D11-1114,W06-2922,0,0.201718,"s. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial"
D11-1114,P89-1018,0,0.0432038,"ng algorithm for simulating the computations of the system from §2–3. Given an input string w, our algorithm produces a compact representation of the set Γ (w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ (w), or else the probability of w, defined as the sum of all probabilities of computations in Γ (w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by 1239 c1 σ h1 minimum stack length in c1 , . . . , cm cm i i+1 i σ i+1 buﬀer size  p(la2 |b3 , b2 , b1 ) = θbrd1 · θbrd22,b1 · θbla32,b2 ,b1 , h1  p(ra1 |b2 , b1 ) = θbrd1 · θbra21,b1 , σ buﬀer p(shb |b1 ) = θbsh1 b , ∀b ∈ Σ , c0 stack distributions p(t |σ) as follows: stack size h2 h3 j Figure 2: Schematic representation of the computations γ associated with item [h1 , i, h2 h3 , j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to de"
D11-1114,W06-2920,0,0.0594122,"g able to handle only projective dependencies. This formulation permits parsing a subset of the non-projective trees, where this subset depends on the degree of the transitions. The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. For instance, on training data for Czech containing 28,934 non-projective relations, 27,181 can be handled by degree two transitions, and 1,668 additional dependencies can be handled by degree three transitions. Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. Our results are based on such a restriction. It is not difficult to extend our algorithms (§4) to higher degree transitions, but this comes at the expense of higher complexity. See §6 for more discussion on this issue. Let w = a0 · · · an−1 be an input string over Σ defined as in §2.1, with a0 = $. Our transition system for non-projective dependency parsing is (np) S (np) = (C, T (np) , I (np) , Ct ), 1236 Deg. 2 180 961 27181 876"
D11-1114,P99-1059,1,0.728094,"been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference. In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on contextfree grammars. See for instance Klein and Manning 1241 (2001) or Nederhof (2003). Such a parse forest encodes all valid computations in Γ (w), as desired. The algorithm runs in O(n8 ) time. Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n7 ). However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section. 5 Inference We turn next to specify exact inference with our model, for computing feature expectations. Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing. Here, a feature is a function over computations, providing the count of a pattern related to a parameter. We denote by fbla32,b2 ,b1 (γ), for instance, the number of occurrences of transition la2 within γ"
D11-1114,H05-1036,0,0.0155602,"the items in the tabular algorithm. More specifically, given a string w, we associate each item [h1 , i, h2 h3 , j] defined as in §4 with two quantities: I([h1 , i, h2 h3 , j]) = X p(γ) ; (5) γ=([h1 ],βi ),...,([h2 ,h3 ],βj ) O([h1 , i, h2 h3 , j]) = X p(γ) · p(γ 0 ) . (6) σ,γ=([¢],β0 ),...,(σ|h1 ,βi ) γ 0 =(σ|h2 |h3 ,βj ),...,([¢,0],βn ) I([h1 , i, h2 h3 , j]) and O([h1 , i, h2 h3 , j]) are called the inside and the outside probabilities, respectively, of item [h1 , i, h2 h3 , j]. The tabular algorithm of §4 can be used to compute the inside probabilities. Using the gradient transformation (Eisner et al., 2005), a technique for deriving outside probabilities from a set of inference rules, we can also compute O([h1 , i, h2 h3 , j]). The use of the gradient transformation is valid in our case because the tabular algorithm is unambiguous (see §4). Using the inside and outside probabilities, we can now efficiently compute feature expectations for our Ep(γ|w) [fbla32,b2 ,b1 (γ)] = X p(γ |w) · fbla32,b2 ,b1 (γ) = γ∈Γ (w) X 1 · p(w) = X p(γ0 ) · p(γ1 ) · p(γ2 ) · p(la2 |b3 , b2 , b1 ) · p(γ3 ) γ1 =(σ|h1 ,βi ),...,(σ|h2 |h3 ,βk ), h1 ,h2 ,h3 ,h4 ,h5 , γ2 =(σ|h2 |h3 ,βk ),...,(σ|h2 |h4 |h5 ,βj ), s.t. ah2 =b"
D11-1114,P10-1151,1,0.917041,"Missing"
D11-1114,E09-1034,1,0.886882,"Missing"
D11-1114,J11-3004,1,0.87406,"Missing"
D11-1114,J99-4004,0,0.0436778,"vre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of eleme"
D11-1114,P10-1110,0,0.56228,"mith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlm"
D11-1114,P98-1106,0,0.150218,"ures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition"
D11-1114,W01-1812,0,0.020146,"Missing"
D11-1114,D07-1015,0,0.0189125,"ed for unsupervised learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been suc"
D11-1114,E09-1055,1,0.906123,"ional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition-based model we use, together with a probabilistic generative i"
D11-1114,P11-1068,1,0.834515,"Missing"
D11-1114,D09-1005,0,0.0203863,"e recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent po"
D11-1114,W07-2216,1,0.910657,"d learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for project"
D11-1114,H05-1066,0,0.196007,"Missing"
D11-1114,J03-1006,0,0.0175004,"e not larger than the size of the stack associated with the initial configuration. As a final remark, we observe that we can keep track of all inference rules that have been applied in the computation of each item by the above algorithm, by encoding each application of a rule as a reference to the pair of items that were taken as antecedent in the inference. In this way, we obtain a parse forest structure that can be viewed as a hypergraph or as a non-recursive context-free grammar, similar to the case of parsing based on contextfree grammars. See for instance Klein and Manning 1241 (2001) or Nederhof (2003). Such a parse forest encodes all valid computations in Γ (w), as desired. The algorithm runs in O(n8 ) time. Using methods similar to those specified in Eisner and Satta (1999), we can reduce the running time to O(n7 ). However, we do not further pursue this idea here, and proceed with the discussion of exact inference, found in the next section. 5 Inference We turn next to specify exact inference with our model, for computing feature expectations. Such inference enables, for example, the derivation of an expectation-maximization algorithm for unsupervised parsing. Here, a feature is a functi"
D11-1114,P05-1013,0,0.0605722,"community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) al"
D11-1114,W04-0308,0,0.121591,"are called reduce transitions, i.e., transitions that consume nodes from the stack. Notice that in the transition system at hand all the reduce transitions decrease the size of the stack by one element. Transition la1 creates a new arc with the topmost node on the stack as the head and the secondtopmost node as the dependent, and removes the latter from the stack. Transition ra1 is symmetric with respect to la1 . Transitions la1 and ra1 have degree one, as already explained. When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). Transition la2 and transition ra2 are very similar to la1 and ra1 , respectively, but with the difference that they create a new arc between the topmost node in the stack and a node which is two positions below the topmost node. Hence, these transitions have degree two, and are the key components in parsing of non-projective dependencies. We turn next to describe the equivalence between our system and the system in Attardi (2006). The transition-based parser presented by Attardi pushes back into the buffer elements that are in the top position of the stack. However, a careful analysis shows"
D11-1114,J08-4003,0,0.65713,"spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1"
D11-1114,P09-1040,0,0.435291,"tical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space represe"
D11-1114,C96-2215,0,0.177144,"Missing"
D11-1114,D07-1014,0,0.0205805,"ve dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and S"
D11-1114,W03-3023,0,0.462207,"n in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic 1234 programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as c"
D11-1114,C98-1102,0,\N,Missing
D12-1029,afonso-etal-2002-floresta,0,0.0151783,"ed parser: for a given transition, first check whether it is possible to build a gold-standard arc of length 1 with a projective buffer transition.2 If so, choose that transition, and if not, just delegate to the original parser’s oracle. 3.2 Experiments To empirically evaluate the effect of projective buffer transitions on parsing accuracy, we have conducted experiments on eight datasets of the CoNLLX shared task (Buchholz and Marsi, 2006): Arabic (Hajiˇc et al., 2004), Chinese (Chen et al., 2003), Czech (Hajiˇc et al., 2006), Danish (Kromann, 2003), German (Brants et al., 2002), Portuguese (Afonso et al., 2002), Swedish (Nilsson et al., 2005) and Turkish (Oflazer et al., 2003; Atalay et al., 2003). As our baseline parser, we use the arc-eager projective transition system by Nivre (2003). Table 1 compares the accuracy obtained by this system alone with that obtained when the L EFT-B UFFER -A RC and R IGHT-B UFFER -A RC transitions are added to it as explained in Section 3.1. Accuracy is reported in terms of labelled (LAS) and unlabelled (UAS) attachment score. We used SVM classifiers from the LIBSVM package (Chang and Lin, 2001) for all languages except for Chinese, Czech and German. In these, we use"
D12-1029,W03-2405,0,0.0412665,"andard arc of length 1 with a projective buffer transition.2 If so, choose that transition, and if not, just delegate to the original parser’s oracle. 3.2 Experiments To empirically evaluate the effect of projective buffer transitions on parsing accuracy, we have conducted experiments on eight datasets of the CoNLLX shared task (Buchholz and Marsi, 2006): Arabic (Hajiˇc et al., 2004), Chinese (Chen et al., 2003), Czech (Hajiˇc et al., 2006), Danish (Kromann, 2003), German (Brants et al., 2002), Portuguese (Afonso et al., 2002), Swedish (Nilsson et al., 2005) and Turkish (Oflazer et al., 2003; Atalay et al., 2003). As our baseline parser, we use the arc-eager projective transition system by Nivre (2003). Table 1 compares the accuracy obtained by this system alone with that obtained when the L EFT-B UFFER -A RC and R IGHT-B UFFER -A RC transitions are added to it as explained in Section 3.1. Accuracy is reported in terms of labelled (LAS) and unlabelled (UAS) attachment score. We used SVM classifiers from the LIBSVM package (Chang and Lin, 2001) for all languages except for Chinese, Czech and German. In these, we used the LIBLINEAR package (Fan et al., 2008) for classification, since it reduces training"
D12-1029,W06-2922,0,0.546173,"ach is generic enough to be applicable to any stackbased dependency parser. 1 Introduction Dependency parsing has become a very active research area in natural language processing in recent years. The dependency representation of syntax simplifies the syntactic parsing task, since no non-lexical nodes need to be postulated by the parsers; while being convenient in practice, since dependency representations directly show the headmodifier and head-complement relationships which form the basis of predicate-argument structure. This In particular, many transition-based parsers (Nivre et al., 2004; Attardi, 2006; Sagae and Tsujii, 2008; Nivre, 2009; Huang and Sagae, 2010; G´omezRodr´ıguez and Nivre, 2010) are stack-based (Nivre, 2008), meaning that they keep a stack of partially processed tokens and an input buffer of unread tokens. In this paper, we show how the accuracy of this kind of parsers can be improved, without compromising efficiency, by extending their set of available transitions with buffer transitions. These are transitions that create a dependency arc involving some node in the buffer, which would typically be considered unavailable for linking by these algo308 Proceedings of the 2012"
D12-1029,W06-2920,0,0.73374,"thods in Natural Language Processing and Computational Natural c Language Learning, pages 308–319, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics rithms. The rationale is that buffer transitions construct some “easy” dependency arcs in advance, before the involved nodes reach the stack, so that the classifier’s job when choosing among standard transitions is simplified. To test the approach, we use the well-known arceager parser by (Nivre, 2003; Nivre et al., 2004) as a baseline, showing improvements in accuracy on most datasets of the CoNLL-X shared task (Buchholz and Marsi, 2006). However, the techniques discussed in this paper are generic and can also be applied to other stack-based dependency parsers. The rest of this paper is structured as follows: Section 2 is an introduction to transition-based parsers and the arc-eager parsing algorithm. Section 3 presents the first novel contribution of this paper, projective buffer transitions, and discusses their empirical results on CoNLL-X datasets. Section 4 does the same for a more complex set of transitions, non-projective buffer transitions. Finally, Section 5 discusses related work and Section 6 sums up the conclusions"
D12-1029,W06-2927,0,0.023005,"vre’s arc-eager transition system is added to a list-based parser. However, the goal of that transition is different from ours (selecting between projective and nonprojective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic – for example, the L EFTA RC transition cannot be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. The idea of creating dependency arcs of length 1 in advance to help the classifier has been used by Cheng et al. (2006). However, their system creates such arcs in a separate preprocessing step rather than dynamically by adding a transition to the parser, and our approach obtains better LAS and UAS results on all the tested datasets. The projective buffer transitions presented here bear some resemblance to the easy-first parser by Goldberg and Elhadad (2010), which allows creation of dependency arcs between any pair of contiguous nodes and is based on the idea of “easy” dependency links being created first. However, while the easy-first parser is an entirely new O(n log(n)) algorithm, our approach is a generic"
D12-1029,P11-2121,0,0.0362566,"y build non-projective arcs in one direction: that of the particular non-projective buffer transition used for each dataset); it does so with greater precision and is more accurate than that algorithm in projective arcs. Like projective buffer transitions, non-projective transitions do not increase the computational complexity of stack-based parsers. The observed training and parsing times for the arc-eager parser with non-projective buffer transitions showed a small 316 Related work The approach of adding an extra transition to a parser to improve its accuracy has been applied in the past by Choi and Palmer (2011). In that paper, the L EFT-A RC transition from Nivre’s arc-eager transition system is added to a list-based parser. However, the goal of that transition is different from ours (selecting between projective and nonprojective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic – for example, the L EFTA RC transition cannot be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. The idea of creating dependency arcs of length 1 i"
D12-1029,C96-1058,0,0.210314,"l. (2005), Martins et al. (2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce accurate analyses very efficiently. In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. Most current data-driven dependency parsers can be classified into two families, commonly called graph-based and transition-based parsers (McDonald and Nivre, 2011). Graph-based parsers (Eisner, 1996; McDonald et al., 2005) are based on global optimization of models that work by scoring subtrees. On the other hand, transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004), which are the focus of this work, use local training to make greedy decisions that deterministically select the next parser state. Among the advantages of transition-based parsers are the linear time complexity of many of them and the possibility of using rich feature models (Zhang and Nivre, 2011). First, we show how adding a specific transition to create either a left or right arc of length one between"
D12-1029,N10-1115,0,0.0346927,"cannot be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. The idea of creating dependency arcs of length 1 in advance to help the classifier has been used by Cheng et al. (2006). However, their system creates such arcs in a separate preprocessing step rather than dynamically by adding a transition to the parser, and our approach obtains better LAS and UAS results on all the tested datasets. The projective buffer transitions presented here bear some resemblance to the easy-first parser by Goldberg and Elhadad (2010), which allows creation of dependency arcs between any pair of contiguous nodes and is based on the idea of “easy” dependency links being created first. However, while the easy-first parser is an entirely new O(n log(n)) algorithm, our approach is a generic extension for stack-based parsers that does not increase their complexity (so, for example, applying it to the arc-eager system as in the experiments in this paper yields O(n) complexity). Non-projective transitions that create dependency arcs between non-contiguous nodes have been used in the transition-based parser by Attardi (2006). Howe"
D12-1029,P10-1151,1,0.906284,"Missing"
D12-1029,P10-1110,0,0.0353017,"ased dependency parser. 1 Introduction Dependency parsing has become a very active research area in natural language processing in recent years. The dependency representation of syntax simplifies the syntactic parsing task, since no non-lexical nodes need to be postulated by the parsers; while being convenient in practice, since dependency representations directly show the headmodifier and head-complement relationships which form the basis of predicate-argument structure. This In particular, many transition-based parsers (Nivre et al., 2004; Attardi, 2006; Sagae and Tsujii, 2008; Nivre, 2009; Huang and Sagae, 2010; G´omezRodr´ıguez and Nivre, 2010) are stack-based (Nivre, 2008), meaning that they keep a stack of partially processed tokens and an input buffer of unread tokens. In this paper, we show how the accuracy of this kind of parsers can be improved, without compromising efficiency, by extending their set of available transitions with buffer transitions. These are transitions that create a dependency arc involving some node in the buffer, which would typically be considered unavailable for linking by these algo308 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Pr"
D12-1029,P09-1039,0,0.0388136,"Missing"
D12-1029,D07-1013,0,0.0890955,"s can only create projective arcs and cannot violate the single-head or acyclicity constraints, given that a buffer node cannot have a head. The idea behind projective buffer transitions is to create dependency arcs of length one (i.e., arcs involving contiguous nodes) in advance of the standard arc-building transitions that need at least one of the nodes to get to the stack (L EFT-A RC and R IGHTA RC in the case of the arc-eager transition system). Our hypothesis is that, as it is known that short-distance dependencies are easier to learn for transition-based parsers than long-distance ones (McDonald and Nivre, 2007), handling these short arcs in advance and removing their dependent nodes will make it easier for the classifier to learn how to make decisions involving the standard arc transitions. Note that the fact that projective buffer transitions create arcs of length 1 is not explicit in the definition of the transitions. For instance, if we add the L EFT-B UFFER -A RCl transition only to the arc-eager transition system, L EFT-B UFFER -A RCl will only be able to create arcs of length 1, since it is easy to see that the first two buffer nodes are contiguous in all the accessible configurations. However"
D12-1029,J11-1007,0,0.0356234,"umoto (2003), Nivre et al. (2004), McDonald et al. (2005), Martins et al. (2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce accurate analyses very efficiently. In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. Most current data-driven dependency parsers can be classified into two families, commonly called graph-based and transition-based parsers (McDonald and Nivre, 2011). Graph-based parsers (Eisner, 1996; McDonald et al., 2005) are based on global optimization of models that work by scoring subtrees. On the other hand, transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004), which are the focus of this work, use local training to make greedy decisions that deterministically select the next parser state. Among the advantages of transition-based parsers are the linear time complexity of many of them and the possibility of using rich feature models (Zhang and Nivre, 2011). First, we show how adding a specific transition to create either a left"
D12-1029,H05-1066,0,0.289942,"Missing"
D12-1029,W06-2932,0,0.0776049,"Missing"
D12-1029,W04-2407,0,0.0604367,"Missing"
D12-1029,W06-2933,0,0.096494,"Missing"
D12-1029,W03-3017,0,0.895206,"ould typically be considered unavailable for linking by these algo308 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 308–319, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics rithms. The rationale is that buffer transitions construct some “easy” dependency arcs in advance, before the involved nodes reach the stack, so that the classifier’s job when choosing among standard transitions is simplified. To test the approach, we use the well-known arceager parser by (Nivre, 2003; Nivre et al., 2004) as a baseline, showing improvements in accuracy on most datasets of the CoNLL-X shared task (Buchholz and Marsi, 2006). However, the techniques discussed in this paper are generic and can also be applied to other stack-based dependency parsers. The rest of this paper is structured as follows: Section 2 is an introduction to transition-based parsers and the arc-eager parsing algorithm. Section 3 presents the first novel contribution of this paper, projective buffer transitions, and discusses their empirical results on CoNLL-X datasets. Section 4 does the same for a more co"
D12-1029,J08-4003,0,0.294997,"y active research area in natural language processing in recent years. The dependency representation of syntax simplifies the syntactic parsing task, since no non-lexical nodes need to be postulated by the parsers; while being convenient in practice, since dependency representations directly show the headmodifier and head-complement relationships which form the basis of predicate-argument structure. This In particular, many transition-based parsers (Nivre et al., 2004; Attardi, 2006; Sagae and Tsujii, 2008; Nivre, 2009; Huang and Sagae, 2010; G´omezRodr´ıguez and Nivre, 2010) are stack-based (Nivre, 2008), meaning that they keep a stack of partially processed tokens and an input buffer of unread tokens. In this paper, we show how the accuracy of this kind of parsers can be improved, without compromising efficiency, by extending their set of available transitions with buffer transitions. These are transitions that create a dependency arc involving some node in the buffer, which would typically be considered unavailable for linking by these algo308 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 308"
D12-1029,P09-1040,0,0.44076,"to any stackbased dependency parser. 1 Introduction Dependency parsing has become a very active research area in natural language processing in recent years. The dependency representation of syntax simplifies the syntactic parsing task, since no non-lexical nodes need to be postulated by the parsers; while being convenient in practice, since dependency representations directly show the headmodifier and head-complement relationships which form the basis of predicate-argument structure. This In particular, many transition-based parsers (Nivre et al., 2004; Attardi, 2006; Sagae and Tsujii, 2008; Nivre, 2009; Huang and Sagae, 2010; G´omezRodr´ıguez and Nivre, 2010) are stack-based (Nivre, 2008), meaning that they keep a stack of partially processed tokens and an input buffer of unread tokens. In this paper, we show how the accuracy of this kind of parsers can be improved, without compromising efficiency, by extending their set of available transitions with buffer transitions. These are transitions that create a dependency arc involving some node in the buffer, which would typically be considered unavailable for linking by these algo308 Proceedings of the 2012 Joint Conference on Empirical Methods"
D12-1029,C08-1095,0,0.023046,"enough to be applicable to any stackbased dependency parser. 1 Introduction Dependency parsing has become a very active research area in natural language processing in recent years. The dependency representation of syntax simplifies the syntactic parsing task, since no non-lexical nodes need to be postulated by the parsers; while being convenient in practice, since dependency representations directly show the headmodifier and head-complement relationships which form the basis of predicate-argument structure. This In particular, many transition-based parsers (Nivre et al., 2004; Attardi, 2006; Sagae and Tsujii, 2008; Nivre, 2009; Huang and Sagae, 2010; G´omezRodr´ıguez and Nivre, 2010) are stack-based (Nivre, 2008), meaning that they keep a stack of partially processed tokens and an input buffer of unread tokens. In this paper, we show how the accuracy of this kind of parsers can be improved, without compromising efficiency, by extending their set of available transitions with buffer transitions. These are transitions that create a dependency arc involving some node in the buffer, which would typically be considered unavailable for linking by these algo308 Proceedings of the 2012 Joint Conference on Empi"
D12-1029,D11-1116,0,0.0363477,"Missing"
D12-1029,W03-3023,0,0.0901829,"alyses very efficiently. In this paper, we show that significant improvements in the accuracy of well-known transition-based parsers can be obtained, without sacrificing efficiency, by enriching the parsers with simple transitions that act on buffer nodes. Most current data-driven dependency parsers can be classified into two families, commonly called graph-based and transition-based parsers (McDonald and Nivre, 2011). Graph-based parsers (Eisner, 1996; McDonald et al., 2005) are based on global optimization of models that work by scoring subtrees. On the other hand, transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004), which are the focus of this work, use local training to make greedy decisions that deterministically select the next parser state. Among the advantages of transition-based parsers are the linear time complexity of many of them and the possibility of using rich feature models (Zhang and Nivre, 2011). First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we sh"
D12-1029,D08-1059,0,0.0335719,"pproximate this oracle when the target tree is unknown. The arc-eager parser has linear time complexity. In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al., 2006) in order to capture a restricted subset of nonprojective forests. Using this setup, it scored as one of the top two systems in the CoNLL-X shared task. 2.3 3 The arc-eager parser Nivre’s arc-eager dependency parser (Nivre, 2003; Nivre et al., 2004) is one of the most widely known and used transition-based parsers (see for example (Zhang and Clark, 2008; Zhang and Nivre, 2011)). This parser works by reading the input sentence from left to right and creating dependency links as soon as possible. This means that links are created in a strict left-to-right order, and implies that while leftward links are built in a bottom-up fashion, a rightward link a → b will be created before the node b has collected its right dependents. The arc-eager transition system has the following four transitions (note that, for convenience, we write a stack with node i on top as σ|i, and a buffer whose first node is i as i|β): • S HIFT : (σ, i|β, A) ⇒ (σ|i, β, A). •"
D12-1029,P11-2033,0,0.138261,"o families, commonly called graph-based and transition-based parsers (McDonald and Nivre, 2011). Graph-based parsers (Eisner, 1996; McDonald et al., 2005) are based on global optimization of models that work by scoring subtrees. On the other hand, transition-based parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004), which are the focus of this work, use local training to make greedy decisions that deterministically select the next parser state. Among the advantages of transition-based parsers are the linear time complexity of many of them and the possibility of using rich feature models (Zhang and Nivre, 2011). First, we show how adding a specific transition to create either a left or right arc of length one between the first two buffer nodes produces improvements in the accuracy of Nivre’s arc-eager projective parser on a number of datasets from the CoNLL-X shared task. Then, we show that accuracy can also be improved by adding transitions involving the topmost stack node and the second buffer node (allowing a limited form of non-projectivity). None of these transitions has a negative impact on the computational complexity of the algorithm. Although the experiments in this paper use the arc-eager"
D14-1099,P11-1068,1,0.932721,"Missing"
D14-1099,W06-2922,0,0.723948,"d Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was further investigated by Kuhlmann and Nivre (2010) and Cohen et al. (2011). We provide an implementation for a dynamic oracle for this parser running in polynomial time. We experimentally compare the parser trained with the dynamic oracle to a baseline obtained by training with a static oracle. Significant accuracy improvements are achieved on many languages when using our dynamic oracle. To our knowledge, these are the first experimental results on non-projective parsing based on a dynamic oracle. 917 Proceedings of the 2014 Conference on Empirical Methods in Natural Lan"
D14-1099,P13-1104,0,0.0729294,"Missing"
D14-1099,J93-2004,0,0.0459156,"(76.77, 68.20); similarly, for Hungarian we measure (75.66, 67.66) against (77.22, 68.42). Unfortunately, we have no explanation for these performance decreases, in terms of the typology of the non-projective patterns found in these two datasets. Note that Goldberg et al. (2014) also observed a performance decrease on the Basque dataset in the projective case, although not on Hungarian. Datasets We evaluate the parser performance over CoNLL 2006 and CoNLL 2007 datasets. If a language is present in both datasets, we use the latest version. We also include results over the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006). For the CoNLL datasets we use the provided part-of-speech tags and the standard training/test partition; for the PTB we use automatically assigned tags, we train on sections 2-21 and test on section 23. 7.2 The parsing times measured in our experiments for the static and the dynamic oracles are the same, since the oracle algorithm is only used during the training stage. Thus the reported improvements in parsing accuracy come at no extra cost for parsing time. In the training stage, the extra processing needed to compute the"
D14-1099,W03-3017,0,0.724913,"is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars. Introduction Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As"
D14-1099,D11-1114,1,0.890516,"Missing"
D14-1099,W04-0308,0,0.38568,"et also h1 , h2 , h3 ∈ Vw . We are interested in computations of the parser processing the substring w0 and having the form c0 , c1 , . . . , cm , m ≥ 1, that satisfy both of the following conditions, exemplified in Figure 2. h3 h2 σ ... cm Figure 2: General form of the computations associated with an item [h1 , h2 , h3 ]. tached as a dependent of the second topmost node. The combination of the shift, left-arc and rightarc transitions provides complete coverage of projective dependency trees, but no support for nonprojectivity, and corresponds to the so-called arcstandard parser introduced by Nivre (2004). • For some sequence of nodes σ with |σ |≥ 0, the stack associated with c0 has the form σ|h1 and the stack associated with cm has the form σ|h2 |h3 . Support for non-projective dependencies is achieved by adding the transitions `la2 and `ra2 , which are variants of the left-arc and right-arc transitions, respectively. These new transitions create dependencies involving the first and the third topmost nodes in the stack. The creation of dependencies between non-adjacent stack nodes might produce crossing arcs and is the key to the construction of non-projective trees. • For each intermediate c"
D14-1099,W03-3023,0,0.796516,"ct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several projective dependency parsers. To our knowledge, no polynomial-time algorithm has been published for transition-based parsers based on non-projective dependency grammars. Introduction Greedy transition-based parsers for dependency grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McC"
D14-1099,de-marneffe-etal-2006-generating,0,0.0913078,"Missing"
D14-1099,P11-2033,0,0.0407215,"grammars have been pioneered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006)"
D14-1099,C12-1059,0,0.816109,"tion among those that are optimal. The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been"
D14-1099,Q13-1033,0,0.620865,"ptimal. The introduction of dynamic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above ment"
D14-1099,Q14-1010,1,0.748368,"namic oracles has considerably improved the accuracy of greedy transition-based dependency parsers, without sacrificing parsing efficiency. However, this enhancement is limited to projective parsing, and dynamic oracles have not yet been implemented for parsers supporting non-projectivity. In this paper we introduce the first such oracle, for a non-projective parser based on Attardi’s parser. We show that training with this oracle improves parsing accuracy over a conventional (static) oracle on a wide range of datasets. 1 Very recently, Goldberg and Nivre (2012), Goldberg and Nivre (2013) and Goldberg et al. (2014) showed that the accuracy of transitionbased parsers can be substantially improved using dynamic oracles. A dynamic oracle returns the set of all transitions that are optimal for a given configuration, with respect to the gold tree, and is well-defined and correct for every configuration that is reachable by the parser. Na¨ıve implementations of dynamic oracles run in exponential time, since they need to simulate all possible computations of the parser for the input configuration. Polynomial-time implementations of dynamic oracles have been proposed by the above mentioned authors for several p"
D14-1099,P10-1110,0,0.0368007,"eered by Yamada and Matsumoto (2003) and Nivre (2003). These methods incrementally process the input sentence from left to right, predicting the next parsing action, called transition, on the basis of a compact representation of the derivation history. Greedy transition-based parsers can be very efficient, allowing web-scale parsing with high throughput. However, the accuracy of these methods still falls behind that of transition-based parsers using beam-search, where the accuracy improvement is obtained at the cost of a decrease in parsing efficiency; see for instance Zhang and Nivre (2011), Huang and Sagae (2010), Choi and McCallum (2013). As an alternative to beamsearch, recent research on transition-based parsing has therefore explored possible ways of improving accuracy at no extra cost in parsing efficiency. The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree. A discriminative model is then trained to simulate the oracle’s behavior, In this paper we consider a restriction of a transition-based, non-projective parser originally presented by Attardi (2006). This restriction was f"
D18-1161,D16-1211,0,0.0939767,"(LST Mf wd [ent , s0 , ..., sm ]; LST Mbwd [ent , sm , ..., s0 ]) where ent is the vector representation of a nonterminal, and si , i ∈ [0, m] is the ith child node. Finally, the exact same word representation strategy and hyper-parameter values as (Dyer et al., 2016) and (Liu and Zhang, 2017a) are used to conduct the experiments. 4.3 Error exploration In order to benefit from training a parser by a dynamic oracle, errors should be made during the training process so that the parser can learn to avoid and recover from them. Unlike more complex error-exploration strategies as those studied in (Ballesteros et al., 2016; Cross and Huang, 2016b; Fried and Klein, 2018), we decided to consider a simple one that follows a non-optimal transition when it is the highest-scoring one, but with a certain probability. In that way, we easily simulate test time conditions, when the parser greedily chooses the highest-scoring transition, even when it is not an optimal one, placing the parser in an incorrect state. In particular, we run experiments on development sets for each benchmark/algorithm with three different error exploration probabilities and choose the one that achieves the best F-score. Table 1 reports all resu"
D18-1161,P16-1017,0,0.476305,"Missing"
D18-1161,D15-1212,0,0.0579669,"Missing"
D18-1161,P16-2006,0,0.290214,"content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependency algorithms, to bottom-up constituent parsing. They prop"
D18-1161,D16-1001,0,0.389516,"content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependency algorithms, to bottom-up constituent parsing. They prop"
D18-1161,N16-1024,0,0.406937,"t of error propagation by training parsers under closer conditions to those found at test time, where mistakes are inevitably made. They are designed to guide the parser through any state it might reach during learning time. This makes it possible to introduce error exploration to force the parser to go through nonoptimal states, teaching it how to recover from mistakes and lose the minimum number of gold constituents. Alternatively, some researchers decided to follow a different direction and explore non-bottomup strategies for producing phrase-structure syntactic analysis. On the one hand, (Dyer et al., 2016; Kuncoro et al., 2017) proposed a top-down transition-based algorithm, which creates a phrase structure tree in the stack by first choosing the non-terminal on the top of the tree, and then considering which should be its child nodes. In contrast to the bottom-up approach, this top-down strategy adds a lookahead 1303 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S NP . VP The public is ADVP still ADJP cautious Figure 1: Simplified constit"
D18-1161,P18-2075,0,0.572358,". In addition, the ordered set of gold non-terminal nodes added to the stack while following a top-down strategy will be {(S, 0), (NP, 0), (VP, 2), (ADVP, 3), (ADJP, 4)} and, according to an in-order approach, {(NP, 1), (S, 2), (VP, 3), (ADVP, 4), (ADJP, 5)}. It is worth mentioning that the index of non-terminal nodes in the top-down method is the same as the leftmost span index of the constituent that it will produce. However, this does not hold in the in-order approach, as the leftmost child is fully processed before the node is added to the stack, so the index 2 1 In parallel to this work, Fried and Klein (2018) present a non-optimal dynamic oracle for training the top-down parser. When two or more non-terminals share their labels within the tree, we use a secondary index to make them unique. 1304 for the node will point to the leftmost span index of the second leftmost child. Note that the information about the span of a constituent, the set of predicted constituents γ and the set α of predicted non-terminal nodes in the stack is not used by the original top-down and inorder parsers. However, we need to include it in parser configurations at learning time to allow an efficient implementation of the"
D18-1161,N18-1091,0,0.10009,"he ordered set of gold non-terminal nodes added to the stack while following a top-down strategy will be {(S, 0), (NP, 0), (VP, 2), (ADVP, 3), (ADJP, 4)} and, according to an in-order approach, {(NP, 1), (S, 2), (VP, 3), (ADVP, 4), (ADJP, 5)}. It is worth mentioning that the index of non-terminal nodes in the top-down method is the same as the leftmost span index of the constituent that it will produce. However, this does not hold in the in-order approach, as the leftmost child is fully processed before the node is added to the stack, so the index 2 1 In parallel to this work, Fried and Klein (2018) present a non-optimal dynamic oracle for training the top-down parser. When two or more non-terminals share their labels within the tree, we use a secondary index to make them unique. 1304 for the node will point to the leftmost span index of the second leftmost child. Note that the information about the span of a constituent, the set of predicted constituents γ and the set α of predicted non-terminal nodes in the stack is not used by the original top-down and inorder parsers. However, we need to include it in parser configurations at learning time to allow an efficient implementation of the"
D18-1161,C12-1059,0,0.515982,"earchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependency algorithms, to bottom-up constituent parsing. They propose to use these dynamic oracles to train shift-reduce parsers instead of a traditional static oracle. The latter follows the standard procedure that uses a gold sequence of transitions to train a model for parsing new sentences at test time. A shift-reduce parser trained with this approach tends to be prone to suffer from error propagation (i.e. errors made in previous states are propagated to subsequent states, causing further mistakes in the transition sequence). Dynamic oracles (Gol"
D18-1161,P18-1249,0,0.0211587,"Missing"
D18-1161,E17-1117,0,0.0453453,"Missing"
D18-1161,Q17-1029,0,0.226931,"nal on the top of the tree, and then considering which should be its child nodes. In contrast to the bottom-up approach, this top-down strategy adds a lookahead 1303 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S NP . VP The public is ADVP still ADJP cautious Figure 1: Simplified constituent tree, taken from English WSJ §22. guidance to the parsing process, while it loses rich local features from partially-built trees. On the other hand, Liu and Zhang (2017a) recently developed a novel strategy that finds a compromise between the strengths of top-down and bottom-up approaches, resulting in state-of-the-art accuracy. Concretely, this parser builds the tree following an in-order traversal: instead of starting the tree from the top, it chooses the non-terminal of the resulting subtree after having the first child node in the stack. In that way each partial constituent tree is created in a bottom-up manner, but the non-terminal node is not chosen when all child nodes are in the stack (as a purely bottom-up parser does), but after the first child is"
D18-1161,Q17-1004,0,0.180036,"nal on the top of the tree, and then considering which should be its child nodes. In contrast to the bottom-up approach, this top-down strategy adds a lookahead 1303 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics S NP . VP The public is ADVP still ADJP cautious Figure 1: Simplified constituent tree, taken from English WSJ §22. guidance to the parsing process, while it loses rich local features from partially-built trees. On the other hand, Liu and Zhang (2017a) recently developed a novel strategy that finds a compromise between the strengths of top-down and bottom-up approaches, resulting in state-of-the-art accuracy. Concretely, this parser builds the tree following an in-order traversal: instead of starting the tree from the top, it chooses the non-terminal of the resulting subtree after having the first child node in the stack. In that way each partial constituent tree is created in a bottom-up manner, but the non-terminal node is not chosen when all child nodes are in the stack (as a purely bottom-up parser does), but after the first child is"
D18-1161,J93-2004,0,0.0649931,"the top-down approach is on par with the bottom-up strategy in terms of accuracy and the in-order parser yields the best accuracy to date on the WSJ. However, despite being two adequate alternatives to the traditional bottom-up strategy, no further work has been undertaken to improve their performance.1 We propose what, to our knowledge, are the first optimal dynamic oracles for both the topdown and in-order shift-reduce parsers, allowing us to train these algorithms with exploration. The resulting parsers outperform the existing versions trained with static oracles on the WSJ Penn Treebank (Marcus et al., 1993) and Chinese Treebank (CTB) benchmarks (Xue et al., 2005). The version of the in-order parser trained with our dynamic oracle achieves the highest accuracy obtained so far by a single fully-supervised greedy shift-reduce system on the WSJ. 2 Preliminaries The original transition system of Sagae and Lavie (2005) parses a sentence from left to right by reading (moving) words from a buffer to a stack, where partial subtrees are built. This process is performed by a sequence of Shift (for reading) and Reduce (for building) transitions that will lead the parser through different states or parser co"
D18-1161,N15-1108,0,0.0262932,"uence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependency algorithms, to bo"
D18-1161,W05-1513,0,0.196108,"in-order transition-based parsers. In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training. In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fullysupervised single-model greedy shift-reduce constituent parser on the WSJ benchmark. 1 Introduction The shift-reduce transition-based framework was initially introduced, and successfully adapted from the dependency formalism, into constituent parsing by Sagae and Lavie (2005), significantly increasing phrase-structure parsing performance. A shift-reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy wa"
D18-1161,P17-1076,0,0.11758,"Missing"
D18-1161,D17-1178,0,0.0245381,"Missing"
D18-1161,P15-1110,0,0.0423566,"Missing"
D18-1161,P15-1113,0,0.0970095,"reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependen"
D18-1161,W09-3825,0,0.0376252,"e-structure parsing performance. A shift-reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), orig"
D18-1161,P13-1043,0,0.154679,"formance. A shift-reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time. Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent. This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabb´e, 2015; Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser. With this, shift-reduce algorithms have become competitive, and are the fastest alternative to perform phrase-structure parsing to date. Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabb´e, 2016; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed fo"
D18-1162,P14-1020,0,0.0541312,"Missing"
D18-1162,D13-1195,0,0.0419242,"Missing"
D18-1162,A00-2018,0,0.760152,"Missing"
D18-1162,D16-1257,0,0.0377732,"g. They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to arrange nodes into constituents. Their approach makes it possible to use off-the-shelf dependency parsers for constituency parsing. In a different line, Vinyals et al. (2015) address the problem by relying on a sequence-to-sequence model where trees are linearized in a depth-first traversal order. Their solution can be seen as a machine translation model that maps a sequence of words into a parenthesized version of the tree. Choe and Charniak (2016) recast parsing as language modeling. They train a generative parser that obtains the phrasal structure of sentences by relying on the Vinyals et al. (2015) intuition and on the Zaremba et al. (2014) model to build the basic language modeling architecture. More recently, Shen et al. (2018) propose an architecture to speed up the current state-of-theart chart parsers trained with deep neural networks (Stern et al., 2017; Kitaev and Klein, 2018). They introduce the concept of syntactic distances, which specify the order in which the splitting points of a sentence will be selected. The model lear"
D18-1162,P97-1003,0,0.681168,"Missing"
D18-1162,N16-1024,0,0.0875573,"while preserving (or even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This line of research has been polished through the years (Wang et al., 2006; Zhu et al., 2013; Dyer et al., 2016; Liu and Zhang, 2017; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018). With an aim more related to our work, other authors have reduced constituency parsing to tasks that can be solved faster or in a more generic way. Fern´andez-Gonz´alez and Martins (2015) reduce phrase structure parsing to dependency parsing. They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to arrange nodes into constituents. Their approach makes it possible to use off-the-shelf dependency parsers for constit"
D18-1162,P15-1147,0,0.160933,"Missing"
D18-1162,P08-1109,0,0.0689594,"mon ancestor. A similar situation can be observed for the closest ancestor of w5 (Z). Non-surjectivity Our encoding, as defined formally in Section 2.1, is injective but not surjective, i.e., not every sequence of |w |− 1 labels of the form (ni , ci ) corresponds to a tree in T|w |. In particular, there are two situations where a label sequence formally has no tree, and thus Φ−1 |w |is not formally defined and we have to use extra heuristics or processing to define it: To overcome this issue, we follow a collapsing approach, as is common in parsers that need special treatment of unary chains (Finkel et al., 2008; Narayan and Cohen, 2016; Shen et al., 2018). For clarity, we use the name intermediate unary chains 1318 • Sequences with conflicting nonterminals. A nonterminal can be the lowest common ancestor of more than two pairs of contiguous words when branches are non-binary. For example, in the tree in Figure 1, the lowest common ancestor of both “the” and “red” and of “red” and “toy” is the same N P node. This translates into c4 = NP , c5 = NP in the label sequence. If we take that sequence and set c5 = VP , we obtain a label sequence that does not strictly correspond to the encoding of any tree,"
D18-1162,Q16-1023,0,0.100604,"Missing"
D18-1162,P18-1249,0,0.107944,"irst traversal order. Their solution can be seen as a machine translation model that maps a sequence of words into a parenthesized version of the tree. Choe and Charniak (2016) recast parsing as language modeling. They train a generative parser that obtains the phrasal structure of sentences by relying on the Vinyals et al. (2015) intuition and on the Zaremba et al. (2014) model to build the basic language modeling architecture. More recently, Shen et al. (2018) propose an architecture to speed up the current state-of-theart chart parsers trained with deep neural networks (Stern et al., 2017; Kitaev and Klein, 2018). They introduce the concept of syntactic distances, which specify the order in which the splitting points of a sentence will be selected. The model learns to predict such distances, to then recursively partition the input in a top-down fashion. Contribution We propose a method to transform constituent parsing into sequence labeling. 1314 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1314–1324 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics This reduces it to the complexity of tasks such as part-"
D18-1162,D12-1096,0,0.03094,"l. (2015) sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach achieves the fastest constituent parsing speeds reported to date on PTB by a wide margin.1 1 Introduction Constituent parsing is a core problem in NLP where the goal is to obtain the syntactic structure of sentences expressed as a phrase structure tree. Traditionally, constituent-based parsers have been built relying on chart-based, statistical models (Collins, 1997; Charniak, 2000; Petrov et al., 2006), which are accurate but slow, with typical speeds well below 10 sentences per second on modern CPUs (Kummerfeld et al., 2012). Several authors have proposed more efficient approaches which are helpful to gain speed while preserving (or even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This l"
D18-1162,Q17-1029,0,0.144512,"r even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This line of research has been polished through the years (Wang et al., 2006; Zhu et al., 2013; Dyer et al., 2016; Liu and Zhang, 2017; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018). With an aim more related to our work, other authors have reduced constituency parsing to tasks that can be solved faster or in a more generic way. Fern´andez-Gonz´alez and Martins (2015) reduce phrase structure parsing to dependency parsing. They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to arrange nodes into constituents. Their approach makes it possible to use off-the-shelf dependency parsers for constituency parsing. In a d"
D18-1162,H94-1020,0,0.825433,"Missing"
D18-1162,P16-1146,0,0.0218959,"ar situation can be observed for the closest ancestor of w5 (Z). Non-surjectivity Our encoding, as defined formally in Section 2.1, is injective but not surjective, i.e., not every sequence of |w |− 1 labels of the form (ni , ci ) corresponds to a tree in T|w |. In particular, there are two situations where a label sequence formally has no tree, and thus Φ−1 |w |is not formally defined and we have to use extra heuristics or processing to define it: To overcome this issue, we follow a collapsing approach, as is common in parsers that need special treatment of unary chains (Finkel et al., 2008; Narayan and Cohen, 2016; Shen et al., 2018). For clarity, we use the name intermediate unary chains 1318 • Sequences with conflicting nonterminals. A nonterminal can be the lowest common ancestor of more than two pairs of contiguous words when branches are non-binary. For example, in the tree in Figure 1, the lowest common ancestor of both “the” and “red” and of “red” and “toy” is the same N P node. This translates into c4 = NP , c5 = NP in the label sequence. If we take that sequence and set c5 = VP , we obtain a label sequence that does not strictly correspond to the encoding of any tree, as it contains a contradi"
D18-1162,W03-3017,0,0.251203,"erfeld et al., 2012). Several authors have proposed more efficient approaches which are helpful to gain speed while preserving (or even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This line of research has been polished through the years (Wang et al., 2006; Zhu et al., 2013; Dyer et al., 2016; Liu and Zhang, 2017; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018). With an aim more related to our work, other authors have reduced constituency parsing to tasks that can be solved faster or in a more generic way. Fern´andez-Gonz´alez and Martins (2015) reduce phrase structure parsing to dependency parsing. They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to ar"
D18-1162,D14-1162,0,0.0799095,"Missing"
D18-1162,N18-1202,0,0.0597077,"Missing"
D18-1162,P06-1055,0,0.305016,"Missing"
D18-1162,N07-1051,0,0.362999,"Missing"
D18-1162,P16-2067,0,0.0451819,"Missing"
D18-1162,N18-1027,0,0.0321176,"ormation from w[i−1:i+1] .3 In particular: • We extract the word form (lowercased), the PoS tag and its prefix of length 2, from w[i−1:i+1] . For these words we also include binary features: whether it is the first word, the last word, a number, whether the word is capitalized or uppercased. • Additionally, for wi we look at the suffixes of both length 3 and 2 (i.e. wi[−3:] and wi[−2:] ). To build our CRF models, we relied on the sklearn-crfsuite library4 . Sequence Labeling Sequence labeling is an structured prediction task that generates an output label for every token in an input sequence (Rei and Søgaard, 2018). Examples of practical tasks that can be formulated under this framework in natural language processing are PoS tagging, chunking or named-entity recognition, which are in general fast. However, to our knowledge, there is no previous work on sequence labeling methods for constituent parsing, as an encoding allowing it was lacking so far. In this work, we consider a range of methods ranging from traditional models to state-of-theart neural models for sequence labeling, to test whether they are valid to train constituency-based parsers following our approach. We give the essential details neede"
D18-1162,W05-1513,0,0.28716,"TB by a wide margin.1 1 Introduction Constituent parsing is a core problem in NLP where the goal is to obtain the syntactic structure of sentences expressed as a phrase structure tree. Traditionally, constituent-based parsers have been built relying on chart-based, statistical models (Collins, 1997; Charniak, 2000; Petrov et al., 2006), which are accurate but slow, with typical speeds well below 10 sentences per second on modern CPUs (Kummerfeld et al., 2012). Several authors have proposed more efficient approaches which are helpful to gain speed while preserving (or even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This line of research has been polished through the years (Wang et al., 2006; Zhu et al., 2013; Dyer et al., 2016; Liu and Zhang, 2017; Fern´andez-Gonz´alez and G´om"
D18-1162,N03-1028,0,0.713239,"and the next, and the nonterminal symbol associated with the lowest common ancestor. We prove that the encoding function is injective for any tree without unary branchings. After applying collapsing techniques, the method can parse unary chains. Second, we use such encoding to present different baselines that can effectively predict the structure of sentences (§3). To do so, we rely on a recurrent sequence labeling model based on BIL STM ’s (Hochreiter and Schmidhuber, 1997; Yang and Zhang, 2018). We also test other models inspired in classic approaches for other tagging tasks (Schmid, 1994; Sha and Pereira, 2003). We use the Penn Treebank (PTB) and the Penn Chinese Treebank (CTB) as testbeds. The comparison against Vinyals et al. (2015), the closest work to ours, shows that our method is able to train more accurate parsers. This is in spite of the fact that our approach addresses constituent parsing as a sequence labeling problem, which is simpler than a sequence-to-sequence problem, where the output sequence has variable/unknown length. Despite being the first sequence labeling method for constituent parsing, our baselines achieve decent accuracy results in comparison to models coming from mature lin"
D18-1162,P18-1108,0,0.501653,"rsing. In a different line, Vinyals et al. (2015) address the problem by relying on a sequence-to-sequence model where trees are linearized in a depth-first traversal order. Their solution can be seen as a machine translation model that maps a sequence of words into a parenthesized version of the tree. Choe and Charniak (2016) recast parsing as language modeling. They train a generative parser that obtains the phrasal structure of sentences by relying on the Vinyals et al. (2015) intuition and on the Zaremba et al. (2014) model to build the basic language modeling architecture. More recently, Shen et al. (2018) propose an architecture to speed up the current state-of-theart chart parsers trained with deep neural networks (Stern et al., 2017; Kitaev and Klein, 2018). They introduce the concept of syntactic distances, which specify the order in which the splitting points of a sentence will be selected. The model learns to predict such distances, to then recursively partition the input in a top-down fashion. Contribution We propose a method to transform constituent parsing into sequence labeling. 1314 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1314–132"
D18-1162,P17-1076,0,0.0858358,"earized in a depth-first traversal order. Their solution can be seen as a machine translation model that maps a sequence of words into a parenthesized version of the tree. Choe and Charniak (2016) recast parsing as language modeling. They train a generative parser that obtains the phrasal structure of sentences by relying on the Vinyals et al. (2015) intuition and on the Zaremba et al. (2014) model to build the basic language modeling architecture. More recently, Shen et al. (2018) propose an architecture to speed up the current state-of-theart chart parsers trained with deep neural networks (Stern et al., 2017; Kitaev and Klein, 2018). They introduce the concept of syntactic distances, which specify the order in which the splitting points of a sentence will be selected. The model learns to predict such distances, to then recursively partition the input in a top-down fashion. Contribution We propose a method to transform constituent parsing into sequence labeling. 1314 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1314–1324 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics This reduces it to the complexi"
D18-1162,P06-1054,0,0.0382509,"ches which are helpful to gain speed while preserving (or even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This line of research has been polished through the years (Wang et al., 2006; Zhu et al., 2013; Dyer et al., 2016; Liu and Zhang, 2017; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018). With an aim more related to our work, other authors have reduced constituency parsing to tasks that can be solved faster or in a more generic way. Fern´andez-Gonz´alez and Martins (2015) reduce phrase structure parsing to dependency parsing. They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to arrange nodes into constituents. Their approach makes it possible to use off-the"
D18-1162,P18-4013,0,0.0417645,"ne.2 The label generated for each word encodes the number of common ancestors in the constituent tree between that word and the next, and the nonterminal symbol associated with the lowest common ancestor. We prove that the encoding function is injective for any tree without unary branchings. After applying collapsing techniques, the method can parse unary chains. Second, we use such encoding to present different baselines that can effectively predict the structure of sentences (§3). To do so, we rely on a recurrent sequence labeling model based on BIL STM ’s (Hochreiter and Schmidhuber, 1997; Yang and Zhang, 2018). We also test other models inspired in classic approaches for other tagging tasks (Schmid, 1994; Sha and Pereira, 2003). We use the Penn Treebank (PTB) and the Penn Chinese Treebank (CTB) as testbeds. The comparison against Vinyals et al. (2015), the closest work to ours, shows that our method is able to train more accurate parsers. This is in spite of the fact that our approach addresses constituent parsing as a sequence labeling problem, which is simpler than a sequence-to-sequence problem, where the output sequence has variable/unknown length. Despite being the first sequence labeling meth"
D18-1162,P13-1043,0,0.146121,"ful to gain speed while preserving (or even improving) accuracy. Sagae and Lavie (2005) present a classifier for constituency parsing that runs in linear time by relying on a shift-reduce stack-based algorithm, instead of a grammar. It is essentially an extension of transition-based dependency parsing 1 This is a revision with improved results of our paper originally published in EMNLP 2018. The previous version contained a bug where the script EVALB was not considering the COLLINS.prm parameter file. (Nivre, 2003). This line of research has been polished through the years (Wang et al., 2006; Zhu et al., 2013; Dyer et al., 2016; Liu and Zhang, 2017; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018). With an aim more related to our work, other authors have reduced constituency parsing to tasks that can be solved faster or in a more generic way. Fern´andez-Gonz´alez and Martins (2015) reduce phrase structure parsing to dependency parsing. They propose an intermediate representation where dependency labels from a head to its dependents encode the nonterminal symbol and an attachment order that is used to arrange nodes into constituents. Their approach makes it possible to use off-the-shelf dependency"
D18-1162,C94-1027,0,0.717096,"ween that word and the next, and the nonterminal symbol associated with the lowest common ancestor. We prove that the encoding function is injective for any tree without unary branchings. After applying collapsing techniques, the method can parse unary chains. Second, we use such encoding to present different baselines that can effectively predict the structure of sentences (§3). To do so, we rely on a recurrent sequence labeling model based on BIL STM ’s (Hochreiter and Schmidhuber, 1997; Yang and Zhang, 2018). We also test other models inspired in classic approaches for other tagging tasks (Schmid, 1994; Sha and Pereira, 2003). We use the Penn Treebank (PTB) and the Penn Chinese Treebank (CTB) as testbeds. The comparison against Vinyals et al. (2015), the closest work to ours, shows that our method is able to train more accurate parsers. This is in spite of the fact that our approach addresses constituent parsing as a sequence labeling problem, which is simpler than a sequence-to-sequence problem, where the output sequence has variable/unknown length. Despite being the first sequence labeling method for constituent parsing, our baselines achieve decent accuracy results in comparison to model"
D19-1160,K18-1030,0,0.265076,"ssumes that such information will be collected during testing and fed as input features to the model (Barrett and Søgaard, 2015b; Hollenstein and Zhang, 2019). What we propose instead is to leverage such information during training. To do so, we rely on MTL and auxiliary tasks. Our work focuses on exploiting the utility of gaze information using just a standard BILSTM, directly building on top of previous work of dependency parsing as sequence labeling (Strzyz et al., 2019b), and ignoring extra tools such as attention. In this line, a future possible solution could be to apply the approach by Barrett et al. (2018) to structured prediction and word-level classification. In their work they used human data as an inductive bias to update the attention weights of the network. In our setup, dependency parsing as sequence labeling is addressed through two main tasks: one to predict the index of the head term, i.e. the subindex (oi , pi ), and another one to predict the dependency relation (di ). Eye-movement discrete labels are learned as auxiliary tasks. We use a hard-sharing architecture where the BILSTMs are fully shared and followed by an independent feedforward layer (followed by a softmax) to predict th"
D19-1160,P16-2094,0,0.461933,"e a transition-based parser, trained on a structured perceptron with discrete features (Collins, 2002; Zhang and Nivre, 2011). However, the experiments were carried out on a parallel toy treebank and the performance was relatively low. Lopopolo et al. (2019) follow the inverse path, and use dependency parsing features to predict eyeregression during training, i.e. cases where the reader goes back to a word of the sentence. In this context, how to retrieve and leverage eye-tracking data has become an active area of research in different NLP fields. Previous studies (Barrett and Søgaard, 2015b; Barrett et al., 2016) suggest that real-time eye-tracking data can be collected at inference time, so that token-level gaze features are used during training but also at test time. However, even if in the near future every user has an eye tracker on top of their screen – a scenario which is far from guaranteed, and raises privacy concerns (Liebling and Preibusch, 2014) – many running NLP applications that process data from various Internet sources will not expect to have any human being reading massive amounts of data. Other studies (Barrett and Søgaard, 2015a; Hollenstein and Zhang, 2019) instead derive gaze feat"
D19-1160,K15-1038,0,0.64993,"atures are useful to improve the performance on new non-gazed annotated treebanks. Accuracy gains are modest but positive, showing the feasibility of the approach. It can serve as a first step towards architectures that can better leverage eye-tracking data or other complementary information available only for training sentences, possibly leading to improvements in syntactic parsing. 1 Introduction Eye trackers and gaze features collected from them have been recently applied to natural language processing (NLP) tasks, such as part-ofspeech tagging (Duffy et al., 1988; Nilsson and Nivre, 2009; Barrett and Søgaard, 2015a), namedentity recognition (Tokunaga et al., 2017) or readability (Gonz´alez-Gardu˜no and Søgaard, 2018). Eye-movement data has been also used for parsing. For example, Barrett and Søgaard (2015b) rank discriminative features to predict syntactic categories (e.g. subject vs. object) and use them to improve a transition-based parser, trained on a structured perceptron with discrete features (Collins, 2002; Zhang and Nivre, 2011). However, the experiments were carried out on a parallel toy treebank and the performance was relatively low. Lopopolo et al. (2019) follow the inverse path, and use d"
D19-1160,W15-2401,0,0.372115,"atures are useful to improve the performance on new non-gazed annotated treebanks. Accuracy gains are modest but positive, showing the feasibility of the approach. It can serve as a first step towards architectures that can better leverage eye-tracking data or other complementary information available only for training sentences, possibly leading to improvements in syntactic parsing. 1 Introduction Eye trackers and gaze features collected from them have been recently applied to natural language processing (NLP) tasks, such as part-ofspeech tagging (Duffy et al., 1988; Nilsson and Nivre, 2009; Barrett and Søgaard, 2015a), namedentity recognition (Tokunaga et al., 2017) or readability (Gonz´alez-Gardu˜no and Søgaard, 2018). Eye-movement data has been also used for parsing. For example, Barrett and Søgaard (2015b) rank discriminative features to predict syntactic categories (e.g. subject vs. object) and use them to improve a transition-based parser, trained on a structured perceptron with discrete features (Collins, 2002; Zhang and Nivre, 2011). However, the experiments were carried out on a parallel toy treebank and the performance was relatively low. Lopopolo et al. (2019) follow the inverse path, and use d"
D19-1160,W02-1001,0,0.419916,"Missing"
D19-1160,W17-5050,0,0.285991,"Missing"
D19-1160,N19-1001,0,0.319913,"tive, then the head of wi is the |oi |-th token to the left whose PoS tag is pi . This parser obtains similar results to competitive transition- and graph-based parsers such as BIST (Kiperwasser and Goldberg, 2016) and can be taken as a strong baseline to test the effect of eye-movement data for dependency parsing. Gaze information In previous studies the choice of number of gaze features used in the experiments has varied, seemingly, depending on the NLP task of interest. For instance, Barrett et al. (2016) distinguish 31 features (where 22 are gaze features) for part-of-speech tagging while Hollenstein and Zhang (2019) use 17 features in the NER task. In another piece of work, 5 gaze features are used for relation classification and sentiment analysis (Hollenstein et al., 2019). Finally, Singh et al. (2016) use gaze features in order to automatically predict reading times for a new text. However, the model predicts only 4 features that are then used as features for readability assessment. We have chosen 12 gaze features and based on the previous work (Barrett et al., 2016; Hollenstein and Zhang, 2019) we have divided them into 4 groups. In particular, we explore the informativeness of the basic gaze feature"
D19-1160,Q16-1023,0,0.0392229,", for each word wi , Strzyz et al.’s approach generates a label li ∈ (oi , pi , di ) that encodes the binary relationship between wi and its head, where: di encodes the dependency relation, and the sub-pair (oi , pi ) the index of such head term, with oi ∈ N and pi ∈ P (a part-of-speech set). If oi is positive, then the head of wi is the oi -th token to the right that has the part-of-speech tag pi . If oi is negative, then the head of wi is the |oi |-th token to the left whose PoS tag is pi . This parser obtains similar results to competitive transition- and graph-based parsers such as BIST (Kiperwasser and Goldberg, 2016) and can be taken as a strong baseline to test the effect of eye-movement data for dependency parsing. Gaze information In previous studies the choice of number of gaze features used in the experiments has varied, seemingly, depending on the NLP task of interest. For instance, Barrett et al. (2016) distinguish 31 features (where 22 are gaze features) for part-of-speech tagging while Hollenstein and Zhang (2019) use 17 features in the NER task. In another piece of work, 5 gaze features are used for relation classification and sentiment analysis (Hollenstein et al., 2019). Finally, Singh et al."
D19-1160,W19-2909,0,0.138164,"Missing"
D19-1160,J93-2004,0,0.0663878,"ets with dependency parsing and gaze annotations. The source code can be found at https://github.com/mstrise/ dep2label-eye-tracking-data. 2 Disjoint data Most of dependency treebanks do not contain gaze annotations. With this setup, in addition to the assumption that eye-movement data is available during training, we aim to show whether we improve the performance of a dependency parser with gaze-annotated data coming from a different corpus. We will use two datasets: one with dependency annotations and another one labeled with eye-movement data. For the former we use the Penn Treebank (PTB) (Marcus et al., 1993), which contains sentences from The Wall Street Journal annotated with phrasestructure trees. We convert it into Stanford Dependencies (de Marneffe et al., 2006) and apply the standard splits for dependency parsing: sections 2 − 21 for training, 22 for development set and 23 for testing, whereas PoS tags are predicted by the Stanford tagger (Toutanova et al., 2003). For gazeannotated data, we will employ the eye-movement annotations from the Dundee Treebank but using a different split (90 − 10 − 0).2 2.2 Methods and materials This section describes in detail the data and the models used in our"
D19-1160,de-marneffe-etal-2006-generating,0,0.178799,"Missing"
D19-1160,W09-1113,0,0.0899237,"Missing"
D19-1160,L16-1262,0,0.0403043,"Missing"
D19-1160,N19-1341,1,0.838303,"s not rely on the assumption of using gaze features as input. Instead, the human data is used as an inductive bias to guide the attention weights in a recurrent neural network for sequence classification (used for tasks such as binary sentiment anal1500 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1500–1506, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ysis a the sentence level). Moreover, it has been shown that both constituency (Vilares et al., 2019) and dependency (Strzyz et al., 2019a) parsing can benefit from multi-task learning. Also related to our work with human data, Gonz´alez-Gardu˜no and Søgaard (2017) show that gaze features learned in a multi-task (MTL) setup can lead to improvements in readability assessment. Contribution In this work we leverage gaze learning for dependency parsing assuming that gaze features are likely to be only available during training – and no gaze features are given, in any form, at inference time. Our approach learns eye-movement features as auxiliary tasks in a multitask framework where both parsing a"
D19-1160,P11-2033,0,0.109026,"Missing"
D19-1160,D17-1035,0,0.0448772,"Missing"
D19-1160,W16-4123,0,0.106656,"ldberg, 2016) and can be taken as a strong baseline to test the effect of eye-movement data for dependency parsing. Gaze information In previous studies the choice of number of gaze features used in the experiments has varied, seemingly, depending on the NLP task of interest. For instance, Barrett et al. (2016) distinguish 31 features (where 22 are gaze features) for part-of-speech tagging while Hollenstein and Zhang (2019) use 17 features in the NER task. In another piece of work, 5 gaze features are used for relation classification and sentiment analysis (Hollenstein et al., 2019). Finally, Singh et al. (2016) use gaze features in order to automatically predict reading times for a new text. However, the model predicts only 4 features that are then used as features for readability assessment. We have chosen 12 gaze features and based on the previous work (Barrett et al., 2016; Hollenstein and Zhang, 2019) we have divided them into 4 groups. In particular, we explore the informativeness of the basic gaze features: total fixation duration on a word w (total fix dur), mean fixation duration on w (mean fix dur, number fixations on w (n fix) and fixation probability on w (fix prob). As early gaze feature"
D19-1160,P19-1531,1,0.877447,"Missing"
D19-1160,N19-1077,1,0.899068,"Missing"
D19-1160,tokunaga-etal-2017-eye,0,0.0265618,"Missing"
D19-1160,N03-1033,0,0.0423763,"ce of a dependency parser with gaze-annotated data coming from a different corpus. We will use two datasets: one with dependency annotations and another one labeled with eye-movement data. For the former we use the Penn Treebank (PTB) (Marcus et al., 1993), which contains sentences from The Wall Street Journal annotated with phrasestructure trees. We convert it into Stanford Dependencies (de Marneffe et al., 2006) and apply the standard splits for dependency parsing: sections 2 − 21 for training, 22 for development set and 23 for testing, whereas PoS tags are predicted by the Stanford tagger (Toutanova et al., 2003). For gazeannotated data, we will employ the eye-movement annotations from the Dundee Treebank but using a different split (90 − 10 − 0).2 2.2 Methods and materials This section describes in detail the data and the models used in our work. 2.1 10 participants is included only in one of the sets.1 Data We will use both parallel data (containing dependency and gaze annotations) and disjoint data (where one dataset contains only parsing annotations and the other one just gaze data). Parallel data The Dundee corpus (Kennedy et al., 2003) contains recordings of measurements for eye movements of ten"
E09-1034,afonso-etal-2002-floresta,0,0.11534,"es Nonprojective By gap degree Gap Gap Gap degree 2 degree 3 deg. > 3 13 2 1 359 4 1 10 0 0 427 13 0 188 10 2 351 51 14 81 21 10 19 7 5 29 0 0 WellNested 204 20257 856 4850 1552 1711 550 1008 665 By nestedness Mildly Strongly Ill-Nested Ill-Nested 1 0 96 0 8 0 15 0 191 0 7 0 5 0 71 0 20 0 Table 1: Counts of dependency trees classified by gap degree, and mild and strong ill-nestedness (for their gap degree); appearing in treebanks for Arabic (Hajiˇc et al., 2004), Czech (Hajiˇc et al., 2006), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), Latin (Bamman and Crane, 2006), Portuguese (Afonso et al., 2002), Slovene (Dˇzeroski et al., 2006), Swedish (Nilsson et al., 2005) and Turkish (Oflazer et al., 2003; Atalay et al., 2003). like the fastest known parsers for LTAG, and can be made O(n6 ) if we use unlexicalised dependencies. When the gap degree is greater than 1, the time complexity goes up by a factor of n2 for each extra unit of gap degree, as in parsers for coupled context-free grammars. Most of the non-projective sentences appearing in treebanks are well-nested and have a small gap degree, so this algorithm directly parses the vast majority of the non-projective constructions present in n"
E09-1034,dzeroski-etal-2006-towards,0,0.13524,"Missing"
E09-1034,P99-1059,0,0.353771,"sed to prove the correctness of a parser: for each input string, a parsing schema’s deduction steps allow us to infer a set of items, called valid items for that string. A schema is said to be sound if all valid final items it produces for any arbitrary string are correct for that string. A schema is said to be complete if all correct final items are valid. A correct parsing schema is one which is both sound and complete. In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions. In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the underlying logic of the parser while abstracting away"
E09-1034,W00-2011,0,0.821295,"only if T is projective. The subtrees induced by nodes wp and wq are interleaved if bwp c ∩ bwq c = ∅ and there are nodes wi , wj ∈ bwp c and wk , wl ∈ bwq c such that i < k < j < l. A dependency tree T is well-nested if it does not contain two interleaved subtrees. A tree that is not well-nested is said to be ill-nested. Note that projective trees are always well-nested, but well-nested trees are not always projective. for well-nested dependency structures of gap degree 1, and prove its correctness. The parser runs in time O(n7 ), the same complexity as the best existing algorithms for LTAG (Eisner and Satta, 2000), and can be optimised to O(n6 ) in the nonlexicalised case; (2) we generalise the previous algorithm to any well-nested dependency structure with gap degree at most k in time O(n5+2k ); (3) we generalise the previous parsers to be able to analyse not only well-nested structures, but also ill-nested structures with gap degree at most k satisfying certain constraints1 , in time O(n4+3k ); and (4) we characterise the set of structures covered by this parser, which we call mildly ill-nested structures, and show that it includes all the trees present in a number of dependency treebanks. 2.2 Depend"
E09-1034,C96-1058,0,0.0911021,"ema is said to be sound if all valid final items it produces for any arbitrary string are correct for that string. A schema is said to be complete if all correct final items are valid. A correct parsing schema is one which is both sound and complete. In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions. In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with D-rules). Furthermore, the choice points in the parsing process and the i"
E09-1034,P08-1110,1,0.793227,"Missing"
E09-1034,N09-1061,1,0.851787,"Missing"
E09-1034,P07-1077,0,0.572169,"Spain cgomezr@udc.es David Weir and John Carroll Department of Informatics University of Sussex, United Kingdom {davidw,johnca}@sussex.ac.uk Abstract the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are “close” to being projective, since they contain only a small proportion of nonprojective arcs. This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007). Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms. Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (VijayShanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and LTAGs (Joshi and"
E09-1034,P07-1021,0,0.686144,"Missing"
E09-1034,P06-2066,0,0.641413,"Universidade da Coru˜na, Spain cgomezr@udc.es David Weir and John Carroll Department of Informatics University of Sussex, United Kingdom {davidw,johnca}@sussex.ac.uk Abstract the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are “close” to being projective, since they contain only a small proportion of nonprojective arcs. This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007). Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms. Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (VijayShanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and L"
E09-1034,W07-2216,0,0.488419,"Missing"
E09-1034,H05-1066,0,0.694573,"Missing"
E09-1034,P05-1013,0,0.235701,"ately, the general problem of parsing ill-nested structures is NPcomplete, even when the gap degree is bounded: this set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism has been proven to be NP-complete (Satta, 1992). The reason for this high complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to interleave in every possible way. However, just as it has been noted that most non-projective structures appearing in practice are only “slightly” nonprojective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in treebanks can be viewed as being only “slightly” ill-nested. In this section, we generalise the algorithms WG1 and WGk to parse a proper superset of the set of well-nested structures in polynomial time; and give a characterisation of this new set of structures, which includes all the structures in several dependency treebanks. 4.2 Computational complexity The WGk parser runs in time O(n5+2k ): as in the case of WG1 , the deduction step with most free variables is Combine Shrinking Gap Centre, and in this case it has 5 + 2k free ind"
E09-1034,P92-1012,0,0.86861,"ring in practice are “close” to being projective, since they contain only a small proportion of nonprojective arcs. This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007). Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms. Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (VijayShanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and LTAGs (Joshi and Schabes, 1997) induce the set of well-nested dependency structures with gap degree at most 1. These results establish that there must be polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, since such parsers exist for their corresponding lexicalised constituency-based formali"
E09-1034,P87-1015,1,0.825185,"Missing"
E09-1034,W03-3023,0,0.231743,"valid final items it produces for any arbitrary string are correct for that string. A schema is said to be complete if all correct final items are valid. A correct parsing schema is one which is both sound and complete. In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions. In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with D-rules). Furthermore, the choice points in the parsing process and the information we can use to make decisions are"
E09-1034,W03-2405,0,\N,Missing
E12-1008,afonso-etal-2002-floresta,0,0.0340658,"9) 83.31 (82.64) 88.30 (86.91) 86.12 (85.48) 88.52 (87.58) 86.60 (84.66) 90.20 (87.73) 82.89 (82.44) 88.61 (87.55) 62.58 (70.96) 73.09 (77.95) Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL) postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP) algorithms, on eight datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006): Arabic (Hajiˇc et al., 2004), Chinese (Chen et al., 2003), Czech (Hajiˇc et al., 2006), Danish (Kromann, 2003), German (Brants et al., 2002), Portuguese (Afonso et al., 2002), Swedish (Nilsson et al., 2005) and Turkish (Oflazer et al., 2003; Atalay et al., 2003). We show labelled (LAS) and unlabelled (UAS) attachment score excluding and including punctuation tokens in the scoring (the latter in brackets). Best results for each language are shown in boldface, and results where the undirected parser outperforms the directed version are marked with an asterisk. Lang. Arabic Chinese Czech Danish German Portug. Swedish Turkish 2Planar LAS(p) UAS(p) 66.73 (67.19) 77.33 (77.11) 84.35 (84.32) 88.31 (88.27) 77.72 (77.91) 83.76 (83.32) 83.81 (83.61) 88.50 (87.63) 86.28 (85."
E12-1008,W03-2405,0,0.0404201,"2.89 (82.44) 88.61 (87.55) 62.58 (70.96) 73.09 (77.95) Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL) postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP) algorithms, on eight datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006): Arabic (Hajiˇc et al., 2004), Chinese (Chen et al., 2003), Czech (Hajiˇc et al., 2006), Danish (Kromann, 2003), German (Brants et al., 2002), Portuguese (Afonso et al., 2002), Swedish (Nilsson et al., 2005) and Turkish (Oflazer et al., 2003; Atalay et al., 2003). We show labelled (LAS) and unlabelled (UAS) attachment score excluding and including punctuation tokens in the scoring (the latter in brackets). Best results for each language are shown in boldface, and results where the undirected parser outperforms the directed version are marked with an asterisk. Lang. Arabic Chinese Czech Danish German Portug. Swedish Turkish 2Planar LAS(p) UAS(p) 66.73 (67.19) 77.33 (77.11) 84.35 (84.32) 88.31 (88.27) 77.72 (77.91) 83.76 (83.32) 83.81 (83.61) 88.50 (87.63) 86.28 (85.76) 88.68 (87.86) 87.04 (84.92) 90.82 (88.14) 83.13 (82.71) 88.57 (87.59) 61.80 (70.09)"
E12-1008,W06-2920,0,0.844425,"is While this may be the simplest cost assignment guaranteed to be correct (as it will simply be the to implement label-based reconstruction, we have tree obtained by decoding the label annotations). found that better empirical results are obtained if we give the algorithm more freedom to create new 5 Experiments arcs from the root, as follows: In this section, we evaluate the performance of the  if (i, j) ∈ A1+ (U ) ∧ (i, j) 6∈ A2 (U ), undirected planar, 2-planar and Covington parsers  1 2 if (i, j) ∈ A1− (U ) ∧ (i, j) 6∈ A2 (U ), on eight datasets from the CoNLL-X shared task c(i, j)  (Buchholz and Marsi, 2006). 2n if (i, j) ∈ A2 (U ). Tables 1, 2 and 3 compare the accuracy of the While the cost of arcs from the dummy root is undirected versions with naive and label-based restill 2n, this is now so even for arcs that are in the construction to that of the directed versions of output of the undirected parser, which had cost 1 the planar, 2-planar and Covington parsers, rebefore. Informally, this means that with this con- spectively. In addition, we provide a comparison figuration the postprocessor does not “trust” the to well-known state-of-the-art projective and nonlinks from the dummy root created"
E12-1008,P10-1151,1,0.832058,"Missing"
E12-1008,P10-1110,0,0.0264753,"transition-based parsers satisfying certain conditions. We apply this approach to obtain undirected variants of the planar and 2-planar parsers and of Covington’s non-projective parser. We perform experiments on several datasets from the CoNLL-X shared task, showing that these variants outperform the original directed algorithms in most of the cases. 1 Introduction Dependency parsing has proven to be very useful for natural language processing tasks. Datadriven dependency parsers such as those by Nivre et al. (2004), McDonald et al. (2005), Titov and Henderson (2007), Martins et al. (2009) or Huang and Sagae (2010) are accurate and efficient, they can be trained from annotated data without the need for a grammar, and they provide a simple representation of syntax that maps to predicateargument structure in a straightforward way. In particular, transition-based dependency parsers (Nivre, 2008) are a type of dependency parsing algorithms which use a model that scores transitions between parser states. Greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity. 0 1 2 3 Figure 1: An example dependency structure where transit"
E12-1008,P06-2066,0,0.0180147,"ar and 2-planar transition systems by G´omez-Rodr´ıguez and Nivre (2010) and the version of the Covington (2001) non-projective parser defined by Nivre (2008). We now outline these directed parsers briefly, a more detailed description can be found in the above references. 2.3.1 Planar The planar transition system by G´omezRodr´ıguez and Nivre (2010) is a linear-time transition-based parser for planar dependency forests, i.e., forests whose dependency arcs do not cross when drawn above the words. The set of planar dependency structures is a very mild extension of that of projective structures (Kuhlmann and Nivre, 2006). Configurations in this system are of the form c = hΣ, B, Ai where Σ and B are disjoint lists of nodes from Vw (for some input w), and A is a set of dependency links over Vw . The list B, called the buffer, holds the input words that are still to be read. The list Σ, called the stack, is initially empty and is used to hold words that have dependency links pending to be created. The system is shown at the top in Figure 2, where the notation Σ |i is used for a stack with top i and tail Σ, and we invert the notation for the buffer for clarity (i.e., i |B as a buffer with top i and tail B). The s"
E12-1008,P09-1039,0,0.048377,"by simplifying existing transition-based parsers satisfying certain conditions. We apply this approach to obtain undirected variants of the planar and 2-planar parsers and of Covington’s non-projective parser. We perform experiments on several datasets from the CoNLL-X shared task, showing that these variants outperform the original directed algorithms in most of the cases. 1 Introduction Dependency parsing has proven to be very useful for natural language processing tasks. Datadriven dependency parsers such as those by Nivre et al. (2004), McDonald et al. (2005), Titov and Henderson (2007), Martins et al. (2009) or Huang and Sagae (2010) are accurate and efficient, they can be trained from annotated data without the need for a grammar, and they provide a simple representation of syntax that maps to predicateargument structure in a straightforward way. In particular, transition-based dependency parsers (Nivre, 2008) are a type of dependency parsing algorithms which use a model that scores transitions between parser states. Greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity. 0 1 2 3 Figure 1: An example dependen"
E12-1008,D07-1013,0,0.130089,"d dependency parsers (Nivre, 2008) are a type of dependency parsing algorithms which use a model that scores transitions between parser states. Greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity. 0 1 2 3 Figure 1: An example dependency structure where transition-based parsers enforcing the single-head constraint will incur in error propagation if they mistakenly build a dependency link 1 → 2 instead of 2 → 1 (dependency links are represented as arrows going from head to dependent). It has been shown by McDonald and Nivre (2007) that such parsers suffer from error propagation: an early erroneous choice can place the parser in an incorrect state that will in turn lead to more errors. For instance, suppose that a sentence whose correct analysis is the dependency graph in Figure 1 is analyzed by any bottom-up or leftto-right transition-based parser that outputs dependency trees, therefore obeying the single-head constraint (only one incoming arc is allowed per node). If the parser chooses an erroneous transition that leads it to build a dependency link from 1 to 2 instead of the correct link from 2 to 1, this will lead"
E12-1008,H05-1066,0,0.272418,"Missing"
E12-1008,P05-1013,0,0.0358793,"the to well-known state-of-the-art projective and nonlinks from the dummy root created by the parser, projective parsers: the planar parsers are comand may choose to change them if it is conve- pared to the arc-eager projective parser by Nivre nient to get a better agreement with label anno- (2003), which is also restricted to planar structations (see Figure 4 for an example of the dif- tures; and the 2-planar parsers are compared with ference between both cost assignments). We be- the arc-eager parser with pseudo-projective translieve that the better accuracy obtained with this formation of Nivre and Nilsson (2005), capable of criterion probably stems from the fact that it is bi- handling non-planar dependencies. ased towards changing links from the root, which We use SVM classifiers from the LIBSVM tend to be more problematic for transition-based package (Chang and Lin, 2001) for all the lanparsers, while respecting the parser output for guages except Chinese, Czech and German. In links located deeper in the dependency structure, these, we use the LIBLINEAR package (Fan et for which transition-based parsers tend to be more al., 2008) for classification, which reduces trainaccurate (McDonald and Nivre,"
E12-1008,W04-2407,0,0.0999522,"Missing"
E12-1008,P81-1022,0,0.761054,"Missing"
E12-1008,W03-3017,0,0.0538739,"ions of head and dependent are lost in undirected graphs. By performing these transformations and leaving the systems otherwise unchanged, we obtain the undirected variants of the planar, 2-planar and Covington algorithms that are shown in Figure 3. Note that the transformation can be applied to any transition system having L EFT-A RC and R IGHT-A RC transitions that are equal except for the direction of the created link, and thus collapsable into one. The above three transition systems fulfill this property, but not every transition system does. For example, the well-known arceager parser of Nivre (2003) pops a node from the stack when creating left arcs, and pushes a node to the stack when creating right arcs, so the transformation cannot be applied to it.2 2 One might think that the arc-eager algorithm could still be transformed by converting each of its arc transitions into an undirected transition, without collapsing them into one. However, this would result into a parser that violates the acyclicity constraint, since the algorithm is designed in such a way that acyclicity is only guaranteed if the single-head constraint is kept. It is easy to see that this problem cannot happen in parser"
E12-1008,J08-4003,0,0.109533,"form the original directed algorithms in most of the cases. 1 Introduction Dependency parsing has proven to be very useful for natural language processing tasks. Datadriven dependency parsers such as those by Nivre et al. (2004), McDonald et al. (2005), Titov and Henderson (2007), Martins et al. (2009) or Huang and Sagae (2010) are accurate and efficient, they can be trained from annotated data without the need for a grammar, and they provide a simple representation of syntax that maps to predicateargument structure in a straightforward way. In particular, transition-based dependency parsers (Nivre, 2008) are a type of dependency parsing algorithms which use a model that scores transitions between parser states. Greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity. 0 1 2 3 Figure 1: An example dependency structure where transition-based parsers enforcing the single-head constraint will incur in error propagation if they mistakenly build a dependency link 1 → 2 instead of 2 → 1 (dependency links are represented as arrows going from head to dependent). It has been shown by McDonald and Nivre (2007) that suc"
E12-1008,W07-2218,0,0.0264922,"cted parsers can be obtained by simplifying existing transition-based parsers satisfying certain conditions. We apply this approach to obtain undirected variants of the planar and 2-planar parsers and of Covington’s non-projective parser. We perform experiments on several datasets from the CoNLL-X shared task, showing that these variants outperform the original directed algorithms in most of the cases. 1 Introduction Dependency parsing has proven to be very useful for natural language processing tasks. Datadriven dependency parsers such as those by Nivre et al. (2004), McDonald et al. (2005), Titov and Henderson (2007), Martins et al. (2009) or Huang and Sagae (2010) are accurate and efficient, they can be trained from annotated data without the need for a grammar, and they provide a simple representation of syntax that maps to predicateargument structure in a straightforward way. In particular, transition-based dependency parsers (Nivre, 2008) are a type of dependency parsing algorithms which use a model that scores transitions between parser states. Greedy deterministic search can be used to select the transition to be taken at each state, thus achieving linear or quadratic time complexity. 0 1 2 3 Figure"
E12-1008,nivre-etal-2006-maltparser,0,\N,Missing
J11-3004,W03-2405,0,0.140806,"Missing"
J11-3004,W06-2922,0,0.801563,"example, the algorithm of Kahane, Nasr, and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with the following initializer step instead of the I NITTER and P REDICTOR: I NITTER : [A(•α ), i, i − 1] A(α ) ∈ P ∧ 1 ≤ i ≤ n The initialization step speciﬁed by Kahane, Nasr, and Rambow (1998) differs from this (directly consuming a nonterminal from the input) but this gives an incomplete algorithm. The problem can be ﬁxed either by using the step shown here instead (bottom–up Earley strategy) or by adding an additional step turning it into a bottom–up left-corner parser. 6.2 Attardi (2006) The non-projective parser of Attardi (2006) extends the algorithm of Yamada and Matsumoto (2003), adding additional shift and reduce actions to handle non-projective dependency structures. These extra actions allow the parser to link to nodes that are several positions deep in the stack, creating non-projective links. In particular, Attardi uses six non-projective actions: two actions to link to nodes that are two positions deep, another two actions for nodes that are three positions deep, and a third pair of actions that generalizes the previous ones to n positions deep for any n. Thus, the"
J11-3004,W98-0507,0,0.655301,"dings of intermediate dependency structures are deﬁned as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules"
J11-3004,P89-1018,0,0.318893,"ditions for the parser’s deduction steps. Side conditions restrict the inference relation by specifying which combinations of values are permissible for the variables appearing in the antecedents and consequent of deduction steps. This parsing schema speciﬁes a recognizer: Given a set of D-rules and an input string w1 . . . wn , the sentence can be parsed (projectively) under those D-rules if and only if the deduction system infers a coherent ﬁnal item. When executing this schema with a deductive engine, the parse forest can be recovered by following back pointers, as in constituency parsers (Billot and Lang 1989). This schema formalizes a parsing logic which is independent of the order and the way linking decisions are taken. Statistical models can be used to determine whether a step linking words a and b in positions i and j—i.e., having (a, i) → (b, j) as a side condition—is executed or not, and probabilities can be attached to items in order to assign different weights to different analyses of the sentence. The side conditions provide an explicit representation of the choice points where probabilistic decisions are made by the control mechanism that is executing the schema. The same principle appli"
J11-3004,W08-2134,0,0.0122277,"MHk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which ﬁnds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pairs of input words, and applies an MST algorithm to ﬁnd a dependency tree covering all the words in the sentence and maximizing the sum of weights. The MST algorithm for directed graphs suggested by McDonald et al. (2005) is not fully constructive: It does not work by building structures and combining them into large structures until it ﬁnds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the spa"
J11-3004,P96-1025,0,0.65053,"nal) items are called correct (ﬁnal) items in the original formulation by Sikkel (1997). 3 Derivable items are called valid items in the original formulation by Sikkel (1997). 545 Computational Linguistics Volume 37, Number 3 Figure 2 Representation of the [i, j, h] item in Collins’s parser, together with one of the dependency structures contained in it (left side); and of the antecedents and consequents of an L-L INK step (right side). White rectangles in an item represent intervals of nodes that have been assigned a head by the parser, and dark squares represent nodes that have no head. 3.1 Collins (1996) One of the most straightforward projective dependency parsing strategies was introduced by Collins (1996), and is based on the CYK bottom–up parsing strategy (Kasami 1965; Younger 1967). Collins’s parser works with dependency trees which are linked to each other by creating links between their heads. The schema for this parser maps every set of D-rules G and input string w1 . . . wn to an instantiated dependency parsing system (ICol96 , H, DCol96 ) such that: Item set: The item set is deﬁned as ICol96 = {[i, j, h] |1 ≤ i ≤ h ≤ j ≤ n}, where item [i, j, h] is deﬁned as the set of forests conta"
J11-3004,N06-1021,0,0.0131231,"number of free variables used in deduction steps of Collins’s parser, it is apparent that its time complexity is O(n5 ): There are O(n5 ) combinations of index values with which each of its L INK steps can be executed.5 This complexity arises because a parentless word (head) may appear in any position in the items generated by the parser; the complexity can be reduced to O(n3 ) by ensuring that parentless words only appear at the ﬁrst or last position of an item. This is the idea behind the parser deﬁned by Eisner (1996), which is still in wide use today (McDonald, Crammer, and Pereira 2005; Corston-Oliver et al. 2006). The parsing schema for this algorithm is deﬁned as follows. Item set: The item set is IEis96 = {[i, j, True, False] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, False, True] |0 ≤ i ≤ j ≤ n} ∪{[i, j, False, False] |0 ≤ i ≤ j ≤ n}, where item [i, j, True, False] corresponds to [i, j, j] ∈ ICol96 , item [i, j, False, True] corresponds to item [i, j, i] ∈ ICol96 , and item [i, j, False, False] is deﬁned as the set of forests 5 For this and the rest of the complexity results in this article, we assume that the linking decision associated with a D-rule can be made in constant time. 547 Computational Linguistics Volu"
J11-3004,W98-0511,0,0.113846,"LETER : [A(αB • β ), i, k] Final items: The ﬁnal item set is {[(S• ), 1, n]}. The schema for Lombardo and Lesmo’s parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the S CANNER always moves the dot over the head symbol ∗, rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between conﬁgurations."
J11-3004,P04-1054,0,0.0273384,"in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves ﬁnding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and ˜ Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), ¨ relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify head–modiﬁer and head–complement relationships, which form the basis of predicate–argument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of dependency parsers ˜ Campus de Elvina, ˜ s/n, 15071 A Coruna, ˜ Spain. ∗ Facultade de Inform´atica, Universidade da Coruna E-mail: cgomezr@udc.es. ∗∗ School"
J11-3004,P05-1067,0,0.0289662,"t includes all gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k) in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves ﬁnding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and ˜ Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), ¨ relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify head–modiﬁer and head–complement relationships, which form the basis of predicate–argument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of depend"
J11-3004,C96-1058,0,0.108842,"parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) → (b, j), which speciﬁes that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps i"
J11-3004,H05-1036,0,0.112317,"Missing"
J11-3004,P99-1059,0,0.152783,"PANS step is used to join two items that overlap at a single word, which must have a parent in only one of the items, so that the result of joining trees coming from both items (without creating any dependency link) is a well-formed dependency tree. Final items: The set of ﬁnal items is {[0, n, False, True]}. Note that these items represent dependency trees rooted at the beginning-of-sentence marker 0, which acts as a “dummy head” for the sentence. In order for the algorithm to parse sentences correctly, we need to deﬁne D-rules to allow the real sentence head to be linked to the node 0. 3.3 Eisner and Satta (1999) Eisner and Satta (1999) deﬁne an O(n3 ) parser for split head automaton grammars which can be used for dependency parsing. This algorithm is conceptually simpler than Eisner’s (1996) algorithm, because it only uses items representing single dependency trees, avoiding items of the form [i, j, False, False]. Item set: The item set is IES99 = {[i, j, i] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, j] |0 ≤ i ≤ j ≤ n}, where items are deﬁned as in Collins’s parsing schema. Deduction steps: The deduction steps for this parser are the following: [i, j, i] [ j + 1, k, k] R-L INK : (wi , i) → (wk , k) [i, k, k] [i, j, i"
J11-3004,W00-2011,0,0.793589,"isms. Developing efﬁcient dependency parsing strategies for these sets of structures has considerable practical interest, in particular, making it possible to parse directly with dependencies in a data-driven manner rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses. In this section, we make four contributions to this enterprise. Firstly, we deﬁne a parser for well-nested structures of gap degree 1, and prove its correctness. The parser runs in time O(n7 ), the same complexity as the best existing algorithms for LTAG (Eisner and Satta 2000), and can be optimized to O(n6 ) in the nonlexicalized case. Secondly, we generalize our algorithm to any well-nested dependency structure with gap degree at most k, resulting in an algorithm with time complexity O(n5+2k ). Thirdly, we generalize the previous parsers in order to include ill-nested structures with gap degree at most k satisfying certain constraints, giving a parser that runs in time O(n4+3k ). Note that parsing unrestricted ill-nested structures, even when the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS for which the recognition problem is NP"
J11-3004,P08-1110,1,0.88419,"Missing"
J11-3004,N09-1061,1,0.809783,"ures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of ﬁnding minimal fan-out binarizations of LCFRS to improve parsing ´ efﬁciency (see Gomez-Rodr´ ıguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our deﬁnition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to ﬁnd a more grammar-oriented deﬁnition that would provide linguistic insight into this set of structures. An alternative generalization of the conce"
J11-3004,E09-1034,1,0.563747,"ures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of ﬁnding minimal fan-out binarizations of LCFRS to improve parsing ´ efﬁciency (see Gomez-Rodr´ ıguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our deﬁnition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to ﬁnd a more grammar-oriented deﬁnition that would provide linguistic insight into this set of structures. An alternative generalization of the conce"
J11-3004,P07-1077,0,0.188126,"m called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dependency grammars. 7. Mildly Non-Projective Dependency Parsing For reasons of computational efﬁciency, many practical implementations of dependency parsing are restricted to projective structures. However, some natural language sentences appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contai"
J11-3004,P98-1106,0,0.567797,"Missing"
J11-3004,P07-1021,0,0.460103,"h n. Note that this parsing schema is not correct, because Covington’s algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reﬂected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dep"
J11-3004,P06-2066,0,0.60887,"Missing"
J11-3004,C96-2122,0,0.453149,"a framework, where the encodings of intermediate dependency structures are deﬁned as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we us"
J11-3004,P05-1012,0,0.551096,"˜ s/n, 15071 A Coruna, ˜ Spain. ∗ Facultade de Inform´atica, Universidade da Coruna E-mail: cgomezr@udc.es. ∗∗ School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: J.A.Carroll@sussex.ac.uk. † School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: D.J.Weir@sussex.ac.uk. Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for publication: 29 January 2011. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 3 are able to represent non-projective structures (McDonald et al. 2005), which is important when parsing free word order languages where discontinuous constituents are common. The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efﬁcient implementations ´ automatically (Gomez-Rodr´ ıguez, Vilares, and Alonso 2009). The f"
J11-3004,D07-1013,0,0.145043,"h n. Note that this parsing schema is not correct, because Covington’s algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reﬂected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dep"
J11-3004,H05-1066,0,0.575344,"Missing"
J11-3004,W07-2216,0,0.686108,"Missing"
J11-3004,W03-3017,0,0.804804,"n]}. The schema for Lombardo and Lesmo’s parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the S CANNER always moves the dot over the head symbol ∗, rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between conﬁgurations. The transition system deﬁned by all the possible conﬁgur"
J11-3004,N07-1050,0,0.163235,"length n. Note that this parsing schema is not correct, because Covington’s algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reﬂected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dep"
J11-3004,W04-2407,0,0.0290287,"Missing"
J11-3004,W06-2933,0,0.0439191,"Missing"
J11-3004,P08-1108,0,0.0320374,"Missing"
J11-3004,P05-1013,0,0.575223,"es appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, ¨ 2005), relating them to lexicalized constituency grammar forKuhlmann, and Mohl malisms. Speciﬁcally, Kuhlmann shows that linear context-free rewriting systems 562 ´ Gomez-Rodr´ ıguez, Carroll, and Weir Dependency"
J11-3004,P92-1012,0,0.732582,"ontain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, ¨ 2005), relating them to lexicalized constituency grammar forKuhlmann, and Mohl malisms. Speciﬁcally, Kuhlmann shows that linear context-free rewriting systems 562 ´ Gomez-Rodr´ ıguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k − 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of wellnested dependency structures with gap degree at most k − 1; and ﬁnally, LTAG (Joshi and Schabes 1997) induces the set of well-nested dependency structures with gap degree at most 1. These results establish that there are polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, because such parsers exist for their corresponding lexicalized constituency-based formalisms. Developin"
J11-3004,P08-1066,0,0.0656057,"Missing"
J11-3004,1993.iwpt-1.22,0,0.540082,"Missing"
J11-3004,W08-2121,0,0.0915845,"Missing"
J11-3004,P87-1015,1,0.765065,"Missing"
J11-3004,W03-3023,0,0.0944423,"ata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) → (b, j), which speciﬁes that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps in data-driven parsers can be"
J11-3004,E99-1020,0,\N,Missing
J11-3004,J13-2004,0,\N,Missing
J11-3004,C98-1102,0,\N,Missing
J11-3004,dzeroski-etal-2006-towards,0,\N,Missing
J11-3004,D07-1096,0,\N,Missing
J11-3004,afonso-etal-2002-floresta,0,\N,Missing
J13-4002,afonso-etal-2002-floresta,0,0.0174617,"Missing"
J13-4002,W03-2405,0,0.0646346,"Missing"
J13-4002,W06-2922,0,0.0770697,"k ´ Universidade da Coruna, ˜ Facultad de Informtica, Campus de Elvina ˜ ∗ Departamento de Computacion, ˜ Spain. E-mail: cgomezr@udc.es. s/n, 15071 A Coruna, ∗∗ Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden. E-mail: joakim.nivre@lingfil.uu.se. Submission received: 13 October 2011; revised submission received: 29 August 2012; accepted for publication: 7 November 2012. doi:10.1162/COLI a 00150 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 data (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Zhang and Clark 2008). Transition systems for dependency parsing come in many different varieties, and our aim in the first part of this article is to deepen our understanding of these systems by analyzing them in a uniform framework. More precisely, we demonstrate that a number of well-known systems from the literature can all be viewed as variants of a stack-based system with five elementary transitions, where different variants are obtained by composing elementary transitions into complex transitions and by adding restrictions on their applicability. We call such systems divisible transit"
J13-4002,P08-2037,0,0.0214824,"Missing"
J13-4002,W06-2920,0,0.0549636,"related to the three main themes of the article: a formal framework for analyzing and constructing transition systems for dependency parsing (Section 3), a procedure for classifying mildly non-projective dependency structures in terms of multiplanarity (Section 4), and a novel transition-based parser for (a subclass of) non-projective dependency structures (Section 5). 6.1 Frameworks for Dependency Parsing Due to the growing popularity of dependency parsing, several proposals have been made that group and study different dependency parsers under common (more or less formal) frameworks. Thus, Buchholz and Marsi (2006) observed that almost all of the systems participating in the CoNLL-X shared task could be classified as belonging to one of two approaches, which they called the “all pairs” and the “stepwise” approaches. This was taken up by McDonald and Nivre (2007), who called the first approach global exhaustive graph-based parsing and the second approach local greedy transition-based parsing. The terms graph-based and transition-based have become well established, even though there now exist graph-based models that do not perform exhaustive search (McDonald and Pereira 2006; Koo et al. 2010) as well as t"
J13-4002,D10-1096,0,0.0247155,"contributions of this article. 2. Dependency Parsing Dependency parsing is based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric relations between the words of a sentence, an idea that has a long tradition in descriptive and theoretical linguistics (Tesni`ere 1959; Sgall, Hajiˇcov´a, and Panevov´a 1986; Mel’ˇcuk 1988; Hudson 1990). In computational linguistics, dependency structures have become increasingly popular in the interface to downstream applications of parsing, such as information extraction (Culotta and Sorensen 2004; Stevenson and Greenwood 2006; Buyko and Hahn 2010), question answering (Shen and Klakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition"
J13-4002,D07-1101,0,0.0189155,"al parsers that specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transit"
J13-4002,D11-1114,1,0.919296,"Missing"
J13-4002,E09-1034,1,0.929585,"Missing"
J13-4002,P07-1077,0,0.51504,"of departure for addressing the problem of non-projective dependency parsing. Despite the impressive results obtained with dependency parsers limited to strictly projective dependency trees—that is, trees where every subtree has a contiguous yield—it is clear that most if not all languages have syntactic constructions whose analysis requires nonprojective trees. It is also clear, however, that allowing arbitrary non-projective trees makes parsing computationally hard (McDonald and Satta 2007) and does not seem justified by the data in available treebanks (Kuhlmann and Nivre 2006; Nivre 2006a; Havelka 2007). This suggests that we should try to find a superset of projective trees that is permissive enough to encompass constructions found in natural language yet restricted enough to permit efficient parsing. Proposals for such a set include trees with bounded arc degree (Nivre 2006a, 2007), well-nested trees with bounded gap degree ¨ 2007), as well as trees parsable by a (Kuhlmann and Nivre 2006; Kuhlmann and Mohl particular transition system such as that proposed by Attardi (2006). In the same vein, Yli-Jyr¨a (2003) introduced the concept of multiplanarity, which generalizes the simple notion of"
J13-4002,D09-1127,0,0.0258658,"Missing"
J13-4002,P10-1110,0,0.221086,"d Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns quality.”) AuxP ✞ ROOT 1  ✞ Adv  ❄ ❄ na7 kvalitu8 quality to ❄ .9 .) Figure 1 Dependency graph for a Czech sentence from the Prague Dependency Treebank. greedy, deterministic parsing (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Nivre 2008), but globally trained models and non-greedy parsing methods such as beam search are increasingly used (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008; Huang, Jiang, and Liu 2009; Huang and Sagae 2010; Zhang and Nivre 2011). In empirical evaluations, the two main approaches to dependency parsing often achieve very similar accuracy, but transition-based parsers tend to be more efficient. In this article, we will be concerned exclusively with transitionbased models. In the remainder of this background section, we first introduce the syntactic representations used by dependency parsers, starting from a general characterization of dependency graphs and discussing a number of different restrictions of this class that will be relevant for the analysis later on. We then go on to review the formal"
J13-4002,W06-2930,0,0.160963,"ied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns quality.”) AuxP ✞ ROOT 1  ✞ Adv  ❄ ❄ na7 kvalitu8 quality to ❄ .9 .) Figure 1 Dependency graph for a Czech sentence from the Prague Dependency Treebank. greedy, deterministic parsing (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Nivre 2008), but globally trained models and non-greedy parsing methods such as beam search are increasingly used (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008; Huang, Jiang, and Liu 2009; Huang and Sagae 2010; Zhang and Nivre 2011). In empirical evaluations, the two main approaches to dependency parsing often achieve very similar accuracy, but transition-based parsers tend to be more efficient. In this article, we will be concerned exclusively with transitionbased models. In the remainder of this background section, we first introduce the syntactic representations used by dependency parsers, starting from a general characterization of dependency graphs and discussing a number of different restrictions"
J13-4002,P98-1106,0,0.184386,"Missing"
J13-4002,P10-1001,0,0.0157734,"specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have"
J13-4002,D10-1125,0,0.0383413,"Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns quality.”) AuxP ✞ ROOT 1  ✞ Adv  ❄ ❄ na7 kvalitu8"
J13-4002,P11-1068,1,0.869244,"Missing"
J13-4002,P07-1021,0,0.0362611,"Missing"
J13-4002,P06-2066,1,0.714911,"he planar parsing system as our point of departure for addressing the problem of non-projective dependency parsing. Despite the impressive results obtained with dependency parsers limited to strictly projective dependency trees—that is, trees where every subtree has a contiguous yield—it is clear that most if not all languages have syntactic constructions whose analysis requires nonprojective trees. It is also clear, however, that allowing arbitrary non-projective trees makes parsing computationally hard (McDonald and Satta 2007) and does not seem justified by the data in available treebanks (Kuhlmann and Nivre 2006; Nivre 2006a; Havelka 2007). This suggests that we should try to find a superset of projective trees that is permissive enough to encompass constructions found in natural language yet restricted enough to permit efficient parsing. Proposals for such a set include trees with bounded arc degree (Nivre 2006a, 2007), well-nested trees with bounded gap degree ¨ 2007), as well as trees parsable by a (Kuhlmann and Nivre 2006; Kuhlmann and Mohl particular transition system such as that proposed by Attardi (2006). In the same vein, Yli-Jyr¨a (2003) introduced the concept of multiplanarity, which gener"
J13-4002,E09-1055,0,0.015666,".9 4.1 Test for Multiplanarity In order for a constraint on non-projective dependency structures to be useful for practical parsing, it must provide a good balance between parsing efficiency and coverage of non-projective phenomena present in natural language treebanks. For example, Kuhlmann and Nivre (2006) and Havelka (2007) have shown that the vast majority of structures present in existing treebanks are well-nested and have a small gap de¨ 2005), leading to an interest in parsers for these gree (Bodirsky, Kuhlmann, and Mohl ´ kinds of structures (Gomez-Rodr´ ıguez, Weir, and Carroll 2009; Kuhlmann and Satta 2009). No similar analysis has been performed for k-planar structures, however. Yli-Jyr¨a (2003) does provide evidence that all except two structures in the Danish Dependency Treebank (Kromann 2003) are at most 3-planar, but his analysis is based on constraints that restrict the possible ways of assigning planes to dependency arcs, and he is not guaranteed to find the minimal number k for which a given structure is k-planar. Here we provide a procedure for finding the minimal natural number k such that a dependency graph is k-planar and use it to show that the vast majority of sentences in a number"
J13-4002,J93-2004,0,0.0497908,"Missing"
J13-4002,H94-1020,0,0.038301,"existing and novel transitionbased models. Finally, we discuss the implementation of efficient parsers based on these transition systems. 2.1 Dependency Graphs In dependency parsing, the syntactic structure of a sentence is modeled by a dependency graph, which represents each token and its syntactic dependents through labeled, directed arcs. This is exemplified in Figure 1 for a Czech sentence taken from the Prague ¨ Dependency Treebank (Hajiˇc et al. 2001; Bohmov´ a et al. 2003), and in Figure 2 for an English sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994).1 In the former case, an artificial token ROOT has been inserted at the beginning of the sentence, serving as the unique root of the graph and ensuring that the graph is a tree even if more than one token is independent of all other tokens. In the latter case, no such device has been used, and we will not in general assume the existence of an artificial root node prefixed to the sentence, although all our models will be compatible with such a device. 1 The dependency graph has in this case been derived automatically from the constituency-based annotation in the treebank using standard head-fi"
J13-4002,P09-1039,0,0.038063,"Missing"
J13-4002,D10-1004,0,0.0132273,"hbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns quality.”) AuxP ✞ ROOT 1  ✞ Adv  ❄ ❄ na7 kvalitu8 quality to ❄ .9 .) Fig"
J13-4002,P05-1012,0,0.0950336,"Missing"
J13-4002,D07-1013,1,0.889495,"Missing"
J13-4002,E06-1011,0,0.309445,"to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only"
J13-4002,W07-2216,0,0.352462,"s that are assumed in most existing systems. In the second part of the article, we take the planar parsing system as our point of departure for addressing the problem of non-projective dependency parsing. Despite the impressive results obtained with dependency parsers limited to strictly projective dependency trees—that is, trees where every subtree has a contiguous yield—it is clear that most if not all languages have syntactic constructions whose analysis requires nonprojective trees. It is also clear, however, that allowing arbitrary non-projective trees makes parsing computationally hard (McDonald and Satta 2007) and does not seem justified by the data in available treebanks (Kuhlmann and Nivre 2006; Nivre 2006a; Havelka 2007). This suggests that we should try to find a superset of projective trees that is permissive enough to encompass constructions found in natural language yet restricted enough to permit efficient parsing. Proposals for such a set include trees with bounded arc degree (Nivre 2006a, 2007), well-nested trees with bounded gap degree ¨ 2007), as well as trees parsable by a (Kuhlmann and Nivre 2006; Kuhlmann and Mohl particular transition system such as that proposed by Attardi (2006)."
J13-4002,D07-1100,0,0.014919,"ouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only on"
J13-4002,W03-3017,1,0.874328,"its transitions can be written as a composition of restrictions of the elementary transitions S HIFT, U NSHIFT, R EDUCE, L EFT-A RC, and R IGHT-A RC. Note that the definition allows the use of unrestricted elementary transitions in the composition, because for any transition t, we have that t C = t.4 3.1 Examples of Divisible Transition Systems In this section, we show that a number of transition-based parsers from the literature use divisible transition systems that can be defined using only elementary transitions. This includes the arc-eager and arc-standard projective parsers described in Nivre (2003) and Nivre (2008), the arc-eager and arc-standard parsers for directed acyclic graphs from ´ Sagae and Tsujii (2008), the hybrid parser of Kuhlmann, Gomez-Rodr´ ıguez, and Satta (2011), and the easy-first parser of Goldberg and Elhadad (2010). We also give examples of transition systems that are not divisible (Attardi 2006; Nivre 2009). First of all, we define four standard subsets of the configuration set C: Hσ (C) = {(σ|i, β, A) ∈ C |∃j : ( j, i) ∈ A} Hσ (C) = {(σ|i, β, A) ∈ C |¬∃j : ( j, i) ∈ A} Hβ (C) = {(σ, i|β, A) ∈ C |∃j : ( j, i) ∈ A} Hβ (C) = {(σ, i|β, A) ∈ C |¬∃j : ( j, i) ∈ A} The s"
J13-4002,W04-0308,1,0.279576,"left-headed arc, S HIFT and R EDUCE jointly remove the dependent of the new arc from the buffer, and U NSHIFT moves the head of the new arc back to the buffer so that it can find a head to the left. It is worth noting that the arc-standard system for projective trees does not make use of restrictions. Although this description of the arc-standard parser corresponds to its definition in Nivre (2008), where arcs are created involving the topmost stack node and the first buffer node, the system has also been presented in an equivalent form with arcs built between the two top nodes in the stack (Nivre 2004). This variant can also be described as a divisible transition system, with L EFT-A RCAS = U NSHIFT; L EFT-A RC; R EDUCE; S HIFT and R IGHT-A RCAS = U NSHIFT; R IGHT-A RC; S HIFT; R EDUCE.5 Example 4 Nivre’s (2003) arc-eager parser is a parser for projective dependency trees, which adds arcs in a strict left-to-right order using the following transitions: S HIFTAE = S HIFT R EDUCEAE = R EDUCE L EFT-A RCAE = L EFT-A RC R IGHT-A RCAE = R IGHT-A RC; S HIFT Hσ (C) Hσ (C) ; R EDUCE As in the first example, the S HIFTAE transition is equivalent to the elementary S HIFT transition, but the R IGHT-A"
J13-4002,N07-1050,1,0.930228,"lakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al."
J13-4002,J08-4003,1,0.0530427,"plexity. Finally, we show that the 2-planar parser, when evaluated on data sets with a non-negligible proportion of non-projective trees, gives significant improvements in parsing accuracy over the corresponding 1-planar and projective parsers, and provides comparable accuracy to the widely used arc-eager pseudo-projective parser. 800 ´ Gomez-Rodr´ ıguez and Nivre Divisible Transition Systems and Multiplanarity The remainder of the article is structured as follows. Section 2 reviews basic concepts of dependency parsing and in particular the formalization of stack-based transition systems from Nivre (2008). Section 3 introduces our system of elementary transitions, uses it to analyze a number of parsing algorithms from the literature as divisible transition systems, proves a number of theoretical results about the expressivity and complexity of such systems, and finally introduces a divisible transition system for 1-planar dependency parsing. Section 4 reviews the notion of multiplanarity, introduces an efficient procedure for determining the smallest k for which a dependency tree is k-planar, and uses this procedure in an empirical investigation of available dependency treebanks. Section 5 sho"
J13-4002,P09-1040,1,0.556945,"Missing"
J13-4002,W04-2407,1,0.771115,"Missing"
J13-4002,nivre-etal-2006-maltparser,1,0.817796,"idely used method for nonprojective transition-based parsing and as such a competitive baseline for the 2-planar parser. In order to make the comparison as exact as possible, we have chosen to implement all four systems in the MaltParser framework and use the same type of classifiers and feature models. For the arc-eager baselines, we copy the set-up from the CoNLL-X shared task on dependency parsing, which includes the use of support vector machines with a polynomial kernel, history-based feature models tuned separately for each language, and pseudo-projective parsing with the Head encoding (Nivre et al. 2006). For the 1-planar and 2-planar parsers, we use the same type of classifier but modify the feature model to take into account the following systematic differences between the transition systems: r r In both the 1-planar and 2-planar parser, we need to add features over the arc connecting the top node of the stack and the first node of the buffer (if any). No such arc can exist in the arc-eager system used by the projective and pseudo-projective baseline systems. In the 2-planar parser, we need to add features over the top nodes of the inactive stack. No such nodes exist in the 1-planar and arc"
J13-4002,W06-2933,1,0.732346,"Missing"
J13-4002,P05-1013,1,0.930901,"ich is a planar graph with n nodes. Moreover, if the S INGLE -H EAD and A CYCLICITY constraints are used, the maximum number of arcs is n − 1, because every node can have at most one incoming arc and there must be at least one root. 835 Computational Linguistics Volume 39, Number 4 algorithm in MaltParser (Nivre, Hall, and Nilsson 2006), this system is also the basis of the ISBN Dependency Parser (Titov and Henderson 2007) and ZPar (Zhang and Clark 2008; Zhang and Nivre 2011). In addition to a strictly projective arc-eager parser, we also include a version that uses pseudo-projective parsing (Nivre and Nilsson 2005) to recover non-projective arcs. This is the most widely used method for nonprojective transition-based parsing and as such a competitive baseline for the 2-planar parser. In order to make the comparison as exact as possible, we have chosen to implement all four systems in the MaltParser framework and use the same type of classifiers and feature models. For the arc-eager baselines, we copy the set-up from the CoNLL-X shared task on dependency parsing, which includes the use of support vector machines with a polynomial kernel, history-based feature models tuned separately for each language, and"
J13-4002,P05-1034,0,0.0361201,"Missing"
J13-4002,W06-1616,0,0.0173853,"arsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM"
J13-4002,C08-1095,0,0.0920755,"NSHIFT, R EDUCE, L EFT-A RC, and R IGHT-A RC. Note that the definition allows the use of unrestricted elementary transitions in the composition, because for any transition t, we have that t C = t.4 3.1 Examples of Divisible Transition Systems In this section, we show that a number of transition-based parsers from the literature use divisible transition systems that can be defined using only elementary transitions. This includes the arc-eager and arc-standard projective parsers described in Nivre (2003) and Nivre (2008), the arc-eager and arc-standard parsers for directed acyclic graphs from ´ Sagae and Tsujii (2008), the hybrid parser of Kuhlmann, Gomez-Rodr´ ıguez, and Satta (2011), and the easy-first parser of Goldberg and Elhadad (2010). We also give examples of transition systems that are not divisible (Attardi 2006; Nivre 2009). First of all, we define four standard subsets of the configuration set C: Hσ (C) = {(σ|i, β, A) ∈ C |∃j : ( j, i) ∈ A} Hσ (C) = {(σ|i, β, A) ∈ C |¬∃j : ( j, i) ∈ A} Hβ (C) = {(σ, i|β, A) ∈ C |∃j : ( j, i) ∈ A} Hβ (C) = {(σ, i|β, A) ∈ C |¬∃j : ( j, i) ∈ A} The set Hσ (C) is the subset of configurations where the node on top of the stack has been assigned a head in A, and Hσ ("
J13-4002,P06-1112,0,0.0307418,"cy Parsing Dependency parsing is based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric relations between the words of a sentence, an idea that has a long tradition in descriptive and theoretical linguistics (Tesni`ere 1959; Sgall, Hajiˇcov´a, and Panevov´a 1986; Mel’ˇcuk 1988; Hudson 1990). In computational linguistics, dependency structures have become increasingly popular in the interface to downstream applications of parsing, such as information extraction (Culotta and Sorensen 2004; Stevenson and Greenwood 2006; Buyko and Hahn 2010), question answering (Shen and Klakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007"
J13-4002,D08-1016,0,0.0112309,"h-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and learn models for scoring entire parse trees for a given sentence. Many of these models permit exact inference using dynamic programming (Eisner 1996; McDonald, Crammer, and Pereira 2005; Carreras 2007; Koo and Collins 2010), but recent work has explored approximate search methods in order to widen the scope of features especially when processing non-projective trees (McDonald and Pereira 2006; Riedel and Clarke 2006; Nakagawa 2007; Smith and Eisner 2008; Martins, Smith, and Xing 2009; Koo et al. 2010; Martins et al. 2010). Transition-based parsers parameterize the parsing problem by the structure of a transition system, or abstract state machine, for mapping sentences to dependency trees and learn models for scoring individual transitions from one state to the other. Traditionally, transition-based parsers have relied on local optimization and 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns qual"
J13-4002,W06-0202,0,0.0235901,"nar parsers, are entirely new contributions of this article. 2. Dependency Parsing Dependency parsing is based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric relations between the words of a sentence, an idea that has a long tradition in descriptive and theoretical linguistics (Tesni`ere 1959; Sgall, Hajiˇcov´a, and Panevov´a 1986; Mel’ˇcuk 1988; Hudson 1990). In computational linguistics, dependency structures have become increasingly popular in the interface to downstream applications of parsing, such as information extraction (Culotta and Sorensen 2004; Stevenson and Greenwood 2006; Buyko and Hahn 2010), question answering (Shen and Klakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into grap"
J13-4002,W07-2218,0,0.396405,"nd 801 Computational Linguistics Volume 39, Number 4 ✞  AuxK ✞  AuxP ✞ Pred Sb ✞  ✞ Atr  ✞ AuxZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns quality.”) AuxP ✞ ROOT 1  ✞ Adv  ❄ ❄ na7 kvalitu8 quality to ❄ .9 .) Figure 1 Dependency graph for a Czech sentence from the Prague Dependency Treebank. greedy, deterministic parsing (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Nivre 2008), but globally trained models and non-greedy parsing methods such as beam search are increasingly used (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008; Huang, Jiang, and Liu 2009; Huang and Sagae 2010; Zhang and Nivre 2011). In empirical evaluations, the two main approaches to dependency parsing often achieve very similar accuracy, but transition-based parsers tend to be more efficient. In this article, we will be concerned exclusively with transitionbased models. In the remainder of this background section, we first introduce the syntactic representations used by dependency parsers, starting from a general characterization of dependency graphs and discussing a number of different restrictions of this class that will b"
J13-4002,D11-1116,0,0.0225824,"Missing"
J13-4002,N09-1028,0,0.013601,"nary, asymmetric relations between the words of a sentence, an idea that has a long tradition in descriptive and theoretical linguistics (Tesni`ere 1959; Sgall, Hajiˇcov´a, and Panevov´a 1986; Mel’ˇcuk 1988; Hudson 1990). In computational linguistics, dependency structures have become increasingly popular in the interface to downstream applications of parsing, such as information extraction (Culotta and Sorensen 2004; Stevenson and Greenwood 2006; Buyko and Hahn 2010), question answering (Shen and Klakow 2006; Bikel and Castelli 2008), and machine translation (Quirk, Menezes, and Cherry 2005; Xu et al. 2009). And although dependency structures can easily be extracted from other syntactic representations, such as phrase structure trees, this has also led to an increased interest in statistical parsers that specifically produce dependency trees (Eisner 1996; Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005). Current approaches to statistical dependency parsing can be broadly grouped into graph-based and transition-based techniques (McDonald and Nivre 2007). Graphbased parsers parameterize the parsing problem by the structure of the dependency trees and l"
J13-4002,W03-3023,0,0.600794,"ncy trees, guided by statistical models trained on treebank ´ Universidade da Coruna, ˜ Facultad de Informtica, Campus de Elvina ˜ ∗ Departamento de Computacion, ˜ Spain. E-mail: cgomezr@udc.es. s/n, 15071 A Coruna, ∗∗ Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden. E-mail: joakim.nivre@lingfil.uu.se. Submission received: 13 October 2011; revised submission received: 29 August 2012; accepted for publication: 7 November 2012. doi:10.1162/COLI a 00150 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 data (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Zhang and Clark 2008). Transition systems for dependency parsing come in many different varieties, and our aim in the first part of this article is to deepen our understanding of these systems by analyzing them in a uniform framework. More precisely, we demonstrate that a number of well-known systems from the literature can all be viewed as variants of a stack-based system with five elementary transitions, where different variants are obtained by composing elementary transitions into complex transitions and by adding restrictions on their applicab"
J13-4002,D08-1059,0,0.178616,"de da Coruna, ˜ Facultad de Informtica, Campus de Elvina ˜ ∗ Departamento de Computacion, ˜ Spain. E-mail: cgomezr@udc.es. s/n, 15071 A Coruna, ∗∗ Department of Linguistics and Philology, Uppsala University, Box 635, 75126 Uppsala, Sweden. E-mail: joakim.nivre@lingfil.uu.se. Submission received: 13 October 2011; revised submission received: 29 August 2012; accepted for publication: 7 November 2012. doi:10.1162/COLI a 00150 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 data (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Zhang and Clark 2008). Transition systems for dependency parsing come in many different varieties, and our aim in the first part of this article is to deepen our understanding of these systems by analyzing them in a uniform framework. More precisely, we demonstrate that a number of well-known systems from the literature can all be viewed as variants of a stack-based system with five elementary transitions, where different variants are obtained by composing elementary transitions into complex transitions and by adding restrictions on their applicability. We call such systems divisible transition systems and prove a"
J13-4002,P11-2033,1,0.790329,"xZ  ❄ ❄ ❄ ❄ ❄ je4 jen5 jedna6 Z2 nich3 only is them (Out-of one-FEM - SG (“Only one of them concerns quality.”) AuxP ✞ ROOT 1  ✞ Adv  ❄ ❄ na7 kvalitu8 quality to ❄ .9 .) Figure 1 Dependency graph for a Czech sentence from the Prague Dependency Treebank. greedy, deterministic parsing (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; Attardi 2006; Nivre 2008), but globally trained models and non-greedy parsing methods such as beam search are increasingly used (Johansson and Nugues 2006; Titov and Henderson 2007; Zhang and Clark 2008; Huang, Jiang, and Liu 2009; Huang and Sagae 2010; Zhang and Nivre 2011). In empirical evaluations, the two main approaches to dependency parsing often achieve very similar accuracy, but transition-based parsers tend to be more efficient. In this article, we will be concerned exclusively with transitionbased models. In the remainder of this background section, we first introduce the syntactic representations used by dependency parsers, starting from a general characterization of dependency graphs and discussing a number of different restrictions of this class that will be relevant for the analysis later on. We then go on to review the formalization of transition s"
J13-4002,J11-3004,1,\N,Missing
J13-4002,C96-1058,0,\N,Missing
J13-4002,P04-1054,0,\N,Missing
J13-4002,C98-1102,0,\N,Missing
J13-4002,P10-1151,1,\N,Missing
J13-4002,N10-1115,0,\N,Missing
J16-4008,W06-2922,0,0.0929471,"lit property (WG1 S) can be parsed in O(n6 ), whereas for M1I trees with the head-split property (M1IS) the complexity is O(n5 ). Mildly Ill-nested. A superset of WGk trees, mildly ill-nested trees of gap degree up to k (MGk ) include all the dependency trees that have at least one binarization of gap ´ degree k. They can be parsed in time O(n4+3k ) (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). Note that this is the same complexity as for WGk for k = 1, but larger for k > 1. Attardi Degree 2. The set of trees that can be parsed with the transitions of degree up to 2 in the transition system of Attardi (2006) is also amenable to dynamic programming ´ parsing, in time O(n7 ) (Cohen, Gomez-Rodr´ ıguez, and Satta 2011). This set, which we will call AD2 , includes ill-nested trees and trees with unbounded gap degree. ´ MHk trees. Gomez-Rodr´ ıguez, Carroll, and Weir (2011) define a generalization of the tabular algorithm obtained from the shift-reduce parser of Yamada and Matsumoto ´ (2003), or from the arc-hybrid transition system (Gomez-Rodr´ ıguez, Carroll, and Weir ´ 2008; Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011). This parser, called MHk , has items representing a span dominated by several hea"
J16-4008,J13-1002,0,0.0294388,"ence assumptions (McDonald and Satta 2007). For this reason, researchers have proposed various classes of mildly non-projective trees: restricted classes of trees that allow a limited degree of non-projectivity, permitting crossing dependencies only under certain ´ ∗ Research Group on Language and Information Society (LyS), Departamento de Computacion, ˜ Campus de A Coruna, ˜ 15071, A Coruna, ˜ Spain. E-mail: carlos.gomez@udc.es. Universidade da Coruna, 1 We will assume the conventional representation of syntactic dependency analyses as trees rooted at a dummy node. Its presence and location (Ballesteros and Nivre 2013) has no effect on the coverage of the considered classes of trees, except for 1-endpoint-crossing trees and crossing-interval trees. In these cases, we assume that the dummy root is located on the left, as in the papers in which they were defined. Submission received: 4 February 2016; accepted for publication: 25 April 2016. doi:10.1162/COLI a 00267 © 2017 Association for Computational Linguistics Computational Linguistics Volume 42, Number 4 conditions. The goal of these classes is to combine a high coverage of the syntactic phenomena found in real sentences with efficient parsing.2 In this a"
J16-4008,D11-1114,1,0.837484,"Missing"
J16-4008,C96-1058,0,0.0202567,"etween coverage and computational complexity of exact parsing is achieved by either 1-endpoint-crossing trees or MHk trees, depending on the level of coverage desired. We also present some properties of the relation of MHk trees to other relevant classes of trees. 1. Introduction A syntactic dependency tree is projective if the yield of each node is a substring of the sentence—or equivalently, if no dependencies cross when drawn above the words.1 Projectivity is advantageous for efficient parsing: Exact inference for parsing models restricted to projective trees can be achieved in cubic time (Eisner 1996), and shift-reduce parsers can process them with very simple transitions in linear time (Nivre 2006). For this reason, and because crossing dependencies have traditionally been rare in corpora of languages like English, Chinese, or Japanese, many implementations of dependency parsers assume projectivity (Nivre 2006). However, crossing dependencies are needed to represent some linguistic phenomena like topicalization, scrambling, wh-movement, or extraposition, so it is necessary for natural language parsers to support non-projectivity, especially when working with languages with flexible word o"
J16-4008,P08-1110,1,0.943725,"Missing"
J16-4008,J11-3004,1,0.937666,"Missing"
J16-4008,P10-1151,1,0.875088,"Missing"
J16-4008,J13-4002,1,0.882258,"Missing"
J16-4008,E09-1034,1,0.941448,"Missing"
J16-4008,P07-1077,0,0.0240726,"tivity, with the goal of evaluating them in terms of the tradeoff between coverage and efficiency. For this purpose, we measure their coverage on a set of syntactic treebanks of 30 languages, analyzed under two different annotation criteria. Thus, the main contribution of this work is that we provide homogeneous measurements of the coverage of a wide range of mildly non-projective classes of trees on a large collection of treebanks, relating them to their computational properties for parsing. To our knowledge, this is the first study providing an extensive comparison of such classes: Although Havelka (2007) also measured the coverage of several restrictions on nonprojectivity, little was known at the time about which restrictions could be exploited for efficient parsing, so only a few of the classes discussed there are relevant for parsing. Furthermore, existing coverage data in the literature (both in that study and in the papers describing subsequently discovered classes of trees, cited herein) refer to small sets of treebanks that vary across reports, when reported at all. Additionally, we present some results relating MHk trees, one of the sets with the best coverage–efficiency tradeoff, wit"
J16-4008,P11-1068,1,0.922774,"Missing"
J16-4008,W07-2216,0,0.372354,"ave traditionally been rare in corpora of languages like English, Chinese, or Japanese, many implementations of dependency parsers assume projectivity (Nivre 2006). However, crossing dependencies are needed to represent some linguistic phenomena like topicalization, scrambling, wh-movement, or extraposition, so it is necessary for natural language parsers to support non-projectivity, especially when working with languages with flexible word order. Unfortunately, exact inference is intractable for models that support arbitrary non-projective trees, except under strong independence assumptions (McDonald and Satta 2007). For this reason, researchers have proposed various classes of mildly non-projective trees: restricted classes of trees that allow a limited degree of non-projectivity, permitting crossing dependencies only under certain ´ ∗ Research Group on Language and Information Society (LyS), Departamento de Computacion, ˜ Campus de A Coruna, ˜ 15071, A Coruna, ˜ Spain. E-mail: carlos.gomez@udc.es. Universidade da Coruna, 1 We will assume the conventional representation of syntactic dependency analyses as trees rooted at a dummy node. Its presence and location (Ballesteros and Nivre 2013) has no effect"
J16-4008,P09-1040,0,0.0803259,"), and Mild+0-Inherit (M0I) trees, or gap-minding trees, in O(n5 ). Head-Split. The head-split property is a restriction that forbids trees where a node’s yield has a gap that includes its head, but not the gap in its head’s yield. This allows dynamic programming parsers to split subtrees into two at the position of their heads, reducing the complexity of parsing several subclasses of WG1 trees: Satta and Kuhlmann (2013) 2 Another option is to use models that forgo exact inference, but still achieve competitive results for non-projective parsing in quadratic (Nivre 2008) or even linear time (Nivre 2009). 810 ´ ıguez Gomez-Rodr´ Restricted Non-Projectivity: Coverage vs. Efficiency show how WG1 trees with the head-split property (WG1 S) can be parsed in O(n6 ), whereas for M1I trees with the head-split property (M1IS) the complexity is O(n5 ). Mildly Ill-nested. A superset of WGk trees, mildly ill-nested trees of gap degree up to k (MGk ) include all the dependency trees that have at least one binarization of gap ´ degree k. They can be parsed in time O(n4+3k ) (Gomez-Rodr´ ıguez, Carroll, and Weir 2011). Note that this is the same complexity as for WGk for k = 1, but larger for k > 1. Attardi"
J16-4008,Q14-1004,0,0.418409,"has items representing a span dominated by several head nodes (hence the acronym, for “multiheaded”). It has complexity O(nk ) and is projective for k = 3, but covers increasingly large sets of non-projective trees for values of k > 3, which we will call MHk trees. 1-Endpoint-Crossing. Pitler, Kannan, and Marcus (2013) define 1-Endpoint-Crossing trees (1EC trees) as dependency trees such that all the arcs that cross a given arc have a common vertex. This set of trees includes trees that are ill-nested and have unbounded gap degree, and can be parsed in O(n4 ) (Pitler, Kannan, and Marcus 2013; Pitler 2014). k-Planar. k-Planar trees (k-P, equivalent to k-page book embeddings in graph theory) are those whose non-dummy arcs can be partitioned into k sets (called planes), in such a way that arcs belonging to the same plane do not cross (Yli-Jyr¨a 2003). No globally optimal parser is known for these trees, but they can be handled by a linear-time ´ transition-based parser with k stacks (Gomez-Rodr´ ıguez and Nivre 2010, 2013). k-Crossing Interval. k-Crossing Interval trees (k-C) are defined by Pitler and McDonald (2015) with a restriction on intervals formed by crossing arcs. 2-C trees can be parsed"
J16-4008,D12-1044,0,0.146121,"Missing"
J16-4008,Q13-1002,0,0.362834,"Missing"
J16-4008,N15-1068,0,0.524507,"d and have unbounded gap degree, and can be parsed in O(n4 ) (Pitler, Kannan, and Marcus 2013; Pitler 2014). k-Planar. k-Planar trees (k-P, equivalent to k-page book embeddings in graph theory) are those whose non-dummy arcs can be partitioned into k sets (called planes), in such a way that arcs belonging to the same plane do not cross (Yli-Jyr¨a 2003). No globally optimal parser is known for these trees, but they can be handled by a linear-time ´ transition-based parser with k stacks (Gomez-Rodr´ ıguez and Nivre 2010, 2013). k-Crossing Interval. k-Crossing Interval trees (k-C) are defined by Pitler and McDonald (2015) with a restriction on intervals formed by crossing arcs. 2-C trees can be parsed accurately with a linear-time shift-reduce parser with two registers (Pitler and McDonald 2015). 2-C trees are a subset of 1EC trees, which in turn are a subset of 2-P trees. 3. Materials and Methods Corpora. We evaluate the coverage of each class described in Section 2 on HamleDT 2.0 (Rosa et al. 2014), a collection of harmonized versions of existing treebanks of 30 diverse languages, under two different annotations: Prague and Universal Stanford dependencies. Both annotation styles are interesting for parsing:"
J16-4008,W15-2131,0,0.0521113,"Missing"
J16-4008,rosa-etal-2014-hamledt,0,0.0653555,"Missing"
J16-4008,Q13-1022,0,0.013974,"rcs that cross a gap in its yield. Imposing gap inheritance bounds as additional restrictions on WG1 trees, two relevant classes of trees are obtained: Mild+1-Inherit (M1I) trees can be parsed in O(n6 ), and Mild+0-Inherit (M0I) trees, or gap-minding trees, in O(n5 ). Head-Split. The head-split property is a restriction that forbids trees where a node’s yield has a gap that includes its head, but not the gap in its head’s yield. This allows dynamic programming parsers to split subtrees into two at the position of their heads, reducing the complexity of parsing several subclasses of WG1 trees: Satta and Kuhlmann (2013) 2 Another option is to use models that forgo exact inference, but still achieve competitive results for non-projective parsing in quadratic (Nivre 2008) or even linear time (Nivre 2009). 810 ´ ıguez Gomez-Rodr´ Restricted Non-Projectivity: Coverage vs. Efficiency show how WG1 trees with the head-split property (WG1 S) can be parsed in O(n6 ), whereas for M1I trees with the head-split property (M1IS) the complexity is O(n5 ). Mildly Ill-nested. A superset of WGk trees, mildly ill-nested trees of gap degree up to k (MGk ) include all the dependency trees that have at least one binarization of g"
J16-4008,W03-3023,0,0.211537,"Missing"
J16-4008,J08-4003,0,\N,Missing
K17-3016,P17-2107,0,0.0625019,"Missing"
K17-3016,W06-2922,0,0.0446912,"two arcs i → − j and k → − l where min(i, j) < min(k, l) < max(i, j) < max(k, l), i.e., if there is any pair of arcs that cross when they are drawn over the sentence, as shown in Figure 1. Unrestricted non-projective parsing allows more accurate syntactic representations than projective parsing, but it comes at a higher computational cost, as there is more flexibility in how the tree can be arranged so that more operations are usually needed to explore the much larger search space. Non-projective transition-based parsing has been actively explored in the last decade (Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2008, 2009; G´omez-Rodr´ıguez and Nivre, 2010; G´omez-Rodr´ıguez et al., 2014). The success of neural networks and word embeddings for proThe LyS-FASTPARSE team presents BIST- COVINGTON , a neural implementation of the Covington (2001) algorithm for non-projective dependency parsing. The bidirectional LSTM approach by Kiperwasser and Goldberg (2016) is used to train a greedy parser with a dynamic oracle to mitigate error propagation. The model participated in the CoNLL 2017 UD Shared Task. In spite of not using any ensemble methods and using the baseline segmentation and PoS tagging,"
K17-3016,D14-1082,0,0.445207,"pair (i, j) is compared at a time. We will be referring to the indexes i and j as the focus words. It is straightforward to conclude that the theoretical complexity of the algorithm is O(|w|2 ). Covington’s algorithm can be easily implemented as a transition system (Nivre, 2008). The set of transitions used in BIST- COVINGTON and their preconditions is specified in Table 1. Each transition corresponds to a parsing configuration represented as a 4-tuple c = (λ1 , λ2 , β, A), such that: He gave a talk yesterday about parsing Figure 1: A non-projective dependency tree jective dependency parsing (Chen and Manning, 2014) also encouraged research on neural nonprojective models (Straka et al., 2016). However, to the best of our knowledge, no neural implementation is available of unrestricted nonprojective transition-based parsing with a dynamic oracle. Here, we present such an implementation for the Covington (2001) algorithm using bidirectional long short-term memory networks (LSTM) (Hochreiter and Schmidhuber, 1997), which is the main contribution of this paper. The system is evaluated at the CoNLL 2017 UD Shared Task: end-to-end multilingual parsing using Universal Dependencies (Zeman et al., 2017). The goal"
K17-3016,P15-2042,1,0.903191,"Missing"
K17-3016,P10-1151,1,0.934349,"Missing"
K17-3016,D14-1099,1,0.910389,"Missing"
K17-3016,P11-1068,1,0.908113,"Missing"
K17-3016,P09-1040,0,0.133391,"Missing"
K17-3016,D10-1004,0,0.0322871,"for BIST- COVINGTON as described in Nivre (2008). a → − ... → − b indicates there is a path in the dependency tree that allows to reach b from a (1996). They both rely on bidirectional LSTM’s (BILSTM’s). We kept the main architecture of the arc-hybrid BIST-parser and changed the parsing algorithm to that described in §2.2.1 and §2.2.2. We encourage the reader to consult Kiperwasser and Goldberg (2016) for a detailed explanation of their architecture, but we now try to give a quick overview of its use as the core part of BISTCOVINGTON .2 In contrast to traditional parsers (Nivre et al., 2006; Martins et al., 2010; Rasooli and Tetreault, 2015), BIST-parsers rely on embeddings as inputs instead of on discrete events (co-occurrences of words, tags, features, etc.). Embeddings are lowdimensional vectors that provide a continuous representation of a linguistic unit (word, PoS tag, etc.) based on its context (Mikolov et al., 2013). Let w=[w1 , ..., w|w |] be a list of word embeddings for a sentence, let u=[u1 , ..., u|w |] be the corresponding list of universal PoS tag embeddings, t=[t1 , ..., t|w |] the list of specific PoS tag embeddings, f =[f1 , ..., f|w |] the list of morphological features (“feats” co"
K17-3016,W15-1002,0,0.0385895,"Missing"
K17-3016,N15-1155,0,0.0350086,"Missing"
K17-3016,P05-1013,0,0.0343131,"projective if it contains two arcs i → − j and k → − l where min(i, j) < min(k, l) < max(i, j) < max(k, l), i.e., if there is any pair of arcs that cross when they are drawn over the sentence, as shown in Figure 1. Unrestricted non-projective parsing allows more accurate syntactic representations than projective parsing, but it comes at a higher computational cost, as there is more flexibility in how the tree can be arranged so that more operations are usually needed to explore the much larger search space. Non-projective transition-based parsing has been actively explored in the last decade (Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2008, 2009; G´omez-Rodr´ıguez and Nivre, 2010; G´omez-Rodr´ıguez et al., 2014). The success of neural networks and word embeddings for proThe LyS-FASTPARSE team presents BIST- COVINGTON , a neural implementation of the Covington (2001) algorithm for non-projective dependency parsing. The bidirectional LSTM approach by Kiperwasser and Goldberg (2016) is used to train a greedy parser with a dynamic oracle to mitigate error propagation. The model participated in the CoNLL 2017 UD Shared Task. In spite of not using any ensemble methods and using the baseline segmentation an"
K17-3016,D13-1170,0,0.00676734,"Missing"
K17-3016,L16-1680,0,0.0576044,"Missing"
K17-3016,P16-2069,1,0.895221,"Missing"
K17-3016,nivre-etal-2006-maltparser,0,\N,Missing
K17-3016,C96-1058,0,\N,Missing
K17-3016,J08-4003,0,\N,Missing
K17-3016,P15-1082,0,\N,Missing
K17-3016,Q16-1023,0,\N,Missing
L16-1655,Q16-1031,0,0.012867,"resented corpus for future research in this area. 4. NLP tools for code-switching texts Together with this paper we make available both partof-speech and dependency parsing models that are able to process English-Spanish code-switching texts (Vilares et al., 2015b), so the research community can use them to explore richer linguistic approaches. They can be downloaded from http://grupolys.org/ software/TAGGERS and http://grupolys.org/ software/PARSERS or by asking any of the authors. Figure 1 shows how these bilingual models work better than the corresponding monolingual models. More recently, Ammar et al. (2016) have also shown the utility of using harmonized treebanks for universal parsing. 5. Conclusions We present the first code-switching Twitter corpus for multilingual sentiment analysis, composed of tweets that merge English and Spanish terms. Some initial experiments have 4151 Figure 1: Example with the en, es and en-es dependency parsers. Dotted lines represent incorrectly-parsed dependencies been already run, providing baselines for future research for the SA community. The results also show that neither monolingual nor multilingual approaches based on language detection are optimal to deal w"
L16-1655,W12-3709,0,0.0263299,"ption of the public with respect to popular events (Thelwall et al., 2011) to real-time political analysis (Vilares et al., 2015d). Some of these trends are global (e.g. the Oscars, Superbowl or Rihanna) and so their trending topics are global too (e.g. ‘#oscars2016’, ‘#superbowl2016’, . . . ). However, the public perception of these trends often changes from one country to another and the task becomes even harder when tweets are written in different languages. This has motivated the need of multilingual sentiment analysis. Usually, researchers evaluate multilingual approaches by translating (Balahur and Turchi, 2012) or merging monolingual corpora in different languages (Vilares et al., 2015c). But there exist cases where these synthetic corpora are not adequate to evaluate more difficult and unexplored multilingual variants, such as code-switching texts (i.e. texts that contain terms in two or more different languages). Colloquial creole languages such as Spanglish (a mix of Spanish and American English) or Singlish (English-based creole from Singapore) or even official languages such as the Haitian creole (which merges Portuguese, Spanish, Ta´ıno, and West African languages), are some of the bestknown s"
L16-1655,P12-3005,0,0.0334363,"igrams of W Bigrams of L Bigrams of P Combined (W, P, T ) Combined (L , P, T ) Combined (W, P) Combined (L , P) 3. Application to Sentiment Analysis • Monolingual approaches: Two monolingual models, one for Spanish (es-model) and one for English (enmodel), were trained using the TASS 2014 (VillenaRom´an et al., 2015) and SemEval 2014 (Rosenthal et al., 2014) corpora, respectively. The aim was to provide a baseline with the performance that a purely monolingual model can achieve on code-switching texts. • Majority language detection approach (mld-model): An automatic language detection system (Lui and Baldwin, 2012) is used to determine which one language is dominant in the tweet (assuming that, intuitively, the language that has a bigger presence in the tweet would contain the sentiment of the sentence), to then run the corresponding monolingual model. • Purely multilingual approach (en-es-model): A supervised model is trained on the union of two monolingual corpora. The sets of features used in the experiments were different bags of words composed of: words (W), lemmas (L) and psychometric properties (P) coming from Pennebaker et al. (2001). Additionally, we include models using bigrams and also more a"
L16-1655,P13-2017,0,0.0622615,"Missing"
L16-1655,S13-2052,0,0.0770697,"Missing"
L16-1655,S16-1001,0,0.0200652,"ce is already done, providing a set of baselines for the research community. Keywords: Sentiment Analysis, Corpus Generation, Code-Switching 1. Introduction Sentiment analysis (SA) is the field of research that deals with the automatic comprehension of the subjective information shared by users, especially on the Web (Pang and Lee, 2008; Cambria et al., 2013). One of its main challenges is polarity classification, focused on classifying a text, sentence or phrase as positive, negative or neutral (or even considering different levels of granularity like strongly positive and strongly negative (Nakov et al., 2016)). The interest of organizations and companies in this task has increased in recent years, due to the rise of social media. In particular, Twitter has become one of the most useful social networks for trending analysis, thanks to its abilities to capture popular trends and the easy interaction among its members. SA techniques have been successfully applied on Twitter to monitor a wide variety of issues ranging from the perception of the public with respect to popular events (Thelwall et al., 2011) to real-time political analysis (Vilares et al., 2015d). Some of these trends are global (e.g. th"
L16-1655,S14-2009,0,0.026722,"res where for each tweet, ps refers to its positive strength, ns to its negative strength, tweetid to its unique identifier, text to its contents, polarity to its polarity class and 	 is used to represent a tab character. Words (W) Lemmas (L ) Psychometric (P) Bigrams of W Bigrams of L Bigrams of P Combined (W, P, T ) Combined (L , P, T ) Combined (W, P) Combined (L , P) 3. Application to Sentiment Analysis • Monolingual approaches: Two monolingual models, one for Spanish (es-model) and one for English (enmodel), were trained using the TASS 2014 (VillenaRom´an et al., 2015) and SemEval 2014 (Rosenthal et al., 2014) corpora, respectively. The aim was to provide a baseline with the performance that a purely monolingual model can achieve on code-switching texts. • Majority language detection approach (mld-model): An automatic language detection system (Lui and Baldwin, 2012) is used to determine which one language is dominant in the tweet (assuming that, intuitively, the language that has a bigger presence in the tweet would contain the sentiment of the sentence), to then run the corresponding monolingual model. • Purely multilingual approach (en-es-model): A supervised model is trained on the union of two"
L16-1655,S15-2078,0,0.0549681,"Missing"
L16-1655,W14-3907,0,0.243684,"corpus of tweets with code-switching (EN - ES - CS CORPUS). To the best of our knowledge, this is the first code-switching collection annotated with sentiment labels. The remainder of the paper is organized as follows. Section 2. describes the corpus. Section 3. shows preliminary research on the corpus, providing baselines for the natural language processing (NLP) community. Section 4. presents available NLP tools for multilingual and code-switching NLP . Finally, Section 5. draws our conclusions. 2. Corpus creation To create the corpus, we take as starting point the collection presented in (Solorio et al., 2014), a workshop on language detection on code-switching tweets, where the goal was to apply language identification at the word level. The organisers proposed four code-switching language detection challenges: Spanish-English, Nepali-English, Mandarin-English and Modern Standard Arabic-Arabic dialects. They made the training corpora available to the research community, together with a small tuning collection, but no test set was released. For building our resource, we just considered the SpanishEnglish training set (originally 11 400 tweets). As a first step, we removed all the non code-switching"
L16-1655,W15-2902,1,0.890388,"Missing"
L16-1655,P16-2069,1,\N,Missing
N09-1061,N07-1019,0,0.20138,"Missing"
N09-1061,E09-1034,1,0.84924,"Missing"
N09-1061,C92-2066,0,0.278857,"valent to LCFRS that has been introduced for syntax-based machine translation. However, the grammar produced by our algorithm has optimal (minimal) fan-out. This is an important improvement over the result in (Melamed et al., 2004), as this quantity enters into the parsing complexity of both multitext grammars and LCFRS as an exponential factor, and therefore must be kept as low as possible to ensure practically viable parsing. Rank reduction is also investigated in Nesson et al. (2008) for synchronous tree-adjoining grammars, a synchronous rewriting formalism based on tree-adjoining grammars Joshi and Schabes (1992). In this case the search space of possible reductions is strongly restricted by the tree structures specified by the formalism, resulting in simplified computation for the reduction algorithms. This feature is not present in the case of LCFRS. There is a close parallel between the technique used in the M INIMAL -B INARIZATION algorithm and deductive parsing techniques as proposed by Shieber et al. (1995), that are usually implemented by means of tabular methods. The idea of exploiting tabular parsing in production factorization was first expressed in Zhang et al. (2006). In fact, the 546 part"
N09-1061,P06-2066,1,0.79325,"th a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally introduced as a generalization of several so-called mildly context-sensitive grammar formalisms. In the context of machine translation, LCFRS is an interesting generalization of SCFG because it does not restrict the fan-out to 2, allowing productions with arbitrary fan-out (and arbitrary rank). Given an LCFRS, our algorithm computes a strongly equivalent grammar with rank 2 and minHuman Langua"
N09-1061,E09-1055,1,0.585799,"re particularly relevant to this paper. 5.1 The tradeoff between rank and fan-out The algorithm introduced in this paper can be used to transform an LCFRS into an equivalent form with rank 2. This will result into a more efficiently parsable LCFRS, since rank exponentially affects parsing complexity. However, we must take into account that parsing complexity is also influenced by fan-out. Our algorithm guarantees a minimal increase in fan-out. In practical cases it seems such an increase is quite small. For example, in the context of dependency parsing, both G´omezRodr´ıguez et al. (2009) and Kuhlmann and Satta (2009) show that all the structures in several wellknown non-projective dependency treebanks are binarizable without any increase in their fan-out. More in general, it has been shown by Seki et al. (1991) that parsing of LCFRS can be carried out in time O(n|pM |), where n is the length of the input string and pM is the production in the grammar with largest size.3 Thus, there may be cases in which one has to find an optimal tradeoff between rank and fanout, in order to minimize the size of pM . This requires some kind of Viterbi search over the space of all possible binarizations, constructed as des"
N09-1061,P04-1084,1,0.120203,"ions of the original grammar can be reconstructed using some simple homomorphism (c.f. Nijholt, 1980). Our contribution is significant because the existing algorithms for decomposing SCFG, based on Uno and Yagiura (2000), cannot be applied to LCFRS, as they rely on the crucial property that components of biphrases are strictly separated in the generated string: Given a pair of synchronized nonterminal symbols, the material derived from the source nonterminal must precede the material derived from the target nonterminal, or vice versa. The problem that we solve has been previously addressed by Melamed et al. (2004), but in contrast to our result, their algorithm does not guarantee an optimal (minimal) increase in the fanout of the resulting grammar. However, this is essential for the practical applicability of the transformed grammar, as the parsing complexity of LCFRS is exponential in both the rank and the fan-out. Structure of the paper The remainder of the paper is structured as follows. Section 2 introduces the terminology and notation that we use for LCFRS. In Section 3, we present the technical background of our algorithm; the algorithm itself is discussed in Section 4. Section 5 concludes the pa"
N09-1061,P08-1069,1,0.851562,"Missing"
N09-1061,P87-1015,1,0.604286,"Missing"
N09-1061,P06-1123,0,0.0348496,"racted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally"
N09-1061,C08-1136,0,0.0189435,"ences therein. One practical problem with this approach, apart from the sheer number of the rules that result from the extraction procedure, is that the parsing complexity of all synchronous formalisms that we are aware of is exponential in the rank of a rule, defined as the number of nonterminals on the righthand side. Therefore, it is important that the rules of the extracted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studi"
N09-1061,N06-1033,0,0.095259,"ear Context-Free Rewriting Systems Carlos G´omez-Rodr´ıguez1 , Marco Kuhlmann2 , Giorgio Satta3 and David Weir4 1 Departamento de Computaci´on, Universidade da Coru˜na, Spain (cgomezr@udc.es) Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se) 2 3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it) 4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk) Abstract terms of parsing complexity, but smaller rules can also improve a translation model’s ability to generalize to new data (Zhang et al., 2006). Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation. The parsing complexity of an LCFRS is exponential in both the rank of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, called fan-out. In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which all productions have rank at most 2, and has minimal fan-out. Our results generalize previous work on Synchronous Context-Free Grammar, and are pa"
N09-1061,J07-2003,0,\N,Missing
N10-1035,C02-1028,0,0.0403826,"e form x1,1 · · · xm,1 ; and (ii) for each i ∈ [m], the sequence obtained from f by reading variables of the form xi,j from left to right has the form xi,1 · · · xi,ki . An LCFRS is called canonical, if each of its composition operations is canonical. We omit the proof that every LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1"
N10-1035,N10-1118,0,0.32636,". . . , (lm,ϕ(Am ) , rm,ϕ(Am ) )] [A0 , (l0,1 , r0,1 ), . . . , (l0,ϕ(A0 ) , r0,ϕ(A0 ) )] (a) The general rule for a parsing schema for LCFRS [B, (l1 , r1 ), . . . , (lm , rm )] [C, (l10 , r10 ), . . . (ln0 , rn0 )] [A, (l1 , r1 ), . . . , (lm , r10 ), . . . (ln0 , rn0 )] rm = l10 (b) Deduction step for concatenation [B, (l1 , r1 ), . . . , (lm , rm )] [C, (l10 , r10 ), . . . (ln0 , rn0 )] [A, (l1 , r1 ), . . . , (li , r10 ), . . . (ln0 , ri+1 ), . . . , (lm , rm )] ri = l10 , rn0 = li+1 (c) Deduction step for wrapping Figure 2: Deduction steps for parsing LCFRS. Thus, the parsing complexity (Gildea, 2010) of a production p = A0 → fP (A1 , . . . , Am ) is determined by ϕ(A0 ) l-indexes and i∈[m] ϕ(Ai ) r-indexes, for a total complexity of P O(|w|ϕ(A0 )+ i∈[m] ϕ(Ai ) ) where |w |is the length of the input string. The parsing complexity of an LCFRS will correspond to the maximum parsing complexity among its productions. Note that this general complexity matches the result given by Seki et al. (1991). In an LCFRS of rank ρ and fan-out ϕ, the maximum possible parsing complexity is O(|w|ϕ(ρ+1) ), obtained by applying the above expression to a production of rank ρ and where each nonterminal has fanou"
N10-1035,N09-1061,1,0.733957,"le parsing complexity is O(|w|ϕ(ρ+1) ), obtained by applying the above expression to a production of rank ρ and where each nonterminal has fanout ϕ. The asymptotic time complexity of LCFRS parsing is therefore exponential both in its rank and its fan-out. This means that it is interesting to transform LCFRS into equivalent forms that reduce their rank while preserving the fan-out. For sets of LCFRS that can be transformed into a binary form (i.e., such that all its rules have rank at most 2), the ρ factor in the complexity is reduced to a constant, and complexity is improved to O(|w|3ϕ ) (see Gómez-Rodríguez et al. (2009) for further discussion). Unfortunately, it is known by previous results (Rambow and Satta, 1999) that it is not always possible to convert an LCFRS into such a binary form without increasing the fan-out. However, we will show that it is always possible to build such a binarization for well-nested LCFRS. Combining this result with the inference rule and complexity analysis given above, we would obtain a parser for well-nested LCFRS running in 280 O(|w|3ϕ ) time. But the construction of our binary normal form additionally restricts binary composition operations in the binarized LCFRS to be of t"
N10-1035,P07-1077,0,0.032643,"arsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrangement of components might be needed. More specifically, analyses of both dependency and constituency treebanks (Kuhlmann and Nivre, 2006; Havelka, 2007; Maier and Lichte, 2009) have shown that rearrangements of argument tuples almost always satisfy the so-called well-nestedness condition, a generalization 276 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276–284, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of the standard condition on balanced brackets. This condition states that any two components x1 , x2 of some tuple will never be composed with any two components y1 , y2 of some other tuple in such a way that a ‘crossing’ configuration is re"
N10-1035,W09-3808,0,0.0143045,"xm,1 ; and (ii) for each i ∈ [m], the sequence obtained from f by reading variables of the form xi,j from left to right has the form xi,1 · · · xi,ki . An LCFRS is called canonical, if each of its composition operations is canonical. We omit the proof that every LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1 $x2,2 x1,2 is wellnested, wh"
N10-1035,P07-1021,1,0.883517,"LCFRS can be transformed into a weakly equivalent canonical LCFRS. However, we point out that both the normal form and the parsing algorithm that we present in this paper can be applied also to general LCFRS. This is in contrast to some left-to-right parsers in the literature on LCFRS and equivalent formalisms (de la Clergerie, 2002; Kallmeyer and Maier, 2009), which actually depend on productions in canonical form. 2.5 Well-nested LCFRS We now characterize the class of well-nested LCFRS that are the focus of this paper. Well-nestedness was first studied in the context of dependency grammars (Kuhlmann and Möhl, 2007). Kanazawa (2009) defines well-nested multiple context-free grammars, which are weakly equivalent to well-nested LCFRS. A composition operation is called well-nested, if it does not contain a substring of the form xi,i1 · · · xj,j1 · · · xi,i2 · · · xj,j2 , where i 6= j . For example, the operation x1,1 x2,1 $x2,2 x1,2 is wellnested, while x1,1 x2,1 $ x1,2 x2,2 is not. An LCFRS is called well-nested, if it contains only well-nested composition operations. The class of languages generated by well-nested LCFRS is properly included in the class of languages generated by general LCFRS; see Kanazaw"
N10-1035,P06-2066,1,0.896161,"ly convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrangement of components might be needed. More specifically, analyses of both dependency and constituency treebanks (Kuhlmann and Nivre, 2006; Havelka, 2007; Maier and Lichte, 2009) have shown that rearrangements of argument tuples almost always satisfy the so-called well-nestedness condition, a generalization 276 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 276–284, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics of the standard condition on balanced brackets. This condition states that any two components x1 , x2 of some tuple will never be composed with any two components y1 , y2 of some other tuple in such a way that a ‘crossing’ conf"
N10-1035,E09-1055,1,0.547229,"stigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), and these systems lack Chomsky-like normal forms for fixed fan-out (Rambow and Satta, 1999) that are especially convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal fo"
N10-1035,P92-1012,1,0.837614,"r et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), and these systems lack Chomsky-like normal forms for fixed fan-out (Rambow and Satta, 1999) that are especially convenient in tabular parsing. This is in contrast with other mildly context-sensitive formalisms, and TAG in particular: TAGs can be parsed in polynomial time both with respect to grammar size and string size, and they can be cast in normal forms having binary derivation trees only. It has recently been argued that LCFRS might be too powerful for modeling languages with discontinuous constituents or with relatively free word order, and that additional restrictions on the rearrange"
N10-1035,P87-1015,0,0.833476,"l linguistics has been the modeling of natural language syntax by means of formal grammars. Following results by Huybregts (1984) and Shieber (1985), special attention has been given to formalisms that enlarge the generative power of context-free grammars, but still remain below the full generative power of context-sensitive grammars. On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi, 1985), including, among several others, the tree adjoining grammars (TAGs) of Joshi et al. (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al. (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. This feature has been used to model phrase structure treebanks with discontinuous constituents (Maier and Søgaard, 2008), as well as to map non-projective dependency trees into discontinuous phrase structures (Kuhlmann and Satta, 2009). The freedom in the rearrangement of components has specific consequences in terms of the computational and descriptional complexity of LCFRS. Even for grammars with bounded fan-out, the universal recognition problem is NP-hard (Satta, 1992), a"
N18-2023,D17-1130,0,0.131604,"nd reentrancy are used to model semantic relations between concepts (as shown in Figure 1) and to identify co-references. By removing the constraints from the Covington transition system, we achieve a natural way to deal with them.1 Also, AMR parsing requires words to be transformed into concepts. Dependency parsing operates on a constant-length sequence. But in AMR, words can be removed, generate a single concept, or generate several concepts. In this paper, additional lookup tables and transitions are defined to create concepts when needed, following the current trend (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Gildea et al., 2018). 3.1 l • LEFT- ARCl : Creates an edge b → − i. i is moved to λ2 . l • RIGHT- ARCl : Creates an edge i → − b. i is moved to λ2 . • SHIFT: Pops b from β. λ1 , λ2 and b are appended. • NO ARC: It is applied when the algorithm determines that there is no semantic relationship between i and b, but there is a relationship between some other node in λ1 and b. • CONFIRM: Pops b from β and puts the concept b in its place. This transition is called to handle words that only need to generate one (more) concept. Formalization Let G=(V, E) be an edge-labeled directed graph where: V ="
N18-2023,W13-2322,0,0.0811377,"ges. 1 time ARG1 ARG4 ARG1 polarity quant prince arrived-01 planet surprise-01 - see-01 any person name wiki ""Earth"" name op1 ""Earth"" Figure 1: AMR graph for ‘When the prince arrived on the Earth, he was surprised not to see any people’. Words can refer to concepts by themselves (green), be mapped to PropBank framesets (red) or be broken down into multiple-term/non-literal concepts (blue). Prince plays different semantic roles. Introduction Abstract Meaning Representation (AMR) is a semantic representation language to map the meaning of English sentences into directed, cycled, labeled graphs (Banarescu et al., 2013). Graph vertices are concepts inferred from words. The concepts can be represented by the words themselves (e.g. dog), PropBank framesets (Palmer et al., 2005) (e.g. eat-01), or keywords (like named entities or quantities). The edges denote relations between pairs of concepts (e.g. eat-01 :ARG0 dog). AMR parsing integrates tasks that have usually been addressed separately in natural language processing (NLP), such as named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Palmer et al., 2010) or co-reference resolution (Ng and Cardie, 2002; Lee et al., 2017). Figure 1 shows"
N18-2023,P14-5010,0,0.00482649,"λ2 , β, E) (λ1 , λ2 , b|β, E) (λ1 , λ2 , b|β, E) (λ1 , λ2 , b|β, E) l l (λ1 , i|λ2 , b|β, E ∪ {i → − b}) l l 2 (λ1 , i|λ2 , b|β, E ∪ {b → − i} ∪ {i − → b}) (λ1 · λ2 |b, [], β, E) (λ1 , i|λ2 , β, E) (λ1 , λ2 , b|β, E) (λ1 , λ2 , b|b|β, E) (λ1 , λ2 , β, E) Table 1: Transitions for AMR - COVINGTON Preprocessing Sentences are tokenized and aligned with the concepts using JAMR (Flanigan et al., 2014). For lemmatization, tagging and dependency parsing we used UDpipe (Straka et al., 2016) and its English pre-trained model (Zeman et al., 2017). Named Entity Recognition is handled by Stanford CoreNLP (Manning et al., 2014). to multiple concepts. To guarantee termination, BREAKDOWN is parametrized with a constant α, banning generation of more than α consecutive concepts by using this operation. Otherwise, concepts could be generated indefinitely without emptying β. • REDUCE: Pops b from β. It is used to remove words that do not add any meaning to the sentence and are not part of the AMR graph. Architecture We use feed-forward neural networks to train the tree classifiers. The transition classifier uses 2 hidden layers (400 and 200 input neurons) and the relation and concept classifiers use 1 hidden layer (200 ne"
N18-2023,P13-2131,0,0.0491138,"is a verb, we generate the concept lemma-01, and otherwise lemma. Edge label identification The classifier is invoked every time an edge is created. We use the list of valid ARGs allowed in propbank framesets by Damonte et al. (2017). Also, if p and o are a propbank and a non-propbank concept, we restore l-of l inverted edges of the form o −−−→ p as o − → p. 4 Methods and Experiments Corpus We use the LDC2015E86 corpus and its official splits: 16 833 graphs for training, 1 368 for development and 1 371 for testing. The final model is only trained on the training split. Metrics We use Smatch (Cai and Knight, 2013) and the metrics from Damonte et al. (2017).3 Sources The code and the pretrained model used in this paper can be found at https:// github.com/aghie/tb-amr. 4.1 Results and discussion Table 3 shows accuracy of Tc on the development set. CONFIRM and REDUCE are the easiest transitions, as local information such as POS-tags and words are discriminative to distinguish between content and function words. BREAKDOWN is the hardest action.4 In early stages of this work, we observed that this transition could learn to correctly generate multiple-term concepts for namedentities that are not sparse (e.g."
N18-2023,P02-1014,0,0.207754,"ected, cycled, labeled graphs (Banarescu et al., 2013). Graph vertices are concepts inferred from words. The concepts can be represented by the words themselves (e.g. dog), PropBank framesets (Palmer et al., 2005) (e.g. eat-01), or keywords (like named entities or quantities). The edges denote relations between pairs of concepts (e.g. eat-01 :ARG0 dog). AMR parsing integrates tasks that have usually been addressed separately in natural language processing (NLP), such as named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Palmer et al., 2010) or co-reference resolution (Ng and Cardie, 2002; Lee et al., 2017). Figure 1 shows an example of an AMR graph. Several transition-based dependency parsing algorithms have been extended to generate AMR. Wang et al. (2015) describe a two-stage model, where they first obtain the dependency parse of a sentence and then transform it into a graph. Damonte et al. (2017) propose a variant of the ARC - EAGER algorithm to identify labeled edges between concepts. These concepts are identified using a lookup table and a set of rules. A restricted subset of reentrant edges are supported by an additional classifier. A similar configuration is used in (G"
N18-2023,E17-1051,0,0.360373,"rs of concepts (e.g. eat-01 :ARG0 dog). AMR parsing integrates tasks that have usually been addressed separately in natural language processing (NLP), such as named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Palmer et al., 2010) or co-reference resolution (Ng and Cardie, 2002; Lee et al., 2017). Figure 1 shows an example of an AMR graph. Several transition-based dependency parsing algorithms have been extended to generate AMR. Wang et al. (2015) describe a two-stage model, where they first obtain the dependency parse of a sentence and then transform it into a graph. Damonte et al. (2017) propose a variant of the ARC - EAGER algorithm to identify labeled edges between concepts. These concepts are identified using a lookup table and a set of rules. A restricted subset of reentrant edges are supported by an additional classifier. A similar configuration is used in (Gildea et al., 2018; Peng et al., 2018), but relying on a cache data structure to handle reentrancy, cycles and restricted non-projectivity. A feed-forward network and additional hooks are used to build the concepts. Ballesteros and AlOnaizan (2017) use a modified ARC - STANDARD algorithm, where the oracle is trained"
N18-2023,J08-4003,0,0.110011,"t concept j ∈ V as i → − j, where l is the semantic label connecting them. The parser will process sentences from left to right. Each decision leads to a new parsing configuration, which can be abstracted as a 4-tuple (λ1 , λ2 , β, E) where: Notation We use typewriter font for concepts and their indexes (e.g. dog or 1), regular font for raw words (e.g. dog or 1), and a bold style font for vectors and matrices (e.g. v, W). Covington (2001) describes a fundamental algorithm for unrestricted non-projective dependency parsing. The algorithm can be implemented as a left-to-right transition system (Nivre, 2008). The key idea is intuitive. Given a word to be processed at a particular state, the word is compared against the words that have previously been processed, deciding to establish or not a syntactic dependency arc from/to each of them. The process continues until all previous words are checked or until the algorithm decides no more connections with previous words need to be built, then the next word is processed. The runtime is O(n2 ) in the worst scenario. To guarantee the single-head and acyclicity conditions that are required in dependency parsing, explicit tests are added to the algorithm t"
N18-2023,P09-1040,0,0.0728628,"etween concepts. These concepts are identified using a lookup table and a set of rules. A restricted subset of reentrant edges are supported by an additional classifier. A similar configuration is used in (Gildea et al., 2018; Peng et al., 2018), but relying on a cache data structure to handle reentrancy, cycles and restricted non-projectivity. A feed-forward network and additional hooks are used to build the concepts. Ballesteros and AlOnaizan (2017) use a modified ARC - STANDARD algorithm, where the oracle is trained using stackLSTM s (Dyer et al., 2015). Reentrancy is handled through SWAP (Nivre, 2009) and they define additional transitions intended to detect concepts, entities and polarity nodes. This paper explores unrestricted non-projective AMR parsing and introduces AMR - COVINGTON , inspired by Covington (2001). It handles arbitrary non-projectivity, cycles and reentrancy in a natural way, as there is no need for specific transitions, but just the removal of restrictions from the original algorithm. The algorithm has full coverage and keeps transitions simple, which is a matter of concern in recent studies (Peng et al., 2018). 142 Proceedings of NAACL-HLT 2018, pages 142–149 c New Orl"
N18-2023,P15-1033,0,0.0670184,"of the ARC - EAGER algorithm to identify labeled edges between concepts. These concepts are identified using a lookup table and a set of rules. A restricted subset of reentrant edges are supported by an additional classifier. A similar configuration is used in (Gildea et al., 2018; Peng et al., 2018), but relying on a cache data structure to handle reentrancy, cycles and restricted non-projectivity. A feed-forward network and additional hooks are used to build the concepts. Ballesteros and AlOnaizan (2017) use a modified ARC - STANDARD algorithm, where the oracle is trained using stackLSTM s (Dyer et al., 2015). Reentrancy is handled through SWAP (Nivre, 2009) and they define additional transitions intended to detect concepts, entities and polarity nodes. This paper explores unrestricted non-projective AMR parsing and introduces AMR - COVINGTON , inspired by Covington (2001). It handles arbitrary non-projectivity, cycles and reentrancy in a natural way, as there is no need for specific transitions, but just the removal of restrictions from the original algorithm. The algorithm has full coverage and keeps transitions simple, which is a matter of concern in recent studies (Peng et al., 2018). 142 Proc"
N18-2023,J05-1004,0,0.0566967,"n the prince arrived on the Earth, he was surprised not to see any people’. Words can refer to concepts by themselves (green), be mapped to PropBank framesets (red) or be broken down into multiple-term/non-literal concepts (blue). Prince plays different semantic roles. Introduction Abstract Meaning Representation (AMR) is a semantic representation language to map the meaning of English sentences into directed, cycled, labeled graphs (Banarescu et al., 2013). Graph vertices are concepts inferred from words. The concepts can be represented by the words themselves (e.g. dog), PropBank framesets (Palmer et al., 2005) (e.g. eat-01), or keywords (like named entities or quantities). The edges denote relations between pairs of concepts (e.g. eat-01 :ARG0 dog). AMR parsing integrates tasks that have usually been addressed separately in natural language processing (NLP), such as named entity recognition (Nadeau and Sekine, 2007), semantic role labeling (Palmer et al., 2010) or co-reference resolution (Ng and Cardie, 2002; Lee et al., 2017). Figure 1 shows an example of an AMR graph. Several transition-based dependency parsing algorithms have been extended to generate AMR. Wang et al. (2015) describe a two-stage"
N18-2023,S16-1186,0,0.14665,"Missing"
N18-2023,P17-2018,0,0.0938501,"Missing"
N18-2023,P14-1134,0,0.309579,"Step t + 1 LEFT- ARC l (λ1 |i, λ2 , b|β, E) (λ1 , i|λ2 , b|β, E ∪ {b → − i}) RIGHT- ARC l MULTIPLE - ARC l1 ,l2 SHIFT NO - ARC CONFIRM BREAKDOWN α REDUCE (λ1 |i, λ2 , b|β, E) (λ1 |i, λ2 , b|β, E) (λ1 , λ2 , b|β, E) (λ1 |i, λ2 , β, E) (λ1 , λ2 , b|β, E) (λ1 , λ2 , b|β, E) (λ1 , λ2 , b|β, E) l l (λ1 , i|λ2 , b|β, E ∪ {i → − b}) l l 2 (λ1 , i|λ2 , b|β, E ∪ {b → − i} ∪ {i − → b}) (λ1 · λ2 |b, [], β, E) (λ1 , i|λ2 , β, E) (λ1 , λ2 , b|β, E) (λ1 , λ2 , b|b|β, E) (λ1 , λ2 , β, E) Table 1: Transitions for AMR - COVINGTON Preprocessing Sentences are tokenized and aligned with the concepts using JAMR (Flanigan et al., 2014). For lemmatization, tagging and dependency parsing we used UDpipe (Straka et al., 2016) and its English pre-trained model (Zeman et al., 2017). Named Entity Recognition is handled by Stanford CoreNLP (Manning et al., 2014). to multiple concepts. To guarantee termination, BREAKDOWN is parametrized with a constant α, banning generation of more than α consecutive concepts by using this operation. Otherwise, concepts could be generated indefinitely without emptying β. • REDUCE: Pops b from β. It is used to remove words that do not add any meaning to the sentence and are not part of the AMR graph."
N18-2023,J18-1004,0,0.732311,"2; Lee et al., 2017). Figure 1 shows an example of an AMR graph. Several transition-based dependency parsing algorithms have been extended to generate AMR. Wang et al. (2015) describe a two-stage model, where they first obtain the dependency parse of a sentence and then transform it into a graph. Damonte et al. (2017) propose a variant of the ARC - EAGER algorithm to identify labeled edges between concepts. These concepts are identified using a lookup table and a set of rules. A restricted subset of reentrant edges are supported by an additional classifier. A similar configuration is used in (Gildea et al., 2018; Peng et al., 2018), but relying on a cache data structure to handle reentrancy, cycles and restricted non-projectivity. A feed-forward network and additional hooks are used to build the concepts. Ballesteros and AlOnaizan (2017) use a modified ARC - STANDARD algorithm, where the oracle is trained using stackLSTM s (Dyer et al., 2015). Reentrancy is handled through SWAP (Nivre, 2009) and they define additional transitions intended to detect concepts, entities and polarity nodes. This paper explores unrestricted non-projective AMR parsing and introduces AMR - COVINGTON , inspired by Covington"
N18-2062,W06-2922,0,0.0384508,"ovel approach outperforms the static training strategy in the vast majority of languages tested and scored better on most datasets than the arc-hybrid parser enhanced with the Swap transition, which can handle unrestricted nonprojectivity. 1 Introduction Linear-time greedy transition-based parsers such as arc-eager, arc-standard and arc-hybrid (Nivre, 2003, 2004; Kuhlmann et al., 2011) are widely used for dependency parsing due to their efficiency and performance, but they cannot deal with nonprojective syntax. To address this, various extensions have been proposed, involving new transitions (Attardi, 2006; Nivre, 2009; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2012), data structures (G´omez-Rodr´ıguez and Nivre, 2010; Pitler and McDonald, 2015) or pre and postprocessing (Nivre and Nilsson, 2005). Among these extensions, the 2-Planar parser (G´omez-Rodr´ıguez and Nivre, 2010) has attractive properties, as it (1) keeps the original worst-case linear time, (2) has close to full coverage of non-projective phenomena, and (3) needs no pre- or post-processing. Dynamic oracles (Goldberg and Nivre, 2012) are known to improve the accuracy of greedy parsers by enabling more robust training, by exploring"
N18-2062,W06-2920,0,0.132473,"he Switch transition if there is any zero-cost transition available in the active plane and changing planes will delay its application. Thus, arcs assigned to the currently active plane will be built before switching if possible, enforcing a global arc creation order. This is similar to the prioritization of monotonic paths in (Honnibal et al., 2013, §6), as they also penalize unneeded actions that will need to be undone later. 4 Experiments 4.1 Data and Evaluation We conduct our experiments on the commonlyused non-projective benchmark compounded of nine datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007).7 We also use the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)8 of the WSJ Penn Treebank (PTB-SD) (Marcus et al., 1993) with standard splits. Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed including punctuation for all datasets except for the PTB where, following common practice, the punctuation is excluded. We train our system for 15 iterations and choose the best model according to development set accuracy. Statistical significance is calculated us"
N18-2062,P10-1151,1,0.939751,"Missing"
N18-2062,W17-6314,0,0.299987,"Missing"
N18-2062,J13-4002,1,0.942135,"Missing"
N18-2062,W08-1301,0,0.0123007,"gned to the currently active plane will be built before switching if possible, enforcing a global arc creation order. This is similar to the prioritization of monotonic paths in (Honnibal et al., 2013, §6), as they also penalize unneeded actions that will need to be undone later. 4 Experiments 4.1 Data and Evaluation We conduct our experiments on the commonlyused non-projective benchmark compounded of nine datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007).7 We also use the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)8 of the WSJ Penn Treebank (PTB-SD) (Marcus et al., 1993) with standard splits. Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed including punctuation for all datasets except for the PTB where, following common practice, the punctuation is excluded. We train our system for 15 iterations and choose the best model according to development set accuracy. Statistical significance is calculated using a paired test with 10,000 bootstrap samples. 4.2 4.3 Results Table 1 shows that the 2-Planar parser trained with a dynamic oracle outper"
N18-2062,D12-1029,1,0.896349,"Missing"
N18-2062,C12-1059,0,0.21028,"with nonprojective syntax. To address this, various extensions have been proposed, involving new transitions (Attardi, 2006; Nivre, 2009; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2012), data structures (G´omez-Rodr´ıguez and Nivre, 2010; Pitler and McDonald, 2015) or pre and postprocessing (Nivre and Nilsson, 2005). Among these extensions, the 2-Planar parser (G´omez-Rodr´ıguez and Nivre, 2010) has attractive properties, as it (1) keeps the original worst-case linear time, (2) has close to full coverage of non-projective phenomena, and (3) needs no pre- or post-processing. Dynamic oracles (Goldberg and Nivre, 2012) are known to improve the accuracy of greedy parsers by enabling more robust training, by exploring configurations beyond the gold path. While dynamic oracles have been defined for many transition-based algorithms (Goldberg and Nivre, 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andezGonz´alez, 2015; de Lhoneux et al., 2017), none is available so far for the 2-Planar system. The 2 The 2-Planar parser We briefly sketch the 2-Planar transition system, which was defined by G´omez-Rodr´ıguez and Nivre (2010, 2013) under the transition-based parsing framew"
N18-2062,P11-1068,1,0.944586,"Missing"
N18-2062,Q13-1033,0,0.45674,"Pitler and McDonald, 2015) or pre and postprocessing (Nivre and Nilsson, 2005). Among these extensions, the 2-Planar parser (G´omez-Rodr´ıguez and Nivre, 2010) has attractive properties, as it (1) keeps the original worst-case linear time, (2) has close to full coverage of non-projective phenomena, and (3) needs no pre- or post-processing. Dynamic oracles (Goldberg and Nivre, 2012) are known to improve the accuracy of greedy parsers by enabling more robust training, by exploring configurations beyond the gold path. While dynamic oracles have been defined for many transition-based algorithms (Goldberg and Nivre, 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andezGonz´alez, 2015; de Lhoneux et al., 2017), none is available so far for the 2-Planar system. The 2 The 2-Planar parser We briefly sketch the 2-Planar transition system, which was defined by G´omez-Rodr´ıguez and Nivre (2010, 2013) under the transition-based parsing framework (Nivre, 2008) and is based on the arc-eager algorithm (Nivre, 2003), keeping its linear time complexity. It works by building, in a single pass, two non-crossing graphs (called planes) whose union provides a dependency parse in the set"
N18-2062,Q14-1010,0,0.605421,") or pre and postprocessing (Nivre and Nilsson, 2005). Among these extensions, the 2-Planar parser (G´omez-Rodr´ıguez and Nivre, 2010) has attractive properties, as it (1) keeps the original worst-case linear time, (2) has close to full coverage of non-projective phenomena, and (3) needs no pre- or post-processing. Dynamic oracles (Goldberg and Nivre, 2012) are known to improve the accuracy of greedy parsers by enabling more robust training, by exploring configurations beyond the gold path. While dynamic oracles have been defined for many transition-based algorithms (Goldberg and Nivre, 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andezGonz´alez, 2015; de Lhoneux et al., 2017), none is available so far for the 2-Planar system. The 2 The 2-Planar parser We briefly sketch the 2-Planar transition system, which was defined by G´omez-Rodr´ıguez and Nivre (2010, 2013) under the transition-based parsing framework (Nivre, 2008) and is based on the arc-eager algorithm (Nivre, 2003), keeping its linear time complexity. It works by building, in a single pass, two non-crossing graphs (called planes) whose union provides a dependency parse in the set of 2-planar (or pagenu"
N18-2062,J93-2004,0,0.0609554,"arc creation order. This is similar to the prioritization of monotonic paths in (Honnibal et al., 2013, §6), as they also penalize unneeded actions that will need to be undone later. 4 Experiments 4.1 Data and Evaluation We conduct our experiments on the commonlyused non-projective benchmark compounded of nine datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007).7 We also use the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)8 of the WSJ Penn Treebank (PTB-SD) (Marcus et al., 1993) with standard splits. Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed including punctuation for all datasets except for the PTB where, following common practice, the punctuation is excluded. We train our system for 15 iterations and choose the best model according to development set accuracy. Statistical significance is calculated using a paired test with 10,000 bootstrap samples. 4.2 4.3 Results Table 1 shows that the 2-Planar parser trained with a dynamic oracle outperforms the static training strategy in terms of UAS in 15 out of 20 languages, with 8 of these improveme"
N18-2062,W03-3017,0,0.227462,"Missing"
N18-2062,P09-1040,0,0.0585047,"Missing"
N18-2062,P05-1013,0,0.156951,"nsition, which can handle unrestricted nonprojectivity. 1 Introduction Linear-time greedy transition-based parsers such as arc-eager, arc-standard and arc-hybrid (Nivre, 2003, 2004; Kuhlmann et al., 2011) are widely used for dependency parsing due to their efficiency and performance, but they cannot deal with nonprojective syntax. To address this, various extensions have been proposed, involving new transitions (Attardi, 2006; Nivre, 2009; Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2012), data structures (G´omez-Rodr´ıguez and Nivre, 2010; Pitler and McDonald, 2015) or pre and postprocessing (Nivre and Nilsson, 2005). Among these extensions, the 2-Planar parser (G´omez-Rodr´ıguez and Nivre, 2010) has attractive properties, as it (1) keeps the original worst-case linear time, (2) has close to full coverage of non-projective phenomena, and (3) needs no pre- or post-processing. Dynamic oracles (Goldberg and Nivre, 2012) are known to improve the accuracy of greedy parsers by enabling more robust training, by exploring configurations beyond the gold path. While dynamic oracles have been defined for many transition-based algorithms (Goldberg and Nivre, 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez et al., 2014"
N18-2062,D14-1162,0,0.0810143,"aper is taken from Kiperwasser and Goldberg (2016). We use the same BiLSTM-based featurization method that concatenates the representations of the top 3 words on the active stack and the leftmost word in the buffer for the arc-hybrid and 2-Planar algorithms, and we add the top 2 words on the inactive stack for the latter. Following Kiperwasser and Goldberg (2016), we also include the BiLSTM vectors of the rightmost and leftmost modifiers of words from the stacks, as well as the leftmost modifier of the first word in the buffer. We initialize word embeddings with 100-dimensional GloVe vectors (Pennington et al., 2014) for English and use 300-dimensional Facebook vectors (Bojanowski et al., 2016) for other languages. The other parameters of the neural network keep the same values as in (Kiperwasser and Goldberg, 2016). Regularization While the above dynamic oracle is theoretically correct, we noticed experimentally that the Switch transition tends to switch stacks very frequently during training, due to exploration. This leads the parser to learn unnecessarily long and complex transition sequences that change planes more than needed, harming accuracy. To avoid this, we add a regularization term to `(c) repr"
N18-2062,N15-1068,0,0.0305089,"Missing"
N18-2062,W04-0308,0,\N,Missing
N18-2062,P10-1000,0,\N,Missing
N18-2062,P10-5000,0,\N,Missing
N18-2067,P99-1059,0,0.141824,"stack item. As an illustration: res2 ,s0 : . 2 rh1 , i, h2 , h3 , ks rh3 , k, h4 , h5 , js rh1 , i, h2 , h5 , js rh1 , i, h2 , h4 , js . The goal of deduction is to produce the Icomputation rϵ, 0, ϵ, 0, ϵs, using the shift and reduce deduction rules starting from the axiom rϵ, 0, ϵ, 0, 1s, corresponding to the first and mandatory shift transition moving the root node from buffer to stack. ϵ stands for an empty stack or buffer. As analyzed by Cohen et al. (2011), direct tabularization for this deduction system takes Opn5 q space and Opn8 q time. With adaptation of the “hook trick” described in Eisner and Satta (1999), we can reduce the running time to Opn7 q. Each reduce rule combines two I-computations into a larger I-computation, e.g. (see Fig. 3): res0 ,s1 : rh1 , i, h2 , h3 , ks rh3 , k, h4 , h5 , js , 3.2 Our New Variants While Opn7 q or Opn10 q is not practical, the result is still impressive, since the search space is exponential. Cohen et al. (2011) were inspired by Huang and Sagae’s (2010) and Kuhlmann et al.’s (2011) dynamic-programming approach for projective systems. 3 See Cohen et al. (2011) for full description. 4 Condition (2) is used for proving completeness of the deduction system (Cohen"
N18-2067,J16-4008,1,0.576781,"plied to generalized Attardi (2006) systems; see 2 Transition-based Parsing We first introduce necessary definitions and notation. 2.1 A General Class of Transition Systems A transition system is given by a 4-tuple pC, T, cs , Cτ q, where C is a set of configurations, T is a set of transition functions between configurations, cs is an initialization function mapping an input sentence to an initial configuration, and Cτ Ă C defines a set of terminal configurations. 1 Faster exact inference algorithms have been defined for some sets of mildly non-projective trees (e.g. Pitler et al. (2013); see Gómez-Rodríguez (2016) for more), but lack an underlying transition system. Having one has the practical advantage of allowing generative models, as in Cohen et al. (2011), and transition-based scoring functions, which have yielded good projective-parsing results (Shi et al., 2017); plus the theoretical advantage of providing a single framework supporting greedy, beam-search, and exact inference. 420 Proceedings of NAACL-HLT 2018, pages 420–425 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics System Reduce Transitions Non-proj. Coverage Time Complexity Max. Degree Attardi ("
N18-2067,P10-1110,0,0.370016,"Missing"
N18-2067,P11-1068,1,0.938245,"Missing"
N18-2067,W07-2216,0,0.454627,"Missing"
N18-2067,D17-1002,1,0.863417,"Missing"
N18-2067,Q13-1002,0,0.283873,"r technique can also be applied to generalized Attardi (2006) systems; see 2 Transition-based Parsing We first introduce necessary definitions and notation. 2.1 A General Class of Transition Systems A transition system is given by a 4-tuple pC, T, cs , Cτ q, where C is a set of configurations, T is a set of transition functions between configurations, cs is an initialization function mapping an input sentence to an initial configuration, and Cτ Ă C defines a set of terminal configurations. 1 Faster exact inference algorithms have been defined for some sets of mildly non-projective trees (e.g. Pitler et al. (2013); see Gómez-Rodríguez (2016) for more), but lack an underlying transition system. Having one has the practical advantage of allowing generative models, as in Cohen et al. (2011), and transition-based scoring functions, which have yielded good projective-parsing results (Shi et al., 2017); plus the theoretical advantage of providing a single framework supporting greedy, beam-search, and exact inference. 420 Proceedings of NAACL-HLT 2018, pages 420–425 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics System Reduce Transitions Non-proj. Coverage Time Comp"
N18-2067,D11-1114,1,\N,Missing
N18-2109,P15-1033,0,0.0998404,"Missing"
N18-2109,D15-1159,0,0.247348,"Missing"
N18-2109,P16-1231,0,0.140602,"Missing"
N18-2109,D12-1029,1,0.876256,"Missing"
N18-2109,W06-2922,0,0.21922,"Missing"
N18-2109,P17-1027,1,0.861368,"Missing"
N18-2109,D16-1211,0,0.165898,"Missing"
N18-2109,P15-2042,1,0.934502,"Missing"
N18-2109,W06-2920,0,0.227103,"arser (LA=L EFT-A RC, RA=R IGHT-A RC, SH=S HIFT). We present a novel transition system called NLCovington (for “non-local Covington”), described in the bottom half of Figure 1. It consists in a modification of the non-projective Covington algorithm where: (1) the Left-Arc and Right-Arc transitions are parameterized with k, allowing the immediate creation of any attachment between j and the kth leftmost word in λ1 and moving k words to λ2 at once, and (2) the No-Arc transition is removed since it is no longer necessary. 4 Experiments 4.1 Data and Evaluation We use 9 datasets2 from the CoNLL-X (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007). To compare our system to the current state-of-theart transition-based parsers, we also evaluate it on the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)3 of the WSJ Penn Treebank (Marcus et al., 1993), hereinafter PT-SD, with standard splits. Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed excluding punctuation only on the PT-SD, for comparability. We repeat each experiment with three independent random initializations and report the average accuracy."
N18-2109,D14-1082,0,0.512872,"Missing"
N18-2109,W08-1301,0,0.189145,"lgorithm where: (1) the Left-Arc and Right-Arc transitions are parameterized with k, allowing the immediate creation of any attachment between j and the kth leftmost word in λ1 and moving k words to λ2 at once, and (2) the No-Arc transition is removed since it is no longer necessary. 4 Experiments 4.1 Data and Evaluation We use 9 datasets2 from the CoNLL-X (Buchholz and Marsi, 2006) and all datasets from the CoNLL-XI shared task (Nivre et al., 2007). To compare our system to the current state-of-theart transition-based parsers, we also evaluate it on the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)3 of the WSJ Penn Treebank (Marcus et al., 1993), hereinafter PT-SD, with standard splits. Labelled and Unlabelled Attachment Scores (LAS and UAS) are computed excluding punctuation only on the PT-SD, for comparability. We repeat each experiment with three independent random initializations and report the average accuracy. Statistical significance is assessed by a paired test with 10,000 bootstrap samples. This new transition system can use some restricted global information to build non-local dependencies and, consequently, reduce the number of tr"
N18-2109,E17-1117,0,0.0458546,"Missing"
N18-2109,D07-1013,0,0.0467892,"r, this greedy process is prone to error propagation: one wrong choice of transition can lead the parser to an erroneous state, causing more incorrect decisions. This is especially crucial for long attachments requiring a larger number of transitions. In addition, transition-based parsers traditionally focus on only two words of the sentence and their local context to choose the next transition. The lack of a global perspective favors the presence of errors when creating arcs involving multiple transitions. As expected, transition-based parsers build short arcs more accurately than long ones (McDonald and Nivre, 2007). Previous research such as (Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2012) and (Qi and Manning, 2017) proves that the widely-used projective arc-eager transition-based parser of Nivre (2003) benefits from shortening the length of transition 2 Non-Projective Covington Parser The original non-projective parser defined by Covington (2001) was modelled under the transitionbased parsing framework by Nivre (2008). We only sketch this transition system briefly for space reasons, and refer to (Nivre, 2008) for details. Parser configurations have the form c = hλ1 , λ2 , B, Ai, where λ1 and λ2 are l"
N18-2109,W03-3017,0,0.25581,"ts requiring a larger number of transitions. In addition, transition-based parsers traditionally focus on only two words of the sentence and their local context to choose the next transition. The lack of a global perspective favors the presence of errors when creating arcs involving multiple transitions. As expected, transition-based parsers build short arcs more accurately than long ones (McDonald and Nivre, 2007). Previous research such as (Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2012) and (Qi and Manning, 2017) proves that the widely-used projective arc-eager transition-based parser of Nivre (2003) benefits from shortening the length of transition 2 Non-Projective Covington Parser The original non-projective parser defined by Covington (2001) was modelled under the transitionbased parsing framework by Nivre (2008). We only sketch this transition system briefly for space reasons, and refer to (Nivre, 2008) for details. Parser configurations have the form c = hλ1 , λ2 , B, Ai, where λ1 and λ2 are lists of partially processed words, B a list (called buffer) of unprocessed words, and A the set of dependency arcs built so far. Given an input string w1 · · · wn , the parser starts at the init"
N18-2109,W04-0308,0,0.281585,"l Transitions Daniel Fern´andez-Gonz´alez and Carlos G´omez-Rodr´ıguez Universidade da Coru˜na FASTPARSE Lab, LyS Research Group, Departamento de Computaci´on Campus de Elvi˜na, s/n, 15071 A Coru˜na, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract sequences by creating non-local attachments. In particular, they augmented the original transition system with new actions whose behavior entails more than one arc-eager transition and involves a context beyond the traditional two focus words. Attardi (2006) and Sartorio et al. (2013) also extended the arc-standard transition-based algorithm (Nivre, 2004) with the same success. In the same vein, we present a novel unrestricted non-projective transition system based on the well-known algorithm by Covington (2001) that shortens the transition sequence necessary to parse a given sentence by the original algorithm, which becomes linear instead of quadratic with respect to sentence length. To achieve that, we propose new transitions that affect non-local words and are equivalent to one or more Covington actions, in a similar way to the transitions defined by Qi and Manning (2017) based on the arc-eager parser. Experiments show that this novel varia"
N18-2109,D14-1162,0,0.0799733,"Missing"
N18-2109,P17-2018,0,0.692129,"torio et al. (2013) also extended the arc-standard transition-based algorithm (Nivre, 2004) with the same success. In the same vein, we present a novel unrestricted non-projective transition system based on the well-known algorithm by Covington (2001) that shortens the transition sequence necessary to parse a given sentence by the original algorithm, which becomes linear instead of quadratic with respect to sentence length. To achieve that, we propose new transitions that affect non-local words and are equivalent to one or more Covington actions, in a similar way to the transitions defined by Qi and Manning (2017) based on the arc-eager parser. Experiments show that this novel variant significantly outperforms the original one in all datasets tested, and achieves the best reported accuracy for a greedy dependency parser on the Stanford Dependencies conversion of the WSJ Penn Treebank. We present a novel transition system, based on the Covington non-projective parser, introducing non-local transitions that can directly create arcs involving nodes to the left of the current focus positions. This avoids the need for long sequences of No-Arc transitions to create long-distance arcs, thus alleviating error"
N18-2109,P13-1014,0,0.0415595,"Missing"
N18-2109,D17-1002,0,0.145938,"Missing"
N18-2109,P15-1032,0,0.113055,"Missing"
N19-1076,D15-1159,0,0.0610946,"Missing"
N19-1076,P15-1033,0,0.149358,"Missing"
N19-1076,P16-1231,0,0.099882,"Missing"
N19-1076,D12-1029,1,0.883444,"Missing"
N19-1076,D16-1211,0,0.199054,"Missing"
N19-1076,N18-2109,1,0.902631,"Missing"
N19-1076,D14-1082,0,0.435315,"Missing"
N19-1076,P10-1151,1,0.905317,"Missing"
N19-1076,D16-1238,0,0.39103,"Missing"
N19-1076,Q16-1023,0,0.235842,"Missing"
N19-1076,Q16-1026,0,0.0374774,"et al., 2015). This kind of neural networks are able to learn the conditional probability of a sequence of discrete numbers that correspond to positions in an input sequence (in this case, indexes of words in a sentence) and, by means of attention (Bahdanau et al., 2014; Luong et al., 2015), implement a pointer that selects a position from the input at decoding time. Their approach initially reads the whole sentence, composed of the n words w1 , . . . , wn , and encodes each wi one by one into an encoder hidden state ei . As encoder, they employ a combination of CNNs and bi-directional LSTMs (Chiu and Nichols, 2016; Ma and Hovy, 2016). For each word, CNNs are used to obtain its character-level representation that is concatenated to the word and PoS embeddings to finally be fed into BiLSTMs that encode word context information. As decoder they present a top-down transition system, where parsing configurations use the classic data structures (Nivre, 2008): a buffer (that vit = score(dt , si ) (1) t (2) t a = sof tmax(v ) As attention scoring function (score()), they adopt the biaffine attention mechanism described in (Luong et al., 2015; Dozat and Manning, 2016). Finally, the attention vector at will be u"
N19-1076,D16-1180,0,0.253884,"Missing"
N19-1076,P81-1022,0,0.72836,"Missing"
N19-1076,D15-1166,0,0.162868,"pts have been made to alleviate the impact of error propagation in transition-based dependency parsing, but the latest and most successful approach was developed by Ma et al. (2018). In particular, they make use of pointer networks (Vinyals et al., 2015) to implement a new neural network architecture called stack-pointer network. The proposed framework provides a global view of the input sentence by capturing information from the whole sentence and all the arcs previously built, crucial for reducing the effect of error propagation; and, thanks to an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015), is able to return a position in that sentence that corresponds to a word related to the word currently on top of the stack. They take advantage of this and propose a novel transition system that follows a top-down depth-first strategy to perform the syntactic analysis. Concretely, it considers the word We propose a novel transition-based algorithm that straightforwardly parses sentences from left to right by building n attachments, with n being the length of the input sentence. Similarly to the recent stack-pointer parser by Ma et al. (2018), we use the pointer network framework that, given"
N19-1076,P16-1101,0,0.0432421,"of neural networks are able to learn the conditional probability of a sequence of discrete numbers that correspond to positions in an input sequence (in this case, indexes of words in a sentence) and, by means of attention (Bahdanau et al., 2014; Luong et al., 2015), implement a pointer that selects a position from the input at decoding time. Their approach initially reads the whole sentence, composed of the n words w1 , . . . , wn , and encodes each wi one by one into an encoder hidden state ei . As encoder, they employ a combination of CNNs and bi-directional LSTMs (Chiu and Nichols, 2016; Ma and Hovy, 2016). For each word, CNNs are used to obtain its character-level representation that is concatenated to the word and PoS embeddings to finally be fed into BiLSTMs that encode word context information. As decoder they present a top-down transition system, where parsing configurations use the classic data structures (Nivre, 2008): a buffer (that vit = score(dt , si ) (1) t (2) t a = sof tmax(v ) As attention scoring function (score()), they adopt the biaffine attention mechanism described in (Luong et al., 2015; Dozat and Manning, 2016). Finally, the attention vector at will be used to return the hi"
N19-1076,I17-1007,0,0.479139,"Missing"
N19-1076,P16-1218,0,0.119197,"Missing"
N19-1076,P18-1130,0,0.182878,"Fern´andez-Gonz´alez Universidade da Coru˜na FASTPARSE Lab, LyS Group Departamento de Computaci´on Elvi˜na, 15071 A Coru˜na, Spain d.fgonzalez@udc.es Carlos G´omez-Rodr´ıguez Universidade da Coru˜na, CITIC FASTPARSE Lab, LyS Group Departamento de Computaci´on Elvi˜na, 15071 A Coru˜na, Spain carlos.gomez@udc.es Abstract Stanford Dependencies conversion of the English Penn Treebank (hereinafter, PTB-SD), but also obtained the best results in the majority of languages in the CoNLL 2017 Shared Task (Dozat et al., 2017). This tendency recently changed, since a transition-based parser developed by Ma et al. (2018) managed to outperform the best graphbased model in the majority of datasets tested. Transition-based parsers incrementally build a dependency graph for an input sentence by applying a sequence of transitions. This results in more efficient parsers with linear time complexity for parsing projective sentences, or quadratic for handling non-projective structures, when implemented with greedy or beam search. However, their main weakness is the lack of access to global context information when transitions are greedily chosen. This favours error propagation, mainly affecting long dependencies that"
N19-1076,P15-1032,0,0.139114,"Missing"
N19-1076,J93-2004,0,0.0787901,"d in (Ma et al., 2018) of the graph-based parser by (Dozat and Manning, 2016), are implemented under the same framework as our approach and use the same training settings. Like (Ma et al., 2018), we report the average accuracy over 5 repetitions. i=1 4 UAS 91.8 93.1 93.99 93.56 93.9 94.23 94.3 94.5 94.61 95.87 96.04 93.1 94.08 94.10 94.26 94.30 94.88 95.74 95.84 Data and Settings 4.2 We use the same implementation as Ma et al. (2018) and conduct experiments on the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (using the Stanford parser v3.3.0)3 of the English Penn Treebank (Marcus et al., 1993), with standard splits and predicted PoS tags. In addition, we compare our approach to the original top-down parser on the same twelve languages from the Universal Dependency Treebanks4 (UD) that were used by Ma et al. (2018).5 Following standard practice, we just exclude punctuation for evaluating on PTB-SD and, for each experiment, we report the average Labelled and Unlabelled Attachment Scores (LAS and UAS) over 3 and 5 repetitions for UD and PTBSD, respectively. Results By outperforming the two current state-of-theart graph-based (Dozat and Manning, 2016) and transition-based (Ma et al., 2"
N19-1076,W08-1301,0,0.165769,"Missing"
N19-1076,W03-3023,0,0.512004,"Missing"
N19-1076,E17-1063,0,0.245056,"2 A practically faster version of the left-to-right parser might be implemented by just ignoring the presence of cycles during decoding, and destroying the cycles generated as a post-processing step that simply removes one of the arcs involved. 712 Parser Chen and Manning (2014) Dyer et al. (2015) Weiss et al. (2015) Ballesteros et al. (2016) Kiperwasser and Goldberg (2016) Alberti et al. (2015) Qi and Manning (2017) Fern´andez-G and G´omez-R (2018) Andor et al. (2016) Ma et al. (2018)∗ This work∗ Kiperwasser and Goldberg (2016) Wang and Chang (2016) Cheng et al. (2016) Kuncoro et al. (2016) Zhang et al. (2017) Ma and Hovy (2017) Dozat and Manning (2016) Ma et al. (2018)∗ states of the previous and next words in the sentence to generate dt , which seems to be more suitable for a left-to-right decoding. In dependency parsing, a tree for an input sentence of length n can be represented as a set of n directed and binary links l1 , . . . , ln . Each link li is characterized by the word wi in position i in the sentence and its head word wh , resulting in a pair (wi , wh ). Therefore, to train this novel variant, we factorize the conditional probability Pθ (y|x) to a set of head-dependent pairs as follows"
N19-1076,P05-1012,0,0.44605,"Missing"
N19-1076,J11-1007,0,0.0599149,"Missing"
N19-1076,H05-1066,0,0.547909,"Missing"
N19-1076,W03-3017,0,0.591137,"Missing"
N19-1076,J08-4003,0,0.131652,"decoding time. Their approach initially reads the whole sentence, composed of the n words w1 , . . . , wn , and encodes each wi one by one into an encoder hidden state ei . As encoder, they employ a combination of CNNs and bi-directional LSTMs (Chiu and Nichols, 2016; Ma and Hovy, 2016). For each word, CNNs are used to obtain its character-level representation that is concatenated to the word and PoS embeddings to finally be fed into BiLSTMs that encode word context information. As decoder they present a top-down transition system, where parsing configurations use the classic data structures (Nivre, 2008): a buffer (that vit = score(dt , si ) (1) t (2) t a = sof tmax(v ) As attention scoring function (score()), they adopt the biaffine attention mechanism described in (Luong et al., 2015; Dozat and Manning, 2016). Finally, the attention vector at will be used to return the highest-scoring position p and choose the next transition. The parsing process ends when only the root remains on the stack. As extra high-order features, Ma et al. (2018) add grandparent and sibling information, whose encoder hidden states are added to that of the word on top of the stack to generate the corresponding decode"
N19-1076,P17-2018,0,0.0665776,"dency parser based on pointer networks. We follow the same neural network architecture as the stack-pointerbased approach developed by Ma et al. (2018), but just using a focus word index instead of a buffer and a stack. Apart from doubling their system’s speed, our approach proves to be a competitive alternative on a variety of languages and achieves the best accuracy to date on the PTB-SD. The good performance of our algorithm can be explained by the shortening of the transition sequence length. In fact, it has been proved by several studies (Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2012; Qi and Manning, 2017; Fern´andez-Gonz´alez and G´omezRodr´ıguez, 2018) that by reducing the number of applied transitions, the impact of error propagation is alleviated, yielding more accurate parsers. Our system’s source code is freely available at https://github.com/danifg/ Left2Right-Pointer-Parser. Related work There is previous work that proposes to implement dependency parsing by independently selecting the head of each word in a sentence, using neural networks. In particular, Zhang et al. (2017) make use of a BiLSTM-based neural architecture to compute the probability of attaching each word to one of the o"
N19-1076,K17-3002,0,\N,Missing
N19-1077,P10-1001,0,0.0180958,"models than before. For example, in dependency parsing, the rich feature models with dozens of features used in transition-based approaches (Zhang and Nivre, 2011) can be simplified when using feedforward neural networks (Chen and Manning, 2014), and even more with BiLSTM architectures (Kiperwasser and Goldberg, 2016), where in fact two positional features can suffice (Shi et al., 2017). Similarly, in graph-based approaches, Dozat and Manning (2017) have shown that an arc-factored model can achieve state-of-the-art accuracy, without the need for the higher-order features used in systems like (Koo and Collins, 2010). In the same way, neural feature representations have made it possible to relax the need for structured representations. This is the case of sequenceto-sequence models that translate sentences into 2 Parsing as sequence labeling Sequence labeling is a structured prediction problem where a single output label is generated for every input token. This is the case of tasks such as PoS tagging, chunking or named-entity recognition, for which different approaches obtain accurate results (Brill, 1995; Ramshaw and Marcus, 1999; Reimers and Gurevych, 2017). On the contrary, previous work on dependency"
N19-1077,J95-4004,0,0.82888,"of-the-art accuracy, without the need for the higher-order features used in systems like (Koo and Collins, 2010). In the same way, neural feature representations have made it possible to relax the need for structured representations. This is the case of sequenceto-sequence models that translate sentences into 2 Parsing as sequence labeling Sequence labeling is a structured prediction problem where a single output label is generated for every input token. This is the case of tasks such as PoS tagging, chunking or named-entity recognition, for which different approaches obtain accurate results (Brill, 1995; Ramshaw and Marcus, 1999; Reimers and Gurevych, 2017). On the contrary, previous work on dependency parsing as sequence labeling is vague and reports results that are significantly lower than those provided by transition-, graph-based or sequence-tosequence models (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Zhang et al., 2017a). Spoustov´a and Spousta 717 Proceedings of NAACL-HLT 2019, pages 717–723 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (2010) encoded dependency trees using a relative PoS-based scheme"
N19-1077,D14-1082,0,0.0958442,"k or buffer). The source code is available at https://github. com/mstrise/dep2label Introduction The application of neural architectures to syntactic parsing, and especially the ability of long shortterm memories (LSTMs) to obtain context-aware feature representations (Hochreiter and Schmidhuber, 1997), has made it possible to parse natural language with conceptually simpler models than before. For example, in dependency parsing, the rich feature models with dozens of features used in transition-based approaches (Zhang and Nivre, 2011) can be simplified when using feedforward neural networks (Chen and Manning, 2014), and even more with BiLSTM architectures (Kiperwasser and Goldberg, 2016), where in fact two positional features can suffice (Shi et al., 2017). Similarly, in graph-based approaches, Dozat and Manning (2017) have shown that an arc-factored model can achieve state-of-the-art accuracy, without the need for the higher-order features used in systems like (Koo and Collins, 2010). In the same way, neural feature representations have made it possible to relax the need for structured representations. This is the case of sequenceto-sequence models that translate sentences into 2 Parsing as sequence la"
N19-1077,C18-1271,0,0.566477,"xisting work, results suggested that the technique was impractical. We show instead that with a conventional BIL STM -based model it is possible to obtain fast and accurate parsers. These parsers are conceptually simple, not needing traditional parsing algorithms or auxiliary structures. However, experiments on the PTB and a sample of UD treebanks show that they provide a good speed-accuracy tradeoff, with results competitive with more complex approaches. 1 Contribution We show that sequence labeling is useful for dependency parsing, in contrast to previous work (Spoustov´a and Spousta, 2010; Li et al., 2018). We explore four different encodings to represent dependency trees for a sentence of length n as a set of n labels associated with its words. We then use these representations to perform dependency parsing with an off-the-shelf sequence labeling model. The results show that we produce models with an excellent speed-accuracy tradeoff, without requiring any explicit parsing algorithm or auxiliary structure (e.g. stack or buffer). The source code is available at https://github. com/mstrise/dep2label Introduction The application of neural architectures to syntactic parsing, and especially the abi"
N19-1077,de-marneffe-etal-2006-generating,0,0.0683628,"Missing"
N19-1077,N15-1142,0,0.0309734,"BiLSTM layer is fed as input to the m+1th layer. Unless otherwise specified, the input token at a given time step is the concatenation of a word, PoS tag, and another word embedding learned through a character LSTM. HebrewHTB , KazakhKTB and TamilTTB , as a representative sample, following (de Lhoneux et al., 2017). As evaluation metrics, we use Labeled (LAS) and Unlabeled Attachment Score (UAS). We measure speed in sentences/second, both on a single core of a CPU2 and on a GPU3 . Setup We use NCRFpp as our sequence labeling framework (Yang and Zhang, 2018). For PTB, we use the embeddings by Ling et al. (2015), for comparison to BIST parser (Kiperwasser and Goldberg, 2016), which uses a similar architecture, but also needs a parsing algorithm and auxiliary structures. For UD, we follow an end-to-end setup and run UDPipe4 (Straka and Strakov´a, 2017) for tokenization and tagging. We use the pretrained word embeddings by Ginter et al. (2017). Appendix A contains additional hyperparameters. Decoder We use a feed-forward network, which is fed the output of the last BiLSTM. The output is computed as P (yi |hi ) = softmax(W · hi + b). Well-formedness (i) Each token must be assigned a head (one must be th"
N19-1077,P15-1033,0,0.0642108,"Missing"
N19-1077,P18-1130,0,0.0833937,"Missing"
N19-1077,J93-2004,0,0.070963,"ed seq2seq and sequence labeling models that use a relative positional encoding. As the relative PoS-based encoding and bracketing-based encoding provide the best results, we will conduct the rest of our experiments with these two encodings. Furthermore, we perform a small hyperparameter search involving encoding, number of hidden layers, their dimension and presence of character embeddings, as these parameters influence speed and accuracy. From z for a PoS-based encoding now on, we write Px,y z model and Bx,y for a bracketing-based encoding Experiments We use the English Penn Treebank (PTB) (Marcus et al., 1993) and its splits for parsing. We transform it into Stanford Dependencies (De Marneffe et al., 2006) and obtain the predicted PoS tags using Stanford tagger (Toutanova et al., 2003). We also select a sample of UDv2.2 treebanks (Nivre et al., 2018): Ancient-GreekPROIEL , CzechPDT , ChineseGSD , EnglishEWT , FinnishTDT , 2 Intel Core i7-7700 CPU 4.2 GHz. GeForce GTX 1080. 4 The pretrained models from the CoNLL18 Shared Task. 3 1 If single-rooted trees are a prerequisite, the most probable node will be selected among multiple root nodes. 719 94.2 94.0 P2,250 C P2,400 C P2,800 B2,250 P2,C 400 93.6 K"
N19-1077,D18-1162,1,0.711609,"Missing"
N19-1077,D17-1035,0,0.027733,"or the higher-order features used in systems like (Koo and Collins, 2010). In the same way, neural feature representations have made it possible to relax the need for structured representations. This is the case of sequenceto-sequence models that translate sentences into 2 Parsing as sequence labeling Sequence labeling is a structured prediction problem where a single output label is generated for every input token. This is the case of tasks such as PoS tagging, chunking or named-entity recognition, for which different approaches obtain accurate results (Brill, 1995; Ramshaw and Marcus, 1999; Reimers and Gurevych, 2017). On the contrary, previous work on dependency parsing as sequence labeling is vague and reports results that are significantly lower than those provided by transition-, graph-based or sequence-tosequence models (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Zhang et al., 2017a). Spoustov´a and Spousta 717 Proceedings of NAACL-HLT 2019, pages 717–723 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics (2010) encoded dependency trees using a relative PoS-based scheme to represent the head of a node, to then train an avera"
N19-1077,Q18-1017,0,0.0336832,". . , n} the dependent, and l a dependency label. The resulting dependency graph must be acyclic and such that each node in {1, . . . , n} has exactly one head, so it will be a directed tree rooted at node 0. Thus, to encode a dependency tree, it suffices to encode the unique head position and dependency label associated with each word of w1 . . . wn . To do so, we will give each word wi a discrete label of the form (xi , li ), where li is the dependency label and xi encodes the position of the head in one of the following four ways (see also Figure 1): for the sequence-to-sequence model in (Kiperwasser and Ballesteros, 2018). 3. Relative PoS-based encoding: xi is a tuple pi , oi . If oi > 0, the head of wi is the oi th closest among the words to the right of wi that have PoS tag pi . If oi &lt; 0, the head of wi is the −oi th closest among the words to the left of wi that have PoS tag pi . For example, (V, −2) means “the second verb to the left” of wi . This scheme is closer to the notion of valency, and was used by Spoustov´a and Spousta (2010). 4. Bracketing-based encoding: based on (YliJyr¨a, 2012; Yli-Jyr¨a and G´omez-Rodr´ıguez, 2017). In each label (xi , li ), the component xi is a string following the regular"
N19-1077,Q16-1023,0,0.497107,"mstrise/dep2label Introduction The application of neural architectures to syntactic parsing, and especially the ability of long shortterm memories (LSTMs) to obtain context-aware feature representations (Hochreiter and Schmidhuber, 1997), has made it possible to parse natural language with conceptually simpler models than before. For example, in dependency parsing, the rich feature models with dozens of features used in transition-based approaches (Zhang and Nivre, 2011) can be simplified when using feedforward neural networks (Chen and Manning, 2014), and even more with BiLSTM architectures (Kiperwasser and Goldberg, 2016), where in fact two positional features can suffice (Shi et al., 2017). Similarly, in graph-based approaches, Dozat and Manning (2017) have shown that an arc-factored model can achieve state-of-the-art accuracy, without the need for the higher-order features used in systems like (Koo and Collins, 2010). In the same way, neural feature representations have made it possible to relax the need for structured representations. This is the case of sequenceto-sequence models that translate sentences into 2 Parsing as sequence labeling Sequence labeling is a structured prediction problem where a single"
N19-1077,D17-1002,0,0.120209,"Missing"
N19-1077,K18-2011,0,0.0518235,"Missing"
N19-1077,P11-2033,0,0.100349,"ithout requiring any explicit parsing algorithm or auxiliary structure (e.g. stack or buffer). The source code is available at https://github. com/mstrise/dep2label Introduction The application of neural architectures to syntactic parsing, and especially the ability of long shortterm memories (LSTMs) to obtain context-aware feature representations (Hochreiter and Schmidhuber, 1997), has made it possible to parse natural language with conceptually simpler models than before. For example, in dependency parsing, the rich feature models with dozens of features used in transition-based approaches (Zhang and Nivre, 2011) can be simplified when using feedforward neural networks (Chen and Manning, 2014), and even more with BiLSTM architectures (Kiperwasser and Goldberg, 2016), where in fact two positional features can suffice (Shi et al., 2017). Similarly, in graph-based approaches, Dozat and Manning (2017) have shown that an arc-factored model can achieve state-of-the-art accuracy, without the need for the higher-order features used in systems like (Koo and Collins, 2010). In the same way, neural feature representations have made it possible to relax the need for structured representations. This is the case of"
N19-1077,D17-1175,0,0.070337,"Missing"
N19-1077,K17-3009,0,0.0369491,"Missing"
N19-1077,N03-1033,0,0.315212,"ill conduct the rest of our experiments with these two encodings. Furthermore, we perform a small hyperparameter search involving encoding, number of hidden layers, their dimension and presence of character embeddings, as these parameters influence speed and accuracy. From z for a PoS-based encoding now on, we write Px,y z model and Bx,y for a bracketing-based encoding Experiments We use the English Penn Treebank (PTB) (Marcus et al., 1993) and its splits for parsing. We transform it into Stanford Dependencies (De Marneffe et al., 2006) and obtain the predicted PoS tags using Stanford tagger (Toutanova et al., 2003). We also select a sample of UDv2.2 treebanks (Nivre et al., 2018): Ancient-GreekPROIEL , CzechPDT , ChineseGSD , EnglishEWT , FinnishTDT , 2 Intel Core i7-7700 CPU 4.2 GHz. GeForce GTX 1080. 4 The pretrained models from the CoNLL18 Shared Task. 3 1 If single-rooted trees are a prerequisite, the most probable node will be selected among multiple root nodes. 719 94.2 94.0 P2,250 C P2,400 C P2,800 B2,250 P2,C 400 93.6 KG (transition-based) KG (graph-based) CM DM Ma et al. (2018) B3,C 400B C 93.4 2, 400 P2,C 250 P2, 250 93.2 93.0 B2, 250 50 100 150 200 250 Speed (sentences/sec) 300 76±1 80±0 654"
N19-1077,P18-4013,0,0.109718,"onsider stacked BiLSTMs, where the output hm i of the mth BiLSTM layer is fed as input to the m+1th layer. Unless otherwise specified, the input token at a given time step is the concatenation of a word, PoS tag, and another word embedding learned through a character LSTM. HebrewHTB , KazakhKTB and TamilTTB , as a representative sample, following (de Lhoneux et al., 2017). As evaluation metrics, we use Labeled (LAS) and Unlabeled Attachment Score (UAS). We measure speed in sentences/second, both on a single core of a CPU2 and on a GPU3 . Setup We use NCRFpp as our sequence labeling framework (Yang and Zhang, 2018). For PTB, we use the embeddings by Ling et al. (2015), for comparison to BIST parser (Kiperwasser and Goldberg, 2016), which uses a similar architecture, but also needs a parsing algorithm and auxiliary structures. For UD, we follow an end-to-end setup and run UDPipe4 (Straka and Strakov´a, 2017) for tokenization and tagging. We use the pretrained word embeddings by Ginter et al. (2017). Appendix A contains additional hyperparameters. Decoder We use a feed-forward network, which is fed the output of the last BiLSTM. The output is computed as P (yi |hi ) = softmax(W · hi + b). Well-formedness"
N19-1077,P17-1160,1,0.629651,"Missing"
N19-1218,P16-1223,0,0.0226233,"Missing"
N19-1218,D14-1181,0,0.0034074,"eddings, w1:n , where wi is a concatenation of an internal embedding learned during the training process for the word wi , and a pretrained embedding extracted from GloVe (Pennington et al., 2014)5 , that is further fine-tuned. Long short-term memory network (Hochreiter and Schmidhuber, 1997): The output for an element wi also depends on the output of wi−1 . The LSTMθ (w1:n )6 takes as input a sequence of word embeddings and produces a sequence of hidden outputs, h1:n (hi size set to 128). The last output of the LSTMθ , hn , is fed to a MLPθ . Convolutional Neural Network (LeCun et al., 1995; Kim, 2014). It captures local properties over continuous slices of text by applying a convolution layer made of different filters. We use a wide convolution, with a window slice size of length 3 and 250 different filters. The convolutional layer uses a relu as the activation function. The output is fed to a max pooling layer, whose output vector is passed again as input to a MLPθ . 4 Experiments Setup All MLPθ ’s have 128 input neurons and 1 hidden layer. We trained up to 15 epochs using mini-batches (size=16), Adam (lr=0.001) (Kingma and Ba, 2015) and early stopping. Table 3 shows the macro and weighte"
N19-1218,Q18-1023,0,0.0618217,"t and Berant, 2017). These are partially due to embedding methods (Mikolov et al., 2013; Devlin et al., 2018) and neural networks (Rosenblatt, 1958; Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017), but also to the availability of new resources and challenges. For instance, in cloze-form tasks (Hermann et al., 2015; Bajgar et al., 2016), the goal is to predict the missing word given a short context. Weston et al. (2015) presented baBI, a set of proxy tasks for reading comprenhension. In the SQuAD corpus (Rajpurkar et al., 2016), the aim is to answer questions given a Wikipedia passage. Kocisky et al. (2018) introduce NarrativeQA, where answering the questions requires to process entire stories. In a related line, Frermann et al. (2017) use fictional crime scene investigation data, from the CSI series, to define a task where the models try to answer the question: ‘who committed the crime?’. In an alternative line of work, script induction (Schank and Abelson, 1977) has been also a useful approach to evaluate inference and semantic capabilities of NLP systems. Here, a model processes a document to infer new sequences that reflect events that are statistically probable (e.g. go to a restaurant, be"
N19-1218,P14-5010,0,0.0034103,"fan fiction (and only fan fiction texts) from https://www. fanfiction.net/book/Harry-Potter/ and a version of the crawler by Milli and Bamman (2016).3 We collected Harry Potter stories written in English and marked with the status ‘completed’. From these we extracted a total of 82 836 spell occurrences, that we used to obtain the scene descriptions. Table 2 details the statistics of the corpus (see also Appendix A). Note that similar to Twitter corpora, fan fiction stories can be deleted over time by users or admins, causing losses in the dataset.4 Preprocessing We tokenized the samples with (Manning et al., 2014) and merged the occurrences of multi-word spells into a single token. 3 Models This work addresses the task as a classification problem, and in particular as a sequence to label classification problem. For this reason, we rely on standard models used for this type of task: multinomial logistic regression, a multi-layered perceptron, convolutional neural networks and long short-term memory networks. We outline the essentials of each of these models, but will treat them as black boxes. In a related line, Kaushik and Lipton (2018) discuss the need of providing rigorous baselines that help better"
N19-1218,D16-1218,0,0.178395,"considers experiments (§4) using snippets with the 32, 64, 96 and 128 previous tokens to an action. We provide the needed scripts to rebuild the corpus using arbitrary lengths.2 2.2 Data crawling The number of occurrences of spells in the original Harry Potter books is small (432 occurrences), which makes it difficult to train and test a machine learning model. However, the amount of available fan fiction for this saga allows to create a large corpus. For HPAC, we used fan fiction (and only fan fiction texts) from https://www. fanfiction.net/book/Harry-Potter/ and a version of the crawler by Milli and Bamman (2016).3 We collected Harry Potter stories written in English and marked with the status ‘completed’. From these we extracted a total of 82 836 spell occurrences, that we used to obtain the scene descriptions. Table 2 details the statistics of the corpus (see also Appendix A). Note that similar to Twitter corpora, fan fiction stories can be deleted over time by users or admins, causing losses in the dataset.4 Preprocessing We tokenized the samples with (Manning et al., 2014) and merged the occurrences of multi-word spells into a single token. 3 Models This work addresses the task as a classification"
N19-1218,W02-1011,0,0.0214876,"ctions (i.e. real-domain setups), or if such transfer is not possible and a model needs to be trained from scratch. Human comparison We collected human annotations from 208 scenes involving frequent actions. The accuracy/F-macro/F-weighted was 39.20/30.00/40.90. The LSTM approach obtained 41.26/25.37/39.86. Overall, the LSTM approach obtained a similar performance, but the lower macro F-score by the LSTM could be an indicator that humans can distinguish within a wider spectrum of actions. As a side note, super-human performance it is not strange in other NLP tasks, such as sentiment analysis (Pang et al., 2002). Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. pages 789–797. 5 Conclusion We explored action prediction from written stories. We first introduced a corpus set in the world of Harry Potter’s literature. Spells in these novels act as keywords that abstract actions. This idea was used to label a collection of fan fiction. We then evaluated standard NLP approaches, from logistic regression to sequential models such as LSTM s. The latter performed better in general, although vanilla models achieved a higher performance for actions that occurred a few"
N19-1218,D14-1162,0,0.0824516,"nce w1:n is encoded as a one-hot vector, v (total occurrence weighting scheme). Multinomial Logistic Regression Let MLRθ (v) be an abstraction of a multinomial logistic regression parametrized by θ, the output for an input v is computed as the arg maxa∈A P (y = a|v), where P (y = a|v) is a sof tmax function, i.e, Wa ·v P (y = a|v) = PAe Wa0 ·v . a0 Sequential models The input sequence is represented as a sequence of word embeddings, w1:n , where wi is a concatenation of an internal embedding learned during the training process for the word wi , and a pretrained embedding extracted from GloVe (Pennington et al., 2014)5 , that is further fine-tuned. Long short-term memory network (Hochreiter and Schmidhuber, 1997): The output for an element wi also depends on the output of wi−1 . The LSTMθ (w1:n )6 takes as input a sequence of word embeddings and produces a sequence of hidden outputs, h1:n (hi size set to 128). The last output of the LSTMθ , hn , is fed to a MLPθ . Convolutional Neural Network (LeCun et al., 1995; Kim, 2014). It captures local properties over continuous slices of text by applying a convolution layer made of different filters. We use a wide convolution, with a window slice size of length 3 a"
N19-1218,P02-1040,0,\N,Missing
N19-1218,E14-1024,0,\N,Missing
N19-1218,P08-1090,0,\N,Missing
N19-1218,D18-1546,0,\N,Missing
N19-1218,N18-2088,0,\N,Missing
N19-1218,Q18-1001,0,\N,Missing
P08-1110,E99-1020,0,0.0947804,"properties (such as correctness), establish relations between them, derive new parsers from existing ones and obtain efficient implementations automatically (G´omez-Rodr´ıguez et al., 2007). The formalism was initially defined for context-free grammars and later applied to other constituencybased formalisms, such as tree-adjoining grammars ∗ Partially supported by Ministerio de Educaci´on y Ciencia and FEDER (TIN2004-07246-C03, HUM2007-66607-C04), Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC10501PN, PGIDIT05PXIC30501PN, Rede Galega de Proc. da Linguaxe e RI) and Programa de Becas FPU. (Alonso et al., 1999). However, since parsing schemata are defined as deduction systems over sets of constituency trees, they cannot be used to describe dependency parsers. In this paper, we define an analogous formalism that can be used to define, analyze and compare dependency parsers. We use this framework to provide uniform, high-level descriptions for a wide range of well-known algorithms described in the literature, and we show how they formally relate to each other and how we can use these relations and the formalism itself to prove their correctness. 1.1 Parsing schemata Parsing schemata (Sikkel, 1997) pro"
P08-1110,W06-2922,0,0.22437,"Missing"
P08-1110,W98-0507,0,0.956481,"e dependency parsers are also grammar2 wi is shorthand for the marked terminal (wi , i). These are used by Sikkel (1997) to link terminal symbols to string positions so that an input sentence can be represented as a set of trees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b"
P08-1110,P89-1018,0,0.557318,"schemata for some well-known dependency parsers. As we can see, we use D-rules as side conditions for deduction steps, since this parsing strategy is not grammar-based. Conceptually, the schema we have just defined describes a recogniser: given a set of Drules and an input string wi . . . wn , the sentence can be parsed (projectively) under those D-rules if and only if this deduction system can infer a correct final item. However, when executing this schema with a deductive engine, we can recover the parse forest by following back pointers in the same way as is done with constituency parsers (Billot and Lang, 1989). Of course, boolean D-rules are of limited interest in practice. However, this schema provides a formalization of a parsing strategy which is independent of the way linking decisions are taken in a particular implementation. In practice, statistical models can be used to decide whether a step linking words a and b (i.e., having a → b as a side condition) is executed or not, and probabilities can be attached to items in order to assign different weights to different analyses of the sentence. The same principle applies to the rest of D-rule-based parsers described in this paper. 3.2 Eis96 (Eisn"
P08-1110,P96-1025,0,0.0603299,"arsers. When defining projective parsers, correct final items will be those containing projective parse trees for w1 . . . wn . This distinction is relevant because the concepts of soundness and correctness of parsing schemata are based on correct final items (cf. section 1.1), and we expect correct projective parsers to produce only projective structures, while nonprojective parsers should find all possible structures including nonprojective ones. 3 Some practical examples 3.1 Col96 (Collins, 96) One of the most straightforward projective dependency parsing strategies is the one described by Collins (1996), directly based on the CYK parsing algorithm. This parser works with dependency trees which are linked to each other by creating links between their heads. Its item set is defined as ICol96 = {[i, j, h] |1 ≤ i ≤ h ≤ j ≤ n}, where an item [i, j, h] is defined as the set of forests containing a single projective dependency tree t such that t is grounded, yield (t) = wi . . . wj and head (t) = wh . For an input string w1 . . . wn , the set of hypotheses is H = {[i, i, i] |0 ≤ i ≤ n + 1}, i.e., the set of forests containing a single dependency tree of the form wi (wi ). This same set of hypothese"
P08-1110,N06-1021,0,0.0432819,"he rest of D-rule-based parsers described in this paper. 3.2 Eis96 (Eisner, 96) By counting the number of free variables used in each deduction step of Collins’ parser, we can conclude that it has a time complexity of O(n5 ). This complexity arises from the fact that a parentless word (head) may appear in any position in the partial results generated by the parser; the complexity can be reduced to O(n3 ) by ensuring that parentless words can only appear at the first or last position of an item. This is the principle behind the parser defined by Eisner (1996), which is still in wide use today (Corston-Oliver et al., 2006; McDonald et al., 971 2005a). The item set for Eisner’s parsing schema is IEis96 = {[i, j, T, F ] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, F, T ] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, F, F ] | 0 ≤ i ≤ j ≤ n}, where each item [i, j, T, F ] is defined as the item [i, j, j] ∈ ICol96 , each item [i, j, F, T ] is defined as the item [i, j, i] ∈ ICol96 , and each item [i, j, F, F ] is defined as the set of forests of the form {t1 , t2 } such that t1 and t2 are grounded, head (t1 ) = wi , head (t2 ) = wj , and ∃k ∈ N(i ≤ k &lt; j)/yield (t1 ) = wi . . . wk ∧ yield (t2 ) = wk+1 . . . wj . Note that the flags b, c in an item [i, j"
P08-1110,W98-0511,0,0.583594,"eduction steps for the schema are shown in Figure 2, and the final item set is {[(S.), 1, n]}. As we can see, the schema for Lombardo and Lesmo’s parser resembles the Earley-style parser in Sikkel (1997), with some changes to adapt it to dependency grammar (for example, the Scanner always moves the dot over the head symbol ∗). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying context-free grammar parsing schemata of Sikkel (1997) in a similar way. The algorithm by Barbero et al. (1998) can be obtained from the leftcorner parser, and the one by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Pseudo-projectivity Pseudo-projective parsers can generate nonprojective analyses in polynomial time by using a projective parsing strategy and postprocessing the results to establish nonprojective links. For example, the algorithm by Kahane et al. (1998) uses a projective parsing strategy like that of LL96, but using the following initializer step instead of the Initter and Predictor:5 Initter 4 [A(α), i, i − 1] sr A(α) ∈ P ∧ 1 ≤ i ≤ n Relations between dependency parsers The framework of parsing schemata can be used to establish relationships betw"
P08-1110,P81-1022,0,0.738346,"ardo and Lesmo, 96) and other Earley-based parsers The algorithms in the above examples are based on taking individual decisions about dependency links, represented by D-rules. Other parsers, such as that of Lombardo and Lesmo (1996), use grammars with context-free like rules which encode the preferred order of dependents for each given governor, as defined by Gaifman (1965). For example, a rule of the form N (Det ∗ P P ) is used to allow N to have Det as left dependent and P P as right dependent. The algorithm by Lombardo and Lesmo (1996) is a version of Earley’s context-free grammar parser (Earley, 1970) using Gaifman’s dependency grammar, and can be written by using an item set ILomLes = {[A(α.β), i, j] |A(αβ) ∈ P ∧ 1 ≤ i ≤ j ≤ n}, where each item [A(α.β), i, j] represents the set of partial dependency trees rooted at A, where the direct children of A are αβ, and the subtrees rooted at α have yield wi . . . wj . The deduction steps for the schema are shown in Figure 2, and the final item set is {[(S.), 1, n]}. As we can see, the schema for Lombardo and Lesmo’s parser resembles the Earley-style parser in Sikkel (1997), with some changes to adapt it to dependency grammar (for example, the Scan"
P08-1110,P99-1059,0,0.512818,"ent dependency trees where the word in position i or j is the head, while items with both flags set to F represent pairs of trees headed at positions i and j, and therefore correspond to disconnected dependency graphs. Deduction steps4 are shown in Figure 2. The set of final items is {[0, n, F, T ]}. Note that these items represent dependency trees rooted at the BOS marker w0 , which acts as a “dummy head” for the sentence. In order for the algorithm to parse sentences correctly, we will need to define D-rules to allow w0 to be linked to the real sentence head. 3.3 ES99 (Eisner and Satta, 99) Eisner and Satta (1999) define an O(n3 ) parser for split head automaton grammars that can be used 4 Alternatively, we could consider items of the form [i, i + 1, F, F ] to be hypotheses for this parsing schema, so we would not need an Initter step. However, we have chosen to use a standard set of hypotheses valid for all parsers because this allows for more straightforward proofs of relations between schemata. for dependency parsing. This algorithm is conceptually simpler than Eis96, since it only uses items representing single dependency trees, avoiding items of the form [i, j, F, F ]. Its item set is IES99 = {[i,"
P08-1110,C96-1058,0,0.925767,"ees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the semantics of these parsing strat"
P08-1110,P98-1106,0,0.628947,"lso grammar2 wi is shorthand for the marked terminal (wi , i). These are used by Sikkel (1997) to link terminal symbols to string positions so that an input sentence can be represented as a set of trees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent"
P08-1110,C96-2122,0,0.788926,"if such a rule exists. Some dependency parsers are also grammar2 wi is shorthand for the marked terminal (wi , i). These are used by Sikkel (1997) to link terminal symbols to string positions so that an input sentence can be represented as a set of trees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b,"
P08-1110,P05-1012,0,0.159405,"Missing"
P08-1110,H05-1066,0,0.225193,"Missing"
P08-1110,D07-1013,0,0.0646823,"Missing"
P08-1110,W03-3023,0,0.888754,"used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the semantics of these parsing strategies that is independent of"
P08-1110,C98-1102,0,\N,Missing
P09-1111,P87-1015,0,0.929142,"n-out at most 2 into a binary form, whenever this is possible. This results in asymptotical run-time improvement for known parsing algorithms for this class. 1 Introduction Since its early years, the computational linguistics field has devoted much effort to the development of formal systems for modeling the syntax of natural language. There has been a considerable interest in rewriting systems that enlarge the generative power of context-free grammars, still remaining far below the power of the class of contextsensitive grammars; see (Joshi et al., 1991) for discussion. Following this line, (Vijay-Shanker et al., 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community. LCFRSs allow the derivation of tuples of strings,1 i.e., discontinuous phrases, that turn out to be very useful in modeling languages with relatively free word order. This feature has recently been used for mapping non-projective dependency grammars into discontinuous phrase structures (Kuhlmann and Satta, 2009). Furthermore, LCFRSs also implement so-called synchronous 1 In its more general definition, an LCFRS provides a framework where abstract"
P09-1111,C08-1136,0,0.0438906,"Missing"
P09-1111,W05-1502,0,0.0387958,"Missing"
P09-1111,P05-1033,0,0.152422,"Missing"
P09-1111,N09-1061,1,0.814772,"Missing"
P09-1111,E09-1055,1,0.725521,"remaining far below the power of the class of contextsensitive grammars; see (Joshi et al., 1991) for discussion. Following this line, (Vijay-Shanker et al., 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community. LCFRSs allow the derivation of tuples of strings,1 i.e., discontinuous phrases, that turn out to be very useful in modeling languages with relatively free word order. This feature has recently been used for mapping non-projective dependency grammars into discontinuous phrase structures (Kuhlmann and Satta, 2009). Furthermore, LCFRSs also implement so-called synchronous 1 In its more general definition, an LCFRS provides a framework where abstract structures can be generated, as for instance trees and graphs. Throughout this paper we focus on so-called string-based LCFRSs, where rewriting is defined over strings only. 985 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 985–993, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP associating each production p of a grammar with a function g that rearranges the string components in the tuples generated by the"
P09-1111,N03-1021,0,0.0566465,"Missing"
P10-1151,afonso-etal-2002-floresta,0,0.077213,"Missing"
P10-1151,W03-2405,0,0.0743588,"Missing"
P10-1151,W06-2920,0,0.578526,"r. Secondly, we present a transition-based parsing algorithm for 2-planar dependency trees, developed in two steps. We begin by showing how the stack-based algorithm of Nivre (2003) can be generalized from projective to planar structures. We then extend the system by adding a second stack and show that the resulting system captures exactly the set of 2-planar structures. Although the contributions of this paper are mainly theoretical, we also present an empirical evaluation of the 2planar parser, showing that it outperforms the projective parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi, 2006). 1492 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1492–1501, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 2 Preliminaries 2.1 Dependency Graphs Let w = w1 . . . wn be an input string.1 An interval (with endpoints i and j) of the string w is a set of the form [i, j] = {wk |i ≤ k ≤ j}. Definition 1. A dependency graph for w is a directed graph G = (Vw , E), where Vw = [1, n] and E ⊆ Vw × Vw . We call an edge (wi , wj ) in a dependency graph G a dependency link2 from wi to wj . We say that wi is the parent ("
P10-1151,C96-1058,0,0.0688547,"led a dependency forest. Definition 3. A dependency forest G for a string w1 . . . wn is projective iff bwi c is an interval for every word wi ∈ [1, n]. Projective dependency trees correspond to the set of structures that can be induced from lexicalised context-free derivations (Kuhlmann, 2007; Gaifman, 1965). Like context-free grammars, projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (Eisner, 1996; G´omez-Rodr´ıguez et al., 2008), while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (McDonald et al., 2005b; McDonald and Satta, 2007). 2.3 Planarity Definition 2. A dependency graph G for a string w1 . . . wn is said to be a forest iff it satisfies: 1. Acyclicity: If wi →∗ wj , then not wj → wi . The concept of planarity (Sleator and Temperley, 1993) is closely related to projectivity3 and can be informally defined as the property of a dependency forest whose links can be drawn above the words without crossing.4 To defi"
P10-1151,P08-1110,1,0.91145,"Missing"
P10-1151,E09-1034,1,0.880589,"Missing"
P10-1151,P07-1077,0,0.0733295,"lanes with colours and say that a dependency graph G is m-planar if it is possible to assign one of m colours to each of its links in such a way that links with the same colour do not cross. Note that there may be multiple ways of dividing an m-planar graph into planes, as shown in the example of Figure 1. 3 Determining Multiplanarity Several constraints on non-projective dependency structures have been proposed recently that seek a good balance between parsing efficiency and coverage of non-projective phenomena present in natural language treebanks. For example, Kuhlmann and Nivre (2006) and Havelka (2007) have shown that the vast majority of structures present in existing treebanks are well-nested and have a small gap degree (Bodirsky et al., 2005), leading to an interest in parsers for these kinds of structures (G´omezRodr´ıguez et al., 2009). No similar analysis has been performed for m-planar structures, although Yli-Jyr¨a (2003) provides evidence that all except two structures in the Danish dependency treebank are at most 3-planar. However, his analysis is based on constraints that restrict the possible ways of assigning planes to dependency links, and he is not guaranteed to find the mini"
P10-1151,P07-1021,0,0.062204,"Missing"
P10-1151,P06-2066,1,0.801977,", i.e., i 6= j ⇔ wi 6= wj . This can be guaranteed in practice by annotating each terminal symbol with its position in the input. 2 In practice, dependency links are usually labeled, but to simplify the presentation we will ignore labels throughout most of the paper. However, all the results and algorithms presented can be applied to labeled dependency graphs and will be so applied in the experimental evaluation. 2.4 Multiplanarity The concept of planarity on its own does not seem to be very relevant as an extension of projectivity for practical dependency parsing. According to the results by Kuhlmann and Nivre (2006), most non-projective structures in dependency treebanks are also non-planar, so being able to parse planar structures will only give us a modest improvement in coverage with respect to a projective parser. However, our interest in planarity is motivated by the fact that it can be generalised to multiplanarity (Yli-Jyr¨a, 2003): 3 For dependency forests that are extended with a unique artificial root located at position 0, as is commonly done, the two notions are equivalent. 4 Planarity in the context of dependency structures is not to be confused with the homonymous concept in graph theory, w"
P10-1151,E09-1055,0,0.066818,"Missing"
P10-1151,P81-1022,0,0.645754,"Missing"
P10-1151,W06-2933,1,0.902061,"Missing"
P10-1151,P09-1039,0,0.103239,"Missing"
P10-1151,W07-2216,0,0.0633145,"roduction Dependency-based syntactic parsing has become a widely used technique in natural language processing, and many different parsing models have been proposed in recent years (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Titov and Henderson, 2007; Martins et al., 2009). One of the unresolved issues in this area is the proper treatment of non-projective dependency trees, which seem to be required for an adequate representation of predicate-argument structure, but which undermine the efficiency of dependency parsing (Neuhaus and Br¨oker, 1997; BuchKromann, 2006; McDonald and Satta, 2007). Caught between the Scylla of linguistically inadequate projective trees and the Charybdis of computationally intractable non-projective trees, some researchers have sought a middle ground by exploring classes of mildly non-projective dependency structures that strike a better balance between expressivity and complexity (Nivre, 2006; In this paper, we explore another characterization of mildly non-projective dependency trees based on the notion of multiplanarity. This was originally proposed by Yli-Jyr¨a (2003) but has so far played a marginal role in the dependency parsing literature, becaus"
P10-1151,P05-1012,0,0.0371918,"ncy trees correspond to the set of structures that can be induced from lexicalised context-free derivations (Kuhlmann, 2007; Gaifman, 1965). Like context-free grammars, projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (Eisner, 1996; G´omez-Rodr´ıguez et al., 2008), while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (McDonald et al., 2005b; McDonald and Satta, 2007). 2.3 Planarity Definition 2. A dependency graph G for a string w1 . . . wn is said to be a forest iff it satisfies: 1. Acyclicity: If wi →∗ wj , then not wj → wi . The concept of planarity (Sleator and Temperley, 1993) is closely related to projectivity3 and can be informally defined as the property of a dependency forest whose links can be drawn above the words without crossing.4 To define planarity more formally, we first define crossing links as follows: let (wi , wk ) and (wj , wl ) be dependency links in a dependency graph G. Without loss of generality, we ass"
P10-1151,H05-1066,0,0.478655,"Missing"
P10-1151,P97-1043,0,0.134556,"Missing"
P10-1151,P05-1013,1,0.824732,"e results in the next section. 6 Empirical Evaluation In order to get a first estimate of the empirical accuracy that can be obtained with transition-based 2-planar parsing, we have evaluated the parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi, 2006): Czech, Danish, German and Portuguese. As our baseline, we take the strictly projective arc-eager transition system proposed by Nivre (2003), as implemented in the freely available MaltParser system (Nivre et al., 2006a), with and without the pseudo-projective parsing technique for recovering non-projective dependencies (Nivre and Nilsson, 2005). For the two baseline systems, we use the parameter settings used by Nivre et al. (2006b) in the original shared task, where the pseudo-projective version of MaltParser was one of the two top performing systems (Buchholz and Marsi, 2006). For our 2planar parser, we use the same kernelized SVM classifiers as MaltParser, using the LIBSVM package (Chang and Lin, 2001), with feature models that are similar to MaltParser but extended with features defined over the second stack.7 In Table 2, we report labeled (LAS) and unlabeled (UAS) attachment score on the four languages for all three systems. Fo"
P10-1151,W04-2407,1,0.668237,"Missing"
P10-1151,W03-3017,1,0.841258,"literature, because no algorithm was known for determining whether an arbitrary tree was mplanar, and no parsing algorithm existed for any constant value of m. The contribution of this paper is twofold. First, we present a procedure for determining the minimal number m such that a dependency tree is m-planar and use it to show that the overwhelming majority of sentences in dependency treebanks have a tree that is at most 2planar. Secondly, we present a transition-based parsing algorithm for 2-planar dependency trees, developed in two steps. We begin by showing how the stack-based algorithm of Nivre (2003) can be generalized from projective to planar structures. We then extend the system by adding a second stack and show that the resulting system captures exactly the set of 2-planar structures. Although the contributions of this paper are mainly theoretical, we also present an empirical evaluation of the 2planar parser, showing that it outperforms the projective parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi, 2006). 1492 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1492–1501, c Uppsala, Sweden, 11-16 July 2010. 2010 As"
P10-1151,J08-4003,1,0.853845,"2 case. 4 Parsing 1-Planar Structures In this section, we present a deterministic lineartime parser for planar dependency structures. The parser is a variant of Nivre’s arc-eager projective parser (Nivre, 2003), modified so that it can also handle graphs that are planar but not projective. As seen in Table 1, this only gives a modest improvement in coverage compared to projective parsing, so the main interest of this algorithm lies in the fact that it can be generalised to deal with 2-planar structures, as shown in the next section. 4.1 Transition Systems In the transition-based framework of Nivre (2008), a deterministic dependency parser is defined by a non-deterministic transition system, specifying a set of elementary operations that can be executed during the parsing process, and an oracle that deterministically selects a single transition at each choice point of the parsing process. Definition 6. A transition system for dependency parsing is a quadruple S = (C, T, cs , Ct ) where 1. C is a set of possible parser configurations, 2. T is a set of transitions, each of which is a partial function t : C → C, 3. cs is a function that maps each input sentence w to an initial configuration cs (w"
P10-1151,C08-1095,0,0.0150134,"e same reason, we remove the constraint in Nivre’s parser by which words without a head cannot be reduced. This has the side effect of making the parser able to output cyclic graphs. Since we are interested in planar dependency forests, which do not contain cycles, we only apply A RC transitions after checking that there is no undirected path between the nodes to be linked. This check can be done without affecting the linear-time complexity of the 1496 parser by storing the weakly connected component of each node in g(c). The fine-grained transitions used by this parser have also been used by Sagae and Tsujii (2008) to parse DAGs. However, the latter parser differs from ours in the constraints, since it does not allow the reduction of words without a head (disallowing forests with covered roots) and does not enforce the acyclicity constraint (which is guaranteed by post-processing the graphs to break cycles). 4.3 Correctness and Complexity For reasons of space, we can only give a sketch of the correctness proof. We wish to prove that the planar transition system is sound and complete for the set Fp of all planar dependency forests. To prove soundness, we have to show that, for every sentence w and transi"
P10-1151,1993.iwpt-1.22,0,0.244733,"guistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (Eisner, 1996; G´omez-Rodr´ıguez et al., 2008), while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (McDonald et al., 2005b; McDonald and Satta, 2007). 2.3 Planarity Definition 2. A dependency graph G for a string w1 . . . wn is said to be a forest iff it satisfies: 1. Acyclicity: If wi →∗ wj , then not wj → wi . The concept of planarity (Sleator and Temperley, 1993) is closely related to projectivity3 and can be informally defined as the property of a dependency forest whose links can be drawn above the words without crossing.4 To define planarity more formally, we first define crossing links as follows: let (wi , wk ) and (wj , wl ) be dependency links in a dependency graph G. Without loss of generality, we assume that min(i, k) ≤ min(j, l). Then, the links are said to be crossing if min(i, k) < min(j, l) < max (i, k) < max (j, l). 2. Single-head: If wj → wi , then not wk → wi (for every k 6= j). Definition 4. A dependency graph is planar iff it does no"
P10-1151,W07-2218,0,0.169298,"Missing"
P10-1151,W03-3023,0,0.525037,"Missing"
P10-1151,E06-1010,1,\N,Missing
P10-1151,nivre-etal-2006-maltparser,1,\N,Missing
P11-1068,W06-2922,0,0.039634,"e transition. It is called complete whenever c0 D I.w/, and cm 2 C t . We note that a computation can be uniquely specified by its initial configuration c0 and the sequence of its transitions, understood as a string over T . Complete computations, where c0 is fixed, can be specified by their transition sequences alone. 3 Arc-Standard Model To introduce the core concepts of the paper, we first look at a particularly simple model for transitionbased dependency parsing, known as the arc-standard model. This model has been used, in slightly different variants, by a number of parsers (Nivre, 2004; Attardi, 2006; Huang and Sagae, 2010). 3.1 Transition System The arc-standard model uses three types of transitions: Shift (sh) removes the first node in the buffer and pushes it to the stack. Left-Arc (la) creates a new arc with the topmost node on the stack as the head and the second-topmost node as the dependent, and removes the second-topmost node from the stack. Right-Arc (ra) is symmetric to Left-Arc in that it creates an arc with the second-topmost node as the head and the topmost node as the dependent, and removes the topmost node. The three transitions can be formally specified as in Figure 1. The"
P11-1068,P89-1018,0,0.0650338,"es where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eage"
P11-1068,P96-1025,0,0.0250951,"ntermediate configuration, and thereby violates property P1. 3.5 Discussion Let us briefly take stock of what we have achieved so far. We have provided a deduction system capable of tabulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.jwj3 / and time in O.jwj5 /. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.jwj4 /. 4 Adding Features 1 are both push computations with strictly fewer transitions than . Suppose that the last transition in is ra. In this case, ˇ.c/ D ˇk for some i < k < j , .c/ D .c0 /jh with h < k, ˇ.cm 1 / D ˇj , and .cm 1 / D  .c0 /jhjh0 for some k  h0 < j . B"
P11-1068,P99-1059,1,0.293239,"bulating the set of all computations of an arcstandard parser on a given input string, and proved the correctness of this system relative to an interpretation based on push computations. Inspecting the system, we can see that its generic implementation takes space in O.jwj3 / and time in O.jwj5 /. Our deduction system is essentially the same as the one for the CKY algorithm for bilexicalized contextfree grammar (Collins, 1996; Gómez-Rodríguez et al., 2008). This equivalence reveals a deep correspondence between the arc-standard model and bilexicalized context-free grammar, and, via results by Eisner and Satta (1999), to head automata. In particular, Eisner’s and Satta’s “hook trick” can be applied to our tabulation to reduce its runtime to O.jwj4 /. 4 Adding Features 1 are both push computations with strictly fewer transitions than . Suppose that the last transition in is ra. In this case, ˇ.c/ D ˇk for some i < k < j , .c/ D .c0 /jh with h < k, ˇ.cm 1 / D ˇj , and .cm 1 / D  .c0 /jhjh0 for some k  h0 < j . By induction, we may assume that we have generated items Œi; h; k and Œk; h0 ; j . Applying the inference 676 The main goal with the tabulation of transition-based dependency parsers is to obta"
P11-1068,P08-1110,1,0.950177,"s: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our algorithms, and in particular the conditions under which the feature models traditionally used in transition-based dependency parsing can be integrated into our framework. While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). We instead simulate computations as in Lang (1974), which results in simpler algorithm specifications, and also reveals deep similarities bet"
P11-1068,J99-4004,0,0.0177162,"ming algorithms, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic progra"
P11-1068,P10-1110,0,0.0834865,"ons in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper"
P11-1068,D09-1127,0,0.0208373,"Missing"
P11-1068,D09-1005,0,0.0163563,", also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing. When applied to formalisms such as context-free grammar, they provide polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a"
P11-1068,W03-3017,0,0.0554594,"s that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of featur"
P11-1068,W04-0308,0,0.783918,"Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008"
P11-1068,J08-4003,0,0.774527,"polynomial-time parsing algorithms and polynomial-space representations of the resulting parse forests, even in cases where the size of the search space is exponential in the length of the input string. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). The basic idea, originally developed in the context of push-down automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989), is that while the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time"
P11-1068,W03-3023,0,0.0736894,"stem takes space O.jwj2 / and time O.jwj3 /. In the original interpretation of the deduction system, an item Œi; j  asserts the existence of a pair of (projective) dependency trees: the first tree rooted at token wi , having all nodes in the substring wi    wk 1 as descendants, where i < k  j ; and the second tree rooted at token wj , having all nodes in the substring wk    wj as descendants. (Note that we use fencepost indexes, while Gómez-Rodríguez et al. (2008) indexes positions.) Deduction System Gómez-Rodríguez et al. (2008) present a deductive version of the dependency parser of Yamada and Matsumoto (2003); their deduction system is given in Fig680 .ji jj; ˇ; A/ ` .ji; ˇ; A [ fi ! j g/ .ra/ We call this transition system the hybrid model, as sh and ra are just like in arc-standard, while lah is like the Left-Arc transition in the arc-eager model (lae ), except that it does not have the precondition. Like the arc-standard but unlike the arc-eager model, the hybrid model builds dependencies bottom-up. 7 Conclusion In this paper, we have provided a general technique for the tabulation of transition-based dependency parsers, and applied it to obtain dynamic programming algorithms for two widely-u"
P11-1068,D08-1059,0,0.415993,"the number of computations of a transition-based parser may be exponential in the length of the input string, several portions of these computations, when appropriately represented, can be shared. This can be effectively implemented through dynamic programming, resulting in a packed representation of the set of all computations. The contributions of this paper can be summarized as follows. We provide (declarative specifications of) novel, polynomial-time algorithms for two widelyused transition-based parsing models: arc-standard (Nivre, 2004; Huang and Sagae, 2010) and arc-eager (Nivre, 2003; Zhang and Clark, 2008). Our algorithm for the arc-eager model is the first tabular algorithm for this model that runs in polynomial time. Both algorithms are derived using the same general technique; in fact, we show that this technique is applicable to all transition-parsing models whose transitions can be classified into “shift” and “reduce” transitions. We also show how to reverse the tabulation to derive a new transition system from an existing tabular algorithm for dependency parsing, originally developed by Gómez-Rodríguez et al. (2008). Finally, we discuss in detail the role of feature information in our alg"
P15-2042,W06-2922,0,0.2899,"that configuration. While it is always easy to do this in exponential time by simulating all possible computations in the algorithm to obtain all reachable trees, it is not always clear how to achieve this calculation in polynomial time. At the moment, this problem has been solved for several projective parsers exploiting either arc-decomposability (Goldberg and Nivre, 2013) or tabularization of computations (Goldberg et al., 2014). However, for parsers that can handle crossing arcs, the only known dynamic oracle (G´omez-Rodr´ıguez et al., 2014) has been defined for a variant of the parser by Attardi (2006) that supports a restricted set of nonprojective trees. To our knowledge, no dynamic oracles are known for any transition-based parser that can handle unrestricted non-projectivity. In this paper, we define such an oracle for the Covington non-projective parser (Covington, 2001; Nivre, 2008), which can handle arbitrary non-projective dependency trees. As this algorithm is not arc-decomposable and its tabularization is NP-hard (Neuhaus and Br¨oker, 1997), we do not use the existing techniques to define dynamic oracles, but a reasoning specific to this parser. It is worth noting that, apart from"
P15-2042,W03-3017,0,0.494837,"on for Computational Linguistics (2014) has O(n8 ) time complexity. The rest of the paper is organized as follows: after a quick outline of Covington’s parser in Sect. 2, we present the oracle and prove its correctness in Sect. 3. Experiments are reported in Sect. 4, and Sect. 5 contains concluding remarks. 2 the resulting configuration will be j and j + 1. The result is a parser that can generate any possible dependency tree for the input, and runs in quadratic worst-case time. Although in theory this complexity can seem like a drawback compared to linear-time transition-based parsers (e.g. (Nivre, 2003; G´omez-Rodr´ıguez and Nivre, 2013)), it has been shown by Volokh and Neumann (2012) to actually outperform linear algorithms in practice, as it allows for relevant optimizations in feature extraction that cannot be implemented in other parsers. In fact, one of the fastest dependency parsers to date uses this algorithm (Volokh, 2013). Preliminaries We will define a dynamic oracle for the nonprojective parser originally defined by Covington (2001), and implemented by Nivre (2008) under the transition-based parsing framework. For space reasons, we only sketch the parser very briefly, and refer"
P15-2042,W06-2920,0,0.060798,"connected in constant time. On the other hand, a more efficient implementation than the one shown in Algorithm 1 (which we chose for clarity) can be achieved by more tightly coupling the oracle to the parser, as the relevant sets of arcs associated with a configuration can be obtained incrementally from those of the previous configuration. 4 Experiments To evaluate the performance of our approach, we conduct experiments on both static and dynamic Covington non-projective oracles. Concretely, we train an averaged perceptron model for 15 iterations on nine datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and all data259 Unigrams L0 w; L0 p; L0 wp; L0 l; L0h w; L0h p; L0h l; L0l0 w; L0l0 p; L0l0 l; L0r0 w; L0r0 p; L0r0 l; L0h2 w; L0h2 p; L0h2 l; L0l w; L0l p; L0l l; L0r w; L0r p; L0r l; L0 wd; L0 pd; L0 wvr ; L0 pvr ; L0 wvl ; L0 pvl ; L0 wsl ; L0 psl ; L0 wsr ; L0 psr ; L1 w; L1 p; L1 wp; R0 w; R0 p; R0 wp; R0l0 w; R0l0 p; R0l0 l; R0l w; R0l p; R0l l; R0 wd; R0 pd; R0 wvl ; R0 pvl ;R0 wsl ; R0 psl ; R1 w; R1 p; R1 wp; R2 w; R2 p; R2 wp; CLw; CLp; CLwp; CRw; CRp; CRwp; Pairs L0 wp+R0 wp; L0 wp+R0 w; L0 w+R0 wp; L0 wp+R0 p; L0 p+R0 wp; L0 w+R0 w; L0 p+R0 p;R0 p+R1 p; L0 w+R0 wd; L0 p+R0 pd; Tri"
P15-2042,J08-4003,0,0.76358,"projective parsers exploiting either arc-decomposability (Goldberg and Nivre, 2013) or tabularization of computations (Goldberg et al., 2014). However, for parsers that can handle crossing arcs, the only known dynamic oracle (G´omez-Rodr´ıguez et al., 2014) has been defined for a variant of the parser by Attardi (2006) that supports a restricted set of nonprojective trees. To our knowledge, no dynamic oracles are known for any transition-based parser that can handle unrestricted non-projectivity. In this paper, we define such an oracle for the Covington non-projective parser (Covington, 2001; Nivre, 2008), which can handle arbitrary non-projective dependency trees. As this algorithm is not arc-decomposable and its tabularization is NP-hard (Neuhaus and Br¨oker, 1997), we do not use the existing techniques to define dynamic oracles, but a reasoning specific to this parser. It is worth noting that, apart from being the first dynamic oracle supporting unrestricted nonprojectivity, our oracle is very efficient, solving the loss calculation in O(n). In contrast, the restricted non-projective oracle of G´omez-Rodr´ıguez et al. We define a dynamic oracle for the Covington non-projective dependency pa"
P15-2042,P13-1104,0,0.0959804,"Missing"
P15-2042,C12-1059,0,0.206802,"to choose the best transition to take at each state (Nivre, 2008). While this kind of parsers have become very popular, as they achieve competitive accuracy with especially fast parsing times; their raw accuracy is still behind that of slower alternatives like transitionbased parsers that use beam search (Zhang and Nivre, 2011; Choi and McCallum, 2013). For this reason, a current research challenge is to improve the accuracy of greedy transition-based parsers as much as possible without sacrificing efficiency. A relevant recent advance in this direction is the introduction of dynamic oracles (Goldberg and Nivre, 2012), an improvement in the training procedure of greedy parsers that can boost their accuracy without any impact on parsing speed. An oracle is a training component that selects the best transition(s) to take at a given configuration, using knowledge about the gold tree. Traditionally, transition-based parsers were trained to follow a so-called static oracle, which is only defined on the configurations of a canonical computation that generates the gold tree, returning the next transition in said computation. In contrast, dynamic 256 Proceedings of the 53rd Annual Meeting of the Association for Co"
P15-2042,C00-2137,0,0.128802,"61 83.66 79.91 82.38 76.15 88.48∗ 85.32∗ 84.98∗ 80.85∗ 81.17∗ 78.54∗ 87.47∗ 85.15∗ 93.79 92.42 86.23 83.27 76.76∗ 70.35∗ 79.87∗ 76.97∗ 86.66 81.21 83.52 79.13 Table 2: Parsing accuracy (UAS and LAS, including punctuation) of Covington non-projective parser with static (s-Covington) and dynamic (dCovington) oracles on CoNLL-XI (first block) and CoNLL-X (second block) datasets. For each language, we run five experiments with the same setup but different seeds and report the averaged accuracy. Best results for each language are shown in boldface. Statistically significant improvements (α = .05) (Yeh, 2000) are marked with ∗ . age points in UAS and 0.71 in LAS, while our approach achieves 0.80 in UAS and 0.74 in LAS. sets from the CoNLL-XI shared task (Nivre et al., 2007). We use the same feature templates for all languages, which result from adapting the features described by Zhang and Nivre (2011) to the data structures of the Covington non-projective parser, and are listed in detail in Table 1. Table 2 reports the accuracy obtained by the Covington non-projective parser with both oracles. As we can see, the dynamic oracle implemented in the Covington algorithm improves over the accuracy of th"
P15-2042,Q13-1033,0,0.519962,"her parsing accuracy. However, defining a usable dynamic oracle for a given parser is non-trivial in general, due to the need of calculating the loss of each configuration, i.e., the minimum Hamming loss to the gold tree from a tree reachable from that configuration. While it is always easy to do this in exponential time by simulating all possible computations in the algorithm to obtain all reachable trees, it is not always clear how to achieve this calculation in polynomial time. At the moment, this problem has been solved for several projective parsers exploiting either arc-decomposability (Goldberg and Nivre, 2013) or tabularization of computations (Goldberg et al., 2014). However, for parsers that can handle crossing arcs, the only known dynamic oracle (G´omez-Rodr´ıguez et al., 2014) has been defined for a variant of the parser by Attardi (2006) that supports a restricted set of nonprojective trees. To our knowledge, no dynamic oracles are known for any transition-based parser that can handle unrestricted non-projectivity. In this paper, we define such an oracle for the Covington non-projective parser (Covington, 2001; Nivre, 2008), which can handle arbitrary non-projective dependency trees. As this a"
P15-2042,P11-2033,0,0.292811,"cle significantly improves parsing accuracy over the static oracle baseline on a wide range of treebanks. 1 Introduction Greedy transition-based dependency parsers build analyses for sentences incrementally by following a sequence of transitions defined by an automaton, using a scoring model to choose the best transition to take at each state (Nivre, 2008). While this kind of parsers have become very popular, as they achieve competitive accuracy with especially fast parsing times; their raw accuracy is still behind that of slower alternatives like transitionbased parsers that use beam search (Zhang and Nivre, 2011; Choi and McCallum, 2013). For this reason, a current research challenge is to improve the accuracy of greedy transition-based parsers as much as possible without sacrificing efficiency. A relevant recent advance in this direction is the introduction of dynamic oracles (Goldberg and Nivre, 2012), an improvement in the training procedure of greedy parsers that can boost their accuracy without any impact on parsing speed. An oracle is a training component that selects the best transition(s) to take at a given configuration, using knowledge about the gold tree. Traditionally, transition-based pa"
P15-2042,Q14-1010,0,0.451079,"le for a given parser is non-trivial in general, due to the need of calculating the loss of each configuration, i.e., the minimum Hamming loss to the gold tree from a tree reachable from that configuration. While it is always easy to do this in exponential time by simulating all possible computations in the algorithm to obtain all reachable trees, it is not always clear how to achieve this calculation in polynomial time. At the moment, this problem has been solved for several projective parsers exploiting either arc-decomposability (Goldberg and Nivre, 2013) or tabularization of computations (Goldberg et al., 2014). However, for parsers that can handle crossing arcs, the only known dynamic oracle (G´omez-Rodr´ıguez et al., 2014) has been defined for a variant of the parser by Attardi (2006) that supports a restricted set of nonprojective trees. To our knowledge, no dynamic oracles are known for any transition-based parser that can handle unrestricted non-projectivity. In this paper, we define such an oracle for the Covington non-projective parser (Covington, 2001; Nivre, 2008), which can handle arbitrary non-projective dependency trees. As this algorithm is not arc-decomposable and its tabularization is"
P15-2042,J13-4002,1,0.916934,"Missing"
P15-2042,D14-1099,1,0.645916,"Missing"
P15-2042,P97-1043,0,0.0318001,"Missing"
P15-2042,D07-1096,0,\N,Missing
P16-2069,E12-2012,0,0.028869,"the corpora. For the bilingual models, for each pair of languages L1 , L2 ; we simply merged their training sets into a single file acting as a training set for L1 ∪L2 , and we did the same for the development sets. The test sets were not merged because comparing the bilingual parsers to monolingual ones requires evaluating each bilingual parser on the two corresponding monolingual test sets. To build the models, we relied on MaltParser (Nivre et al., 2007). Due to the large number of language pairs that complicates manual optimization, and to ensure a fair comparison, we used MaltOptimizer (Ballesteros and Nivre, 2012), an automatic optimizer for MaltParser models. This system works in three phases: Phase 1 and 2 choose a parsing algorithm by analyzing the training set, and performing experiments with default features. Phase 3 tunes the feature model and algorithm parameters. We hypothesize that the bilingual models will learn a set of features that fits both languages, and check this hypothesis by evaluating on the test sets. We propose two training configurations: (1) a treebank-dependent tags configuration where we include the information in the POSTAG column and (2) a universal tags only configuration,"
P16-2069,C08-1015,0,0.0398924,"Missing"
P16-2069,I11-1172,0,0.0549795,"Missing"
P16-2069,W14-4606,0,0.0883369,"Missing"
P16-2069,D11-1006,0,0.247659,"Missing"
P16-2069,P13-2017,0,0.180375,"Missing"
P16-2069,P12-1066,0,0.204635,"tent in different languages has been discussed recently (Dang et al., 2014), and multilingual dependency parsing is no stranger to this challenge. Datadriven parsing models (Nivre, 2006) can be trained for any language, given enough annotated data. On languages where treebanks are not available, cross-lingual transfer can be used to train parsers for a target language with data from one or more source languages. Data transfer approaches (e.g. Yarowsky et al. (2001), Tiedemann (2014)) map linguistic annotations across languages through parallel corpora. Instead, model transfer approaches (e.g. Naseem et al. (2012)) rely on crosslinguistic syntactic regularities to learn aspects of the source language that help parse an unseen language, without parallel corpora. 2 Bilingual training Universal Dependency Treebanks v2.0 (McDonald et al., 2013) is a set of CoNLL-formatted treebanks for ten languages, annotated with common criteria. They include two versions of PoS tags: universal tags (Petrov et al., 2011) in the CPOSTAG column, and a refined annotation with treebankspecific information in the POSTAG column. Some of the latter tags are not part of the core universal set, and they can denote linguistic phen"
P16-2069,J08-4003,0,0.0316491,", and they can denote linguistic phenomena that are language-specific, or phenomena that not all the corpora have annotated in the same way. To train monolingual parsers (our baseline), we used the official training-dev-set splits provided 425 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 425–431, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics the corresponding monolingual models. In view of this, and as it is known that different parsing algorithms can be more or less competitive depending on the language (Nivre, 2008), we ran a control experiment to evaluate the models setting the same parsing algorithm for all cases, executing only phase 3 of MaltOptimizer. We chose the arc-eager parser for this experiment, as it was the algorithm that MaltOptimizer chose most frequently for the monolingual models in the previous configuration. The aim was to compare the accuracy of the bilingual models with respect to the monolingual ones, when there is no variation on the parsing algorithm between them. The results of this control experiment are not shown for space reasons, but they were very similar to those of the ori"
P16-2069,W04-3207,0,0.0411882,"8 Figure 1: Example with the en, es, en-es models. Dotted lines represent incorrectly-parsed dependencies. The corresponding English sentence is: ‘We are working hard on putting available the best products of Spain, thank you’ Tagger en es en-es en-es en-es Parser en es en es en-es LAS 37.82 27.56 66.03 67.95 87.18 UAS 44.23 41.03 78.85 77.56 92.31 4 Conclusions and future work To our knowledge, this is the first attempt to train purely bilingual parsers to analyze sentences irrespective of which of the two languages they are written in; as existing work on training a parser on two languages (Smith and Smith, 2004) focused on using parallel corpora to transfer linguistic knowledge between languages. Our results reflect that bilingual parsers do not lose accuracy with respect to monolingual parsers on their corresponding language, and can even outperform them, especially if fine-grained tags are used. This shows that, thanks to universal dependencies and shared syntactic structures across different languages, using treebank-dependent tag sets is not a drawback, but even an advantage. The applications include parsing sentences of different languages with a single model, improving the accuracy of monolingu"
P16-2069,C14-1175,0,0.0445216,"promising on texts with code-switching and when more languages are added. 1 Introduction The need of frameworks for analyzing content in different languages has been discussed recently (Dang et al., 2014), and multilingual dependency parsing is no stranger to this challenge. Datadriven parsing models (Nivre, 2006) can be trained for any language, given enough annotated data. On languages where treebanks are not available, cross-lingual transfer can be used to train parsers for a target language with data from one or more source languages. Data transfer approaches (e.g. Yarowsky et al. (2001), Tiedemann (2014)) map linguistic annotations across languages through parallel corpora. Instead, model transfer approaches (e.g. Naseem et al. (2012)) rely on crosslinguistic syntactic regularities to learn aspects of the source language that help parse an unseen language, without parallel corpora. 2 Bilingual training Universal Dependency Treebanks v2.0 (McDonald et al., 2013) is a set of CoNLL-formatted treebanks for ten languages, annotated with common criteria. They include two versions of PoS tags: universal tags (Petrov et al., 2011) in the CPOSTAG column, and a refined annotation with treebankspecific"
P16-2069,W00-1308,0,0.169691,"Missing"
P16-2069,H01-1035,0,0.056421,"show the approach to be promising on texts with code-switching and when more languages are added. 1 Introduction The need of frameworks for analyzing content in different languages has been discussed recently (Dang et al., 2014), and multilingual dependency parsing is no stranger to this challenge. Datadriven parsing models (Nivre, 2006) can be trained for any language, given enough annotated data. On languages where treebanks are not available, cross-lingual transfer can be used to train parsers for a target language with data from one or more source languages. Data transfer approaches (e.g. Yarowsky et al. (2001), Tiedemann (2014)) map linguistic annotations across languages through parallel corpora. Instead, model transfer approaches (e.g. Naseem et al. (2012)) rely on crosslinguistic syntactic regularities to learn aspects of the source language that help parse an unseen language, without parallel corpora. 2 Bilingual training Universal Dependency Treebanks v2.0 (McDonald et al., 2013) is a set of CoNLL-formatted treebanks for ten languages, annotated with common criteria. They include two versions of PoS tags: universal tags (Petrov et al., 2011) in the CPOSTAG column, and a refined annotation with"
P16-2069,I08-3008,0,0.147708,"Missing"
P16-2069,zeman-etal-2012-hamledt,0,0.0292122,"Missing"
P16-2069,petrov-etal-2012-universal,0,\N,Missing
P16-2069,P07-1033,0,\N,Missing
P17-1027,W06-2920,0,0.448293,"Missing"
P17-1027,W13-3518,0,0.237842,"´ıguez Universidade da Coru˜na FASTPARSE Lab, LyS Research Group, Departamento de Computaci´on Campus de Elvi˜na, s/n, 15071 A Coru˜na, Spain d.fgonzalez@udc.es, carlos.gomez@udc.es Abstract in an incorrect and unknown configuration, causing more mistakes in the rest of the transition sequence. Training with a dynamic oracle (Goldberg and Nivre, 2012) improves robustness in these situations, but in a monotonic transition system, erroneous decisions made in the past are permanent, even when the availability of further information in later states might be useful to correct them. Honnibal et al. (2013) show that allowing some degree of non-monotonicity, by using a limited set of non-monotonic actions that can repair past mistakes and replace previously-built arcs, can increase the accuracy of a transition-based parser. In particular, they present a modified arc-eager transition system where the Left-Arc and Reduce transitions are non-monotonic: the former is used to repair invalid attachments made in previous states by replacing them with a leftward arc, and the latter allows the parser to link two words with a rightward arc that were previously left unattached due to an erroneous decision."
P17-1027,D14-1082,0,0.1665,"Missing"
P17-1027,D15-1162,0,0.073387,"eager parser and rendered inaccessible, this approach can only repair certain kinds of mistakes: namely, it can fix erroneous rightward arcs by replacing them with a leftward arc, and connect a limited set of unattached words with rightward arcs. In addition, they argue that non-monotonicity in the training oracle can be harmful for the final accuracy and, therefore, they suggest to apply it only as a fallback component for a monotonic oracle, which is given priority over the non-monotonic one. Thus, this strategy will follow the path dictated by the monotonic oracle the majority of the time. Honnibal and Johnson (2015) present an extension of this transition system with an Unshift transition allowing it some extra flexibility to correct past errors. However, the restriction that only rightward Restricted non-monotonicity has been shown beneficial for the projective arceager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the"
P17-1027,P15-1033,0,0.118777,"Missing"
P17-1027,D07-1013,0,0.109978,"different NLP tasks due to their speed and efficiency. They parse a sentence from left to right by greedily choosing the highestscoring transition to go from the current parser configuration or state to the next. The resulting sequence of transitions incrementally builds a parse for the input sentence. The scoring of the transitions is provided by a statistical model, previously trained to approximate an oracle, a function that selects the needed transitions to parse a gold tree. Unfortunately, the greedy nature that grants these parsers their efficiency also represents their main limitation. McDonald and Nivre (2007) show that greedy transition-based parsers lose accuracy to error propagation: a transition erroneously chosen by the greedy parser can place it 288 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 288–298 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1027 arcs can be deleted, and only by replacing them with leftward arcs, is still in place. Furthermore, both versions of the algorithm are limited to projective trees. In this paper, we propose a non-monotonic transition"
P17-1027,C12-1059,0,0.0253952,"esize that, even if it were feasible to build an oracle with the exact loss, it would not provide practical improvements over these approximate oracles; as it appears difficult for a statistical model to learn the situations where replacing a wrong arc with another indirectly helps due to breaking prospective cycles. Comparison In order to provide a broader contextualization of our approach, Table 4 presents a comparison of the average accuracy and parsing speed obtained by some well-known transition-based systems with dynamic oracles. Concretely, we include in this comparison both monotonic (Goldberg and Nivre, 2012) and non-monotonic (Honnibal et al., 2013) versions of the arc-eager parser, as well as the original monotonic Covington system (G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). The three of them were ran with our own implementation so the comparison is homogeneous. We also report the published accuracy of the non-projective Attardi algorithm (G´omezRodr´ıguez et al., 2014) on the nineteen datasets used in our experiments. From Table 4 we can see that our approach achieves the best average UAS score, but is slightly slower at parsing time than the monotonic Covington algorithm. This can be e"
P17-1027,W03-3017,0,0.221337,"one incoming arc) or the acyclicity constraint (the dependency graph cannot have cycles). After applying any of these three transitions, i is moved to the second list to make i − 1 and j the focus words for the next step. As an alternative, we can instead choose to execute a Shift transition which lets the parser read a new input word, placing the focus on j and j + 1. The resulting parser can generate any possible dependency tree for the input, including arbitrary non-projective trees. While it runs in quadratic worst-case time, in theory worse than lineartime transition-based parsers (e.g. (Nivre, 2003; G´omez-Rodr´ıguez and Nivre, 2013)), it has been shown to outspeed linear algorithms in practice, thanks to feature extraction optimizations that cannot be implemented in other parsers (Volokh and Neumann, 2012). In fact, one of the fastest dependency parsers ever reported uses this algorithm Preliminaries Non-Projective Covington Transition System The non-projective Covington parser was originally defined by Covington (2001), and then recast by Nivre (2008) under the transition-based parsing framework. 1 The only restriction is that parsing must still proceed in left-to-right order. For thi"
P17-1027,Q13-1033,0,0.105543,"intly reachable due to the acyclicity constraint. In spite of this, they prove that a dynamic oracle for the Covington parser can be efficiently built by counting individually unreachable arcs, and correcting for the presence of such cycles. Concretely, the loss is computed as: (Volokh, 2013). 2.2 Monotonic Dynamic Oracle A dynamic oracle is a function that maps a configuration c and a gold tree tG to the set of transitions that can be applied in c and lead to some parse tree t minimizing the Hamming loss with respect to tG (the amount of nodes whose head is different in t and tG ). Following Goldberg and Nivre (2013), we say that an arc set A is reachable from configuration c, and we write c A, if there is some (possibly empty) path of transitions from c to some configuration c0 = hλ1 , λ2 , B, A0 i, with A ⊆ A0 . Then, we can define the loss of configuration c as `(c) = |U(c, tG ) |+ nc (A ∪ I(c, tG )) where I(c, tG ) = {x → y ∈ tG |c (x → y)} is the set of individually reachable arcs of tG from configuration c; U(c, tG ) is the set of individually unreachable arcs of tG from c, computed as tG I(c, tG ); and nc (G) denotes the number of cycles in a graph G. `(c) = min L(t, tG ), t|c t Therefore, to calc"
P17-1027,P15-2042,1,0.910008,"Missing"
P17-1027,J13-4002,1,0.904846,"Missing"
P17-1027,C00-2137,0,0.264298,"Missing"
P17-1160,E03-1036,0,0.124827,"Missing"
P17-1160,C96-1058,0,0.127415,"Missing"
P17-1160,P99-1059,0,0.704322,"es are separated purely on the basis of forbidden patterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms: one algorithm works for all when the search space can be controlled in parser input. 1 Introduction Dependency parsing has received wide attention in recent years, as accurate and efficient dependency parsers have appeared that are applicable to many languages. Traditionally, dependency parsers have produced syntactic analyses in tree form, including exact inference algorithms that search for maximum projective trees (Eisner and Satta, 1999) and maximum spanning trees (McDonald et al., 2005) in weighted digraphs, as well as greedy and beamsearch approaches that forgo exact search for extra efficiency (Zhang and Nivre, 2011). Recently, there has been growing interest in providing a richer analysis of natural language by going beyond trees. In semantic dependency parsing (Oepen et al., 2015; Kuhlmann and Oepen, 2016), the desired syntactic representations can have indegree greater than 1 (re-entrancy), suggesting the search for maximum acyclic subgraphs (Schluter, 2014, 2015). As this inference task is intractable (Guruswami et al."
P17-1160,J11-3004,1,0.899472,"Missing"
P17-1160,E09-2008,0,0.0339008,"dges all brackets for inverted edges [-,/-brackets ]-,&gt;-brackets {, } / }, &gt;., ., ]. R reaching F,Q,I,A Validation Experiments The current experiments were designed (1) to help in developing the components of Reglat and the constraint languages of axiomatic properties, (2) to validate the representation, the constraint languages and their unambiguity, (3) to learn about the ontology and (4) to sample the integer sequences associated with the cardinality of each family in the ontology. Finding the Components Representations of Reglat were built with scripts written using a finitestate toolkit (Hulden, 2009) that supports rapid exploration with regular languages and transducers. Validation of Languages Our scripts presented alternative approaches to compute languages of encoded digraphs with n vertices up to n = 9. We also implemented a Python script that enumerated elements of families of graphs up to n = 6. The solutions were used to cross-validate one another. ∗ ∗ The constraint Gn = B ({}B )n−1 ensures nvertices in encoded digraphs. The finite set of encoded acyclic 5-vertex digraphs was computed with a finite-state approach (Yli-Jyr¨a et al., 2012) that takes the input projection of the comp"
P17-1160,J94-3001,0,0.728325,"merated elements of families of graphs up to n = 6. The solutions were used to cross-validate one another. ∗ ∗ The constraint Gn = B ({}B )n−1 ensures nvertices in encoded digraphs. The finite set of encoded acyclic 5-vertex digraphs was computed with a finite-state approach (Yli-Jyr¨a et al., 2012) that takes the input projection of the composition Id(Reglat ∩AD ∩G5 )◦T55 ◦T55 ◦T55 ◦T55 ◦T55 ◦ Id(ε) where Id defines an identity relation and transducer T55 eliminates matching adjacent brackets. This composition differs from the typical use where the purpose is to construct a regular relation (Kaplan and Kay, 1994) or its output projection (Roche, 1996; Oflazer, 2003). For digraphs with a lot of vertices, we had an option to employ a dynamic programming scheme (Yli-Jyr¨a, 2012) that uses weighted transducers. Building the Ontology To build the ontology in Figure 1 we first found out which combinations of digraph properties co-occur to define distinguishable families of digraphs. After the nodes of the 1750 lattice were found, we were able to see the partial order between these. Integer Sequences We sampled, for important families of digraphs, the prefixes of their related integer sequences. We found out"
P17-1160,Q15-1040,0,0.622236,"igraphs, as well as greedy and beamsearch approaches that forgo exact search for extra efficiency (Zhang and Nivre, 2011). Recently, there has been growing interest in providing a richer analysis of natural language by going beyond trees. In semantic dependency parsing (Oepen et al., 2015; Kuhlmann and Oepen, 2016), the desired syntactic representations can have indegree greater than 1 (re-entrancy), suggesting the search for maximum acyclic subgraphs (Schluter, 2014, 2015). As this inference task is intractable (Guruswami et al., 2011), noncrossing digraphs have been studied instead, e.g. by Kuhlmann and Johnsson (2015) who provide a O(n3 ) parser for maximum noncrossing acyclic subgraphs. Carlos G´omez-Rodr´ıguez Universidade da Coru˜na, Spain carlos.gomez@udc.es Yli-Jyr¨a (2005) studied how to axiomatize dependency trees as a special case of noncrossing digraphs. This gave rise to a new homomorphic representation of context-free languages that proves the classical Chomsky and Sch¨utzenberger theorem using a quite different internal language. In this language, the brackets indicate arcs in a dependency tree in a way that is reminiscent to encoding schemes used earlier by Greibach (1973) and Oflazer (2003)."
P17-1160,H05-1066,0,0.694209,"tterns in latent encoding, eliminating the need to differentiate the families of non-crossing graphs in inference algorithms: one algorithm works for all when the search space can be controlled in parser input. 1 Introduction Dependency parsing has received wide attention in recent years, as accurate and efficient dependency parsers have appeared that are applicable to many languages. Traditionally, dependency parsers have produced syntactic analyses in tree form, including exact inference algorithms that search for maximum projective trees (Eisner and Satta, 1999) and maximum spanning trees (McDonald et al., 2005) in weighted digraphs, as well as greedy and beamsearch approaches that forgo exact search for extra efficiency (Zhang and Nivre, 2011). Recently, there has been growing interest in providing a richer analysis of natural language by going beyond trees. In semantic dependency parsing (Oepen et al., 2015; Kuhlmann and Oepen, 2016), the desired syntactic representations can have indegree greater than 1 (re-entrancy), suggesting the search for maximum acyclic subgraphs (Schluter, 2014, 2015). As this inference task is intractable (Guruswami et al., 2011), noncrossing digraphs have been studied ins"
P17-1160,W03-3016,0,0.316708,"¨a (2005) studied how to axiomatize dependency trees as a special case of noncrossing digraphs. This gave rise to a new homomorphic representation of context-free languages that proves the classical Chomsky and Sch¨utzenberger theorem using a quite different internal language. In this language, the brackets indicate arcs in a dependency tree in a way that is reminiscent to encoding schemes used earlier by Greibach (1973) and Oflazer (2003). Cubic-time parsing algorithms that are incidentally or intentionally applicable to this kind of homomorphic representations have been considered, e.g., by Nederhof and Satta (2003), Hulden (2011), and Yli-Jyr¨a (2012). Extending these insights to arbitrary noncrossing digraphs, or to relevant families of them, is far from obvious. In this paper, we develop (1) a linear encoding supporting general noncrossing digraphs, and (2) show that the encoded noncrossing digraphs form a context-free language. We then give it (3) two homomorphic, nonderivative representations and use the latent local features of the latter to characterize various families of digraphs. Apart from the obvious relevance to the theory of context-free languages, this contribution has the practical potent"
P17-1160,S15-2153,0,0.177622,"eedy and beamsearch approaches that forgo exact search for extra efficiency (Zhang and Nivre, 2011). Recently, there has been growing interest in providing a richer analysis of natural language by going beyond trees. In semantic dependency parsing (Oepen et al., 2015; Kuhlmann and Oepen, 2016), the desired syntactic representations can have indegree greater than 1 (re-entrancy), suggesting the search for maximum acyclic subgraphs (Schluter, 2014, 2015). As this inference task is intractable (Guruswami et al., 2011), noncrossing digraphs have been studied instead, e.g. by Kuhlmann and Johnsson (2015) who provide a O(n3 ) parser for maximum noncrossing acyclic subgraphs. Carlos G´omez-Rodr´ıguez Universidade da Coru˜na, Spain carlos.gomez@udc.es Yli-Jyr¨a (2005) studied how to axiomatize dependency trees as a special case of noncrossing digraphs. This gave rise to a new homomorphic representation of context-free languages that proves the classical Chomsky and Sch¨utzenberger theorem using a quite different internal language. In this language, the brackets indicate arcs in a dependency tree in a way that is reminiscent to encoding schemes used earlier by Greibach (1973) and Oflazer (2003)."
P17-1160,J03-4001,0,0.715233,"Johnsson (2015) who provide a O(n3 ) parser for maximum noncrossing acyclic subgraphs. Carlos G´omez-Rodr´ıguez Universidade da Coru˜na, Spain carlos.gomez@udc.es Yli-Jyr¨a (2005) studied how to axiomatize dependency trees as a special case of noncrossing digraphs. This gave rise to a new homomorphic representation of context-free languages that proves the classical Chomsky and Sch¨utzenberger theorem using a quite different internal language. In this language, the brackets indicate arcs in a dependency tree in a way that is reminiscent to encoding schemes used earlier by Greibach (1973) and Oflazer (2003). Cubic-time parsing algorithms that are incidentally or intentionally applicable to this kind of homomorphic representations have been considered, e.g., by Nederhof and Satta (2003), Hulden (2011), and Yli-Jyr¨a (2012). Extending these insights to arbitrary noncrossing digraphs, or to relevant families of them, is far from obvious. In this paper, we develop (1) a linear encoding supporting general noncrossing digraphs, and (2) show that the encoded noncrossing digraphs form a context-free language. We then give it (3) two homomorphic, nonderivative representations and use the latent local fea"
P17-1160,Q13-1002,0,0.0498521,"ncoding and allow inference of best dependency graphs in a family simply by intersection with a weighted CFG grammar for a Dyck language that models position-indexed edges of the complete digraph. Since the families of digraphs are distinguished by forbidden local patterns, the choice of search space can be made purely on lexical grounds, blending well with lexicalized parsing and allowing possibilities such as choosing, per each word, what kind of structures the word can go with. Future work We are planning to extend the coverage of the approach by exploring 1-endpointcrossing and MHk trees (Pitler et al., 2013; G´omez-Rodr´ıguez, 2016), and related digraphs — see (Yli-Jyr¨a, 2004; G´omez-Rodr´ıguez et al., 2011). Properties such as weakly projective, out, and strongly unambiguous prompt further study. An interesting avenue for future work is to explore higher order factorizations for noncrossing digraphs and the related inference. We would also like to have more insight on the transformation of MSO definable properties to the current framework and to logspace algorithms. Acknowledgements AYJ has received funding as Research Fellow from the Academy of Finland (dec. No 270354 - A Usable Finite-State"
P17-1160,W14-2412,0,0.0136198,"e algorithms that search for maximum projective trees (Eisner and Satta, 1999) and maximum spanning trees (McDonald et al., 2005) in weighted digraphs, as well as greedy and beamsearch approaches that forgo exact search for extra efficiency (Zhang and Nivre, 2011). Recently, there has been growing interest in providing a richer analysis of natural language by going beyond trees. In semantic dependency parsing (Oepen et al., 2015; Kuhlmann and Oepen, 2016), the desired syntactic representations can have indegree greater than 1 (re-entrancy), suggesting the search for maximum acyclic subgraphs (Schluter, 2014, 2015). As this inference task is intractable (Guruswami et al., 2011), noncrossing digraphs have been studied instead, e.g. by Kuhlmann and Johnsson (2015) who provide a O(n3 ) parser for maximum noncrossing acyclic subgraphs. Carlos G´omez-Rodr´ıguez Universidade da Coru˜na, Spain carlos.gomez@udc.es Yli-Jyr¨a (2005) studied how to axiomatize dependency trees as a special case of noncrossing digraphs. This gave rise to a new homomorphic representation of context-free languages that proves the classical Chomsky and Sch¨utzenberger theorem using a quite different internal language. In this la"
P17-1160,S15-1031,0,0.0238174,"Missing"
P17-1160,W04-1504,1,0.884027,"Missing"
P17-1160,W12-6218,1,0.709175,"Missing"
P17-1160,P11-2033,0,0.141096,"ithm works for all when the search space can be controlled in parser input. 1 Introduction Dependency parsing has received wide attention in recent years, as accurate and efficient dependency parsers have appeared that are applicable to many languages. Traditionally, dependency parsers have produced syntactic analyses in tree form, including exact inference algorithms that search for maximum projective trees (Eisner and Satta, 1999) and maximum spanning trees (McDonald et al., 2005) in weighted digraphs, as well as greedy and beamsearch approaches that forgo exact search for extra efficiency (Zhang and Nivre, 2011). Recently, there has been growing interest in providing a richer analysis of natural language by going beyond trees. In semantic dependency parsing (Oepen et al., 2015; Kuhlmann and Oepen, 2016), the desired syntactic representations can have indegree greater than 1 (re-entrancy), suggesting the search for maximum acyclic subgraphs (Schluter, 2014, 2015). As this inference task is intractable (Guruswami et al., 2011), noncrossing digraphs have been studied instead, e.g. by Kuhlmann and Johnsson (2015) who provide a O(n3 ) parser for maximum noncrossing acyclic subgraphs. Carlos G´omez-Rodr´ıg"
P18-1248,P16-1231,0,0.0412957,"f it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asy"
P18-1248,W06-2922,0,0.549968,"al.’s (2017a) approach to mildly nonprojective parsing in what, to our knowledge, is the first implementation of exact decoding for a non-projective transition-based parser. As in the projective case, a mildly non1 http://universaldependencies.org/ 2664 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2664–2675 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics projective decoder has been known for several years (Cohen et al., 2011), corresponding to a variant of the transition-based parser of Attardi (2006). However, its Opn7 q runtime — or the Opn6 q of a recently introduced improvedcoverage variant (Shi et al., 2018) — is still prohibitively costly in practice. Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of GómezRodríguez et al.’s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn4 q time (Gómez-Rodríguez, 2016). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (Gómez"
P18-1248,P17-1193,0,0.0164891,"ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this is the first practical implementation of exact infe"
P18-1248,D17-1003,0,0.0158959,"ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this is the first practical implementation of exact infe"
P18-1248,D14-1082,0,0.177618,"troduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of co"
P18-1248,D11-1114,1,0.895329,"us heads, such as those deriving from Attardi (2006). 3.2 The MH 4 Transition System Kuhlmann et al. (2011) show how the items of a variant of MH 3 can be given a transition-based interpretation under the “push computation” framework, yielding the arc-hybrid projective transition system. However, such a derivation has not been made for the non-projective case (k ą 3), and the known techniques used to derive previous associations between tabular and transition-based parsers do not seem to be applicable in this case. The specific issue is that the deduction systems of Kuhlmann et al. (2011) and Cohen et al. (2011) have in common that the structure of their derivations is similar to that of a Dyck (or balancedbrackets) language, where steps corresponding to shift transitions are balanced with those corresponding to reduce transitions. This makes it possible to group derivation subtrees, and the transition sequences that they yield, into “push computations” that increase the length of the stack by a constant amount. However, this does not seem possible in MH 4 . Instead, we derive a transition-based interpretation of MH 4 by a generalization of that of MH 3 that departs from push computations. To do so,"
P18-1248,P16-1034,0,0.0367949,"Missing"
P18-1248,P16-2006,0,0.0890398,"Missing"
P18-1248,K17-3002,0,0.0422677,"stantiation of the MH k parser family. This corresponds to the global version of the arc-hybrid transition system (Kuhlmann et al., 2011). We adopt the minimal feature representation ts0 , b0 u, following Shi et al. (2017a). For this model, we also implement a greedy incremental version. The edge-factored non-projective maximal spanning tree (MST) parser allows arbitrary non-projective structures. This decoding approach has been shown to be very competitive in parsing non-projective treebanks (McDonald et al., 2005), and was deployed in the top-performing system at the CoNLL 2017 shared task (Dozat et al., 2017). We score each edge individually, with the features being the bi-LSTM vectors th, mu, where h is the head, and m the modifier of the edge. The crossing-sensitive third-order 1EC parser provides a hybrid dynamic program for parsing 1-Endpoint-Crossing non-projective dependency trees with higher-order factorization (Pitler, 2014). Depending on whether an edge is crossed, we can access the modifier’s grandparent g, head h, and sibling si. We take their corresponding bi-LSTM features tg, h, m, siu for scoring each edge. This is a re-implementation of Pitler (2014) with neural scoring functions. M"
P18-1248,P15-1033,0,0.205709,"ed interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resu"
P18-1248,C96-1058,0,0.711945,"4 -hybrid, 1EC) and combine them into an ensemble. The average result is competitive with the best CoNLL submis6 Related Work Results on Projective Languages (Table 6) For completeness, we also test our models on the 10 most projective languages that have a single treebank. MH 4 remains the most effective, but by a much smaller margin. Interestingly, MH 3 , which is strictly projective, matches the performance of 1EC; both outperform the fully nonprojective MST by half a point. Exact inference for dependency parsing can be achieved in cubic time if the model is restricted to projective trees (Eisner, 1996). However, nonprojectivity is needed for natural language parsers to satisfactorily deal with linguistic phenomena like topicalization, scrambling and extraposition, which cause crossing dependencies. In UD 2.0, 68 out of 70 treebanks were reported to contain 2670 Same Model Architecture MST MH 4 -hybrid Lan. MH 3 eu ur got hu cu da el hi de ro 78.17˘0.33 80.91˘0.10 67.10˘0.10 76.09˘0.25 71.28˘0.29 80.00˘0.15 85.89˘0.29 89.88˘0.18 76.23˘0.21 83.53˘0.35 79.90˘0.08 80.05˘0.13 67.26˘0.45 75.79˘0.36 72.18˘0.20 79.69˘0.24 85.48˘0.25 89.93˘0.12 75.99˘0.23 82.73˘0.36 80.22˘0.48 80.69˘0.19 67.92˘0.29"
P18-1248,P17-1027,1,0.899815,"Missing"
P18-1248,J16-4008,1,0.824047,"iation for Computational Linguistics projective decoder has been known for several years (Cohen et al., 2011), corresponding to a variant of the transition-based parser of Attardi (2006). However, its Opn7 q runtime — or the Opn6 q of a recently introduced improvedcoverage variant (Shi et al., 2018) — is still prohibitively costly in practice. Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of GómezRodríguez et al.’s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn4 q time (Gómez-Rodríguez, 2016). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (GómezRodríguez et al., 2008; Kuhlmann et al., 2011), its own relation to transition-based parsing was not known. Here, we show that MH 4 can be interpreted as exploring a subset of the search space of a transition-based parser that generalizes the arc-hybrid system, under a mapping that differs from the “push computation” paradigm used by the previously-known dynamic-programming decoders for transition systems. This allows us to extend S"
P18-1248,P08-1110,1,0.84519,"Missing"
P18-1248,J11-3004,1,0.88251,"Missing"
P18-1248,E09-1034,1,0.733544,"ency trees that allow only restricted forms of non-projectivity. A number of such sets, called mildly non-projective classes of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et al. (2016) report an implementation of the WG1 parser, a Opn7 q mildly non-projective parser introduced in Gómez-Rodríguez et al. (2009), but it could not be run for real sentences of length greater than 20. On the other hand, Pitler et al. (2012) provide an implementation of an Opn5 q parser for a mildly non-projective class of structures called gap-minding trees, but they need to resort to aggressive pruning to make it practical, exploring only a part of the search space in Opn4 q time. To the best of our knowledge, the only practical system that actually implements exact inference for mildly non-projective parsing is the 1Endpoint-Crossing (1EC) parser of Pitler (2013; 2014), which runs in Opn4 q worst-case time like the MH"
P18-1248,P10-1110,0,0.0782498,"Missing"
P18-1248,P98-1106,0,0.435379,"Missing"
P18-1248,Q16-1023,0,0.0411972,"items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asymptotic runtime complexity to imp"
P18-1248,P11-1068,1,0.80246,"ieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature models: incorporation of complex features resulted in jumps in asymptotic runtime complexity to impractical levels. However, the recent popularization of bidirectional long-short term memory networks (biLSTMs; Hochreiter and Schmidhuber, 1997) to derive feature representations for parsing, given their capacity to capture long-range information, has demonstrated that one may not need to use complex feature models to obtain good accuracy (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016)."
P18-1248,Q17-1031,0,0.0182093,"oting that 1EC has recently been ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this is the first practical implem"
P18-1248,W17-6312,0,0.175491,"usive margin. It is worth noting that 1EC has recently been ex6 Corro et al. (2016) describe a parser that enforces mildly non-projective constraints (bounded block degree and wellnestedness), but it is an arc-factored model, so it is subject to the same strong independence assumptions as maximumspanning-tree parsers like McDonald et al. (2005) and does not support the greater flexibility in scoring that is the main advantage of mildly non-projective parsers over these. Instead, mild non-projectivity is exclusively used as a criterion to discard nonconforming trees. tended to graph parsing by Kurtz and Kuhlmann (2017), Kummerfeld and Klein (2017), and Cao et al. (2017a,b), with the latter providing a practical implementation of a parser for 1-EndpointCrossing, pagenumber-2 graphs. 7 Conclusion We have extended the parsing architecture of Shi et al. (2017a) to non-projective dependency parsing by implementing the MH 4 parser, a mildly non-projective Opn4 q chart parsing algorithm, using a minimal set of transition-based bi-LSTM features. For this purpose, we have established a mapping between MH 4 items and transition sequences of an underlying non-projective transition-based parser. To our knowledge, this"
P18-1248,H05-1066,0,0.538094,"are trained with the cost-augmented large-margin loss function. The MH 3 parser is the projective instantiation of the MH k parser family. This corresponds to the global version of the arc-hybrid transition system (Kuhlmann et al., 2011). We adopt the minimal feature representation ts0 , b0 u, following Shi et al. (2017a). For this model, we also implement a greedy incremental version. The edge-factored non-projective maximal spanning tree (MST) parser allows arbitrary non-projective structures. This decoding approach has been shown to be very competitive in parsing non-projective treebanks (McDonald et al., 2005), and was deployed in the top-performing system at the CoNLL 2017 shared task (Dozat et al., 2017). We score each edge individually, with the features being the bi-LSTM vectors th, mu, where h is the head, and m the modifier of the edge. The crossing-sensitive third-order 1EC parser provides a hybrid dynamic program for parsing 1-Endpoint-Crossing non-projective dependency trees with higher-order factorization (Pitler, 2014). Depending on whether an edge is crossed, we can access the modifier’s grandparent g, head h, and sibling si. We take their corresponding bi-LSTM features tg, h, m, siu fo"
P18-1248,W07-2216,0,0.0933426,"Missing"
P18-1248,E06-1011,0,0.160259,"languages (sorted by projective ratio; ja (Japanese) is fully projective). non-projectivity (Wang et al., 2017). However, exact inference has been shown to be intractable for models that support arbitrary nonprojectivity, except under strong independence assumptions (McDonald and Satta, 2007). Thus, exact inference parsers that support unrestricted non-projectivity are limited to edge-factored models (McDonald et al., 2005; Dozat et al., 2017). Alternatives include treebank transformation and pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), approximate inference (e.g. McDonald and Pereira (2006); Attardi (2006); Nivre (2009); Fernández-González and Gómez-Rodríguez (2017)) or focusing on sets of dependency trees that allow only restricted forms of non-projectivity. A number of such sets, called mildly non-projective classes of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et"
P18-1248,P09-1040,0,0.082074,"panese) is fully projective). non-projectivity (Wang et al., 2017). However, exact inference has been shown to be intractable for models that support arbitrary nonprojectivity, except under strong independence assumptions (McDonald and Satta, 2007). Thus, exact inference parsers that support unrestricted non-projectivity are limited to edge-factored models (McDonald et al., 2005; Dozat et al., 2017). Alternatives include treebank transformation and pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), approximate inference (e.g. McDonald and Pereira (2006); Attardi (2006); Nivre (2009); Fernández-González and Gómez-Rodríguez (2017)) or focusing on sets of dependency trees that allow only restricted forms of non-projectivity. A number of such sets, called mildly non-projective classes of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et al. (2016) report an implemen"
P18-1248,P05-1013,0,0.148399,"Missing"
P18-1248,C04-1010,0,0.0771663,"th minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibilit"
P18-1248,Q14-1004,0,0.562421,"ers from the “push computation” paradigm used by the previously-known dynamic-programming decoders for transition systems. This allows us to extend Shi et al. (2017a)’s work to non-projective parsing, by implementing MH 4 with a minimal set of transition-based features. Experimental results show that our approach outperforms the projective approach of Shi et al. (2017a) and maximum-spanning-tree nonprojective parsing on the most highly nonprojective languages in the CoNLL 2017 sharedtask data that have a single treebank. We also compare with the third-order 1-Endpoint-Crossing (1EC) parser of Pitler (2014), the only other practical implementation of an exact mildly nonprojective decoder that we know of, which also runs in Opn4 q but without a transition-based interpretation. We obtain comparable results for these two algorithms, in spite of the fact that the MH 4 algorithm is notably simpler than 1EC. The MH 4 parser remains effective in parsing projective treebanks, while our baseline parser, the fully non-projective maximum spanning tree algorithm, falls behind due to its unnecessarily large search space in parsing these languages. Our code, including our re-implementation of the third-order"
P18-1248,D12-1044,0,0.0208706,"sses of trees, have been identified that both exhibit good empirical coverage of the non-projective phenomena found in natural languages and are known to have polynomial-time exact parsing algorithms; see Gómez-Rodríguez (2016) for a survey. However, most of these algorithms have not been implemented in practice due to their prohibitive complexity. For example, Corro et al. (2016) report an implementation of the WG1 parser, a Opn7 q mildly non-projective parser introduced in Gómez-Rodríguez et al. (2009), but it could not be run for real sentences of length greater than 20. On the other hand, Pitler et al. (2012) provide an implementation of an Opn5 q parser for a mildly non-projective class of structures called gap-minding trees, but they need to resort to aggressive pruning to make it practical, exploring only a part of the search space in Opn4 q time. To the best of our knowledge, the only practical system that actually implements exact inference for mildly non-projective parsing is the 1Endpoint-Crossing (1EC) parser of Pitler (2013; 2014), which runs in Opn4 q worst-case time like the MH 4 algorithm used in this paper. Thus, the system presented here is the second practical implementation of exac"
P18-1248,Q13-1002,0,0.104846,"Missing"
P18-1248,N15-1068,0,0.107247,"set of trees covered by MH 4 has not been characterized with a non-operational definition, while the set of 1-Endpoint-Crossing trees can be simply defined. However, it also has the following advantages: (+1) It can be given a transition-based interpretation, allowing us to use transition-based scoring functions and to implement the analogous algorithm with greedy or beam search apart from exact inference. No transition-based interpretation is known for 1EC. While a transition-based algorithm has been defined for a strict subset of 1-Endpoint-Crossing trees, called 2-Crossing Interval trees (Pitler and McDonald, 2015), this is a separate algorithm with no known mapping or relation to 1EC or any other dynamic programming model. Thus, we provide the first exact inference algorithm for a non-projective transitionbased parser with practical complexity. (+2) It is conceptually much simpler, with one kind of item and two deduction steps, while the 1-EndpointCrossing parser has five classes of items and several dozen distinct deduction steps. It is also a purely bottom-up parser, whereas the 1-EndpointCrossing parser does not have the bottom-up property. This property is necessary for models that involve composit"
P18-1248,N18-2067,1,0.894022,"of exact decoding for a non-projective transition-based parser. As in the projective case, a mildly non1 http://universaldependencies.org/ 2664 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2664–2675 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics projective decoder has been known for several years (Cohen et al., 2011), corresponding to a variant of the transition-based parser of Attardi (2006). However, its Opn7 q runtime — or the Opn6 q of a recently introduced improvedcoverage variant (Shi et al., 2018) — is still prohibitively costly in practice. Instead, we seek a more efficient algorithm to adapt, and thus develop a transition-based interpretation of GómezRodríguez et al.’s (2011) MH 4 dynamic programming parser, which has been shown to provide very good non-projective coverage in Opn4 q time (Gómez-Rodríguez, 2016). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (GómezRodríguez et al., 2008; Kuhlmann et al., 2011), its own relation to transition-based parsing was not known. Here,"
P18-1248,D17-1002,1,0.905144,"Missing"
P18-1248,K17-3003,1,0.634683,"). While the MH 4 parser was originally presented as a non-projective generalization of the dynamic program that later led to the arc-hybrid transition system (GómezRodríguez et al., 2008; Kuhlmann et al., 2011), its own relation to transition-based parsing was not known. Here, we show that MH 4 can be interpreted as exploring a subset of the search space of a transition-based parser that generalizes the arc-hybrid system, under a mapping that differs from the “push computation” paradigm used by the previously-known dynamic-programming decoders for transition systems. This allows us to extend Shi et al. (2017a)’s work to non-projective parsing, by implementing MH 4 with a minimal set of transition-based features. Experimental results show that our approach outperforms the projective approach of Shi et al. (2017a) and maximum-spanning-tree nonprojective parsing on the most highly nonprojective languages in the CoNLL 2017 sharedtask data that have a single treebank. We also compare with the third-order 1-Endpoint-Crossing (1EC) parser of Pitler (2014), the only other practical implementation of an exact mildly nonprojective decoder that we know of, which also runs in Opn4 q but without a transition-"
P18-1248,L16-1680,0,0.0139431,"5 Experiments Data and Evaluation We experiment with the Universal Dependencies (UD) 2.0 dataset used for the CoNLL 2017 shared task (Zeman et al., 2017). We restrict our choice of languages to be those with only one training treebank, for a better comparison with the shared task results.5 Among these languages, we pick the top 10 most non-projective languages. Their basic statistics are listed in Table 3. For all development-set results, we assume gold-standard tokenization and sentence delimitation. When comparing to the shared task results on test sets, we use the provided baseline UDPipe (Straka et al., 2016) segmentation. Our models do not use part-of-speech tags or morphological tags as features, but rather leverage such information via stack propagation (Zhang and Weiss, 2016), i.e., we learn to predict them as a secondary training objective. We report unlabeled attachment F1scores (UAS) on the development sets for better focus on comparing our (unlabeled) parsing modules. We report its labeled variant (LAS), the main metric of the shared task, on the test sets. For each experiment setting, we ran the model with 5 different random initializations, and report the mean and standard deviation. We"
P18-1248,K17-3020,0,0.0222026,"Missing"
P18-1248,W03-3023,0,0.328263,". To make MH 4 compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due"
P18-1248,P16-1147,0,0.0903915,"Missing"
P18-1248,P11-2033,0,0.0505462,"sed feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is more effective than its projective counterpart in parsing a number of highly non-projective languages. 1 Introduction Transition-based dependency parsers are a popular approach to natural language parsing, as they achieve good results in terms of accuracy and efficiency (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). Until very recently, practical implementations of transition-based parsing were limited to approximate inference, mainly in the form of greedy search or beam search. While cubic-time exact inLillian Lee Cornell University llee@cs.cornell.edu ference algorithms for several well-known projective transition systems had been known since the work of Huang and Sagae (2010) and Kuhlmann et al. (2011), they had been considered of theoretical interest only due to their incompatibility with rich feature mod"
P19-1092,W06-3114,0,0.0159828,"on. Blindx a ˜ik = aix ∀i. Always chosing the xth option. Tests made by the examiners are not totally random (Poundstone, 2014) and right answers tend occur more in middle options. English version HEAD - QA is in Spanish, but we include a translation to English (HEAD - QA - EN) using the Google API, which we use to perform cross-lingual experiments. We evaluated the quality of the translation using a sample of 60 random questions and their answers. We relied on two fluent Spanish-English speakers to score the adequacy6 and on one native English speaker for the fluency,7 following the scale by Koehn and Monz (2006). The average scores for adequacy were 4.35 and 4.71 out of 5, i.e. most of the meaning is captured; and for fluency 4 out of 5, i.e. good. As a side note, it was observed by the annotators that most names of diseases were successfully translated to English. On the negative side, the translator tended to struggle with elements such as molecular formulae, relatively common in chemistry questions.8 3 Control methods Length Choosing the longest answer.9 Poundstone (2014) points out that examiners have to make sure that the right answer is totally correct, which might take more space. 3.2 Strong m"
P19-1092,P17-1171,0,0.339253,"care experts who already try to avoid these biases. Second (and more relevant), random splits would impede comparison against official (and aggregated) human results. Finally, we hope to increase the size of HEAD QA by including questions from future exams. Kaushik and Lipton (2018) discuss on the need of providing rigorous baselines that help better understand the improvement coming from future models, and also the need of avoiding architectural novelty when introducing new datasets. For this reason, our baselines are based on state-of-the-art systems used in open-domain and multi-choice QA (Chen et al., 2017; Kembhavi et al., 2017; Khot et al., 2018; Clark et al., 2018). 3.1 Given the complex nature of the task, we include three control methods: Random Sampling a ˜ ∼ Multinomial (φ), where φ is a random distribution. Blindx a ˜ik = aix ∀i. Always chosing the xth option. Tests made by the examiners are not totally random (Poundstone, 2014) and right answers tend occur more in middle options. English version HEAD - QA is in Spanish, but we include a translation to English (HEAD - QA - EN) using the Google API, which we use to perform cross-lingual experiments. We evaluated the quality of the transl"
P19-1092,D18-1456,0,0.0126659,"(pharmacology) The antibiotic treatment of choice for Meningitis caused by Haemophilus influenzae serogroup b is: 1. Gentamicin 2. Erythromycin 3. Ciprofloxacin 4. Cefotaxime Question (psychology) According to research derived from the Eysenck model, there is evidence that extraverts, in comparison with introverts: 1. Perform better in surveillance tasks. 2. Have greater salivary secretion before the lemon juice test. 3. Have a greater need for stimulation. 4. Have less tolerance to pain. Introduction Recent progress in question answering (QA) has been led by neural models (Seo et al., 2016; Kundu and Ng, 2018), due to their ability to process raw texts. However, some authors (Kaushik and Lipton, 2018; Clark et al., 2018) have discussed the tendency of research to develop datasets and methods that accomodate the data-intensiveness and strengths of current neural methods. This is the case of popular English datasets such as bAbI (Weston et al., 2015) or SQuAD (Rajpurkar et al., 2016, 2018), where some systems achieve near human-level performance (Hu et al., 2018; Xiong et al., 2017) and often surface-level knowledge suffices to answer. To counteract this, Clark et al. (2016) and Clark et al. (2018) h"
P19-1092,D17-1082,0,0.03961,"e data-intensiveness and strengths of current neural methods. This is the case of popular English datasets such as bAbI (Weston et al., 2015) or SQuAD (Rajpurkar et al., 2016, 2018), where some systems achieve near human-level performance (Hu et al., 2018; Xiong et al., 2017) and often surface-level knowledge suffices to answer. To counteract this, Clark et al. (2016) and Clark et al. (2018) have encouraged progress by developing multi-choice datasets that require reasoning. The questions match grade-school science, due to the difficulties to collect specialized questions. With a similar aim, Lai et al. (2017) released 100k questions and 28k passages intended for middle or high school Chinese students, and Zellers et al. (2018) introduced a dataset for common sense reasoning from a spectrum of daily situations. Table 1: Samples from HEAD - QA However, this kind of dataset is scarce for complex domains like medicine: while challenges have been proposed in such domains, like textual entailment (Abacha et al., 2015; Abacha and Dina, 2016) or answering questions about specific documents and snippets (Nentidis et al., 2018), we know of no resources that require general reasoning on complex domains. The"
P19-1092,P02-1040,0,0.104287,"by a non-machine learning model based on information retrieval. We hope this work will encourage research on designing more powerful QA systems that can carry out effective information extraction and reasoning. We also believe there is room for alternative challenges in HEAD - QA. In this work we have used it as a closed QA dataset (the potential answers are used as input to determine the right one). Nothing prevents to use the dataset in an open setting, where the system is given no clue about the possible answers. This would require to think as well whether widely used metrics such as BLEU (Papineni et al., 2002) or exact match could be appropriate for this particular problem. Table 7: Accuracy on the HEAD - QA and HEAD - QA - EN corpora (supervised setting) ES EIR Table 9: Human performance on the 2016 exams. The results are not strictly comparable, as the last 10 questions are considered as backup questions in the human exams, but still show how far the tested baselines are from human performance. Table 6: POINTS on the HEAD - QA and HEAD - QA - EN corpora (unsupervised setting) Model MIR Avg 10 best 627.1 592.2 515.2 575.5 602.1 529.1 humans Pass mark 219.0 207.0 180.0 201.0 210.0 185.0 EN IR 168.0"
P19-1092,D16-1244,0,0.0978303,"Missing"
P19-1092,D18-1546,0,0.093303,"influenzae serogroup b is: 1. Gentamicin 2. Erythromycin 3. Ciprofloxacin 4. Cefotaxime Question (psychology) According to research derived from the Eysenck model, there is evidence that extraverts, in comparison with introverts: 1. Perform better in surveillance tasks. 2. Have greater salivary secretion before the lemon juice test. 3. Have a greater need for stimulation. 4. Have less tolerance to pain. Introduction Recent progress in question answering (QA) has been led by neural models (Seo et al., 2016; Kundu and Ng, 2018), due to their ability to process raw texts. However, some authors (Kaushik and Lipton, 2018; Clark et al., 2018) have discussed the tendency of research to develop datasets and methods that accomodate the data-intensiveness and strengths of current neural methods. This is the case of popular English datasets such as bAbI (Weston et al., 2015) or SQuAD (Rajpurkar et al., 2016, 2018), where some systems achieve near human-level performance (Hu et al., 2018; Xiong et al., 2017) and often surface-level knowledge suffices to answer. To counteract this, Clark et al. (2016) and Clark et al. (2018) have encouraged progress by developing multi-choice datasets that require reasoning. The ques"
P19-1092,P18-2124,0,0.0631093,"Missing"
P19-1092,D16-1264,0,0.190858,"salivary secretion before the lemon juice test. 3. Have a greater need for stimulation. 4. Have less tolerance to pain. Introduction Recent progress in question answering (QA) has been led by neural models (Seo et al., 2016; Kundu and Ng, 2018), due to their ability to process raw texts. However, some authors (Kaushik and Lipton, 2018; Clark et al., 2018) have discussed the tendency of research to develop datasets and methods that accomodate the data-intensiveness and strengths of current neural methods. This is the case of popular English datasets such as bAbI (Weston et al., 2015) or SQuAD (Rajpurkar et al., 2016, 2018), where some systems achieve near human-level performance (Hu et al., 2018; Xiong et al., 2017) and often surface-level knowledge suffices to answer. To counteract this, Clark et al. (2016) and Clark et al. (2018) have encouraged progress by developing multi-choice datasets that require reasoning. The questions match grade-school science, due to the difficulties to collect specialized questions. With a similar aim, Lai et al. (2017) released 100k questions and 28k passages intended for middle or high school Chinese students, and Zellers et al. (2018) introduced a dataset for common sens"
P19-1092,D18-1009,0,0.0202022,"AbI (Weston et al., 2015) or SQuAD (Rajpurkar et al., 2016, 2018), where some systems achieve near human-level performance (Hu et al., 2018; Xiong et al., 2017) and often surface-level knowledge suffices to answer. To counteract this, Clark et al. (2016) and Clark et al. (2018) have encouraged progress by developing multi-choice datasets that require reasoning. The questions match grade-school science, due to the difficulties to collect specialized questions. With a similar aim, Lai et al. (2017) released 100k questions and 28k passages intended for middle or high school Chinese students, and Zellers et al. (2018) introduced a dataset for common sense reasoning from a spectrum of daily situations. Table 1: Samples from HEAD - QA However, this kind of dataset is scarce for complex domains like medicine: while challenges have been proposed in such domains, like textual entailment (Abacha et al., 2015; Abacha and Dina, 2016) or answering questions about specific documents and snippets (Nentidis et al., 2018), we know of no resources that require general reasoning on complex domains. The novelty of this work falls in this direction, presenting a multi-choice QA task that combines the need of knowledge and"
P19-1531,W13-4907,0,0.0597267,"Missing"
P19-1531,D15-1041,0,0.0496823,"Missing"
P19-1531,K18-1030,0,0.0503747,"idden patterns in the data and lead to 5351 better generalization of the model.2 For instance, Hershcovich et al. (2018) have shown that semantic parsing benefits from that approach. The input is the same for both types of parsing and the same number of timesteps are required to compute a tree (equal to the length of the sentence), which simplifies the joint modeling. In this work, we focus on parallel data (we train on the same sentences labeled for both constituency and dependency abstractions). In the future, we plan to explore the idea of exploiting joint training over disjoint treebanks (Barrett et al., 2018). 3.1 Model UAS LAS S-S 93.81 91.59 S - MTL (2) 94.03 93.66 91.78 91.47 S - MTL (3) Table 1: Comparison of the single-paradigm models for dependency parsing evaluated on the PTB dev set where each label is learned as single, 2- or 3-tasks. Baselines and models We test different sequence labeling parsers to determine whether there are any benefits in learning across representations. We compare: (i) a single-task model for constituency parsing and another one for dependency parsing, (ii) a multi-task model for constituency parsing (and another for dependency parsing) where each element of the 3t"
P19-1531,E17-2026,0,0.0278421,"representations To learn across representations we cast the problem as multi-task learning. MTL enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation (Caruana, 1997; Ruder, 2017). In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked BILSTMs shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task’s outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks (Plank et al., 2016; Bingel and Søgaard, 2017; Coavoux and Crabb´e, 2017), where tasks are learned together with the main task in the MTL setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to 5351 better generalization of the model.2 For instance, Hershcovich et al. (2018) have shown that semantic parsing benefits from that approach. The input is the same for both types of parsing and the same number of timesteps are required to compute a tree (equal to the length of the sentence), which simplifies the joint modeling. In this work, we focus on parallel data (we"
P19-1531,W13-4916,0,0.0357695,"Missing"
P19-1531,A00-2018,0,0.781685,"ubler et al., 2009) are the two main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities (Kahane and Mazziotta, 2015). While in constituency parsing the structure of sentences is abstracted as a phrasestructure tree (see Figure 1a), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure 1b). When it comes to developing natural language processing (NLP) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately (Charniak, 2000; Nivre, 2003; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018). Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. Klein and Manning (2003) considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal Contribution (i) We use sequence labeling for constituency (G´omez-Rodr´ıguez and Vilares, 2018) and dependency parsing (Strzyz et al., 2019) combined with multi-task learning (MTL) (Caruana"
P19-1531,D14-1082,0,0.166252,"Missing"
P19-1531,P16-1017,0,0.0369226,"Missing"
P19-1531,E17-2053,0,0.0364825,"Missing"
P19-1531,W13-4906,0,0.0714511,"Missing"
P19-1531,de-marneffe-etal-2006-generating,0,0.114537,"Missing"
P19-1531,N16-1024,0,0.0674432,"Missing"
P19-1531,N19-1076,1,0.8887,"Missing"
P19-1531,P15-1147,0,0.0333513,"Missing"
P19-1531,D18-1162,1,0.913758,"Missing"
P19-1531,P18-1035,0,0.0185829,"entence is first processed by stacked BILSTMs shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task’s outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks (Plank et al., 2016; Bingel and Søgaard, 2017; Coavoux and Crabb´e, 2017), where tasks are learned together with the main task in the MTL setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to 5351 better generalization of the model.2 For instance, Hershcovich et al. (2018) have shown that semantic parsing benefits from that approach. The input is the same for both types of parsing and the same number of timesteps are required to compute a tree (equal to the length of the sentence), which simplifies the joint modeling. In this work, we focus on parallel data (we train on the same sentences labeled for both constituency and dependency abstractions). In the future, we plan to explore the idea of exploiting joint training over disjoint treebanks (Barrett et al., 2018). 3.1 Model UAS LAS S-S 93.81 91.59 S - MTL (2) 94.03 93.66 91.78 91.47 S - MTL (3) Table 1: Compar"
P19-1531,W15-2313,0,0.0308225,"other paradigm. Secondly, we explore an MTL sequence labeling model that parses both representations, at almost no cost in terms of performance and speed. The results across the board show that on average MTL models with auxiliary losses for constituency parsing outperform singletask ones by 1.05 F1 points, and for dependency parsing by 0.62 UAS points. 1 Introduction Constituency (Chomsky, 1956) and dependency grammars (Mel’cuk, 1988; K¨ubler et al., 2009) are the two main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities (Kahane and Mazziotta, 2015). While in constituency parsing the structure of sentences is abstracted as a phrasestructure tree (see Figure 1a), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure 1b). When it comes to developing natural language processing (NLP) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately (Charniak, 2000; Nivre, 2003; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018). Despite the potential benefits of learning across represe"
P19-1531,Q16-1023,0,0.0493719,"wo main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities (Kahane and Mazziotta, 2015). While in constituency parsing the structure of sentences is abstracted as a phrasestructure tree (see Figure 1a), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure 1b). When it comes to developing natural language processing (NLP) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately (Charniak, 2000; Nivre, 2003; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018). Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. Klein and Manning (2003) considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal Contribution (i) We use sequence labeling for constituency (G´omez-Rodr´ıguez and Vilares, 2018) and dependency parsing (Strzyz et al., 2019) combined with multi-task learning (MTL) (Caruana, 1997) to learn across syntactic representat"
P19-1531,P18-1249,0,0.0312712,"entence, and each of them has its own particularities (Kahane and Mazziotta, 2015). While in constituency parsing the structure of sentences is abstracted as a phrasestructure tree (see Figure 1a), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure 1b). When it comes to developing natural language processing (NLP) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately (Charniak, 2000; Nivre, 2003; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018). Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. Klein and Manning (2003) considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal Contribution (i) We use sequence labeling for constituency (G´omez-Rodr´ıguez and Vilares, 2018) and dependency parsing (Strzyz et al., 2019) combined with multi-task learning (MTL) (Caruana, 1997) to learn across syntactic representations. To do so, we take a parsing paradigm (constituency or depende"
P19-1531,C18-1271,0,0.0335317,"to a label (Rei and Søgaard, 2018). Many NLP tasks suit this setup, including part-of-speech tag5350 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5350–5357 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ging, named-entity recognition or chunking (Sang and Buchholz, 2000; Toutanova and Manning, 2000; Tjong Kim Sang and De Meulder, 2003). More recently, syntactic tasks such as constituency parsing and dependency parsing have been successfully reduced to sequence labeling (Spoustov´a and Spousta, 2010; Li et al., 2018; G´omezRodr´ıguez and Vilares, 2018; Strzyz et al., 2019). Such models compute a tree representation of an input sentence using |w |tagging actions. We will also cast parsing as sequence labeling, to then learn across representations using multitask learning. Two are the main advantages of this approach: (i) it does not require an explicit parsing algorithm nor explicit parsing structures, and (ii) it massively simplifies joint syntactic modeling. We now describe parsing as sequence labeling and the architecture used in this work. Constituency parsing as tagging G´omezRodr´ıguez and Vilares ("
P19-1531,P18-1130,0,0.0167048,"ture of a given sentence, and each of them has its own particularities (Kahane and Mazziotta, 2015). While in constituency parsing the structure of sentences is abstracted as a phrasestructure tree (see Figure 1a), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure 1b). When it comes to developing natural language processing (NLP) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately (Charniak, 2000; Nivre, 2003; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018). Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. Klein and Manning (2003) considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal Contribution (i) We use sequence labeling for constituency (G´omez-Rodr´ıguez and Vilares, 2018) and dependency parsing (Strzyz et al., 2019) combined with multi-task learning (MTL) (Caruana, 1997) to learn across syntactic representations. To do so, we take a parsing paradigm"
P19-1531,J93-2004,0,0.0656614,"Missing"
P19-1531,W03-3017,0,0.0641706,"09) are the two main abstractions for representing the syntactic structure of a given sentence, and each of them has its own particularities (Kahane and Mazziotta, 2015). While in constituency parsing the structure of sentences is abstracted as a phrasestructure tree (see Figure 1a), in dependency parsing the tree encodes binary syntactic relations between pairs of words (see Figure 1b). When it comes to developing natural language processing (NLP) parsers, these two tasks are usually considered as disjoint tasks, and their improvements therefore have been obtained separately (Charniak, 2000; Nivre, 2003; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Ma et al., 2018; Kitaev and Klein, 2018). Despite the potential benefits of learning across representations, there have been few attempts in the literature to do this. Klein and Manning (2003) considered a factored model that provides separate methods for phrase-structure and lexical dependency trees and combined them to obtain optimal Contribution (i) We use sequence labeling for constituency (G´omez-Rodr´ıguez and Vilares, 2018) and dependency parsing (Strzyz et al., 2019) combined with multi-task learning (MTL) (Caruana, 1997) to le"
P19-1531,W00-0726,0,0.110437,"Missing"
P19-1531,W14-6111,0,0.0700176,"Missing"
P19-1531,N19-1077,1,0.840433,"Missing"
P19-1531,P16-2067,0,0.0232026,"k. 3 Learning across representations To learn across representations we cast the problem as multi-task learning. MTL enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation (Caruana, 1997; Ruder, 2017). In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked BILSTMs shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task’s outputs. In particular, to benefit from a specific parsing abstraction we will be using the concept of auxiliary tasks (Plank et al., 2016; Bingel and Søgaard, 2017; Coavoux and Crabb´e, 2017), where tasks are learned together with the main task in the MTL setup even if they are not of actual interest by themselves, as they might help to find out hidden patterns in the data and lead to 5351 better generalization of the model.2 For instance, Hershcovich et al. (2018) have shown that semantic parsing benefits from that approach. The input is the same for both types of parsing and the same number of timesteps are required to compute a tree (equal to the length of the sentence), which simplifies the joint modeling. In this work, we"
P19-1531,W00-1308,0,0.0644568,"Missing"
P19-1531,N18-1027,0,0.0227553,"can robustly produce both constituency and dependency trees, obtaining a performance and speed comparable with previous sequence labeling models for (either) constituency or dependency parsing. The source code is available at https://github.com/ mstrise/seq2label-crossrep 2 Parsing as Sequence Labeling Notation We use w = [wi , ..., w|w |] to denote an input sentence. We use bold style lower-cased and math style upper-cased characters to refer to vectors and matrices (e.g. x and W). Sequence labeling is a structured prediction task where each token in the input sentence is mapped to a label (Rei and Søgaard, 2018). Many NLP tasks suit this setup, including part-of-speech tag5350 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5350–5357 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ging, named-entity recognition or chunking (Sang and Buchholz, 2000; Toutanova and Manning, 2000; Tjong Kim Sang and De Meulder, 2003). More recently, syntactic tasks such as constituency parsing and dependency parsing have been successfully reduced to sequence labeling (Spoustov´a and Spousta, 2010; Li et al., 2018; G´omezRodr´ıguez"
P19-1531,P18-4013,0,0.041025,"dency trees with their encodings. tion of a LSTM that processes the input from left to right, and let LSTM← (x) be another LSTM processing the input in the opposite direction, the output hi of a BILSTM at a timestep i is computed as: BILSTM (x, i) = LSTM → (x0:i ) ◦ LSTM ← (xi:|w |). Then, hi is further processed by a feed-forward layer to compute the output label, i.e. P (y|hi ) = softmax (W ∗ hi + b). To optimize the model, we minimize P the categorical cross-entropy loss, i.e. L = − log(P (y|hi )). In Appendix A we detail additional hyperpameters of the network. In this work we use NCRFpp (Yang and Zhang, 2018) as our sequence labeling framework. 3 Learning across representations To learn across representations we cast the problem as multi-task learning. MTL enables learning many tasks jointly, encapsulating them in a single model and leveraging their shared representation (Caruana, 1997; Ruder, 2017). In particular, we will use a hard-sharing architecture: the sentence is first processed by stacked BILSTMs shared across all tasks, with a task-dependent feed-forward network on the top of it, to compute each task’s outputs. In particular, to benefit from a specific parsing abstraction we will be usin"
P19-1531,P13-1043,0,0.0684109,"Missing"
P19-1531,nivre-etal-2006-maltparser,0,\N,Missing
P19-1531,W03-0419,0,\N,Missing
P19-1531,N19-1341,1,\N,Missing
S14-2071,S14-2071,1,0.0513054,"Missing"
S14-2071,W11-0705,0,0.307912,"Missing"
S14-2071,S13-2055,0,0.322951,"Missing"
S14-2071,P09-2079,0,0.200925,"Missing"
S14-2071,J93-2004,0,0.0464957,"Missing"
S14-2071,pak-paroubek-2010-twitter,0,\N,Missing
S14-2071,S13-2052,0,\N,Missing
S16-1009,S13-2053,0,0.0801214,"Missing"
S16-1009,S16-1001,0,0.0311197,"y the Ministerio de Econom´ıa y Competitividad (FFI2014-51978-C2). David Vilares is funded by the Ministerio de Educaci´on, Cultura y Deporte (FPU13/01180). Yerai Doval is funded by the Ministerio de Econom´ıa y Competitividad (BES-2015-073768). Carlos G´omez-Rodr´ıguez is funded by an Oportunius program grant (Xunta de Galicia). The SemEval organization proposed two different types of challenges in its 2016 edition: (1) classification into two, three and five classes and (2) quantification into two and five categories. A detailed explanation of the task can be found in the description paper (Nakov et al., 2016). For all subtasks, three official splits are provided: training, development and development test sets. In this paper, we use the training and development sets for training, and the development test set for evaluation.1 2.1 Convolutional Neural Network As a starting point, we train a deep neural network (DNN), in particular a convolutional neural network (CNN), following a similar configuration to the one 1 For classification into 3 polarities, we include the training set of SemEval 2013 as part of our training set and its development set as a part of our collection for tuning. 79 Proceedings"
S16-1009,D14-1162,0,0.0791577,"Missing"
S16-1009,D15-1303,0,0.0329965,"er the CNN without pretraining. A preliminary analysis suggests that: (1) we need more tweets to exploit distant supervision, (2) fine hyperparameter engineering needs to be explored to ensure that the fine-tuning on the labeled data does not completely overwrite what the network has already learned and (3), it is easy to collect tweets for analysis into 2 classes, but downloading non-noisy tweets for analysis into 3 and 5 classes is a more challenging issue. In the following section we show how to exploit the hidden activation values of our deep learning model as part of a supervised system (Poria et al., 2015), when pretraining and fast hyper-parameter engineering are not feasible options. #1 ... #2 ... I w11 w12 do w21 w22 n&apos;t w31 w32 like w41 w42 this w51 w52 movie w61 w62 arggg 0 0 ... ... ... ... ... ... ... #m w1n ... w2n w3n w4n ... ... w5n . . . ... . . . w6n 0 ... embedding dimension (n) m convolutional lters applied Max pooling from fully connected layers each lter (size m) Figure 1: Topology of our CNN from where we will extract the neural activation values 2.2 Classification Let S={s1 , ..., sn } be a set of tweets and let L={l1 , ..., ln } be a set of labels, the classification subtask"
S16-1009,S15-2079,0,0.0209399,"e training and development sets for training, and the development test set for evaluation.1 2.1 Convolutional Neural Network As a starting point, we train a deep neural network (DNN), in particular a convolutional neural network (CNN), following a similar configuration to the one 1 For classification into 3 polarities, we include the training set of SemEval 2013 as part of our training set and its development set as a part of our collection for tuning. 79 Proceedings of SemEval-2016, pages 79–84, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics used by Severyn and Moschitti (2015). Figure 1 illustrates the topology of the CNN from where we will extract the hidden activation values. 2.1.1 Embeddings layer Let w be a token of a vocabulary V , a word embedding is a distributed representation of that token as a low dimensional vector v ∈ Rn . In that way, it is possible to create a matrix of embeddings, E ∈ R|V |×n , to act as the input layer to the CNN. Particularly, we rely on a collection of Twitter word embeddings pretrained with Glove2 (Pennington et al., 2014) with |V |≈ 106 and n=100. Thus, given a tweet t=[w1 , w2 , ..., wt ], after running our input layer we will"
S16-1009,J11-2001,0,0.00514639,"erties (e.g. anger or anxiety) or topics (e.g. family or religion). • Part-of-speech tags (T). Additionally, this year we have included: • The last word of the tweet (LW): The last term of each tweet is used as a separate feature. • The psychometric properties of the last word of the tweet (LP). • Hidden activation values from the CNN (HV): We take the hidden activation values of the last hidden layer. • Features extracted from sentiment dictionaries: We extract the total, maximum, minimum and last sentiment score of a tweet from the Sentiment140 (Mohammad et al., 2013), Hu and Liu (2004) and Taboada et al. (2011) subjective lexica. 2.2.1 Experimental results Table 1 shows the experimental results for classification into two classes obtained using the SVM with different feature sets and the CNN. The neural network outperforms most of the SVM approaches. Only when we combine a number of linguistic features with the hidden activation values and we Features Recall-P Recall-N Macro avg. R HV. P. D . LW. LP. FT* 0.721 0.803 0.762 HV. P. D . LW. LP. FT 0.856 0.581 0.719 HV 0.864 0.560 0.712 P 0.953 0.192 0.573 W 0.969 0.162 0.566 D 0.892 0.249 0.564 CNN 0.802 0.671 0.737 tion, cost parameter (C) and class we"
S16-1009,S14-2071,1,0.907806,"Missing"
W06-1514,W00-2027,0,0.0278731,"lgorithms without unification support can be found at (Alonso et al., 1999). These schemata were extended as described in the previous sections, and used as input to our system which generated their corresponding parsers. These parsers were then run on the test sentences shown in table 2, obtaining the performance measures (in terms of runtime and amount of items generated) that can be seen in table 3. Note that the sentences are ordered by minimal runtime. As we can see, the execution times are not as good as the ones we would obtain if we used Sarkar’s XTAG distribution parser written in C (Sarkar, 2000). This is not surprising, since our parsers have been generated by a generic tool without knowledge of the grammar, while the XTAG parser has been designed specifically for optimal performance in this grammar and uses additional information (such as tree usage frequency data from several corpora, see (XTAG, 2001)). However, our comparison allows us to draw conclusions about which parsing algorithms are better suited for the XTAG grammar. In terms of memory usage, CYK is the clear winner, since it clearly generates less items than the other algorithms, and a CYK item doesn’t take up more 4 Conc"
W06-1514,E99-1020,1,0.90319,"grammar. The parse forest can then be retrieved from the intermediate items used to infer the final items, as in (Billot and Lang, 1989). As an example, we introduce a CYK-based algorithm (Vijay-Shanker and Joshi, 1985) for TAG. Given a tree adjoining grammar G = (VT , VN , S, I, A)1 and a sentence of length n which we denote by a1 a2 . . . an 2 , we denote by P (G) the set of productions {N γ → N1γ N2γ . . . Nrγ } such that N γ is an inner node of a tree γ ∈ (I ∪ A), and N1γ N2γ . . . Nrγ is the ordered sequence of direct children of N γ . The parsing schema for the TAG CYK-based algorithm (Alonso et al., 1999) is a function that maps such a grammar G to a deduction system whose domain is the set of items {[N γ , i, j, p, q, adj]} verifying that N γ is a tree node in an elementary In this paper, a generic system that generates parsers from parsing schemata is applied to the particular case of the XTAG English grammar. In order to be able to generate XTAG parsers, some transformations are made to the grammar, and TAG parsing schemata are extended with feature structure unification support and a simple tree filtering mechanism. The generated implementations allow us to study the performance of differe"
W06-1514,P89-1018,0,0.200676,"of incomplete parse trees which the algorithm can generate. An input sentence to be analyzed produces an initial set of items. Additionally, a parsing schema must define a criterion to determine which items are final, i.e. which items correspond to complete parses of the input sentence. If it is possible to obtain a final item from the set of initial items by using the schema’s inference rules (called deductive steps), then the input sentence belongs to the language defined by the grammar. The parse forest can then be retrieved from the intermediate items used to infer the final items, as in (Billot and Lang, 1989). As an example, we introduce a CYK-based algorithm (Vijay-Shanker and Joshi, 1985) for TAG. Given a tree adjoining grammar G = (VT , VN , S, I, A)1 and a sentence of length n which we denote by a1 a2 . . . an 2 , we denote by P (G) the set of productions {N γ → N1γ N2γ . . . Nrγ } such that N γ is an inner node of a tree γ ∈ (I ∪ A), and N1γ N2γ . . . Nrγ is the ordered sequence of direct children of N γ . The parsing schema for the TAG CYK-based algorithm (Alonso et al., 1999) is a function that maps such a grammar G to a deduction system whose domain is the set of items {[N γ , i, j, p, q,"
W06-1514,P85-1011,0,0.637089,"ce to be analyzed produces an initial set of items. Additionally, a parsing schema must define a criterion to determine which items are final, i.e. which items correspond to complete parses of the input sentence. If it is possible to obtain a final item from the set of initial items by using the schema’s inference rules (called deductive steps), then the input sentence belongs to the language defined by the grammar. The parse forest can then be retrieved from the intermediate items used to infer the final items, as in (Billot and Lang, 1989). As an example, we introduce a CYK-based algorithm (Vijay-Shanker and Joshi, 1985) for TAG. Given a tree adjoining grammar G = (VT , VN , S, I, A)1 and a sentence of length n which we denote by a1 a2 . . . an 2 , we denote by P (G) the set of productions {N γ → N1γ N2γ . . . Nrγ } such that N γ is an inner node of a tree γ ∈ (I ∪ A), and N1γ N2γ . . . Nrγ is the ordered sequence of direct children of N γ . The parsing schema for the TAG CYK-based algorithm (Alonso et al., 1999) is a function that maps such a grammar G to a deduction system whose domain is the set of items {[N γ , i, j, p, q, adj]} verifying that N γ is a tree node in an elementary In this paper, a generic s"
W06-1514,J99-3002,0,\N,Missing
W06-1514,H86-1020,0,\N,Missing
W14-4204,W12-2108,0,0.0241609,"nce of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coordinates -120◦ , -55◦ and -29◦ , 30◦ (roughly delimiting Latin America), as well as -10◦ , 35◦ and 3◦ , 46◦ (roughly delimiting Spain). The download ran from July 2 to July 4, 2014. In a second step, we sorted the tweets a"
W14-4204,R13-1026,0,0.0218585,"for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). reaches an overall F-score of 67.72 on the fiveclass problem. On the two-class problem, human classification is outper"
W14-4204,W13-1729,0,0.0233276,"usses the results, and Section 8 concludes the article. 2 Related Work Research on language identification has seen a variety of methods. A well established technique is the use of character n-gram models. Cavnar and Trenkle (1994) build n-gram frequency “profiles” for several languages and classify text by matching it to the profiles. Dunning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, ˇ uˇrek and Kolkus (2009), who treat “noisy” and Reh˚ web text and therefore consider the particular influence of single words in discriminating between languages. Language i"
W14-4204,P11-1038,0,0.0746979,"Missing"
W14-4204,N10-1027,0,0.0172443,"ties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, ˇ uˇrek and Kolkus (2009), who treat “noisy” and Reh˚ web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and"
W14-4204,P12-3005,0,0.0318804,"le language, such as the regional varieties of Spanish. This task is especially challenging because the differences between 1 We are aware that there are natively Spanish-speaking communities elsewhere, such as on the Philippines, but we do not consider them in this study. 25 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 25–35, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied very recently. Zampieri et al. (2013) have addressed the task for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating"
W14-4204,W14-5307,0,0.311402,"Missing"
W14-4204,W14-1303,0,0.0629176,", 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coordinates -120◦ , -55◦ and -29◦ , 30◦ (roughly delimiting Latin America), as well as -10◦ , 35◦ and 3◦ , 46◦ (roughly delimiting Spain). The download ran from July 2 to July 4, 2014. In a second step, we sorted the tweets according to the respective countries. Twitter is not used to the same extent in all countries where Spanish is spoken. In the time 2 https://dev.twitter.com/docs/api/ stream"
W14-4204,U13-1003,0,0.380598,"word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). reaches an overall F-score of 67.72 on the fiveclass problem. On the two-class problem, human classification is outperformed by a large margin. The remainder of t"
W14-4204,vatanen-etal-2010-language,0,0.335234,"entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, ˇ uˇrek and Kolkus (2009), who treat “noisy” and Reh˚ web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesti"
W15-2902,balahur-etal-2014-resource,0,0.0406971,"Missing"
W15-2902,C94-1097,0,0.0665748,"Missing"
W15-2902,P12-3005,0,0.0710309,"Missing"
W15-2902,E12-2012,0,0.0522261,"Missing"
W15-2902,P13-2017,0,0.0322836,"Missing"
W15-2902,C10-1004,0,0.028016,"Missing"
W15-2902,S13-2052,0,0.0348675,"Missing"
W15-2902,R09-1010,0,0.0244937,"Missing"
W15-2902,S14-2009,0,0.0297358,"corpus We created a polarity corpus with code-switching tweets based on the training collection6 (en-es) presented by Solorio et al. (2014). Each word in the corpus is labelled with its language, serving as the starting point to obtain a collection of multilingual tweets. We first filtered the tweets containing both Spanish and English words, obtaining 3,062 tweets. Those were manually labelled by three annotators according to the SentiStrength strategy, a Monolingual corpora Two corpora are used to compare the performance of monolingual and multilingual models: • SemEval 2014 task B corpus (Rosenthal et al., 2014): A set of English tweets4 split into a 5 It also contained short texts coming from SMS and messages from LiveJournal, that we removed as they are out of the scope of this study. 6 The test set was not released for the research community. 4 Due to Twitter restrictions some of the tweets are not available anymore, so the corpus statistics may vary slightly from those of other researchers that used the corpus. 4 1K test set es pipe en-es Words (W) 56.60 56.50 54.60 Lemmas (L ) 56.40 56.30 56.60 Psychometric (P) 54.70 54.70 53.10 PoS-tags (T ) 48.90 48.80 41.70 Bigrams of W 52.90 52.70 52.10 Bigr"
W15-2902,W14-3907,0,0.0661216,"ions between words (e.g. ‘not good’ becomes ‘not good’). 4 Experimental framework The proposed sets of features and models are evaluated on standard monolingual corpora, taking accuracy as the reference metric. These monolingual collections are then joined to create a multilingual corpus, which helps us compare the performance of the approaches when tweets come from two different languages. An evaluation over a codeswitching test set is also carried out. 4.1 4.3 Code-switching corpus We created a polarity corpus with code-switching tweets based on the training collection6 (en-es) presented by Solorio et al. (2014). Each word in the corpus is labelled with its language, serving as the starting point to obtain a collection of multilingual tweets. We first filtered the tweets containing both Spanish and English words, obtaining 3,062 tweets. Those were manually labelled by three annotators according to the SentiStrength strategy, a Monolingual corpora Two corpora are used to compare the performance of monolingual and multilingual models: • SemEval 2014 task B corpus (Rosenthal et al., 2014): A set of English tweets4 split into a 5 It also contained short texts coming from SMS and messages from LiveJournal"
W15-2902,taule-etal-2008-ancora,0,0.0250625,"Missing"
W15-2902,W00-1308,0,0.0325475,"Missing"
W15-2902,S14-2071,1,0.896126,"Missing"
W15-2902,petrov-etal-2012-universal,0,\N,Missing
W15-2902,P16-2069,1,\N,Missing
W15-4315,W15-4319,0,0.0685569,"Missing"
W15-4315,P14-2111,0,0.0498856,"el as a knowledge source. Only unigrams and bigrams could be used because of unsolved memory limitations. However, in contrast with previous experiments performed for Spanish, the resulting performance was unsatisfactory. Because of this, the use of these language models for our final submission was dismissed. According to our analysis, the cause for this seems to be the great differences, at both the lexical and syntactical levels, between the texts used to build this model, which could be considered as “regular” texts, and those corresponding to tweets, which agrees with the observations of Chrupała (2014). As illustrative examples of this type of expressions we can take “I like them girls” and “Why you no do that?”, which are lexically correct but not syntactically valid, so language models built using regular texts will not recognize them. In the case of our previous experiments on Spanish, this difference was not so clear. 5 • Using POS tags and syntactic information to improve the candidate selection process. • Integrating a classifier in the extraction process of the final normalization candidates, taking as features aspects such as the syntactic and morphosyntactic information obtained, t"
W15-4315,P11-1038,0,0.0353985,"Architecture Our tweet normalization system was developed taking as basic premises its flexibility, scalability and maintainability. As a starting point, we took a previous prototype for Spanish tweet normalization (Vilares et al., 2013) which, although fully functional, did not turn out to be as flexible and maintainable as expected. This could have become a problem for future developments, since the adaptation effort needed to integrate new techniques would have been too large, so we decided to refactor the whole system to solve this. The general scheme of the original system mimics that of Han and Baldwin (2011) and comprises three stages: 1. Tweet preprocessing. 2. In-vocabulary word identification (IV), based on the lexicon of the system, obtaining as 99 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 99–105, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics still sharing the same basic processor interface, thus preserving the flexibility of the decorator. Thereby, the resulting structure allows for the dynamic construction of different pipeline configurations of varying complexity and different levels of abstraction, not being restricted to th"
W15-4315,D13-1007,0,0.0736598,"Training results. constrained unconstrained precision 0.4646 0.4592 recall 0.6281 0.6296 F1 0.5341 0.5310 Table 2: Testing results. Table 2 shows the results obtained for the test corpus. At the sight of these figures, which differ considerably from the previous ones, we decided to analyse them in more detail. For this purpose, 3 http://redis.io/ 103 • Obtaining a representative language model of the target domain by using a larger normalized tweet corpus. This corpus will be comprised of tweets without non-standard words, so we can still capture the morphosyntactic structure of these texts (Yang and Eisenstein, 2013). language model in use. In this respect, tuning experiments were also made by extending our unconstrained configuration through the addition of the Web 1T 5-gram v1 English language model as a knowledge source. Only unigrams and bigrams could be used because of unsolved memory limitations. However, in contrast with previous experiments performed for Spanish, the resulting performance was unsatisfactory. Because of this, the use of these language models for our final submission was dismissed. According to our analysis, the cause for this seems to be the great differences, at both the lexical a"
W15-4315,C08-1056,0,0.0254093,"ich are out of the system lexicon and proper lexical variants, obtaining for each one of the latter a normalized form. This last step can be in turn decomposed into two: the first one, which generates a set of possible normalization candidates based on the application of certain normalization techniques; and the second one, which selects one of these candidates as the normalized form (in our case, in a scoredriven process). As for the particular normalization techniques employed throughout our system, we decided to try first a combination of two of the traditional approximations to this task (Kobus et al., 2008): the spell checking and the automatic speech recognition metaphors. 2.1 The pipeline We decided to give our system an object oriented approach (using JAVA) as opposed to the imperative approach of the original prototype (in P ERL). The new system is structured in processors, formerly known as modules in the prototype, whose goal is to apply a certain process to the input tweets so that we can obtain the normalization candidates of their terms at its output. The core component of our system is the pipeline, consisting of a classic cascade structure where we can insert an arbitrary number of pr"
W15-4315,N13-1039,0,0.0233371,"Missing"
W15-4315,padro-stanilovsky-2012-freeling,0,0.0644033,"Missing"
W15-4315,P11-1027,0,\N,Missing
W17-5209,agerri-etal-2014-ixa,0,0.0128377,"(Brooke et al., 2009): It contains SO’s for subjective words that range from 1 to 5 for positive and negative terms. We translated it to ca, eu, gl and pt using apertium (Forcada et al., 2011). We removed the unknown words and obtained the numbers in Table 1.2 2. ML-Senticon (Cruz et al., 2014): Multi-layered lexica (not available for pt) with SO’s where each layer contains a larger number of terms, but less trustable. We used the seventh layer for each language. As eu, ca and gl files have the same PoStag for adverbs and adjectives, they were automatically classified using monolingual tools (Agerri et al., 2014; Padr´o and Stanilovsky, 2012; Garcia and Gamallo, 2015) (Table 2 contains the statistics). SO’s (originally from 0 to 1) were linearly transformed to the scale of the SFU lexicon. SISA: Syntactic Iberian SA Preliminaries Vilares et al. (2017) propose a formalism to define compositional operations. Given a dependency tree for a text, a compositional operation defines how a node in the tree modifies the semantic orientation (SO) of a branch or node, based on elements such as the word form, part-of-speech (PoS) tag or dependency type, without limitations in terms of its location inside such tre"
W17-5209,Q16-1031,0,0.0551624,"Missing"
W17-5209,W13-4829,0,0.0263927,"Missing"
W17-5209,D14-1108,0,0.060949,"Missing"
W17-5209,R09-1010,0,0.218002,"l languages in the Iberian Peninsula: Basque (eu), Catalan (ca), Galician (gl), Portuguese (pt) and Spanish (es). We rely on three premises: 2 Related work Polarity classification has been addressed through machine learning (Mohammad et al., 2013; Socher et al., 2013; Vo and Zhang, 2016), and lexiconbased models (Turney, 2002). Most of the research involves English texts, although studies can be found for other languages such as Chinese (Chen and Chen, 2016) or Arabic (Shoukry and Rafea, 2012). For the official languages in the Iberian Peninsula, much of the literature has focused on Spanish. Brooke et al. (2009) proposed a lexiconbased SA system that defines rules at the lexical level to handle negation, intensification or adversative subordinate clauses. They followed a crosslingual approach, adapting their English method (Taboada et al., 2011) to obtain the semantic orientation (SO) of Spanish texts. Vilares et al. created a syntactic rule-based system, by making an interpretation of Brooke et al.’s system, but limited to AnCora trees (Taul´e et al., 2008). Mart´ınezC´amara et al. (2011) were one of the first to report a wide set of experiments on a number of 1. Syntactic structures can be defined"
W17-5209,P13-2017,0,0.082405,"Missing"
W17-5209,P11-2099,0,0.015729,"x. β is extracted from a lexicon with booster values (in this work obtained from SFU, where ‘muy’ has a booster value of 0.25). 4 Evaluation This section presents the results of the experiments we carried out with our system using both the monolingual and the multilingual lexica, compared to the performance of a supervised classifier for three of the five analyzed languages. 4.1 Testing corpora • Spanish SFU (Brooke et al., 2009): A set of 400 long reviews (200 positive, 200 negative) from different domains such as movies, music, computers or washing machines. • Portuguese SentiCorpus-PT 0.1 (Carvalho et al., 2011): A collection of comments from the Portuguese newspaper P´ublico with polarity annotation at the entity level. As our system assigns the polarity at the sentence level, we selected the SentiCorpus sentences with (a) only one SO and (b) with &gt; 1 SO iff all of them were the same, generating a corpus with 2, 086 (from 2, 604) sentences. 2. Subordinate adversative clauses: This rule is designed for dealing with structures coordinated by adversative conjunctions (such as but), which usually involve opposite polarities between the two joint elements (e.g., “good but expensive”). Here, the SO of the"
W17-5209,S13-2053,0,0.027921,"Missing"
W17-5209,P16-2004,0,0.0313125,"nk, and so the rules are annotation-dependent too. Advances in NLP make it now possible to overcome such issues. We present a model that analyzes five official languages in the Iberian Peninsula: Basque (eu), Catalan (ca), Galician (gl), Portuguese (pt) and Spanish (es). We rely on three premises: 2 Related work Polarity classification has been addressed through machine learning (Mohammad et al., 2013; Socher et al., 2013; Vo and Zhang, 2016), and lexiconbased models (Turney, 2002). Most of the research involves English texts, although studies can be found for other languages such as Chinese (Chen and Chen, 2016) or Arabic (Shoukry and Rafea, 2012). For the official languages in the Iberian Peninsula, much of the literature has focused on Spanish. Brooke et al. (2009) proposed a lexiconbased SA system that defines rules at the lexical level to handle negation, intensification or adversative subordinate clauses. They followed a crosslingual approach, adapting their English method (Taboada et al., 2011) to obtain the semantic orientation (SO) of Spanish texts. Vilares et al. created a syntactic rule-based system, by making an interpretation of Brooke et al.’s system, but limited to AnCora trees (Taul´e"
W17-5209,padro-stanilovsky-2012-freeling,0,0.100334,"Missing"
W17-5209,L16-1149,0,0.0840243,"es. Souza and Vieira (2012) carried out a study of Twitter data, exploring preprocessing techniques, subjectivity data and negation approaches. They concluded that those have a small impact on the polarity classification of tweets. Balage Filho et al. (2013) evaluate the quality of the Brazilian LIWC dictionary (Pennebaker et al., 2001) for SA, comparing it with existing lexica for this language. For Basque, Catalan and Galician, literature is scarce. Cruz et al. (2014) introduce a method to create multiple layered lexicons for different languages including co-official languages in Spain. San Vicente and Saralegi (2016) explore different ways to create lexicons, and apply them to the Basque case. They report an evaluation on a Basque dataset intended for polarity classification. Bosco et al. (2016) discuss the collection of data for the Catalan Elections and design an annotation scheme to apply SA techniques, but the dataset is still not available. With respect to Galician, in this article we will present the first published results for this language. 3 3.1 ADJ NOUN ADV VERB es 2,045 1,323 594 739 pt 1,865 1,183 570 688 ca 1,686 1,168 533 689 eu 1,757 1,211 535 563 gl 2,002 1,270 599 723 Table 1: Size of the"
W17-5209,P16-2069,1,0.898786,"Missing"
W17-5209,D13-1170,0,0.00462101,"butions of the paper are: 1. A single set of syntactic rules to handle linguistic phenomena across five Iberian languages from different families. 2. The first end-to-end multilingual syntax-based SA system that analyzes five official languages of the Iberian Peninsula. This is also the first evaluation for SA that provides results for some of them. Introduction Finding the scope of linguistic phenomena in natural language processing (NLP) is a core utility of parsing. In sentiment analysis (SA), it is used to address structures that play a role in polarity classification, both in supervised (Socher et al., 2013) and symbolic (Vilares et al.) models. In the latter case, these are mostly monolingual and dependent on the annotation of the training treebank, and so the rules are annotation-dependent too. Advances in NLP make it now possible to overcome such issues. We present a model that analyzes five official languages in the Iberian Peninsula: Basque (eu), Catalan (ca), Galician (gl), Portuguese (pt) and Spanish (es). We rely on three premises: 2 Related work Polarity classification has been addressed through machine learning (Mohammad et al., 2013; Socher et al., 2013; Vo and Zhang, 2016), and lexico"
W17-5209,P16-2036,0,0.0125546,"upervised (Socher et al., 2013) and symbolic (Vilares et al.) models. In the latter case, these are mostly monolingual and dependent on the annotation of the training treebank, and so the rules are annotation-dependent too. Advances in NLP make it now possible to overcome such issues. We present a model that analyzes five official languages in the Iberian Peninsula: Basque (eu), Catalan (ca), Galician (gl), Portuguese (pt) and Spanish (es). We rely on three premises: 2 Related work Polarity classification has been addressed through machine learning (Mohammad et al., 2013; Socher et al., 2013; Vo and Zhang, 2016), and lexiconbased models (Turney, 2002). Most of the research involves English texts, although studies can be found for other languages such as Chinese (Chen and Chen, 2016) or Arabic (Shoukry and Rafea, 2012). For the official languages in the Iberian Peninsula, much of the literature has focused on Spanish. Brooke et al. (2009) proposed a lexiconbased SA system that defines rules at the lexical level to handle negation, intensification or adversative subordinate clauses. They followed a crosslingual approach, adapting their English method (Taboada et al., 2011) to obtain the semantic orient"
W17-5209,W11-4507,0,0.0213266,"classifiers. The TASS workshop on sentiment analysis focused on Spanish language (Villena-Rom´an et al., 2013) annually proposes different challenges related to polarity classification, and a number of approaches have used its framework to build their Spanish systems, most of them based on supervised learning (Saralegi and San Vicente, 2013; Gamallo et al., 2013; Hurtado et al., 2015; Vilares et al., 2015). Sentiment analysis for Portuguese has also attracted the interest of the research community. Silva et al. (2009) presented a system for detection of opinions about Portuguese politicians. Souza et al. (2011) built a lexicon for Brazilian Portuguese exploring different techniques (e.g. translation and thesaurus-based approaches) and available resources. Souza and Vieira (2012) carried out a study of Twitter data, exploring preprocessing techniques, subjectivity data and negation approaches. They concluded that those have a small impact on the polarity classification of tweets. Balage Filho et al. (2013) evaluate the quality of the Brazilian LIWC dictionary (Pennebaker et al., 2001) for SA, comparing it with existing lexica for this language. For Basque, Catalan and Galician, literature is scarce."
W17-5209,J11-2001,0,0.0640994,"al., 2013; Socher et al., 2013; Vo and Zhang, 2016), and lexiconbased models (Turney, 2002). Most of the research involves English texts, although studies can be found for other languages such as Chinese (Chen and Chen, 2016) or Arabic (Shoukry and Rafea, 2012). For the official languages in the Iberian Peninsula, much of the literature has focused on Spanish. Brooke et al. (2009) proposed a lexiconbased SA system that defines rules at the lexical level to handle negation, intensification or adversative subordinate clauses. They followed a crosslingual approach, adapting their English method (Taboada et al., 2011) to obtain the semantic orientation (SO) of Spanish texts. Vilares et al. created a syntactic rule-based system, by making an interpretation of Brooke et al.’s system, but limited to AnCora trees (Taul´e et al., 2008). Mart´ınezC´amara et al. (2011) were one of the first to report a wide set of experiments on a number of 1. Syntactic structures can be defined in a universal way (Nivre et al., 2015). ∗ DV was funded by MECD (FPU13/01180). MG is funded by a Juan de la Cierva grant (FJCI-2014-22853). CGR has received funding from the ERC, under the European Union’s Horizon 2020 research and innov"
W17-5209,taule-etal-2008-ancora,0,0.0417147,"Missing"
W17-5209,W00-1308,0,0.301461,"Missing"
W17-5209,P02-1053,0,0.00879725,"Vilares et al.) models. In the latter case, these are mostly monolingual and dependent on the annotation of the training treebank, and so the rules are annotation-dependent too. Advances in NLP make it now possible to overcome such issues. We present a model that analyzes five official languages in the Iberian Peninsula: Basque (eu), Catalan (ca), Galician (gl), Portuguese (pt) and Spanish (es). We rely on three premises: 2 Related work Polarity classification has been addressed through machine learning (Mohammad et al., 2013; Socher et al., 2013; Vo and Zhang, 2016), and lexiconbased models (Turney, 2002). Most of the research involves English texts, although studies can be found for other languages such as Chinese (Chen and Chen, 2016) or Arabic (Shoukry and Rafea, 2012). For the official languages in the Iberian Peninsula, much of the literature has focused on Spanish. Brooke et al. (2009) proposed a lexiconbased SA system that defines rules at the lexical level to handle negation, intensification or adversative subordinate clauses. They followed a crosslingual approach, adapting their English method (Taboada et al., 2011) to obtain the semantic orientation (SO) of Spanish texts. Vilares et"
W18-1116,W03-0611,0,0.134159,"Missing"
W18-6019,P16-1231,0,0.0159201,"es annotated following common guidelines. This also makes it possible to extract a robust and fair comparative analysis. Introduction Transition-based models have achieved significant improvements in the last decade (Nivre et al., 2007; Chen and Manning, 2014; Rasooli and Tetreault, 2015; Shi et al., 2017). Some of them already achieve a level of agreement similar to that of experts on English newswire texts (Berzak et al., 2016), although this does not generalize to other configurations (e.g. lower-resource languages). These higher levels of accuracy often come at higher computational costs (Andor et al., 2016) and lower bandwidths, which can be a disadvantage for scenarios where speed is more relevant than accuracy (G´omez-Rodr´ıguez et al., 2017). Furthermore, running neural models on small devices for tasks such as part-of-speech tagging or word segmentation has become a matter of study (Botha et al., 2017), showing that small feed-forward networks are suitable for these challenges. However, for parsers that are trained using 2 2.1 Related Work Computational efficiency The usefulness of dependency parsing is partially thanks to the efficiency of existing transitionbased algorithms, although to th"
W18-6019,ballesteros-nivre-2012-maltoptimizer-system,0,0.0193464,"of existing transitionbased algorithms, although to the date it is an open question which algorithms suit certain languages better. To predict projective structures, a number of algorithms that run in O(n) with respect to the length of the input string are available. Broadly speaking, these parsers usually keep two 162 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 162–172 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics and hand-crafted (Huang et al., 2009; Zhang and Nivre, 2011) or automatically optimized sets of features (Ballesteros and Nivre, 2012). The goal usually is to maximize accuracy, which often comes at a cost of bandwidth. In this sense, efforts were made in order to obtain speed-ups. Using linear classifiers might lead to faster parsers, at a cost of accuracy and larger memory usage (Nivre and Hall, 2010). Bohnet (2010) illustrates that mapping the features into weights for a support vector machine is the major issue for the execution time and introduces a hash kernel approach to mitigate it. Volokh (2013) made efforts on optimizing the feature extraction time for the Covington (2001) algorithm, defining the concept of static"
W18-6019,D16-1239,0,0.018731,"possible to explore how the same configuration affects different languages. This study is made on the Universal Dependencies v2.1, a testbed that allows us to compare a variety of languages annotated following common guidelines. This also makes it possible to extract a robust and fair comparative analysis. Introduction Transition-based models have achieved significant improvements in the last decade (Nivre et al., 2007; Chen and Manning, 2014; Rasooli and Tetreault, 2015; Shi et al., 2017). Some of them already achieve a level of agreement similar to that of experts on English newswire texts (Berzak et al., 2016), although this does not generalize to other configurations (e.g. lower-resource languages). These higher levels of accuracy often come at higher computational costs (Andor et al., 2016) and lower bandwidths, which can be a disadvantage for scenarios where speed is more relevant than accuracy (G´omez-Rodr´ıguez et al., 2017). Furthermore, running neural models on small devices for tasks such as part-of-speech tagging or word segmentation has become a matter of study (Botha et al., 2017), showing that small feed-forward networks are suitable for these challenges. However, for parsers that are t"
W18-6019,D17-1309,0,0.0141216,"7). Some of them already achieve a level of agreement similar to that of experts on English newswire texts (Berzak et al., 2016), although this does not generalize to other configurations (e.g. lower-resource languages). These higher levels of accuracy often come at higher computational costs (Andor et al., 2016) and lower bandwidths, which can be a disadvantage for scenarios where speed is more relevant than accuracy (G´omez-Rodr´ıguez et al., 2017). Furthermore, running neural models on small devices for tasks such as part-of-speech tagging or word segmentation has become a matter of study (Botha et al., 2017), showing that small feed-forward networks are suitable for these challenges. However, for parsers that are trained using 2 2.1 Related Work Computational efficiency The usefulness of dependency parsing is partially thanks to the efficiency of existing transitionbased algorithms, although to the date it is an open question which algorithms suit certain languages better. To predict projective structures, a number of algorithms that run in O(n) with respect to the length of the input string are available. Broadly speaking, these parsers usually keep two 162 Proceedings of the Second Workshop on"
W18-6019,W06-2920,0,0.0781285,", A) where: = softmax (W2 ·relu(W1 ·v+b1 )+b2 ) (1) where Wi and bi are the weights and bias tensors to be learned at the ith layer and softmax and relu correspond to the activation functions in their standard form. MLPθ (v) 4.3 • σ is a stack that contains the words that are awaiting for remaining arcs to be created. In σ|i, i represents the first word of the stack. Universal Dependencies v2.1 Universal dependencies (UD) v2.1 (Nivre et al., 2017) is a set of 101 dependency treebanks for up to 60 different languages. They are labeled in the CoNLLU format, heavily inspired in the CoNLL format (Buchholz and Marsi, 2006). For each word in a sentence there is available the following information: ID, WORD, LEMMA, UPOSTAG (universal postag, available for all languages), XPOSTAG (language-specific postag, available for some languages), FEATS (additional morphosyntactic information, available for some languages), HEAD , DEPREL and other optional columns with additional information. • β is a buffer structure containing the words that still have not been processed (awaiting to be moved to σ. In i|β, i denotes the first word of the buffer. • A is the set of arcs that have been created. We rely on two transition-based"
W18-6019,D14-1082,0,0.686422,"is of help in downstream applications of natural language processing, such as those running on small devices and also of interest for syntactic parsing itself, as it makes it possible to explore how the same configuration affects different languages. This study is made on the Universal Dependencies v2.1, a testbed that allows us to compare a variety of languages annotated following common guidelines. This also makes it possible to extract a robust and fair comparative analysis. Introduction Transition-based models have achieved significant improvements in the last decade (Nivre et al., 2007; Chen and Manning, 2014; Rasooli and Tetreault, 2015; Shi et al., 2017). Some of them already achieve a level of agreement similar to that of experts on English newswire texts (Berzak et al., 2016), although this does not generalize to other configurations (e.g. lower-resource languages). These higher levels of accuracy often come at higher computational costs (Andor et al., 2016) and lower bandwidths, which can be a disadvantage for scenarios where speed is more relevant than accuracy (G´omez-Rodr´ıguez et al., 2017). Furthermore, running neural models on small devices for tasks such as part-of-speech tagging or wo"
W18-6019,K17-3022,0,0.266069,"Missing"
W18-6019,P14-1129,0,0.0923904,"Missing"
W18-6019,J93-2004,0,0.077716,"chines, 163 3 Motivation to manage non-projective structures. The election of the algorithms is based on their computational complexity as both run in O(n) empirically. The set of transitions is shown in Table 1. Let ci = ([0], β, {}) be an initial configuration, the parser will apply transitions until a final configuration cf = ([0], [], A) is reached. Transition-based dependency parsers whose oracles are trained using feed-forward neural networks have adopted as the de facto standard set of features the one proposed by Chen and Manning (2014) to parse the English and Chinese Penn Treebanks (Marcus et al., 1993; Xue et al., 2005). We hypothesize this de facto standard set of features and the size of the embeddings used to represent them can be reduced for a wide variety of languages, obtaining significant speed-ups at a cost of a marginal impact on their performance. To test this hypothesis, we are performing an evaluation over the Universal Dependencies v2.1 (Nivre et al., 2017) a wide multilingual testbed to approximate relevant features over a wide variety of languages from different families. Transition Step t (σ|i|j, β, A) (σ|i|j, β, A) (σ, i|β, A) (σ|i|j, β, A) STANDARD LEFT- ARC l (projective"
W18-6019,N18-2109,1,0.88142,"Missing"
W18-6019,H05-1066,0,0.277385,"Missing"
W18-6019,W04-0308,0,0.14508,"hash kernel approach to mitigate it. Volokh (2013) made efforts on optimizing the feature extraction time for the Covington (2001) algorithm, defining the concept of static features, which can be reused through different configuration steps. The concept itself does not imply a reduction in terms of efficiency, but it is often employed in conjunction with the reduction of nonstatic features, which causes a drop in accuracy. structures: a stack (containing the words that are waiting for some arcs to be created) and a buffer (containing words awaiting to be processed). The ARC - STANDARD parser (Nivre, 2004) follows a strictly bottom-up strategy, where a word can only be assigned a head (and removed from the stack) once every daughter node has already been processed. The ARC - EAGER parser avoids this restriction by including a specific transition for the reduce action. The ARC - HYBRID algorithm (Kuhlmann et al., 2011) mixes characteristics of both algorithms. More recent algorithms, such as ARC - SWIFT, have focused on the ability to manage non-local transitions (Qi and Manning, 2017) to reduce the limitations of transition-based parsers with respect to graph-based ones (McDonald et al., 2005;"
W18-6019,J08-4003,0,0.0536428,"ble the following information: ID, WORD, LEMMA, UPOSTAG (universal postag, available for all languages), XPOSTAG (language-specific postag, available for some languages), FEATS (additional morphosyntactic information, available for some languages), HEAD , DEPREL and other optional columns with additional information. • β is a buffer structure containing the words that still have not been processed (awaiting to be moved to σ. In i|β, i denotes the first word of the buffer. • A is the set of arcs that have been created. We rely on two transition-based algorithms: the stack-based ARC - STANDARD (Nivre, 2008) algorithm for projective parsing and its corresponding version with the SWAP operation (Nivre, 2009) 164 In this paper, we are only considering experiments on the unsuffixed treebanks (where UD English is an unsuffixed treebank and UD English-PUD is a suffixed treebank). The motivation owes to practical issues and legibility of tables and discussions. 5 and 66 different features. In the case of UD treebanks, it is worth noting that for some languages the FEATS features are not available. We thought of two strategies in this situation: (1) not to consider any FEATS vector as input or (2) assum"
W18-6019,P09-1040,0,0.431581,"d on the ability to manage non-local transitions (Qi and Manning, 2017) to reduce the limitations of transition-based parsers with respect to graph-based ones (McDonald et al., 2005; Dozat and Manning, 2017), that consider a more global context. To manage non-projective structures, there are also different options available. The Covington (2001) algorithm runs in O(n2 ) in the worst case, by comparing the word in the top of the buffer with a subset of the words that have been already processed, deciding whether or not to create a link with each of them. More efficient algorithms such as SWAP (Nivre, 2009) manage non-projectivity by learning when to swap pairs of words that are involved in a crossing arc, transforming it into a projective problem, with expected execution in linear time. The 2- PLANAR algorithm (G´omezRodr´ıguez and Nivre, 2010) decomposes trees into at most two planar graphs, which can be used to implement a parser that runs in linear time. The NON - LOCAL COVINGTON algorithm (Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) combines the advantages of the wide coverage of the Covington (2001) algorithm with the non-local capabilities of the Qi and Manning (2017) transition syst"
W18-6019,P10-1151,1,0.814801,"Missing"
W18-6019,D09-1127,0,0.0514346,"Missing"
W18-6019,Q16-1023,0,0.528458,"at grand-daughter features can be removed for the majority of treebanks without a significant (negative or positive) LAS difference. They also show how the size of the embeddings can be notably reduced. 1 Contribution We explore efficient and light dependency parsers for languages with a variety of structures and morphologies. We rely on neural feed-forward dependency parsers, since their architecture offers a competitive accuracy vs bandwidth ratio and they are also the inspiration for more complex parsers, which also rely on embedded features but previously processed by bidirectional LSTMs (Kiperwasser and Goldberg, 2016). In particular, we study if the de facto standard embedded features and their sizes can be reduced without having a significant impact on their accuracy. Building these models is of help in downstream applications of natural language processing, such as those running on small devices and also of interest for syntactic parsing itself, as it makes it possible to explore how the same configuration affects different languages. This study is made on the Universal Dependencies v2.1, a testbed that allows us to compare a variety of languages annotated following common guidelines. This also makes it"
W18-6019,P11-1068,1,0.918574,"Missing"
W18-6019,P17-2018,0,0.120096,"aiting for some arcs to be created) and a buffer (containing words awaiting to be processed). The ARC - STANDARD parser (Nivre, 2004) follows a strictly bottom-up strategy, where a word can only be assigned a head (and removed from the stack) once every daughter node has already been processed. The ARC - EAGER parser avoids this restriction by including a specific transition for the reduce action. The ARC - HYBRID algorithm (Kuhlmann et al., 2011) mixes characteristics of both algorithms. More recent algorithms, such as ARC - SWIFT, have focused on the ability to manage non-local transitions (Qi and Manning, 2017) to reduce the limitations of transition-based parsers with respect to graph-based ones (McDonald et al., 2005; Dozat and Manning, 2017), that consider a more global context. To manage non-projective structures, there are also different options available. The Covington (2001) algorithm runs in O(n2 ) in the worst case, by comparing the word in the top of the buffer with a subset of the words that have been already processed, deciding whether or not to create a link with each of them. More efficient algorithms such as SWAP (Nivre, 2009) manage non-projectivity by learning when to swap pairs of"
W18-6019,D17-1002,0,0.173311,"Missing"
W18-6019,D07-1099,0,0.0328834,"ctive problem, with expected execution in linear time. The 2- PLANAR algorithm (G´omezRodr´ıguez and Nivre, 2010) decomposes trees into at most two planar graphs, which can be used to implement a parser that runs in linear time. The NON - LOCAL COVINGTON algorithm (Fern´andezGonz´alez and G´omez-Rodr´ıguez, 2018) combines the advantages of the wide coverage of the Covington (2001) algorithm with the non-local capabilities of the Qi and Manning (2017) transition system, running in quadratic time in the worst case. 2.2 In more modern parsers, the oracles are trained using feed-forward networks (Titov and Henderson, 2007; Chen and Manning, 2014; Straka et al., 2015) and sequential models (Kiperwasser and Goldberg, 2016). In this sense, to obtain significant speed improvements it is common to use the pre-computation trick from Devlin et al. (2014), initially intended for machine translation. Broadly speaking, they precompute the output of the hidden layer for each individual feature and each position in the input vector where they might occur, saving computation time during the test phase, with an affordable memory cost. Vacariu (2017) proposes an optimized parser and also includes a brief evaluation about red"
W18-6019,K17-3016,1,0.888494,"Missing"
W18-6019,P11-2033,0,0.0810117,"fulness of dependency parsing is partially thanks to the efficiency of existing transitionbased algorithms, although to the date it is an open question which algorithms suit certain languages better. To predict projective structures, a number of algorithms that run in O(n) with respect to the length of the input string are available. Broadly speaking, these parsers usually keep two 162 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 162–172 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics and hand-crafted (Huang et al., 2009; Zhang and Nivre, 2011) or automatically optimized sets of features (Ballesteros and Nivre, 2012). The goal usually is to maximize accuracy, which often comes at a cost of bandwidth. In this sense, efforts were made in order to obtain speed-ups. Using linear classifiers might lead to faster parsers, at a cost of accuracy and larger memory usage (Nivre and Hall, 2010). Bohnet (2010) illustrates that mapping the features into weights for a support vector machine is the major issue for the execution time and introduces a hash kernel approach to mitigate it. Volokh (2013) made efforts on optimizing the feature extractio"
W19-7815,E99-1014,0,0.0979473,"as input to augment sequence-labelling dependency parsing. Finally, we investigate the impact chunks have on dependency parsing in a multi-task framework. Our results from these analyses show that these chunks improve performance at different levels of syntactic abstraction on English UD treebanks and a small, diverse subset of non-English UD treebanks. 1 Introduction Shallow parsing, or chunking, consists of identifying constituent phrases (Abney, 1997). As such, it is fundamentally associated with constituency parsing, as it can be used as a first step for finding a full constituency tree (Ciravegna and Lavelli, 1999; Tsuruoka and Tsujii, 2005). However, chunking information can also be beneficial for dependency parsing (Attardi and DellOrletta, 2008; Tammewar et al., 2015), and vice versa (Kutlu and Cicekli, 2016). Latterly, Lacroix (2018) explored the efficacy of noun phrase (NP) chunking with respect to universal dependency (UD) parsing and POS tagging for English treebanks. As UD treebanks do not contain chunking annotation, they deduced chunks by adopting linguistic-based phrase rules. They observed improvements on POS and morphological feature tagging in a shared multi-task framework for the English"
W19-7815,W18-6010,0,0.0951748,"nt levels of syntactic abstraction on English UD treebanks and a small, diverse subset of non-English UD treebanks. 1 Introduction Shallow parsing, or chunking, consists of identifying constituent phrases (Abney, 1997). As such, it is fundamentally associated with constituency parsing, as it can be used as a first step for finding a full constituency tree (Ciravegna and Lavelli, 1999; Tsuruoka and Tsujii, 2005). However, chunking information can also be beneficial for dependency parsing (Attardi and DellOrletta, 2008; Tammewar et al., 2015), and vice versa (Kutlu and Cicekli, 2016). Latterly, Lacroix (2018) explored the efficacy of noun phrase (NP) chunking with respect to universal dependency (UD) parsing and POS tagging for English treebanks. As UD treebanks do not contain chunking annotation, they deduced chunks by adopting linguistic-based phrase rules. They observed improvements on POS and morphological feature tagging in a shared multi-task framework for the English treebanks in UD version 2.1 (Nivre et al., 2017). However, an increase in performance for parsing was only obtained for one treebank. Contribution 1. We first relax the standard definition of chunks and present an evolutionary"
W19-7815,K18-2008,0,0.0259903,"English treebanks. In the same table, it is clear that they do not aid Bulgarian, but they do improve POS tagging performance for German and Japanese. Table 3 shows that chunking performance consistently improves in the multi-task setting. Parsing performance is improved across all treebanks when the predictions from experiment 1 are used as features (Table 4), but only for English-EWT (the largest treebank) and ParTUT (the smallest) do the predicted chunks explicitly improve performance and for the other treebanks only the other predicted features help. This is in contrast to the findings of Nguyen and Verspoor (2018), who obtained higher performance for larger treebanks. In the multi-task setting for the dependency parser (Table 5), the chunking information consistently aids udpipe single pos+feats pos+feats+chunks75 pos+feats+chunks95 ewt pos feats 94.44 95.37 95.08 96.09 95.23 96.21 95.89 96.72 95.86 96.52 gum pos feats 93.88 94.21 94.61 94.92 94.60 95.26 95.58 96.31 95.52 96.21 bg pos 97.78 97.41 97.69 97.49 97.44 udpipe single pos+feats pos+feats+chunks75 pos+feats+chunks95 lines pos feats 94.73 94.83 95.64 95.57 95.59 95.71 96.38 96.45 96.35 96.33 partut pos feats 94.10 94.01 94.69 94.54 94.63 94.16"
W19-7815,P16-2038,0,0.0348743,"ork are continuous word representations and character embeddings. In this paper we used NCRF++ (Yang and Zhang, 2018), which uses stacked BiLSTMs, to generate contextualised hidden representations for every word (~hi ) in the input sentence. For decoding, it uses a feed-forward layer followed by a softmax activation: ~ × ~hi +~b) P(y|~hi ) = softmax(W (3) The single task models are optimised with cross-entropy loss, L , defined as: L = − ∑ log(P(y|hi ) (4) For the multi-task learning models, we implemented a hard-sharing architecture, where all the stacked BiLSTMs are shared across all tasks (Søgaard and Goldberg, 2016) . A separate feed-forward layer (as the one used in the single task setup) is used to decode the output for each task. With respect to the computation of the loss under the multi-task learning (MTL) setup, LMT L , is defined as: LMT L = ∑ βt Lt (5) t∈T where t is a task from the set of all tasks, T ; βt is the corresponding weight for task t; and Lt is the cross-entropy loss for task t. A schematic of the network can be seen in Figure 3. 4.1 Dependency parsing as sequence labelling In order to more readily utilise the multi-task framework for dependency parsing, we have cast dependency parsin"
W19-7815,N19-1077,1,0.889924,"Missing"
W19-7815,W05-1514,0,0.0528716,"-labelling dependency parsing. Finally, we investigate the impact chunks have on dependency parsing in a multi-task framework. Our results from these analyses show that these chunks improve performance at different levels of syntactic abstraction on English UD treebanks and a small, diverse subset of non-English UD treebanks. 1 Introduction Shallow parsing, or chunking, consists of identifying constituent phrases (Abney, 1997). As such, it is fundamentally associated with constituency parsing, as it can be used as a first step for finding a full constituency tree (Ciravegna and Lavelli, 1999; Tsuruoka and Tsujii, 2005). However, chunking information can also be beneficial for dependency parsing (Attardi and DellOrletta, 2008; Tammewar et al., 2015), and vice versa (Kutlu and Cicekli, 2016). Latterly, Lacroix (2018) explored the efficacy of noun phrase (NP) chunking with respect to universal dependency (UD) parsing and POS tagging for English treebanks. As UD treebanks do not contain chunking annotation, they deduced chunks by adopting linguistic-based phrase rules. They observed improvements on POS and morphological feature tagging in a shared multi-task framework for the English treebanks in UD version 2.1"
W19-7815,N19-1341,1,0.842004,"which is the subject of a verb (son in the input sentence in Figure 3) would have a label of +1,nsubj,VERB, where +1 indicates the head is the next VERB in the sentence and nsubj is the relation label. 5 Experiments Data The analyses were undertaken using the English treebanks (EWT, GUM, LinES, and ParTUT) and also Bulgarian-BTB, German-GSD, and Japanese-GSD from UD v2.3 (Nivre et al., 2018). No results are given for Japanese-GSD for morphological feature tagging as it does not contain this information. Network hyperparameters We used the framework as described above and hyperparameters from Vilares et al. (2019) which can be seen in Table 8 in the Appendix B. The standard input to the system consisted of word embeddings concatenated with character embeddings. All embeddings were randomly initialised. Figure 3: Multi-task architecture shown with sequence-labelling dependency parsing (as described in subsection 4.1), POS tagging, and chunking as shared tasks. Network input is a concatenation of word embeddings (circles) and character-level word embeddings (triangles) obtained from a character-based LSTM layer. The network is constructed of BiLSTM layers followed by a softmax layer for inference. Experi"
W19-7815,P18-4013,0,0.0294479,"Missing"
W19-7815,D14-1162,0,\N,Missing
