2021.findings-emnlp.256,P13-3005,0,0.0730578,"Missing"
2021.findings-emnlp.256,2020.lrec-1.633,0,0.0308458,"Missing"
2021.findings-emnlp.256,W16-1202,0,0.036137,"Missing"
2021.findings-emnlp.256,W17-6314,0,0.0273297,"Missing"
2021.findings-emnlp.256,W18-6008,0,0.0169258,"mes using a range of quantitative syntactic properties, which may also reflect their learnability. The results of the experiments show that SUD tends to be more learnable than UD, but the advantage of one or the other scheme depends on the parser and the corpus in question. 1 Introduction and Background 2016), favours content words over function words as dependency heads, as this increases crosslinguistic uniformity of the resulting scheme; here we will call this approach ‘semantic’.1 Another, represented by Surface-Syntactic Universal Dependencies (SUD; https://surfacesyntacticud. github.io; Gerdes et al., 2018, 2019), uses purely syntactic criteria for determining headedness; hence the moniker ‘syntactic’. The SUD scheme was designed as minimally different from – ‘near-isomorphic to’ – UD, and many UD treebanks have been converted to SUD, so differences in learnability between the two approaches should be relatively easy to assess and interpret. As is clear from Figure 1, which juxtaposes the UD basic tree (at the top) and the SUD tree (at the bottom), SUD generally adopts the principle that function words such as auxiliaries (e.g., do), subordinating conjunctions (until), copula (’re), and preposi"
2021.findings-emnlp.256,W19-7814,0,0.0404339,"Missing"
2021.findings-emnlp.256,L18-1550,0,0.0283703,"u is represented in the conllu scheme by two lines (one corresponding to de with information ‘SpaceAfter=No’ and the other to le) rather than three (one for de, another for le and another for their contraction du). This was done to remove some inconsistencies between training and testing subsets of some corpora. Additionally, all tokens with PUNCT as their UPOS tag were removed, unless they had dependents. Pretrained embeddings. Where possible (i.e., in the case of UDPipe, UUParser, and COMBO), pretrained fasttext word embeddings were utilised (https://fasttext.cc/docs/en/ crawl-vectors.html; Grave et al., 2018) as opposed to learning embeddings during the training process. The fasttext architecture is based on embeddings of character n-grams, but only the resulting word-level vectors were used in the training procedure, as all of the selected systems which offer an option of including external embeddings can work with word embeddings only. Each embedding model was pruned to 300,000 most frequent forms, to ease the computational load. 2.2 Parsers Two transition-based and three graph-based parsers were used in the experiments. Some of these tools offer robust pipelines for NLP, including tokenisation,"
2021.findings-emnlp.256,Q16-1025,0,0.020852,"introduced because the SUD scheme has substantially smaller sets of labels for dependency relations, and LE offers a more informed way of assessing the baseline difficulty of a label scheme than the mere cardinality of the labelset. SUD versions of the same treebanks are in all cases characterized by lower LE. Both measures were calculated using the dependency label transformations defined in §2.3, i.e., with the exception of some SUD labels, all labels were split after a colon. We were not able to confirm the correlation between differences in learnability and differences in ADE reported in Gulordava and Merlo (2016) (on the basis of artificially created data) and in Rehbein et al. (2017). Most probably, this is because of the very small differences in ADE between SUD and UD, much lower than in the experiments cited in these two papers. In fact, the differences are so insignificant that we would prefer to be cautious in interpreting the one statistically significant, positive correlation that concerns COMBO parser. Following the results from the papers cited above, one would expect to obtain a negative correlation between ADE and learnability (i.e. higher ADE leads to lower learnability). The opposite is"
2021.findings-emnlp.256,Q16-1023,0,0.0189361,"et, 2010) of the graph-based parser was utilised; it was adapted from the version 3.61 (available here: http://code.google.com/p/mate-tools/) to our study.6 Seven models were trained for each treebank (and each annotation scheme), and in every training run a different non-projective approximation threshold was selected from the following list: 0.75, 0.5, 0.4, 0.3, 0.2, 0.15, 0.1. UUParser. Version 2.4 (https://github.com/ UppsalaNLP/uuparser; de Lhoneux et al., 2017) of both graph- and transition-based methodologies were applied in the experiment. UUParser is an adaptation of the BIST parser (Kiperwasser and Goldberg, 2016). In UUParser, swap transition and Eisner algorithms were implemented, among others, in place of their projective counterparts – used by BIST parser – in transition- and graph-based versions respectively. Universal POS tags dimension was set to 20 and external word embedding dimension was adapted to the size of the embeddings used. Five models with different random seeds were trained, and the one which performed best as measured by LAS on the dev set, was then selected for testing. COMBO. The graph-based dependency parsing component from Version 1.0.1 https://gitlab. clarin-pl.eu/syntactic-too"
2021.findings-emnlp.256,E17-2001,0,0.069858,"e case of copula–predicate constructions. Other papers that report better learn2 The other two constructions, which have the same representation in the two schemes, are: noun–noun (e.g., John Doe), determiner–noun (e.g., the apple). ability of a more syntactic scheme converted automatically from a more semantic scheme include: Nilsson et al. (2006, 2007) (auxiliary–verb constructions in Arabic, Czech, Dutch and Slovene, small improvement observed in the case of the transition based MaltParser, but not with the graphbased MSTParser), Rosa (2015) (adposition–noun constructions in 30 languages), Kohita et al. (2017) (various constructions involving function and content words in 19 typologically varied languages), and Rehbein et al. (2017) (15 languages, although the extent of the improvements varied considerably, and in the exceptional case of Turkish regress was observed for all three parsers used in the experiments).3 On the other hand, de Lhoneux and Nivre (2016) report on an experiment involving 24 languages, in which the original UD representation of verb groups (modal–verb constructions) turns out to be more learnable by MaltParser than the converted representation with main verbs acting as depende"
2021.findings-emnlp.256,P06-1033,0,0.0949534,"ements in the learnability of the syntactic scheme in the case of preposition–noun (but not complementiser–clause), auxiliary–verb (rather than the more general modal–verb, considered in Schwartz et al., 2012) and – and this is were the improvement was most clear – in the case of copula–predicate constructions. Other papers that report better learn2 The other two constructions, which have the same representation in the two schemes, are: noun–noun (e.g., John Doe), determiner–noun (e.g., the apple). ability of a more syntactic scheme converted automatically from a more semantic scheme include: Nilsson et al. (2006, 2007) (auxiliary–verb constructions in Arabic, Czech, Dutch and Slovene, small improvement observed in the case of the transition based MaltParser, but not with the graphbased MSTParser), Rosa (2015) (adposition–noun constructions in 30 languages), Kohita et al. (2017) (various constructions involving function and content words in 19 typologically varied languages), and Rehbein et al. (2017) (15 languages, although the extent of the improvements varied considerably, and in the exceptional case of Turkish regress was observed for all three parsers used in the experiments).3 On the other hand,"
2021.findings-emnlp.256,P07-1122,0,0.145007,"Missing"
2021.findings-emnlp.256,L16-1262,0,0.0761687,"Missing"
2021.findings-emnlp.256,W17-6525,0,0.0608595,"same representation in the two schemes, are: noun–noun (e.g., John Doe), determiner–noun (e.g., the apple). ability of a more syntactic scheme converted automatically from a more semantic scheme include: Nilsson et al. (2006, 2007) (auxiliary–verb constructions in Arabic, Czech, Dutch and Slovene, small improvement observed in the case of the transition based MaltParser, but not with the graphbased MSTParser), Rosa (2015) (adposition–noun constructions in 30 languages), Kohita et al. (2017) (various constructions involving function and content words in 19 typologically varied languages), and Rehbein et al. (2017) (15 languages, although the extent of the improvements varied considerably, and in the exceptional case of Turkish regress was observed for all three parsers used in the experiments).3 On the other hand, de Lhoneux and Nivre (2016) report on an experiment involving 24 languages, in which the original UD representation of verb groups (modal–verb constructions) turns out to be more learnable by MaltParser than the converted representation with main verbs acting as dependents of modal verbs. In a similar vein, Wisniewski and Lacroix (2017) report that languages and particular constructions vary"
2021.findings-emnlp.256,W15-2131,0,0.0234617,"and – and this is were the improvement was most clear – in the case of copula–predicate constructions. Other papers that report better learn2 The other two constructions, which have the same representation in the two schemes, are: noun–noun (e.g., John Doe), determiner–noun (e.g., the apple). ability of a more syntactic scheme converted automatically from a more semantic scheme include: Nilsson et al. (2006, 2007) (auxiliary–verb constructions in Arabic, Czech, Dutch and Slovene, small improvement observed in the case of the transition based MaltParser, but not with the graphbased MSTParser), Rosa (2015) (adposition–noun constructions in 30 languages), Kohita et al. (2017) (various constructions involving function and content words in 19 typologically varied languages), and Rehbein et al. (2017) (15 languages, although the extent of the improvements varied considerably, and in the exceptional case of Turkish regress was observed for all three parsers used in the experiments).3 On the other hand, de Lhoneux and Nivre (2016) report on an experiment involving 24 languages, in which the original UD representation of verb groups (modal–verb constructions) turns out to be more learnable by MaltPars"
2021.findings-emnlp.256,K18-2004,0,0.0217418,"nsition and Eisner algorithms were implemented, among others, in place of their projective counterparts – used by BIST parser – in transition- and graph-based versions respectively. Universal POS tags dimension was set to 20 and external word embedding dimension was adapted to the size of the embeddings used. Five models with different random seeds were trained, and the one which performed best as measured by LAS on the dev set, was then selected for testing. COMBO. The graph-based dependency parsing component from Version 1.0.1 https://gitlab. clarin-pl.eu/syntactic-tools/combo of the COMBO (Rybak and Wróblewska, 2018) pipeline was utilized, with word embeddings, characters, and gold UPOS tags as features. For each treebank four models with different combinations of learning rate (0.001 or 0.002) and dropout probability (0.4 or 0.25) have been trained, for 100 epochs each. 6 The implemented change forces the parser to produce only one root in each sentence. We thank Bernd Bohnet for adjusting the parser to our needs and for allowing to share the new Mate 3.62 version on our Github page. 2.3 Evaluation In each case, models produced by the parsers on the basis of training sets were used to parse the test part"
2021.findings-emnlp.256,C12-1147,0,0.0413583,"Missing"
2021.findings-emnlp.256,W15-2134,0,0.0173653,"ent scores). The results of these experiments favour SUD-like representations in all four cases. In the case of constructions involving a preposition or a complementiser, having them as heads – as in SUD, but unlike in UD – results in extremely strong (‘unanimous’) learnability improvements. The effect is weaker in the case of verb groups containing a modal and still weaker in the case of infinitivals introduced by to, but in both cases having the main lexical verb as the dependent – as in SUD, but unlike in UD – gives generally better results. A similar range of constructions is inspected in Silveira and Manning (2015). For each kind of construction, 3 different variants of conversion from semantic to syntactic headedness are considered, depending on how many of the dependents of the semantic head are moved to the syntactic head. The best variant gives significant improvements in the learnability of the syntactic scheme in the case of preposition–noun (but not complementiser–clause), auxiliary–verb (rather than the more general modal–verb, considered in Schwartz et al., 2012) and – and this is were the improvement was most clear – in the case of copula–predicate constructions. Other papers that report bette"
2021.findings-emnlp.256,K17-3009,0,0.0192972,"for NLP, including tokenisation, lemmatisation and tagging, but in the current experiments only the parser component of the tool was trained; in particular, POS tags were extracted from the gold standard and used as features. Below, training procedures of each parser are described separately, including only information about parsers’ hyperparameters that differ from the default setting.5 UDPipe. Version 1.2.0 (http://ufal.mff. 5 In total 1764 models were trained (882 with UDPipe, 294 with Mate, 210 with each graph-based and transition-based UUParser, and 168 with COMBO). 2989 cuni.cz/udpipe; Straka and Straková, 2017) of this transition-based parser was used without the default values of various hyperparameters, as these were fitted on UD, and thus could skew the results against SUD. Instead, 21 models were trained on each treebank (for either annotation scheme). That is, for each transition system available – projective, swap, link2 – seven models were trained using random hyperparameter search – a feature provided by UDPipe that randomises some of the training hyperparameters. Mate. Version 3.62 (Bohnet, 2010) of the graph-based parser was utilised; it was adapted from the version 3.61 (available here: h"
2021.findings-emnlp.256,W17-0419,0,0.0139162,"ion and content words in 19 typologically varied languages), and Rehbein et al. (2017) (15 languages, although the extent of the improvements varied considerably, and in the exceptional case of Turkish regress was observed for all three parsers used in the experiments).3 On the other hand, de Lhoneux and Nivre (2016) report on an experiment involving 24 languages, in which the original UD representation of verb groups (modal–verb constructions) turns out to be more learnable by MaltParser than the converted representation with main verbs acting as dependents of modal verbs. In a similar vein, Wisniewski and Lacroix (2017) report that languages and particular constructions vary drastically in the extent to which the syntactic or the semantic approach to headedness is more or less learnable by their own transition-based parser. However, out of the seven constructions they consider (similar to those considered in Silveira and Manning, 2015), four differentiate UD and SUD, and out of these four, two (copula–predicate and case–noun, but not mark–verb) are more learnable in the syntactic encoding in the majority of languages – copula–predicate con3 See also Ivanova et al. (2013) and Kirilin and Versley (2015), where"
buczynski-przepiorkowski-2008-demo,A00-1033,0,\N,Missing
buczynski-przepiorkowski-2008-demo,J93-2002,0,\N,Missing
buczynski-przepiorkowski-2008-demo,P93-1032,0,\N,Missing
buczynski-przepiorkowski-2008-demo,P07-2022,1,\N,Missing
buczynski-przepiorkowski-2008-demo,przepiorkowski-etal-2004-search,1,\N,Missing
buczynski-przepiorkowski-2008-demo,felipe-zamorano-2000-pos,0,\N,Missing
C10-2001,A97-1017,0,0.095517,"Missing"
C10-2001,P07-2022,1,0.784107,"Missing"
C10-2001,W03-2415,1,0.830942,"Missing"
C12-1134,patejuk-przepiorkowski-2012-towards,1,0.890298,"uistic journals. One of the topics that has received much attention in formal theoretical linguistics is coordination, a phenomenon which is not only theoretically challenging, but also – due to its textual frequency – crucial to grammar engineering. Unfortunately, as coordination remains difficult to describe accurately and exhaustively, this theoretical interest is not fully reflected in existing grammar implementations. The aim of this paper is to present a comprehensive implementation of constituent coordination, a part of an ongoing effort to develop a wide-coverage LFG parser of Polish (Patejuk and Przepiórkowski, 2012b). Polish is a good test-bed for the task at hand, as it offers a wide range of interactions between coordination on one hand and various agreement, case assignment and valence phenomena on the other. One aspect of coordination that has remained especially elusive in grammar engineering is the possibility to coordinate elements which are unlike in some sense. Polish offers a much wider range of unlike constituent coordination than has been discussed in NLP, including the so-called lexico-semantic coordination (Kallas 1993; Chaves and Paperno 2007; Gazdik 2010), i.e., the coordination of very"
C12-1134,przepiorkowski-etal-2010-recent,1,0.840642,"xample is given below: (29) Jana dziwiło, [˙ze Maria wybiera Piotra], i [jej brak gustu]. Jan.ACC puzzled.3.SG.N that Maria chooses Piotr and her lack of taste.NOM.SG ‘(The fact) that Maria prefers Piotr and her lack of taste puzzled Jan.’ ´ (Swidzi´ nski, 1992, 1993) For such verbs, the following constraint holds: 12 See Przepiórkowski 1999 for extensive justification. We limit our considerations to arguments of verbs here. Where necessary, square brackets are used for the purpose of grouping constraints. “NKJP” marks attested examples found in the National Corpus of Polish (http://nkjp.pl/; Przepiórkowski et al. 2010, 2012). 13 14 2197 (30) (↑ SUBJ ) PRED [(← ACM) =c REC ∧ (← CASE) =c ACC] ∨ (← CASE) =c NOM ∨ (← COMP-FORM) =c Z˙ E A similar account can be offered for syntactically case-assigned objects, which – for some verbs – may be alternatively realised as clauses. Consider the following examples: (31) (32) Doradził mu [wyjazd] i [˙zeby nie wracał]. advised him leave.ACC and that NEG come back ‘He advised him to leave and not to come back.’ Kallas (1993) (Wcale) nie doradził mu [wyjazdu] ani [˙zeby nie wracał]. not at all NEG advised him leave.GEN nor that NEG come back ‘He did not advise him to leave"
C18-1324,kingsbury-palmer-2002-treebank,0,0.341204,"pendent of RECALL The argument/adjunct status of various depictive dependents is perhaps even more controversial than that of other types of dependents. The Valency Dictionary of English (Herbst et al. 2004) mentions the depictive dependent introduced by as in the case of the verb REMEMBER (and illustrates it with “I remember her as pretty and sort of tallish”), but not in the case of RECALL. On the other hand, this type of dependent is considered an argument (or core) of both verbs in both FrameNet (Ruppenhofer et al. 2016) (both verbs evoke the Remembering_experience frame) and in PropBank (Kingsbury and Palmer 2002) (rolesets remember.01 and recall.02).15 Moreover, neither dictionary mentions the possibility of an as-less depictive in the case of REMEMBER, as in “I look at Macie [. . . ] and try to remember him young” (abridged from the iWeb corpus). Given the level of uncertainty about the argument/adjunct status of such secondary predicates, UD representations should not differ dramatically depending on their AAD classification. But – as discussed in the main text and illustrated in (9)–(12) – they currently do. Appendix C proposes a solution that minimises such differences – see (48)–(51) there. B Coo"
C18-1324,W00-2021,0,0.0199213,"rything.ACC and everyone.DAT ‘One may promise everything to everyone.’ As discussed in Patejuk and Przepiórkowski 2012a, 2012b and in Paperno 2012, such constructions are limited to certain classes of pronouns and quantifiers, including question pronouns (so-called wh-words), negative pronouns (so-called n-words) and pronominal-like words expressing existential or universal quantifiers (the latter illustrated in (41)). Again, such exceptional constructions are easily distinguished from run-of-the-mill cases of coordination, where the sameness of grammatical functions is preserved. 15 VerbNet (Kipper et al. 2000, Kipper et al. 2006) does not seem to contain the relevant meaning of discussed here. 16 http://universaldependencies.org/u/overview/syntax.html 17 The labels obj and iobj reflect how this example would be annotated in Polish UD treebanks. 3847 RECALL , so it is not C Open Dependents in UD UD assumes that obligatory control only targets object-like core dependents, and not subjects or obliques. This is a reasonable first approximation, but cross-lingual facts show that it is ultimately false. First, obligatory control into subjects, while rare, occurs in languages as diverse as Balinese and P"
C18-1324,kipper-etal-2006-extending,0,0.0146403,"yone.DAT ‘One may promise everything to everyone.’ As discussed in Patejuk and Przepiórkowski 2012a, 2012b and in Paperno 2012, such constructions are limited to certain classes of pronouns and quantifiers, including question pronouns (so-called wh-words), negative pronouns (so-called n-words) and pronominal-like words expressing existential or universal quantifiers (the latter illustrated in (41)). Again, such exceptional constructions are easily distinguished from run-of-the-mill cases of coordination, where the sameness of grammatical functions is preserved. 15 VerbNet (Kipper et al. 2000, Kipper et al. 2006) does not seem to contain the relevant meaning of discussed here. 16 http://universaldependencies.org/u/overview/syntax.html 17 The labels obj and iobj reflect how this example would be annotated in Polish UD treebanks. 3847 RECALL , so it is not C Open Dependents in UD UD assumes that obligatory control only targets object-like core dependents, and not subjects or obliques. This is a reasonable first approximation, but cross-lingual facts show that it is ultimately false. First, obligatory control into subjects, while rare, occurs in languages as diverse as Balinese and Polish. For example, A"
C18-1324,C12-1134,1,0.816495,"ion, both very constrained empirically. The first (pointed to us by Amir Zeldes, p.c.) is sylleptic zeugma, as in: (40) He made [[his apologies]obj and [for the door]obl ]. Such constructions, in which the two conjuncts invoke two different meanings of the head, have a metalinguistic feel and they are easy to distinguish from genuine coordination. The second exception is the so-called lexico-semantic coordination (Sannikov 1979, 1980), occurring mainly in Slavic and in some neighbouring languages (Paperno 2012), as in the following sentence from the National Corpus of Polish (cited here after Patejuk and Przepiórkowski 2012b: 463):17 (41) Obieca´c moz˙ na [[wszystko]obj i [wszystkim]iobj ]. promise.INF may everything.ACC and everyone.DAT ‘One may promise everything to everyone.’ As discussed in Patejuk and Przepiórkowski 2012a, 2012b and in Paperno 2012, such constructions are limited to certain classes of pronouns and quantifiers, including question pronouns (so-called wh-words), negative pronouns (so-called n-words) and pronominal-like words expressing existential or universal quantifiers (the latter illustrated in (41)). Again, such exceptional constructions are easily distinguished from run-of-the-mill cases"
C18-1324,W17-6532,0,0.0829228,". . 13 14 http://universaldependencies.org/u/dep/iobj.html http://universaldependencies.org/u/dep/obj.html 3845 This supports the decision to treat both realisations – bare nominal and prepositional – as oblique (given that only subjects and direct objects are core). 4.3 Limited Reintroduction of AAD Of course, any proposal to relegate to the obliques various dependents treated so far as core only increases the sometimes perceived need to distinguish between argument-like obliques and adjunct-like obliques. To accommodate this desire, we propose to adopt a version of the solution presented in Zeman 2017, namely, to optionally subtype non-core dependents into ‘arguments’ – obl:arg, advcl:arg and xadvcl:arg – and ‘adjuncts’ (no :arg suffix), but with an important proviso that this subtyping should be optional and treebank-specific. Linguists have not made much progress since Tesnière’s 1959 three pairwise-incompatible criteria, so particular languages and treebanks should be free in applying such subtyping or not and, if so, they should be free in deciding how they understand this distinction. In summary, we propose the system of basic ad-verbal dependency relations in (38) (complemented by vo"
degorski-etal-2008-definition,W07-1706,1,\N,Missing
degorski-etal-2008-definition,saggion-2004-identifying,0,\N,Missing
E12-2002,2011.eamt-1.27,0,0.0149711,"d natural languages, implemented in Java, C++ and Perl. Examples are: 7 patterns (Hohpe and Woolf, 2004) and thus allows the processing framework to be easily scaled horizontally. Machine Translation (prototype phase) The machine translation (MT) sub-component implements the hybrid MT paradigm, combining an example-based (EBMT) component and a Moses-based statistical approach (SMT). Firstly, the input is processed by the example-based MT engine and if the whole or important chunks of it are found in the translation database, then the translation equivalents are used and if necessary combined (Gavrila, 2011). In all other cases the input is processed by the categorisation subcomponent in order to select the top-level domain and respectively, the most appropriate SMT domain- and POS-translation model (Niehues and Waibel, 2010). The translation engine in the system, based on MT Server Land (Federmann and Eisele, 2010), is able to accommodate and use different third party translation engines, such as the Google, Bing, Lusy or Yahoo translators. Figure 2. Top-level architecture of our CMS and its major components. Case Study: Multilingual Library Text Categorisation i-Librarian 5 is a free online lib"
E12-2002,2010.eamt-1.29,0,\N,Missing
glowinska-przepiorkowski-2010-design,erjavec-2004-multext,0,\N,Missing
glowinska-przepiorkowski-2010-design,savary-etal-2010-towards,1,\N,Missing
glowinska-przepiorkowski-2010-design,przepiorkowski-etal-2008-towards,1,\N,Missing
ogrodniczuk-etal-2012-towards,przepiorkowski-etal-2010-recent,1,\N,Missing
ogrodniczuk-etal-2012-towards,wolinski-etal-2012-polimorf,1,\N,Missing
P07-2022,ide-etal-2000-xces,0,0.279301,"stract This paper presents recent extensions to Poliqarp, an open source tool for indexing and searching morphosyntactically annotated corpora, which turn it into a tool for indexing and searching certain kinds of treebanks, complementary to existing treebank search engines. In particular, the paper discusses the motivation for such a new tool, the extended query syntax of Poliqarp and implementation and efficiency issues. 1 Introduction The aim of this paper is to present extensions to Poliqarp,1 an efficient open source indexer and search tool for morphosyntactically annotated XCES-encoded (Ide et al., 2000) corpora, with query syntax based on that of CQP (Christ, 1994), but extending it in interesting ways. Poliqarp has been in constant development since 2003 (Przepiórkowski et al., 2004) and it is currently employed as the search engine of the IPI PAN Corpus of Polish (Przepiórkowski, 2004) and the Lisbon corpus of Portuguese (Barreto et al., 2006), as well as in other projects. Poliqarp has a typical server-client architecture, with various Poliqarp clients developed so far, including GUI clients for a variaty of operating systems (Linux, Windows, MacOS, Solaris) and architectures (big-endian"
P07-2022,przepiorkowski-etal-2004-search,1,0.404792,"Missing"
P07-2022,barreto-etal-2006-open,0,\N,Missing
patejuk-przepiorkowski-2012-towards,przepiorkowski-etal-2010-recent,1,\N,Missing
przepiorkowski-etal-2004-search,W03-2909,1,\N,Missing
przepiorkowski-etal-2004-search,W03-2905,1,\N,Missing
przepiorkowski-etal-2004-search,P98-1080,0,\N,Missing
przepiorkowski-etal-2004-search,C98-1077,0,\N,Missing
przepiorkowski-etal-2008-towards,ide-etal-2000-xces,0,\N,Missing
przepiorkowski-etal-2008-towards,P07-2022,1,\N,Missing
przepiorkowski-etal-2008-towards,W06-2716,0,\N,Missing
przepiorkowski-etal-2008-towards,piskorski-2004-extraction,0,\N,Missing
przepiorkowski-etal-2008-towards,przepiorkowski-etal-2004-search,1,\N,Missing
przepiorkowski-etal-2010-recent,W03-2905,1,\N,Missing
przepiorkowski-etal-2010-recent,P07-2022,1,\N,Missing
przepiorkowski-etal-2010-recent,przepiorkowski-etal-2004-search,1,\N,Missing
przepiorkowski-etal-2010-recent,glowinska-przepiorkowski-2010-design,1,\N,Missing
przepiorkowski-etal-2010-recent,savary-etal-2010-towards,1,\N,Missing
przepiorkowski-etal-2010-recent,przepiorkowski-etal-2008-towards,1,\N,Missing
przepiorkowski-etal-2010-recent,W09-3011,1,\N,Missing
przepiorkowski-etal-2014-walenty,przepiorkowski-etal-2010-recent,1,\N,Missing
przepiorkowski-etal-2014-walenty,patejuk-przepiorkowski-2012-towards,1,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S14-1011,W04-1906,0,0.0947569,"Missing"
S14-1011,W00-2021,0,0.0417617,"n P¯an.ini (4th century BC); see, e.g., Dowty 1991 for a historical introduction. Fillmore’s deep cases are Agentive, Dative, Instrumental, Factive, Locative, Objective, as well as Benefactive, Time and Comitative, but many other sets of semantic roles may be found in the literature; for example, Dalrymple 2001, p. 206, cites – after Bresnan and Kanerva 1989 – the following ranked list of thematic roles: Agent, Benefactive, Recipient/Experiencer, Instrument, Theme/Patient, Locative. In Natural Language Processing (NLP), one of the most popular repertoires of semantic roles is that of VerbNet (Kipper et al. 2000; http://verbs.colorado.edu/ ~mpalmer/projects/verbnet.html), 1 Table 2 on the VerbNet webpage lists 21 roles, of which Actor is not actually used; the 10 roles which are used but not listed there are Goal, Initial_Location (apart from Location), Pivot, Reflexive, Result, Trajectory and Value, as well as CoAgent, Co-Patient and Co-Theme. 81 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 81–86, Dublin, Ireland, August 23-24 2014. inference more difficult. For example, in Ed travels to Boston, VerbNet identifies Ed as a Theme, while in Ed flie"
S14-1011,Y06-1061,0,0.0333093,"ico-semantic LFG (Lexical-Functional Grammar; Bresnan 2001; Dalrymple 2001) parser for Polish, we build on the usual LFG approach of obtaining semantic representations on the basis of f-structures, i.e., non-tree-configurational syntactic representations (as opposed to more surfacy tree-configurational c-structures) containing information about predicates, grammatical functions and morphosyntactic features; this so-called description-by-analysis (DBA) approach has been adopted for German (Frank and Erk, 2004; Frank and Semecký, 2004; Frank, 2004), English (Crouch and King, 2006) and Japanese (Umemoto, 2006). In the usual DBA approach, semantic roles are added to the resulting representations on the basis of semantic dictionaries external to LFG grammars (Frank and Semecký, 2004; Frank, 2004; Crouch and King, 2005, 2006). When such FrameNet- or VerbNet-like dictionaries are not available, grammatical function names (subject, object, etc.) are used instead of semantic roles (Umemoto, 2006). Unfortunately, this latter approach is detrimental for tasks such as textual entailment, as LFG grammatical functions represent the surface relations, so, e.g., a passivised (deep) object bears the grammatical"
S14-1011,W07-1403,0,0.0547459,"Missing"
savary-etal-2010-towards,przepiorkowski-etal-2010-recent,1,\N,Missing
savary-etal-2010-towards,sekine-etal-2002-extended,0,\N,Missing
savary-etal-2010-towards,C08-1085,0,\N,Missing
savary-etal-2010-towards,W03-0804,0,\N,Missing
savary-etal-2010-towards,glowinska-przepiorkowski-2010-design,1,\N,Missing
savary-etal-2010-towards,maurel-2008-prolexbase,0,\N,Missing
W03-2905,W03-2909,0,0.117791,"Missing"
W03-2905,W03-2415,1,0.751407,"Missing"
W07-1701,W07-1702,0,0.0395323,"Missing"
W07-1701,P99-1065,0,0.0610401,"Missing"
W07-1701,W07-1708,0,0.013436,"th regular grammars. Moreover, for Bulgarian a more general integrated system was developed, called LINGUA (Tanev and Mitkov, 2002), which — apart from modules for 12 Again, this test may fail due to case syncretisms; cf. §2.4. 5 tokenisation, morphosyntactic analysis and disambiguation, and anaphora resolution — includes an NP extractor and a bottom-up grammar of Bulgarian. This system, together with a set of shallow patterns for identifying definition patterns, has been employed in a Question Answering prototype system (Tanev, 2004). Bulgarian pattern-matching grammars are also employed in (Koeva, 2007). Apart from these language-specific implementations, there exist tools and toolboxes which facilitate various IE tasks, including shallow parsing. Probably the best known such a general system is GATE (Cunningham et al., 1995; Cunningham et al., 2002), which contains some NE resources for Bulgarian and Russian (Humphreys et al., 2002; Popov et al., 2004) and allows to write shallow (regular) grammars in the JAPE subsystem (Cunningham et al., 2000). A system similar in scope is SProUT (Becker et al., 2002), whose shallow parsing language allows to write regular grammars over HPSG-style (Pollar"
W07-1701,W07-1705,0,0.0294835,"r et al., 2002), whose shallow parsing language allows to write regular grammars over HPSG-style (Pollard and Sag, 1994) typed feature structures and which includes the operation of unification. Preliminary work on adapting SProUT to the processing of Baltic and Slavonic languages is presented in (Droz˙ dz˙ y´nski et al., 2003), with much subsequent work devoted to the processing of Polish, especially, in the area of Information Extraction from medical texts (Piskorski et al., 2004; Piskorski, 2004a; Piskorski, 2004b; Marciniak et al., 2005; Mykowiecka et al., 2005a; Mykowiecka et al., 2005b; Marciniak and Mykowiecka, 2007). Although GATE and SProUT may be adapted to the processing of XML documents, they are perhaps not the most natural choice for the further processing of morphosyntactically annotated documents in, for example, the XCES (XML Corpus Encoding Standard; (Ide et al., 2000)) format, as assumed, e.g., in the IPI PAN Corpus of Polish (Przepiórkowski, 2004a), in the Slovak National Corpus (Garabík and Gianitsová-Ološtiaková, 2005), or in the LT4eL project (http://www.lt4el.eu/). Specialised XML-aware tools exist for such tasks. One of the earliest collections of XML processing tools is the LT XML libra"
W07-1701,piskorski-2004-extraction,0,0.0261934,"w (regular) grammars in the JAPE subsystem (Cunningham et al., 2000). A system similar in scope is SProUT (Becker et al., 2002), whose shallow parsing language allows to write regular grammars over HPSG-style (Pollard and Sag, 1994) typed feature structures and which includes the operation of unification. Preliminary work on adapting SProUT to the processing of Baltic and Slavonic languages is presented in (Droz˙ dz˙ y´nski et al., 2003), with much subsequent work devoted to the processing of Polish, especially, in the area of Information Extraction from medical texts (Piskorski et al., 2004; Piskorski, 2004a; Piskorski, 2004b; Marciniak et al., 2005; Mykowiecka et al., 2005a; Mykowiecka et al., 2005b; Marciniak and Mykowiecka, 2007). Although GATE and SProUT may be adapted to the processing of XML documents, they are perhaps not the most natural choice for the further processing of morphosyntactically annotated documents in, for example, the XCES (XML Corpus Encoding Standard; (Ide et al., 2000)) format, as assumed, e.g., in the IPI PAN Corpus of Polish (Przepiórkowski, 2004a), in the Slovak National Corpus (Garabík and Gianitsová-Ološtiaková, 2005), or in the LT4eL project (http://www.lt4el.eu/"
W07-1701,W07-1704,0,0.0291942,"Missing"
W07-1701,popov-etal-2004-creation,0,0.122611,"nately, for the majority of Slavonic languages, there are no (freely) publicly available resources that could provide such “minimum morphological treatment” of proper names. For example, the only large free (but not open source) morphological analyser for Polish, Morfeusz (Woli´nski, 2006), contains very few proper names.3 Moreover, the NE content of commercial analysers is often rather low, so that simple resource-light heuristics sometimes give better results (Urba´nska and Mykowiecka, 2005, p. 214). Such heuristics usually involve the creation of inflected forms by adding typical suffixes (Popov et al., 2004; Urba´nska and Mykowiecka, 2005; Steinberger and Pouliquen, 2007), where the suffix addition/substitution rules are either handgenerated (Urba´nska and Mykowiecka, 2005) or automatically acquired (Steinberger and Pouliquen, 2007). 3 A new version of Morfeusz, containing a large dictionary of proper names, is being prepared, but it is currently not clear if it is going to be freely available for non-commercial research purposes (M. Woli´nski, p.c.). 2 2.2 Different Inflection of Homonymous Common and Proper Nouns As mentioned in (Piskorski, 2005) and discussed at length in (Piskorski et al., 2"
W07-1701,W03-2905,1,0.8922,"Missing"
W07-1701,W03-2415,1,0.89395,"Missing"
W07-1701,przepiorkowski-etal-2004-search,1,0.904844,"Missing"
W07-1701,W07-1706,1,0.873498,"Missing"
W07-1701,W04-0403,0,0.0135032,"ctually form a coordinate structure. 4 Slavonic is Processable After discussing ways in which Slavonic languages seem to be hard or easy for Information Extraction, let us look at practical attempts at Slavonic IE, especially those involving partial parsing. It seems that there have been relatively few attempts at applying shallow (or partial; cf. fn. 2) grammars to particular practical tasks. In some of these attempts no particular dedicated language processing system was used to implement shallow grammars: apparently they were coded directly in the host programming language. One example is (Sharoff, 2004), where shallow parsing is used for the identification of prepositional Multi Word Expressions in Russian, with the following explanation of reasons for performing some language-dependent processing: “Given that the word order in Russian (and other Slavonic languages) is relatively free and a typical word (i.e. lemma) has many forms (typically from 9 for nouns to 50 for verbs), the sequences of exact N-grams are much less frequent than in English, thus rendering purely statistical approaches useless.” For Polish, simple shallow grammars were implemented for the tasks of question answering (Pie"
W07-1701,simov-osenova-2004-hybrid,0,0.0192136,"nder preparation. One of the tools in that new edition, lxtransduce (Tobin, 2005), is an efficient program to add mark-up to XML files via regular grammars over XML elements; this tool is currently used for implementing definition-extraction grammars for Bulgarian, Czech and Polish (Przepiórkowski et al., 2007). A system well-known in Slavonic NLP is CLaRK (Simov et al., 2001; Simov et al., 2002); it implements various XML mechanism and proposes a language for developing shallow grammars over XML documents; such grammars have been implemented for Bulgarian, as reported in (Simov et al., 2004; Simov and Osenova, 2004). Finally, a new system, SPADE (Shallow Parsing and Disambiguation Engine), abbreviated to “♠” (Unicode character 0x2660), has recently been developed at the Institute of Computer Science, Polish Academy of Sciences (Przepiórkowski, 2007b; Buczy´nski, 2007). This tool, unlike many other shallow parsing tools,13 accepts a possibly morphosyntactically ambiguous (XCES-encoded) input and performs simultaneous morphosyntactic disambiguation and shallow parsing. For example, the rule below, called P + co/kto, will match a possible preposition followed by a possible form of one of the pronouns CO ‘wh"
W07-1701,simov-etal-2004-language,0,0.0176633,"-XML2 is currently under preparation. One of the tools in that new edition, lxtransduce (Tobin, 2005), is an efficient program to add mark-up to XML files via regular grammars over XML elements; this tool is currently used for implementing definition-extraction grammars for Bulgarian, Czech and Polish (Przepiórkowski et al., 2007). A system well-known in Slavonic NLP is CLaRK (Simov et al., 2001; Simov et al., 2002); it implements various XML mechanism and proposes a language for developing shallow grammars over XML documents; such grammars have been implemented for Bulgarian, as reported in (Simov et al., 2004; Simov and Osenova, 2004). Finally, a new system, SPADE (Shallow Parsing and Disambiguation Engine), abbreviated to “♠” (Unicode character 0x2660), has recently been developed at the Institute of Computer Science, Polish Academy of Sciences (Przepiórkowski, 2007b; Buczy´nski, 2007). This tool, unlike many other shallow parsing tools,13 accepts a possibly morphosyntactically ambiguous (XCES-encoded) input and performs simultaneous morphosyntactic disambiguation and shallow parsing. For example, the rule below, called P + co/kto, will match a possible preposition followed by a possible form of"
W07-1701,C02-1027,0,0.0182258,"ast, 2005); in the latter case a grammar was implemented as a cascade of Perl regular expressions. Similarly, (Zeman, 2001) describes a Perl regular expression implementation of a shallow preprocessor for a deep statistical parser. Much earlier, (Nenadi´c and Vitas, 1998; Nenadi´c, 2000) developed shallow grammars of SerboCroatian for the recognition of noun phrases (NPs) and certain kinds of coordinate structures. See also (Bekavac and Tadi´c, 2007) on the recognition of Croatian NEs with regular grammars. Moreover, for Bulgarian a more general integrated system was developed, called LINGUA (Tanev and Mitkov, 2002), which — apart from modules for 12 Again, this test may fail due to case syncretisms; cf. §2.4. 5 tokenisation, morphosyntactic analysis and disambiguation, and anaphora resolution — includes an NP extractor and a bottom-up grammar of Bulgarian. This system, together with a set of shallow patterns for identifying definition patterns, has been employed in a Question Answering prototype system (Tanev, 2004). Bulgarian pattern-matching grammars are also employed in (Koeva, 2007). Apart from these language-specific implementations, there exist tools and toolboxes which facilitate various IE tasks"
W07-1701,W01-1832,0,0.0149181,"Russian (and other Slavonic languages) is relatively free and a typical word (i.e. lemma) has many forms (typically from 9 for nouns to 50 for verbs), the sequences of exact N-grams are much less frequent than in English, thus rendering purely statistical approaches useless.” For Polish, simple shallow grammars were implemented for the tasks of question answering (Piechoci´nski and Mykowiecka, 2005) and automatic valence acquisition (Fast and Przepiórkowski, 2005; Przepiórkowski and Fast, 2005); in the latter case a grammar was implemented as a cascade of Perl regular expressions. Similarly, (Zeman, 2001) describes a Perl regular expression implementation of a shallow preprocessor for a deep statistical parser. Much earlier, (Nenadi´c and Vitas, 1998; Nenadi´c, 2000) developed shallow grammars of SerboCroatian for the recognition of noun phrases (NPs) and certain kinds of coordinate structures. See also (Bekavac and Tadi´c, 2007) on the recognition of Croatian NEs with regular grammars. Moreover, for Bulgarian a more general integrated system was developed, called LINGUA (Tanev and Mitkov, 2002), which — apart from modules for 12 Again, this test may fail due to case syncretisms; cf. §2.4. 5 t"
W07-1701,ide-etal-2000-xces,0,\N,Missing
W07-1701,A97-1017,0,\N,Missing
W07-1701,P07-3003,0,\N,Missing
W07-1701,P06-2062,0,\N,Missing
W07-1701,P07-2022,1,\N,Missing
W07-1706,W06-2609,0,0.512131,"tive languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. 43 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have"
W07-1706,C92-2082,0,0.0138684,"Missing"
W07-1706,W04-1807,0,0.621777,"Missing"
W07-1706,C04-1199,0,0.2312,"Missing"
W07-1706,storrer-wellinghoff-2006-automated,0,0.346347,"ll as some possible solutions, are discussed. Finally, section 6 concludes the paper. 43 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and"
W07-1706,W06-0203,0,0.205749,"s the paper. 43 Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and Osenova, 2005). Other work on Slavic information extraction has bee"
W07-1706,P99-1016,0,\N,Missing
W09-3011,przepiorkowski-etal-2008-towards,1,\N,Missing
W12-3406,J07-4002,0,0.0148318,"eneral, and we cannot hope to do it sufficient justice here. One line of work, exemplified by the early influential paper (Hindle and Rooth, 1993), posits the problem of PP-attachment as the problem of choosing between a verb v and a noun n1 when attaching a prepositional phrase defined by the syntactic head p and the semantic head n2 . Early work, including (Hindle and Rooth, 1993), concentrated on lexical associations, later also using wordnet information, e.g., (Clark and Weir, 2000), in a way similar to that described above. Let us note that this scenario was criticised as unrealistic by (Atterer and Schütze, 2007), who argue that “PP attachment should not be evaluated in isolation, but instead as an integral component of a parsing system, without using information from the gold-standard oracle”, as in the 46 Conclusion Treebanks are very expensive, morphosyntactically annotated corpora are relatively cheap. The main contribution of the current paper is a novel approach to factoring out syntactic training in the process of learning of syntactic attachment. All the finegrained lexical training data were collected from a relatively large morphosyntactically annotated and chunked corpus, and only less than"
W12-3406,C00-1029,0,0.0243574,"ed Work There is a plethora of relevant work on resolving PPattachment ambiguities in particular and finding dependency links in general, and we cannot hope to do it sufficient justice here. One line of work, exemplified by the early influential paper (Hindle and Rooth, 1993), posits the problem of PP-attachment as the problem of choosing between a verb v and a noun n1 when attaching a prepositional phrase defined by the syntactic head p and the semantic head n2 . Early work, including (Hindle and Rooth, 1993), concentrated on lexical associations, later also using wordnet information, e.g., (Clark and Weir, 2000), in a way similar to that described above. Let us note that this scenario was criticised as unrealistic by (Atterer and Schütze, 2007), who argue that “PP attachment should not be evaluated in isolation, but instead as an integral component of a parsing system, without using information from the gold-standard oracle”, as in the 46 Conclusion Treebanks are very expensive, morphosyntactically annotated corpora are relatively cheap. The main contribution of the current paper is a novel approach to factoring out syntactic training in the process of learning of syntactic attachment. All the finegr"
W12-3406,J93-1005,0,0.217887,"tactic and semantic heads of all syntactic objects; in case of words, the word itself is its own syntactic and semantic head. In effect, any syntactic object may be represented by a pair of words (the two 42 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 42–47, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics heads), and each word is characterised by its base form and its morphosyntactic tag. 2 Algorithm The standard method of solving the PP-attachment problem is based on collocation extraction (cf., e.g., (Hindle and Rooth, 1993)) and consists of three main steps: first a training corpus is scanned and frequencies of co-occurrences of pairs of words (or more general: syntactic objects) are gathered; then the collected data are normalised to obtain, for each pair, the strength of their connection; finally, information about such collocation strengths is employed to solve PP-attachment in new texts. An instance of the PP-attachment problem is the choice between two possible edges in a parse tree: (n1 , pp) and (n2 , pp), where pp is the prepositional phrase, and n1 and n2 are nodes in the tree (possible attachment sites"
W12-3406,przepiorkowski-etal-2010-recent,1,0.866053,"Missing"
W12-3406,R09-1003,0,0.0258153,"Missing"
W12-3406,P93-1032,0,\N,Missing
W12-3615,H91-1060,0,0.370381,"ce Polish Academy of Sciences ul. Jana Kazimierza 5 01-248 Warszawa, Poland michal.lenart@ipipan.waw.pl concentrate on finding inconsistencies in linguistic annotations: if similar (in some well-defined way) inputs receive different annotations, the less frequent of these annotations is suspected of being erroneous. Experiments (reported elsewhere) performed on a Polish treebank show that such methods reach reasonable precision but lack in recall. The second relevant line of research is concerned with the evaluation of syntactic parsers. The standard measure is the so-called Parseval measure (Black et al. 1991), used in the eponymous series of competitions. It calculates precision and recall on the set of (perhaps labelled, Magerman 1995) spans of words, i.e., on brackets identified in parse results and in the gold standard. Unfortunately, this measure – regardless of the fact that it has been repeatedly criticised on various grounds (Briscoe and Carroll 1996, Sampson and Babarczy 2003, Rehbein and van Genabith 2007, Kübler et al. 2008) – is not applicable to the current problem, as spans of discovered constituents are very different by design. A more promising measure, older than Parseval (cf. Samp"
W12-3615,E03-1068,0,0.0282771,"Missing"
W12-3615,A00-2000,0,0.166625,"Missing"
W12-3615,P95-1037,0,0.0493253,"stencies in linguistic annotations: if similar (in some well-defined way) inputs receive different annotations, the less frequent of these annotations is suspected of being erroneous. Experiments (reported elsewhere) performed on a Polish treebank show that such methods reach reasonable precision but lack in recall. The second relevant line of research is concerned with the evaluation of syntactic parsers. The standard measure is the so-called Parseval measure (Black et al. 1991), used in the eponymous series of competitions. It calculates precision and recall on the set of (perhaps labelled, Magerman 1995) spans of words, i.e., on brackets identified in parse results and in the gold standard. Unfortunately, this measure – regardless of the fact that it has been repeatedly criticised on various grounds (Briscoe and Carroll 1996, Sampson and Babarczy 2003, Rehbein and van Genabith 2007, Kübler et al. 2008) – is not applicable to the current problem, as spans of discovered constituents are very different by design. A more promising measure, older than Parseval (cf. Sampson et al. 1989), but gaining prominence only recently, is Leaf-Ancestor (LA; Sampson 2000, Sampson and Babarczy 2003), which comp"
W12-3615,przepiorkowski-etal-2010-recent,1,0.826146,"Missing"
W12-3615,W95-0107,0,0.180183,"ing prominence only recently, is Leaf-Ancestor (LA; Sampson 2000, Sampson and Babarczy 2003), which compares trees word-by-word. For each word, the similarity of the path from this word to the root of the tree in both trees is calculated as a number in h0, 1i, and the mean of these similarities over all words in a sentence is the score for this sentence.1 While also not There are two strands of work relevant to the current enterprise. First, there is a line of work on discovering errors in manually annotated corpora (van Halteren 2000, Eskin 2000, Dickinson and Meurers 1 The very lenient IOB (Ramshaw and Marcus 1995, Tjong 2003a), including treebanks (Dickinson and Meur- Kim Sang and Veenstra 1999) accuracy measure, used someers 2003b, Boyd et al. 2008, Dickinson and Lee times in chunking, can be considered as an extreme case of the 2008, Kato and Matsubara 2010). These methods LA measure. 118 Proceedings of the 6th Linguistic Annotation Workshop, pages 118–123, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics directly applicable to the current scenario, this measure is much more flexible, as path similarity may be defined in various ways. The method proposed in"
W12-3615,D07-1066,0,0.0381278,"Missing"
W12-3615,E99-1023,0,0.0549899,"zy 2003), which compares trees word-by-word. For each word, the similarity of the path from this word to the root of the tree in both trees is calculated as a number in h0, 1i, and the mean of these similarities over all words in a sentence is the score for this sentence.1 While also not There are two strands of work relevant to the current enterprise. First, there is a line of work on discovering errors in manually annotated corpora (van Halteren 2000, Eskin 2000, Dickinson and Meurers 1 The very lenient IOB (Ramshaw and Marcus 1995, Tjong 2003a), including treebanks (Dickinson and Meur- Kim Sang and Veenstra 1999) accuracy measure, used someers 2003b, Boyd et al. 2008, Dickinson and Lee times in chunking, can be considered as an extreme case of the 2008, Kato and Matsubara 2010). These methods LA measure. 118 Proceedings of the 6th Linguistic Annotation Workshop, pages 118–123, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics directly applicable to the current scenario, this measure is much more flexible, as path similarity may be defined in various ways. The method proposed in section 4 has been inspired by this measure. Another general source of inspiration h"
W12-3615,W00-1907,0,0.11087,"Missing"
W12-3615,dickinson-lee-2008-detecting,0,\N,Missing
W12-3615,kubler-etal-2008-compare,0,\N,Missing
W12-3615,P10-2014,0,\N,Missing
W12-3615,A00-2020,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W14-5811,lopatkova-etal-2006-valency,0,0.0340632,"uld reduce the number of errors, especially syntactic inconsistencies. On the other hand, the experiment confirmed that some of the most difficult lexicalisations are those currently marked as fixed, and they clearly require special attention. 4 Discussion and conclusion Many valence dictionaries mark some valence schemata as idiomatic – this is true, e.g., of the VALBU valence dictionary for German developed at the Institut für Deutsche Sprache (Schumacher et al. 2004; http://hypermedia.ids-mannheim.de/evalbu/), of the VALLEX dictionary of Czech developed at the Charles University in Prague (Lopatková et al. 2006; http://ufal.mff.cuni.cz/ vallex/), as well as some previous valence dictionaries of Polish, including Polański 1980–1992 and the dictionary that was used to bootstrap the first version of Walenty, i.e., Świdziński 1998. However, we are not aware of another valence dictionary that would explicitly describe lexicalised arguments at the same level of detail as the version of Walenty presented in Przepiórkowski et al. 2014. Regardless of this level of detail, though, the formalism employed in that version suffers from a number of problems discussed in §1.3, limiting its ability to describe phras"
W14-5811,patejuk-przepiorkowski-2012-towards,1,0.558127,"is practically employed in parsing by two different parsers of Polish (cf. §1.1). We believe that these traits make the current proposal to further extend the underlying formalism potentially interesting to the wider audience. 1.1 Walenty Walenty is a valence dictionary which is meant to be both human- and machine-readable; in particular, it is being employed by two parsers of Polish, Świgra (an implementation of a Definite Clause Grammar description of fragments of Polish syntax; Woliński 2004) and POLFIE (an implementation of a Lexical Functional Grammar description of fragments of Polish; Patejuk and Przepiórkowski 2012). As these parsers are based on two rather different linguistic approaches, the valence dictionary must be sufficiently expressive to accommodate for the needs of both – and perhaps other to come. Each verb is assigned a number of valence schemata1 and each schema is a set of argument specifications. Walenty is explicit about what counts as an argument: if two morphosyntactically different phrases may occur coordinated in an argument position, they are taken to be different realisations of the same argument. This is exemplified in the following schema for tłumaczyć ‘explain’, as in Musiałem im"
W14-5811,przepiorkowski-etal-2014-walenty,1,\N,Missing
W18-4902,C12-1134,1,0.72916,"in (4) (with the basic tree displayed above the text and the enhanced structure – below the text, with the differences shown in red). The second aim of this paper is to examine to what extent rich information available in LFG structures is or may in principle be preserved in such enhanced UD representations. The empirical basis for the conversion is a manually disambiguated LFG parsebank of Polish (Patejuk and Przepiórkowski 2014) consisting of over 17,000 sentences (almost 131,000 tokens). Since this is a parsebank, it only contains analyses successfully provided by the LFG parser of Polish (Patejuk and Przepiórkowski 2012b, 2015) and selected by human annotators as correct. While this constrains the number and kinds of constructions present in the corpus, the underlying LFG grammar of Polish is currently one of the largest implemented LFG grammars, and it includes a comprehensive analysis of various kinds of coordination and its interaction with other phenomena (Patejuk and Przepiórkowski 2012a), so there is no shortage of sentences which pose potential difficulties for the conversion. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons."
W18-4902,patejuk-przepiorkowski-2012-towards,1,0.780299,"in (4) (with the basic tree displayed above the text and the enhanced structure – below the text, with the differences shown in red). The second aim of this paper is to examine to what extent rich information available in LFG structures is or may in principle be preserved in such enhanced UD representations. The empirical basis for the conversion is a manually disambiguated LFG parsebank of Polish (Patejuk and Przepiórkowski 2014) consisting of over 17,000 sentences (almost 131,000 tokens). Since this is a parsebank, it only contains analyses successfully provided by the LFG parser of Polish (Patejuk and Przepiórkowski 2012b, 2015) and selected by human annotators as correct. While this constrains the number and kinds of constructions present in the corpus, the underlying LFG grammar of Polish is currently one of the largest implemented LFG grammars, and it includes a comprehensive analysis of various kinds of coordination and its interaction with other phenomena (Patejuk and Przepiórkowski 2012a), so there is no shortage of sentences which pose potential difficulties for the conversion. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons."
W19-7705,C12-1134,1,0.827759,"an) and in some neighbouring languages (Romanian, Hungarian, West Armenian), as well as – though signiﬁcantly constrained – in English, French, German, Dutch, Italian and Spanish (Paperno, 2012, Lipták, 2012, Bîlbîie and Gazdik, 2012). In the case of these Germanic and Romance languages, the phenomenon seems to be limited to the coordination of optional wh-items (Gračanin-Yüksek, 2007, Lipták, 2012) – e.g., an adjunct and an optional argument – and often occurs in titles, as in (3) above. In the case of the “Slavic sprachbund“, the phenomenon is much more robust. First of all, as discussed in Patejuk and Przepiórkowski, 2012a,b and in Paperno, 2012, in Slavic such constructions are not limited to wh-items, although they particularly often involve such items; most of the examples in this paper are of this kind. But apart from wh-items, coordination of diﬀerent grammatical functions may involve negative pronouns (so-called n-words; cf., e.g., the Russian (11) and the Polish (16)), certain items expressing existential or universal quantiﬁers (the latter illustrated in (6) below), and items belonging to a number of other pronominal or quantiﬁcational classes.6 Second, the coordinated items may be obligatory arguments"
W19-7705,P13-1051,0,0.0193387,"oretical linguistic perspective. 1 Introduction Coordination is a well-known and long-standing problem for dependency representations of natural language utterances, both in theoretical linguistics and in natural language processing. Representational devices beyond the usual dependency trees are proposed especially for the treatment of coordination in Lucien Tesnière’s Dependency Syntax (1959, 2015), Richard Hudson’s Word Grammar (1984, 1990, 2010), and Igor Mel’čuk’s Meaning–Text Theory (1974, 1988, 2009). Also, the representation of coordination diﬀers widely in diﬀerent dependency corpora (Popel et al., 2013). Coordination is also problematic for Universal Dependencies (UD; Nivre et al., 2016; http:// universaldependencies.org/). In the current version 2 of the standard, each utterance may be represented by two dependency structures: the basic dependency tree and the enhanced representation, which does not have to be a tree. For example, the two representations of (1) (on one of its interpretations) are shown in (2).1,2 (1) I wanted to buy fresh apples and oranges. (2) punct xcomp conj obj nsubj cc amod mark I. wanted . to. buy . fresh . . apples . and . oranges . .. mark nsubj amod cc xcomp conj"
W19-7705,W19-8007,1,0.894003,", the additional obj (direct object) dependency from buy to oranges emphasises the symmetric nature of the two conjuncts with respect to the governing verb buy. One problematic aspect of this representation of coordination, known to the UD community, concerns nested – i.e. immediately embedded – coordination: in the case of three conjuncts, A, B, C, the proposed representation does not distinguish between the ﬂat structure (A, B,C), and the structure in which A and B are conjoined and the resulting coordination is conjoined with C, i.e., ((A, B),C).3 Solutions to this problem are discussed in Przepiórkowski and Patejuk, 2019b. In this paper we deal with another phenomenon problematic for UD, namely, the possibility to coordinate diﬀerent grammatical functions, as in the attested (3):4 (3) [[What]obj and [when]advmod ] to eat to reduce insulin5 Such examples violate the overwhelming generalisation that only the same grammatical functions may be coordinated. Normally, languages satisfy this generalisation and attempts to coordinate phrases bearing diﬀerent grammatical functions result in unacceptability, as in (4)–(5): (4) *I and an apple have already eaten. (intended meaning: I have already eaten an apple.) (5) *I"
W19-7705,W17-0416,0,0.0160311,"ccompanying noun is its dependent (it receives case from the numeral). However, following UD guidelines, this dependency relation is reversed: numerals are dependents of nominal heads, so the interrogative numeral ile ‘how much’ is a det dependent of czego ‘what. ’, which is in turn the direct object of dostarczyć ‘provide, supply’. One potential problem with the UD representation arises at the level of enhanced dependencies, where ile is also a conj dependent of czego; as shown in (30), there are two diﬀerent equidirectional dependency relations between these two tokens: det and conj. 20 See Schuster et al., 2017, 130–131 for arguments against encoding paths in dependency labels in the context of the UD representation of gapping, the most important of which is that this would introduce an unbounded number of dependency relations. 21 Though relations could be disambiguated by, for instance, adding indices, e.g. obl1 and obl2, but this would further aggravate the problem of number and complexity of labels (resulting in nsubj_obl1:det, among others). (30) punct obj iobj det xcomp Czego . i. cc . ile. trzeba . dostarczyć . organizmowi . ?. cc xcomp conj iobj det obj punct This problem arises regardless of"
W19-7705,W15-2134,0,0.056105,"Missing"
W19-8007,W15-2113,0,0.194338,"onservative (i.e., least conservative from the point of view of UD) and moving to the most UD-conservative. 3.1 Diﬀerent Topology A theoretically possible solution would be to change the general UD topology of coordinate structures and represent them as headed by the conjunction. While this was probably the most popular representation of coordination in pre-UD treebanks (Popel et al., 2013), the idea that conjunctions head coordinate structures is widely rejected on theoretical linguistic grounds, both within dependency approaches (e.g. Mel’čuk and Pertsov, 1987, 65, Hudson, 1988, 314–315 and Gerdes and Kahane, 2015, 102–105) and within constituency approaches (Borsley, 2005). Hence, we will not consider this possibility here. A proposal to distinguish diﬀerent nestings in dependency graphs which does not assume mechanisms outside of dependency relations and in which coordination is represented as headed by the ﬁrst conjunct is outlined in Gerdes and Kahane, 2015, 108. According to that proposal, the three representations of diﬀerent nestings of Tom and Jerry and Scooby-Doo would be:4 (17) para para dep beq Tom . and . beq Jerry . . and . dep Scooby-Doo . 4 See Gerdes and Kahane, 2015 for the explanation"
W19-8007,W15-2313,0,0.0686952,"Missing"
W19-8007,W18-6009,0,0.0200054,"site direction. The practical advantage of this proposal is that, for any number of conjuncts in a coordinate structure, any two diﬀerent nestings will provably diﬀer either in their basic tree representation, or in their enhanced representation, or in both. Hence, diﬀerent nestings of a coordinate structure may now be distinguished in UD; Table 1 gives all 11 possibilities for the case of 4 conjuncts.7 This would also be true if we did not insist on the bidirectionality, but instead allowed for chaining, say, from left to right. Moreover, this solution is also compatible with the proposal of Kanayama et al. (2018), who convincingly argue for right-headed basic tree representations of coordination in the case of head-ﬁnal languages such as Japanese and Korean, i.e., representations symmetric with respect to the strictly left-headed trees currently imposed by UD. In the case of such head-ﬁnal languages, the unidirectional version of the enhanced representation proposed here would make more sense with chains from right to left. The theoretical advantage which does, however, rely on bidirectionality is related to the frequently expressed (but rarely implemented) sentiment that all conjuncts are heads of a"
W19-8007,P13-1051,0,0.641585,"dependency graphs (with the option of introducing empty nodes). How could then the three diﬀerent structures of (1) be represented in UD? In the following subsections, we consider various solutions starting from the least UD-conservative (i.e., least conservative from the point of view of UD) and moving to the most UD-conservative. 3.1 Diﬀerent Topology A theoretically possible solution would be to change the general UD topology of coordinate structures and represent them as headed by the conjunction. While this was probably the most popular representation of coordination in pre-UD treebanks (Popel et al., 2013), the idea that conjunctions head coordinate structures is widely rejected on theoretical linguistic grounds, both within dependency approaches (e.g. Mel’čuk and Pertsov, 1987, 65, Hudson, 1988, 314–315 and Gerdes and Kahane, 2015, 102–105) and within constituency approaches (Borsley, 2005). Hence, we will not consider this possibility here. A proposal to distinguish diﬀerent nestings in dependency graphs which does not assume mechanisms outside of dependency relations and in which coordination is represented as headed by the ﬁrst conjunct is outlined in Gerdes and Kahane, 2015, 108. According"
W19-8007,W17-0416,0,0.108239,"om . and . Jerry . and . .Spike . and . Scooby-Doo . cc conj 5 Mel’čuk, cc conj cc conj 2009, 94 also claims that this solution is ‘not suﬃcient formally’ to represent the diﬀerence between ‘hungry [men and women and children]’ and ‘[hungry [men and women]] and children’, but this diﬀerence can in fact be represented by the combination of Mel’čuk’s suggestion concerning the scope of modiﬁers illustrated in (26)–(27) and its extension to the case of nested coordination discussed here. 6 Having a limited number of dependency labels is often perceived as important by the UD community; see, e.g., Schuster et al., 2017, 130–131 for arguments against encoding paths in dependency labels in the context of the UD representation of gapping, the most important of which is that this would introduce an unbounded number of dependency relations. (34) [Tom and Jerry] and Spike and Scooby-Doo (= (29)): conj conj conj cc cc cc Tom . and . Jerry . and . .Spike . and . Scooby-Doo . cc cc cc conj conj conj (35) [[Tom and Jerry] and Spike] and Scooby-Doo (= (30)): conj conj conj cc cc cc Tom . and . Jerry . and . .Spike . and . Scooby-Doo . cc cc conj cc conj conj According to this proposal, any two neighbouring conjuncts i"
wolinski-etal-2012-polimorf,przepiorkowski-etal-2010-recent,1,\N,Missing
wolinski-etal-2012-polimorf,obrebski-stolarski-2006-uam,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,steinberger-etal-2012-dgt,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,D09-1086,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,W09-3803,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,P12-1066,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,P07-2045,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,P09-2010,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,I08-3008,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,P09-1042,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,D11-1006,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,N13-1126,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,P11-2120,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,przepiorkowski-etal-2014-walenty,1,\N,Missing
wroblewska-przepiorkowski-2014-projection,2005.mtsummit-papers.11,0,\N,Missing
wroblewska-przepiorkowski-2014-projection,tiedemann-2012-parallel,0,\N,Missing
