2003.mtsummit-papers.37,J93-2003,0,0.00482477,"Missing"
2003.mtsummit-papers.37,carbonell-etal-2002-automatic,0,0.0305704,"period (Germann, 2001). All of this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002) or in (Nirenburg and Raskin, 1998). 3 Cebuano Resources The surprise language dry run resulted in production of six types of resources that that we used in our experiments:1 • B: 912,775 words of verse-aligned parallel text from the Bible, provided by the University of Maryland. Both the English and the Cebuano bibles were obtained from the Web in character-coded form. The English Bible was the World English Bible (WEB) version. Verse alignment was used in lieu of sentence alignment for this data. • E: 214,327 words of parallel text from examples of usage that were automatically extracted fro"
2003.mtsummit-papers.37,W01-1409,0,0.1079,"999). The first foray that we are aware of into rapid development of statistical machine translation systems was by a team at the 1999 Johns Hopkins Summer Workshop. They built a Chinese-to-English MT system in one day, although the parallel text collection that they used had been assembled over an extended period at considerable expense by the Linguistic Data Consortium (AlOnizan et al., 1994). Germann was the first to try similar techniques with rapidly developed resources, building a Tamil-to-English MT system by manually translating 24,000 words of Tamil into English in a six week period (Germann, 2001). All of this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002"
2003.mtsummit-papers.37,P98-2160,0,0.0236882,"this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002) or in (Nirenburg and Raskin, 1998). 3 Cebuano Resources The surprise language dry run resulted in production of six types of resources that that we used in our experiments:1 • B: 912,775 words of verse-aligned parallel text from the Bible, provided by the University of Maryland. Both the English and the Cebuano bibles were obtained from the Web in character-coded form. The English Bible was the World English Bible (WEB) version. Verse alignment was used in lieu of sentence alignment for this data. • E: 214,327 words of parallel text from examples of usage that were automatically extracted from a printed bilingual dictionary af"
2003.mtsummit-papers.37,N03-2026,1,0.814744,"ting the language resources that this community will need in June. The Philippine language Cebuano was chosen for the dry run, and eleven institutions contributed to the resulting data collection and construction effort over the next ten days (Oard, 2003). Several teams used the resulting resources as a basis for constructing systems that worked with English and Cebuano. Dictionary-based CrossLanguage Information Retrieval (CLIR) proved to be a tractable task, with batch experiments demonstrating respectable retrieval effectiveness after three ∗ This work was performed while at USC-ISI. days (Oard et al., 2003) and two fully integrated interactive CLIR systems available by the tenth day. Machine Translation (MT) proved to be a bigger challenge, however. The interactive CLIR systems were forced to rely on term-by-term gloss translation, because suitable statistical machine translation results were not available during the dry run. Our purpose in this paper is to explore the potential for building MT systems using the resources that were constructed, with an eye towards contributing MT results for use in integrated systems during the surprise language experiment in June. Interactive CLIR systems that"
2003.mtsummit-papers.37,P02-1038,1,0.674161,"by using a log-linear model. In this framework, we have a set of M feature functions hm (e, f ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: Pr(e|f ) = pλM (e|f ) 1 (1) P exp[ M m=1 λm hm (e, f )] =P PM 0I e0 I exp[ m=1 λm hm (e 1 , f )] (2) 1 In this framework, the modeling problem amounts to developing suitable feature functions that capture the relevant properties of the translation task. The basic translation model feature functions (FF) of our model are identical to the ones used in (Och and Ney, 2002): an alignment template FF, alignment template reordering FF, lexicon FF. The training process to obtain these feature functions makes use of a word-to-word alignment. More details can be found in (Och et al., 1999). In addition, we use a word penalty and an alignment template penalty feature function that count the number of produced words and the number of used alignment templates used to translate the sentence. Four English language models are used as separate feature functions: two from a large collection of news, one from the English Bible, and one from the English side of the training da"
2003.mtsummit-papers.37,W99-0604,1,0.761479,"ation probability is given by: Pr(e|f ) = pλM (e|f ) 1 (1) P exp[ M m=1 λm hm (e, f )] =P PM 0I e0 I exp[ m=1 λm hm (e 1 , f )] (2) 1 In this framework, the modeling problem amounts to developing suitable feature functions that capture the relevant properties of the translation task. The basic translation model feature functions (FF) of our model are identical to the ones used in (Och and Ney, 2002): an alignment template FF, alignment template reordering FF, lexicon FF. The training process to obtain these feature functions makes use of a word-to-word alignment. More details can be found in (Och et al., 1999). In addition, we use a word penalty and an alignment template penalty feature function that count the number of produced words and the number of used alignment templates used to translate the sentence. Four English language models are used as separate feature functions: two from a large collection of news, one from the English Bible, and one from the English side of the training data. From separating out different language models into different feature functions, we expect that the used discriminative training procedure (see below) can counteract the fact that the dominating domains of our tr"
2003.mtsummit-papers.37,P03-1021,1,0.154965,"nts to obtaining suitable parameter values λM 1 for the M = 10 different parameters of the log-linear model given the development corpus (f1S , eS1 ): ˆ M = argmax M SOME-CRITERION(f S , eS ) λ 1 1 1 λ1 (3) A standard criterion for log-linear models is the MMI (maximum mutual information) criterion, which can be derived from the maximum entropy principle. Here, we use a different discriminative training procedure that directly optimizes translation quality measured by the BLEU metric on our development corpus. The used greedy search algorithm for the optimal parameter setting is described in (Och, 2003). The search problem for log-linear models amounts to solve the following optimization problem: ˆ = argmaxe pλˆM (e|f ) e (4) 1 = argmaxe M X λm hm (f , e) (5) m=1 We use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extract n-best candidate translations using A* search (Ueffing et al., 2002). These n-best candidate translations are the basis for discriminative training of the model parameters with respect to translation quality. 6 Results Figure 1 shows the results of our experiments. English is the language of instruction"
2003.mtsummit-papers.37,2001.mtsummit-papers.68,0,0.0712429,"eam at the 1999 Johns Hopkins Summer Workshop. They built a Chinese-to-English MT system in one day, although the parallel text collection that they used had been assembled over an extended period at considerable expense by the Linguistic Data Consortium (AlOnizan et al., 1994). Germann was the first to try similar techniques with rapidly developed resources, building a Tamil-to-English MT system by manually translating 24,000 words of Tamil into English in a six week period (Germann, 2001). All of this work was done before automatic evaluation of MT system output using measures such as BLEU (Papineni et al., 2001) became commonplace, so a wide variety of techniques were used to assess the results of those early efforts. Resnik and Oard used a classification consistency measure, the Hopkins workshop team assessed their output by inspection, and Germann performed a suite of taskbased user studies. In each case, the MT results were found to be useful. Alternative ways to quickly develop MT systems for a rule-based machine translation system are suggested in (Carbonell et al., 2002) or in (Nirenburg and Raskin, 1998). 3 Cebuano Resources The surprise language dry run resulted in production of six types of"
2003.mtsummit-papers.37,W02-1021,1,0.848088,"Missing"
2003.mtsummit-papers.37,P02-1040,0,\N,Missing
2003.mtsummit-papers.37,C98-2155,0,\N,Missing
2004.iwslt-evaluation.9,N03-1017,1,0.0272244,"Missing"
2004.iwslt-evaluation.9,J04-4002,1,\N,Missing
2005.mtsummit-tutorials.1,J93-2003,0,\N,Missing
2005.mtsummit-tutorials.1,C96-2141,0,\N,Missing
2005.mtsummit-tutorials.1,N03-2021,0,\N,Missing
2005.mtsummit-tutorials.1,J04-4002,1,\N,Missing
2005.mtsummit-tutorials.1,J03-1002,1,\N,Missing
2005.mtsummit-tutorials.1,P97-1037,0,\N,Missing
2012.amta-papers.18,2010.amta-papers.16,0,0.292091,"em to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of domain-specific data obtain"
2012.amta-papers.18,D08-1064,0,0.129598,"optimization problem, this objective does not explicitly take genre into account. However, the weights of different domains, {λdi , d = 1 . . . D, i = 1 . . . Id }, are decoupled (by the runtime feature name re-labeling (Section 3)) in the n-best lists, rather than being shared, thus the weight optimization for one domain can concentrate on the domain itself, without being constrained too much by any other domain. Since BLEU is not decomposable at the sentence level, this objective generally can not guarantee maximized BLEUs on respective domain partitions, but rather an optimal overall BLEU (Chiang et al., 2008). Another optimization objective is the max BLEU sum that maximizes the sum of BLEUs of the individual genre partitions: ! D X BLEU(Sd ) (6) max {λdi ,d=1...D,i=1...Id } d=1 When none of the features is shared in the n-best lists across different domains (which is our case), this objective is equivalent to the summation of the individually maximized BLEUs: ! D X max BLEU(Sd ) (7) d=1 {λdi ,i=1...Id } There could be other genre-aware tuning objectives that mix (or “nest”) the above two.1 But in our paper, we are mainly interested in the experimental comparison between the max joint BLEU and the"
2012.amta-papers.18,P11-2080,0,0.0600089,"is no explicit treatment on genre, but there is a subtlety: Features are decoupled in n-best lists across domains and this gives MERT the freedom to adjust weights for one domain without much constraints from any other domain. In other words, MERT is made implicitly genre-aware. Actually, we can further tailor the MERT tuning objectives to be explicitly aware of genre. We explain alternative genre-aware tuning objectives that the classical MERT can be altered to adopt in a later section and compare their effects experimentally. Daume III (2007) does domain adaptation by augumenting features. Chiang et al. (2011) improve lexical smoothing also by augmenting features refined by genres. In our approach, the n-best lists for tuning can be viewed as containing augmented features as well. But we augment features in order to decouple them across domains, rather than providing refined domain/genre bias in the model. The language model feature deserves further explanation. Even though we do not have domain specific features in the translation model, we have domain specific language models. In our approach, the generic language model is used by different domains, but a domain language model is turned on only i"
2012.amta-papers.18,P07-1033,0,0.262823,"Missing"
2012.amta-papers.18,W07-0717,0,0.271504,"n system that is capable of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability. 1 Introduction Research in domain adaption for machine translation (MT) has been mostly focusing on one domain. Various methods have been proposed to make a system work best on a resource-scarce domain when most of the training data is from another open, resource-rich domain, e.g., (Foster et al., 2010; Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Decent improvements have been made on domain translation accuracy, but often, accuracy improvements for one domain are obtained at the expense of accuracy losses in another (e.g., the background) domain. With these methods, it remains unaddressed how they can be generalized to work equally well with more than one domains at the same time. One could trivially build one system/model per domain, but that does not scale and will require manual domain detection if the incoming texts belong to heterogeneous domains. So far, there has been little work on better infrastru"
2012.amta-papers.18,D10-1044,0,0.18498,"em to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of domain-specific data obtain"
2012.amta-papers.18,W07-0733,0,0.499117,"e of producing adapted domain translations and preserving the original generic accuracy at the same time. The approach unifies automatic domain detection and domain model parameterization into one system. Experiment results on 20 language pairs demonstrate its viability. 1 Introduction Research in domain adaption for machine translation (MT) has been mostly focusing on one domain. Various methods have been proposed to make a system work best on a resource-scarce domain when most of the training data is from another open, resource-rich domain, e.g., (Foster et al., 2010; Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Decent improvements have been made on domain translation accuracy, but often, accuracy improvements for one domain are obtained at the expense of accuracy losses in another (e.g., the background) domain. With these methods, it remains unaddressed how they can be generalized to work equally well with more than one domains at the same time. One could trivially build one system/model per domain, but that does not scale and will require manual domain detection if the incoming texts belong to heterogeneous domains. So far, there has been little work on better infrastructure for building and deplo"
2012.amta-papers.18,N03-1017,1,0.0230989,"Missing"
2012.amta-papers.18,W04-3250,0,0.165773,"Missing"
2012.amta-papers.18,D08-1076,1,0.891121,"Missing"
2012.amta-papers.18,P03-1021,1,0.022694,") d(f ) eˆ = arg max λi · hi (f, e) (2) e i=1 Generalizing decoding for genre awareness in turn makes tuning genre-aware. Our tuning development set consists of sentences from D domains. And we do not need to know the domain for each sentence beforehand. The genre-aware decoding on the entire set automatically classifies it into D partitions. The runtime feature re-labeling makes each partition, Sd , d = 0, . . . , D − 1, have its own set of features that are decoupled from other domains. Therefore the system has D sets of features in total. We then can use Minimum Error Rate Training (MERT) (Och, 2003) out of the box to learn the weights for all these features in a single MERT run which maximizes the overall BLEU of the entire genre-mixed development set: max {λdi ,d=1...D,i=1...Id } BLEU(∪D−1 d=0 Sd ) (3) In this formula, there is no explicit treatment on genre, but there is a subtlety: Features are decoupled in n-best lists across domains and this gives MERT the freedom to adjust weights for one domain without much constraints from any other domain. In other words, MERT is made implicitly genre-aware. Actually, we can further tailor the MERT tuning objectives to be explicitly aware of gen"
2012.amta-papers.18,2007.mtsummit-papers.68,0,0.458729,"in detector, to generalize an MT system to handle different domains. We study this approach with two domains (generic and patent), carry out large-scale experiments for 20 language pairs, demonstrating the viability of our approach. The rest of the paper is organized as follows. Section 2 describes related work. Section 3 presents our domain adaptation approach. Section 4 explains our method to classify an input sentence into its domain. Sections 5 and 6 talk about genre-aware decoding and tuning. We present experiment results in Section 7 and conclude in Section 8. 2 Related Work The work of Xu et al. (2007) and Banerjee et al. (2010) are perhaps the most relevant to our work. Xu et al. (2007) adapt a shared, generic translation model for better web or broadcast conversation translations and use a source-document classifier to classify an input document into a domain. This work makes the translation model shared across different domains, but domain-specific training data is not used and thus its impact is not studied. Neither did they report the generic MT accuracy. Banerjee et al. (2010) use source-sentence classification to combine two separate domain models, each trained from small amounts of"
2012.amta-papers.18,N09-1028,1,0.895385,"Missing"
C00-2163,J93-2003,0,0.283447,"ine translation. We propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a re ned annotation scheme to produce suitable reference alignments. We also compare the impact of di erent alignment models on the translation quality of a statistical machine translation system. 1 Introduction In statistical machine translation (SMT) it is necessary to model the translation probability P r(f1J jeI1 ). Here f1J = f denotes the (French) source and eI1 = e denotes the (English) target string. Most SMT models (Brown et al., 1993; Vogel et al., 1996) try to model word-to-word correspondences between source and target words using an alignment mapping from source position j to target position i = aj . We can rewrite the probability P r(f1J jeI1 ) by introducing the `hidden&apos; alignments aJ1 := a1 :::aj :::aJ (aj 2 f0; : : : ; I g): P r(f1 je1 ) = J I X aJ P r(f1 ; a1 je1 ) J J I 1 = J XY P r(f ; a jf1 1 ; a1 1 ; e1 ) j j J a 1 j j I j =1 To allow for French words which do not directly correspond to any English word an arti cial &apos;empty&apos; word e0 is added to the target sentence at position i = 0. The di erent alignment model"
C00-2163,niessen-etal-2000-evaluation,1,0.45998,"Missing"
C00-2163,W99-0604,1,0.676963,"or words which are in the dictionary but indirectly also for other words. The additional sentences in the training corpus are weighted with a factor Flex during the EM-training of the lexicon probabilities. We assign the dictionary entries which really cooccur in the training corpus a high weight Flex and the remaining entries a very low weight. In our experiments we use Flex = 10 for the co-occurring dictionary entries which is equivalent to adding every dictionary entry ten times to the training corpus. 6 The Alignment Template System The statistical machine-translation method described in (Och et al., 1999) is based on a word aligned training corpus and thereby makes use of singleword based alignment models. The key element of this approach are the alignment templates which are pairs of phrases together with an alignment between the words within the phrases. The advantage of the alignment template approach over word based statistical translation models is that word context and local re-orderings are explicitly taken into account. We typically observe that this approach produces better translations than the single-word based models. The alignment templates are automatically trained using a parall"
C00-2163,C96-2141,1,0.953355,"propose to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually-produced alignment and describe a re ned annotation scheme to produce suitable reference alignments. We also compare the impact of di erent alignment models on the translation quality of a statistical machine translation system. 1 Introduction In statistical machine translation (SMT) it is necessary to model the translation probability P r(f1J jeI1 ). Here f1J = f denotes the (French) source and eI1 = e denotes the (English) target string. Most SMT models (Brown et al., 1993; Vogel et al., 1996) try to model word-to-word correspondences between source and target words using an alignment mapping from source position j to target position i = aj . We can rewrite the probability P r(f1J jeI1 ) by introducing the `hidden&apos; alignments aJ1 := a1 :::aj :::aJ (aj 2 f0; : : : ; I g): P r(f1 je1 ) = J I X aJ P r(f1 ; a1 je1 ) J J I 1 = J XY P r(f ; a jf1 1 ; a1 1 ; e1 ) j j J a 1 j j I j =1 To allow for French words which do not directly correspond to any English word an arti cial &apos;empty&apos; word e0 is added to the target sentence at position i = 0. The di erent alignment models we present provide"
C00-2163,1993.mtsummit-1.11,0,0.0746119,"of the alignment template approach over word based statistical translation models is that word context and local re-orderings are explicitly taken into account. We typically observe that this approach produces better translations than the single-word based models. The alignment templates are automatically trained using a parallel training corpus. For more information about the alignment template approach see (Och et al., 1999). 7 Results We present results on the Verbmobil Task which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993). We measure the quality of the above mentioned alignment models with respect to alignment quality and translation quality. To obtain a reference alignment for evaluating alignment quality, we manually aligned about 1.4 percent of our training corpus. We allowed the humans who performed the alignment to specify two di erent kinds of alignments: an S (sure) alignment which is used for alignments which are unambiguously and a P (possible) alignment which is used for alignments which might or might not exist. The P relation is used especially to align words within idiomatic expressions, free tran"
C02-1032,C00-2163,1,\N,Missing
C02-1032,J93-2003,0,\N,Missing
C02-1032,E99-1010,1,\N,Missing
C02-1032,J96-1002,0,\N,Missing
C02-1032,P01-1027,1,\N,Missing
C02-1032,W00-0707,0,\N,Missing
C04-1072,P04-1077,1,0.632252,"tical machine translation framework – A metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework (Och 2003). The overall system performance in terms of 1 Oracles refer to the reference translations used in the evaluation procedure. generating more human like translations should also be improved. Before we demonstrate how to use ORANGE to evaluate automatic metrics, we briefly introduce three new metrics in the next section. 3 Three New Metrics ROUGE-L and ROUGE-S are described in details in Lin and Och (2004). Since these two metrics are relatively new, we provide short summaries of them in Section 3.1 and Section 3.3 respectively. ROUGE-W, an extension of ROUGE-L, is new and is explained in details in Section 3.2. 3.1 ROUGE-L: sequence Longest Common SubGiven two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length (Cormen et al. 1989). To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition is that the longer the LCS of two translations is, the more similar the two translations are. We p"
C04-1072,niessen-etal-2000-evaluation,1,0.635456,"Missing"
C04-1072,P03-1021,1,0.11247,"heir reference translations. Therefore, reference translations should be ranked higher than machine translations on average if a good automatic evaluation metric is used. Based on these assumptions, we propose a new automatic evaluation method for evaluation of automatic machine translation metrics as follows: Given a source sentence, its machine translations, and its reference translations, we compute the average rank of the reference translations within the combined machine and reference translation list. For example, a statistical machine translation system such as ISI’s AlTemp SMT system (Och 2003) can generate a list of n-best alternative translations given a source sentence. We compute the automatic scores for the n-best translations and their reference translations. We then rank these translations, calculate the average rank of the references in the n-best list, and compute the ratio of the average reference rank to the length of the n-best list. We call this ratio “ORANGE” (Oracle1 Ranking for Gisting Evaluation) and the smaller the ratio is, the better the automatic metric is. There are several advantages of the proposed ORANGE evaluation method: • No extra human involvement – ORAN"
C04-1072,2001.mtsummit-papers.3,0,0.0128539,"Missing"
C04-1072,2001.mtsummit-papers.68,0,0.0489947,"Missing"
C04-1072,C92-2067,0,0.0155207,"Missing"
C04-1072,2003.mtsummit-papers.32,0,0.408803,"Missing"
C04-1072,P02-1040,0,\N,Missing
C08-1144,D07-1079,0,0.0946974,"ng in the context of a nonterminal of type “VB” (verb). VP → ne VB pas, do not VB : w1 ∗ Work done during internships at Google Inc. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. ∗ 1145 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145–1152 Manchester, August 2008 sive improvements of PSCFG over phrase-based approaches for large Chinese-to-English data scenarios (Chiang, 2005; Chiang, 2007; Marcu et al., 2006; DeNeefe et al., 2007), these phrase-based baseline systems were constrained to distortion limits of four (Chiang, 2005) and seven (Chiang, 2007; Marcu et al., 2006; DeNeefe et al., 2007), respectively, while the PSCFG systems were able to operate within an implicit reordering window of 10 and higher. In this work, we evaluate the impact of the extensions suggested by the PSCFG methods above, looking to answer the following questions. Do the relative improvements of PSCFG methods persist when the phrase- based approach is allowed comparable long-distance reordering, and when the ngram language model is strong enoug"
C08-1144,W05-1506,0,0.00590689,"o score chart items. In order to reduce the number of such roundtrip requests in the chart parsing decoding algorithm used for PSCFGs, we batch all n-gram requests for each cell. This single batched request per cell paradigm requires some adaptation of the Cube-Pruning algorithm. Cube-Pruning is an early pruning technique used to limit the generation of low quality chart items during decoding. The algorithm calls for the generation of N-Best chart items at each cell (across all rules spanning that cell). The ngram LM is used to score each generated item, driving the N-Best search algorithm of Huang and Chiang (2005) toward items that score well from a translation model and language model perspective. In order to accomodate batched asynchronous LM requests, we queue n-gram requests for the top N*K chart items without the n-gram LM where K=100. We then generate the top N chart items with the n-gram LM once these probabilties are available. Chart items attempted to be generated during Cube-Pruning that would require LM probabilities of n-grams not in the queued set are discarded. While discarding these items could lead to search errors, in practice they tend to be poorly performing items that do not affect"
C08-1144,P07-1019,0,0.0334331,"1 is short-hand for f1 . . . fi−1 , and where k is an index for the nonterminal X that indicates the one-to-one correspondence between the new X tokens on the two sides (it is not in the space of word indices like i, j, u, v, m, n). The recursive form of this generalization operation allows the generation of rules with multiple nonterminal symbols. Performing translation with PSCFG grammars amounts to straight-forward generalizations of chart parsing algorithms for PCFG grammars. Adaptations to the algorithms in the presence of ngram LMs are discussed in (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007). Extracting hierarchical rules in this fashion can generate a large number of rules and could introduce significant challenges for search. Chiang (2005) places restrictions on the extracted rules which we adhere to as well. We disallow rules with more than two nonterminal pairs, rules with adjacent source-side nonterminals, and limit each rule’s source side length (i.e., number of source terminals and nonterminals) to 6. We extract rules from initial phrases of maximal length 12 (exactly matching the phrase based system).1 Higher length limits or allowing more than two nonterminals per rule d"
C08-1144,N03-1017,1,0.139261,"ove), which constrains the rule’s usage in further composition, and is assigned a weight w, estimating the quality of the rule based on some underlying statistical model. Translation with a PSCFG is thus a process of composing such rules to parse the source language while synchronously generating target language output. PSCFG approaches such as Chiang (2005) and Zollmann and Venugopal (2006) typically begin with a phrase-based model as the foundation for the PSCFG rules described above. Starting with bilingual phrase pairs extracted from automatically aligned parallel text (Och and Ney, 2004; Koehn et al., 2003), these PSCFG approaches augment each contiguous (in source and target words) phrase pair with a left-hand-side symbol (like the VP in the example above), and perform a generalization procedure to form rules that include nonterminal symbols. We can thus view PSCFG methods as an attempt to generalize beyond the purely lexical knowledge represented in phrase based models, allowing reordering decisions to be explicitly encoded in each rule. It is important to note that while phrase-based models cannot explicitly represent context sensitive reordering effects like those in the example above, in pr"
C08-1144,koen-2004-pharaoh,0,0.0483142,"s) on held out data (Och, 2003). Both phrase-based and PSCFG approaches make independence assumptions to structure this search space and thus most features hi (e, f ) are designed to be local to each phrase pair or rule. A notable exception is the n-gram language model (LM), which evaluates the likelihood of the sequential target words output. Phrase-based systems also typically allow source segments to be translated out of order, and include distortion models to evaluate such operations. These features suggest the efficient dynamic programming algorithms for phrase-based systems described in Koehn et al. (2004). We now discuss the translation models compared in this work. 2.1 Phrase Based MT Phrase-based methods identify contiguous bilingual phrase pairs based on automatically generated word alignments (Och et al., 1999). Phrase pairs are extracted up to a fixed maximum length, since very long phrases rarely have a tangible impact during translation (Koehn et al., 2003). During decoding, extracted phrase pairs are reordered to generate fluent target output. Reordered translation output is evaluated under a distortion model and corroborated by one or more n-gram language models. These models do not h"
C08-1144,W06-1606,0,0.0639332,"performing reordering in the context of a nonterminal of type “VB” (verb). VP → ne VB pas, do not VB : w1 ∗ Work done during internships at Google Inc. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. ∗ 1145 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145–1152 Manchester, August 2008 sive improvements of PSCFG over phrase-based approaches for large Chinese-to-English data scenarios (Chiang, 2005; Chiang, 2007; Marcu et al., 2006; DeNeefe et al., 2007), these phrase-based baseline systems were constrained to distortion limits of four (Chiang, 2005) and seven (Chiang, 2007; Marcu et al., 2006; DeNeefe et al., 2007), respectively, while the PSCFG systems were able to operate within an implicit reordering window of 10 and higher. In this work, we evaluate the impact of the extensions suggested by the PSCFG methods above, looking to answer the following questions. Do the relative improvements of PSCFG methods persist when the phrase- based approach is allowed comparable long-distance reordering, and when the ngram languag"
C08-1144,J04-4002,1,0.626597,"in the two rules above), which constrains the rule’s usage in further composition, and is assigned a weight w, estimating the quality of the rule based on some underlying statistical model. Translation with a PSCFG is thus a process of composing such rules to parse the source language while synchronously generating target language output. PSCFG approaches such as Chiang (2005) and Zollmann and Venugopal (2006) typically begin with a phrase-based model as the foundation for the PSCFG rules described above. Starting with bilingual phrase pairs extracted from automatically aligned parallel text (Och and Ney, 2004; Koehn et al., 2003), these PSCFG approaches augment each contiguous (in source and target words) phrase pair with a left-hand-side symbol (like the VP in the example above), and perform a generalization procedure to form rules that include nonterminal symbols. We can thus view PSCFG methods as an attempt to generalize beyond the purely lexical knowledge represented in phrase based models, allowing reordering decisions to be explicitly encoded in each rule. It is important to note that while phrase-based models cannot explicitly represent context sensitive reordering effects like those in the"
C08-1144,W99-0604,1,0.273908,"ule. A notable exception is the n-gram language model (LM), which evaluates the likelihood of the sequential target words output. Phrase-based systems also typically allow source segments to be translated out of order, and include distortion models to evaluate such operations. These features suggest the efficient dynamic programming algorithms for phrase-based systems described in Koehn et al. (2004). We now discuss the translation models compared in this work. 2.1 Phrase Based MT Phrase-based methods identify contiguous bilingual phrase pairs based on automatically generated word alignments (Och et al., 1999). Phrase pairs are extracted up to a fixed maximum length, since very long phrases rarely have a tangible impact during translation (Koehn et al., 2003). During decoding, extracted phrase pairs are reordered to generate fluent target output. Reordered translation output is evaluated under a distortion model and corroborated by one or more n-gram language models. These models do not have an explicit representation of how to reorder phrases. To avoid search space explosion, most systems place a limit on the distance that source segments can be moved within the source sentence. This limit, along"
C08-1144,P03-1021,1,0.0739705,"ertain conditions. 2 Summary of approaches Given a source language sentence f , statistical machine translation defines the translation task as selecting the most likely target translation e under a model P (e|f ), i.e.: ˆ(f ) = arg max P (e|f ) = arg max e e e m X hi (e, f )λi i=1 where the arg max operation denotes a search through a structured space of translation ouputs in the target language, hi (e, f ) are bilingual features of e and f and monolingual features of e, and weights λi are trained discriminitively to maximize translation quality (based on automatic metrics) on held out data (Och, 2003). Both phrase-based and PSCFG approaches make independence assumptions to structure this search space and thus most features hi (e, f ) are designed to be local to each phrase pair or rule. A notable exception is the n-gram language model (LM), which evaluates the likelihood of the sequential target words output. Phrase-based systems also typically allow source segments to be translated out of order, and include distortion models to evaluate such operations. These features suggest the efficient dynamic programming algorithms for phrase-based systems described in Koehn et al. (2004). We now dis"
C08-1144,P02-1040,0,0.104303,"Missing"
C08-1144,P99-1039,0,0.117514,"Missing"
C08-1144,N07-1063,1,0.915986,"Xk env+1 where e.g. f1i−1 is short-hand for f1 . . . fi−1 , and where k is an index for the nonterminal X that indicates the one-to-one correspondence between the new X tokens on the two sides (it is not in the space of word indices like i, j, u, v, m, n). The recursive form of this generalization operation allows the generation of rules with multiple nonterminal symbols. Performing translation with PSCFG grammars amounts to straight-forward generalizations of chart parsing algorithms for PCFG grammars. Adaptations to the algorithms in the presence of ngram LMs are discussed in (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007). Extracting hierarchical rules in this fashion can generate a large number of rules and could introduce significant challenges for search. Chiang (2005) places restrictions on the extracted rules which we adhere to as well. We disallow rules with more than two nonterminal pairs, rules with adjacent source-side nonterminals, and limit each rule’s source side length (i.e., number of source terminals and nonterminals) to 6. We extract rules from initial phrases of maximal length 12 (exactly matching the phrase based system).1 Higher length limits or allowing more than tw"
C08-1144,D07-1090,1,0.363886,"such as “NP/NN.” or “DTNP”. In the spirit of isolating the additional benefit of syntactic categories, the SAMT system used here also generates a purely hierarchical (single generic nonterminal symbol) variant for each syntax-augmented rule. This allows the decoder to choose between translation derivations that use syntactic labels and those that do not. Additional features introduced in SAMT rules are: a relative frequency estimated probability of the rule given its left-hand-side nonterminal, and a binary feature for the the purely hierachial variants. 3 Large N-Gram LMs for PSCFG decoding Brants et al. (2007) demonstrate the value of large high-order LMs within a phrase-based system. Recent results with PSCFG based methods have typically relied on significantly smaller LMs, as a result of runtime complexity within the decoder. In this work, we started with the publicly available PSCFG decoder described in Venugopal et al. (2007) and extended it to efficiently use distributed higher-order LMs under the Cube-Pruning decoding method from Chiang (2007). These extensions allow us to verify that the benefits of PSCFG models persist in the presence of large, powerful ngram LMs. 3.1 Asynchronous N-Gram LM"
C08-1144,W06-3108,0,0.0643621,"ource segments can be moved within the source sentence. This limit, along with the phrase length limit (where local reorderings are implicit in the phrase), determine the scope of reordering represented in a phrase-based system. All experiments in this work limit phrase pairs to have source and target length of at most 12, and either source length or target length of at most 6 (higher limits did not result in additional improvements). In our experiments phrases are extracted by the method described in Och and Ney (2004) and reordering during decoding with the lexicalized distortion model from Zens and Ney (2006). The reordering limit for the phrase based system (for each language pair) is increased until no additional improvements result. 2.2 Hierarchical MT Building upon the success of phrase-based methods, Chiang (2005) presents a PSCFG model of translation that uses the bilingual phrase pairs of phrase-based MT as starting point to learn hierarchical rules. For each training sentence pair’s set of extracted phrase pairs, the set of induced PSCFG rules can be generated as follows: First, each 1146 phrase pair is assigned a generic X-nonterminal as left-hand-side, making it an initial rule. We can n"
C08-1144,W06-3119,1,0.803986,"e translation tasks; Chinese-to-English, Arabic-to-English and Urdu-to-English, each representing unique challenges. As with probabilistic context-free grammars, each rule has a left-hand-side nonterminal (VP and VB in the two rules above), which constrains the rule’s usage in further composition, and is assigned a weight w, estimating the quality of the rule based on some underlying statistical model. Translation with a PSCFG is thus a process of composing such rules to parse the source language while synchronously generating target language output. PSCFG approaches such as Chiang (2005) and Zollmann and Venugopal (2006) typically begin with a phrase-based model as the foundation for the PSCFG rules described above. Starting with bilingual phrase pairs extracted from automatically aligned parallel text (Och and Ney, 2004; Koehn et al., 2003), these PSCFG approaches augment each contiguous (in source and target words) phrase pair with a left-hand-side symbol (like the VP in the example above), and perform a generalization procedure to form rules that include nonterminal symbols. We can thus view PSCFG methods as an attempt to generalize beyond the purely lexical knowledge represented in phrase based models, al"
C08-1144,P05-1033,0,0.901162,"three NIST language translation tasks; Chinese-to-English, Arabic-to-English and Urdu-to-English, each representing unique challenges. As with probabilistic context-free grammars, each rule has a left-hand-side nonterminal (VP and VB in the two rules above), which constrains the rule’s usage in further composition, and is assigned a weight w, estimating the quality of the rule based on some underlying statistical model. Translation with a PSCFG is thus a process of composing such rules to parse the source language while synchronously generating target language output. PSCFG approaches such as Chiang (2005) and Zollmann and Venugopal (2006) typically begin with a phrase-based model as the foundation for the PSCFG rules described above. Starting with bilingual phrase pairs extracted from automatically aligned parallel text (Och and Ney, 2004; Koehn et al., 2003), these PSCFG approaches augment each contiguous (in source and target words) phrase pair with a left-hand-side symbol (like the VP in the example above), and perform a generalization procedure to form rules that include nonterminal symbols. We can thus view PSCFG methods as an attempt to generalize beyond the purely lexical knowledge repr"
C08-1144,J07-2003,0,0.701429,"e) word “not”, performing reordering in the context of a nonterminal of type “VB” (verb). VP → ne VB pas, do not VB : w1 ∗ Work done during internships at Google Inc. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. ∗ 1145 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1145–1152 Manchester, August 2008 sive improvements of PSCFG over phrase-based approaches for large Chinese-to-English data scenarios (Chiang, 2005; Chiang, 2007; Marcu et al., 2006; DeNeefe et al., 2007), these phrase-based baseline systems were constrained to distortion limits of four (Chiang, 2005) and seven (Chiang, 2007; Marcu et al., 2006; DeNeefe et al., 2007), respectively, while the PSCFG systems were able to operate within an implicit reordering window of 10 and higher. In this work, we evaluate the impact of the extensions suggested by the PSCFG methods above, looking to answer the following questions. Do the relative improvements of PSCFG methods persist when the phrase- based approach is allowed comparable long-distance reordering, and wh"
C08-1144,A00-2018,0,\N,Missing
C98-2157,J93-2003,0,0.0160365,"Missing"
C98-2157,1995.tmi-1.18,0,0.0590398,"r m u t a t i o n of the numbers 1 , . . . , L . 5 Learning of Category S y s t e m s During the last decade some publications have discussed the problem of learning WCs using clustering techniques based on m a x i m m n likelihood criteria applied to single language corpora. The question which we pose in addition is: Which WCs are suitable for translation? It seems to make sense to require that the used WCs in the two languages are correlated, so that the information about the class of a SL word gives much information about the class of the generated TL word. Therefore it has been argued in (Fung and Wu, 1995) that independently generated WCs are not good for the use in translation. For the automatic generation of class systems exists a well known procedure (see (Kneser and Ney, 1993), (Och, 1995)) which maximizes the perplexity of the language model for a training corpus by moving one word from a class to another in an iterative procedure. T h e function ML(CINw--+w,) which has to be optinfized depends only on the count flmction Nw-+w, which counts the frequency that the word w' comes after the word w. Using two sets of WCs for tile TL and SL which are independent (method INDEP) does not guarantee"
C98-2157,C96-2141,0,0.0579863,"for the probability that cj is generated by di. The assumption that each SL word influences every TL word with the same strength appears to be too simple. In the refined model 2 (Brown et al., 1993) alignment probabilities a(itj ,l, m) are included to model the effect that the position of a word influences the position of its translation. The phrasal organization of natural languages is well known and has been described by (Jackendorff, 1977) among many others. The traditional alignment probabilities depend on absolute positions and do not take that into account, as has already been noted by (Vogel et al., 1996). Therefore we developed a kind of relative weighting probability. The following model - - which we will call the model 2~ - makes the weight between the words di and ej dependent on the relative distances between the words dk which generated the previous word ~~ ~stmonlag Figure 1: Alignment example. els can be described in the form m ej-1: l P(eld) ~ l~ ~ j=l i=0 l s(ilj, e j _ l , d ) ~ ~ d(i - kll) . t(ej_lJdk) (3) k---0 Here d(i - kll ) is the probability that word di influences a word ej if the previous word ejLt is influenced by dk. As an effect of such a weight a ( p h r a s e ) c l u"
D07-1005,N06-1013,0,0.0157372,"9 53.2 40.7 52.4 39.9 49.7 37.9 53.8 54.1 54.8∗ 41.5 42.0∗ 42.2∗ 53.9 54.3+ 54.9+ 42.2 42.3 42.6+ Table 7: Translation performance for Sets 2 and 3 on test and blind:NIST portion of 2006 NIST eval set. terpolating posterior probability matrices from different sources. In our case, these sources are multiple bridge languages. However, this method is more generally applicable for combining posterior matrices from different alignment models such as HMM and Model-4. Such an approach contrasts with the log-linear HMM/Model-4 combination proposed by Och and Ney (2003). There has been recent work by Ayan and Dorr (2006) on combining word alignments from different alignment systems; this paper describes a maximum entropy framework for this combination. Their approach operates at the level of the alignment links and uses maximum entropy to decide whether or not to include an alignment link in the final output. In contrast, we use posterior probabilities as the interface between different alignment models. Another difference is that this maxent framework requires human word aligned data for training feature weights. We do not require any human word aligned data to train our combiner. Another advantage of our ap"
D07-1005,C02-1134,0,0.02351,"h versus Russian-English) make errors which are somewhat orthogonal. In such cases, incorrect alignment links between a sentence-pair can be corrected when a translation in a third language is available. Thus it can help resolve errors in word alignment. We combine word alignments using several bridge languages with the aim of correcting some of the alignment errors. The second advantage of this approach is that the word alignment from each bridge language can be utilized to build a phrasebased SMT system. This provides a diverse collection of translation hypotheses for MT system combination (Bangalore et al., 2002; Sim et al., 2007; Matusov et al., 2006; Macherey and Och, 2007). Finally, a side benefit of this paper is that it provides a study that compares alignment qualities and BLEU scores for models in different languages trained on parallel text which is held identical across all languages. We show that parallel corpora in multiple languages can be exploited to improve the translation performance of a phrase-based translation system. This paper gives specific recipes for using a bridge language to construct a word alignment and for combining word alignments produced by multiple statistical alignme"
D07-1005,C00-1015,0,0.0862933,"phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used to combine direct translation and indirect translation via a third language. Filali and Bilmes (2005) present a multi-lingual extension to the IBM/HMM models. Our current approach differs from this latter work in that we propose a simple framework to combine word alignments from any underlying statistical alignment model without the need for changing the structure of the model. While both of the above papers focus on improving word alignment quality, we demonstrate that our approach can yield improvements in translation perf"
D07-1005,D07-1090,1,0.317483,"Missing"
D07-1005,J93-2003,0,0.0199082,"lignments produced by multiple statistical alignment models. The rest of this paper is organized as follows: Section 2 gives an overview of our framework for generating word alignments in a single language-pair. In Section 3, we describe how a bridge language may be used for producing word alignments. In Section 4, we describe a scheme to combine word alignments from several bridge languages. Section 5 describes our experimental setup and reports the alignment and translation performance. A final discussion is presented in Section 6. 2 Word Alignment Framework A statistical translation model (Brown et al., 1993; Och and Ney, 2003) describes the relationship between a pair of sentences in the source and target languages (f = f1J , e = eI1 ) using a translation probability P (f |e). Alignment models introduce a hidden alignment variable a = aJ1 to specify a mapping between source and target words; aj = i indicates that the j th source word is linked to the ith 43 target word. Alignment models assign a probability P (f , a|e) to the source sentence and alignment conditioned on the target sentence. The translation probability is related to the alignment model as: P P (f |e) = a Pθ (f , a|e), where θ is"
D07-1005,H05-1022,0,0.0108564,"ultilingual, parallel text in Spanish, French, Russian, and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task. 1 Introduction Word Alignment of parallel texts forms a crucial component of phrase-based statistical machine translation systems. High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used to combine direct translation and indirect translation via a third language. Filali and Bilmes (2005) present a multi-lingual extension to the IBM/HMM models. Our current approach differs"
D07-1005,P06-1097,0,0.0279256,"inal translation is obtained by consensus decoding that combines hypotheses obtained using all bridge language word alignments. We present experiments showing that multilingual, parallel text in Spanish, French, Russian, and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task. 1 Introduction Word Alignment of parallel texts forms a crucial component of phrase-based statistical machine translation systems. High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used to combine direct transla"
D07-1005,H05-1012,0,0.0172848,"tion performance on an Arabic-to-English task. 1 Introduction Word Alignment of parallel texts forms a crucial component of phrase-based statistical machine translation systems. High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used to combine direct translation and indirect translation via a third language. Filali and Bilmes (2005) present a multi-lingual extension to the IBM/HMM models. Our current approach differs from this latter work in that we propose a simple framework to combine word alignments from any underlying statistical alignment"
D07-1005,W04-3250,0,0.101081,"Missing"
D07-1005,N04-1022,1,0.102735,"Missing"
D07-1005,D07-1105,1,0.0614967,"nal. In such cases, incorrect alignment links between a sentence-pair can be corrected when a translation in a third language is available. Thus it can help resolve errors in word alignment. We combine word alignments using several bridge languages with the aim of correcting some of the alignment errors. The second advantage of this approach is that the word alignment from each bridge language can be utilized to build a phrasebased SMT system. This provides a diverse collection of translation hypotheses for MT system combination (Bangalore et al., 2002; Sim et al., 2007; Matusov et al., 2006; Macherey and Och, 2007). Finally, a side benefit of this paper is that it provides a study that compares alignment qualities and BLEU scores for models in different languages trained on parallel text which is held identical across all languages. We show that parallel corpora in multiple languages can be exploited to improve the translation performance of a phrase-based translation system. This paper gives specific recipes for using a bridge language to construct a word alignment and for combining word alignments produced by multiple statistical alignment models. The rest of this paper is organized as follows: Sectio"
D07-1005,N01-1020,0,0.208634,"weights. We do not require any human word aligned data to train our combiner. Another advantage of our approach is that it is based on word alignment posterior probability matrices that can be generated by any underlying alignment model. Therefore, this method can be used to combine word alignments generated by fairly dissimilar word alignment systems as long as the systems can produce posterior probabilities. Bridge languages have been used by NLP researchers as a means to induce translation lexicons between distant languages without the need for parallel corpora (Schafer and Yarowsky, 2002; Mann and Yarowsky, 2001). Our current approach differs 49 from these efforts in that we use bridge languages to improve word alignment quality between sentence pairs. Furthermore, we do not use linguistic insight to identify bridge languages. In our framework, a good bridge language is one that provides the best translation performance using the posterior matrix multiplication. Our experiments show that Spanish is a better bridge language relative to Chinese for Arabic-to-English translation. We speculate that if our approach was carried out on a data set with hundreds of languages, we might be able to automatically"
D07-1005,W05-0809,0,0.0105281,"text in Spanish, French, Russian, and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task. 1 Introduction Word Alignment of parallel texts forms a crucial component of phrase-based statistical machine translation systems. High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used to combine direct translation and indirect translation via a third language. Filali and Bilmes (2005) present a multi-lingual extension to the IBM/HMM models. Our current approach differs from this latter work"
D07-1005,C04-1032,0,0.0213198,"aJ1 to specify a mapping between source and target words; aj = i indicates that the j th source word is linked to the ith 43 target word. Alignment models assign a probability P (f , a|e) to the source sentence and alignment conditioned on the target sentence. The translation probability is related to the alignment model as: P P (f |e) = a Pθ (f , a|e), where θ is a set of parameters. Given a sentence-pair (f , e), the most likely (Viterbi) word alignment is found as (Brown et al., ˆ = argmaxa P (f , a|e). An alternate cri1993): a terion is the Maximum A-Posteriori (MAP) framework (Ge, 2004; Matusov et al., 2004). We use a refinement of this technique. Given any word alignment model, posterior probabilities can be computed as (Brown et al., 1993) P (aj = i|e, f ) = X P (a|f , e)δ(i, aj ), (1) a where i ∈ {0, 1, ..., I}. The assignment aj = 0 corresponds to the NULL (empty) alignment. These posterior probabilities form a matrix of size (I +1)× J, where entries along each column sum to one. The MAP alignment for each source position j ∈ {1, 2, ..., J} is then computed as aM AP (j) = argmax P (aj = i|e, f ). (2) i We note that these posterior probabilities can be computed efficiently for some alignment m"
D07-1005,E06-1005,0,0.0308912,"h are somewhat orthogonal. In such cases, incorrect alignment links between a sentence-pair can be corrected when a translation in a third language is available. Thus it can help resolve errors in word alignment. We combine word alignments using several bridge languages with the aim of correcting some of the alignment errors. The second advantage of this approach is that the word alignment from each bridge language can be utilized to build a phrasebased SMT system. This provides a diverse collection of translation hypotheses for MT system combination (Bangalore et al., 2002; Sim et al., 2007; Matusov et al., 2006; Macherey and Och, 2007). Finally, a side benefit of this paper is that it provides a study that compares alignment qualities and BLEU scores for models in different languages trained on parallel text which is held identical across all languages. We show that parallel corpora in multiple languages can be exploited to improve the translation performance of a phrase-based translation system. This paper gives specific recipes for using a bridge language to construct a word alignment and for combining word alignments produced by multiple statistical alignment models. The rest of this paper is org"
D07-1005,H05-1011,0,0.0262485,"prove translation performance on an Arabic-to-English task. 1 Introduction Word Alignment of parallel texts forms a crucial component of phrase-based statistical machine translation systems. High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used to combine direct translation and indirect translation via a third language. Filali and Bilmes (2005) present a multi-lingual extension to the IBM/HMM models. Our current approach differs from this latter work in that we propose a simple framework to combine word alignments from any un"
D07-1005,J03-1002,1,0.13439,"ge languages. The final translation is obtained by consensus decoding that combines hypotheses obtained using all bridge language word alignments. We present experiments showing that multilingual, parallel text in Spanish, French, Russian, and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task. 1 Introduction Word Alignment of parallel texts forms a crucial component of phrase-based statistical machine translation systems. High quality word alignments can yield more accurate phrase-pairs which improve quality of a phrase-based SMT system (Och and Ney, 2003; Fraser and Marcu, 2006b). Much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling (Och and Ney, 2003; Deng and Byrne, 2005; Martin et al., 2005) or alternative approaches to training (Fraser and Marcu, 2006b; Moore, 2005; Ittycheriah and Roukos, 2005). In this paper we explore a complementary approach to improve word alignments using multi-lingual, parallel (or multi-parallel) corpora. Two works in the literature are very relevant to our approach. Borin (2000) describes a non-statistical approach where a pivot alignment is used t"
D07-1005,J04-4002,1,0.434777,". The posterior matrix for AE is obtained using AX and XE matrices while the EA matrix is obtained from EX and XA matrices (Equation 4). The AE (EA) matrices from the bridge languages are then interpolated with the AE (EA) matrix obtained from the alignment model trained directly on Arabic-English (Section 4). The MAP word alignment for AE (EA) direction is computed from the AE (EA) matrix. We next outline how these word alignments are utilized in building a phrasebased SMT system. 5.4 Phrase-based SMT system Our phrase-based SMT system is similar to the alignment template system described in Och and Ney (2004). We first extract an inventory of phrasepairs up to length 7 from the union of AE and EA word alignments. Various feature functions (Och and Ney, 2004) are then computed over the entries in the phrase table. 5-gram word language models in English are trained on a variety of monolingual corpora (Brants et al., 2007). Minimum Error Rate Training (MERT) (Och, 2003) under BLEU criterion is used to estimate 20 feature function weights over the larger development set (dev1). Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004). Decoding is done in tw"
D07-1005,P03-1021,1,0.0253399,"Missing"
D07-1005,W02-2026,0,0.049273,"d data for training feature weights. We do not require any human word aligned data to train our combiner. Another advantage of our approach is that it is based on word alignment posterior probability matrices that can be generated by any underlying alignment model. Therefore, this method can be used to combine word alignments generated by fairly dissimilar word alignment systems as long as the systems can produce posterior probabilities. Bridge languages have been used by NLP researchers as a means to induce translation lexicons between distant languages without the need for parallel corpora (Schafer and Yarowsky, 2002; Mann and Yarowsky, 2001). Our current approach differs 49 from these efforts in that we use bridge languages to improve word alignment quality between sentence pairs. Furthermore, we do not use linguistic insight to identify bridge languages. In our framework, a good bridge language is one that provides the best translation performance using the posterior matrix multiplication. Our experiments show that Spanish is a better bridge language relative to Chinese for Arabic-to-English translation. We speculate that if our approach was carried out on a data set with hundreds of languages, we might"
D07-1005,W99-0602,0,0.270847,"can still give smaller improvements (0.3/0.5 and 1.0/0.7 points) relative to this baseline. As mentioned earlier, our approach requires sentence-aligned corpora. In our experiments, we use a single sentence aligner for each language pair (total of 9 aligners). Since these aligners make independent decisions on sentence boundaries, we end 48 up with a smaller pool of sentences (1.9M) that is common across all language pairs. In contrast, a sentence aligner that makes simultaneous decisions in multiple languages would result in a larger set of common sentence pairs (close to 7M sentence pairs). Simard (1999) describes a sentence aligner of this type that improves alignment on a trilingual parallel text. Since we do not currently have access to such an aligner, we simulate that situation with Sets 2 and 3: AC2/AC3 do not insist that a sentence-pair be present in all input word alignments. We note that Set 2 is a data scenario that falls between Sets 1 and 3. Set 3 provides the best baseline for ArabicEnglish based on the UN data by training on all parallel sentence-pairs. In this situation, system combination with bridge languages (AC3/TC3) gives reasonable improvements in BLEU on the test set (0."
D07-1005,H05-1096,0,0.0100669,"tion. Our experiments show that Spanish is a better bridge language relative to Chinese for Arabic-to-English translation. We speculate that if our approach was carried out on a data set with hundreds of languages, we might be able to automatically identify language families. A downside of our approach is the requirement for exact sentence-aligned parallel data. Except for a few corpora such as UN, European Parliament etc, such a resource is hard to find. One solution is to create such parallel data by automatic translation and then retaining reliable translations by using confidence metrics (Ueffing and Ney, 2005). Our approach to using bridge languages is extremely simple. Despite its simplicity, the system combination gives improvements in alignment and translation performance. In future work, we will consider several extensions to this framework that lead to more powerful system combination strategies using multiple bridge languages. We recall that the present approach trains bridge systems (e.g. Arabicto-French, French-to-English) until the alignment stage and then uses these for constructing Arabicto-English word alignment. An alternate scenario would be to build phrase-based SMT systems for Arabi"
D07-1005,C96-2141,0,0.16313,"f this technique. Given any word alignment model, posterior probabilities can be computed as (Brown et al., 1993) P (aj = i|e, f ) = X P (a|f , e)δ(i, aj ), (1) a where i ∈ {0, 1, ..., I}. The assignment aj = 0 corresponds to the NULL (empty) alignment. These posterior probabilities form a matrix of size (I +1)× J, where entries along each column sum to one. The MAP alignment for each source position j ∈ {1, 2, ..., J} is then computed as aM AP (j) = argmax P (aj = i|e, f ). (2) i We note that these posterior probabilities can be computed efficiently for some alignment models such as the HMM (Vogel et al., 1996; Och and Ney, 2003), Models 1 and 2 (Brown et al., 1993). In the next two sections, we describe how posterior probabilities can be used to a) construct alignment systems from a bridge language, and b) merge several alignment systems. 3 Constructing Word Alignment Using a Bridge Language We assume here that we have triples of sentences that are translations of each other in languages F, E, and the bridge language G: f = f1J , e = eI1 , g = g1K . Our goal is to obtain posterior probability estimates for the sentence-pair in FE: (f , e) using the posterior probability estimates for the sentence"
D07-1005,J07-3002,0,\N,Missing
D07-1090,J93-2003,0,0.0604968,"to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 1 Introduction Given a source-language (e.g., French) sentence f , the problem of machine translation is to automatically produce a target-language (e.g., English) transˆ. The mathematics of the problem were forlation e malized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization ˆ = arg max e e M X λm hm (e, f ) (1) m=1 where {hm (e, f )} is a set of M feature functions and {λm } a set of weights. One or more feature functions may be of the form h(e, f ) = h(e), in which case it is referred to as a language model. We focus on n-gram language models, which are trained on unlabeled monolingual text. As a general rule, more data tends to yield better language models. Questions that arise in this context include: (1) How might one build a language model that allows scaling to very large amounts of tra"
D07-1090,W04-3250,0,0.0773316,"are kept the same. Results are shown in Figure 5. The first part of the curve uses target data for training the language model. With Kneser-Ney smoothing (KN), the BLEU score improves from 0.3559 for 13 million tokens to 0.3832 for 237 million tokens. At such data sizes, Stupid Backoff (SB) with a constant backoff parameter α = 0.4 is around 1 BP worse than KN. On average, one gains 0.62 BP for each doubling of the training data with KN, and 0.66 BP per doubling with SB. Differences of more than 0.51 BP are statistically significant at the 0.05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004). We then add a second language model using ldcnews data. The first point for ldcnews shows a large improvement of around 1.4 BP over the last point for target for both KN and SB, which is approximately twice the improvement expected from doubling the amount of data. This seems to be caused by adding a new domain and combining two models. After that, we find an improvement of 0.56–0.70 BP for each doubling of the ldcnews data. The gap between Kneser-Ney Smoothing and Stupid Backoff narrows, starting with a difference of 0.85 BP and ending with a not significant difference of 0.24 BP. Adding a"
D07-1090,J04-4002,1,0.289967,"models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases. 1 Introduction Given a source-language (e.g., French) sentence f , the problem of machine translation is to automatically produce a target-language (e.g., English) transˆ. The mathematics of the problem were forlation e malized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization ˆ = arg max e e M X λm hm (e, f ) (1) m=1 where {hm (e, f )} is a set of M feature functions and {λm } a set of weights. One or more feature functions may be of the form h(e, f ) = h(e), in which case it is referred to as a language model. We focus on n-gram language models, which are trained on unlabeled monolingual text. As a general rule, more data tends to yield better language models. Questions that arise in this context include: (1) How might one build a language model that allows scaling to very large amounts of training data? (2) How much does translation"
D07-1090,P02-1040,0,0.10668,"ing. We focused on machine translation when describing the queued language model access. However, it is general enough that it may also be applicable to speech decoders and optical character recognition systems. 7 Experiments We trained 5-gram language models on amounts of text varying from 13 million to 2 trillion tokens. The data is divided into four sets; language models are trained for each set separately 4 . For each training data size, we report the size of the resulting language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al., 2002) obtained by the machine translation system. For smaller training sizes, we have also computed test-set perplexity using Kneser-Ney Smoothing, and report it for comparison. 7.1 Data Sets We compiled four language model training data sets, listed in order of increasing size: 3 One additional round for the sentence end marker. Experience has shown that using multiple, separately trained language models as feature functions in Eq (1) yields better results than using a single model trained on all data. 4 1e+12 x1.6/x2 1000 x1.8/x2 100 1e+10 x1.8/x2 10 1e+09 1 x1.8/x2 target +ldcnews +webnews +web"
D07-1090,W06-1626,0,0.0216258,"01) for a discussion of n-gram models and smoothing. In principle, the predictive accuracy of the language model can be improved by increasing the order of the n-gram. However, doing so further exacerbates the sparse data problem. The present work addresses the challenges of processing an amount of training data sufficient for higher-order n-gram models and of storing and managing the resulting values for efficient use by the decoder. 3 Related Work on Distributed Language Models The topic of large, distributed language models is relatively new. Recently a two-pass approach has been proposed (Zhang et al., 2006), wherein a lowerorder n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model. The resulting translation performance was shown to improve appreciably over the hypothesis deemed best by the first-stage system. The amount of data used was 3 billion words. More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al., 2007). The underlying architecture is similar to (Zhang et al., 2006). The difference is that they"
D07-1105,W05-0909,0,0.0362644,"01) used a multiple string alignment algorithm in order to compute a single confusion network, on which a consensus hypothesis was computed through majority voting. Because the alignment procedure was based on the Levenshtein distance, it was unable to align translations with significantly different word orders. (Jayaraman and Lavie, 2005) tried to overcome this problem by using confidence scores and language models in order to rank a collection of synthetic combinations of words extracted from the original translation hypotheses. Experimental results were only reported for the METEOR metric (Banerjee and Lavie, 2005). In (Matusov et al., 2006), pairwise word alignments of the original translation hypotheses were estimated for an enhanced statistical alignment model in order 989 Table 1: Corpus statistics for two Chinese-English text translation sets: ZHEN-05 is a random selection of test data used in NIST evaluations prior to 2006; ZHEN-06 comprises the NIST portion of the Chinese-English evaluation data used in the 2006 NIST machine translation evaluation. corpus ZHEN-05 ZHEN-06 sentences chars / words sentences chars / words Chinese English 2390 110647 67737 1664 64292 41845 to explicitly capture word r"
D07-1105,D07-1090,1,0.417883,"Missing"
D07-1105,P05-3026,0,0.071748,"oach is described that combines outputs from three translation systems to build a consensus translation. (Nomoto, 2004) and (Paul et al., 2005) used translation scores, language and other models to select one of the original translations as consensus translation. (Bangalore et al., 2001) used a multiple string alignment algorithm in order to compute a single confusion network, on which a consensus hypothesis was computed through majority voting. Because the alignment procedure was based on the Levenshtein distance, it was unable to align translations with significantly different word orders. (Jayaraman and Lavie, 2005) tried to overcome this problem by using confidence scores and language models in order to rank a collection of synthetic combinations of words extracted from the original translation hypotheses. Experimental results were only reported for the METEOR metric (Banerjee and Lavie, 2005). In (Matusov et al., 2006), pairwise word alignments of the original translation hypotheses were estimated for an enhanced statistical alignment model in order 989 Table 1: Corpus statistics for two Chinese-English text translation sets: ZHEN-05 is a random selection of test data used in NIST evaluations prior to"
D07-1105,N03-1017,1,0.00851594,"nslation quality benefit from combining systems developed by multiple research labs? Despite an increasing number of translation engines, most state-of-the-art systems in statistical machine translation are nowadays based on implementations of the same techniques. For instance, word alignment models are often trained using the GIZA++ toolkit (Och and Ney, 2003); error minimizing training criteria such as the Minimum Error Rate Training (Och, 2003) are employed in order to learn feature function weights for log-linear models; and translation candidates are produced using phrase-based decoders (Koehn et al., 2003) in combination with n-gram language models (Brants et al., 2007). All these methods are established as de facto standards and form an integral part of most statistical machine translation systems. This, however, raises the question as to what extent translation quality can be expected to improve when similarly designed systems are combined. • How can a set of diverse translation systems be built from a single translation engine? Without having access to different translation 986 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Na"
D07-1105,W04-3250,0,0.151561,"Missing"
D07-1105,N04-1022,0,0.0863793,"useful properties: • The selection does not depend on scored translation outputs; the mere target word sequence is sufficient. Hence, this technique is also applicable to rule-based translation systems 2 . • Using the components of the row-vector bm as feature function values for the candidate translation em (m  1, ..., M ), the system prior weights ω can easily be trained using the Minimum Error Rate Training described in (Och, 2003). Note that the candidate selection rule in Eq. (2) is equivalent to re-ranking candidate translations according to the Minimum Bayes Risk (MBR) decision rule (Kumar and Byrne, 2004), provided that the system prior weights are used as estimations of the posterior probabilities ppe|f q for a source sentence f . Due to the proximity of this method to the MBR selection rule, we call this combination scheme MBR-like system combination. 2.2 ROVER-Like Combination Schemes ROVER-like combination schemes aim at computing a composite translation by voting on confusion networks that are built from translation outputs of multiple machine translation engines via an iterative application of alignments (Fiscus, 1997). To accomplish this, one of the original candidate translations, e.g."
D07-1105,E06-1005,0,0.540655,"cognition (Fiscus, 1997), a composite translation is computed by voting on the translation outputs of multiple machine translation systems. Depending on how the translations are combined and how the voting scheme is implemented, the composite translation may differ from any of the original hypotheses. While elementary approaches simply select for each sentence one of the original translations, more sophisticated methods allow for combining translations on a word or a phrase level. Although system combination could be shown to result in substantial improvements in terms of translation quality (Matusov et al., 2006; Sim et al., 2007), not every possible ensemble of translation outputs has the potential to outperform the primary translation system. In fact, an adverse combination of translation systems may even deteriorate translation quality. This holds to a greater extent, when the ensemble of translation outputs contains a significant number of translations produced by low performing but highly correlated systems. In this paper we present an empirical study on how different ensembles of translation outputs affect performance in system combination. In particular, we will address the following questions"
D07-1105,P04-1063,0,0.351403,"te translation from system outputs of multiple machine translation engines. Depending on how the systems are combined and which voting scheme is implemented, the consensus translation may differ from any of the original candidate translations. In this section, we discuss three approaches to system combination. 2.1 System Combination via Candidate Selection The easiest and most straightforward approach to system combination simply returns one of the original candidate translations. Typically, this selection is made based on translation scores, confidence estimations, language and other models (Nomoto, 2004; Paul et al., 2005). For many machine translation systems, however, the scores are often not normalized or may even not be available, which makes it difficult to apply this technique. We therefore propose an alternative method based on “correlation matrices” computed from the BLEU performance measure (Papineni et al., 2001). Let e1 , ..., eM denote the outputs of M translation systems, each given as a sequence of words in the target language. An element of the BLEU correlation matrix B  pbij q is defined as the sentence-based BLEU score between a candidate translation ei and a pseudo-referen"
D07-1105,J03-1002,1,0.0110244,"forming but highly correlated systems. In this paper we present an empirical study on how different ensembles of translation outputs affect performance in system combination. In particular, we will address the following questions: • To what extent can translation quality benefit from combining systems developed by multiple research labs? Despite an increasing number of translation engines, most state-of-the-art systems in statistical machine translation are nowadays based on implementations of the same techniques. For instance, word alignment models are often trained using the GIZA++ toolkit (Och and Ney, 2003); error minimizing training criteria such as the Minimum Error Rate Training (Och, 2003) are employed in order to learn feature function weights for log-linear models; and translation candidates are produced using phrase-based decoders (Koehn et al., 2003) in combination with n-gram language models (Brants et al., 2007). All these methods are established as de facto standards and form an integral part of most statistical machine translation systems. This, however, raises the question as to what extent translation quality can be expected to improve when similarly designed systems are combined."
D07-1105,P03-1021,1,0.265874,"ent ensembles of translation outputs affect performance in system combination. In particular, we will address the following questions: • To what extent can translation quality benefit from combining systems developed by multiple research labs? Despite an increasing number of translation engines, most state-of-the-art systems in statistical machine translation are nowadays based on implementations of the same techniques. For instance, word alignment models are often trained using the GIZA++ toolkit (Och and Ney, 2003); error minimizing training criteria such as the Minimum Error Rate Training (Och, 2003) are employed in order to learn feature function weights for log-linear models; and translation candidates are produced using phrase-based decoders (Koehn et al., 2003) in combination with n-gram language models (Brants et al., 2007). All these methods are established as de facto standards and form an integral part of most statistical machine translation systems. This, however, raises the question as to what extent translation quality can be expected to improve when similarly designed systems are combined. • How can a set of diverse translation systems be built from a single translation engine"
D07-1105,2001.mtsummit-papers.68,0,0.0431115,"ns, the first of which is compiled from a random selected subset of evaluation data used in the NIST MT evaluations up to the year 2005. The second data set consists of the NIST portion of the Chinese-English data used in the MT06 evaluation and comprises 1664 Chinese sentences collected from broadcast news articles (565 sentences), newswire texts (616 sentences), and news group texts (483 sentences). Both corpora provide 4 reference translations per source sentence. Table 1 summarizes some corpus statistics. For all experiments, system performance was measured in terms of the IBM-BLEU score (Papineni et al., 2001). Compared to the NIST implementation of the BLEU score, IBM-BLEU follows the original definition of the brevity penalty (BP) factor: while in the NIST implementation the BP is always based on the length of the shortest reference translation, the BP in the IBM-BLEU score is based on the length of the reference translation which is closest to the candidate translation length. Typically, IBM-BLEU scores tend to be smaller than NISTBLEU scores. In the following, BLEU always refers to the IBM-BLEU score. Except for the results reported in Section 3.2, we used uniform system prior weights throughou"
D07-1105,2005.iwslt-1.5,0,0.202754,"from system outputs of multiple machine translation engines. Depending on how the systems are combined and which voting scheme is implemented, the consensus translation may differ from any of the original candidate translations. In this section, we discuss three approaches to system combination. 2.1 System Combination via Candidate Selection The easiest and most straightforward approach to system combination simply returns one of the original candidate translations. Typically, this selection is made based on translation scores, confidence estimations, language and other models (Nomoto, 2004; Paul et al., 2005). For many machine translation systems, however, the scores are often not normalized or may even not be available, which makes it difficult to apply this technique. We therefore propose an alternative method based on “correlation matrices” computed from the BLEU performance measure (Papineni et al., 2001). Let e1 , ..., eM denote the outputs of M translation systems, each given as a sequence of words in the target language. An element of the BLEU correlation matrix B  pbij q is defined as the sentence-based BLEU score between a candidate translation ei and a pseudo-reference translation ej pi"
D07-1105,A94-1016,0,0.191492,"re based on the repository of weighted pseudo references. Lowscoring hypotheses are pruned to keep the space of active hypotheses small. The algorithm will finish if either no constituents are left or if expanding the set of active hypotheses does not further decrease the TER score. Optionally, the best consensus hypothesis found by the two-pass search is combined with all input translation systems via the MBR-like combination scheme described in Section 2.1. This refinement is called two-pass+. 2.4 Related Work Research on multi-engine machine translation goes back to the early nineties. In (Robert and Nirenburg, 1994), a semi-automatic approach is described that combines outputs from three translation systems to build a consensus translation. (Nomoto, 2004) and (Paul et al., 2005) used translation scores, language and other models to select one of the original translations as consensus translation. (Bangalore et al., 2001) used a multiple string alignment algorithm in order to compute a single confusion network, on which a consensus hypothesis was computed through majority voting. Because the alignment procedure was based on the Levenshtein distance, it was unable to align translations with significantly d"
D07-1105,2006.amta-papers.25,0,0.0630503,"Missing"
D07-1105,P02-1040,0,\N,Missing
D07-1105,2005.eamt-1.20,0,\N,Missing
D08-1065,J90-2002,0,0.709734,"Missing"
D08-1065,W07-0414,0,0.0250485,"s are given by ∂ log B −1 = , (9) ∂c0 c0 ∂ log B 1 = . ∂cn 4cn Substituting the derivatives in Equation 8 gives 4 ∆c0 1 X ∆cn G = ∆ log B ≈ − + , (10) c0 4 cn n=1 where each ∆cn = c0n − cn counts the statistic in the sentence of interest, rather than the corpus as a whole. This score is therefore a linear function in counts of words ∆c0 and n-gram matches ∆cn . Our approach ignores the count clipping present in the exact BLEU score where a correct n-gram present once in the reference but several times in the hypothesis will be counted only once as correct. Such an approach is also followed in Dreyer et al. (2007). Using the above first-order approximation to gain in log corpus BLEU, Equation 9 implies that θ0 , θw from Section 3 would have the following values: −1 θ0 = (11) c0 1 θw = . 4c|w| 624 5.1 N-gram Factors We now describe how the n-gram factors (Equation 11) are computed. The factors depend on a set of n-gram matches and counts (cn ; n ∈ {0, 1, 2, 3, 4}). These factors could be obtained from a decoding run on a development set. However, doing so could make the performance of lattice MBR very sensitive to the actual BLEU scores on a particular run. We would like to avoid such a dependence and i"
D08-1065,P07-2026,0,0.0533243,"2 28.1 44.2 42 28 44 41.8 27.9 0 0.2 0.4 0.6 Scale Factor 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 Scale Factor 0.4 0.6 0.8 1 Scale Factor Figure 3: Lattice MBR with various scale factors α: aren/zhen/enzh. tor on the performance of lattice MBR. The scale factor determines the flatness of the posterior distribution over translation hypotheses. A scale of 0.0 means a uniform distribution while 1.0 implies that there is no scaling. This is an important parameter that needs to be tuned on a development set. There has been prior work in MBR speech recognition and machine translation (Goel and Byrne, 2000; Ehling et al., 2007) which has shown the need for tuning this factor. Our MT system parameters are trained with Minimum Error Rate Training which assigns a very high posterior probability to the MAP translation. As a result, it is necessary to flatten the probability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis. Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases,"
D08-1065,P96-1024,0,0.0360172,"/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximati"
D08-1065,W04-3250,0,0.31722,"Missing"
D08-1065,W02-1019,1,0.823353,"Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU los"
D08-1065,N04-1022,1,0.86405,"sed on string-edit distance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) giv"
D08-1065,W07-0734,0,0.0142537,"o the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with transla628 tion metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures. We believe that our lattice MBR framework can be extended to such hypergraphs with loss functions that take into account both BLEU scores as well as parse tree structures. Lattice and Forest based search and training procedures are not yet common in statistical machine translation. However, they are promising because the search space of translations is much larger than the ty"
D08-1065,D08-1076,1,0.481937,"63 919 919 1360 1357 1859 Table 1: Statistics over the development and test sets. gain. We start with a description of the data sets and the SMT system. 7.1 Development and Blind Test Sets We present our experiments on the constrained data track of the NIST 2008 Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh) machine translation tasks.5 In all language pairs, the parallel and monolingual data consists of all the allowed training sets in the constrained track. For each language pair, we use two development sets: one for Minimum Error Rate Training (Och, 2003; Macherey et al., 2008), and the other for tuning the scale factor for MBR decoding. Our development sets consists of the NIST 2004/2003 evaluation sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5 http://www.nist.gov/speech/tests/mt/ 625 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We"
D08-1065,C04-1032,0,0.0141837,"ption Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We first perform sentence and sub-sentence chunk alignment on the parallel documents. We then train word alignment models (Och and Ney, 2003) using 6 Model-1 iterations and 6 HMM iterations. An additional 2 iterations of Model-4 are performed for zhen and enzh pairs. Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework (Matusov et al., 2004). An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments. Several feature functions are then computed over the phrasepairs. 5-gram word language models are trained on the allowed monolingual corpora. Minimum Error Rate Training under BLEU is used for estimating approximately 20 feature function weights over the dev1 development set. Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N -best list"
D08-1065,P08-1023,0,0.0666309,"Missing"
D08-1065,J97-2003,0,0.0579808,"E) with a summation over the number of n-grams that occur in E, which is at worst polynomial in the number of edges in the lattice that defines E. We compute the posterior probability of each n-gram w as: X Z(Ew ) p(w|E) = P (E|F ) = , (7) Z(E) E∈Ew 0 where Z(E) = E 0 ∈E exp(αH(E , F )) (denominator in Equation P 3) and 0 Z(Ew ) = E 0 ∈Ew exp(αH(E , F )). Z(E) and 2 Z(Ew ) represent the sums of weights of all paths in the lattices Ew and E respectively. P 4 WFSA MBR Computations We now show how the Lattice MBR Decision Rule (Equation 6) can be implemented using Weighted Finite State Automata (Mohri, 1997). There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system. We will describe these in the log semiring, where log +(x, y) = log(ex + ey ) is the collect operator (Mohri, 2002) 2 steps in the setting where the evidence lattice Ee may be different from the hypothesis lattice Eh (Equation 2). 1. Extract the set of n-grams that occur in the evidence lattice Ee . For the usual BLEU score, n ranges from one to four. 2. Compute the posterior probability p(w|E) of each of these n-grams. 3. Intersect each n-gram w,"
D08-1065,J82-2005,0,0.806279,"Missing"
D08-1065,J03-1002,1,0.0103282,"tion sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5 http://www.nist.gov/speech/tests/mt/ 625 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We first perform sentence and sub-sentence chunk alignment on the parallel documents. We then train word alignment models (Och and Ney, 2003) using 6 Model-1 iterations and 6 HMM iterations. An additional 2 iterations of Model-4 are performed for zhen and enzh pairs. Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework (Matusov et al., 2004). An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments. Several feature functions are then computed over the phrasepairs. 5-gram word language models are trained on the allowed monolingual corpora. Minimum Error Rate Training under BLEU is used for"
D08-1065,J04-4002,1,0.705211,". In Section 5, we introduce the corpus BLEU approximation that makes it possible to perform efficient lattice MBR decoding. An example of lattice MBR with a toy lattice is presented in Section 6. We present lattice MBR experiments in Section 7. A final discussion is presented in Section 8. 2 Minimum Bayes Risk Decoding Minimum Bayes-Risk (MBR) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model (Bickel and Doksum, 1977). We begin with a review of MBR decoding for Statistical Machine Translation (SMT). Statistical MT (Brown et al., 1990; Och and Ney, 2004) can be described as a mapping of a word sequence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder δ(F ). If the reference translation E is known, the decoder performance can be 621 measured by the loss function L(E, δ(F )). Given such a loss function L(E, E 0 ) between an automatic translation E 0 and the reference E, and an underlying probability model P (E|F ), the MBR decoder has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): ˆ = argmin R(E 0 ) E E 0 ∈E X = argmin L(E, E 0 )P (E|F ), E 0 ∈E E∈E where R(E 0 )"
D08-1065,P03-1021,1,0.0552258,"1788 1664 663 919 919 1360 1357 1859 Table 1: Statistics over the development and test sets. gain. We start with a description of the data sets and the SMT system. 7.1 Development and Blind Test Sets We present our experiments on the constrained data track of the NIST 2008 Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh) machine translation tasks.5 In all language pairs, the parallel and monolingual data consists of all the allowed training sets in the constrained track. For each language pair, we use two development sets: one for Minimum Error Rate Training (Och, 2003; Macherey et al., 2008), and the other for tuning the scale factor for MBR decoding. Our development sets consists of the NIST 2004/2003 evaluation sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5 http://www.nist.gov/speech/tests/mt/ 625 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in th"
D08-1065,2001.mtsummit-papers.68,0,0.0918665,"Machine Translation Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2 1 2 Google Inc. 1600 Amphitheatre Pkwy. Mountain View, CA 94043, USA Department of Computer Science Johns Hopkins University Baltimore, MD 21218, USA royt@jhu.edu {shankarkumar,och,wmach}@google.com Abstract We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance. 1 Introduction Statistical language processing systems for speech recognition, machine translation or"
D08-1065,P06-2101,0,0.16738,"speech recognition and machine translation (Goel and Byrne, 2000; Ehling et al., 2007) which has shown the need for tuning this factor. Our MT system parameters are trained with Minimum Error Rate Training which assigns a very high posterior probability to the MAP translation. As a result, it is necessary to flatten the probability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis. Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with transla628 tion metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hyp"
D08-1065,D07-1014,0,0.00916492,"imum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU. A different MBR"
D08-1065,2006.amta-papers.25,0,0.0230128,"mentation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with transla628 tion metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures. We believe that our lattice MBR framework can be extended to such hypergraphs with loss functions that take into account both BLEU scores as well as parse tree structures. Lattice and Forest based search and training procedures are not yet common in statistical machine translation. However, they are promising because the search space of tra"
D08-1065,W06-1666,0,0.0155002,"e to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-leve"
D08-1065,P08-1025,0,0.0342462,"ance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N -best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is"
D08-1065,W06-3119,0,0.0347484,"e when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with transla628 tion metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures. We believe that our lattice MBR framework can be extended to such hypergraphs with loss functions that take into account both BLEU scores as well as parse tree structures. Lattice and Forest based search and training procedures are not yet common in statistical machine translation. However, they are promising because the search space of translations is much larger than the typical N -best list (Mi et al., 2008). We hope that our approach will provide some insight into the design"
D08-1065,D08-1022,0,\N,Missing
D08-1065,W02-1021,1,\N,Missing
D08-1065,P02-1040,0,\N,Missing
D08-1076,W08-0304,0,0.146313,"lgorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on 731 Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Machine Translation Evaluations. data set collection dev1 dev2 blind nist02 nist04 nist08 # of sentences aren zhen enzh 1043 878 – 1353 1788 – 1360 1357 1859 N -best re-ranking tasks. The incorporation of a large number of sparse fea"
D08-1076,P05-1033,0,0.544403,"Missing"
D08-1076,P08-2010,0,0.021018,"r objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on 731 Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Machine Translation Evaluations. data set collection dev1 dev2 blind nist02 nist04 nist08 # of sentences aren zhen enzh 1043 878 – 1353 1788 – 1360 1357 1859 N -best re-ranking tasks. The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007). The paper investigates a perceptron-like on"
D08-1076,N04-1022,0,0.475264,"Missing"
D08-1076,D07-1105,1,0.496791,"Missing"
D08-1076,P03-1021,1,0.694975,"eses represented in a phrase lattice. This upper bound is + # used to prove the space and runtime efficiency of S ¸  M M ˆ  arg min ˆpfs ; λ1 q λ (3) the suggested algorithm. Section 5 lists some best E rs , e 1 practices for MERT. Section 6 discusses related λM 1 s1 + work. Section 7 reports on experiments conducted # K S ¸ ¸  on the NIST 2008 translation tasks. The paper ˆ pf s ; λ M  arg min q , es,k E prs , es,k qδ e 1 M concludes with a summary in Section 8. λ1 s1 k 1 with ˆpfs ; λM e 1 q  arg max e # M ¸ m1 2 Minimum Error Rate Training on N -best Lists λm hm pe, fs q + (4) In (Och, 2003), it was shown that linear models can effectively be trained under the MERT criterion using a special line optimization algorithm. This line optimization determines for each feature function hm and sentence fs the exact error surface on a set of candidate translations Cs . The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum. Candidate translations in MERT are typically represented as N -best lists which contain the N most probable translati"
D08-1076,2001.mtsummit-papers.68,0,0.0779073,"Missing"
D08-1076,N07-1029,0,0.187922,"Missing"
D08-1076,P06-2101,0,0.337185,"ptimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509). In (Zens et al., 2007), the MERT criterion is optimized on N -best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on 731 Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST"
D08-1076,D08-1065,1,0.787928,"Missing"
D08-1076,W02-1021,1,0.408127,"hypotheses which are merged into the candidate repositories if they are ranked among the top N candidates. The relation K  N holds therefore only in the first iteration. From the second iteration on, K is usually larger than N . The sequence of line optimizations and decodings is repeated until (1) the candidate repositories remain unchanged and (2) γopt  0. 3 Minimum Error Rate Training on Lattices Algorithm 2 Lattice Envelope In this section, the algorithm for computing the upper envelope on N -best lists is extended to phrase lattices. For a description on how to generate lattices, see (Ueffing et al., 2002). Formally, a phrase lattice for a source sentence f is defined as a connected, directed acyclic graph Gf  pVf , Ef q with vertice set Vf , unique source and sink nodes s, t P Vf , and a set of arcs Ef  Vf  Vf . Each arc is labeled with a phrase ϕij  ei1 , ..., eij and the (local) feature function values hM 1 pϕij , f q. A path π  pv0 , ε0 , v1 , ε1 , ..., εn1 , vn q in Gf (with εi P Ef and vi , vi 1 P Vf as the tail and head of εi , 0 ¤ i n) defines a partial translation eπ of f which is the concatenation of all phrases along this path. The corresponding feature function values are obta"
D08-1076,D07-1080,0,0.344818,"nsights into the optimization process. In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on 731 Table 1: Corpus statistics for three text translation sets: Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh). Development and test data are compiled from evaluation data used in past NIST Machine Translation Evaluations. data set collection dev1 dev2 blind nist02 nist04 nist08 # of sentences aren zhen enzh 1043 878 – 1353 1788 – 1360 1357 1859 N -best re-ranking tasks. The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007). The paper investigates a perceptron-like online large-margin training for statistical machine translation. The described approach is reported to yield significant improvements on top of a baseline system which employs a small number of feature functions whose weights are optimized under the MERT criterion. A study which is complementary to the upper bound on the size of envelopes derived in Section 4 is provided in (Elizalde and Woods, 2006) which shows that the number of inference functions of any graphical model as, for instance, Bayesian networks and Markov random fields is polynomial in"
D08-1076,D07-1055,0,0.559134,"nctions. This problem can be alleviated by exploring a large number of random directions which update many feature weights simultaneously. The random directions are chosen as the lines which connect some randomly distributed points on the surface of an M -dimensional hypersphere with the hypersphere’s center. The center of the hypersphere is defined as the initial parameter set. 6 Related Work As suggested in (Och, 2003), an alternative method for the optimization of the unsmoothed error count is Powell’s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509). In (Zens et al., 2007), the MERT criterion is optimized on N -best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503). The optimization procedure allows for optimizing other objective function as, e.g., the expected BLEU score. A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions. A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface"
D08-1076,W06-3119,0,0.292499,"Missing"
D08-1076,P02-1040,0,\N,Missing
D10-1016,P85-1034,0,0.645528,"(assuming the weight for this feature function is set high). 5 General poetic form feature function In this section we discuss a framework for tracking any poetic genre, specified as a genre description object (Section 3.3 above). As in the case of the stress pattern function, we use a statistical MT system, which is now required to be phrase-based only. We also use a pronunciation dictionary, but in addition to tracking the number and stress of syllables, we must now be able to provide a function that classifies a pair of words as rhyming or nonrhyming. This is in itself a non-trivial task (Byrd and Chodorow, 1985), due to lack of a clear definition of what constitutes a rhyme. In fact rhyming is a continuum, from very strong rhymes to weak ones. We use a very weak definition which is limited to a single syllable: if the final syllables of both words have the same nucleus and coda1 , we say that the words rhyme. We accept this weak definition because we prefer to err on the side of over-generation and accept even really bad poetry. 5.1 Tracking the target length The hardest constraint to track efficiently is the range of lengths of the resulting sentence. Phrasebased decoders use a limited-width beam as"
D10-1016,P07-2045,0,0.00419022,"aints hold. This requires us to include in a state, for any outstanding rhyme letter, the word that occurred at the end of that line. It is not sufficient to include just the syllable that must rhyme, because we wish to avoid self-rhymes (word rhyming with an identical word). 4 Stress pattern feature function We will first discuss an easier special case, namely a feature function for blank verse, which we will refer to as stress pattern feature function. This feature function can be used for both phrase-based and hierarchical systems. In addition to a statistical MT system (Och and Ney, 2004; Koehn et al., 2007), it is necessary to have the means to count the syllables in a word and to find out which ones are stressed. This can be done with a pronunciation module of a text-to-speech system, or a freely available pronunciation dictionary, such as CMUDict (Rudnicky, 2010). Out-of160 vocabulary words can be treated as always imposing a high cost. 4.1 Stress pattern for a phrase-based system In a phrase based system, the feature function state consists of the current hypothesis length modulo foot length. For a 2-syllable foot, it is either 0 or 1, for a 3-syllable foot, 0, 1, or 2. The proposed target ph"
D10-1016,P02-1038,1,0.327149,"ons. This approach would not succeed very often, since the haikus that may be among the possible translations are a very small fraction of all translations, and the MT decoder is not actively looking for them, since it is not part of the cost it attempts to minimize. Instead, we would want to recast “Haikuness” as a feature function, such that a real haiku has 0 cost, and those outputs that are not, have large cost. This feature function must be local, rather than global, so as to guide the decoder search. The concept of feature functions as used in statistical MT is described by Och and Ney (Och and Ney, 2002). For a phrase based system, a feature function is a function whose inputs are a partial hypothesis state sin , and a phrase pair p, and whose outputs are the hypothesis state after p is appended to the hypothesis: sout , and the cost incurred, c. For hierarchical, tree-to-string and some other types of MT systems which combine two partial hypotheses and are not generating translations left-to-right, one instead has two partial hypotheses states slef t and sright as inputs, and the outputs are the same. Our first goal is to describe how these functions can be efficiently implemented. The featu"
D10-1016,J04-4002,1,0.584151,"that rhyming constraints hold. This requires us to include in a state, for any outstanding rhyme letter, the word that occurred at the end of that line. It is not sufficient to include just the syllable that must rhyme, because we wish to avoid self-rhymes (word rhyming with an identical word). 4 Stress pattern feature function We will first discuss an easier special case, namely a feature function for blank verse, which we will refer to as stress pattern feature function. This feature function can be used for both phrase-based and hierarchical systems. In addition to a statistical MT system (Och and Ney, 2004; Koehn et al., 2007), it is necessary to have the means to count the syllables in a word and to find out which ones are stressed. This can be done with a pronunciation module of a text-to-speech system, or a freely available pronunciation dictionary, such as CMUDict (Rudnicky, 2010). Out-of160 vocabulary words can be treated as always imposing a high cost. 4.1 Stress pattern for a phrase-based system In a phrase based system, the feature function state consists of the current hypothesis length modulo foot length. For a 2-syllable foot, it is either 0 or 1, for a 3-syllable foot, 0, 1, or 2. T"
D10-1016,P02-1040,0,0.0965651,"Missing"
D10-1016,W09-2006,0,0.140762,"Missing"
D10-1016,C08-1048,0,\N,Missing
D10-1016,W09-0401,0,\N,Missing
D11-1017,P07-1036,0,0.00547072,"to additional features. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build be"
D11-1017,A00-2018,0,0.0628137,"ween those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 20"
D11-1017,W10-2903,0,0.016302,"aining is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build better structured predictors. In our case t"
D11-1017,P05-1066,0,0.596748,"Missing"
D11-1017,P97-1003,0,0.0555975,"correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petr"
D11-1017,de-marneffe-etal-2006-generating,0,0.0173365,"Missing"
D11-1017,N10-1060,0,0.0202747,"ng has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (Section 2). Similar to self-training,"
D11-1017,N04-1035,0,0.046446,"ence reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reord"
D11-1017,C10-1043,0,0.387001,"ne way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans"
D11-1017,W01-0521,0,0.021726,"regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (S"
D11-1017,2007.mtsummit-papers.29,0,0.312625,"le component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of co"
D11-1017,D11-1138,1,0.521563,"e parse ranked most highly in the n-best list. The motivation of this selection step is that good performance on the downstream external task, measured by the extrinsic metric, should be predictive of an intrinsically good parse. At the very least, even if the selected parse is not syntactically correct, or even if it goes against the original treebanking guidelines, it results in a higher extrinsic score and should therefore be preferred. One could imagine extending this framework by repeatedly running self-training on successively improving parsers in an EM-style algorithm. A recent work by Hall et al. (2011) on training a parser with multiple objective functions investigates a similar idea in the context of online learning. In this paper we focus our attention on machine translation as the final application, but one could envision applying our techniques to other applications such as information extraction or question answering. In particular, we explore one application of targeted self-training, where computing the extrinsic metric involves plugging the parse into an MT system’s reordering component and computing the accuracy of the reordering compared to a reference word order. We now direct ou"
D11-1017,D10-1092,0,0.0712737,"omputing the accuracy of the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transforma"
D11-1017,W10-1736,0,0.160736,"Missing"
D11-1017,P06-1063,0,0.0599599,"ysis and is extremely sensitive to parser errors. 4 4.1 Experimental Setup Good parse Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In particular, we split tokens containing hyphens into multiple tokens and, somewhat simplistically, gave the original token’s part-of-speech tag to all newly created tokens. In Section 6 we make also use of the Question Treebank (QTB) (Judge et al., 2006), as a source of syntactically annotated out-of-domain data. Though we experiment with both dependency parsers and phrase structure parsers, our MT system assumes dependency parses as input. We use the Stanford converter (de Marneffe et al., 2006) to convert phrase structure parse trees to dependency parse trees (for both treebank trees and predicted trees). Reordered: 15 or greater of an SPF has that sunscreen Wear Reordering score: 1.0 (matches reference) Bad parse Reordered: 15 or greater of an SPF has that Wear sunscreen Reordering score: 0.78 (“Wear” is out of place) Figure 1: Examples of"
D11-1017,P06-1096,0,0.0282082,"Missing"
D11-1017,J93-2004,0,0.0436852,"hile there is a good correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to t"
D11-1017,N06-1020,0,0.0229852,"Missing"
D11-1017,P05-1012,1,0.122844,"nsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsi"
D11-1017,P08-1006,0,0.0120471,"ems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training 10x shift-reduce parser. ceptron model for an end-to-end MT system where the alignment parameters are updated based on selecting an alignment from a n-best list that leads to highest BLEU score. As mentioned earlier, this also makes our work similar to Hall et al. (2011) who train a perceptron algorithm on multiple objective functions with the goal of producing parsers that are optimized for extrinsic metrics. It has previously been observed that parsers often perform differently for downstream applications. Miyao et al. (2008) compared parser quality in the biomedical domain using a protein-protein interaction (PPI) identification accuracy metric. This allowed them to compare the utility of extant dependency parsers, phrase structure parsers, and deep structure parsers for the PPI identification task. One could apply the targeted self-training technique we describe to optimize any of these parsers for the PPI task, similar to how we have optimized our parser for the MT reordering task. 8 Conclusion reordering component of a machine translation system. This significantly improves the subjective quality of the system"
D11-1017,J08-4003,0,0.104441,"the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese,"
D11-1017,P03-1021,1,0.0248517,"mework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given lan187 guage pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of parallel documents. They come from various sources with a substantial portion coming from the web after using simple heuristics to identi"
D11-1017,P02-1040,0,0.108652,"th metrics.4 One explanation for the drops in LAS is that some parts of the parse tree are important for downstream reordering quality while others are not (or only to a lesser extent). Some distinctions between labels become less important; for example, arcs labeled “amod” and “advmod” are transformed identically by the reordering rules. Some semantic distinctions also become less important; for example, any sane interpretation of “red hot car” would be reordered the same, that is, not at all. 5.2 Translation quality improvement To put the improvement of the MT system in terms of BLEU score (Papineni et al., 2002), a widely used metric for automatic MT evaluation, we took 5000 sentences from Web-Test and had humans generate reference translations into Japanese, Korean, and 4 We did not attempt this experiment for the BerkeleyParser since training was too slow. 188 Turkish. We then trained MT systems varying only the parser used for reordering in training and decoding. Table 2 shows that targeted self-training data increases BLEU score for translation into all three languages. In addition to BLEU increase, a side-by-side human evaluation on 500 sentences (sampled from the 5000 used to compute BLEU score"
D11-1017,N07-1051,1,0.0360042,"1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (Section 2). Similar to self-training, a baseline model is used to produce predictions on an unlabeled data set. However, rather than directly training on the output of the b"
D11-1017,P06-1055,1,0.0472315,"uality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment"
D11-1017,D10-1069,1,0.571776,"to produce predictions on an unlabeled data set. However, rather than directly training on the output of the baseline model, we generate a list of hypotheses and use an external signal to select the best candidate. The selected parse trees are added to the training data and the model is then retrained. The experiments in Section 5 show that this simple procedure noticeably improves our parsers for the task at hand, resulting in significant improvements in downstream translation quality, as measured in a human evaluation on web text. This idea is similar in vein to McClosky. et al. (2006) and Petrov et al. (2010), except that we use an extrinsic quality metric instead of a second parsing model for making the selection. It is also similar to Burkett and Klein (2008) and Burkett et al. (2010), but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection. It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings. Details of the reordering metric and the annotated data we used are given in Sections 3 and 4. While this annotation requires some effort, such annotations are much"
D11-1017,W11-2102,1,0.704327,"s into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans whose content appears contiguously in the same order in the reference. The reordering score is then computed as ρ(esys , eref ) = 1 − |C |− 1 . |e |− 1 This metric assigns a score between 0"
D11-1017,D07-1077,0,0.168629,"mportance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the"
D11-1017,C04-1073,0,0.448999,"s a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into"
D11-1017,N09-1028,1,0.710453,"uide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reordering accuracy, BL"
D11-1017,P01-1067,0,0.0915201,"s of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics,"
D11-1017,W06-3108,0,0.0368459,"icable to any language pair. We chose to use manually written rules to eliminate the variance induced by the automatic reordering-rule learning framework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given lan187 guage pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of"
D11-1017,D08-1059,0,0.0478733,"derings is straightforward because annotators need little special background or training, as long as they can speak both the source and target languages. We chose Japanese as the target language through which to create the English reference reorderings because we had access to bilingual annotators fluent in English and Japanese. 186 Parsers The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). The parser uses the following features: word identity of the first two words on the buffer, the top word on the stack and the head of the top word on the stack (if available); part-ofspeech identities of the first four words on the buffer and top two words on the stack; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the leftmost modifier of the first word in the buffer. We also include conjunctions over all nonlexical features. We also give results for the latent variable parser (a.k.a. BerkeleyParser) of Petro"
D11-1017,W10-2906,1,\N,Missing
D11-1017,A00-1031,0,\N,Missing
D11-1017,W10-1749,0,\N,Missing
D11-1017,D08-1092,0,\N,Missing
D11-1126,J93-2003,0,0.0244318,"Missing"
D11-1126,N09-1025,0,0.0216892,"alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available in Dk (q). The parameter k determines the extent to which lesser quality alternatives can be chosen. If all the alternatives in each Dk (q) are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such 1 In the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. 1367 • cost(r): a weighted sum of features (not normalized over the search space) in a loglinear model such as those mentioned in (Och, 2003). • r1 : the highest ranked alternative in Dk (q). lossrank provides a generally applicable criteria to select alternatives, penalizing selection from deep within Dk (q). This estimate of the quality degradation does not reflect the generatin"
D11-1126,P06-1096,0,0.0255461,"Equation 3 chooses alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available in Dk (q). The parameter k determines the extent to which lesser quality alternatives can be chosen. If all the alternatives in each Dk (q) are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such 1 In the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. 1367 • cost(r): a weighted sum of features (not normalized over the search space) in a loglinear model such as those mentioned in (Och, 2003). • r1 : the highest ranked alternative in Dk (q). lossrank provides a generally applicable criteria to select alternatives, penalizing selection from deep within Dk (q). This estimate of the quality degradation does not"
D11-1126,J05-4003,0,0.114276,"Missing"
D11-1126,J04-4002,1,0.713364,"Missing"
D11-1126,P03-1021,1,0.0210276,"escribed in Equation 3 chooses alternative results on a per result basis, with the goal of influencing collection level bit sequences. The selection criteria as described will choose the most biased candidates available in Dk (q). The parameter k determines the extent to which lesser quality alternatives can be chosen. If all the alternatives in each Dk (q) are of relatively similar quality, we expect minimal degradation due to watermarking. Specific tasks however can be particularly sensitive to choosing alternative results. Discriminative approaches that optimize for arg max selection like (Och, 2003; Liang et al., 2006; Chiang et al., 2009) train model parameters such 1 In the final version of this paper we will perform sampling to create a more reliable estimate of the false positive rate that is not overly influenced by document length distributions. 1367 • cost(r): a weighted sum of features (not normalized over the search space) in a loglinear model such as those mentioned in (Och, 2003). • r1 : the highest ranked alternative in Dk (q). lossrank provides a generally applicable criteria to select alternatives, penalizing selection from deep within Dk (q). This estimate of the quality"
D11-1126,1983.tc-1.13,0,0.282587,"Missing"
D11-1126,C10-1124,1,0.937803,"ices that output structured results are made available to the public and the results disseminated on the web, we face a daunting new challenge: Machine generated structured results contaminate the pool of naturally generated human data. For example, machine translated output 1363 2 Franz J. Och1 Juri Ganitkevitch2 Center for Language and Speech Processing Johns Hopkins University Baltimore, MD 21218, USA juri@cs.jhu.edu and human generated translations are currently both found extensively on the web, with no automatic way of distinguishing between them. Algorithms that mine data from the web (Uszkoreit et al., 2010), with the goal of learning to simulate human behavior, will now learn models from this contaminated and potentially selfgenerated data, reinforcing the errors committed by earlier versions of the algorithm. It is beneficial to be able to identify a set of encountered structured results as having been generated by one’s own algorithm, with the purpose of filtering such results when building new models. Problem Statement: We define a structured result of a query q as r = {z1 · · · zL } where the order and identity of elements zi are important to the quality of the result r. The structural aspec"
D11-1126,J03-3002,0,\N,Missing
E03-1032,J90-2002,0,0.139099,"will present some results. Finally, we will describe the implemented prototype system. 2 Statistical Machine Translation We are given a source language ('French') sentence = f3 . . . ff, which is to be translated into a target language ( 'English') sentence ef = e l ... 6, ... el-. Among all possible target language sentences, we will choose the sentence of unknown length / with the highest probability: = argmax {Pr (ei f )} (1) argmax {Pr (e)) • Pr(fil lef)} (2) The decomposition into two knowledge sources in Eq. 2 is the so-called source-channel approach to statistical machine translation (Brown et al., 1990). It allows an independent modeling of target language model Pr (ef ) and translation model Pr(filef)- The target language model describes the well-formedness of the target language sentence. The translation model links the source language sentence to the target language sentence. The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Here, we maximize over all possible target language sentences. 3 Interactive Machine Translation In a statistical approach, the problem of finding an extension ef+1 of a given prefix 61 can be described"
E03-1032,C96-1067,0,0.600538,"n such an environment, the main goal of the machine translation system is not to produce translations that are understandable for an inexperienced recipient but to support a professional human posteditor. Typically, a better quality of the produced machine translation text yields a reduced post-editing effort. From an application point of view, many additional aspects have to be considered: the user interface, the used formats and the additional support tools such as lexicons, terminological databases or translation memories. The concept of interactive machine translation, first suggested by (Foster et al., 1996), finds a very natural implementation in the framework of statistical machine translation. In interactive machine translation, the basic idea is to provide an environment to a human translator that interactively reacts upon the input as the user writes or corrects the translation. In such an approach, the system suggests an extension of a sentence that the human user either accepts or ignores. An implementation of such a tool was performed in the TransType project (Foster et al., 1996; Foster et al., 1997; Langlais et al., 2000). The user interface of the TransType system combines a machine tr"
E03-1032,1997.mtsummit-papers.1,0,0.757582,"ranslation memories. The concept of interactive machine translation, first suggested by (Foster et al., 1996), finds a very natural implementation in the framework of statistical machine translation. In interactive machine translation, the basic idea is to provide an environment to a human translator that interactively reacts upon the input as the user writes or corrects the translation. In such an approach, the system suggests an extension of a sentence that the human user either accepts or ignores. An implementation of such a tool was performed in the TransType project (Foster et al., 1996; Foster et al., 1997; Langlais et al., 2000). The user interface of the TransType system combines a machine translation system and a text editor into a single application. The human translator types the translation of a given source text. For each prefix of a word, the machine translation system computes the most probable extension of this word and presents this to the user. The human translator either accepts this translation by press387 ing a certain key or ignores the suggestion and continues typing. Rather than single-word predictions, as in the TransType approach, it is preferable that the suggested extensio"
E03-1032,W02-1020,0,0.173018,"n. The server performs the actual translations as well as all time-consuming operations such as computing the extensions. The client includes only the user interface and can therefore run on a small computer. Client and server are connected via Internet or Intranet. There is ongoing research to experimentally study the productivity gain of such a system for professional human translators. 9 Related Work As already mentioned, previous work towards interactive machine translation has been carried out in the TransType project (Foster et al., 1996; Foster et al., 1997; Langlais et al., 2000). In (Foster et al., 2002) a so-called ""user model"" has been introduced to maximize the expected benefit of the human translator. This user model consists of two components. The first component models the benefit of a certain extension. The second component models the acceptance probability of this extension. The user model is used to determine the length of the proposed extension measured in characters. The resulting decision rule is more centered on the human user than the one in Eq. 3. It takes into account, e.g., the time the user needs to read the extension (at least approximatively). In principle, the decision ru"
E03-1032,W00-0507,0,0.482506,"The concept of interactive machine translation, first suggested by (Foster et al., 1996), finds a very natural implementation in the framework of statistical machine translation. In interactive machine translation, the basic idea is to provide an environment to a human translator that interactively reacts upon the input as the user writes or corrects the translation. In such an approach, the system suggests an extension of a sentence that the human user either accepts or ignores. An implementation of such a tool was performed in the TransType project (Foster et al., 1996; Foster et al., 1997; Langlais et al., 2000). The user interface of the TransType system combines a machine translation system and a text editor into a single application. The human translator types the translation of a given source text. For each prefix of a word, the machine translation system computes the most probable extension of this word and presents this to the user. The human translator either accepts this translation by press387 ing a certain key or ignores the suggestion and continues typing. Rather than single-word predictions, as in the TransType approach, it is preferable that the suggested extension consists of multiple w"
E03-1032,W99-0604,1,0.231989,"ciency without an unacceptable amount of search errors. The one we will use usually takes a few seconds per sentence. Hence, we have to perform certain simplifications making the search problem feasible. Our solution is to precompute a subset of possible word sequences. The search in Eq. 3 is then constrained to this set of hypotheses. As data structure for efficiently representing the set of possible word sequences, we use word hypotheses graphs (Ney and Aubert, 1994; Ueffing et al., 2002) . 4 Alignment Templates As specific machine translation method, we use the alignment template approach (Och et al., 1999). The key elements of this approach are the alignment templates, which are pairs of source and target language phrases together with an alignment between the words within the phrases. The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered. The alignment template model refines the translation probability Pr Cf by introducing two hidden variables z and a fc for the K alignment templates and the alignment of the alignment temnode S (n): plates: Pr(fiilef) = E Pr(af"
E03-1032,W02-1021,1,0.618849,"larger than a fraction of a second is not acceptable. The search algorithms developed so far are not able to achieve this efficiency without an unacceptable amount of search errors. The one we will use usually takes a few seconds per sentence. Hence, we have to perform certain simplifications making the search problem feasible. Our solution is to precompute a subset of possible word sequences. The search in Eq. 3 is then constrained to this set of hypotheses. As data structure for efficiently representing the set of possible word sequences, we use word hypotheses graphs (Ney and Aubert, 1994; Ueffing et al., 2002) . 4 Alignment Templates As specific machine translation method, we use the alignment template approach (Och et al., 1999). The key elements of this approach are the alignment templates, which are pairs of source and target language phrases together with an alignment between the words within the phrases. The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered. The alignment template model refines the translation probability Pr Cf by introducing two hidden variabl"
E03-1055,J96-1002,0,0.0308739,"ation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apich warde gerne von KOln nach MUnchen fahren @want question V V @origin @destination @going Figure 1: Example of a word! concept mapping. proach uses the maximum entropy (ME) framework (Berger et al., 1996). For both frameworks, the objective can be described as follows. Given a natural source sentence fiJ = fj...f./ we choose the formal target language sentence ef = el •••e,...ei with the highest probability among all possible target sentences: argmax { Pr (ej ei fi&apos; ) (1) argmax r(ge,)•pr(ef) Pr (f&apos;) el argmax { Pr(fief) • Pr(ef) • (2) e Using Bayes&apos; theorem, Eq. 1 can be rewritten to Eq. 2, where the denominator can be neglected. The argmax operation denotes the search problem, i.e. the generation of the sequence of formal semantic concepts in the target language. An example is depicted in Fi"
E03-1055,M98-1018,0,0.0382589,"hold K, we only include those features that E p4i(en fTh)} n=1 where u p(Ai )=H 2, , a2 . 7ra exp [ 2A7 m 15 4.3 Search In the test phase, the search is performed using the so called maximum approximation, i.e. the most likely sequence of concepts ef is chosen among all possible sequences ef : Table 1: Training and testing conditions for the TABA corpus. Natural Language Train argmax {Pr(ei fij )} C&apos; argma,x EA rrt h,„,(e -1, fiJ )} . Test rrt=1 Therefore, the time-consuming renormalization in Eq. 5 is not needed during search. We run a Viterbi search to find the highest probability sequence (Borthwick et al., 1998). 5 Results Experiments were performed on the German inhouse Philips TABA corpus l and the German inhouse TELDIR corpus 2 . The TABA corpus is a text corpus in the domain of a train timetable information system (Aust et al., 1995). The TELDIR corpus is derived from the domain of a telephone directory assistance. Along with the bilingual annotation consisting of the source and target sentences, the corpora also provide the affiliated alignments between source words and concepts. The corpora allocations are summarized in table 1 and table 2. For the TABA corpus, the target language consists of 2"
E03-1055,J93-2003,0,0.00932534,"uction The objective of natural language understanding (NLU) is to extract all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-to-speech translation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apich warde gerne von KOln nach MUnchen fahren @want question V V @origin @destination @going Figure 1: Example of a word! concept mapping. proach uses the maximum entropy (ME) framework (Berger et al., 1996). For both frameworks, the objective can be described as follows. Given a natural source sentence fiJ = fj...f./ we choose the formal target language sentence ef = el •••e,...ei with the highest probability among all possible target sentences: argmax { Pr (ej ei fi&apos; ) (1) argmax r(ge,)•pr(ef) Pr (f&apos;) e"
E03-1055,P94-1004,0,0.0151305,"ts results for both the alignment templates approach and the ME framework. For both approaches, experiments were carried out on two different German NLU tasks. 2 Concept based semantic representation - A crucial decision, when designing an NLU system, is the choice of a suitable semantic representation, since interpreting a user&apos;s request requires an appropriate formalism to represent the meaning of an utterance. Different semantic representations have been proposed. Among them, case frames (Issar and Ward, 1993), semantic frames (Bennacef et al., 1994), and variants of hierarchical concepts (Miller et al., 1994) as well as flat concepts (Levin and Pieraccini, 1995) are the most prominent. Since we regard NLU as a special case of a translation problem, we have chosen a flat 12 concept-based target language as meaning representation. A semantic concept (in the following briefly termed as concept) is defined as the smallest unit of meaning that is relevant to a specific task (Levin and Pieraccini, 1995). Figure 1 depicts an example of a concept-based meaning representation for the input utterance &apos;I would like to go from Munich to Cologne&apos; from the domain of a German traintimetable information system. T"
E03-1055,W99-0604,1,0.919994,"t all the information from a natural language based input which are relevant for a specific task. Typical applications using NLU components are spoken dialogue systems (Levin and Pieraccini, 1995) or speech-to-speech translation systems (Zhou et al., 2002). In this paper we present two approaches for analyzing the semantics of natural language inputs and discuss their advantages and drawbacks. The first approach is derived from the field of statistical machine translation (MT) and is based on the source-channel paradigm (Brown et al., 1993). Here, we apply a method called alignment templates (Och et al., 1999). The alternative apich warde gerne von KOln nach MUnchen fahren @want question V V @origin @destination @going Figure 1: Example of a word! concept mapping. proach uses the maximum entropy (ME) framework (Berger et al., 1996). For both frameworks, the objective can be described as follows. Given a natural source sentence fiJ = fj...f./ we choose the formal target language sentence ef = el •••e,...ei with the highest probability among all possible target sentences: argmax { Pr (ej ei fi&apos; ) (1) argmax r(ge,)•pr(ef) Pr (f&apos;) el argmax { Pr(fief) • Pr(ef) • (2) e Using Bayes&apos; theorem, Eq. 1 can be"
E03-1055,C96-2141,1,0.64461,"ranslation approach decomposes Pr(eflg) into two probability distributions, the language model probability and the translation probability. The architecture of this method is depicted in figure 2. For the translation approach, we use the same training procedure as for the automatic translation of natural languages. When rewriting the translation probability Pr(fiJ 4) by introducing a &apos;hidden&apos; alignment al = with a j C {1,...,1}, we obtain: 611) = E Pr(fi&apos;,aPef) (3) a = Efl a = Pr(fi,aj 1.3-1 , a13 -1 • ei) • The IBM models as proposed by (Brown et al., 1993) and the HMM model as suggested by (Vogel et al., 1996) result from different decompositions of Pr(fif ,a .i! 4). For training the alignment model, we train a sequence of models of increasing complexity. Starting from the first model IBM1, we proceed over the HMM model, IBM3 up to IBMS. Using the model IBMS as a result of the last training step, we use the alignment template approach to model whole word groups. Source Language Text ( Preprocessing ) Pr(fij leD Global Search H H Lexicon Model Alignment Model @destination @origin @train determination @want_guestion @hello @yes U. 0 = argmax {Pr(eI) • Pr(fiT Pr(e) Language Model 0 0 Target Language T"
E99-1010,P98-2162,1,0.671528,"tro nuestros s&apos;ub~nme $3: hoy m a n a n a mismo $4: hacerme ll&apos;ameme ll&apos;amenos llama llamar llamarme llamarnos llame p&apos;idame p&apos;idanos pedir pedirme pedirnos - Table 2: The EUTRANS-II corpus. Train: Test: Sentences Words Vocabulary Size Sentences Words Bigr. Perplexity how it pardon what when where which. who why German English 16 226 266080 299945 39511 25751 187 2 556 2 853 157 pida pide - cambiarme cambiarnos despertarme despertarnos llevar llevarme llevarnos subirme subirnos usted ustedes $6: completa cuarto media menos $5: 5 Results The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes. The key element of this approach are the alignment templates (originally referred to as translation rules)which are pairs of phrases together with an alignment between the words of the phrases. Examples of alignment templates are shown in Figure 2. The advantage of the alignment template approach against word-based statistical translation models is that word context and local re-orderings are explicitly taken into account. The alignment templates are automatically trained using a parallel training corpus. The translation of a sentence is done by a search p"
E99-1010,J92-4003,0,0.132381,"em is to group words into equivalence classes which is a standard method in statistical language modeling. In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation. We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm. We will show that the usage of the bilingual word classes we get can improve statistical machine translation. 1 2 Introduction Word classes are often used in language modelling to solve the problem of sparse data. Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms. In the field of statistical machine translation we also face the problem of sparse data. Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models. A naive approach for doing this would be the use of mono-lingually optimized word classes in source and target language. Unfortunately we can not expect these independently optimized classes to be correspondent. Therefore"
E99-1010,J93-2003,0,0.00870649,"perform the maximization of Eq. (6) we have to model the monolingual a priori probability p(e I IE) and the translation probability p(fJte~; E, .T). For the first we use the class-based bigram probability from Eq. (1). To model p(fJle~;8,.T) we assume the existence of an alignment a J. We assume that every word fj is produced by the word e~j at position aj in the training corpus with the probability P(f~le,~i): E,F +2Eh(n(E)) E +Z F h(~-~&apos; nt(FIE))+ E = - ~ 72 h(ng(ZlX&apos;)) + X,X&apos; ~h(ng,l(X)) + The word alignment a J is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996). The idea is to introduce the unknown alignment alJ as hidden variable into a statistical model of the translation probability p(fJle~). By applying the EMalgorithm we obtain the model parameters. The nt(FIE)) F The two count functions n(EIE&apos; ) and nt(FIE ) can be combined into one count function ng(X[Y ) := n(XIY)+nt(X[Y ) as for all words f and all words e and e&apos; holds n(fle ) = 0 and nt(ele&apos; ) = O. Using the function ng we arrive at the following optimization criterion: (7) j=l ~--~h ( E E LP2((C,~&apos;),ng) J p(f lc &apos;) = 1] p(L Icon) h(nt(FIE)) - ~ E,E&apos; X Eh(ng,2(X)) (9)"
E99-1010,1995.tmi-1.18,0,0.01433,"Missing"
E99-1010,C96-2141,0,0.185736,"maximumlikelihood approach: C = argmpx p(w lc) (2) We estimate the probabilities of Eq. (1) by relative frequencies: p(CIC&apos; ) := n(CIC&apos;)/n(C&apos;), p(wlC ) = n ( w ) / n ( C ) . The function n(-) provides the frequency of a uni- or bigram in the training corpus. If we insert this into Eq. (2) and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL &apos;99 criterion LP1 (Kneser and Ney, 1991): LPx(C,n) = h(n(C]C&apos;)) - ~ C,C&apos; +2 Z h ( n ( C ) ) (3) alignment a J that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996). By rewriting the translation probability using word classes, we obtain (corresponding to Eq. (1)): C = argm~n LPI(C,n). J (4) p(f le &apos;; E, = 1] j=l The function h(n) is a shortcut for n . log(n). It is necessary to fix the number of classes in C in advance as the optimum is reached if every word is a class of its own. Because of this it is necessary to perform an additional optimization process which determines the number of classes. The use of leaving-one-out in a modified optimization criterion as in (Kneser and Ney, 1993) could in principle solve this problem. An efficient optimization al"
E99-1010,C98-2157,1,\N,Missing
garcia-varea-etal-2002-efficient,C00-2163,1,\N,Missing
garcia-varea-etal-2002-efficient,J93-2003,0,\N,Missing
garcia-varea-etal-2002-efficient,E99-1010,1,\N,Missing
garcia-varea-etal-2002-efficient,J96-1002,0,\N,Missing
garcia-varea-etal-2002-efficient,P01-1027,1,\N,Missing
garcia-varea-etal-2002-efficient,W00-0707,0,\N,Missing
J03-1002,P98-1006,0,0.168636,"Missing"
J03-1002,H94-1028,0,0.147099,"nd every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disam"
J03-1002,H93-1039,0,0.176439,"Missing"
J03-1002,J93-2003,0,0.199711,"Missing"
J03-1002,1997.tmi-1.13,0,0.159281,"gnment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to languages for which such resources are scarce. 1.3 Overview In Section 2, we revi"
J03-1002,W93-0301,0,0.165602,"Missing"
J03-1002,W00-0801,0,0.146757,"Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to languages for which such resources are scarce. 1.3 Overview In Section 2, we review various statistical alignment models and heuristic models. We present a new statistical alignment model, a log-linear combination of the best models of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, and Mercer (1993). In Section 3, we describe the training of the"
J03-1002,P98-2158,0,0.160398,"Missing"
J03-1002,P01-1030,0,0.147953,"ents in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic an"
J03-1002,P00-1050,0,0.170082,"Missing"
J03-1002,J97-2004,0,0.146953,"chnique do indeed obtain a good alignment quality. In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6. All these models use a different decomposition of the probability Pr (f1J , aJ1 |eI1 ). 2.1.2 Heuristic Models. Considerably simpler methods for obtaining word alignments use a function of the similarity between the types of the two languages (Smadja, McKeown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000). Frequently, variations of the Dice coefficient (Dice 1945) are used as this similarity function. For each sentence pair, a matrix including the association scores between every word at every position is then obtained: dice(i, j) = 2 · C(ei , fj ) C(ei ) · C(fj ) (6) 23 Computational Linguistics Volume 29, Number 1 C(e, f ) denotes the co-occurrence count of e and f in the parallel training corpus. C(e) and C(f ) denote the count of e in the target sentences and the count of f in the source sentences, respectively. From this association score matrix, the word alignment is then"
J03-1002,J99-4005,0,0.185271,"2, we obtain Pr (f1J , aJ1 |eI1 ) = p(J |I) · J  [p(aj |j, I, J) · p(fj |eaj )] (18) j=1 1 δ(i, i ) is the Kronecker function, which is one if i = i and zero otherwise. 25 Computational Linguistics Volume 29, Number 1 To reduce the number of alignment parameters, we ignore the dependence on J in the alignment model and use a distribution p(aj |j, I) instead of p(aj |j, I, J). 2.3 Fertility-Based Alignment Models In the following, we give a short description of the fertility-based alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993). A gentle introduction can be found in Knight (1999b). The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra, Della Pietra, and Mercer 1993) have a significantly more complicated structure than the simple Models 1 and 2. The fertility φi of a word ei in position i is defined as the number of aligned source words: φi =  δ(aj , i) (19) j The fertility-based alignment models contain a probability p(φ |e) that the target word e is aligned to φ words. By including this probability, it is possible to explicitly describe the fact that for instance the German word ¨ ubermorgen produces four English words (the day after tomorr"
J03-1002,J00-2004,0,0.18143,"s with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney"
J03-1002,C00-2163,1,0.155591,"mented into aligned sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998"
J03-1002,W99-0604,1,0.155797,"Missing"
J03-1002,W01-1408,1,0.150274,"Missing"
J03-1002,P98-2162,1,0.152837,"(Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation (Diab 2000). In Yarowsky, Ngai, and Wicentowski (2001), word alignment is used to transfer text analysis tools such as morphologic analyzers or part-of-speech taggers from a language, such as English, for which many tools already exist to l"
J03-1002,1992.tmi-1.7,0,0.173068,"Missing"
J03-1002,J96-1001,0,0.148919,"Missing"
J03-1002,P96-1021,0,0.174327,"thod suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997). In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000). 21 Computational Linguistics Volume 29, Number 1 Another application of word alignments is in the field of word sense disambiguation"
J03-1002,P01-1067,0,0.187275,"nt models dependent on word classes (as in Models 4 and 5). increasing the number of alignments used in the approximation of the EM algorithm for the fertility-based alignment models. Further improvements in alignments are expected to be produced through the adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment 45 Computational Linguistics Volume 29, Number 1 models based on word groups rather than single words (Och, Tillmann, and Ney 1999). The use of models that explicitly deal with the hierarchical structures of natural language is very promising (Wu 1996; Yamada and Knight 2001). We plan to develop structured models for the lexicon, alignment, and fertility probabilities using maximum-entropy models. This is expected to allow an easy integration of more dependencies, such as in a second-order alignment model, without running into the problem of the number of alignment parameters getting unmanageably large. Furthermore, it will be important to verify the applicability of the statistical alignment models examined in this article to less similar language pairs such as ChineseEnglish and Japanese-English. Appendix: Efficient Training of Fertility-Based Alignment Models I"
J03-1002,H01-1035,0,0.216375,"Missing"
J03-1002,P00-1027,0,0.152889,"d sentences and that the sentences are segmented into words. Obviously, there are additional implicit assumptions in the models that are needed to obtain a good alignment quality. For example, in languages with a very rich morphology, such as Finnish, a trivial segmentation produces a high number of words that occur only once, and every learning method suffers from a significant data sparseness problem. 1.2 Applications There are numerous applications for word alignments in natural language processing. These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000). An obvious application for word alignment methods is the automatic extraction of bilingual lexica and terminology from corpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000). Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al. 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001). In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) o"
J03-1002,C96-2141,1,\N,Missing
J03-1002,C98-2153,0,\N,Missing
J03-1002,J04-4002,1,\N,Missing
J03-1002,C98-1006,0,\N,Missing
J03-1002,C98-2157,1,\N,Missing
J04-4002,J00-1004,0,0.0526268,"Missing"
J04-4002,H94-1028,0,0.0189479,"Missing"
J04-4002,J96-1002,0,0.113258,"Missing"
J04-4002,J90-2002,0,0.471491,"Computational Linguistics Volume 30, Number 4 Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through 20031 obtain the best results. In addition, the field of statistical machine translation is rapidly progressing, and the quality of systems is getting better and better. An important factor in these improvements is definitely the availability of large amounts of data for training statistical models. Yet the modeling, training, and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s (Brown et al. 1990; Brown et al. 1993; Berger et al. 1994). This article focuses on an important improvement, namely, the use of (generalized) phrases instead of just single words as the core elements of the statistical translation model. We describe in Section 2 the basics of our statistical translation model. We suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters. This approach can be seen as a generalization of the originally suggested source–channel modeling framework for sta"
J04-4002,J93-2003,0,0.237322,"uistics Volume 30, Number 4 Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through 20031 obtain the best results. In addition, the field of statistical machine translation is rapidly progressing, and the quality of systems is getting better and better. An important factor in these improvements is definitely the availability of large amounts of data for training statistical models. Yet the modeling, training, and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s (Brown et al. 1990; Brown et al. 1993; Berger et al. 1994). This article focuses on an important improvement, namely, the use of (generalized) phrases instead of just single words as the core elements of the statistical translation model. We describe in Section 2 the basics of our statistical translation model. We suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters. This approach can be seen as a generalization of the originally suggested source–channel modeling framework for statistical machine tr"
J04-4002,2003.mtsummit-papers.6,0,0.0862691,"Missing"
J04-4002,P98-2158,0,0.0628605,"Missing"
J04-4002,P01-1027,1,0.202339,"Missing"
J04-4002,P01-1030,0,0.0237461,"uning: beam search. In pruning, we constrain the set of considered translation candidates (the “beam”) only to the promising ones. We compare in beam search those hypotheses that cover different parts of the input sentence. This makes the comparison of the probabilities problematic. Therefore, we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation (Section 5.6) Many of the other search approaches suggested in the literature do not meet the described aims: • Neither optimal A* search (Och, Ueffing, and Ney 2001) nor optimal integer programming (Germann et al. 2001) for statistical MT allows efficient search for long sentences. • Greedy search algorithms (Wang 1998; Germann et al. 2001) typically commit severe search errors (Germann et al. 2001). • Other approaches to solving the search problem obtain polynomial time algorithms by assuming monotone alignments (Tillmann et al. 1997) or imposing a simplified recombination structure (Nießen et al. 1998). Others make simplifying assumptions about the search space (Garc´ıa-Varea, Casacuberta, and Ney 1998; Garc´ıa-Varea et al. 2001), as does the original IBM stack search decoder (Berger et al. 1994). All thes"
J04-4002,P03-1011,0,0.0521328,"a translation of a new sentence be composed of a set of alignment templates that covers the source sentence and the produced translation. Other feature functions score the well-formedness of the produced target language sentence (i.e., language model feature functions), the number of produced words, or the order of the alignment templates. Note that all components of our statistical machine translation model are purely data-driven and that there is no need for linguistically annotated corpora. This is an important advantage compared to syntax-based translation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada 2003) that require a parser for source or target language. In Section 5, we describe in detail our search algorithm and discuss an efficient implementation. We use a dynamic-programming-based beam search algorithm that allows a trade-off between efficiency and quality. We also discuss the use of heuristic functions to reduce the number of search errors for a fixed beam size. In Section 6, we describe various results obtained on different tasks. For the German–English Verbmobil task, we analyze the effect of various system compo1 http://www.nist.gov/speech/tests/m"
J04-4002,J99-4005,0,0.506674,"sible translations that can be produced given a certain input sentence. To solve this problem, we define as reference translation for maximum-entropy training each sentence that has the minimal number of word errors with respect to any of the reference translations in the n-best list. More details of the training procedure can be found in Och and Ney (2002). 5. Search In this section, we describe an efficient search architecture for the alignment template model. 5.1 General Concept In general, the search problem for statistical MT even using only Model 1 of Brown et al. (1993) is NP-complete (Knight 1999). Therefore, we cannot expect to develop 431 Computational Linguistics Volume 30, Number 4 efficient search algorithms that are guaranteed to solve the problem without search errors. Yet for practical applications it is acceptable to commit some search errors (Section 6.1.2). Hence, the art of developing a search algorithm lies in finding suitable approximations and heuristics that allow an efficient search without committing too many search errors. In the development of the search algorithm described in this section, our main aim is that the search algorithm should be efficient. It should be"
J04-4002,N03-1017,1,0.234788,"Missing"
J04-4002,W02-1018,0,0.163773,"to be distinguished from the use of the term in a linguistic sense. The learned bilingual phrases are not constrained by linguistic phrase boundaries. Compared to the word-based statistical translation models in Brown et al. (1993), this model is based on a (statistical) phrase lexicon instead of a single-word-based lexicon. Looking at the results of the recent machine translation evaluations, this approach seems currently to give the best results, and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes (Marcu and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and Marcu 2003). Our approach to learning a phrase translation lexicon works in two stages: In the first stage, we compute an alignment between words, and in the second stage, we extract the aligned phrase pairs. In our machine translation system, we then use generalized versions of these phrases, called alignment templates, that also include the word alignment and use word classes instead of the words themselves. In Section 4, we describe the various components of the statistical translation model. The backbone of the translation"
J04-4002,H01-1062,0,0.0651359,"Missing"
J04-4002,niessen-etal-2000-evaluation,1,0.307398,"ults. Therefore, we use various criteria. In the following experiments, we use: • WER (word error rate)/mWER (multireference word error rate): The WER is computed as the minimum number of substitution, insertion, and deletion operations that have to be performed to convert the generated sentence into the target sentence. In the case of the multireference word error rate for each test sentence, not just a single reference translation is used, as for the WER, but a whole set of reference translations. For each translation hypothesis, the edit distance to the most similar sentence is calculated (Nießen et al. 2000). • PER (position-independent WER): A shortcoming of the WER is the fact that it requires a perfect word order. An acceptable sentence can have a word order that is different from that of the target sentence, so the WER measure alone could be misleading. To overcome this problem, we introduce as an additional measure the position-independent word error rate. This measure compares the words in the two sentences, ignoring the word order. • BLEU (bilingual evalutation understudy) score: This score measures the precision of unigrams, bigrams, trigrams, and 4-grams with respect to a whole set of re"
J04-4002,E99-1010,1,0.31818,"onds to a bilingual phrase together with an alignment within this phrase. Figure 4 shows examples of alignment templates. ˜ is represented as a matrix with J · (I + 1) binary elements. A The alignment A matrix element with value 1 means that the words at the corresponding positions are aligned, and the value 0 means that the words are not aligned. If a source word is not aligned with a target word, then it is aligned with the empty word e0 , which is at the imaginary position i = 0.   The classes used in FJ1 and EI1 are automatically trained bilingual classes using the method described in Och (1999) and constitute a partition of the vocabulary of source and target language. In general, we are not limited to disjoint classes as long as each specific instance of a word is disambiguated, that is, uniquely belongs to a specific class. In the following, we use the class function C to map words to their classes. Hence, it would be possible to employ parts-of-speech or semantic categories instead of the automatically trained word classes used here. The use of classes instead of the words themselves has the advantage of better generalization. For example, if there exist classes in source and tar"
J04-4002,P03-1021,1,0.349853,"probability in Bayes’ decision rule is referred to as discriminative training (Ney 1995) because we directly take into account the overlap in the probability distributions. The optimization problem under this criterion has very nice properties: There is one unique global optimum, and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the global optimum. Yet the ultimate goal is to obtain good translation quality on unseen test data. An alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion (Och 2003). Typically, the translation probability Pr(eI1 |f1J ) is decomposed via additional hidden variables. To include these dependencies in our log-linear model, we extend the feature functions to include the dependence on the additional hidden variable. Using for example the alignment aJ1 as hidden variable, we obtain M feature functions of the form hm (eI1 , f1J , aJ1 ), m = 1, . . . , M and the following model:   M exp λm hm (eI1 , f1J , aJ1 ) m=1   Pr(eI1 , aJ1 |f1J ) =  M  I , f J , a J ) λ h (e J exp I m m   1 m=1 1 1 e ,a 1 1 Obviously, we can perform the same step for translation"
J04-4002,P02-1038,1,0.743587,"ical Machine Translation We are given a source (French) sentence f = f1J = f1 , . . . , fj , . . . , fJ , which is to be translated into a target (English) sentence e = eI1 = e1 , . . . , ei , . . . , eI . Among all possible target sentences, we will choose the sentence with the highest probability:2 ˆeI1 = argmax {Pr(eI1 |f1J )} (1) eI1 The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language. As an alternative to the often used source–channel approach (Brown et al. 1993), we directly model the posterior probability Pr(eI1 |f1J ) (Och and Ney 2002). An especially well-founded framework for doing this is the maximum-entropy framework (Berger, Della Pietra, and Della Pietra 1996). In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M. For each feature function, there exists a model 2 The notational convention employed in this article is as follows. We use the symbol Pr(·) to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·). 419 Computational Linguistics Volume 30, Number 4 parameter λm ,"
J04-4002,J03-1002,1,0.0920734,"ion model and the alignment model is given by Pr(f1J |eI1 ) =  Pr(f1J , aJ1 |eI1 ) (5) aJ1 The alignment aJ1 may contain alignments aj = 0 with the “empty” word e0 to account for source words that are not aligned with any target word. In general, the statistical model depends on a set of unknown parameters θ that is learned from training data. To express the dependence of the model on the parameter set, we use the following notation: Pr(f1J , aJ1 |eI1 ) = pθ (f1J , aJ1 |eI1 ) (6) A detailed description of different specific statistical alignment models can be found in Brown et al. (1993) and Och and Ney (2003). Here, we use the hidden Markov model (HMM) alignment model (Vogel, Ney, and Tillmann 1996) and Model 4 of Brown et al. (1993) to compute the word alignment for the parallel training corpus. To train the unknown parameters θ, we are given a parallel training corpus consisting of S sentence pairs {(fs , es ): s = 1, . . . , S}. For each sentence pair (fs , es ), the alignment variable is denoted by a = aJ1 . The unknown parameters θ are determined by maximizing the likelihood on the parallel training corpus: θˆ = argmax θ  S  s=1  pθ (fs , a |es ) (7) a This optimization can be performed us"
J04-4002,W01-1408,1,0.0914589,"Missing"
J04-4002,2001.mtsummit-papers.68,0,0.0374077,"s the fact that it requires a perfect word order. An acceptable sentence can have a word order that is different from that of the target sentence, so the WER measure alone could be misleading. To overcome this problem, we introduce as an additional measure the position-independent word error rate. This measure compares the words in the two sentences, ignoring the word order. • BLEU (bilingual evalutation understudy) score: This score measures the precision of unigrams, bigrams, trigrams, and 4-grams with respect to a whole set of reference translations, with a penalty for too-short sentences (Papineni et al. 2001). Unlike all other evaluation criteria used here, BLEU measures accuracy, that is, the opposite of error rate. Hence, the larger BLEU scores, the better. In the following, we analyze the effect of various system components: alignment template length, search pruning, and language model n-gram size. A systematic evaluation of the alignment template system comparing it with other translation approaches (e.g., rule-based) has been performed in the Verbmobil project and is described in Tessiore and von Hahn (2000). There, the alignment-template-based system achieved a significantly larger number of"
J04-4002,W03-1001,0,0.0132657,"istic sense. The learned bilingual phrases are not constrained by linguistic phrase boundaries. Compared to the word-based statistical translation models in Brown et al. (1993), this model is based on a (statistical) phrase lexicon instead of a single-word-based lexicon. Looking at the results of the recent machine translation evaluations, this approach seems currently to give the best results, and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes (Marcu and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and Marcu 2003). Our approach to learning a phrase translation lexicon works in two stages: In the first stage, we compute an alignment between words, and in the second stage, we extract the aligned phrase pairs. In our machine translation system, we then use generalized versions of these phrases, called alignment templates, that also include the word alignment and use word classes instead of the words themselves. In Section 4, we describe the various components of the statistical translation model. The backbone of the translation model is the alignment template feature function,"
J04-4002,C00-2123,1,0.0571653,"A) (27) j 3 Note that here some of the simplifying notation of Section 4 has been used. 433 Computational Linguistics Volume 30, Number 4 If the source sentence contains words that have not been seen in the training data, we introduce a new alignment template that performs a one-to-one translation of each of these words by itself. In the second step, we determine a set of probable target language words for each target word position in the alignment template instantiation. Only these words are then hypothesized in the search. We call this selection of highly probable words observation pruning (Tillmann and Ney 2000). As a criterion for a word e at position i in the alignment template instantiation, we use  δ(Ei , C(e)) · J  j=0 ˜ j) A(i,  ˜  · p(e |fj )  A(i , j) (28) i In our experiments, we hypothesize only the five best-scoring words. A decision is a triple d = (Z, e, l) consisting of an alignment template instantiation Z, the generated word e, and the index l of the generated word in Z. A hypothesis n corresponds to a valid sequence of decisions di1 . The possible decisions are as follows: 1. Start a new alignment template: di = (Zi , ei , 1). In this case, the index l = 1. This decision can be"
J04-4002,J03-1005,1,0.356489,"ch space (Garc´ıa-Varea, Casacuberta, and Ney 1998; Garc´ıa-Varea et al. 2001), as does the original IBM stack search decoder (Berger et al. 1994). All these simplifications ultimately make the search problem simpler but introduce fundamental search errors. In the following, we describe our search algorithm based on the concept of beam search, which allows a trade-off between efficiency and quality by adjusting the size of the beam. The search algorithm can be easily adapted to other phrase-based translation models. For single-word-based search in MT, a similar algorithm has been described in Tillmann and Ney (2003). 5.2 Search Problem Putting everything together and performing search in maximum approximation, we obtain the following decision rule:  M   I I J K K ˆe1 = argmax λm · hm (e1 , f1 , π1 , z1 ) (22) eI1 ,π1K ,zK1 432 m=1 Och and Ney The Alignment Template Approach to Statistical Machine Translation Using the four feature functions AT, AL, WRD, and LM, we obtain the following decision rule:3  ˆeI1 = argmax (23) eI1 ,π1K ,zK1 I  λLM log p(ei |ei−2 , ei−1 ) + λWRD log p(ei |{fj |(i, j) ∈ A}, Ei ) (24) i=1 + K   jπ λAT log p(zk |fjπ k k−1 k=1  ) + λ · |j − j | πk−1 +1 +1 AL πk  +λAL · (J −"
J04-4002,P03-1041,0,0.240823,"Missing"
J04-4002,C96-2141,1,0.565161,"Missing"
J04-4002,P97-1047,0,0.0116127,"f edge probabilities of reaching a goal node is always equal to or smaller than the estimated probability. For an A*-based search algorithm, a good heuristic function is crucial to being able to translate long sentences. For a beam search algorithm, the heuristic function has a different motivation. It is used to improve the scoring of search hypotheses. The goal is to make the probabilities of all hypotheses more comparable, in order to minimize the chance that the hypothesis leading to the optimal translation is pruned away. Heuristic functions for search in statistical MT have been used in Wang and Waibel (1997) and Och, Ueffing, and Ney (2001). Wang and Waibel (1997) have described a simple heuristic function for Model 2 of Brown et al. (1993) that was not admissible. Och, Ueffing, and Ney (2001) have described an admissible heuristic function for Model 4 of Brown et al. (1993) and an almost-admissible heuristic function that is empirically obtained. We have to keep in mind that a heuristic function is helpful only if the overhead introduced in computing the heuristic function is more than compensated for by the gain obtained through a better pruning of search hypotheses. The heuristic functions des"
J04-4002,P98-2230,0,0.174529,"Missing"
J04-4002,P01-1067,0,0.712464,"ion, which requires that a translation of a new sentence be composed of a set of alignment templates that covers the source sentence and the produced translation. Other feature functions score the well-formedness of the produced target language sentence (i.e., language model feature functions), the number of produced words, or the order of the alignment templates. Note that all components of our statistical machine translation model are purely data-driven and that there is no need for linguistically annotated corpora. This is an important advantage compared to syntax-based translation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada 2003) that require a parser for source or target language. In Section 5, we describe in detail our search algorithm and discuss an efficient implementation. We use a dynamic-programming-based beam search algorithm that allows a trade-off between efficiency and quality. We also discuss the use of heuristic functions to reduce the number of search errors for a fixed beam size. In Section 6, we describe various results obtained on different tasks. For the German–English Verbmobil task, we analyze the effect of various system compo1 http://www.nist.gov/s"
J04-4002,P02-1040,0,\N,Missing
J04-4002,C98-2153,0,\N,Missing
J04-4002,P97-1037,1,\N,Missing
J04-4002,C98-2225,0,\N,Missing
N03-1017,P97-1003,0,0.13436,"slation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997]. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typi"
N03-1017,P01-1030,1,0.141267,"Missing"
N03-1017,2002.tmi-papers.9,0,0.138734,"ad sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from rece"
N03-1017,W02-1018,1,0.154807,"Missing"
N03-1017,P00-1056,1,0.180643,"ed phrases could filter out such non-intuitive pairs. We carried out experiments to compare the performance of three different methods to build phrase translation probability tables. We also investigate a number of variations. We report most experimental results on a GermanEnglish translation task, since we had sufficient resources available for this language pair. We confirm the major points in experiments on additional language pairs. As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993]. The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002]. They only consider word sequences as phrases, if they are constituents, i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde,"
N03-1017,W99-0604,1,0.228551,"f phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction. 3.1 Phrases from Word-Based Alignments The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described in more detail in Section 4.5. We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside [Och et al., 1999]. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency:    count           count     No smoothing is performed. 3.2 Syntactic Phrases If we collect all phrase pairs that are consistent with word alignments, this includes many non-intuitive phrases. For instance, translations for phrases such as “house the” may be learned. Intuitively we would be inclined to believe that such phrases do not help: Restricting possible Consistent with Imamura [2002], we define a syntactic phrase as a word sequence that is covere"
N03-1017,W01-1408,1,0.139726,"Missing"
N03-1017,2001.mtsummit-papers.68,0,0.154084,"14k 373k BLEU  .27    .26  Table 1: Size of the phrase translation table in terms of distinct phrase pairs (maximum phrase length 4)   .25    .24    4 Experiments .23 We used the freely available Europarl corpus 2 to carry out experiments. This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the European Parliament 1996-2001. 1755 sentences of length 5-15 were reserved for testing. In all experiments in Section 4.1-4.6 we translate from German to English. We measure performance using the BLEU score [Papineni et al., 2001], which estimates the accuracy of translation output with respect to a reference translation. .22 4.1 Comparison of Core Methods     .21   .20  .19 AP Joint M4 Syn  .18 10k 20k 40k 80k 160k 320k Training Corpus Size   Figure 1: Comparison of the core methods: all phrase pairs consistent with a word alignment (AP), phrase pairs from the joint model (Joint), IBM Model 4 (M4), and only syntactic phrases (Syn) First, we compared the performance of the three methods for phrase extraction head-on, using the same decoder (Section 2) and the same trigram language model. Figure 1 displays th"
N03-1017,C00-2105,0,0.142196,"Missing"
N03-1017,J97-3002,0,0.294318,"i.e. subtrees in a syntax tree (such as a noun phrase). To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000]. The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002]. This model learns directly a phrase-level alignment of the parallel corpus. Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997]. In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees. When augmenting such models with phrase translations, typically only translation of phrases that span entire syntactic subtrees is possible. It is important to know if this is a helpful or harmful restriction. 3.1 Phrases from Word-Based Alignments The Giza++ toolkit was developed to train word-based translation models from parallel corpora. As a byproduct, it generates word alignments for this data. We improve this alignment with a number of heuristics, which are described"
N03-1017,P01-1067,0,0.277456,"r to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table. Our experiments show that high levels of performance can be achieved with fairly simple means. In fact, for most of the steps necessary to build a phrase-based system, tools and resources are freely available for researchers in the field. More sophisticated approaches that make use of syntax do not lead to better performance. In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models [Yamada and Knight, 2001], proves to be harmful. Our experiments also show, that small phrases of up to three words are sufficient for obtaining high levels of accuracy. Performance differs widely depending on the methods used to build the phrase translation table. We found extraction heuristics based on word alignments to be better than a more principled phrase-based alignment method. However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus. 1 Introduction 2 Evaluation Framework Various researchers have improved the quality of statistica"
N03-1017,J93-2003,0,\N,Missing
N03-1017,P02-1040,0,\N,Missing
N04-1021,J00-1004,0,0.0540744,"Missing"
N04-1021,J93-2003,0,0.0210066,"e different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear"
N04-1021,P03-1011,1,0.218398,"of words in tree fragment and k for maximum height of with some probability, to transform one tree into another. tree fragment. We proceed from left to right in the ChiHowever, when training the model, trees for both the nese sentence and incrementally grow a pair of subtrees, source and target languages are provided, in our case one subtree in Chinese and the other in English, such that from the Chinese and English parsers. each word in the Chinese subtree is aligned to a word in We began with the tree-to-tree alignment model prethe English subtree. We grow this pair of subtrees unsented by Gildea (2003). The model was extended to hantil we can no longer grow either subtree without violatdle dependency trees, and to make use of the word-level ing the two parameter values n and k. Note that these alignments produced by the baseline MT system. The aligned subtree pairs have properties similar to alignment probability assigned by the tree-to-tree alignment model, templates. They can rearrange in complex ways between given the word-level alignment with which the candidate source and target. Figure 2 shows how subtree-pairs for translation was generated, was used as a feature in our parameters n ="
N04-1021,P03-1021,1,0.129951,"nsisting of S sentence pairs {(fs , es ) : s = 1, . . . , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment te"
N04-1021,P02-1038,1,0.203592,"the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting"
N04-1021,J04-4002,1,0.260908,"ters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to π1K ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases e˜K 1 constitutes the sequence of words eI1 . Our baseline system incorporat"
N04-1021,W99-0604,1,0.236444,"Missing"
N04-1021,W03-1002,0,0.0153043,"of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P (ATi is right-continuous) or 1 − P (ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). 5 Shallow Syntactic Feature Functions By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. 5.1 Projected POS Language Model This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: CD +0 M +1 NN +3 NN -1 NN"
N04-1021,C96-2141,0,0.254737,"Missing"
N04-1021,P98-2230,0,0.0760244,"Missing"
N04-1021,P01-1067,1,0.0551165,"ical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. 6.2 Tree-to-String Alignment A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f |T (e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English par"
N04-1021,C98-2225,0,\N,Missing
N04-1023,J90-2002,0,0.65721,"a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Pe"
N04-1023,P02-1034,0,0.064415,"Missing"
N04-1023,P03-1011,0,0.0173227,"thin a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model  estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations genera"
N04-1023,P02-1038,1,0.3858,"thms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs). In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been"
N04-1023,P98-2162,1,0.686374,"ke word context into account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not u"
N04-1023,W99-0604,1,0.666456,"account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yama"
N04-1023,P03-1021,1,0.0734557,"ne translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs). In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been proposed in recent years. Based o"
N04-1023,2001.mtsummit-papers.68,0,0.0624691,"Missing"
N04-1023,W03-0402,1,0.817649,"2 - . 3.6  from .- , but not for the case of Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its large margin version is able to provide relatively good results in general. 3.7 Pairwise Samples In previous work on the PRank algorithm, ranks are defined on the entire training and test data. Thus we can define boundaries between consecutive ranks on the entire data. But in MT reranking, ranks are defined over every single source sentence. For example, in our data set, the rank of a translation is only the rank among all th"
N04-1023,P98-2221,0,0.0109325,"lity over candidate translations and the translation model   which is a generative conditional probability of the source sentence given a candidate translation  . The lexicon of the single-word based IBM models does not take word context into account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in"
N04-1023,J97-3002,0,0.0127999,"del based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used t"
N04-1023,P01-1067,0,0.0765215,"999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model"
N04-1023,A00-2018,0,\N,Missing
N04-1023,J93-2003,0,\N,Missing
N04-1023,W03-1012,1,\N,Missing
N04-1023,P02-1040,0,\N,Missing
N04-1023,J05-1003,0,\N,Missing
N04-1023,C98-2157,1,\N,Missing
N04-1023,C98-2216,0,\N,Missing
N09-1028,P06-1067,0,0.123367,"ecomes one of the key weaknesses. Many reordering methods have been proposed in recent years to address this problem in different aspects. 245 The first class of approaches tries to explicitly model phrase reordering distances. Distance based distortion model (Och, 2002; Koehn et.al., 2003) is a simple way of modeling phrase level reordering. It penalizes non-monotonicity by applying a weight to the number of words between two source phrases corresponding to two consecutive target phrases. Later on, this model was extended to lexicalized phrase reordering (Tillmann, 2004; Koehn, et.al., 2005; Al-Onaizan and Papineni, 2006) by applying different weights to different phrases. Most recently, a hierarchical phrase reordering model (Galley and Manning, 2008) was proposed to dynamically determine phrase boundaries using efficient shift-reduce parsing. Along this line of research, discriminative reordering models based on a maximum entropy classifier (Zens and Ney, 2006; Xiong, et.al., 2006) also showed improvements over the distance based distortion model. None of these reordering models changes the word alignment step in SMT systems, therefore, they can not recover from the word alignment errors. These models are al"
N09-1028,N04-4026,0,\N,Missing
N09-1028,C04-1073,0,\N,Missing
N09-1028,W08-0406,0,\N,Missing
N09-1028,C08-1027,0,\N,Missing
N09-1028,D08-1089,0,\N,Missing
N09-1028,P02-1040,0,\N,Missing
N09-1028,P07-1002,0,\N,Missing
N09-1028,P06-1077,0,\N,Missing
N09-1028,P01-1067,0,\N,Missing
N09-1028,P06-1066,0,\N,Missing
N09-1028,J04-4002,1,\N,Missing
N09-1028,C08-1144,1,\N,Missing
N09-1028,C04-1010,0,\N,Missing
N09-1028,P07-1091,0,\N,Missing
N09-1028,P05-1033,0,\N,Missing
N09-1028,N03-1017,1,\N,Missing
N09-1028,P05-1034,0,\N,Missing
N09-1028,W06-3108,0,\N,Missing
N09-1028,J97-3002,0,\N,Missing
N09-1028,P07-1019,0,\N,Missing
N09-1028,P05-1066,0,\N,Missing
N09-1028,P06-1121,0,\N,Missing
N09-1028,W06-3119,0,\N,Missing
N09-1028,W04-3250,0,\N,Missing
N09-1028,D07-1077,0,\N,Missing
N09-1028,2005.iwslt-1.8,0,\N,Missing
N09-1028,2007.iwslt-1.3,0,\N,Missing
N09-1028,W02-2018,0,\N,Missing
N09-1028,D08-1076,1,\N,Missing
N09-1028,P03-1021,1,\N,Missing
N10-1141,N09-2019,0,0.0608563,"Missing"
N10-1141,P09-1064,1,0.923825,"mprovements across data sets and language pairs in large-scale experiments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a lin"
N10-1141,A94-1016,0,0.199907,"ted. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By r"
N10-1141,D09-1125,0,0.38116,"t does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via weights on system indicator features. Despite its simplicity, model combination provides strong performance by leveraging existing consensus, search, and training techniques. The technique outperforms MBR and consensus decoding on each of the component systems. In addition, it performs better than standard sentence-based or word-based system combination techniques applied to either max-derivation or MBR ou"
N10-1141,D08-1011,0,0.243258,"(or a lattice) of translations. This flexibility allows the technique to be applied quite broadly. For instance, de Gispert et al. (2009) describe combining systems based on multiple source representations using minimum Bayes risk decoding—likewise, they could be combined via model combination. Model combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via w"
N10-1141,N04-1022,1,0.62066,"ystem combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivatio"
N10-1141,P09-1019,1,0.566957,"forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; the latent derivation structures of component systems can differ arbitrarily. This flexibility allows us to combine phrase-based, hierarchical, and syntax-augmented"
N10-1141,D09-1005,0,0.0131418,"7: 8: for ` ∈ Leaves(r) do 9: b ← b × β(`) 10: for g ∈ Ngrams(n)  do  ¯b(g) ← ¯b(g) × β(`) − β(`, ˆ g) 11: 12: 13: 14: 15: 16: 17: 18: 19: β(n) ← β(n) + b for g ∈ Ngrams(n) do if g ∈ Ngrams(r) then ˆ g) ← β(n, ˆ g)+b β(n, else ˆ g) ← β(n, ˆ g)+b − ¯b(g) β(n, for g ∈ Ngrams(root) (all g in the HG) do ˆ ,g) P (g|f ) ← β(β(root root) This algorithm can in principle compute the posterior probability of any indicator function on local features of a derivation. More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4 Indicator functions on derivations are not locally additive 978 3.2 Ensuring N -gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). I"
N10-1141,W08-0402,0,0.00573615,"ures (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4 Indicator functions on derivations are not locally additive 978 3.2 Ensuring N -gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). In this case, a forest encoding of a posterior distribution may not exhibit n-gram locality in all regions of the search space. Figure 3 shows a hypergraph which contains nonlocal trigrams, along with its local expansion. Algorithm 2 expands a forest to ensure n-gram locality while preserving the encoded distribution over derivations. Let a forest (N, R) consist of nodes N and hyperedges R, which correspond to rule applications. Let Rules(n) be the subset of R rooted by n, and Leaves(r) be the leaf nodes of rule application r. The expanded forest (Ne , Re ) is constructed by a function Reapp"
N10-1141,P09-1066,0,0.628281,"iments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component."
N10-1141,P09-1067,0,0.177181,"iments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component."
N10-1141,P09-1065,0,0.208006,"pio cedure could be applied to statistical systems that mar et al. (2009).5 only generate k-best lists. However, we would not ! 2 "" expect the same strong performance from model 4.1v2 =System Combination “saw the”: vpb = 0.9, 0.7 h combination in these constrained settings. System combination techniques in machine translation take as input the outputs {e1 , · · · , ek } of k dd Features for the Combination Model 4.2 Joint Decoding and Collaborative Decoding translation systems, where ei is a structured translaModel Training and Inference tion object (or k-best lists thereof), typically viewed Liu et al. (2009) describes two techniques for com!"" # $ as a sequence of words. The dominant approach in bining multiple synchronous grammars, which the arg max BLEU arg max sw (d) ; e the field chooses a primary translation ep as a back- Rauthors characterize as joint decoding. Joint dew d∈D(f ) Rpb Rh not involve a consensus or minimumbone, then finds an alignment ai to the backbone for coding does arg max sw (d) d∈D each ei . A new search space is constructed from Bayes-risk decoding objective; indeed, their best these backbone-aligned outputs, and then a voting results come from standard max-derivation de"
N10-1141,D07-1105,1,0.929139,"Missing"
N10-1141,J04-4002,1,0.241364,"Missing"
N10-1141,P02-1040,0,0.108937,"ing minimum Bayes risk decoding—likewise, they could be combined via model combination. Model combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via weights on system indicator features. Despite its simplicity, model combination provides strong performance by leveraging existing consensus, search, and training techniques. The technique outperforms MBR and consens"
N10-1141,N07-1029,0,0.164137,"Missing"
N10-1141,D08-1065,1,0.887618,"vations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; the latent derivation structures of component systems can differ arbitrarily. This flexibility allows us to combine phrase-based, hierarchical,"
N10-1141,N09-2052,0,0.247129,"Missing"
N10-1141,W06-3119,0,0.0204,"Missing"
N10-1141,J07-2003,0,\N,Missing
N15-1186,N06-2001,0,0.0784517,"uang et al., 2012; Le and Mikolov, 2014). The main advantage offered by these techniques is that they can be both trained in an unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words. Previous attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor1628 phology using vector-space representations (Luong et al., 2013; Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representati"
N15-1186,P14-2131,0,0.0247038,"factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor1628 phology using vector-space representations (Luong et al., 2013; Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representations for existing morphemes, and also to combine them into (possibly novel) embedding representations for words that may not have been seen at training time. Common to these proposals is the fact that the morphological analysis of words is treated as an external, preprocessing-style step. This step is done using off-the-shelf analyzers such as Morfessor (Creutz and Lagus, 2007). As a result, the morphological analysis happens within a different model compared to the model in which the resulting morphemes are consequently used. In contrast, the work pre"
N15-1186,D09-1124,0,0.0622909,"ublicly-available word-similarity datasets. Most relevant for our approach is the Stanford English Rare-Word (RW) dataset (Luong et al., 2013), consisting of 2034 word pairs with a higher degree of English morphology compared to other word-similarity datasets. We also use for English the WS353 (Finkelstein et al., 2002) and RG65 datasets (Rubenstein and Goodenough, 1965). For German, we use the Gur350 and ZG222 datasets (Zesch and Gurevych, 2006). For French we use the RG65 French version (Joubarne and Inkpen, 2011); for Spanish, Romanian, and Arabic we use their respective versions of WS353 (Hassan and Mihalcea, 2009). Results We present in Table 4 the results obtained across 6 language pairs and 9 datasets, using a count threshold for SG+Morph of C = 100. We also include the results obtained by two previouslyproposed methods, LSM2013 (Luong et al., 2013) and BB2014 (Botha and Blunsom, 2014), which share some of the characteristics of our method. Even in the absence of any morphological treatment, our word representations are better than previously used ones. For instance, LSM2013 uses exactly the same EN Wikipedia (Shaoul and Westbury, 2010) training data, and achieves 26.8 and 34.4 Spearman ρ correlation"
N15-1186,P12-1092,0,0.0496334,", French, Spanish, Romanian, Arabic, and Uzbek. The results indicate that the induced morphological analysis deals successfully with sophisticated morphological variations. 2 Previous Work Many recent proposals in the literature use wordrepresentations as the basic units for tackling sentence-level tasks such as language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b), discriminative parsing (Collobert, 2011), as well as similar tasks involving larger units such as documents (Glorot et al., 2011; Huang et al., 2012; Le and Mikolov, 2014). The main advantage offered by these techniques is that they can be both trained in an unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words. Previous attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and"
N15-1186,J14-1004,0,0.0212892,"method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages. 1 Introduction Word representations obtained via neural networks (Bengio et al., 2003; Socher et al., 2011a) or specialized models (Mikolov et al., 2013a) have been used to address various natural language processing tasks (Mnih et al., 2009; Huang et al., 2014; Bansal et al., 2014). These vector representations capture various syntactic and semantic properties of natural language (Mikolov et al., 2013b). In many instances, natural language uses a small set of concepts to render a much larger set of meaning variations via morphology. We show in this paper that morphological transformations can be captured by exploiting regularities present in wordrepresentations as the ones trained using the SkipGram model (Mikolov et al., 2013a). In contrast to previous approaches that combine morphology with vector-based word representations (Luong et al., 2013; B"
N15-1186,P13-1149,0,0.141675,"unsupervised manner, and also tuned using supervised labels. However, most of these approaches treat words as units, and fail to account for phenomena involving the relationship between various morphological forms that affect word semantics, especially for rare or unseen words. Previous attempts at dealing with sub-word units and their compositionality have looked at explicitlyengineered features such as stems, cases, POS, etc., and used models such as factored NLMs (Alexandrescu and Kirchhoff, 2006) to obtain representations for unseen words, or compositional distributional semantic models (Lazaridou et al., 2013) to derive representations for morphologically-inflected words, based on the composing morphemes. A more recent trend has seen proposals that deal with mor1628 phology using vector-space representations (Luong et al., 2013; Botha and Blunsom, 2014). Given word morphemes (affixes, roots), a neural-network architecture (recursive neural networks in the work of Luong et al (2013), log-bilinear models in the case of Botha and Blunsom (2014)), is used to obtain embedding representations for existing morphemes, and also to combine them into (possibly novel) embedding representations for words that m"
N15-1186,P14-2050,0,0.0409262,"rast to previous approaches that combine morphology with vector-based word representations (Luong et al., 2013; Botha and Blunsom, 2014), we do not rely on an external morphological analyzer, such as Morfessor (Creutz and La∗ Work done at Google, now at Human Longevity Inc. gus, 2007). Instead, our method automatically induces morphological rules and transformations, represented as vectors in the same embedding space. At the heart of our method is the SkipGram model described in (Mikolov et al., 2013a). We further exploit the observations made by Mikolov et al (2013b), and further studied by (Levy and Goldberg, 2014; Pennington et al., 2014), regarding the regularities exhibited by such embedding spaces. These regularities have been shown to allow inferences of certain types (e.g., king is to man what queen is to woman). Such regularities also hold for certain morphological relations (e.g., car is to cars what dog is to dogs). In this paper, we show that one can exploit these regularities to model, in a principled way, prefix- and suffix-based morphology. The main contributions of this paper are as follows: 1. provides a method by which morphological rules are learned in an unsupervised, languageagnostic"
N15-1186,W13-3512,0,0.46238,"Missing"
N15-1186,N13-1090,0,0.317691,"present a language agnostic, unsupervised method for inducing morphological transformations between words. The method relies on certain regularities manifest in highdimensional vector spaces. We show that this method is capable of discovering a wide range of morphological rules, which in turn are used to build morphological analyzers. We evaluate this method across six different languages and nine datasets, and show significant improvements across all languages. 1 Introduction Word representations obtained via neural networks (Bengio et al., 2003; Socher et al., 2011a) or specialized models (Mikolov et al., 2013a) have been used to address various natural language processing tasks (Mnih et al., 2009; Huang et al., 2014; Bansal et al., 2014). These vector representations capture various syntactic and semantic properties of natural language (Mikolov et al., 2013b). In many instances, natural language uses a small set of concepts to render a much larger set of meaning variations via morphology. We show in this paper that morphological transformations can be captured by exploiting regularities present in wordrepresentations as the ones trained using the SkipGram model (Mikolov et al., 2013a). In contrast"
N15-1186,D14-1162,0,0.116762,"Missing"
N15-1186,D11-1014,0,0.121673,"Missing"
N15-1186,W06-1104,0,0.0778886,"illustrate the richness of the morphological phenomena present in languages such as German, Romanian, Arabic, and Uzbek, compared to English. As test sets, we use standard, publicly-available word-similarity datasets. Most relevant for our approach is the Stanford English Rare-Word (RW) dataset (Luong et al., 2013), consisting of 2034 word pairs with a higher degree of English morphology compared to other word-similarity datasets. We also use for English the WS353 (Finkelstein et al., 2002) and RG65 datasets (Rubenstein and Goodenough, 1965). For German, we use the Gur350 and ZG222 datasets (Zesch and Gurevych, 2006). For French we use the RG65 French version (Joubarne and Inkpen, 2011); for Spanish, Romanian, and Arabic we use their respective versions of WS353 (Hassan and Mihalcea, 2009). Results We present in Table 4 the results obtained across 6 language pairs and 9 datasets, using a count threshold for SG+Morph of C = 100. We also include the results obtained by two previouslyproposed methods, LSM2013 (Luong et al., 2013) and BB2014 (Botha and Blunsom, 2014), which share some of the characteristics of our method. Even in the absence of any morphological treatment, our word representations are better"
N15-1186,W13-2201,1,\N,Missing
niessen-etal-2000-evaluation,1993.mtsummit-1.11,0,\N,Missing
niessen-etal-2000-evaluation,P98-2158,0,\N,Missing
niessen-etal-2000-evaluation,C98-2153,0,\N,Missing
niessen-etal-2000-evaluation,P98-1006,0,\N,Missing
niessen-etal-2000-evaluation,C98-1006,0,\N,Missing
P00-1056,ahrenberg-etal-2000-evaluation,0,\N,Missing
P00-1056,C00-2163,1,\N,Missing
P00-1056,J93-2003,0,\N,Missing
P00-1056,H93-1039,0,\N,Missing
P00-1056,C96-2141,1,\N,Missing
P01-1027,J93-2003,0,\N,Missing
P01-1027,C00-2123,1,\N,Missing
P01-1027,E99-1010,1,\N,Missing
P01-1027,J96-1002,0,\N,Missing
P01-1027,P98-2158,0,\N,Missing
P01-1027,C98-2153,0,\N,Missing
P01-1027,W00-0707,0,\N,Missing
P01-1027,P97-1037,1,\N,Missing
P01-1027,H94-1028,0,\N,Missing
P01-1027,P00-1056,1,\N,Missing
P01-1027,P97-1047,0,\N,Missing
P01-1027,P00-1006,0,\N,Missing
P02-1038,J96-1002,0,0.0948631,"observe that comparable results are obtained by using the following decision rule (6) eI1 Here, we replaced pθˆ(f1J |eI1 ) by pθˆ(eI1 |f1J ). From a theoretical framework of the sourcechannel approach, this approach is hard to justify. Yet, if both decision rules yield the same translation quality, we can use that decision rule which is better suited for efficient search. 1.2 Direct Maximum Entropy Translation Model As alternative to the source-channel approach, we directly model the posterior probability P r(eI1 |f1J ). An especially well-founded framework for doing this is maximum entropy (Berger et al., 1996). In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given Source Language Text  Preprocessing  o λ1 · h1 (eI1 , f1J ) o λ2 · h2 (eI1 , f1J ) Global Search argmax n P M m=1 eI1 o λm hm (eI1 , f1J ) o ...  Postprocessing  Target Language Text Figure 2: Architecture of the translation approach based on direct maximum entropy models. the following two feature functions: by: P r(eI1 |f1J ) = pλM (eI1 |f1J ) 1 PM exp[ m=1 λm hm (eI1 , f"
P02-1038,J93-2003,0,0.0622118,"(1) eI1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. 1 (2) eI1 Introduction eˆI1 = argmax {P r(eI1 |f1J )} Source-Channel Model The notational convention will be as follows. We use the symbol P r(·) to denote general probability distributions with (nearly) no specific assumptions. In contrast, for model-based probability distributions, we use the generic symbol p(·). This approach is referred to as source-channel approach to statistical MT. Sometimes, it is also referred to as the ‘fundamental equation of statistical MT’ (Brown et al., 1993). Here, P r(eI1 ) is the language model of the target language, whereas P r(f1J |eI1 ) is the translation model. Typically, Eq. 2 is favored over the direct translation model of Eq. 1 with the argument that it yields a modular approach. Instead of modeling one probability distribution, we obtain two different knowledge sources that are trained independently. The overall architecture of the source-channel approach is summarized in Figure 1. In general, as shown in this figure, there may be additional transformations to make the translation task simpler for the algorithm. Typically, training is"
P02-1038,niessen-etal-2000-evaluation,1,0.314719,"ord order of an acceptable sentence can be different from that of the target sentence, so that the WER measure alone could be misleading. To overcome this problem, we introduce as additional measure the position-independent word error rate (PER). This measure compares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, there is not only used a single reference translation, as for the WER, but a whole set of reference translations. For each translation hypothesis, the edit distance to the most similar sentence is calculated (Nießen et al., 2000). • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001). Unlike all other evaluation criteria used here, BLEU measures accuracy, i.e. the opposite of error rate. Hence, large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (Nießen et al., 2000)."
P02-1038,W99-0604,1,0.825262,"nd the translation model P r(f1J |eI1 ) = pθ (f1J |eI1 ) depends on parameters θ, then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus f S1 , eS1 (Brown et al., 1993): θˆ = argmax θ γˆ = argmax γ S Y s=1 S Y s=1 pθ (fs |es ) (3) pγ (es ) (4) Source Language Text  Preprocessing  o P r(eI1 ): Language Model Global Search eˆI1 = argmax {P r(eI1 ) · P r(f1J |eI1 )} eI1 o P r(f1J |eI1 ): Translation Model  Postprocessing  Target Language Text Figure 1: Architecture of the translation approach based on source-channel models. instead of Eq. 5 (Och et al., 1999): We obtain the following decision rule: eˆI1 = argmax{pγˆ (eI1 ) · pθˆ(f1J |eI1 )} (5) eˆI1 = argmax{pγˆ (eI1 ) · pθˆ(eI1 |f1J )} eI1 State-of-the-art statistical MT systems are based on this approach. Yet, the use of this decision rule has various problems: 1. The combination of the language model pγˆ (eI1 ) and the translation model pθˆ(f1J |eI1 ) as shown in Eq. 5 can only be shown to be optimal if the true probability distributions pγˆ (eI1 ) = P r(eI1 ) and pθˆ(f1J |eI1 ) = P r(f1J |eI1 ) are used. Yet, we know that the used models and training methods provide only poor approximations of"
P02-1038,2001.mtsummit-papers.68,0,0.0600561,"on-independent word error rate (PER). This measure compares the words in the two sentences ignoring the word order. • mWER (multi-reference word error rate): For each test sentence, there is not only used a single reference translation, as for the WER, but a whole set of reference translations. For each translation hypothesis, the edit distance to the most similar sentence is calculated (Nießen et al., 2000). • BLEU score: This score measures the precision of unigrams, bigrams, trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences (Papineni et al., 2001). Unlike all other evaluation criteria used here, BLEU measures accuracy, i.e. the opposite of error rate. Hence, large BLEU scores are better. • SSER (subjective sentence error rate): For a more detailed analysis, subjective judgments by test persons are necessary. Each translated sentence was judged by a human examiner according to an error scale from 0.0 to 1.0 (Nießen et al., 2000). • IER (information item error rate): The test sentences are segmented into information items. For each of them, if the intended information is conveyed and there are no syntactic errors, the sentence is counted"
P02-1038,1993.mtsummit-1.11,0,0.0929456,"the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence. To solve this problem, we define for maximum entropy training each sentence as reference translation that has the minimal number of word errors with respect to any of the reference translations. 5 Results We present results on the V ERBMOBIL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993). Table 1 shows the corpus statistics of this task. We use a training corpus, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus. Table 1: Characteristics of training corpus (Train), manual lexicon (Lex), development corpus (Dev), test corpus (Test). Train Lex Dev Test Sentences Words Singletons Vocabulary Entries Ext. Vocab. Sentences Words PP (trigr. LM) Sentences Words PP (trigr. LM) German English 58 073 519 523 549 921 3 453 1 698 7 939 4 672 12 779 11 501 6 867 276 3 159"
P02-1038,C00-2163,1,\N,Missing
P02-1038,P02-1040,0,\N,Missing
P03-1021,niessen-etal-2000-evaluation,1,0.199072,"Missing"
P03-1021,P02-1038,1,0.154986,"the  complete set of interval boundaries and error count changes on the whole corpus are obtained. The optimal can now be computed easily by traversing the sequence of interval boundaries while updating an error count. It is straightforward to refine this algorithm to also handle the BLEU and NIST scores instead of sentence-level error counts by accumulating the relevant statistics for computing these scores (n-gram precision, translation length and reference length) .  6 Baseline Translation Approach The basic feature functions of our model are identical to the alignment template approach (Och and Ney, 2002). In this translation model, a sentence is translated by segmenting the input sentence into phrases, translating these phrases and reordering the translations in the target language. In addition to the feature functions described in (Och and Ney, 2002), our system includes a phrase penalty (the number of alignment templates used) and special alignment features. Altogether, the log-linear model includes   different features. Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabi"
P03-1021,W99-0604,1,0.125715,"lignment templates used) and special alignment features. Altogether, the log-linear model includes   different features. Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabilistic model. Therefore, the feature functions are much more ’informative’ than for instance the binary feature functions used in standard maximum entropy models in natural language processing. For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extract . best candidate translations using A* search (Ueffing et al., 2002). Using an . -best approximation, we might face the problem that the parameters trained are good for the list of . translations used, but yield worse translation results if these parameters are used in the dynamic programming search. Hence, it is possible that our new search produces translations with more errors on the training corpus. This can happen because with the modified model scaling factors the . -best list can change significantly and can include  sentences not in the existing . -best list. To avoid thi"
P03-1021,2001.mtsummit-papers.68,0,0.133217,"( ,+ that are the basis for our training procedure. In Section 7, we evaluate the different training criteria in the context of several MT experiments. 3 Automatic Assessment of Translation Quality In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations. Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002). All these criteria try to approximate human assessment and often achieve an astonishing degree of correlation to human subjective evaluation of fluency and adequacy (Papineni et al., 2001; Doddington, 2002). In this paper, we use the following methods: - multi-reference word error rate (mWER): (4) The optimization problem under this criterion has very nice properties: there is one unique global optimum, and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the global optimum. Yet, the ultimate goal is to obtain good translation qua"
P03-1021,W00-1401,0,0.108111,"Missing"
P03-1021,P96-1024,0,0.177903,"e model. A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq. 3. The loss function is either identical or closely related to the final evaluation criterion. In contrast to the approach presented in this paper, the training criterion and the statistical models used remain unchanged in the minimum Bayes risk approach. In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002). 9 Conclusions We presented alternative training criteria for loglinear statistical machine translation models which are directly related to translation quality: an unsmoothed error count and a smoothed error count on a development corpus. For the unsmoothed error count, we presented a new line optimization algorithm which can efficiently find the optimal solution along a line. We showed that this approach obtains significantly better results than using the MMI training criterion (with our method to define pseudoreferences) and that optimizing error"
P03-1021,W02-1021,1,0.129141,"linear model includes   different features. Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabilistic model. Therefore, the feature functions are much more ’informative’ than for instance the binary feature functions used in standard maximum entropy models in natural language processing. For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extract . best candidate translations using A* search (Ueffing et al., 2002). Using an . -best approximation, we might face the problem that the parameters trained are good for the list of . translations used, but yield worse translation results if these parameters are used in the dynamic programming search. Hence, it is possible that our new search produces translations with more errors on the training corpus. This can happen because with the modified model scaling factors the . -best list can change significantly and can include  sentences not in the existing . -best list. To avoid this problem, we adopt the following solution: First, we perform search (using a man"
P03-1021,C00-2163,1,\N,Missing
P03-1021,P02-1040,0,\N,Missing
P03-1021,W02-1019,0,\N,Missing
P04-1077,2001.mtsummit-papers.3,0,0.0346222,"rict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An"
P04-1077,N03-1003,0,0.0107407,"Missing"
P04-1077,2003.mtsummit-papers.32,0,0.0552346,"translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two r"
P04-1077,W04-1013,1,0.115126,"Missing"
P04-1077,C04-1072,1,0.629085,"Missing"
P04-1077,W95-0115,0,0.0222944,"Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while"
P04-1077,J82-2005,0,0.673361,"Missing"
P04-1077,niessen-etal-2000-evaluation,1,0.175187,"rds in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that"
P04-1077,2001.mtsummit-papers.68,0,0.134315,"ences. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). An n-gram co-occurrence measure, BLEU, proposed by Papineni et al. (2001) that calculates co-occurrence statistics based on n-gram overlaps have shown great potential. A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations. Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation. However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n &gt; 1) matches achieved similar per"
P04-1077,C02-1073,0,0.00991641,"men et al. 1989). Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length. We can find the LCS of two sequences of length m and n using standard dynamic programming technique in O(mn) time. LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text. Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. He used as an approximate string matching algorithm. Saggion et al. (2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation. NP-LCS can be shown as a special case of Equation (6) with β = 1. However, they did not provide the correlation analysis of NP-LCS with 1 This is a real machine translation output. The “kill” in S2 or S3 does not match with “killed” in S1 in strict word-to-word comparison. 2 human judgments and its effectiveness as an automatic evaluation measure. To apply LCS in machine translation evaluation, we view a translation as a sequence of words. The intuition is that the longer the L"
P04-1077,C92-2067,0,0.03926,"vel structure similarity naturally and identifies longest co-occurring insequence n-grams automatically. The second method relaxes strict n-gram matching to skipbigram matching. Skip-bigram is any pair of words in their sentence order. Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations. The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency. 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new. Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations. Akiba et al. (2001) extended the idea to accommodate multiple references. Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations. Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead. Instead of error measures, we can also use accuracy measures that compute similarity b"
P04-1077,N03-2021,0,\N,Missing
P04-1077,P02-1040,0,\N,Missing
P04-1077,N03-1020,1,\N,Missing
P09-1019,J07-2003,0,0.308769,"soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair H = hV, Ei consisting of a vertex set V and a set of hyperedges E ⊆ V ∗ × V. Each hyperedge e ∈ E connects a head vertex h(e) with a sequence of tail vertices T (e) = {v1 , ..., vn }. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arit"
P09-1019,C08-5001,0,0.0762935,"ew, CA 94043, USA {shankarkumar,wmach,och}@google.com Abstract number of translation alternatives relative to N best lists. The extension to lattices reduces the runtimes for both MERT and MBR, and gives performance improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MB"
P09-1019,N04-1022,1,0.945384,"nts from MBR decoding on several language pairs. 1 Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu Introduction Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher Rally World Championship its future in X1 X2 X1 X2 Suzuki X1 X2 X1 X2 soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Sin"
P09-1019,D08-1076,1,0.459184,"ce improvements from MBR decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algo"
P09-1019,P08-1023,0,0.0627745,"ill soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair H = hV, Ei consisting of a vertex set V and a set of hyperedges E ⊆ V ∗ × V. Each hyperedge e ∈ E connects a head vertex h(e) with a sequence of tail vertices T (e) = {v1 , ..., vn }. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice). Each hyperedge"
P09-1019,J04-4002,1,0.491531,"the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher Rally World Championship its future in X1 X2 X1 X2 Suzuki X1 X2 X1 X2 soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number o"
P09-1019,P03-1021,1,0.18736,"rom MERT and MBR as well as performance improvements from MBR decoding on several language pairs. 1 Department of Linguistics University of Maryland College Park, MD 20742, USA redpony@umd.edu Introduction Statistical Machine Translation (SMT) systems have improved considerably by directly using the error criterion in both training and decoding. By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric. Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004). These two techniques were originally developed for N -best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004). Translation lattices contain a significantly higher Rally World Championship its future in X1 X2 X1 X2 Suzuki X1 X2 X1 X2 soon X1 X2 X1 announces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL"
P09-1019,2001.mtsummit-papers.68,0,0.157746,"llmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects the hypothesis in the hypergraph with the maximum expected approximate corpus BLEU score (Tr"
P09-1019,D08-1065,1,0.301213,"R decoding. SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT. For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008). In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems. We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al. (2008). Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice. We employ MERT to select these weights by optimizing BLEU score on a development set. A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008). In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation. In contrast, our MBR algorithm directly selects"
P09-1019,P08-1025,0,0.0280132,"Missing"
P09-1019,W06-3119,0,0.0621183,"nnounces X1 its future in the will soon announce X1 its future in the Figure 1: An example hypergraph. 163 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163–171, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP 2 Translation Hypergraphs n o M > M ˆ ; γ) = arg max (λM E(F 1 + γ · d1 ) · h1 (E, F ) E∈C ff X X dm hm (E, F ) = arg max λm hm (E, F ) + γ · A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system. The corresponding representation for an SMT system based on SCFGs (e.g. Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008). Formally, a hypergraph is a pair H = hV, Ei consisting of a vertex set V and a set of hyperedges E ⊆ V ∗ × V. Each hyperedge e ∈ E connects a head vertex h(e) with a sequence of tail vertices T (e) = {v1 , ..., vn }. The number of tail vertices is called the arity (|e|) of the hyperedge. If the arity of a hyperedge is zero, h(e) is called a source vertex. The arity of a hypergraph is the maximum arity of its hyperedges. A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice"
P09-1019,D08-1022,0,\N,Missing
P09-1019,P02-1040,0,\N,Missing
P09-1019,P06-1121,0,\N,Missing
P11-1140,2002.tmi-papers.3,0,0.743505,"we aim to solve the following problems: • Define a cost function that favors valid compound parts and rejects invalid ones. • Learn morphological operations, which is important for languages that have complex compound forming processes. • Apply compound splitting to machine translation to aid in translation of compounds that have not been seen in the bilingual training data. • Avoid splitting non-compounds and named entities as this may result in wrong translations. 2 Related work Previous work concerning decompounding can be divided into two categories: monolingual and bilingual approaches. Brown (2002) describes a corpus-driven approach for splitting compounds in a German-English translation task derived from a medical domain. A large proportion of the tokens in both texts are cognates 1397 with a Latin or Greek etymological origin. While the English text keeps the cognates as separate tokens, they are combined into compounds in the German text. To split these compounds, the author compares both the German and the English cognates on a character level to find reasonable split points. The algorithm described by the author consists of a sequence of if-then-else conditions that are applied on"
P11-1140,N09-1046,0,0.0808145,"the 1 However, the glue characters found by this procedure seem to be biased for at least German and Albanian. A very frequent glue morpheme like -es- is not listed, while glue morphemes like -k- and -h- rank very high, although they are invalid glue morphemes for German. Albanian shows similar problems. authors look for compound parts that occur in different anchor texts pointing to the same document. All these signals are combined and the weights are trained using a support vector machine classifier. Alfonseca et al. (2008a) apply this compound splitter on various other Germanic languages. Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. To train the model, reference segmentations are required. Here, we produce only single best segmentations, but otherwise do not rely on reference segmentations. 3 In this section, we describe the underlying optimization problem and the algorithm used to split a token into its compound parts. Starting from Bayes&apos; decision rule, we develop the Bellman equation and formulate a dynamic programming-based algorithm that takes a word as input and outputs the constituen"
P11-1140,I08-1053,0,0.381335,"ied to split more complex compounds. Koehn and Knight (2003) present a frequencybased approach to compound splitting for German. The compound parts and their frequencies are estimated from a monolingual corpus. As an extension to the frequency approach, the authors describe a bilingual approach where they use a dictionary extracted from parallel data to find better split options. The authors allow only two linking morphemes between compound parts and a few letters that can be dropped. In contrast to our approach, those operations are not learned automatically, but must be provided explicitly. Garera and Yarowsky (2008) propose an approach to translate compounds without the need for bilingual training texts. The compound splitting procedure mainly follows the approach from (Brown, 2002) and (Koehn and Knight, 2003), so the emphasis is put on finding correct translations for compounds. To accomplish this, the authors use crosslanguage compound evidence obtained from bilingual dictionaries. In addition, the authors describe a simple way to learn glue characters by allowing the deletion of up to two middle and two end characters.1 More complex morphological operations are not taken into account. Alfonseca et al"
P11-1140,E03-1076,0,0.804974,"eeps the cognates as separate tokens, they are combined into compounds in the German text. To split these compounds, the author compares both the German and the English cognates on a character level to find reasonable split points. The algorithm described by the author consists of a sequence of if-then-else conditions that are applied on the two cognates to find the split points. Furthermore, since the method relies on finding similar character sequences between both the source and the target tokens, the approach is restricted to cognates and cannot be applied to split more complex compounds. Koehn and Knight (2003) present a frequencybased approach to compound splitting for German. The compound parts and their frequencies are estimated from a monolingual corpus. As an extension to the frequency approach, the authors describe a bilingual approach where they use a dictionary extracted from parallel data to find better split options. The authors allow only two linking morphemes between compound parts and a few letters that can be dropped. In contrast to our approach, those operations are not learned automatically, but must be provided explicitly. Garera and Yarowsky (2008) propose an approach to translate"
P11-1140,P07-2045,0,0.00483418,"Missing"
P11-1140,P02-1040,0,0.116106,"nds. Unfortunately, such lists do not exist for many languages. While the training algorithm for our compound splitter shall be unsupervised, the evaluation data needs to be verified by human experts. Since we are interested in improving machine translation and to circumvent the problem of explicitly annotating compounds, we evaluate the compound splitter within a machine translation task. By decompounding training and test data of a machine translation system, we expect an increase in the number of matching phrase table entries, resulting in better translation quality measured in BLEU score (Papineni et al., 2002). If BLEU score is sensitive enough to measure the quality improvements obtained from decompounding, there is no need to generate a separate gold standard for compounds. Finally, we do not want to split non-compounds and named entities because we expect them to be translated non-compositionally. For example, the German word Deutschland (Germany) could be split into two parts Deutsch (German) + Land (country). Although this is a valid split, named entities should be kept as single units. An example for a non-compound is the German participle vereinbart (agreed) which could be wrongly split into"
P11-1140,P08-2064,0,\N,Missing
P98-2162,J93-2003,0,0.017871,"Missing"
P98-2162,1995.tmi-1.18,0,0.207033,"a p e r m u t a t i o n of the numbers 1 , . . . , L. 5 Learning of Category Systems During the last decade some publications have discussed the problem of learning WCs using clustering techniques based on m a x i m u m likelihood criteria applied to single language corpora. The question which we pose in addition is: Which WCs are suitable for translation? It seems to make sense to require that the used WCs in the two languages are correlated, so that the information about the class of a SL word gives much information about the class of the generated TL word. Therefore it has been argued in (Fung and Wu, 1995) that independently generated WCs are not good for the use in translation. For the automatic generation of class systems exists a well known procedure (see (Kneser and Ney, 1993), (Och, 1995)) which maximizes the perplexity of the language model for a training corpus by moving one word from a class to another in an iterative procedure. The function ML(CINw_~w, ) which has to be optimized depends only on the count function Nw~w, which counts the frequency that the word w&apos; comes after the word w. Using two sets of WCs for the TL and SL which are independent (method INDEP) does not guarantee that"
P98-2162,C96-2141,0,0.0704451,"or the probability that ej is generated by di. The assumption that each SL word influences every TL word with the same strength appears to be too simple. In the refined model 2 (Brown et al., 1993) alignment probabilities a(ilj , l, m) are included to model the effect that the position of a word influences the position of its translation. The phrasal organization of natural languages is well known and has been described by (Jackendorff, 1977) among many others. The traditional alignment probabilities depend on absolute positions and do not take that into account, as has already been noted by (Vogel et al., 1996). Therefore we developed a kind of relative weighting probability. The following model - - which we will call the model 2~ - makes the weight between the words di and ej dependent on the relative distances between the words dk which generated the previous word ~ ~ tmontag Figure 1: Alignment example. els can be described in the form m ej-1 : l P(eld) """" 1-I ~ l s(i]j, ej_z,d) ~ ~ d ( i - k]l).t(ej_z]dk) (3) k=0 Here d(i - kll ) is the probability that word di influences a word ej if the previous word ej-1 is influenced by dk. As an effect of such a weight a (phrase-)cluster of words being move"
W01-1408,J93-2003,0,0.117121,"language into a target language. We are given a source string f1J = f1 ...fj ...fJ , which is to be translated into a target string eI1 = e1 ...ei ...eI . Among all possible target strings, we will choose the string with the highest probability: n o eˆI1 = arg max P r(e1J |f1I ) eI1   ( X n o = arg max P r(e1I ) · P r(f1J |eI1 ) eI1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. P r(eI1 ) is the language model of the target language, whereas P r(f1J |e1I ) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) e1I aJ 1 The search space consists of the set of all possible target language strings e1I and all possible alignments aJ1 . 2 IBM Model 4 Various statistical alignment models of the form P r(f1J , aJ1 |eI1 ) have been introduced in (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000a). In this paper we use the so-called Model 4 from (Brown et al., 1993). In Model 4 the statistical alignment model is decomposed into five sub-models: • the lexicon model p(f |e) for the probability that the source word f is a translation of the target word e, • the"
W01-1408,C00-2163,1,0.573158,"given a source string f1J = f1 ...fj ...fJ , which is to be translated into a target string eI1 = e1 ...ei ...eI . Among all possible target strings, we will choose the string with the highest probability: n o eˆI1 = arg max P r(e1J |f1I ) eI1   ( X n o = arg max P r(e1I ) · P r(f1J |eI1 ) eI1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. P r(eI1 ) is the language model of the target language, whereas P r(f1J |e1I ) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) e1I aJ 1 The search space consists of the set of all possible target language strings e1I and all possible alignments aJ1 . 2 IBM Model 4 Various statistical alignment models of the form P r(f1J , aJ1 |eI1 ) have been introduced in (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000a). In this paper we use the so-called Model 4 from (Brown et al., 1993). In Model 4 the statistical alignment model is decomposed into five sub-models: • the lexicon model p(f |e) for the probability that the source word f is a translation of the target word e, • the distortion model p=1 (j−j 0 |C(fj ), E"
W01-1408,P00-1056,1,0.47088,"given a source string f1J = f1 ...fj ...fJ , which is to be translated into a target string eI1 = e1 ...ei ...eI . Among all possible target strings, we will choose the string with the highest probability: n o eˆI1 = arg max P r(e1J |f1I ) eI1   ( X n o = arg max P r(e1I ) · P r(f1J |eI1 ) eI1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. P r(eI1 ) is the language model of the target language, whereas P r(f1J |e1I ) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) e1I aJ 1 The search space consists of the set of all possible target language strings e1I and all possible alignments aJ1 . 2 IBM Model 4 Various statistical alignment models of the form P r(f1J , aJ1 |eI1 ) have been introduced in (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000a). In this paper we use the so-called Model 4 from (Brown et al., 1993). In Model 4 the statistical alignment model is decomposed into five sub-models: • the lexicon model p(f |e) for the probability that the source word f is a translation of the target word e, • the distortion model p=1 (j−j 0 |C(fj ), E"
W01-1408,C00-2123,1,0.825236,"rty that if the count of the bigram N (u, v) = 0, then the probability P (w|u, v) depends only on v. In this case the recombination can be significantly improved by recombining all nodes whose language model state has the property N (u, v) = 0 only with respect to v. Obviously, this could be generalized to other types of language models as well. Experiments have shown that by using this efficient recombination, the number of needed hypotheses can be reduced by about a factor of 4. Search algorithms We evaluate the following two search algorithms: • beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner. The search algorithm is based on a dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses. For more details see (Tillmann, 2001). • A* search algorithm: In A*, all search hypotheses are managed in a priority queue. The basic A* search (Nilsson, 1971) can be described as follows: 1. initialize priority queue with an empty hypothesis 2. remove the hypothesis with the highest score from the priority queue 3. if this hypothesis is a goal hypothesis: outpu"
W01-1408,C96-2141,1,0.914441,"et language. We are given a source string f1J = f1 ...fj ...fJ , which is to be translated into a target string eI1 = e1 ...ei ...eI . Among all possible target strings, we will choose the string with the highest probability: n o eˆI1 = arg max P r(e1J |f1I ) eI1   ( X n o = arg max P r(e1I ) · P r(f1J |eI1 ) eI1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. P r(eI1 ) is the language model of the target language, whereas P r(f1J |e1I ) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) e1I aJ 1 The search space consists of the set of all possible target language strings e1I and all possible alignments aJ1 . 2 IBM Model 4 Various statistical alignment models of the form P r(f1J , aJ1 |eI1 ) have been introduced in (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000a). In this paper we use the so-called Model 4 from (Brown et al., 1993). In Model 4 the statistical alignment model is decomposed into five sub-models: • the lexicon model p(f |e) for the probability that the source word f is a translation of the target word e, • the distortion model p="
W01-1408,P97-1047,0,0.712311,"extensions of this hypothesis and put the extensions to the queue 5. goto 2 The so-called heuristic function estimates the probability of a completion of a partial hypothesis. This function is called admissible if it never underestimates this probability. Thus, admissible heuristic functions are always optimistic. The A* search algorithm corresponds to the Dijkstra algorithm if the heuristic function is equal to zero. 4 Admissible heuristic function In order to perform an efficient search with the A* search algorithm it is crucial to use a good heuristic function. We only know of the work by (Wang and Waibel, 1997) dealing with heuristic functions for search in statistical machine translation. They developed a simple heuristic function for Model 2 from (Brown et al., 1993) which was non admissible. In the following we develop a guaranteed admissible heuristic function for Model 4 taking into account distortion probabilities and the coupling of lexicon, fertility, and language model probabilities. The basic idea for developing a heuristic function for the alignment models is the fact that all source sentence positions which have not been covered so far still have to be translated in order to complete the"
W01-1408,J03-1005,1,\N,Missing
W02-1021,W98-1426,0,\N,Missing
W02-1021,J93-2003,0,\N,Missing
W02-1021,C00-2123,1,\N,Missing
W02-1021,P95-1034,0,\N,Missing
W02-1021,W01-1408,1,\N,Missing
W03-0420,J96-1002,0,0.00871905,"nizations (ORG), and names of miscellaneous entities (MISC) that do not belong to the previous three groups, e.g. [PER Clinton] ’s [ORG Ballybunion] fans invited to [LOC Chicago] . Additionally, the task requires the processing of two different languages from which only English was specified before the submission deadline. Therefore, the $'&)(   *   , 9 ;*  ) 1  <   , $ ' ) & ( :  9 m=?>A@!B   *  221130 <  2/71600 , (  : DC  A well-founded framework for directly modeling the 21   /70 posterior probability   *  2160 <  2160 , is maximum en( C tropy (Berger et al., 1996). In this framework, we have 21   /70 a set of E feature functions F3G  2160 <   <  2160 , <IH  J < K L < E . For each feature function ( F3G , there exists a model parameter M G . The posterior probability can then be modeled as follows: - Digits and numbers: ASCII digit strings and number expressions activate these features. Input Sequence   Preprocessing Global Search         - Pre- and suffixes: If the prefix (suffix) of 8 equals a given prefix (suffix), these features will fire.  &  -,' 0 2134   #   &(')  ) + &('. - ,' 0 ' 13 '  &"
W03-0420,M98-1018,0,0.0265655,"Missing"
W03-1209,P97-1062,0,0.0280039,"Missing"
W03-1209,P02-1054,0,0.0636082,"Missing"
W11-2102,W10-1749,0,0.540731,", particularly with respect to reordering, have long been recognized (Callison-burch and Osborne, 2006). Reordering has also been identified as a major factor in determining the difficulty of statistical machine translation between two languages (Birch et al., 2008) hence BLEU scores may be most unreliable precisely for those language pairs for which statistical machine translation is most difficult (Isozaki et al., 2010). There have been many results showing that metrics that account for reordering are better correlated with human judgements of translation quality (Lavie and Denkowski, 2009; Birch and Osborne, 2010; Isozaki et al., 2010). Examples given in Isozaki et al. (2010) where object and subject arguments are reversed in a Japanese to English statistical machine translation system demonstrate how damaging reordering errors can be and it should therefore not come as a surprise that word order is a strong predictor of translation quality; however, there are other advantages to be gained by focusing on this specific aspect of the translation process in isolation. One problem for all automatic evaluation metrics is that multiple equally good translations can be constructed for most input sentences an"
W11-2102,D08-1078,0,0.199539,"we describe ways in which the framework has facilitated development of the reordering components in our system. 13 2 2.1 Related Work Evaluating Reordering The ability to automatically evaluate machine translation output has driven progress in statistical machine translation; however, shortcomings of the dominant metric, BLEU (Papineni et al., 2001) , particularly with respect to reordering, have long been recognized (Callison-burch and Osborne, 2006). Reordering has also been identified as a major factor in determining the difficulty of statistical machine translation between two languages (Birch et al., 2008) hence BLEU scores may be most unreliable precisely for those language pairs for which statistical machine translation is most difficult (Isozaki et al., 2010). There have been many results showing that metrics that account for reordering are better correlated with human judgements of translation quality (Lavie and Denkowski, 2009; Birch and Osborne, 2010; Isozaki et al., 2010). Examples given in Isozaki et al. (2010) where object and subject arguments are reversed in a Japanese to English statistical machine translation system demonstrate how damaging reordering errors can be and it should th"
W11-2102,W09-0434,0,0.0215431,"of the whole system. Following Collins et al. (2005a) and Wang (2007), Xu et al. (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as 12 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12–21, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics a preprocessing step that manipulates a source sentence parse tree can significantly outperform stateof-the-art phrase-based and hierarchical machine translation systems. This result is corroborated by Birch et al. (2009) whose results suggest that both phrase-based and hierarchical translation systems fail to capture long-distance reordering phenomena. In this paper we describe a lightweight framework for measuring the quality of the reordering components in a machine translation system. While our framework can be applied to any translation system in which it is possible to derive a token-level alignment from the input source tokens to the output target tokens, it is of particular practical interest when applied to a system that performs reordering as a preprocessing step (Xia and McCord, 2004). In this case,"
W11-2102,E06-1032,0,0.0130735,"uate the framework by analyzing how robustly it is able to predict improvements in subjective translation quality for an English to Japanese machine translation system. Finally, we describe ways in which the framework has facilitated development of the reordering components in our system. 13 2 2.1 Related Work Evaluating Reordering The ability to automatically evaluate machine translation output has driven progress in statistical machine translation; however, shortcomings of the dominant metric, BLEU (Papineni et al., 2001) , particularly with respect to reordering, have long been recognized (Callison-burch and Osborne, 2006). Reordering has also been identified as a major factor in determining the difficulty of statistical machine translation between two languages (Birch et al., 2008) hence BLEU scores may be most unreliable precisely for those language pairs for which statistical machine translation is most difficult (Isozaki et al., 2010). There have been many results showing that metrics that account for reordering are better correlated with human judgements of translation quality (Lavie and Denkowski, 2009; Birch and Osborne, 2010; Isozaki et al., 2010). Examples given in Isozaki et al. (2010) where object an"
W11-2102,P05-1066,0,0.770579,"al., 2001) and other metrics that work with the final output of a machine translation system are both insensitive to reordering phenomena and relatively time-consuming to compute: changes to the system may require the realignment of the parallel training data, extraction of phrasal statistics and translation of a test set. As training sets grow in size, the cost of end-to-end experimentation can become significant. However, it is not clear that measurements made on any single part of the system will correlate well with human judgments of the translation quality of the whole system. Following Collins et al. (2005a) and Wang (2007), Xu et al. (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as 12 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12–21, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics a preprocessing step that manipulates a source sentence parse tree can significantly outperform stateof-the-art phrase-based and hierarchical machine translation systems. This result is corroborated by Birch et al. (2009) whose results suggest that both"
W11-2102,W02-1039,0,0.0128999,"n the other hand penalizes every pair of words that are in the wrong order and hence has a quadratic (all-pairs) flavor which in turn might explain why Birch et al. (2010) found that the squareroot of this quantity was a better predictor of translation quality. 2.2 Evaluation Reference Data To create the word-aligned translations from which we generate our reference reordering data, we used a novel alignment-oriented translation method. The method (described in more detail below) seeks to generate reference reorderings that a machine translation system might reasonably be expected to achieve. Fox (2002) has analyzed the extent to which translations seen in a parallel corpus can be broken down into clean phrasal units: they found that most sentence pairs contain examples of reordering that violate phrasal cohesion, i.e. the corresponding words in the target language are not completely contiguous or solely aligned to the corresponding source phrase. These reordering phenomena are difficult for current statistical translation models to learn directly. We therefore deliberately chose to create reference data that avoids these phenomena as much as possible by having a single annotator generate bo"
W11-2102,J07-3002,0,0.00943984,"rate the experimental cycle. In a large statistical machine translation system, we should ideally be able to experiment with separate components without retraining the complete system. Measures such as perplexity have been successfully used to evaluate language models independently in speech recognition eliminating some of the need for end-to-end speech recognition experiments. In machine translation, alignment error rate has been used with some mixed success to evaluate word-alignment algorithms but no standard evaluation frameworks exist for other components of a machine translation system (Fraser and Marcu, 2007). Unfortunately, BLEU (Papineni et al., 2001) and other metrics that work with the final output of a machine translation system are both insensitive to reordering phenomena and relatively time-consuming to compute: changes to the system may require the realignment of the parallel training data, extraction of phrasal statistics and translation of a test set. As training sets grow in size, the cost of end-to-end experimentation can become significant. However, it is not clear that measurements made on any single part of the system will correlate well with human judgments of the translation quali"
W11-2102,D11-1017,1,0.693787,"head category (a coarse-grained part of speech). For example, a simplified version of the precedence order for child labels of a verbal head HEADV ERB is: advcl, nsubj, prep, [other children], dobj, prt, aux, neg, HEADV ERB, mark, ref, compl. The dependency parser we use is an implementation of a transition-based dependency parser (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). We created five systems using different parsers; here targeted self-training refers to a training procedure proposed by Katz-Brown et al. (2011) that uses our reordering metric and separate reference reordering data to pick parses for self-training: an nbest list of parses is generated for each English sentence for which we have reference reordering data and the parse tree that results in the highest fuzzy reordering score is added to our parser’s training set. Parsers P3, P4 and P5 differ in how that framework is applied and how much data is used. • P1 Penn Treebank, perceptron, greedy search • P2 Penn Treebank, perceptron, beam search • P3 Penn Treebank, perceptron, beam search, targeted self-training on web data • P4 Penn Treebank,"
W11-2102,W04-3250,0,0.0252875,"dering rules. In each case three bilingual evaluators were shown the source sentence and the translations produced by all five systems. 4.2 Meta-analysis We perform a meta-analysis of the following metrics and the framework by computing correlations with the results of these subjective evaluations of translation quality: 17 The automatic word alignments were generated using IBM Model 1 in order to avoid directional biases that higher-order models such as HMMs have. Results presented in square parentheses are 95 percent confidence intervals estimated by bootstrap resampling on the test corpus (Koehn, 2004). Our test set contains 500 sentences randomly sampled from the web. We have both professional and alignment-friendly translations for these sentences. We created reference reorderings for this data using the method described in Section 3.1. The lack of a broad domain and publically available Japanese test corpus makes the use of this nonstandard test set unfortunately unavoidable. The human raters were presented with the source sentence, the human reference translation and the translations of the various systems simultaneously, blind and in a random order. Each rater was allowed to rate no mo"
W11-2102,J08-4003,0,0.0128647,"l portion coming from the web after using simple heuristics to identify potential document pairs. We trained our system on about 300 million source words. The reordering rules applied to the English dependency tree define a precedence order for the children of each head category (a coarse-grained part of speech). For example, a simplified version of the precedence order for child labels of a verbal head HEADV ERB is: advcl, nsubj, prep, [other children], dobj, prt, aux, neg, HEADV ERB, mark, ref, compl. The dependency parser we use is an implementation of a transition-based dependency parser (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). We created five systems using different parsers; here targeted self-training refers to a training procedure proposed by Katz-Brown et al. (2011) that uses our reordering metric and separate reference reordering data to pick parses for self-training: an nbest list of parses is generated for each English sentence for which we have reference reordering data and the parse tree that results in the highest fuzzy reordering score is added to our parser’s training set."
W11-2102,P03-1021,1,0.0121967,"t al., 2010). There has also been a tendency to measure corpus-level correlation. We are more interested in comparing systems that differ in a realistic manner from one another as would typically be required in development. We also believe sentence-level correlation is more important than corpus-level correlation since good sentence-level correlation implies that a metric can be used for detailed analysis of a system and potentially to optimize it. 16 4.1 Systems We carried out all our experiments using a state-ofthe-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al., 2009) that reorder an English dependency tree into target word order. During decoding, we set the reordering window to 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). For parallel training data, we use an in-house collection of parallel documents. These come from various sources with a substantial portion comin"
W11-2102,2001.mtsummit-papers.68,0,0.259138,"tical machine translation system, we should ideally be able to experiment with separate components without retraining the complete system. Measures such as perplexity have been successfully used to evaluate language models independently in speech recognition eliminating some of the need for end-to-end speech recognition experiments. In machine translation, alignment error rate has been used with some mixed success to evaluate word-alignment algorithms but no standard evaluation frameworks exist for other components of a machine translation system (Fraser and Marcu, 2007). Unfortunately, BLEU (Papineni et al., 2001) and other metrics that work with the final output of a machine translation system are both insensitive to reordering phenomena and relatively time-consuming to compute: changes to the system may require the realignment of the parallel training data, extraction of phrasal statistics and translation of a test set. As training sets grow in size, the cost of end-to-end experimentation can become significant. However, it is not clear that measurements made on any single part of the system will correlate well with human judgments of the translation quality of the whole system. Following Collins et"
W11-2102,D07-1077,0,0.138205,"cs that work with the final output of a machine translation system are both insensitive to reordering phenomena and relatively time-consuming to compute: changes to the system may require the realignment of the parallel training data, extraction of phrasal statistics and translation of a test set. As training sets grow in size, the cost of end-to-end experimentation can become significant. However, it is not clear that measurements made on any single part of the system will correlate well with human judgments of the translation quality of the whole system. Following Collins et al. (2005a) and Wang (2007), Xu et al. (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as 12 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12–21, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics a preprocessing step that manipulates a source sentence parse tree can significantly outperform stateof-the-art phrase-based and hierarchical machine translation systems. This result is corroborated by Birch et al. (2009) whose results suggest that both phrase-based and h"
W11-2102,C04-1073,0,0.908301,"corroborated by Birch et al. (2009) whose results suggest that both phrase-based and hierarchical translation systems fail to capture long-distance reordering phenomena. In this paper we describe a lightweight framework for measuring the quality of the reordering components in a machine translation system. While our framework can be applied to any translation system in which it is possible to derive a token-level alignment from the input source tokens to the output target tokens, it is of particular practical interest when applied to a system that performs reordering as a preprocessing step (Xia and McCord, 2004). In this case, as we show, it allows for extremely rapid and sensitive analysis of changes to parser, reordering rules and other reordering components. In our framework we evaluate the reordering proposed by a system separately from its choice of target words by comparing it to a reference reordering of the sentence generated from a manually wordaligned translation. Unlike previous work (Isozaki et al., 2010), our approach does not rely on the system’s output matching the reference translation lexically. This makes the evaluation more robust as there may be many ways to render a source phrase"
W11-2102,N09-1028,1,0.785597,"with the final output of a machine translation system are both insensitive to reordering phenomena and relatively time-consuming to compute: changes to the system may require the realignment of the parallel training data, extraction of phrasal statistics and translation of a test set. As training sets grow in size, the cost of end-to-end experimentation can become significant. However, it is not clear that measurements made on any single part of the system will correlate well with human judgments of the translation quality of the whole system. Following Collins et al. (2005a) and Wang (2007), Xu et al. (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as 12 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 12–21, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics a preprocessing step that manipulates a source sentence parse tree can significantly outperform stateof-the-art phrase-based and hierarchical machine translation systems. This result is corroborated by Birch et al. (2009) whose results suggest that both phrase-based and hierarchical transl"
W11-2102,W06-3108,0,0.191293,"mize it. 16 4.1 Systems We carried out all our experiments using a state-ofthe-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al., 2009) that reorder an English dependency tree into target word order. During decoding, we set the reordering window to 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). For parallel training data, we use an in-house collection of parallel documents. These come from various sources with a substantial portion coming from the web after using simple heuristics to identify potential document pairs. We trained our system on about 300 million source words. The reordering rules applied to the English dependency tree define a precedence order for the children of each head category (a coarse-grained part of speech). For example, a simplified version of the precedence order for child labels of a verbal head HEADV ERB is: advcl, nsubj, prep, [other children], dobj, prt"
W11-2102,D08-1059,0,0.0192746,"about 300 million source words. The reordering rules applied to the English dependency tree define a precedence order for the children of each head category (a coarse-grained part of speech). For example, a simplified version of the precedence order for child labels of a verbal head HEADV ERB is: advcl, nsubj, prep, [other children], dobj, prt, aux, neg, HEADV ERB, mark, ref, compl. The dependency parser we use is an implementation of a transition-based dependency parser (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). We created five systems using different parsers; here targeted self-training refers to a training procedure proposed by Katz-Brown et al. (2011) that uses our reordering metric and separate reference reordering data to pick parses for self-training: an nbest list of parses is generated for each English sentence for which we have reference reordering data and the parse tree that results in the highest fuzzy reordering score is added to our parser’s training set. Parsers P3, P4 and P5 differ in how that framework is applied and how much data is used. • P1 Penn Treebank, perceptron, greedy sear"
W11-2102,D11-1138,1,\N,Missing
W11-2102,P02-1040,0,\N,Missing
W11-2102,D10-1092,0,\N,Missing
W99-0604,E99-1010,1,0.604945,"alignment A is represented as a matrix with binary values. A matrix element&quot; with value 1 means that the words at the corresponding positions are aligned and the value 0 means that the words are not aligned. If a source word is not aligned to a target word then it is aligned to the empty word e0 which shall be at the imaginary position i = 0. This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments. The classes used in F and E are automatically trained bilingual classes using the method described in (Och, 1999) and constitute a partition of the vocabulary of source and target language. The class functions .T and E map words to their classes. The use of classes instead of words themselves has the advantage of a better generalization. If there exist classes in source and target language which contain all towns it is possible that an alignment template learned using a special town can be generalized to all towns. In Fig. 2 an example of an alignment template is shown. An alignment template z = (F, E, A) is app(ilj; A) A(i,j) = EiA(i,j) (4) 3.2 The phrase level alignment In order to describe the phrase"
W99-0604,P97-1037,1,0.808341,"t given in some source language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)} e1 = argmax {Pr(e[). Pr(f/le~) } • (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(e{) is the language 20 model of the target language, whereas Pr (ff~lelI) is the translation model. Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words. The model is often further restricted that each source word is assigned exactly one target word. These alignment models are sireilar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j ~ i = aj from source position j to target position i = aj. The use of this alignment model raises major problems as it fails to capture dependencies between groups of words. As experiments have shown it is difficult to handle different word order and the"
W99-0604,C96-2141,1,0.916143,"translation of a text given in some source language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)} e1 = argmax {Pr(e[). Pr(f/le~) } • (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(e{) is the language 20 model of the target language, whereas Pr (ff~lelI) is the translation model. Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words. The model is often further restricted that each source word is assigned exactly one target word. These alignment models are sireilar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j ~ i = aj from source position j to target position i = aj. The use of this alignment model raises major problems as it fails to capture dependencies between groups of words. As experiments have shown it is difficult to handle diffe"
W99-0604,1993.mtsummit-1.11,0,0.062018,"Missing"
W99-0604,P98-2221,0,0.078569,"emented extensions allowing for one-to-many alignments. In section 3 we describe the alignment template approach which explicitly models shallow phrases and in doing so tries to overcome the above mentioned restrictions of singleword alignments. The described method is an improvement of (Och and Weber, 1998), resulting in an improved training and a faster search organization. The basic idea is to model two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words within these phrases. Similar aims are pursued by (Alshawi et al., 1998; Wang and Waibel, 1998) but differently approached. In section 4 we compare the two methods using the Verbmobil task. 2 Single-Word Based Approach new target word is generated. 2.1 B a s i c A p p r o a c h In this section, we shortly review a translation approach based on the so-called monotonicity requirement (Tillmann et al., 1997). Our aim is to provide a basis for comparing the two different translation approaches presented. In Eq. (1), Pr(e~) is the language model, which is a trigram language model in this case. For the translation model Pr(flJ[e{) we make the assumption that each source word is aligned to exa"
W99-0604,J93-2003,0,0.126301,"language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)} e1 = argmax {Pr(e[). Pr(f/le~) } • (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(e{) is the language 20 model of the target language, whereas Pr (ff~lelI) is the translation model. Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words. The model is often further restricted that each source word is assigned exactly one target word. These alignment models are sireilar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j ~ i = aj from source position j to target position i = aj. The use of this alignment model raises major problems as it fails to capture dependencies between groups of words. As experiments have shown it is difficult to handle different word order and the translation of compound nouns• In this pap"
W99-0604,P98-2158,0,0.0277594,"language into a target language. We are given a source string f / = fl...fj...fJ, which is to be translated into a target string e{ = el...ei...ex. Among all possible target strings, we will choose the string with the highest probability: = argmax {Pr(ezIlflJ)} e1 = argmax {Pr(e[). Pr(f/le~) } • (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(e{) is the language 20 model of the target language, whereas Pr (ff~lelI) is the translation model. Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words. The model is often further restricted that each source word is assigned exactly one target word. These alignment models are sireilar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is j ~ i = aj from source position j to target position i = aj. The use of this alignment model raises major problems as it fails to capture dependencies between groups of words. As experiments have shown it is difficult to handle different word order and the translation of compou"
W99-0604,C98-2153,0,\N,Missing
W99-0604,P98-1006,0,\N,Missing
W99-0604,C98-1006,0,\N,Missing
W99-0604,P98-2162,1,\N,Missing
W99-0604,C98-2157,1,\N,Missing
