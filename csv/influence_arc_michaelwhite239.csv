2007.mtsummit-ucnlg.4,W07-2305,1,0.849364,"Missing"
2007.mtsummit-ucnlg.4,P02-1041,0,0.0830803,"hite, 2006a) for further explanation. To better support broad coverage surface realization, we have enhanced OpenCCG for robustness in several ways. The most significant extension implemented so far has been a technique to improve robustness in the event that the realizer fails to find 2 Surface Realization with OpenCCG The OpenCCG open source surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a hybrid symbolicstatistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations makes use of n-gram language models over words represented as vectors of factors, including surface form, part of speech, supertag and semantic class. The search proceeds in one of two modes, anytime or two-stage (pa"
2007.mtsummit-ucnlg.4,E03-1036,0,0.039752,"alternative or optional elements, as illustrated in Figure 1; see (White, 2006a) for further explanation. To better support broad coverage surface realization, we have enhanced OpenCCG for robustness in several ways. The most significant extension implemented so far has been a technique to improve robustness in the event that the realizer fails to find 2 Surface Realization with OpenCCG The OpenCCG open source surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a hybrid symbolicstatistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations makes use of n-gram language models over words represented as vectors of factors, including surface form, part of speech, supertag and semantic clas"
2007.mtsummit-ucnlg.4,P96-1027,0,0.120042,"d OpenCCG for robustness in several ways. The most significant extension implemented so far has been a technique to improve robustness in the event that the realizer fails to find 2 Surface Realization with OpenCCG The OpenCCG open source surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a hybrid symbolicstatistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations makes use of n-gram language models over words represented as vectors of factors, including surface form, part of speech, supertag and semantic class. The search proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configura"
2007.mtsummit-ucnlg.4,W02-2103,0,0.825496,"from the CCGbank suitable for realization with OpenCCG involve adding semantic representations to the lexical categories and, where feasible, converting the corpus to reflect more precise analyses. While we are finding this process to be a time-consuming and nontrivial one, we expect our efforts to yield a linguistically informed and moderately precise, broad coverage English grammar—suitable for both parsing and realization—in much less time than it would take to scale up a manually written grammar to cover all the phenomena in the Penn Treebank. Our approach may be contrasted with those of (Langkilde-Geary, 2002; Callaway, 2003), who developed converters for the outputs of Treebank parsers to produce inputs for their realizers, rather than pursue parsers and realizers that share a bidirectional grammar. Instead, our approach is more similar to the ones pursued by (Carroll and Oepen, 2005; Nakanishi et al., 2005; Cahill and van Genabith, 2006) with HPSG and LFG grammars, except for our greater focus on Introduction In this paper, we report on progress towards developing the first broad coverage English surface realizer for Combinatory Categorial Grammar (Steedman, 2000, CCG), using a grammar engineere"
2007.mtsummit-ucnlg.4,C00-1007,0,0.740819,"Missing"
2007.mtsummit-ucnlg.4,A00-2023,0,0.349423,"Villeroy_and_Boch &lt;CREATOR&gt; Figure 1: Simplified disjunctive semantic dependency graph from the COMIC dialogue system, for the eight paraphrases The design (is|’s) based on (the Funny Day (collection|series) by Villeroy and Boch | Villeroy and Boch’s Funny Day (collection|series)). the agenda, and thus have an impact on realization speed. In the two-stage mode, a packed forest of all possible realizations is created in the first stage; in the second stage, the packed representation is unpacked in bottom-up fashion, with scores assigned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. At present, pruning is only done within equivalence classes of edges, and all lexical category assignments are considered for each input predicate, to ensure that pruning does not prevent a complete realization from being found. To realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enabling the selection of realizations acc"
2007.mtsummit-ucnlg.4,P06-4014,0,0.0221717,"Missing"
2007.mtsummit-ucnlg.4,W05-1510,0,0.754076,"d a linguistically informed and moderately precise, broad coverage English grammar—suitable for both parsing and realization—in much less time than it would take to scale up a manually written grammar to cover all the phenomena in the Penn Treebank. Our approach may be contrasted with those of (Langkilde-Geary, 2002; Callaway, 2003), who developed converters for the outputs of Treebank parsers to produce inputs for their realizers, rather than pursue parsers and realizers that share a bidirectional grammar. Instead, our approach is more similar to the ones pursued by (Carroll and Oepen, 2005; Nakanishi et al., 2005; Cahill and van Genabith, 2006) with HPSG and LFG grammars, except for our greater focus on Introduction In this paper, we report on progress towards developing the first broad coverage English surface realizer for Combinatory Categorial Grammar (Steedman, 2000, CCG), using a grammar engineered from the CCGbank (Hockenmaier, 2003)—a corpus of CCG derivations created by transforming the Penn Treebank—together with the OpenCCG (White, 2006b; White, 2006a) realizer, enhanced for robustness. In previous work, OpenCCG has been used with precise, manually developed grammars for dialogue systems. By"
2007.mtsummit-ucnlg.4,W07-0702,0,0.0544999,"Missing"
2007.mtsummit-ucnlg.4,P06-1140,1,0.835565,"asses when they have the same syntactic category and cover the same parts of the input logical form. At present, pruning is only done within equivalence classes of edges, and all lexical category assignments are considered for each input predicate, to ensure that pruning does not prevent a complete realization from being found. To realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (Nakatsu and White, 2006), and avoiding repetition in the output of a dialogue system (Foster and White, 2007). The disjunctive logical forms describe semantic dependency graphs with alternative or optional elements, as illustrated in Figure 1; see (White, 2006a) for further explanation. To better support broad coverage surface realization, we have enhanced OpenCCG for robustness in several ways. The most significant extension implemented so far has been a technique to improve robustness in the event that the realizer fails to find 2 Surface Realization with OpenCCG The OpenCCG open source surface realizer is based on"
2007.mtsummit-ucnlg.4,P06-1130,0,0.668479,"Missing"
2007.mtsummit-ucnlg.4,J05-1004,0,0.033795,"stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configurable time limit: the scores assigned by the ngram model determine the order of the edges on 23 fied frequency thresholds. Categories for function words that have no semantics or introduce only semantic features or relations are marked in an XSLT transformation. A separate transformation then uses around two dozen generalized templates to add logical forms to the categories. The effect of this transformation is illustrated below. Examples (1) and (2) show how numbered semantic roles, as in PropBank (Palmer et al., 2005), are added to the active and passive categories for a transitive verb, where *pred* is a placeholder for the lexical predicate; examples (3) and (4) show how more specific relations are introduced in the category for determiners and the category for the possessive ’s, respectively. a complete realization, in which case fragments are greedily assembled to cover as much of the input semantics as possible. The algorithm begins with the edge for the best partial realization, i.e. the one that covers the most elementary predications in the input logical form, with ties broken according to the n-gr"
2007.mtsummit-ucnlg.4,2001.mtsummit-papers.68,0,0.0329808,"terpolated supertag model showing a substantial increase over word-only trigram models. i−1 pW (F~i |F~i−2 ) = p(Wi |Wi−2 , Wi−1 ) i−1 P ~ ~ p (Fi |Fi−2 ) = p(Pi |Pi−2 , Pi−1 ) i−1 pS (F~i |F~i−2 ) = p(Si |Pi−2 , Pi−1 ) scoring model w3g + pos3g * stag3g word 3g + pos 3g word 3g, Kneser-Ney null w3g + pos3g * stag3g word 3g, Kneser-Ney 4.2 Results Tables 1 and 2 show initial results for the different scoring methods, using grammars extracted from the development section (non-blind) and training sections (blind), respectively. Results are given in terms of percentage of exact matches and BLEU (Papineni et al., 2001) n-gram precision scores using the corpus sentence as a reference. While realizations that exactly match the corpus sentence can be presumed to be of high quality, this metric is overly harsh in that it does not recognize acceptable variants. BLEU scores have been shown to correlate with human judgments of adequacy and fluency, but must be interpreted with caution, as discussed below. As the tables indicate, the best performing models interpolate the word-level trigram model with the chained part-of-speech and supertag trigram model. In Table 1, this model scores nearly four BLEU points1 bette"
2007.mtsummit-ucnlg.4,E06-1032,0,0.0407652,"findings were reported more than a year ago , the latest results appear in today ’s New England Journal of Medicine , a forum likely to bring new attention to the problem . 0.65 likely to bring new attention to the problem , today’s New England Journal of Medicine in a forum the latest results appear in although preliminary findings were reported more than a year ago . Table 4: Example outputs from PTB file wsj 0003 for the top scoring model in Table 2, where the simplified BLEU scores do not mirror relative quality. may no longer be useful in measuring progress, given the concerns raised by (Callison-Burch et al., 2006) and (Stent et al., 2005) discussed earlier, and especially our goal of producing desirable variation. As such, we expect that targeted human evaluations will become essential. In support of this view, we note that it is easy to find pairs of sentences whose n-gram precision scores are both fairly high where the difference in their scores does not mirror relative quality. For example, Table 4 shows two sentences from PTB file wsj 0003, along with the realizations generated using our top scoring model and the grammar derived from the training sections. The scores listed are calculated using a s"
2007.mtsummit-ucnlg.4,I05-1015,0,0.0296986,"xpect our efforts to yield a linguistically informed and moderately precise, broad coverage English grammar—suitable for both parsing and realization—in much less time than it would take to scale up a manually written grammar to cover all the phenomena in the Penn Treebank. Our approach may be contrasted with those of (Langkilde-Geary, 2002; Callaway, 2003), who developed converters for the outputs of Treebank parsers to produce inputs for their realizers, rather than pursue parsers and realizers that share a bidirectional grammar. Instead, our approach is more similar to the ones pursued by (Carroll and Oepen, 2005; Nakanishi et al., 2005; Cahill and van Genabith, 2006) with HPSG and LFG grammars, except for our greater focus on Introduction In this paper, we report on progress towards developing the first broad coverage English surface realizer for Combinatory Categorial Grammar (Steedman, 2000, CCG), using a grammar engineered from the CCGbank (Hockenmaier, 2003)—a corpus of CCG derivations created by transforming the Penn Treebank—together with the OpenCCG (White, 2006b; White, 2006a) realizer, enhanced for robustness. In previous work, OpenCCG has been used with precise, manually developed grammars"
2007.mtsummit-ucnlg.4,C04-1041,0,0.0315264,"odels, but 4-gram models showed no improvement. In the end, we experimented with two word-based models, a trigram model using GoodTuring smoothing (SRILM’s default method), and one using interpolated Kneser-Ney smoothing. Both models used the default frequency cutoffs. In addition to the usual word-based trigram models, we also created trigram models over partof-speech tags and supertags (using Kneser-Ney smoothing). In the latter model, the probability of the current word’s supertag (category label) is conditioned on the previous two part-of-speech tags, inspired by work on CCG supertagging (Clark and Curran, 2004; Curran et al., 2006). This is illustrated in (1)-(2). Equation (1) shows how the chain rule and the Markov assumption is used to approximate the probability of a sentence or phrase consisting of a sequence of factor vectors F~i , for words 1 i−1 i−1 to n. In (2), pW (F~i |F~i−2 ), pP (F~i |F~i−2 ) and i−1 S p (F~i |F~i−2 ) are defined to be approximations of i−1 ) that pay the conditional probability p(F~i |F~i−2 attention only to the word (W ), part-of-speech (P ) and supertag (S) factors of F~ , as indicated. 4 Evaluation In this section, we report initial results on the standard developme"
2007.mtsummit-ucnlg.4,2005.mtsummit-papers.15,0,0.359505,"provide initial automatic evaluation results, and discuss concerns about relying too heavily on automatic metrics for judging realization quality. In this section, we also contribute to the debate that has arisen recently over the relative merits of language models and syntactic features in realization ranking, by showing that factored language models that interpolate word-level n-grams with n-grams over partof-speech tags and supertags provide similar absolute performance improvements over word-level ngrams as have been observed with log-linear models using parsing-inspired features, as in (Velldal and Oepen, 2005). Finally, in Section 5 we conclude with a summary and discussion of future work. e be&lt;TENSE&gt;pres,&lt;MOOD&gt;dcl &lt;ARG&gt; design d &lt;DET&gt;the,&lt;NUM&gt;sg &lt;PROP&gt; p based_on &lt;SOURCE&gt; &lt;ARTIFACT&gt; &lt;HASPROP&gt; Funny_Day f c collection|series(&lt;DET&gt;the)?,&lt;NUM&gt;sg &lt;GENOWNER&gt; v Villeroy_and_Boch &lt;CREATOR&gt; Figure 1: Simplified disjunctive semantic dependency graph from the COMIC dialogue system, for the eight paraphrases The design (is|’s) based on (the Funny Day (collection|series) by Villeroy and Boch | Villeroy and Boch’s Funny Day (collection|series)). the agenda, and thus have an impact on realization speed. In the"
2007.mtsummit-ucnlg.4,P06-1088,0,0.0434693,"showed no improvement. In the end, we experimented with two word-based models, a trigram model using GoodTuring smoothing (SRILM’s default method), and one using interpolated Kneser-Ney smoothing. Both models used the default frequency cutoffs. In addition to the usual word-based trigram models, we also created trigram models over partof-speech tags and supertags (using Kneser-Ney smoothing). In the latter model, the probability of the current word’s supertag (category label) is conditioned on the previous two part-of-speech tags, inspired by work on CCG supertagging (Clark and Curran, 2004; Curran et al., 2006). This is illustrated in (1)-(2). Equation (1) shows how the chain rule and the Markov assumption is used to approximate the probability of a sentence or phrase consisting of a sequence of factor vectors F~i , for words 1 i−1 i−1 to n. In (2), pW (F~i |F~i−2 ), pP (F~i |F~i−2 ) and i−1 S p (F~i |F~i−2 ) are defined to be approximations of i−1 ) that pay the conditional probability p(F~i |F~i−2 attention only to the word (W ), part-of-speech (P ) and supertag (S) factors of F~ , as indicated. 4 Evaluation In this section, we report initial results on the standard development and test sections ("
2007.mtsummit-ucnlg.4,W06-1403,1,0.916595,"e parsers and realizers that share a bidirectional grammar. Instead, our approach is more similar to the ones pursued by (Carroll and Oepen, 2005; Nakanishi et al., 2005; Cahill and van Genabith, 2006) with HPSG and LFG grammars, except for our greater focus on Introduction In this paper, we report on progress towards developing the first broad coverage English surface realizer for Combinatory Categorial Grammar (Steedman, 2000, CCG), using a grammar engineered from the CCGbank (Hockenmaier, 2003)—a corpus of CCG derivations created by transforming the Penn Treebank—together with the OpenCCG (White, 2006b; White, 2006a) realizer, enhanced for robustness. In previous work, OpenCCG has been used with precise, manually developed grammars for dialogue systems. By developing a broad 22 engineering a broad coverage grammar where the logical forms more closely resemble those used traditionally in generation. The rest of the paper is organized as follows. In Section 2, we give an overview of surface realization with OpenCCG. In Section 3, we describe our approach to engineering a grammar from the CCGbank suitable for realization with OpenCCG. In Section 4, we describe our scoring methods, provide ini"
2012.amta-monomt.3,D08-1089,0,0.0256851,". This provides a better (uncased) match to one system’s output during tuning: the minister said , however , did not name any assistant organization. We apply this method both to the original references and to those that have been paraphrased by the method in Section 2. 4 Improved Parameter Optimization in Phrase-Based MT We use both paraphrase methods described above to test the effects of paraphrasing on MT tuning performance in an Urdu-English translation task. We train phrase-based systems using the Moses toolkit (Koehn et al., 2007). All systems use “hierarchical” lexicalized reordering (Galley and Manning, 2008) and a large distortion limit of 15 to account for the differences in Urdu and English word order. We tune system parameters using MERT with B LEU as the tuning metric. For each experimental condition, we run MERT three times and test for significance 1 Och (2003) showed empirically that the metric used for tuning was the one that systems performed best on at test. 2 28 http://openccg.sourceforge.net (ref.) He said that all these parties have been making electoral alliances . (hyp.) He said all of the parties form the election alliance . + (paraphrase) He said that all the parties have been ma"
2012.amta-monomt.3,P07-2045,0,0.00612081,"CCG paraphrases as The minister, however, did not name any associated agency. This provides a better (uncased) match to one system’s output during tuning: the minister said , however , did not name any assistant organization. We apply this method both to the original references and to those that have been paraphrased by the method in Section 2. 4 Improved Parameter Optimization in Phrase-Based MT We use both paraphrase methods described above to test the effects of paraphrasing on MT tuning performance in an Urdu-English translation task. We train phrase-based systems using the Moses toolkit (Koehn et al., 2007). All systems use “hierarchical” lexicalized reordering (Galley and Manning, 2008) and a large distortion limit of 15 to account for the differences in Urdu and English word order. We tune system parameters using MERT with B LEU as the tuning metric. For each experimental condition, we run MERT three times and test for significance 1 Och (2003) showed empirically that the metric used for tuning was the one that systems performed best on at test. 2 28 http://openccg.sourceforge.net (ref.) He said that all these parties have been making electoral alliances . (hyp.) He said all of the parties for"
2012.amta-monomt.3,P03-1021,0,0.0921602,"ons produce reference translations that are more likely reachable and focused (relevant) w.r.t. a particular translation system being tuned, while grammatical paraphrase helps ensure correctness. These are three qualities that Madnani (2010) has argued are important for MT parameter tuning. In a MERT tuning scenario, we find that both paraphrase methods (lexical and grammatical) lead to improved translation results on two held-out validation sets. String comparison methods such as B LEU (Papineni et al., 2002) are the de facto standard in MT evaluation (MTE) and in MT system parameter tuning (Och, 2003). It is difficult for these metrics to recognize legitimate lexical and grammatical paraphrases, which is important for MT system tuning (Madnani, 2010). We present two methods to address this: a shallow lexical substitution technique and a grammar-driven paraphrasing technique. Grammatically precise paraphrasing is novel in the context of MTE, and demonstrating its usefulness is a key contribution of this paper. We use these techniques to paraphrase a single reference, which, when used for parameter tuning, leads to superior translation performance over baselines that use only human-authored"
2012.amta-monomt.3,W06-3112,0,0.0413737,"Missing"
2012.amta-monomt.3,P02-1040,0,0.104795,"rating its usefulness for parameter tuning is a key contribution of this paper. Targeted lexical substitutions produce reference translations that are more likely reachable and focused (relevant) w.r.t. a particular translation system being tuned, while grammatical paraphrase helps ensure correctness. These are three qualities that Madnani (2010) has argued are important for MT parameter tuning. In a MERT tuning scenario, we find that both paraphrase methods (lexical and grammatical) lead to improved translation results on two held-out validation sets. String comparison methods such as B LEU (Papineni et al., 2002) are the de facto standard in MT evaluation (MTE) and in MT system parameter tuning (Och, 2003). It is difficult for these metrics to recognize legitimate lexical and grammatical paraphrases, which is important for MT system tuning (Madnani, 2010). We present two methods to address this: a shallow lexical substitution technique and a grammar-driven paraphrasing technique. Grammatically precise paraphrasing is novel in the context of MTE, and demonstrating its usefulness is a key contribution of this paper. We use these techniques to paraphrase a single reference, which, when used for parameter"
2012.amta-monomt.3,D12-1023,1,0.880673,"Missing"
2012.amta-monomt.3,W06-1610,0,0.0536639,"Missing"
2012.amta-monomt.3,P05-1074,0,0.126216,"Missing"
2012.amta-monomt.3,N10-1084,0,0.0554229,"Missing"
2012.amta-monomt.3,P11-2031,0,0.0317065,"Missing"
2012.amta-monomt.3,W11-2107,0,0.0232454,"ring comparisons makes it difficult to recognize legitimate morphological, syntactic, lexical and paraphrase variation (Callison-Burch et al., 2006), and such recognition is important for MT tuning (Madnani, 2010). One way to address this issue is to to devise extensions to B LEU (Zhou et al., 27 2 A Simple Method for Targeted Lexical Paraphrase sion of their paraphrase substitutions. We do not use this second filter, as it was not designed to address multiple substitutions in the same sentence. Arguably, metrics such as Meteor, which have high correlation to human judgments (Owczarzak, 2008; Denkowski and Lavie, 2011), should be incorporated into system building pipelines. But B LEU is often the metric of evaluation in cross-system comparisons and hence is usually optimized.1 The technique presented here allows Meteor’s lexical knowledge to be injected into the reference set, and therefore into a B LEU-based tuning regime. Meteor (Denkowski and Lavie, 2011) aligns hypotheses with their references in a greedy multistage process that matches with word forms, then stems, then lexical synonyms, then automatically derived, multiword paraphrases (Bannard and Callison-Burch, 2005). This process is primarily inten"
2012.amta-monomt.3,W11-2139,0,0.0142614,"oing so in a way that targets each hypothesis under evaluation (Madnani, 2010). The approach described here draws inspiration from both of these tactics, and uses the Meteor metric and a large corpus of n-grams to extend a reference set in a way that is targeted to the output of a baseline system (Section 2). In addition, we generate word-order variants of both the original and the lexically paraphrased references by using a high-precision, grammar-driven parsing and realization system (Section 3). The use of sentencelevel paraphrase (Madnani, 2010) — or a rough-andready approximations to it (Dyer et al., 2011) — is not new to MTE or parameter tuning. Using deep, grammatically-driven paraphrase, however, is novel in the context of MTE, and demonstrating its usefulness for parameter tuning is a key contribution of this paper. Targeted lexical substitutions produce reference translations that are more likely reachable and focused (relevant) w.r.t. a particular translation system being tuned, while grammatical paraphrase helps ensure correctness. These are three qualities that Madnani (2010) has argued are important for MT parameter tuning. In a MERT tuning scenario, we find that both paraphrase method"
2020.coling-industry.7,P19-1080,1,0.860524,"Missing"
2020.coling-industry.7,D18-1547,0,0.0397469,"Missing"
2020.coling-industry.7,2020.acl-main.18,0,0.257189,"Missing"
2020.coling-industry.7,P16-2008,0,0.382887,"Missing"
2020.coling-industry.7,W17-3518,0,0.0376347,"Missing"
2020.coling-industry.7,W19-8672,0,0.0929865,"Missing"
2020.coling-industry.7,D16-1139,0,0.0786499,"Missing"
2020.coling-industry.7,2020.tacl-1.47,0,0.0353141,"Missing"
2020.coling-industry.7,W17-5525,0,0.133303,"Missing"
2020.coling-industry.7,P02-1040,0,0.106216,"Missing"
2020.coling-industry.7,2020.findings-emnlp.17,0,0.265502,"Missing"
2020.inlg-1.37,P19-1080,1,0.877959,"Missing"
2020.inlg-1.37,P17-1017,0,0.0284122,"With respect to the FACT models, LBL 312 makes mistakes, but improves upon self-training and reranking. Nonetheless RST models outperform the FACT models. While the best FACT model performs well with respect to producing the correct discourse connective/structure, this model produces serious content errors that render some outputs (discussed in Section 8.1) incoherent. 9 Related and Future Work While traditional natural language generation systems, e.g. Methodius, often employ knowledge graphs, the use of such structure in neural NLG is underdeveloped. An exception in this respect is WebNLG (Gardent et al., 2017), which is a multilingual corpus for natural language generation. An Mean Errors per Item Omissions Hallucinations 1 0.5 ST -L R R ST -S R FA FA B M L ST -ST -V -L A G N -S TR M R R -S G TL C TS C M TR TR -S M FA C TL M B R L 0 Figure 2: Standard Set Mean Errors per Item 3 Repetitions Omissions Hallucinations 2 1 L ST -ST -V -L A G N -S TR M R M ST -L R R ST -S R C TL G -S B R TR M M TR -S FA C TS M FA C TL B L R 0 FA 8.2.2 Challenge Test On the challenge test, no model achieved perfect accuracy. The best performances are by RST-SM and RST-LG. Their performance is similar. It is worth noting t"
2020.inlg-1.37,N19-4009,0,0.0264773,"Missing"
2020.inlg-1.37,prasad-etal-2008-penn,0,0.042004,"Missing"
2020.inlg-1.37,W19-8669,0,0.0478299,"Missing"
2020.inlg-1.37,P98-2202,0,0.597031,"Missing"
2020.inlg-1.37,L16-1273,1,0.938886,"e domain- or applicationspecific rules to be developed, even if the modules themselves are reusable. Accompanying the increase in crowd-sourced corpora has been a comparative simplification of both MRs and tasks. In particular, classic NLG systems typically made use of hierarchically structured content plans that included discourse relations as central components, where the discourse relations — often based on Rhetorical Structure Theory (RST) (Mann and Thompson, 1988; Taboada and Mann, 2006) — group together and connect elementary propositions or messages (Hovy, 1993; Stede and Umbach, 1998; Isard, 2016). By contrast, more recent neural approaches — in particular, those developed for the E2E and WebNLG shared task challenges — have mostly mapped simple, flat inputs to texts without representing discourse relations explicitly. The absence of discourse relations in work on neural NLG to date is somewhat understandable given that neural systems have primarily tackled texts that merely describe entities, rather than comparing them, situating them in time, discussing causal or other contingency relations among them, or constructing persuasive arguments about them, where discourse relations are cru"
2020.inlg-1.37,2020.inlg-1.14,0,0.0603985,"Missing"
2020.inlg-1.37,W19-8672,0,0.0256275,"in like; (c) Target contains unlike but Hypothesis generates like; (d) Target contains like but Hypothesis generates unlike (Like vs. Unlike); (e) Target contains neither like nor like and the same holds of Hypothesis (No rel in both); (f) the rest of the cases. 6 Self-training With most NLG applications, large amounts of parallel data are not readily available. This is true even in the case of Methodius, because there are a finite number of exhibits and facts and thus the number of meaningful combinations which can be constructed from them is limited. In order to reduce annotated data needs, Kedzie and McKeown (2019), Qader et al. (2019) and He et al. (2020) propose self-training methods for NLG. Li and White 5 6 7 Train a model on L; repeat Pseudo-label the unlabeled data in U; Train a model on the pseudo-parallel data; Fine-tune the model on L; until convergence or maximum iteration; Reranking with reverse models In the syntax-semantics interface, the parsing task is usually to build a correct semantic (or syntactic) representation of a sentence. One can consider this task with respect to neural networks—which operate on sequences—straightforwardly by reversing the order of the parallel data: the source"
2020.webnlg-1.12,P19-1080,1,0.899965,"Missing"
2020.webnlg-1.12,W17-4755,0,0.0188182,"e WebNLG 2017 three-fold human evaluation scores. See text for model descriptions. report the single reference BLEU here to be consistent with other metrics which cannot consider multi-reference, e.g. the following BLEURT metric. BLEURT As an n-gram metric, BLEU may not fairly judge the correctness of the generated texts, and since human evaluation is expensive, we investigate using a recently developed modelbased metric, known as BLEURT (Sellam et al., 2020), for automatic evaluation. BLEURT’s evaluation shows state-of-the-art consistency with human judgements on the WMT Metrics Shared Task (Bojar et al., 2017). And it is also shown to be well adapted to other domains with its pretrained model, such as WebNLG 2017.3 Considering we do not have the human evaluation data for WebNLG 2020 now, the best we can do is to use WebNLG 20174 as the data to fine-tune BLEURT to evaluate our models for WebNLG 2020. The human evaluation in WebNLG is three fold, including fluency, grammar correctness, and semantics adequacy. Each of them has one of three possible values (1 or 2 or 3) for the human annotator. Each sentence is assigned up to three annotators. The range of average score is from 1 to 3. Then, we fine-tu"
2020.webnlg-1.12,2020.webnlg-1.7,0,0.185192,"Missing"
2020.webnlg-1.12,W18-6539,0,0.0317132,"Missing"
2020.webnlg-1.12,P17-1017,0,0.220074,"some success, both in terms of lexical and morpho-syntactic choices for generation, as well as for content aggregation. Nevertheless, in a number of cases, the model can be unpredictable, both in terms of failure or success. Omissions of the content and hallucinations, which in many cases occurred at the same time, were major problems. By contrast, the models for English showed near perfect performance on the validation set. 1 Introduction The WebNLG Challenge has attracted increasing attention since 2017. It aims to promote research on realizing resource description framework (RDF) triples (Gardent et al., 2017) of various categories in fluent and accurate natural language. The RDF triple sets on which the WEBNLG dataset is best are from DBPedia (Auer et al., 2007) and consist of one or several triples, where each triple contains one subject, one predicate, and one object. The most challenging part of WebNLG 2017 is to generate from data containing unseen categories, where many predicates and subject/object entities don’t appear in the training data. Consequently, for WebNLG 2020, we are especially interested in improving realization quality for data from unseen categories. Because the size of the tr"
2020.webnlg-1.12,2020.inlg-1.14,0,0.275546,"ct. The most challenging part of WebNLG 2017 is to generate from data containing unseen categories, where many predicates and subject/object entities don’t appear in the training data. Consequently, for WebNLG 2020, we are especially interested in improving realization quality for data from unseen categories. Because the size of the training data in the WebNLG challenge is quite modest, we think it is important to introduce extra knowledge into the pipeline. Considering that large pretrained models have achieved tremendous success in various NLP tasks, including recently for data-to-text NLG (Kale, 2020; Kale and Rastogi, 2020), we investigate using pretrained models to enhance generation from unseen categories, and find they work remarkably well. In addition, we try to further improve the finetuned large pretrained models by using reverse model reranking, which involves reranking the outputs of the beam of the original forward model for a given source input using the perplexity of that input (conditioned on the output) according to the reverse model. As shown in Shen et al. (2019); Yee et al. (2019), techniques similar to the reverse model reranking method we use here, under the guise of RS"
2020.webnlg-1.12,2020.tacl-1.47,0,0.0252904,"ies, even though the outputs with seen categories were mostly acceptable. This is due to the unseen categories frequently introducing unseen predicates and noun phrases, on which models, having never seen these expressions, struggle to consistently produce text from. Considering that large pretrained models have achieved tremendous success in many areas, we think they should also be helpful for generating unseen content with small training datasets. To test the conjecture that pretrained models would improve performance on unseen content, we use T5 (Kale, 2020) for the English task and mBART (Liu et al., 2020) for the Russian task. These models are chosen because T5 is designed for the monolingual task and mBART is designed for multi-lingual tasks. To be more specific, we use the pretrained T5-Large HuggingFace transformer model (Wolf et al., 2019) and the mBART-Large fairseq model (Ott et al., 2019). Fine-tuning1 Considering that both these models tokenize the sentences into subwords and the triple delimiters (e.g. predicate ) are supposed 1 Code: https://github.com/znculee/webnlg2020 118 to be indivisible, we add the three special delimiters to the vocabulary of the pretrained models.2 The embedd"
2020.webnlg-1.12,N19-4009,0,0.0261298,"ained models have achieved tremendous success in many areas, we think they should also be helpful for generating unseen content with small training datasets. To test the conjecture that pretrained models would improve performance on unseen content, we use T5 (Kale, 2020) for the English task and mBART (Liu et al., 2020) for the Russian task. These models are chosen because T5 is designed for the monolingual task and mBART is designed for multi-lingual tasks. To be more specific, we use the pretrained T5-Large HuggingFace transformer model (Wolf et al., 2019) and the mBART-Large fairseq model (Ott et al., 2019). Fine-tuning1 Considering that both these models tokenize the sentences into subwords and the triple delimiters (e.g. predicate ) are supposed 1 Code: https://github.com/znculee/webnlg2020 118 to be indivisible, we add the three special delimiters to the vocabulary of the pretrained models.2 The embeddings of these three special delimiters are randomly initialized. After extending the vocabularies, the total parameters to be fine-tuned for T5 and mBART are 737,643,008 and 610,851,840, respectively. The T5 model is fine-tuned using cross entropy loss without label smoothing. The learning rate"
2020.webnlg-1.12,P02-1040,0,0.10718,"model is fine-tuned with different hyper-parameters than T5 because we followed the recommended fine-tuning guidelines of the mBART authors. It is optimized according to cross entropy loss with label smoothing of 0.2. The batch size is 2048 tokens. The learning rate is 3 × 10−5 and the scheduler is polynomial decay with 2500 warmup updates. The optimizer is Adam where β1 = 0.9, β2 = 0.98,  = 1 × 10−6 . The best checkpoint is selected by validation with patience of 20 training epochs. The best checkpoint is at the end of the 32nd epoch. 3 Experiments 3.1 Evaluation Metrics BLEU We use BLEU-4 (Papineni et al., 2002) implemented by the e2e-metrics (Duˇsek et al., 2018). Although we can evaluate with multiple references by combining the same MR, we still 2 We only extend the vocabulary for the T5-Large of HuggingFace transformer by the add token() function, because fairseq does not support this feature currently to the best of our knowledge. Models BLEU BLEURT Fluency Grammar Semantics T5 46.51 0.464176 2.58516 2.70785 2.53218 T5-RMR 45.40 0.443449 2.56646 2.69307 2.51706 T5-RMRB 45.31 0.435909 2.55813 2.68477 2.51426 T5-RMRF 45.16 0.439485 2.56026 2.68461 2.51901 T5-ORAC 55.13 0.538256 2.62836 2.74878 2.5"
2020.webnlg-1.12,2020.acl-main.704,0,0.0251204,"RT is the pretrained BLEURT-base-128 model without any fine-tuning on WebNLG data. Fluency, grammar, and semantics are the scores from BLEURT finetuned on the WebNLG 2017 three-fold human evaluation scores. See text for model descriptions. report the single reference BLEU here to be consistent with other metrics which cannot consider multi-reference, e.g. the following BLEURT metric. BLEURT As an n-gram metric, BLEU may not fairly judge the correctness of the generated texts, and since human evaluation is expensive, we investigate using a recently developed modelbased metric, known as BLEURT (Sellam et al., 2020), for automatic evaluation. BLEURT’s evaluation shows state-of-the-art consistency with human judgements on the WMT Metrics Shared Task (Bojar et al., 2017). And it is also shown to be well adapted to other domains with its pretrained model, such as WebNLG 2017.3 Considering we do not have the human evaluation data for WebNLG 2020 now, the best we can do is to use WebNLG 20174 as the data to fine-tune BLEURT to evaluate our models for WebNLG 2020. The human evaluation in WebNLG is three fold, including fluency, grammar correctness, and semantics adequacy. Each of them has one of three possible"
2020.webnlg-1.12,N19-1410,0,0.0228305,"large pretrained models have achieved tremendous success in various NLP tasks, including recently for data-to-text NLG (Kale, 2020; Kale and Rastogi, 2020), we investigate using pretrained models to enhance generation from unseen categories, and find they work remarkably well. In addition, we try to further improve the finetuned large pretrained models by using reverse model reranking, which involves reranking the outputs of the beam of the original forward model for a given source input using the perplexity of that input (conditioned on the output) according to the reverse model. As shown in Shen et al. (2019); Yee et al. (2019), techniques similar to the reverse model reranking method we use here, under the guise of RSA or noisy channel models, have shown significant benefits for generation tasks when there is room for improvement. However, in our experiments, reverse model reranking not only provides no benefit but also slightly harms realization quality. By looking at the BLEU oracle reranking scores, we see a considerable potential for reranking, so we surmise that the quality of the reverse model must be too low to be useful. This could indicate that it is more difficult for large-scale pretra"
2020.webnlg-1.12,J82-2005,0,0.508637,"Missing"
2021.emnlp-main.53,2020.acl-main.703,0,0.121413,"improves upon existing techniques, and that we achieve stateof-the-art results by combining our adaptive data generation approaches with Harkous et al.’s nonadaptive ones. 2 Related Work Work on automated evaluation metrics in the tradition of BLEU (Papineni et al., 2002) shares similar goals as our work, except that such metrics make use of reference sentences and thus are not designed for use at inference time. Moreover, such methods have not been found to correlate well with human evaluation of individual texts outside of the machine translation paradigm (Reiter, 2018). Çelikyilmaz et al. (2020) presents a comprehensive literature survey of the three broad categories of evaluation of the text generation models—human, automated and machine-learned—along with providing strong motivation for doing NLG evaluation.1 Our approach is inspired by work in the third category of machine-learned evaluation. 1 See also Kry´sci´nski et al. (2019), who explore NLG evaluation in the specific NLG sub-fields of summarization and paraphrasing. 682 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 682–697 c November 7–11, 2021. 2021 Association for Computation"
2021.emnlp-main.53,2020.inlg-1.19,0,0.0303898,". Also related is Sellam et al.’s (2020) work on building a machine-learned scorer, BLEURT, to replace automated metrics such as BLEU. They use mask filling with a pretrained language model for creating synthetic unacceptable examples. In this paper, we introduce several new techniques for synthetic data generation, and comprehensively evaluate them in comparison to Harkous et al. (2020)’s methods, as well as to BLEU and BLEURT. In addition, we introduce a validation framework to sort the samples into the 2 classes. Our validation framework uses a pretrained entailment model, similarly to how Dušek and Kasner (2020) use one for semantic evaluation; here, we go beyond their approach by using it to develop an adaptive acceptability classifier that is better suited to runtime use. As an alternative to using acceptability classifiers, one can make use of reconstruction models (Shen et al., 2019; Yee et al., 2019) to determine how well the NLG model’s output predicts its input. These models are capable of detecting content errors but are not designed to capture grammatical mistakes. Additionally, since such approaches employ a second autoregressive decoding step, they are less well-suited to runtime inference"
2021.emnlp-main.53,P02-1040,0,0.109991,"that better resemble the output of these models. We then pass these synthetic samples through a novel validation framework that filters and sorts them into acceptable and unacceptable classes, further improving the quality of the overall synthetic dataset. We show that an acceptability classifier built on top of the data generated by our approach improves upon existing techniques, and that we achieve stateof-the-art results by combining our adaptive data generation approaches with Harkous et al.’s nonadaptive ones. 2 Related Work Work on automated evaluation metrics in the tradition of BLEU (Papineni et al., 2002) shares similar goals as our work, except that such metrics make use of reference sentences and thus are not designed for use at inference time. Moreover, such methods have not been found to correlate well with human evaluation of individual texts outside of the machine translation paradigm (Reiter, 2018). Çelikyilmaz et al. (2020) presents a comprehensive literature survey of the three broad categories of evaluation of the text generation models—human, automated and machine-learned—along with providing strong motivation for doing NLG evaluation.1 Our approach is inspired by work in the third"
2021.emnlp-main.53,2020.findings-emnlp.17,0,0.0286792,"Cumberland County and to its southeast is Carroll County. 11th Mississippi Infantry Monument was established in 2000 and is located to the south in Adams County Pennsylvania, the north of which is which is Cumberland County and to its southeast is Carroll County. Table 2: Example Synthetic Acceptable(+) and Unacceptable(-) samples generated from a seed WebNLG response (Ref). The abbreviations follow the naming discussed in Section 9. 6.3 BLEU Score Validator (BLEU - VAL) 8 Few-Shot Setting Recently, there have been several efforts to train NLG models in a few-shot setting (Chen et al., 2020; Peng et al., 2020; Arun et al., 2020; Heidari et al., 2021). We adopt the self-training strategy introduced by Heidari et al. (2021) to generate training data for generative models, which is 7 Classifier Model Architecture also used as the seed data needed for acceptability modeling. Self-training consists of several cyWe formulate the task as binary classification with cles of generation and reconstruction. For generalabels {Acceptable, Unacceptable} and learn a dis- tion, we fine-tune BART (Lewis et al., 2020) using criminator model using both the training data of only 500 annotated examples to generate NLG"
2021.emnlp-main.53,W19-8669,0,0.0255252,"e a variety of errors in sufficient quantities to be able to effectively test the acceptability classifiers. Additionally, the Weather and Alarm test sets are representative of current SOTA models built for these domains. Human evaluations were done for all test sets except WebNLG to determine acceptability, using two annotators and a tie-breaker round in case of disagreement. The number of samples in all human annotated test sets can be found in Table 17. Regarding our self-training experiments, we note that self-training has been previously investigated for NLG by Kedzie and McKeown (2019), Qader et al. (2019) and Stevens-Guille et al. (2020), though they do not explore using pre-trained models with self-training. Also related are earlier approaches that use cycle consistency between parsing and generation models for automatic data cleaning (Nie et al., 2019; Chisholm et al., 2017). More recently, Chang et al. (2021) have developed a method for randomly generating new text samples with GPT-2 then automatically pairing them with data samples. By comparison, we take a much 2 more direct and traditional approach to generatObtained under CC BY-NC-SA 4.0 license ing new text samples from unpaired inputs"
2021.emnlp-main.53,J18-3002,0,0.0120656,"the data generated by our approach improves upon existing techniques, and that we achieve stateof-the-art results by combining our adaptive data generation approaches with Harkous et al.’s nonadaptive ones. 2 Related Work Work on automated evaluation metrics in the tradition of BLEU (Papineni et al., 2002) shares similar goals as our work, except that such metrics make use of reference sentences and thus are not designed for use at inference time. Moreover, such methods have not been found to correlate well with human evaluation of individual texts outside of the machine translation paradigm (Reiter, 2018). Çelikyilmaz et al. (2020) presents a comprehensive literature survey of the three broad categories of evaluation of the text generation models—human, automated and machine-learned—along with providing strong motivation for doing NLG evaluation.1 Our approach is inspired by work in the third category of machine-learned evaluation. 1 See also Kry´sci´nski et al. (2019), who explore NLG evaluation in the specific NLG sub-fields of summarization and paraphrasing. 682 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 682–697 c November 7–11, 2021. 2021"
2021.emnlp-main.53,2020.acl-main.704,0,0.0243502,"etric for E2E human evaluations. Figure 3: Comparing Macro-F1 scores of full data and few-shot acceptability classifiers using delexicalized WebNLG dataset. For the full data, the best adaptive combination (BADP) is NBM+FTB and the best adaptive methods combined with SNT (SNT+ADP) are SBTG + NBM + BART + FTB. For the few shot case, BADP is SBTG + NBM + BART + FTB and SNT + ADP is SBTG + FTB . 10 WebNLG 45.4 47.4 51.9 50.1 48.1 78.8 (E 2 E) for E2E human evaluations.5 Specifically, we used all 5,363 items, sampling 1,000 of them as validation data, following BLEURT paper conclusions. Following Sellam et al. (2020), we stopped fine-tuning at 40,000 steps. We obtained confidence thresholds for BLEU and all BLEURT based models by optimizing 10fold cross validation Macro F1 scores as described in Section 9 at every 2 unit interval (step size of 0.02 for BLEURT and 2 for BLEU) between minimum and maximum BLEU/BLEURT scores. The threshold was then used to determine the predicted class. We show results in Table 12. As expected, BLEURT variants generally outperform BLEU. BLEURT fine-tuned on WebNLG outperforms BLEURT fine-tuned on E2E on WebNLG, and vice-versa for E2E, with an outlier outperformance of BLEU on"
2021.emnlp-main.53,N19-1410,0,0.0221139,"for synthetic data generation, and comprehensively evaluate them in comparison to Harkous et al. (2020)’s methods, as well as to BLEU and BLEURT. In addition, we introduce a validation framework to sort the samples into the 2 classes. Our validation framework uses a pretrained entailment model, similarly to how Dušek and Kasner (2020) use one for semantic evaluation; here, we go beyond their approach by using it to develop an adaptive acceptability classifier that is better suited to runtime use. As an alternative to using acceptability classifiers, one can make use of reconstruction models (Shen et al., 2019; Yee et al., 2019) to determine how well the NLG model’s output predicts its input. These models are capable of detecting content errors but are not designed to capture grammatical mistakes. Additionally, since such approaches employ a second autoregressive decoding step, they are less well-suited to runtime inference in systems with tight latency budgets. training (He et al., 2020), using pre-trained models fine-tuned on the few-shot data for both generation and reconstruction filtering. Dataset WebNLG Cleaned E2E ViGGO Weather Alarm Delexed WebNLG #of samples 2453 500 500 493 1470 500 #of u"
2021.gem-1.12,2021.gem-1.12,1,0.0530913,"Missing"
2021.gem-1.12,2020.coling-industry.7,1,0.868647,"Missing"
2021.gem-1.12,P19-1080,1,0.882284,"Missing"
2021.gem-1.12,2021.eacl-main.64,0,0.432283,"Missing"
2021.gem-1.12,2020.acl-main.18,0,0.141949,"Missing"
2021.gem-1.12,E17-1060,0,0.333577,"Missing"
2021.gem-1.12,W16-3622,0,0.612772,"Missing"
2021.gem-1.12,P16-2008,0,0.294964,"Missing"
2021.gem-1.12,2020.coling-main.218,0,0.205932,"Missing"
2021.gem-1.12,2021.sigdial-1.8,1,0.244101,"Missing"
2021.gem-1.12,2020.emnlp-main.527,0,0.31832,"Missing"
2021.gem-1.12,W19-8672,0,0.244614,"Missing"
2021.gem-1.12,J82-2005,0,0.449347,"Missing"
2021.gem-1.12,2020.acl-main.703,0,0.147126,"Missing"
2021.gem-1.12,2020.webnlg-1.12,1,0.674326,"Missing"
2021.gem-1.12,P19-1256,0,0.235005,"Missing"
2021.gem-1.12,W17-5525,0,0.161262,"Missing"
2021.gem-1.12,P02-1040,0,0.110173,"Missing"
2021.gem-1.12,2020.findings-emnlp.17,0,0.171199,"Missing"
2021.gem-1.12,W19-8669,0,0.383149,"Missing"
2021.gem-1.12,W19-8611,1,0.607388,"Missing"
2021.gem-1.12,N06-2031,0,0.190847,"Missing"
2021.gem-1.12,N09-2048,0,0.784012,"Missing"
2021.gem-1.12,D15-1199,0,0.178954,"Missing"
2021.gem-1.12,2020.webnlg-1.11,0,0.489952,"Missing"
2021.inlg-1.10,2020.coling-industry.7,1,0.932335,"n challenging cases. However, their constrained decoding method incurs a substantial runtime cost, making it too slow to deploy in task-oriented dialogue systems where low latency is a priority. Thus, finding ways to improve data efficiency for training models that perform satisfactorily with vanilla decoding remains an important challenge. In order to reduce annotated data needs, Kedzie and McKeown (2019) and Qader et al. (2019) propose self-training methods for NLG, though they do not explore self-training for the more challenging case of generating from compositional input representations. Arun et al. (2020) do explore self-training with compositional inputs, but they do not consider constrained decoding. In this paper, we investigate for the first time whether constrained decoding can be used during self-training to enhance data efficiency for compositional neural NLG, since the speed of constrained decoding is much less of a concern during self-training than it is at runtime in dialogue systems. In particular, we adapt and extend He et al.’s (2020) approach to self-training for MT to the setting of neural NLG from compositional MRs, comparing vanilla self-training to self-training enhanced with"
2021.inlg-1.10,P16-2008,0,0.0675905,"Missing"
2021.inlg-1.10,W18-6539,0,0.0343353,"Missing"
2021.inlg-1.10,N18-1177,0,0.0275691,"show impressive results with just 200 training items using a specialized table encoder with GPT-2, while Peng et al. (2020) use cross-domain training (an orthogonal approach) together with GPT-2; neither investigates more challenging compositional inputs. Although Arun et al. (2020) do use BART on compositional inputs, their tree accuracy levels are much lower even when using considerably more data. More generally, reverse (or reconstructor) models have taken on greater theoretical interest thanks to Rational Speech Act (RSA) theory (Frank et al., 2009) and have recently proved useful in NLG (Fried et al., 2018; Shen et al., 2019). Our approach differs in using reverse models during selftraining rather than at runtime. Work on combining parsing and generation for ambiguity avoidance goes back much farther (Neumann and van Noord, 1992), with managing the trade-off between fluency and ambiguity avoidance a more recent topic (Duan and White, 2014) that we also leave for future work. Constrained decoding (Balakrishnan et al., 2019) is inspired by coverage tracking in grammar-based approaches to realization (Kay, 1996; Carroll and Oepen, 2005; White, 2006); its use during self-training is novel to this w"
2021.inlg-1.10,P19-1256,0,0.0180847,"Rs, but do so directly by just deleting nodes in the input trees. Likewise, our general approach to self-training (He et al., 2020) is much simpler than in Chang et al.’s (2021) work, where they generate new text samples using GPT-2 (unconditioned on any input) then pair them with data samples. Earlier, Chisholm et al. (2017) train NLG and NLU models that share parameters to reduce the risk of hallucination. Our iterative method of training forward and reverse seq2seq models instead draws from Yee et al.’s (2019) reverse model reranking approach and is much simpler to implement. Additionally, Nie et al. (2019) apply self-training to a NLU model to reduce the noise in the original MR-text pairs in order to reduce the hallucination problem in NLG, but they do not investigate data efficiency issues. Also related is work on back-translation (Sennrich et al., 2016) in MT, which starts from the assumption that More recent work takes advantage of pre-trained language models to develop few-shot NLG methods. Chen et al. (2019) show impressive results with just 200 training items using a specialized table encoder with GPT-2, while Peng et al. (2020) use cross-domain training (an orthogonal approach) together"
2021.inlg-1.2,2020.acl-main.703,0,0.0423103,"ing the classic rulebased NLG system Methodius (Isard, 2016) using a neural generator. In their corpus, the meaning representation (MR) of a text is a tree that encodes the overall discourse structure of the texts plus facts related by discourse relations therein. They were concerned with whether explicit encoding of discourse relations improves the quality of generated texts by LSTM recurrent neural networks (Hochreiter and Schmidhuber, 1997). However, they left open the question whether discourse relations are helpful for pre-trained transformer-based (Vaswani et al., 2017) language models (Lewis et al., 2020; Raffel et al., 2019), which have recently shown remarkable performance on NLG tasks. In this work, we address that question using the T5-Large implementation of Wolf et al. (2019). A particularly attractive quality of pre-trained models is their ability to generalize from limited data. For example, Peng et al. (2020) proposed to fine-tune a model pre-trained on a large NLG corpus using a small amount of labeled data from a specific domain to adapt the model to generate texts in that domain. In a similar vein, when the labeled data is limited, Arun et al. (2020) suggest to use a large pre-tra"
2021.inlg-1.2,2020.coling-industry.7,1,0.764841,"et al., 2017) language models (Lewis et al., 2020; Raffel et al., 2019), which have recently shown remarkable performance on NLG tasks. In this work, we address that question using the T5-Large implementation of Wolf et al. (2019). A particularly attractive quality of pre-trained models is their ability to generalize from limited data. For example, Peng et al. (2020) proposed to fine-tune a model pre-trained on a large NLG corpus using a small amount of labeled data from a specific domain to adapt the model to generate texts in that domain. In a similar vein, when the labeled data is limited, Arun et al. (2020) suggest to use a large pre-trained model with self-training and knowledge distillation to smaller, faster models. Kale and Rastogi (2020) argue that pre-trained language models make it possible to transform a sequence of semantically correct, but (possibly) ungrammatical template-based texts into a natural sounding, felicitous text of English. They find that template-based textual input is beneficial to use with pre-trained language models when the model needs to generalize from relatively few examples. Recent developments in natural language generation (NLG) have bolstered arguments in favor"
2021.inlg-1.2,2020.findings-emnlp.17,0,0.0243765,"iscourse relations improves the quality of generated texts by LSTM recurrent neural networks (Hochreiter and Schmidhuber, 1997). However, they left open the question whether discourse relations are helpful for pre-trained transformer-based (Vaswani et al., 2017) language models (Lewis et al., 2020; Raffel et al., 2019), which have recently shown remarkable performance on NLG tasks. In this work, we address that question using the T5-Large implementation of Wolf et al. (2019). A particularly attractive quality of pre-trained models is their ability to generalize from limited data. For example, Peng et al. (2020) proposed to fine-tune a model pre-trained on a large NLG corpus using a small amount of labeled data from a specific domain to adapt the model to generate texts in that domain. In a similar vein, when the labeled data is limited, Arun et al. (2020) suggest to use a large pre-trained model with self-training and knowledge distillation to smaller, faster models. Kale and Rastogi (2020) argue that pre-trained language models make it possible to transform a sequence of semantically correct, but (possibly) ungrammatical template-based texts into a natural sounding, felicitous text of English. They"
2021.inlg-1.2,P11-2031,0,0.0604537,"2 Statistical Significance: Stratified Approximate Randomization tured models and we see more rapid improvements for T2T models. To compare various models, we use stratified approximate randomization (AR; Noreen 1989), which is a powerful and generic method of establishing significant differences between models. One advantage of AR over more traditional paired tests for NLP tasks is that it does not require independence of samples, which is usually violated when we consider various runs of the same model on the same test set (as the same test item gets tested several times by the same model) (Clark et al., 2011). In the present work, we rely on stratified AR to identify whether differences between the performance of various models over several runs is significant. (The description of the stratified AR algorithm is provided in Section A.1 of Appendix A.) 5 Summary: RST vs. FACT The question whether models with discourse relations (RSTS TRUCT and RST T2T) perform better than ones without discourse relations (FACT S TRUCT and FACT T2T respectively) can be answered positively. As for T2T models, we declare with high confidence that RST T2T outperforms FACT T2T in every collected statistics. We are not ho"
2021.inlg-1.2,2020.inlg-1.37,1,0.661014,"o the same phenomena). (3) Test various aspects of generated texts, both with respect to discourse structure congruence and correctness (factual information). 2 Instead of training the models directly on the Methodius corpus or texts harvested through crowdsourcing, we modify the Methodius corpus (i.e., MRs paired with texts) by substituting custom homogeneous texts for the Methodius corpus’s special terminals. We substitute some predetermined names for named entities in the Methodius corpus to further homogenize the inputs. This procedure deterministically rewrites the texts in the corpus of Stevens-Guille et al. (2020) into pure English texts and thus maintains the homogeneity of the Methodius corpus. The corresponding MRs are also rewritten into their lexicalized versions.1 Moreover, we transform Rhetorical structure theory (Mann and Thompson, 1988) style hierarchically structured meaning representations of Methodius texts into a flat, textual input by translating every fact and every discourse relation into a sequence of sentences. Figure 1 shows an MR from the Methodius corpus, the corresponding text from the Methodius corpus, and the new MR that we have substituted for the Methodius corpus MR. 3 Models:"
2021.sigdial-1.8,2020.coling-main.218,0,0.14757,"Missing"
2021.sigdial-1.8,2020.coling-industry.7,1,0.92441,"Since an NLG response directly impacts the user’s experience, it should convey all of the information accurately, should be contextualized with respect to the user request, and be fluent and natural. ∗ Work done while on leave from Ohio State University. 66 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 66–76 July 29–31, 2021. ©2021 Association for Computational Linguistics 2020; Chen et al., 2020). However, these methods have not usually addressed production concerns such as balancing latency and accuracy, which we explore in this paper. Arun et al. (2020) do also consider this trade-off in their data efficiency study, ultimately recommending several sampling and modeling techniques to attain production quality with fast, light-weight neural network models. Since their work is the most similar to ours, we focus our experiments on the most complex domain examined by Arun et al. (2020), the weather dataset, and demonstrate that we can achieve production quality with approximately 8X higher data-efficiency levels by making use of textualized inputs and iterative self-training. In particular, we propose scalable mini-templates to convert structured"
2021.sigdial-1.8,W18-5408,0,0.0138935,"tability checking mechanisms as alternatives (Harkous et al., 2020; Anonymous, 2021). Building an acceptability model requires collecting positive and negative examples. We use the samples that pass the reconstruction step of self-training as the positive ones. The challenge lies in approximating mistakes a model is likely to make in production, and creating a dataset of synthetic negative examples. Anonymous (2021) use mask filling with pre-trained models for creating synthetic incorrect examples, which we adopt using BART. We train two models, a production-grade convolutional (DocNN) model (Jacovi et al., 2018) with median latency of 8 ms and a high-capacity pretrained RoBERTa-Base model (Liu et al., 2019) with latency 100 ms. These binary classification models determine whether a sequence of delexicalized textualized input MR concatenated with the delexicalized model output is correct at runtime. 5 5.1 Results Input and Output Representation Table 4 shows the correctness and grammaticality (c&g) evaluations for various few-shot models in comparison to the full data setting. The results validate our hypothesis that transforming the structured data into a textual form (similar to those used for pre-t"
2021.sigdial-1.8,P19-1080,1,0.862442,"ime and resources it usually takes to instill linguistic information into the templates, they are not contextualized on the user query, and the limited set of templates results in bounded naturalness of the system’s responses. Recently, generative models (Wen et al., 2015; Duˇsek and Jurcıcek, 2016; Rao et al., 2019) have become popular for their data-driven scaling story and superior naturalness over the typical templatebased systems (Gatt and Krahmer, 2018; Dale, 2020). However, training reliable and low-latency generative models has typically required tens of thousands of training samples (Balakrishnan et al., 2019; Novikova et al., 2017). Model maintenance with such a large dataset has proven to be challenging, as it is resource-intensive to debug and fix responses, make stylistic changes, and add new capabilities. Therefore, it is of paramount importance to bring up new domains and languages with as few examples as possible while maintaining quality. Pre-trained models like GPT2 (Radford et al., 2019) have been recently adapted to perform fewshot learning for task-oriented dialog (Peng et al., In this paper, we study the utilization of pretrained language models to enable few-shot Natural Language Gen"
2021.sigdial-1.8,2020.inlg-1.9,0,0.0406005,"Missing"
2021.sigdial-1.8,2021.eacl-main.64,0,0.247992,"th efficiency and generalization. Also related is the approach of Kasner and Duˇsek (2020), who use templates extracted from the training data in part, though their approach is then followed by automatic editing and reranking steps. Self-training has been previously investigated for NLG by Kedzie and McKeown (2019) and Qader et al. (2019), though they do not explore using pretrained models with self-training. Also related are earlier approaches that use cycle consistency between parsing and generation models for automatic data cleaning (Nie et al., 2019; Chisholm et al., 2017). More recently, Chang et al. (2021) have developed a method for randomly generating new text samples with GPT-2 then automatically pairing them with data samples. By comparison, we take a much more direct and traditional approach to generating new text samples from unpaired inputs in self-training (He et al., 2020), using pre-trained models fine-tuned on the few-shot data for both generation and reconstruction filtering. 1. we introduce a generalizable bottom-up templating strategy to convert structured inputs to semi-natural text; 2. we present results of experiments with different representations of input data and output text"
2021.sigdial-1.8,W19-8672,0,0.19347,"d dialog acts to semi-natural and telegraphic text. As such, we don’t need to have various templates for simple scenarios and require only one rule for each new slot to be published with the possibility of choosing from several predefined rules. Moreover, the rules can be reused across domains which helps with efficiency and generalization. Also related is the approach of Kasner and Duˇsek (2020), who use templates extracted from the training data in part, though their approach is then followed by automatic editing and reranking steps. Self-training has been previously investigated for NLG by Kedzie and McKeown (2019) and Qader et al. (2019), though they do not explore using pretrained models with self-training. Also related are earlier approaches that use cycle consistency between parsing and generation models for automatic data cleaning (Nie et al., 2019; Chisholm et al., 2017). More recently, Chang et al. (2021) have developed a method for randomly generating new text samples with GPT-2 then automatically pairing them with data samples. By comparison, we take a much more direct and traditional approach to generating new text samples from unpaired inputs in self-training (He et al., 2020), using pre-trai"
2021.sigdial-1.8,2020.acl-main.18,0,0.210372,"e generation (NLG) is an essential part of task-oriented dialog systems, which converts data into natural language output to be subsequently served to the users. Since an NLG response directly impacts the user’s experience, it should convey all of the information accurately, should be contextualized with respect to the user request, and be fluent and natural. ∗ Work done while on leave from Ohio State University. 66 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 66–76 July 29–31, 2021. ©2021 Association for Computational Linguistics 2020; Chen et al., 2020). However, these methods have not usually addressed production concerns such as balancing latency and accuracy, which we explore in this paper. Arun et al. (2020) do also consider this trade-off in their data efficiency study, ultimately recommending several sampling and modeling techniques to attain production quality with fast, light-weight neural network models. Since their work is the most similar to ours, we focus our experiments on the most complex domain examined by Arun et al. (2020), the weather dataset, and demonstrate that we can achieve production quality with approximately 8X high"
2021.sigdial-1.8,E17-1060,0,0.157021,"be reused across domains which helps with efficiency and generalization. Also related is the approach of Kasner and Duˇsek (2020), who use templates extracted from the training data in part, though their approach is then followed by automatic editing and reranking steps. Self-training has been previously investigated for NLG by Kedzie and McKeown (2019) and Qader et al. (2019), though they do not explore using pretrained models with self-training. Also related are earlier approaches that use cycle consistency between parsing and generation models for automatic data cleaning (Nie et al., 2019; Chisholm et al., 2017). More recently, Chang et al. (2021) have developed a method for randomly generating new text samples with GPT-2 then automatically pairing them with data samples. By comparison, we take a much more direct and traditional approach to generating new text samples from unpaired inputs in self-training (He et al., 2020), using pre-trained models fine-tuned on the few-shot data for both generation and reconstruction filtering. 1. we introduce a generalizable bottom-up templating strategy to convert structured inputs to semi-natural text; 2. we present results of experiments with different represent"
2021.sigdial-1.8,D16-1139,0,0.0227075,"compared to cache latency. generation input (without the input query), given the responses. After generation in each cycle, we use the reconstruction model to select samples with exact reconstruction match. Finally, the selected samples are added to the training pool for knowledge distillation or the next self-training cycle.3 4.3 Knowledge Distillation One of the biggest obstacles in real-world application of pre-trained language models such as BART is their prohibitive latency. We explored knowledge distillation to mitigate this issue, here. We perform sequence-level knowledge distillation (Kim and Rush, 2016) from BART large to BART models with various smaller sizes, in addition to a small LSTM model (Table 3). Self-Training 4.4 Annotating large quantities of high-quality data is time and resource consuming. However, it is often possible to automatically generate a lot of unlabeled data using a synthetic framework. Here, we adapt and extend the semi-supervised self-training strategy introduced by He et al. (2020). As shown in Figure 2, self-training consists of multiple cycles of generation and reconstruction. We fine-tune BART (Lewis et al., 2020), a pretrained seq2seq language model, for both st"
2021.sigdial-1.8,2020.acl-main.703,0,0.342868,"Missing"
2021.sigdial-1.8,P16-2008,0,0.284006,"Missing"
2021.sigdial-1.8,2020.emnlp-main.414,1,0.723168,"pped to and. • Slot values: A possible behavior for textualizing slots inside dialog acts is just to mention the slot value. For example, we chose to represent weather condition using this scheme. The second technique that we explore is delexicalizing the slot values in order to mitigate model hallucination. During our initial experiments, we observed that in few-shot settings, pre-trained language models can drop some slots or fail to exactly copy their values, which can be catastrophic in a production system. This has been observed in other generation tasks using pre-trained models as well (Einolghozati et al., 2020). Therefore, we ex• Slot name and values: Slot names are replaced by an engineer-defined string and placed before slot values. For example, we represent slots such as low temperature and high temperature using this 70 Model BART large BART base BART 3 3 BART 5 1 LSTM Cache plore delexicalization of slots when linguistically permissible. For example, weather condition can not be delexicalized since its different values such as sand storm or fog will change the surface form of the sentence significantly while a slot such as weekday can be delexicalized. We also combine the few-shot Weather sampl"
2021.sigdial-1.8,P19-1256,0,0.187873,"er, the rules can be reused across domains which helps with efficiency and generalization. Also related is the approach of Kasner and Duˇsek (2020), who use templates extracted from the training data in part, though their approach is then followed by automatic editing and reranking steps. Self-training has been previously investigated for NLG by Kedzie and McKeown (2019) and Qader et al. (2019), though they do not explore using pretrained models with self-training. Also related are earlier approaches that use cycle consistency between parsing and generation models for automatic data cleaning (Nie et al., 2019; Chisholm et al., 2017). More recently, Chang et al. (2021) have developed a method for randomly generating new text samples with GPT-2 then automatically pairing them with data samples. By comparison, we take a much more direct and traditional approach to generating new text samples from unpaired inputs in self-training (He et al., 2020), using pre-trained models fine-tuned on the few-shot data for both generation and reconstruction filtering. 1. we introduce a generalizable bottom-up templating strategy to convert structured inputs to semi-natural text; 2. we present results of experiments"
2021.sigdial-1.8,W17-5525,0,0.147619,"Missing"
2021.sigdial-1.8,2020.findings-emnlp.17,0,0.111222,", knowledge distillation, and caching to train productiongrade few-shot NLG models; and 4. we release datasets, model predictions, and human judgements to study the NLG domain stand-up under the few-shot setting. 2 3 Task Our task is to convert a tree-based scenario into natural text, given the original query. An example data item together with its transformations (Section 4) is shown in Table 1. Related Work Pre-trained language models have shown promising results for generation tasks such as translation, summarization and data-to-text (Lewis et al., 2020; Yang et al., 2020). As noted above, Peng et al. (2020) and Chen et al. (2020) likewise explore pretrained models for few-shot NLG in task-oriented 3.1 Data Our experiments were conducted using 4 taskoriented datasets. We focused on the most challenging dataset, Conversational Weather, which is 67 Query How is the weather over the next weekend? INFORM 1[temp low[20] temp high[45] date time[colloquial[next weekend]]] Structured CONTRAST 1[ MR INFORM 2[condition[ sun ] date time[weekday[Saturday]]] INFORM 3[condition[ rain ] date time[weekday[Sunday]]] ] INFORM 1[temp low[temp low 1] temp high[temp high 1] date time[colloquial Delexicalized [next we"
2021.sigdial-1.8,W19-8669,0,0.379589,"Missing"
2021.sigdial-1.8,W19-8611,1,0.46963,"since new templates need to be authored for different response variations; templates authored for a prior domain are not usually reusable for future domains; and it becomes increasingly arduous to author high-quality templates for complex domains. More importantly, in spite of the high amount of time and resources it usually takes to instill linguistic information into the templates, they are not contextualized on the user query, and the limited set of templates results in bounded naturalness of the system’s responses. Recently, generative models (Wen et al., 2015; Duˇsek and Jurcıcek, 2016; Rao et al., 2019) have become popular for their data-driven scaling story and superior naturalness over the typical templatebased systems (Gatt and Krahmer, 2018; Dale, 2020). However, training reliable and low-latency generative models has typically required tens of thousands of training samples (Balakrishnan et al., 2019; Novikova et al., 2017). Model maintenance with such a large dataset has proven to be challenging, as it is resource-intensive to debug and fix responses, make stylistic changes, and add new capabilities. Therefore, it is of paramount importance to bring up new domains and languages with as"
2021.sigdial-1.8,D15-1199,0,0.189473,"Missing"
2021.sigdial-1.8,2020.webnlg-1.11,1,0.460323,"ained language models, self-training, knowledge distillation, and caching to train productiongrade few-shot NLG models; and 4. we release datasets, model predictions, and human judgements to study the NLG domain stand-up under the few-shot setting. 2 3 Task Our task is to convert a tree-based scenario into natural text, given the original query. An example data item together with its transformations (Section 4) is shown in Table 1. Related Work Pre-trained language models have shown promising results for generation tasks such as translation, summarization and data-to-text (Lewis et al., 2020; Yang et al., 2020). As noted above, Peng et al. (2020) and Chen et al. (2020) likewise explore pretrained models for few-shot NLG in task-oriented 3.1 Data Our experiments were conducted using 4 taskoriented datasets. We focused on the most challenging dataset, Conversational Weather, which is 67 Query How is the weather over the next weekend? INFORM 1[temp low[20] temp high[45] date time[colloquial[next weekend]]] Structured CONTRAST 1[ MR INFORM 2[condition[ sun ] date time[weekday[Saturday]]] INFORM 3[condition[ rain ] date time[weekday[Sunday]]] ] INFORM 1[temp low[temp low 1] temp high[temp high 1] date ti"
A97-1038,J88-3006,0,0.111328,"ich have proved useful in the present effort. In developing the exemplars for CogentHelp, we have made use of three NLG techniques: structuring texts by traversing domain relations, automatically grouping related information, and using revisions to simplify the handling of constraint interactions. The first two of these make life simpler for the end user, while the third makes life simpler for the developer. 5.2.1 C a p i t a l i z i n g on D o m a i n S t r u c t u r e When text structure follows domain structure, one can generate text by selectively following appropriate links in the input (Paris, 1988; Sibun, 1992). In the case at hand, we have chosen to use the group and cluster structure combined with a top-down, left-to-right spatial sort: while such a spatial sort alone is insufficient, as we saw above, a spatial sort which respects functional groups turns out to work well. Returning to the example of Section 5.1.2 (regarding the window shown in Figure 1), traversing the clusters in this way yields a listing which (naturally!) mirrors the order and groupings of the one-sentence description of the window's organization we have provided - - that is, following a general description of the"
A97-1038,A92-1006,0,0.0780542,"Missing"
A97-1038,W96-0416,0,0.01651,"ody the "" e v o l u t i o n - f r i e n d l y "" properties of tools in the literate programming tradition (Knuth, 1992) - - e.g., the by now well-known j avadoc utility for generating API documentation from comments embedded in Java source code (Friendly, 1995; cf. also Johnson and Erdem, 1995; Priestly et al., 1996; Korgen, 1996) - - in a tool for generating end user-level documentation. CogentHelp is also unusual in that it is (to date) one of the few tools to bring NLG techniques to bear on the problem of a u t h o r i n g dynamically generated documents (cf. Paris and Vander Linden, 1996; Knott et al., 1996; Hirst and DiMarco, 1995); traditionally, most applied NLG systems have focused on niches where texts can be generated fully automatically, such as routine reports of various types (e.g. Goldberg et al., 1994; Kukich et al., 1994) or explanations of expert system reasoning (cf. Moore, 1995 and references therein). While striving to design highly sophisticated, fully automatic systems has undoubtedly led to a deeper understanding of the text generation process, it has had the unfortunate effect (to date) of limiting the use of techniques pioneered in the NLG community to just a few niches wher"
A97-1038,W96-0401,0,\N,Missing
A97-1038,P83-1022,0,\N,Missing
boxwell-white-2008-projecting,W03-2401,0,\N,Missing
boxwell-white-2008-projecting,J93-2004,0,\N,Missing
boxwell-white-2008-projecting,W03-1008,0,\N,Missing
boxwell-white-2008-projecting,W06-0609,0,\N,Missing
boxwell-white-2008-projecting,P07-1032,0,\N,Missing
boxwell-white-2008-projecting,J05-1004,0,\N,Missing
boxwell-white-2008-projecting,J07-3004,0,\N,Missing
C10-2119,P02-1041,0,0.322621,"by using CCG com1034 He has a point he wants to make np sdcl 
p/np np/n n np sdcl 
p/(sto 
p) sto 
p/(sb 
p) sb 
p/np &gt; np &gt;T s/(s
p) sto 
p/np sdcl 
p/np sdcl /np &gt;B &gt;B &gt;B np
p np sdcl 
p sdcl &lt; &gt; &lt; Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [. . . ] dency relations (e.g. hA RG 0i). (Gold-standard supertags, or category labels, are also shown; see Section 2.2 for their role in hypertagging.) Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. s[dcl]
p/np have.03 &lt;TENSE&gt;pres &lt;Arg0&gt; h1 &lt;Arg1&gt; n he point h2 np &lt;NUM&gt;sg p1 &lt;Arg1&gt; &lt;GenRel&gt; &lt;Det&gt; s[dcl]
p/(s[to]
p) a1 a w1 np h3 &lt;TENSE&gt;pres &lt;Arg1&gt; &lt;Arg0&gt; np/n want.01 he m1 make.03 s[b]
p/np &lt;Arg0&gt; Figure 2: Semantic dependency graph from the CCGbank for He has a point he wants to make [. . . ], along with gold-standard supertags (category labels) binators to combine sig"
C10-2119,C00-1007,0,0.0438751,"the heterogeneous nature of English agreement. Compared to writing grammar rules, our method is more robust and allows incorporating information from diverse sources in realization. We also show that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-filter. The full model yields significant improvements in BLEU scores on Section 23 of the CCGbank and makes many fewer agreement errors. 1 Introduction In recent years a variety of statistical models for realization ranking that take syntax into account have been proposed, including generative models (Bangalore and Rambow, 2000; Cahill and van Genabith, 2006; Hogan et al., 2007; Guo et Traditionally, grammatical agreement phenomena have been modelled using hard constraints in the grammar. Taking into consideration the range of acceptable variation in the case of animacy agreement and facts about the variety of factors contributing to number agreement, the question arises: tackle agreement through grammar engineering, or via a ranking model? In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly dif"
C10-2119,boxwell-white-2008-projecting,1,0.716355,"stly support broad coverage surface realization, OpenCCG greedily assembles fragments in the event that the realizer fails to find a complete realization. To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 2. In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. hNUMisg); nodes are connected via depenFor our experiments, we use an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). Additionally, certain multi-word NEs were collapsed using underscores so that they are treated as atomic entities in the input to the realizer. To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in particular, the features and unification constraints in the categories"
C10-2119,P06-1130,0,0.124333,"Missing"
C10-2119,J07-4004,0,0.282816,"Missing"
C10-2119,P08-1022,1,0.864296,"reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in particular, the features and unification constraints in the categories related to semantically empty function words such complementizers, infinitival-to, expletive subjects, and casemarking prepositions are adjusted to reflect their purely syntactic status. 1035 2.2 A crucial component of the OpenCCG realizer is the hypertagger (Espinosa et al., 2008), or supertagger for surface realization, which uses a maximum entropy model to assign the most likely lexical categories to the predicates in the input logical form, thereby greatly constraining the realizer’s search space.1 Category label prediction is done at run-time and is based on contexts within the directed graph structure as shown in Figure 2, instead of basing category assignment on linear word and POS context as in the parsing case. 3 Feature Design The features we employ in our baseline perceptron ranking model are of three kinds. First, as in the log-linear models of Velldal & Oep"
C10-2119,C08-1038,0,0.512845,"Missing"
C10-2119,J07-3004,0,0.24498,"proposed, including generative models (Bangalore and Rambow, 2000; Cahill and van Genabith, 2006; Hogan et al., 2007; Guo et Traditionally, grammatical agreement phenomena have been modelled using hard constraints in the grammar. Taking into consideration the range of acceptable variation in the case of animacy agreement and facts about the variety of factors contributing to number agreement, the question arises: tackle agreement through grammar engineering, or via a ranking model? In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. With sufficient effort, it is conceivable that an approach incorporating hard agreement constraints could be refined to underspecify cases where variation is acceptable, but even so, one would want a ranking model to capture preferences in these cases, which might vary depending on genre, dialect or domain. Given that 1032 Coling 2010: Poster Volume, pages 1032–1040, Beijing, August 2"
C10-2119,J05-1004,0,0.0299353,"hin equivalence classes of edges. To more robustly support broad coverage surface realization, OpenCCG greedily assembles fragments in the event that the realizer fails to find a complete realization. To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 2. In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. hNUMisg); nodes are connected via depenFor our experiments, we use an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). Additionally, certain multi-word NEs were collapsed using underscores so that they are treated as atomic entities in the input to the realizer. To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in particular, the feat"
C10-2119,P04-1007,0,0.0875243,"g to our linearly interpolated language models as a single feature in the perceptron model. Since our language model linearly interpolates three component models, we also include the log prob from each component language model as a feature so that the combination of these components can be optimized. Second, we include syntactic features in our model by implementing Clark & Curran’s (2007) normal form model in OpenCCG. The features of this model are listed in Table 1; they are integer-valued, representing counts of occurrences in a derivation. Third, we include discriminative n-gram features (Roark et al., 2004), which count the occurrences of each n-gram that is scored by our factored language model, rather than a feature whose value is the log probability determined by the language model. Table 2 depicts the new animacy, agreement and punctuation features being introduced as part of this work. The next two sections describe these features in more detail. 3.1 Animacy and Number Agreement Underspecification as to the choice of pronoun in the input leads to competing realizations involving the relative pronouns who, that, which etc. The 1 Feature Type LexCat + Word LexCat + POS Rule Rule + Word Rule +"
C10-2119,2005.mtsummit-papers.15,0,0.239255,"sgow Herald; Jun 25, 2010) (2) Mr. Dorsch says the HIAA is working on a proposal to establish a privately funded reinsurance mechanism to help cover small groups that ca n’t get insurance without excluding certain employees . (WSJ0518.35) 1.2 The Heterogeneous Nature of Number Agreement Subject-verb agreement can be described as a constraint where the verb agrees with the subject in terms of agreement features (number and person). Agreement has often been considered to be a syntactic phenomenon and grammar implementations generally use syntactic features to enforce agreement constraints (e.g. Velldal and Oepen, 2005). However a closer look at our data and a survey of the theoretical linguistics literature points toward a more heterogeneous conception of English agreement. Purely syntactic accounts are problematic when the following examples are considered: (3) Five miles is a long distance to walk. (Kim, 2004) (4) King prawns cooked in chili salt and pepper was very much better, a simple dish succulently executed. (Kim, 2004) (5) “ I think it will shake confidence one more time , and a lot of this business is based on client confidence . ” (WSJ1866.10) (6) It ’s interesting to find that a lot of the expen"
C10-2119,W08-1703,1,0.94788,"ases where variation is acceptable, but even so, one would want a ranking model to capture preferences in these cases, which might vary depending on genre, dialect or domain. Given that 1032 Coling 2010: Poster Volume, pages 1032–1040, Beijing, August 2010 a ranking model is desirable in any event, we investigate here the extent to which agreement phenomena can be more robustly and simply handled using a ranking model alone, with no hard constraints in the grammar. We also show here that the perceptron model can reduce balanced punctuation errors that would otherwise require a post-filter. As White and Rajkumar (2008) discuss, in CCG it is not feasible to use features in the grammar to ensure that balanced punctuation (e.g. paired commas for NP appositives) is used in all and only the appropriate places, given the word-order flexibility that crossing composition allows. While a post-filter is a reasonably effective solution, it can be prone to search errors and does not allow balanced punctuation choices to interact with other choices made by the ranking model. The starting point for our work is a CCG realization ranking model that incorporates Clark & Curran’s (2007) normal-form syntactic model, developed"
C10-2119,D09-1043,1,0.688008,"Missing"
C10-2119,D07-1028,0,0.360324,"Missing"
C10-2119,W05-1510,0,0.466466,"Missing"
C12-2120,N03-1003,0,0.123777,"Missing"
C12-2120,J05-3002,0,0.163291,"Missing"
C12-2120,P11-2069,0,0.0517229,"Missing"
C12-2120,W07-1427,0,0.190904,"Missing"
C12-2120,N10-1066,0,0.130194,"Missing"
C12-2120,J08-4005,0,0.277879,"Missing"
C12-2120,P08-2007,0,0.106796,"Missing"
C12-2120,W11-2107,0,0.0847348,"Missing"
C12-2120,P05-1045,0,0.0123573,"Missing"
C12-2120,S12-1076,0,0.0479741,"Missing"
C12-2120,N06-1014,0,0.137372,"Missing"
C12-2120,P03-1020,0,0.0614787,"Missing"
C12-2120,D08-1084,0,0.715616,"Missing"
C12-2120,N12-1019,0,0.0968031,"Missing"
C12-2120,W05-1612,0,0.0727185,"Missing"
C12-2120,J03-1002,0,0.0184667,"Missing"
C12-2120,W04-3219,0,0.0603616,"Missing"
C12-2120,C08-1110,1,0.863732,"Missing"
C12-2120,P11-2044,1,0.864459,"Missing"
C12-2120,C96-2141,0,0.513584,"Missing"
C12-2120,W02-1001,0,\N,Missing
C12-2120,N06-1006,0,\N,Missing
C92-1040,J88-2003,0,0.0676483,"Missing"
C92-4181,P92-1016,1,0.682851,"Missing"
C92-4181,C90-2068,1,0.824336,"Missing"
C92-4181,C92-1040,1,0.866602,"Missing"
D09-1043,P02-1041,0,0.0531254,"Figure 2. In Figure 2: Semantic dependency graph from the CCGbank for He has a point he wants to make [. . . ], along with gold-standard supertags (category labels) the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. hNUMisg); nodes are connected via dependency relations (e.g. hA RG 0i). (Gold-standard supertags, or category labels, are also shown; see Section 2.4 for their role in hypertagging.) Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first"
D09-1043,J99-2004,0,0.0278813,"their semantic classes for scoring using the semantic class–replaced model, similar to Oh and Rudnicky (2002). Note that the use of supertags in the factored language model to score possible realizations is 3 Perceptron Reranking As Collins (2002) observes, perceptron training involves a simple, on-line algorithm, with few iterations typically required to achieve good performance. Moreover, averaged perceptrons—which 2 The approach has been dubbed hypertagging since it operates at a level “above” the syntax, moving from semantic representations to syntactic categories. 1 With CCG, supertags (Bangalore and Joshi, 1999) are lexical categories considered as fine-grained syntactic labels. 413 Input: training examples (xi , yi ) Initialization: set α = 0, or use optional input model Algorithm: for t = 1 . . . T , i = 1 . . . N zi = argmaxy∈GEN(xi ) Φ(xi , y) · α if zi 6= yi α = α + Φ(xi , yi ) − Φ(xi , zi ) P P Output: α = Tt=1 N i=1 αti /T N rent training section (in jack-knifed fashion) from the word-based parts of the language model, in order to make the language model scores more realistic. It remains for future work to determine whether using a different compromise between ensuring high-quality training da"
D09-1043,C00-1007,0,0.0219013,"Missing"
D09-1043,boxwell-white-2008-projecting,1,0.792453,"id Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in 411 He has a point he wants to make np sdcl 
p/np np/n n np sdcl 
p/(sto 
p) sto 
p/(sb 
p) sb 
p/np np > >T s/(s
p) sto 
p/np sdcl 
p/np sdcl /np np sdcl 
p sdcl np
p >B >B >B &lt; > &lt; Figure 1: Syntactic"
D09-1043,W05-1619,0,0.169063,"Missing"
D09-1043,N09-1025,0,0.0140435,"he use of linguistically-motivated and non-local features, a topic which we plan to investigate in future work. outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tack"
D09-1043,W07-1202,0,0.342963,"l Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log 410 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410–419, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP using a reversible, corpus-engineered grammar. The paper is organized as follows. Section 2 reviews previous work on broad coverage realization with OpenCCG. Section 3 describes our approach to realization reranking with averaged perceptron models. Section 4 presents our evaluation of the perceptron models, comparin"
D09-1043,J07-4004,0,0.74558,"l Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log 410 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410–419, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP using a reversible, corpus-engineered grammar. The paper is organized as follows. Section 2 reviews previous work on broad coverage realization with OpenCCG. Section 3 describes our approach to realization reranking with averaged perceptron models. Section 4 presents our evaluation of the perceptron models, comparin"
D09-1043,N09-2041,1,0.782189,"Missing"
D09-1043,P04-1015,0,0.0517337,"Missing"
D09-1043,C04-1097,0,0.0134808,"aces in statistical realization, including the use of linguistically-motivated and non-local features, a topic which we plan to investigate in future work. outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side)"
D09-1043,W02-1001,0,0.244336,"nerative baseline feature provides the perceptron algorithm with a much better starting point for learning. We also show that discriminative training allows the combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU (Papineni et al., 2002) score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been"
D09-1043,P06-1088,0,0.00923124,"as the current one, as shown below: A crucial component of the OpenCCG realizer is the hypertagger (Espinosa et al., 2008), or supertagger for surface realization, which uses a maximum entropy model to assign the most likely lexical categories to the predicates in the input logical form, thereby greatly constraining the realizer’s search space.2 Figure 2 shows gold-standard supertags for the lexical predicates in the graph; such category labels are predicted by the hypertagger at run-time. As in recent work on using supertagging in parsing, the hypertagger operates in a multitagging paradigm (Curran et al., 2006), where a variable number of predictions are made per input predicate. Instead of basing category assignment on linear word and POS context, however, the hypertagger predicts lexical categories based on contexts within a directed graph structure representing the logical form (LF) of the sentence to be realized. The hypertagger generalizes Bangalore and Rambow’s (2000) method of using supertags in generation by using maximum entropy models with a larger local context. During realization, the hypertagger returns a βbest list of supertags in order of decreasing probability. Increasing the number"
D09-1043,P04-1007,0,0.156359,"Missing"
D09-1043,P08-1022,1,0.917746,"bank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead of 4-grams suggested as a possible explanation). With these mixed results, the utility of n-gram baseline features for PTBscale discriminative realization ranking has been unclear. In our particular setting, the question is: Do n-gram log prob features improve performance in broad coverage realization ranking with CCG, where factored language models over words, partof-speech tags and supertags have previously been employed (White et al., 2007; Espinosa et al., 2008)? This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG. The paper confirms the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU score of 0.8506 on S"
D09-1043,N09-2057,0,0.0651012,"lization, including the use of linguistically-motivated and non-local features, a topic which we plan to investigate in future work. outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tack"
D09-1043,C08-1038,0,0.278487,"Missing"
D09-1043,2005.mtsummit-papers.15,0,0.23262,"iminative training allows the combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU (Papineni et al., 2002) score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. A"
D09-1043,J07-3004,0,0.0401555,"ndard supertags, or category labels, are also shown; see Section 2.4 for their role in hypertagging.) Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in 411 He has a point he wants to make np sdcl 
p/np np/n n np sdcl 
p"
D09-1043,D07-1080,0,0.0168947,"set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. With progress on discriminative training with large numbers of features in syntax-based MT, the features found to be useful for high-quality surface realization may become increasingly relevant for MT as well. 6 Acknowledgements This work was supported in part by NSF grant IIS0812297 and by an allocation of computing time from"
D09-1043,D07-1028,0,0.236789,"Missing"
D09-1043,P08-1067,0,0.196844,"ory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the question of whether it is beneficial to incorporate n-gram log 410 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410–419, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP using a reversible, corpus-engineered grammar. The paper is organized as follows. Section 2 reviews previous work on broad coverage realization with OpenCCG. Section 3 describes our approach to realization reranking with averaged perceptron models. Section 4 presents our evaluation of the per"
D09-1043,W08-1703,1,0.931277,"between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in 411 He has a point he wants to make np sdcl 
p/np np/n n np sdcl 
p/(sto 
p) sto 
p/(sb 
p) sb 
p/np np > >T s/(s
p) sto 
p/np sdcl 
p/np sdcl /np np sdcl 
p sdcl np
p >B >B >B &lt; > &lt; Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [. . . ] 2331 named entities (NEs) annotated by the BBN corpus (Weischedel and Brunstein, 2005), 238 were not realized correctly. For example, multiword NPs like Texas Instruments Japan Ltd. were realized as Japan Texa"
D09-1043,W02-2103,0,0.0633862,"Missing"
D09-1043,W05-1510,0,0.0819947,"combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU (Papineni et al., 2002) score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron models also perform well for realization ranking with CCG. Averaged perceptron models are very simple, just requiring a decoder and a simple update function, yet despite their simplicity they have been shown to achieve state-of-the-art results in Treebank and CCG parsing (Huang, 2008; Clark and Curran, 2007a) as well as on other NLP tasks. Along the way, we address the"
D09-1043,2007.mtsummit-ucnlg.4,1,0.936161,"ce; on the Penn Treebank (PTB), however, Nakanishi et al. found that including an n-gram log prob feature in their model was of no benefit (with the use of bigrams instead of 4-grams suggested as a possible explanation). With these mixed results, the utility of n-gram baseline features for PTBscale discriminative realization ranking has been unclear. In our particular setting, the question is: Do n-gram log prob features improve performance in broad coverage realization ranking with CCG, where factored language models over words, partof-speech tags and supertags have previously been employed (White et al., 2007; Espinosa et al., 2008)? This paper shows that discriminative reranking with an averaged perceptron model yields substantial improvements in realization quality with CCG. The paper confirms the utility of including language model log probabilities as features in the model, which prior work on discriminative training with log linear models for HPSG realization had called into question. The perceptron model allows the combination of multiple n-gram models to be optimized and then augmented with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art B"
D09-1043,J05-1004,0,0.0295549,"rnally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents are modeled as modal relations. 2.2 Realization from an Enhanced CCGbank Our starting point is an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007)—a corpus of CCG derivations derived from the Penn Treebank—with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). To engineer a grammar from this corpus suitable for realization with OpenCCG, the derivations are first revised to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG (White and Rajkumar, 2008). Further changes are necessary to support semantic dependencies rather than surface syntactic ones; in 411 He has a point he wants to make np sdcl 
p/np np/n n np sdcl 
p/(sto 
p) sto 
p/(sb 
p) sb 
p/np np > >T s/(s
p) sto 
p/np sdcl 
p/np sdcl /np np sdcl"
D09-1043,N09-2058,0,0.0407952,"inguistically-motivated and non-local features, a topic which we plan to investigate in future work. outputs. Nevertheless, his high scores do suggest the potential for precise grammar engineering to improve realization quality. While we have yet to perform a thorough error analysis, our impression is that although the current set of syntactic features substantially improves clausal constituent ordering, a variety of disfluent cases remain. More thorough investigations of features for constituent ordering in English have been performed by Ringger et al. (2004), Filippova and Strube (2009) and Zhong and Stent (2009), all of whom develop classifiers for determining linear order. In future work, we plan to investigate whether features inspired by these approaches can be usefully integrated into our perceptron reranker. Also related to the present work is discriminative training in syntax-based MT (Turian et al., 2007; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009). Not surprisingly, since MT is a harder problem than surface realization, syntaxbased MT systems have made use of less precise grammars and more impoverished (target-side) feature sets than those tackling realization ranking. W"
D09-1043,P02-1040,0,0.0949097,"ed without these features performed worse than the generative baseline. These findings are in line with Collins & Roark’s (2004) results with incremental parsing with perceptrons, where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning. We also show that discriminative training allows the combination of multiple n-gram models to be optimized, and that the best model augments the n-gram log prob features with both syntactic features and discriminative n-gram features. The full model yields a stateof-the-art BLEU (Papineni et al., 2002) score of 0.8506 on Section 23 of the CCGbank, which is to our knowledge the best score reported to date In this paper, we show how discriminative training with averaged perceptron models (Collins, 2002) can be used to substantially improve surface realization with Combinatory Categorial Grammar (Steedman, 2000, CCG). Velldal and Oepen (2005) and Nakanishi et al. (2005) have shown that discriminative training with log-linear (maximum entropy) models is effective in realization ranking with Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG). Here we show that averaged perceptron"
D09-1043,N03-2002,0,\N,Missing
D09-1043,P08-1024,0,\N,Missing
D10-1055,W05-0909,0,0.108672,"d to transform a hypothesis sentence into the reference sentence (Snover et al., 2006) Human judgments Automatic evaluation The realizations were also evaluated using seven automatic metrics: • IBM’s BLEU, which scores a hypothesis by counting n-gram matches with the reference sentence (Papineni et al., 2001), with smoothing as described in (Lin and Och, 2004) • The NIST n-gram evaluation metric, similar to BLEU , but rewarding rarer n-gram matches, and using a different length penalty • METEOR, which measures the harmonic mean of unigram precision and recall, with a higher weight for recall (Banerjee and Lavie, 2005) 567 • TERP, an augmented version of TER which performs phrasal substitutions, stemming, and checks for synonyms, among other improvements (Snover et al., 2009) • TERPA, an instantiation of TERP with edit weights optimized for correlation with adequacy in MT evaluations • GTM (General Text Matcher), a generalization of the F-measure that rewards contiguous matching spans (Turian et al., 2003) Additionally, targeted versions of BLEU, ME and GTM were computed by using the human-repaired outputs as the reference set. The human repair was different from the reference sentence in 193 cases (about 9"
D10-1055,N03-1003,0,0.0136725,"ch et al., 2006). BLEU is designed to work with multiple reference sentences, but in treebank realization, there is only a single reference sentence available for comparison. A few other studies have investigated the use of such metrics in evaluating the output of NLG systems, notably (Reiter and Belz, 2009) and (Stent et al., 2005). The former examined the performance of BLEU and ROUGE with computer-generated weather reports, finding a moderate correlation with human fluency judgments. The latter study applied several MT metrics to paraphrase data from Barzilay and Lee’s corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. Cahill (2009) examined the performance of six MT metrics (including BLEU) in evaluating the output of a LFG-based surface realizer for 564 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564–574, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics German, also finding only weak correlations with the human judgments. To study the usefulness of evaluation metrics such as BLEU on the output of grammar"
D10-1055,P09-2025,0,0.377314,"ence sentence available for comparison. A few other studies have investigated the use of such metrics in evaluating the output of NLG systems, notably (Reiter and Belz, 2009) and (Stent et al., 2005). The former examined the performance of BLEU and ROUGE with computer-generated weather reports, finding a moderate correlation with human fluency judgments. The latter study applied several MT metrics to paraphrase data from Barzilay and Lee’s corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. Cahill (2009) examined the performance of six MT metrics (including BLEU) in evaluating the output of a LFG-based surface realizer for 564 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564–574, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics German, also finding only weak correlations with the human judgments. To study the usefulness of evaluation metrics such as BLEU on the output of grammar-based surface realizers used with the PTB, we assembled a corpus of surface realizations from three different realizers oper"
D10-1055,E06-1032,0,0.134948,"her the metrics developed for MT evaluation tasks can be used to reliably evaluate the outputs of surface realizers, and which of these metrics are best suited to this task. A number of surface realizers have been developed using the Penn Treebank (PTB), and BLEU scores are often reported in the evaluations of these systems. But how useful is BLEU in this context? The original BLEU study (Papineni et al., 2001) scored MT outputs, which are of generally lower quality than grammar-based surface realizations. Furthermore, even for MT systems, the usefulness of BLEU has been called into question (Callison-Burch et al., 2006). BLEU is designed to work with multiple reference sentences, but in treebank realization, there is only a single reference sentence available for comparison. A few other studies have investigated the use of such metrics in evaluating the output of NLG systems, notably (Reiter and Belz, 2009) and (Stent et al., 2005). The former examined the performance of BLEU and ROUGE with computer-generated weather reports, finding a moderate correlation with human fluency judgments. The latter study applied several MT metrics to paraphrase data from Barzilay and Lee’s corpus-based system (Barzilay and Lee"
D10-1055,W07-0718,0,0.374489,"izer systems on average, and the best-rated realizer systems achieved mean fluency scores above 4. 4.2 Inter-annotator agreement Inter-annotator agreement was measured using the κ-coefficient, which is commonly used to measure the extent to which annotators agree in category (E) judgment tasks. κ is defined as P (A)−P 1−P (E) , where P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996). Chance agreement for this data is calculated by the method discussed in Carletta’s squib. However, in previous work in MT meta-evaluation, Callison-Burch et al. (2007), assume the less strict criterion of uniform chance agreement, i.e. 51 for a five-point scale. They also Score 5 4 3 2 1 Adequacy All the meaning of the reference Most of the meaning Much of the meaning Meaning substantially different Meaning completely different Fluency Perfectly grammatical Awkward or non-native; punctuation errors Agreement errors or minor syntactic problems Major syntactic problems, such as missing words Completely ungrammatical Figure 1: Rating scale and guidelines Ref. Realiz. Repair It wasn’t clear how NL and Mr. Simmons would respond if Georgia Gulf spurns them again"
D10-1055,C08-1013,0,0.0203422,"at statistically significant system-level differences in human judgments, we found that some of the metrics get some of the rankings correct, but none get them all correct, with different metrics making different ranking errors. This suggests that multiple metrics should be routinely consulted when comparing realizer systems. Overall, our methodology is similar to that of previous MT meta-evaluations, in that we collected human judgments of system outputs, and compared these scores with those assigned by automatic metrics. A recent alternative approach to paraphrase evaluation is ParaMetric (Callison-Burch et al., 2008); however, it requires a corpus of annotated (aligned) paraphrases (which does not yet exist for PTB data), and is arguably focused more on paraphrase analysis than paraphrase generation. The plan of the paper is as follows: Section 2 discusses the preparation of the corpus of surface realizations. Section 3 describes the human evaluation task and the automated metrics applied. Sections 4 and 5 present and discuss the results of these evaluations. We conclude with some general observations about automatic evaluation of surface realizers, and some directions for further research. 565 2 Data Pre"
D10-1055,J96-2004,0,0.0117081,"well as the mean adequacy and fluency scores garnered from the human evaluation. Overall adequacy and fluency judgments were high (4.16, 3.63) for the realizer systems on average, and the best-rated realizer systems achieved mean fluency scores above 4. 4.2 Inter-annotator agreement Inter-annotator agreement was measured using the κ-coefficient, which is commonly used to measure the extent to which annotators agree in category (E) judgment tasks. κ is defined as P (A)−P 1−P (E) , where P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996). Chance agreement for this data is calculated by the method discussed in Carletta’s squib. However, in previous work in MT meta-evaluation, Callison-Burch et al. (2007), assume the less strict criterion of uniform chance agreement, i.e. 51 for a five-point scale. They also Score 5 4 3 2 1 Adequacy All the meaning of the reference Most of the meaning Much of the meaning Meaning substantially different Meaning completely different Fluency Perfectly grammatical Awkward or non-native; punctuation errors Agreement errors or minor syntactic problems Major syntactic problems, such as missing words C"
D10-1055,W04-3250,0,0.100207,"Missing"
D10-1055,2006.amta-papers.25,0,0.0255688,"Figure 2. These repairs resulted in new reference sentences for a substantial number of sentences. These repaired realizations were later used to calculate targeted versions of the evaluation metrics, i.e., using the repaired sentence as the reference sentence. Although targeted metrics are not fully automatic, they are of interest because they allow the evaluation algorithm to focus on what is actually wrong with the input, rather than all textual differences. Notably, targeted TER ( HTER ) has been shown to be more consistent with human judgments than human annotators are with one another (Snover et al., 2006). 3.2 • TER (Translation Edit Rate), a measure of the number of edits required to transform a hypothesis sentence into the reference sentence (Snover et al., 2006) Human judgments Automatic evaluation The realizations were also evaluated using seven automatic metrics: • IBM’s BLEU, which scores a hypothesis by counting n-gram matches with the reference sentence (Papineni et al., 2001), with smoothing as described in (Lin and Och, 2004) • The NIST n-gram evaluation metric, similar to BLEU , but rewarding rarer n-gram matches, and using a different length penalty • METEOR, which measures the har"
D10-1055,W09-0441,0,0.015727,"nk correlation coefficient between these scores and each of the metrics. Spearman’s correlation makes fewer assumptions about the distribution of the data, but may not reflect a linear rela568 tionship that is actually present. Both are frequently reported in the literature. Due to space constraints, we show only Spearman’s correlation, although the TER family scored slightly better on Pearson’s coefficient, relatively. The results for Spearman’s correlation are given in Table 3. Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al., 2009), and the Spearman’s correlation of each of the automatic metrics with these scores are given in Table 4. All reported correlations are significant at p &lt; 0.001. 4.4 Bootstrap sampling of correlations For each of the sub-corpora shown in Table 1, we computed confidence intervals for the correlations between adequacy and fluency human scores with selected automatic metrics (BLEU, HBLEU, TER, TERP, and HTER ) as described in (Koenh, 2004). We sampled each sub-corpus 1000 times with replacement, and calculated correlations between the rankings induced by the human scores and those induced by the"
D10-1055,2003.mtsummit-papers.51,0,0.111174,"Missing"
D10-1055,D09-1043,1,0.889154,"Missing"
D10-1055,J09-4008,0,\N,Missing
D10-1055,P02-1040,0,\N,Missing
D10-1055,C04-1072,0,\N,Missing
D10-1055,P02-1035,0,\N,Missing
D12-1023,P02-1041,0,0.0350538,"rks and verbs, and are capped at 3, 3 and 2, respectively categorization as well as syntactic features (e.g. number and agreement). OpenCCG is a parsing/generation library which includes a hybrid symbolic-statistical chart realizer (White, 2006; White and Rajkumar, 2009). The input to the OpenCCG realizer is a semantic graph, where each node has a lexical predication and a set of semantic features; nodes are connected via dependency relations. Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). Alternative realizations are ranked using integrated ngram or averaged perceptron scoring models. In the experiments reported below, the inputs are derived from the gold standard derivations in the CCGbank (Hockenmaier and Steedman, 2007), and the outputs are the highest-scoring realizations found during the realizer’s chart-based search.1 3 Feature Design In the realm of paraphrasing using tree linearization, Kempen and Harbusch (2004) explore features which have later been appropriated into classification approaches for surface realization (Filippova and Strube, 2007). Prominent features i"
D12-1023,W11-2832,1,0.857923,"r is the first to examine the impact of dependency length minimization on realization ranking. While there have been quite a few papers to date reporting results on Penn Treebank data, since the various systems make different assumptions regarding the specificity of their inputs, all but the most broad-brushed comparisons remain impossible at present, and thus detailed studies such as the present one can only be made within the context of different models for the same system. Some progress on this issue has been made in the context of the Generation Challenges Surface Realization Shared Task (Belz et al., 2011), but it remains to be seen to what extent fair cross-system comparisons using common inputs can be achieved. For (very) rough comparison purposes, Table 13 lists our results in the context of those reported for various other systems on PTB Section 23. As the table shows, the OpenCCG scores are quite competitive, exceeded only by Callaway’s (2005) extensively hand-crafted system as well as Bohnet et al.’s (2011) system on shared task shallow inputs (-S), which performs much better than their system on deep inputs (-D) that more closely resemble OpenCCG’s. 6 Conclusions In this paper, we have i"
D12-1023,W11-2835,0,0.125674,"Missing"
D12-1023,P09-1092,0,0.0188112,"e features which have later been appropriated into classification approaches for surface realization (Filippova and Strube, 2007). Prominent features include in1 The realizer can also be run using inputs derived from OpenCCG’s parser, though informal experiments suggest that parse errors tend to decrease generation quality. 247 formation status, animacy and phrase length. In the case of ranking models for surface realization, by far the most comprehensive experiments involving linguistically motivated features are reported in work of Cahill for German realization ranking (Cahill et al., 2007; Cahill and Riester, 2009). Apart from language model and Lexical Functional Grammar (LFG) c-structure and f -structure based features, Cahill also designed and incorporated features modeling information status considerations. The feature sets explored in this paper extend those in previous work on realization ranking with OpenCCG using averaged perceptron models (White and Rajkumar, 2009; Rajkumar et al., 2009; Rajkumar and White, 2010) to include more comprehensive ordering features. The feature classes are listed below, where DEPLEN, H OCKENMAIER and DEPORD are novel, and the rest are as in earlier OpenCCG models. T"
D12-1023,W05-1619,0,0.0655615,"Missing"
D12-1023,J07-4004,0,0.105498,"Missing"
D12-1023,P07-1041,0,0.104181,"linguistic meaning (Baldridge and Kruijff, 2002). Alternative realizations are ranked using integrated ngram or averaged perceptron scoring models. In the experiments reported below, the inputs are derived from the gold standard derivations in the CCGbank (Hockenmaier and Steedman, 2007), and the outputs are the highest-scoring realizations found during the realizer’s chart-based search.1 3 Feature Design In the realm of paraphrasing using tree linearization, Kempen and Harbusch (2004) explore features which have later been appropriated into classification approaches for surface realization (Filippova and Strube, 2007). Prominent features include in1 The realizer can also be run using inputs derived from OpenCCG’s parser, though informal experiments suggest that parse errors tend to decrease generation quality. 247 formation status, animacy and phrase length. In the case of ranking models for surface realization, by far the most comprehensive experiments involving linguistically motivated features are reported in work of Cahill for German realization ranking (Cahill et al., 2007; Cahill and Riester, 2009). Apart from language model and Lexical Functional Grammar (LFG) c-structure and f -structure based feat"
D12-1023,N09-2057,0,0.174975,"Missing"
D12-1023,P07-1024,0,0.176337,"is the class of examples involving pre-modifying adjunct sequences that precede both the subject and the verb. Assuming that their parent head is the main verb of the sentence, a longshort sequence would minimize overall dependency length. However, in 613 examples found in the Penn Treebank, the average length of the first adjunct was 3.15 words while the second adjunct was 3.48 words long, thus reflecting a short-long pattern, as illustrated in the Temperley p.c. example in Table 2. Apart from these, Hawkins (2001) shows that arguments are generally located closer to the verb than adjuncts. Gildea and Temperley (2007) also suggest 245 that adverb placement might involve cases which go against dependency length minimization. An examination of 295 legitimate long-short post-verbal constituent orders (counter to dependency length) from Section 00 of the Penn Treebank revealed that temporal adverb phrases are often involved in long-short orders, as shown in wsj 0075.13 in Table 2. In our setup, the preference to minimize dependency length can be balanced by features capturing preferences for alternate choices (e.g. the argument-adjunct distinction in our dependency ordering model, Table 4). Via distributional"
D12-1023,C08-1038,0,0.213477,"Missing"
D12-1023,J07-3004,0,0.0106462,"e, 2006; White and Rajkumar, 2009). The input to the OpenCCG realizer is a semantic graph, where each node has a lexical predication and a set of semantic features; nodes are connected via dependency relations. Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). Alternative realizations are ranked using integrated ngram or averaged perceptron scoring models. In the experiments reported below, the inputs are derived from the gold standard derivations in the CCGbank (Hockenmaier and Steedman, 2007), and the outputs are the highest-scoring realizations found during the realizer’s chart-based search.1 3 Feature Design In the realm of paraphrasing using tree linearization, Kempen and Harbusch (2004) explore features which have later been appropriated into classification approaches for surface realization (Filippova and Strube, 2007). Prominent features include in1 The realizer can also be run using inputs derived from OpenCCG’s parser, though informal experiments suggest that parse errors tend to decrease generation quality. 247 formation status, animacy and phrase length. In the case of r"
D12-1023,D07-1028,0,0.0412899,"Missing"
D12-1023,W04-3250,0,0.107295,"Missing"
D12-1023,W02-2103,0,0.217739,"Missing"
D12-1023,W05-1510,0,0.0991004,"Missing"
D12-1023,P02-1040,0,0.104472,"Missing"
D12-1023,C10-2119,1,0.860831,"e realization, by far the most comprehensive experiments involving linguistically motivated features are reported in work of Cahill for German realization ranking (Cahill et al., 2007; Cahill and Riester, 2009). Apart from language model and Lexical Functional Grammar (LFG) c-structure and f -structure based features, Cahill also designed and incorporated features modeling information status considerations. The feature sets explored in this paper extend those in previous work on realization ranking with OpenCCG using averaged perceptron models (White and Rajkumar, 2009; Rajkumar et al., 2009; Rajkumar and White, 2010) to include more comprehensive ordering features. The feature classes are listed below, where DEPLEN, H OCKENMAIER and DEPORD are novel, and the rest are as in earlier OpenCCG models. The inclusion of the DE PORD features is intended to yield a model with a similarly rich set of ordering features as Cahill and Forster’s (2009) realization ranking model for German. Except where otherwise indicated, features are integer-valued, representing counts of occurrences in a derivation. The total of the length between all semantic heads and dependents for a realization, where length is in intervening wo"
D12-1023,N09-2041,1,0.842339,"nking models for surface realization, by far the most comprehensive experiments involving linguistically motivated features are reported in work of Cahill for German realization ranking (Cahill et al., 2007; Cahill and Riester, 2009). Apart from language model and Lexical Functional Grammar (LFG) c-structure and f -structure based features, Cahill also designed and incorporated features modeling information status considerations. The feature sets explored in this paper extend those in previous work on realization ranking with OpenCCG using averaged perceptron models (White and Rajkumar, 2009; Rajkumar et al., 2009; Rajkumar and White, 2010) to include more comprehensive ordering features. The feature classes are listed below, where DEPLEN, H OCKENMAIER and DEPORD are novel, and the rest are as in earlier OpenCCG models. The inclusion of the DE PORD features is intended to yield a model with a similarly rich set of ordering features as Cahill and Forster’s (2009) realization ranking model for German. Except where otherwise indicated, features are integer-valued, representing counts of occurrences in a derivation. The total of the length between all semantic heads and dependents for a realization, where"
D12-1023,C04-1097,0,0.0496478,"Missing"
D12-1023,2005.mtsummit-papers.15,0,0.14135,"Missing"
D12-1023,D09-1043,1,0.906905,"np sdcl 
p, boughti hbought, sdcl → np sdcl 
pi + dw hVBD, sdcl → np sdcl 
pi + dw hbought, sdcl → np sdcl 
pi + dp hVBD, sdcl → np sdcl 
pi + dp hbought, sdcl → np sdcl 
pi + dv hVBD, sdcl → np sdcl 
pi + dv Table 3: Basic and dependency features from Clark & Curran’s (2007) normal form model; distances are in intervening words, punctuation marks and verbs, and are capped at 3, 3 and 2, respectively categorization as well as syntactic features (e.g. number and agreement). OpenCCG is a parsing/generation library which includes a hybrid symbolic-statistical chart realizer (White, 2006; White and Rajkumar, 2009). The input to the OpenCCG realizer is a semantic graph, where each node has a lexical predication and a set of semantic features; nodes are connected via dependency relations. Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). Alternative realizations are ranked using integrated ngram or averaged perceptron scoring models. In the experiments reported below, the inputs are derived from the gold standard derivations in the CCGbank (Hockenmaier and Steedman, 2007), a"
D19-6309,P19-1080,1,0.898936,"nly submit complete results (including linearization) for English. Preliminary linearization results were decent, with a small benefit from reranking to prefer valid output trees, but inadequate control over the words in the output led to poor quality on longer sentences. 1 Introduction With our entry in the shallow surface realization shared task, we aimed to (1) implement an up-to-date morphological inflection model based on the approach of Faruqui et al. (2016) and Kann and Schütze (2016), and (2) conduct exploratory experiments with linearization using the constrained decoding approach of Balakrishnan et al. (2019) adapted to dependency trees. Our system is a pipeline that begins by generating inflected wordforms from uninflected terminals in the tree using character seq2seq models. We then serialize these inflected syntactic trees as constituent trees by converting the relations to non-terminals. The serialized constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders), whose outputs also contain tokens 2 Inflection Our pipeline begins by producing fully inflected word forms from the citation forms pro1 To handle non-projective cases, the arc-lifting method of"
D19-6309,D12-1085,0,0.0331118,"adapted to dependency trees. Our system is a pipeline that begins by generating inflected wordforms from uninflected terminals in the tree using character seq2seq models. We then serialize these inflected syntactic trees as constituent trees by converting the relations to non-terminals. The serialized constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders), whose outputs also contain tokens 2 Inflection Our pipeline begins by producing fully inflected word forms from the citation forms pro1 To handle non-projective cases, the arc-lifting method of Bohnet et al. (2012) could be applied as a preprocessing step. 68 Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019), pages 68–74 c Hong Kong, China, November 3rd, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 vided in the UD input. In a sense, at this stage, the system has to be able to perform the wug test (Berko, 1958): having never seen a word before, we need to have the ability to produce the correct form for a given paradigm cell. We utilize sequence-to-sequence models (Bahdanau et al., 2014) in keeping with previous successful approaches (K"
D19-6309,K17-2001,0,0.0432931,"Missing"
D19-6309,N16-1077,0,0.193436,"quenceto-sequence models on serialized trees. Results for morphological inflection were competitive across languages. Due to time constraints, we could only submit complete results (including linearization) for English. Preliminary linearization results were decent, with a small benefit from reranking to prefer valid output trees, but inadequate control over the words in the output led to poor quality on longer sentences. 1 Introduction With our entry in the shallow surface realization shared task, we aimed to (1) implement an up-to-date morphological inflection model based on the approach of Faruqui et al. (2016) and Kann and Schütze (2016), and (2) conduct exploratory experiments with linearization using the constrained decoding approach of Balakrishnan et al. (2019) adapted to dependency trees. Our system is a pipeline that begins by generating inflected wordforms from uninflected terminals in the tree using character seq2seq models. We then serialize these inflected syntactic trees as constituent trees by converting the relations to non-terminals. The serialized constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders), whose outputs also contain tokens 2"
D19-6309,W18-3605,1,0.90915,"linearizations are projective.1 While we found that this validity checking step provided a small benefit, fully adapting the constrained decoding approach to dependency trees would have required adding a step to ensure that all and only the input words appeared in the output tree, and enforcing these constraints during beam search. Due to time constraints, however, we were only able to obtain preliminary linearization results for English without these word-level checks. Development results for morphological inflection were competitive across languages as compared to previous implementations (King and White, 2018; Puzikov and Gurevych, 2018). With linearization, the preliminary results were decent, but showed substantial degradation for longer sentences where problems with lack of control over the output words became more severe. In the rest of the paper, we describe our inflection and linearization components in more detail, along with our experimental results. We describe our exploratory system for the shallow surface realization task, which combines morphological inflection using character sequence-to-sequence models with a baseline linearizer that implements a tree-to-tree model using sequenceto-s"
D19-6309,P17-1183,0,0.0538129,"Missing"
D19-6309,D19-6301,0,0.0271783,"gains obtained by doing validity reranking (again with gold inflected forms); here the scores shown are calculated without non-terminals. Given our time constraints, we only submitted English results for evaluation. Although we generated inflected forms for all languages in the T1 task, we could only obtain linearization results for English. Our results are decent (with the exception of the en_partutud-test dataset), suggesting that the approach may represent a viable starting point for future work. In particular, in the human evaluation results for English in the shared task overview paper (Mille et al., 2019), our system was ranked in the middle group of systems for meaning preservation and in the large group of systems tied for third–twelfth place in readability. Consistent with the human evaluation, the automatic scores for our system (Table 5) were also in the middle of the pack. Note that the test scores are lower than the dev scores at least in part because only the former are calculated with generated inflected forms. Results We picked the approach that gave the best performance on dev set. We combined samples of all English train sets, training on all sets together gave better dev BLEU scor"
D19-6309,W18-3602,0,0.0154283,"ojective.1 While we found that this validity checking step provided a small benefit, fully adapting the constrained decoding approach to dependency trees would have required adding a step to ensure that all and only the input words appeared in the output tree, and enforcing these constraints during beam search. Due to time constraints, however, we were only able to obtain preliminary linearization results for English without these word-level checks. Development results for morphological inflection were competitive across languages as compared to previous implementations (King and White, 2018; Puzikov and Gurevych, 2018). With linearization, the preliminary results were decent, but showed substantial degradation for longer sentences where problems with lack of control over the output words became more severe. In the rest of the paper, we describe our inflection and linearization components in more detail, along with our experimental results. We describe our exploratory system for the shallow surface realization task, which combines morphological inflection using character sequence-to-sequence models with a baseline linearizer that implements a tree-to-tree model using sequenceto-sequence models on serialized"
D19-6309,W19-8611,1,0.841625,"anguage. Figure 2: Example of serialized tree representation used for linearization. 70 Model Seq2Seq Tree2Seq Seq2Seq-Copy and finally a closing-bracket indicating the end of the non-terminal’s span, as exemplified in Figure 2 shows an example of serialized inputs and outputs. We experimented with three different variants of sequence-to-sequence models: Seq2Seq: Simple encoder-decoder model with attention (Bahdanau et al., 2014). Both the encoder and decoder are LSTMs. Tree2Seq: Similar to Seq2Seq, but we use a variant of the N-ary tree-LSTM (Tai et al., 2015) as the encoder, as described in Rao et al. (2019), thereby potentially taking better advantage of the input tree structure. Seq2Seq-Copy: Seq2Seq model with a pointer-generator mechanism (See et al., 2017) for copying tokens from input. The decoder can choose to either generate a word from the vocabulary or copy an input token instead. We did not have an off-the-shelf implementation for a Tree2Seq-Copy model, though our experiments suggest it would be worth developing one. Additionally, we also experimented with constrained decoding (Balakrishnan et al., 2019) with each of the above model. Using this method, in each step of beam search, we c"
D19-6309,P17-1099,0,0.0326043,"dicating the end of the non-terminal’s span, as exemplified in Figure 2 shows an example of serialized inputs and outputs. We experimented with three different variants of sequence-to-sequence models: Seq2Seq: Simple encoder-decoder model with attention (Bahdanau et al., 2014). Both the encoder and decoder are LSTMs. Tree2Seq: Similar to Seq2Seq, but we use a variant of the N-ary tree-LSTM (Tai et al., 2015) as the encoder, as described in Rao et al. (2019), thereby potentially taking better advantage of the input tree structure. Seq2Seq-Copy: Seq2Seq model with a pointer-generator mechanism (See et al., 2017) for copying tokens from input. The decoder can choose to either generate a word from the vocabulary or copy an input token instead. We did not have an off-the-shelf implementation for a Tree2Seq-Copy model, though our experiments suggest it would be worth developing one. Additionally, we also experimented with constrained decoding (Balakrishnan et al., 2019) with each of the above model. Using this method, in each step of beam search, we check for and remove candidates whose tree structures deviate from that of the input tree. The constraints include ensuring that a parent node only accepts v"
D19-6309,P15-1150,0,0.114038,"Missing"
E93-1048,J86-2003,0,0.0486726,"Missing"
E93-1048,P88-1012,0,0.0959127,"Missing"
E93-1048,P91-1008,0,0.029702,"Missing"
E93-1048,J88-2003,0,0.547822,"Missing"
E93-1048,P92-1030,0,\N,Missing
H01-1054,H01-1065,0,0.0170363,"ocuments into a coherent event-oriented view, though considerable challenges remain to be addressed in this area. The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al. [7] and Radev et al. [11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary. It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set. As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization. Having also noted this problem, we have focused our efforts in this area on attempting to balance coherence and informativeness in selecting sets of sentences to include in the summary. In ongoing work, we are investigating techniques for improving merging accuracy and summary fluency in the context of summarizing the more than 150 news articles we have collecte"
H01-1054,W00-0405,0,0.0152076,"ortant difference is that SUMMONS sidestepped the problem of comparing reported numbers of varying specificity (e.g. several thousand vs. anywhere from 2000 to 5000 vs. up to 4000 vs. 5000), whereas we have implemented rules for doing so. Finally, we have begun to address some of the difficult issues that arise in merging information from multiple documents into a coherent event-oriented view, though considerable challenges remain to be addressed in this area. The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al. [7] and Radev et al. [11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary. It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set. As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization. Having also n"
H01-1054,J93-2004,0,0.0355149,"o template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89). The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text. The RIPTIDES system for the most part employs a traditional IE architecture [4]. In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations. A preprocessor first finds sentences and tokens. For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems. Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document. The extraction phase of the system identifies domain-specific relations among relevant entities in the text. It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns. Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training — one set comprises texts relevant to the domain of"
H01-1054,J98-3005,0,0.690231,"earch efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques. This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization. (RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization.) Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques. In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system. We conclude with a brief discussion of related and ongoing work. 2. SYSTEM DESIGN Figure 1 depicts the IE-supported summarization system. The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario te"
H01-1054,W98-1428,1,0.601029,"as however, nevertheless, etc. We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid. Figure 4. 200 word summary of actual IE output, with emphasis on Red Cross cases. We then perform a randomized local search for a good set of sentences according to these scoring criteria. 2.2.4 Implementation The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex’s Exemplars Framework [13]. The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations. In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a “rough draft” of the summary text. In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text. In the second generation step, the Exemplars text polishing component makes use of this markup to trigger s"
H01-1054,A00-2018,0,\N,Missing
J10-2001,P02-1041,0,0.189312,"Missing"
J10-2001,C00-1007,0,0.0839111,"extended to apply across turns as well. 172 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also a very cheap ﬂight on Air France, whereas in other cases it typically precedes the verb, as in I also have a ﬂight that leaves London at 3:45 p.m. It al"
J10-2001,W03-2123,0,0.0691506,"re ARE seats in business class)theme (on the British Airways ﬂight)rheme (that arrives at four twenty p.m.)rheme , where the division of the sentence into theme and rheme phrases is shown informally using parentheses, and contrastive emphasis (on ARE) is shown using small caps. (See Section 2.6.2 for details of how emphasis and phrasing are realized by pitch accents and edge tones.) 2.2 Architecture The architecture of the FLIGHTS generator appears in Figure 3. OAA (Martin, Cheyer, and Moran 1999) serves as a communications hub, with the following agents responsible for speciﬁc tasks: DIPPER (Bos et al. 2003) for dialogue management; a Java agent that implements an additive multi-attribute value function (AMVF), a decision-theoretic model of the user’s preferences (Carenini and Moore 2000, 2006), for user modeling; OPlan (Currie and Tate 1991) for content planning; Xalan XSLT5 and OpenCCG (White 5 http://xml.apache.org/xalan-j/. 163 Computational Linguistics Volume 36, Number 2 Figure 3 FLIGHTS generation architecture. 2004, 2006a, 2006b) for sentence planning and surface realization; and Festival (Taylor, Black, and Caley 1998) for speech synthesis. The user modeling, content planning, sentence p"
J10-2001,W05-0307,0,0.0391251,"Missing"
J10-2001,W00-1407,1,0.657912,"or sequential presentation. In particular, we require better algorithms for: 1. selecting the most relevant subset of options to mention, as well as the attributes that are most relevant to choosing among them; and 2. determining how to organize and express the descriptions of the selected options and attributes, in ways that are both easy to understand and memorable.1 In this article, we describe how we have addressed these points in the FLIGHTS2 system, reviewing and extending the description given in Moore et al. (2004). FLIGHTS follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore 2000; Walker et al. 2002) in applying decision-theoretic models of user preferences to the generation of tailored descriptions of the most relevant available options. Multiattribute decision theory provides a detailed account of how models of user preferences can be used in decision making (Edwards and Barron 1994). Such preference models have been shown to enable systems to present information in ways that are concise and 1 An issue we do not address in this article is whether a multimodal system would be more effective than a voice-only one. We believe that these needs, and in particular the nee"
J10-2001,I05-1015,0,0.0310471,"istical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also a very cheap ﬂight on Air France, whereas in other cases it typically precedes the verb, as in I also have a ﬂight that leaves London at 3:45 p.m. It also enforces subject–verb agreement, for example, between is and ﬂight, or between are and seats. Less typically, in FLIGHTS and in the COMIC12 system, the OpenCCG re"
J10-2001,P04-1009,0,0.0916248,"(SR) approach, in which the system structures a large number of options into a small number 194 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation of clusters that share attributes. The system then summarizes the clusters based on their attributes, implicitly prompting the user to provide additional constraints. The system produces summaries such as I have found 983 restaurants. Most of them are located in Boston and Cambridge. There are 32 choices for cuisine. I also have information about price range. which help the user get an overview of the option space. Chung (2004) extended this approach by proposing a constraint relaxation strategy for coping with queries that are too restrictive to be satisﬁed by any option. Pon-Barry, Weng, and Varges (2006) found that fewer dialogue turns were necessary when the system proactively suggested reﬁnements and relaxations. However, as argued in Demberg and Moore (2006), there are several limitations to the SR approach. First, many turns may be required during the reﬁnement process. Second, if there is no optimal solution, exploration of trade-offs is difﬁcult. Finally, the attributes on which the data has been clustered"
J10-2001,E06-1009,1,0.947729,"straints to winnow down a large set before querying the database of options. Other researchers have argued that it is important to allow users to browse the data for a number of reasons: (1) if there are many options that share attribute values, they will be very close in score when ranked using the UM-based approach; (2) users may not be able to provide constraints until they hear more information about the space of options; and (3) the UM-based approach does not give users an overview of the option space, and this may reduce their conﬁdence that they have been told about the best option(s) (Demberg and Moore 2006). Polifroni, Chung, and Seneff (2003) proposed a “summarize and reﬁne” (SR) approach, in which the system structures a large number of options into a small number 194 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation of clusters that share attributes. The system then summarizes the clusters based on their attributes, implicitly prompting the user to provide additional constraints. The system produces summaries such as I have found 983 restaurants. Most of them are located in Boston and Cambridge. There are 32 choices for cuisine. I also have information about"
J10-2001,W04-0601,1,0.792537,"the Xalan XSLT processor to transform the output of the content planner into a sequence of LFs that can be realized by the OpenCCG agent. It is intended to be a relatively straightforward component, as the content planner has been designed to implement the most important high-level generation choices. Its primary responsibility is to lexicalize the basic speech acts in the content plan—which may appear in referring expressions—along with the rhetorical speech acts that connect them together. When alternative lexicalizations are available, all possibilities are included in a packed structure (Foster and White 2004; White 2006a). The sentence planner is also responsible for adding discourse markers such as also and but, adding pronouns, and choosing sentence boundaries. It additionally implements a small number of rhetorical restructuring operations for enhanced ﬂuency. The sentence planner makes use of approximately 50 XSLT templates to recursively transform content plans into logical forms. An example logical form that results from applying these templates to the content plan shown in Figure 7 appears in Figure 8 (with alternative lexicalizations suppressed). As described further in Section 2.6.1, the"
J10-2001,P96-1027,0,0.0744473,"CG open source realizer (White 2004, 2006a, 2006b). A distinguishing feature of OpenCCG is that 11 In future work, these checks could be extended to apply across turns as well. 172 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also"
J10-2001,P95-1034,0,0.0223846,"G is that 11 In future work, these checks could be extended to apply across turns as well. 172 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also a very cheap ﬂight on Air France, whereas in other cases it typically precedes the verb, as in I also have a"
J10-2001,E03-1057,0,0.0771914,"Missing"
J10-2001,A00-2023,0,0.0397805,"checks could be extended to apply across turns as well. 172 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also a very cheap ﬂight on Air France, whereas in other cases it typically precedes the verb, as in I also have a ﬂight that leave"
J10-2001,W02-2103,0,0.0261241,"urns as well. 172 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also a very cheap ﬂight on Air France, whereas in other cases it typically precedes the verb, as in I also have a ﬂight that leaves London at 3:45 p.m. It also enforces subject–ve"
J10-2001,W02-2106,0,0.0286781,"006b). A distinguishing feature of OpenCCG is that 11 In future work, these checks could be extended to apply across turns as well. 172 White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996; Shemtov 1997; Carroll et al. 1999; Moore 2002) and statistical realization (Knight and Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary 2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to combining these traditions appears in Carroll and Oepen (2005), where parse selection techniques are incorporated into an HPSG realizer. Like other realizers, the OpenCCG realizer is partially responsible for determining word order and inﬂection. For example, the realizer determines that also should preferably follow the verb in There is also a very cheap ﬂight on Air France, whereas in ot"
J10-2001,P06-1140,1,0.892961,"ence of words, pitch accents, and edge tones that maximizes the probability assigned by an n-gram model for the domain. In an approach that is more similar in spirit to ours, Bulyko and Ostendorf (2002) likewise aim to reproduce distinctive intonational patterns in a limited domain. However, unlike our approach, theirs makes use of simple templates for generating paraphrases, as their focus is on how deferring the ﬁnal choice of wording and prosodic realization to their synthesizer enables them to achieve more natural sounding synthetic speech. Following on the work described in this article, Nakatsu and White (2006) present a discriminative approach to realization ranking based on predicted synthesis quality that is directly compatible with the FLIGHTS system. Turning to our synthesis evaluation, we note that debate over the standardization of speech synthesis evaluation continues, with the Blizzard Challenge (Black and Tokuda 2005; Fraser and King 2007) proving to be a useful forum for discussing and performing evaluation across different synthesis platforms. Mayo, Clark, and King (2005) have proposed to evaluate speech synthesis evaluation from a perceptual viewpoint to discover 196 White, Clark, and M"
J10-2001,2001.mtsummit-papers.68,0,0.0187001,"Missing"
J10-2001,2006.amta-papers.25,0,0.0227143,"Missing"
J10-2001,J05-1002,0,0.0608301,"Missing"
J10-2001,W02-2110,0,0.105773,"n. In particular, we require better algorithms for: 1. selecting the most relevant subset of options to mention, as well as the attributes that are most relevant to choosing among them; and 2. determining how to organize and express the descriptions of the selected options and attributes, in ways that are both easy to understand and memorable.1 In this article, we describe how we have addressed these points in the FLIGHTS2 system, reviewing and extending the description given in Moore et al. (2004). FLIGHTS follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore 2000; Walker et al. 2002) in applying decision-theoretic models of user preferences to the generation of tailored descriptions of the most relevant available options. Multiattribute decision theory provides a detailed account of how models of user preferences can be used in decision making (Edwards and Barron 1994). Such preference models have been shown to enable systems to present information in ways that are concise and 1 An issue we do not address in this article is whether a multimodal system would be more effective than a voice-only one. We believe that these needs, and in particular the need to express informat"
J10-2001,P01-1066,0,0.197174,"Missing"
J10-2001,W06-1403,1,0.804155,"or to transform the output of the content planner into a sequence of LFs that can be realized by the OpenCCG agent. It is intended to be a relatively straightforward component, as the content planner has been designed to implement the most important high-level generation choices. Its primary responsibility is to lexicalize the basic speech acts in the content plan—which may appear in referring expressions—along with the rhetorical speech acts that connect them together. When alternative lexicalizations are available, all possibilities are included in a packed structure (Foster and White 2004; White 2006a). The sentence planner is also responsible for adding discourse markers such as also and but, adding pronouns, and choosing sentence boundaries. It additionally implements a small number of rhetorical restructuring operations for enhanced ﬂuency. The sentence planner makes use of approximately 50 XSLT templates to recursively transform content plans into logical forms. An example logical form that results from applying these templates to the content plan shown in Figure 7 appears in Figure 8 (with alternative lexicalizations suppressed). As described further in Section 2.6.1, the logical for"
J10-2001,N03-2002,0,\N,Missing
J10-2001,P02-1040,0,\N,Missing
L16-1506,W11-2107,0,0.0116692,"or each question asked by the medical student which of the set of questions anticipated by the content author best matches the student’s question. In previous work on developing a log-linear ranking model for this system (Jaffe et al., 2015), we found the interpretation task to be quite challenging, since it requires discriminating among many topically related questions; by contrast, the MSRP task (Dolan et al., 2004) is to classify pairs of sentences as paraphrases or non-paraphrases, where the pairs themselves are mostly unrelated. In particular, we found that alignments produced by Meteor (Denkowski and Lavie, 2011) were less helpful than anticipated, given that Meteor played an important role in an ensemble of MT metrics (Madnani et al., 2012) that until recently yielded the best paraphrase classification results on the MSRP corpus. As such, we expect our corpus to be a valuable asset for research on improved methods of monolingual alignment (Thadani et al., 2012; Yao et al., 2013) as well as paraphrase detection and semantic similarity measurement more generally.2 In constructing the corpus, we have made use of a novel process for selecting data to annotate with both word alignments and paraphrase stat"
L16-1506,C04-1051,0,0.057361,"dardized patient (Figure 1) from which we have manually annotated word alignments for 942 sentence pairs. With this dialogue system (Danforth et al., 2013), the interpretation task is to determine for each question asked by the medical student which of the set of questions anticipated by the content author best matches the student’s question. In previous work on developing a log-linear ranking model for this system (Jaffe et al., 2015), we found the interpretation task to be quite challenging, since it requires discriminating among many topically related questions; by contrast, the MSRP task (Dolan et al., 2004) is to classify pairs of sentences as paraphrases or non-paraphrases, where the pairs themselves are mostly unrelated. In particular, we found that alignments produced by Meteor (Denkowski and Lavie, 2011) were less helpful than anticipated, given that Meteor played an important role in an ensemble of MT metrics (Madnani et al., 2012) that until recently yielded the best paraphrase classification results on the MSRP corpus. As such, we expect our corpus to be a valuable asset for research on improved methods of monolingual alignment (Thadani et al., 2012; Yao et al., 2013) as well as paraphras"
L16-1506,P13-1158,0,0.0273105,"is process and these tool and guideline enhancements. Together with the paper, we are making the corpus of dialogues and gold standard word alignments freely available, along with the enhanced alignment tool. 2. The corpus consists of two main parts — the dialogues, and the word-level alignments. The dialogues are a set of 104 conversations of 5347 total turns, where each turn consists of an asked question, a label, and a response. 1 See Xu et al. (2014) for a review of paraphrase corpora without gold standard alignments. Also closely related is the much larger WikiAnswers Paraphrase Dataset (Fader et al., 2013), which again does not have manually annotated alignments and which exhibits much less topical coherence. Annotation Process 2 Naturally, we do not discourage the development of additional annotated corpora for enabling research on monolingual alignment and paraphrase detection, e.g. ones from naturalistic task settings primarily involving declarative sentences rather than questions. 3174 Input Gold Label ChatScript System Response hello there, why did you come in to see us today what brings you in today i was hoping you could help me with my back pain, it really hurts! it has been awful. i ca"
L16-1506,W15-0611,1,0.886705,"odeling word alignment and paraphrase classification as a joint process. In this paper, we present a corpus of 104 dialogues between early stage medical students and a virtual standardized patient (Figure 1) from which we have manually annotated word alignments for 942 sentence pairs. With this dialogue system (Danforth et al., 2013), the interpretation task is to determine for each question asked by the medical student which of the set of questions anticipated by the content author best matches the student’s question. In previous work on developing a log-linear ranking model for this system (Jaffe et al., 2015), we found the interpretation task to be quite challenging, since it requires discriminating among many topically related questions; by contrast, the MSRP task (Dolan et al., 2004) is to classify pairs of sentences as paraphrases or non-paraphrases, where the pairs themselves are mostly unrelated. In particular, we found that alignments produced by Meteor (Denkowski and Lavie, 2011) were less helpful than anticipated, given that Meteor played an important role in an ensemble of MT metrics (Madnani et al., 2012) that until recently yielded the best paraphrase classification results on the MSRP"
L16-1506,N12-1019,0,0.0157771,"question. In previous work on developing a log-linear ranking model for this system (Jaffe et al., 2015), we found the interpretation task to be quite challenging, since it requires discriminating among many topically related questions; by contrast, the MSRP task (Dolan et al., 2004) is to classify pairs of sentences as paraphrases or non-paraphrases, where the pairs themselves are mostly unrelated. In particular, we found that alignments produced by Meteor (Denkowski and Lavie, 2011) were less helpful than anticipated, given that Meteor played an important role in an ensemble of MT metrics (Madnani et al., 2012) that until recently yielded the best paraphrase classification results on the MSRP corpus. As such, we expect our corpus to be a valuable asset for research on improved methods of monolingual alignment (Thadani et al., 2012; Yao et al., 2013) as well as paraphrase detection and semantic similarity measurement more generally.2 In constructing the corpus, we have made use of a novel process for selecting data to annotate with both word alignments and paraphrase status. We have also enhanced the Edinburgh alignment tool to better support this process, and revised and extended the Edinburgh guide"
L16-1506,C12-2120,1,0.788563,"d questions; by contrast, the MSRP task (Dolan et al., 2004) is to classify pairs of sentences as paraphrases or non-paraphrases, where the pairs themselves are mostly unrelated. In particular, we found that alignments produced by Meteor (Denkowski and Lavie, 2011) were less helpful than anticipated, given that Meteor played an important role in an ensemble of MT metrics (Madnani et al., 2012) that until recently yielded the best paraphrase classification results on the MSRP corpus. As such, we expect our corpus to be a valuable asset for research on improved methods of monolingual alignment (Thadani et al., 2012; Yao et al., 2013) as well as paraphrase detection and semantic similarity measurement more generally.2 In constructing the corpus, we have made use of a novel process for selecting data to annotate with both word alignments and paraphrase status. We have also enhanced the Edinburgh alignment tool to better support this process, and revised and extended the Edinburgh guidelines. In the ensuing sections, we review this process and these tool and guideline enhancements. Together with the paper, we are making the corpus of dialogues and gold standard word alignments freely available, along with"
L16-1506,Q14-1034,0,0.124828,"naturalistic task setting: the MSRP (Brockett, 2007) and Edinburgh (Cohn et al., 2008) corpora.1 Moreover, both of these corpora have additional shortcomings. In particular, the Edinburgh corpus consists only of aligned paraphrase pairs, rather than containing both paraphrase and non-paraphrase pairs; and although the MSRP corpus does include a mix of paraphrase and non-paraphrase pairs, the word alignments were annotated without taking into consideration the paraphrase status of the sentence pair. This potentially makes the MSRP alignments less useful than they might otherwise be, given that Xu et al. (2014) have recently shown that there can be considerable benefit to modeling word alignment and paraphrase classification as a joint process. In this paper, we present a corpus of 104 dialogues between early stage medical students and a virtual standardized patient (Figure 1) from which we have manually annotated word alignments for 942 sentence pairs. With this dialogue system (Danforth et al., 2013), the interpretation task is to determine for each question asked by the medical student which of the set of questions anticipated by the content author best matches the student’s question. In previous"
L16-1506,P13-2123,0,0.060403,"Missing"
N09-2041,E06-1032,0,0.020898,"ad of Mr. Otero , which . . . . (Note that our input LFs do not directly specify the choice of function words such as case-marking prepositions, relative pronouns and complementizers, and thus class-based scoring can help to select the correct surface word form.) 5.4 Targeted manual evaluation While the language models employing NE classes certainly improve some examples, others are made worse, and some are just changed to different, but equally acceptable paraphrases. For this reason, we carried out a targeted manual evaluation to confirm the BLEU results. 5.4.1 Procedure Along the lines of (Callison-Burch et al., 2006), two native speakers (two of the authors) provided ratings for a random sample of 49 realizations that differed between the baseline and best conditions on the collapsed corpus. Note that the selection procedure excludes exact matches and thus focuses on sentences whose realization quality may be lower on average than in an arbitrary sample. Sentences were rated in the context of the preceding sentence (if any) for both fluency and adequacy in comparison to the original sentence. The judges were not 5 0.8173 on CCGbank Section 23. We have also confirmed the increase in quality through a targe"
N09-2041,P08-1022,1,0.684445,"ization quality through better language models and better hypertagging (supertagging for realization) models, yielding a state-of-the-art BLEU score of 0.8173 on Section 23 of the CCGbank. 161 CCG Surface Realization CCG (Steedman, 2000) is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode subcategorization as well as syntactic features (e.g. number and agreement). OpenCCG is a parsing/generation library which includes a hybrid symbolic-statistical chart realizer (White, 2006). A vital component of the realizer is the hypertagger (Espinosa et al., 2008), which predicts lexical category assignments using a maxent model trained on contexts within a directed graph structure representing the logical form (LF) input; features and relations in the graph as well as parent child relationships are the main features used to train the model. The realizer takes as input an LF description (see Figure 1 of Espinosa et al., 2008), but here we also Proceedings of NAACL HLT 2009: Short Papers, pages 161–164, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics use LFs with class information on some elementary predications (e.g. @x:M"
N09-2041,D07-1028,0,0.407949,"Missing"
N09-2041,W02-2103,0,0.104378,"Missing"
N09-2041,W05-1510,0,0.490864,"Missing"
N09-2041,P08-1039,0,0.0623104,"Missing"
N18-5011,D12-1110,0,0.0231176,"ity. The game could be extended to include other sentences and types of structural ambiguity, such as with coordination (e.g., The old dogs and cats went to the vet, where old may modify dogs and cats or dogs alone). This may call for additional illustrative pictures, however. Other expansions might incorporate different successful vector-based methods into the word2vec mode to make it even more sophisticated. Compositional character models, as in Ling et al. (2015), could allow the system to meaningfully model even outof-vocabulary words; syntactically/semantically compositional models as in Socher et al. (2012) could yield a single vector for multi-word phrases that composes the representations for each word rather than averaging them, potentially providing more separation between clusters. Another direction would be to dynamically generate explanations. It is an open source project, so anyone could contribute to the code! Acknowledgments We thank David King, Matt Metzger, and Kaleb White for their contributions to the interface and advanced mode functionality. The interactive demo materials were contributed by Laura Wagner and Victoria Sevich. Special thanks also to Kathryn Campbell-Kibler and Chri"
N18-5011,W16-1718,1,0.904791,"Missing"
N18-5011,P14-1039,1,0.816896,"work approaches. Qualitative feedback from the system’s use in online, classroom, and science museum settings indicates that it is engaging and successful in conveying the intended take home messages. 1 Figure 1: Mr. Computer Head, who acts as the opponent in the game, also narrates the introduction and explanation. Introduction Madly Ambiguous has been developed as an outreach component of a project whose aim is to develop methods for avoiding ambiguity in natural language generation and for using disambiguating paraphrases to crowd source interpretations of structurally ambiguous sentences (Duan and White, 2014; Duan et al., 2016; White et al., 2017). The game was initially intended solely as an iPad demo outside of Ohio State’s Language Sciences Research Lab, or “Language Pod,” a fully functional research lab embedded within the Columbus Center of Science and Industry (COSI), one of the premier science centers in the country (Wagner et al., 2015). The COSI research pods are glass-enclosed research spaces where museum visitors can observe actual scientific research as it is occurring, creating excitement in children about science and encouraging scientific careers. Outside the pod, Ohio State gradua"
N18-5011,W17-3702,1,0.8424,"m the system’s use in online, classroom, and science museum settings indicates that it is engaging and successful in conveying the intended take home messages. 1 Figure 1: Mr. Computer Head, who acts as the opponent in the game, also narrates the introduction and explanation. Introduction Madly Ambiguous has been developed as an outreach component of a project whose aim is to develop methods for avoiding ambiguity in natural language generation and for using disambiguating paraphrases to crowd source interpretations of structurally ambiguous sentences (Duan and White, 2014; Duan et al., 2016; White et al., 2017). The game was initially intended solely as an iPad demo outside of Ohio State’s Language Sciences Research Lab, or “Language Pod,” a fully functional research lab embedded within the Columbus Center of Science and Industry (COSI), one of the premier science centers in the country (Wagner et al., 2015). The COSI research pods are glass-enclosed research spaces where museum visitors can observe actual scientific research as it is occurring, creating excitement in children about science and encouraging scientific careers. Outside the pod, Ohio State graduate and undergraduate students (the “expl"
N18-5011,D12-1096,0,0.0329782,"Missing"
N18-5011,D15-1176,0,0.0151172,"judgments collected, which could be used as dynamic feedback for training future versions or possibly as data for other studies of structural ambiguity. The game could be extended to include other sentences and types of structural ambiguity, such as with coordination (e.g., The old dogs and cats went to the vet, where old may modify dogs and cats or dogs alone). This may call for additional illustrative pictures, however. Other expansions might incorporate different successful vector-based methods into the word2vec mode to make it even more sophisticated. Compositional character models, as in Ling et al. (2015), could allow the system to meaningfully model even outof-vocabulary words; syntactically/semantically compositional models as in Socher et al. (2012) could yield a single vector for multi-word phrases that composes the representations for each word rather than averaging them, potentially providing more separation between clusters. Another direction would be to dynamically generate explanations. It is an open source project, so anyone could contribute to the code! Acknowledgments We thank David King, Matt Metzger, and Kaleb White for their contributions to the interface and advanced mode funct"
P05-3012,W02-2110,0,0.0249508,"contrasts with task-oriented embodied dialogue systems such as SmartKom (Wahlster, 2003). Since guided browsing requires extended descriptions, in COMIC we have placed greater emphasis on producing highquality adaptive output than have previous embodied dialogue projects such as August (Gustafson et al., 1999) and Rea (Cassell et al., 1999). To generate its adaptive output, COMIC uses information from the dialogue history and the user model throughout the generation process, as in FLIGHTS (Moore et al., 2004); both systems build upon earlier work on adaptive content planning (Carenini, 2000; Walker et al., 2002). An experimental study (Foster and White, 2005) has shown that this adaptation is perceptible to users of COMIC. 2 Dialogue Management The task of the Dialogue and Action Manager (DAM) is to decide what the system will show and say in response to user input. The input to the 45 Proceedings of the ACL Interactive Poster and Demonstration Sessions, c pages 45–48, Ann Arbor, June 2005. 2005 Association for Computational Linguistics (a) Bathroom-design application (b) Talking head Figure 1: Components of the COMIC interface User Tell me about this design [click on Alt Mettlach] COMIC [Look at scr"
P05-3012,W03-2705,1,0.870867,"Missing"
P05-3012,W04-0601,1,\N,Missing
P05-3012,N03-2002,0,\N,Missing
P05-3012,W05-1103,1,\N,Missing
P06-1140,P05-1018,0,0.0117915,"e ’s, or words such as style sounding emphasized when they should be deaccented; unnatural rate changes; “choppy” speech from poor joins; and some unintelligible proper names. 3.4 Ranking While Collins (2000) and Walker et al. (2002) develop their rankers using the RankBoost algorithm (Freund et al., 1998), we have instead chosen to use Joachims’ (2002) method of formulating ranking tasks as Support Vector Machine (SVM) constraint optimization problems.8 This choice has been motivated primarily by convenience, as Joachims’ SVMlight package is easy to 7 http://www.hcrc.ed.ac.uk/web exp/ 8 See (Barzilay and Lapata, 2005) for another application of SVM ranking in generation, namely to the task of ranking alternative text orderings for local coherence. use; we leave it for future work to compare the performance of RankBoost and SVMlight on our ranking task. The ranker takes as input a set of paraphrases that express the desired content of each sentence, optionally together with synthesized utterances for each paraphrase. The output is a ranking of the paraphrases according to the predicted naturalness of their corresponding synthesized utterances. Ranking is more appropriate than classification for our purposes"
P06-1140,P01-1008,0,0.0262103,"ponse according to the predicted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality for the various paraphrases—and if these predictions are generally borne out—then, by selecting paraphrases with high predicted synthesis quality, the dialogue system (as a whole) can more reliably produce natural sounding speech. In this paper, we present an application of dis2 1 See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). See e.g. (Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Pang et al., 2003) for discussion of paraphrase in generation. 1113 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120, c Sydney, July 2006. 2006 Association for Computational Linguistics criminative reranking to the task of adapting a language generator to the strengths and weaknesses of a particular synthetic voice. Our method involves training a reranker to select paraphrases that are predicted to sound natural when synthesized, from the N-best realizations produced by the generator. The ranker is trained in su"
P06-1140,P98-1116,0,0.063444,"paraphrase to use as its response according to the predicted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality for the various paraphrases—and if these predictions are generally borne out—then, by selecting paraphrases with high predicted synthesis quality, the dialogue system (as a whole) can more reliably produce natural sounding speech. In this paper, we present an application of dis2 1 See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). See e.g. (Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Pang et al., 2003) for discussion of paraphrase in generation. 1113 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120, c Sydney, July 2006. 2006 Association for Computational Linguistics criminative reranking to the task of adapting a language generator to the strengths and weaknesses of a particular synthetic voice. Our method involves training a reranker to select paraphrases that are predicted to sound natural when synthesized, from the N-best realizations produced by the generator."
P06-1140,W02-2107,0,0.0198775,"jointly determining wording, speech and gesture. In their approach, a template-based generator produces a word lattice with intonational phrase breaks. A unit selection algorithm then searches for a low-cost way of realizing a path through this lattice that combines captured motion samples with recorded speech samples to create coherent phrases, blending segments of speech and motion together phrase-by-phrase into extended utterances. Video demonstrations indicate that natural and highly expressive results can be achieved, though no human evaluations are reported. In an alternative approach, Pan and Weng (2002) proposed integrating instance-based realization and synthesis. In their framework, sentence structure, wording, prosody and speech waveforms from a domain-specific corpus are simultaneously reused. To do so, they add prosodic and acoustic costs to the insertion, deletion and replacement costs used for instance-based surface realization. Their contribution focuses on how to design an appropriate speech corpus to facilitate an integrated approach to instance-based realization and synthesis, and does not report evaluation results. A drawback of these approaches to integrating choice in language"
P06-1140,N03-1024,0,0.0240089,"cted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality for the various paraphrases—and if these predictions are generally borne out—then, by selecting paraphrases with high predicted synthesis quality, the dialogue system (as a whole) can more reliably produce natural sounding speech. In this paper, we present an application of dis2 1 See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). See e.g. (Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Pang et al., 2003) for discussion of paraphrase in generation. 1113 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120, c Sydney, July 2006. 2006 Association for Computational Linguistics criminative reranking to the task of adapting a language generator to the strengths and weaknesses of a particular synthetic voice. Our method involves training a reranker to select paraphrases that are predicted to sound natural when synthesized, from the N-best realizations produced by the generator. The ranker is trained in supervised fashion, us"
P06-1140,P88-1023,0,0.0488303,"me possible. The paper is organized as follows. In Section 2, we review previous work on integrating choice in language generation and speech synthesis, and on learning discriminative rerankers for generation. In Section 3, we present our method. In Section 4, we describe a cross-validation study whose results indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average. Finally, in Section 5, we conclude with a summary and a discussion of future work. 2 Previous Work Most previous work on integrating language generation and synthesis, e.g. (Davis and Hirschberg, 1988; Prevost and Steedman, 1994; Hitzeman et al., 1998; Pan et al., 2002), has focused on how to use the information present in the language generation component in order to specify contextually appropriate intonation for the speech synthesizer to target. For example, syntactic structure, information structure and dialogue context have all been argued to play a role in improving prosody prediction, compared to unrestricted textto-speech synthesis. While this topic remains an important area of research, our focus is instead on a different opportunity that arises in a dialogue system, namely, the p"
P06-1140,W04-0601,1,0.759644,"me content adapted to a different user model or adapted to a different dialogue history. For example, a description of a certain design’s colour scheme for one user might be phrased as As you can see, the tiles have a blue and green colour scheme, whereas a variant expression of the same content for a different user could be Although the tiles have a blue colour scheme, the design does also feature green, if the user disprefers blue. In COMIC, the sentence planner uses XSLT to generate disjunctive logical forms (LFs), which specify a range of possible paraphrases in a nested free-choice form (Foster and White, 2004). Such disjunctive LFs can be efficiently realized using the OpenCCG realizer (White, 2004; White, 2006b; White, 2006a). Note that for the experiments reported here, we manually augmented the disjunctive LFs for the 104 sentences in our sample to make greater use of the periphrastic capabilities of the COMIC grammar; it remains for future work to augment the COMIC sentence planner produce these more richly disjunctive LFs automatically. OpenCCG includes an extensible API for integrating language modeling and realization. To select preferred word orders, from among all those allowed by the gram"
P06-1140,W06-1403,1,\N,Missing
P06-1140,C98-1112,0,\N,Missing
P08-1022,P02-1041,0,0.416434,"on 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2 Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. 184 have.03 <TENSE>pres <Arg0> he h2 h1 <Arg1> point <NUM>sg p1 <Arg1> <GenRel> <Det> a1 a w1 <TENSE>pres <Arg1> <Arg0> h3 want.0"
P08-1022,E03-1036,0,0.0229906,"the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2 Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. 184 have.03 <TENSE>pres <Arg0> he h2 h1 <Arg1> point <NUM>sg"
P08-1022,J99-2004,0,0.104644,", it is possible to separate lexical category assignment — the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates — from the combinatory processes that make use of such categories — such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignmen"
P08-1022,C00-1007,0,0.0427231,"o a forest. Carroll, Oepen and Velldal’s approach is like Nakanishi et al.’s in that they adapt log-linear parsing models to the realization task; however, they employ manually written grammars on much smaller corpora, and perhaps for this reason they have not faced the need to employ an iterative beam search. 190 6 Conclusion We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000). To our knowledge, we are the first to report tagging results in the semantic-to-syntactic direction. We have also shown that, by integrating this hypertagger with a broad-coverage CCG chart realizer, considerably faster realization times are possible (approximately twice as fast as compared with a realizer that performs simple lexical look-ups) with higher B LEU, M ETEOR and exact string match scores. Moreover, the hypertagger-augmented realizer finds more than twice the number of complete realizations, and further analysis revealed that the realization quality (as per modified B LEU and M E"
P08-1022,boxwell-white-2008-projecting,1,0.554438,"ned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Pruning takes place within equivalence classes of edges. Additionally, to realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. hNUMisg); nodes are connected via dependency relations (e.g. hA RG 0i). Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff (2002). In HLDS, hybrid logic (Blackburn, 2000) terms are used to describe dependency graphs. These graphs have been suggested as representations for discourse structure, and have their own underlying semantics (White, 2006b). To more rob"
P08-1022,P06-1130,0,0.0932147,"Missing"
P08-1022,I05-1015,0,0.116235,"orial Grammar (Steedman, 2000, CCG) and Head-Driven PhraseStructure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment — the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates — from the combinatory processes that make use of such categories — such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency"
P08-1022,J07-4004,0,0.0912523,"vely recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treebank-derived syntactic grammar (Hockenmaier and Steedman, 2007) and a factored language model (Bilmes and Kirchhoff, 2003), they con"
P08-1022,W02-2203,0,0.0488663,"ion thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treeban"
P08-1022,P06-1088,0,0.0361877,"as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005). A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999), a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed “almost parsing” by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006), leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007). We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG.1 Instead of basing category assignment on linear word and POS context, however, we predict lexical categories based on contexts within a directed graph structure representing the logical form (LF) of a proposition to be realized. Assigned categories are instantiated in OpenCCG’s chart realizer where, together with a treebank-derived syntactic gr"
P08-1022,J07-3004,0,0.158373,"this process repeats until no further edges can be added to the set of selected fragments. In the final step, these fragments are concatenated, again in a greedy fashion, this time according to the n-gram score of the concatenated edges: starting with the original best edge, the fragment whose concatenation on the left or right side yields the highest score is chosen as the one to concatenate next, until all the fragments have been concatenated into a single output. 2.2 Realization from an Enhanced CCGbank White et al. (2007) describe an ongoing effort to engineer a grammar from the CCGbank (Hockenmaier and Steedman, 2007) — a corpus of CCG derivations derived from the Penn Treebank — suitable for realization with OpenCCG. This process involves converting the corpus to reflect more precise analyses, where feasible, and adding semantic representations to the lexical categories. In the first step, the derivations in the CCGbank are revised to reflect the desired syntactic derivations. Changes to the derivations are necessary to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-modal version of CCG that is implemented in OpenCCG. Further changes are necessary to support semanti"
P08-1022,P96-1027,0,0.0833621,"scribes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2 Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. 184 have.03 <TENSE>pres <Arg0> he h2 h1 <Arg1> point <NUM>sg p1 <Arg1> <GenRel> <Det> a1 a w1 <TENSE>pres <Arg1> <Arg0> h3 want.01 he m1 make.03 Figure 1: Semantic dependency graph from the CCGbank for He has a"
P08-1022,W02-2103,0,0.0697618,"ore dependencies required to form a fully connected graph. These missing dependencies usually reflect inadequacies in the current logical form templates. 2.3 Factored Language Models Following White et al. (2007), we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2–21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates a word trigram model with a trigram model that chains a POS model with a supertag model, where the POS model conditions on the previous two POS tags, and the supertag model conditions on the previous two POS tags as well as the current one. Note that the use of supertags in the factored language model"
P08-1022,A00-2023,0,0.0119639,"of speech, supertag and semantic class. The search proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configurable time limit: the scores assigned by the ngram model determine the order of the edges on the agenda, and thus have an impact on realization speed. In the two-stage mode, a packed forest of all possible realizations is created in the first stage; in the second stage, the packed representation is unpacked in bottom-up fashion, with scores assigned to the edge for each sign as it is unpacked, much as in (Langkilde, 2000). Edges are grouped into equivalence classes when they have the same syntactic category and cover the same parts of the input logical form. Pruning takes place within equivalence classes of edges. Additionally, to realize a wide range of paraphrases, OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms (White, 2006a). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1, which is taken from section 00 of a Propbank-enhanced version of the CCGbank (Boxwell and White, 2008). In the graph, each node has a lexical predication ("
P08-1022,W07-0734,0,0.0278039,"Missing"
P08-1022,W05-1510,0,0.400042,"ch follows Langkilde-Geary (2002) and Callaway (2003) in aiming to leverage the Penn Treebank to develop a broad-coverage surface realizer for English. However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. In this regard, our approach is more similar to the ones pursued more recently by Carroll, Oepen and Velldal (2005; 2005; 2006), Nakanishi et al. (2005) and Cahill and van Genabith (2006) with HPSG and LFG grammars. While we consider our approach to be the first to employ a supertagger for realization, or hypertagger, the approach is clearly reminiscent of the LTAG tree models of Srinivas and Rambow (2000). The main difference between the approaches is that ours consists of a multitagging step followed by the bottomup construction of a realization chart, while theirs involves the top-down selection of the single most likely supertag for each node that is grammatically compatible with the parent node, with the probability conditioned only on t"
P08-1022,J05-1004,0,0.0299538,"re adjusted to reflect their purely syntactic status. In the second step, a grammar is extracted from the converted CCGbank and augmented with logical forms. Categories and unary type changing rules (corresponding to zero morphemes) are sorted by frequency and extracted if they meet the specified frequency thresholds. A separate transformation then uses around two dozen generalized templates to add logical forms to the categories, in a fashion reminiscent of (Bos, 2005). The effect of this transformation is illustrated below. Example (1) shows how numbered semantic roles, taken from PropBank (Palmer et al., 2005) when available, are added to the category of an active voice, past tense transitive verb, where *pred* is a placeholder for the lexical predicate; examples (2) and (3) show how more specific relations are introduced in the category for determiners and the category for the possessive ’s, respectively. (1) s1 :dcl 
p2 /np3 =⇒ s1 :dcl,x1 
p2 :x2 /np3 :x3 : @x1 (*pred* ∧ hTENSEipres ∧ hA RG 0ix2 ∧ hA RG 1ix3) (2) np1 /n1 =⇒ np1 :x1 /n1 :x1 : @x1 (hD ETi(d ∧ *pred*)) (3) np1 /n1 
p2 =⇒ np1 :x1 /n1 :x1 
p2 :x2 : @x1 (hG EN OWNix2) After logical form insertion, the extracted and augmented gramma"
P08-1022,P02-1040,0,0.0973707,"Missing"
P08-1022,C88-2121,0,0.42462,"Missing"
P08-1022,2005.mtsummit-papers.15,0,0.0892331,"ed to form a fully connected graph. These missing dependencies usually reflect inadequacies in the current logical form templates. 2.3 Factored Language Models Following White et al. (2007), we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2–21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005), the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates a word trigram model with a trigram model that chains a POS model with a supertag model, where the POS model conditions on the previous two POS tags, and the supertag model conditions on the previous two POS tags as well as the current one. Note that the use of supertags in the factored language model to score possible realiza"
P08-1022,W06-1661,0,0.0211349,"Missing"
P08-1022,2007.mtsummit-ucnlg.4,1,0.800888,"rched for the best edge whose semantic coverage is disjoint from those selected so far; this process repeats until no further edges can be added to the set of selected fragments. In the final step, these fragments are concatenated, again in a greedy fashion, this time according to the n-gram score of the concatenated edges: starting with the original best edge, the fragment whose concatenation on the left or right side yields the highest score is chosen as the one to concatenate next, until all the fragments have been concatenated into a single output. 2.2 Realization from an Enhanced CCGbank White et al. (2007) describe an ongoing effort to engineer a grammar from the CCGbank (Hockenmaier and Steedman, 2007) — a corpus of CCG derivations derived from the Penn Treebank — suitable for realization with OpenCCG. This process involves converting the corpus to reflect more precise analyses, where feasible, and adding semantic representations to the lexical categories. In the first step, the derivations in the CCGbank are revised to reflect the desired syntactic derivations. Changes to the derivations are necessary to reflect the lexicalized treatment of coordination and punctuation assumed by the multi-mo"
P08-1022,W06-1403,1,0.942719,"and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6. 2 Background 2.1 Surface Realization with OpenCCG The OpenCCG surface realizer is based on Steedman’s (2000) version of CCG elaborated with Baldridge and Kruijff’s multi-modal extensions for lexically specified derivation control (Baldridge, 2002; Baldridge and Kruijff, 2003) and hybrid logic dependency semantics (Baldridge and Kruijff, 2002). OpenCCG implements a symbolic-statistical chart realization algorithm (Kay, 1996; Carroll et al., 1999; White, 2006b) combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models (Bilmes and Kirchhoff, 2003) for making choices among the options left open by the grammar. In OpenCCG, the search for complete realizations 2 Note that the multitagger is “correct” if the correct tag is anywhere in the multitag set. 184 have.03 <TENSE>pres <Arg0> he h2 h1 <Arg1> point <NUM>sg p1 <Arg1> <GenRel> <Det> a1 a w1 <TENSE>pres <Arg1> <Arg0> h3 want.01 he m1 make.03 Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [. . . ] mak"
P08-1022,N03-2002,0,\N,Missing
P14-1039,W10-4237,0,0.050957,"Missing"
P14-1039,W11-2832,1,0.825986,"measuring accuracy of parser dependency recovery. In a realistic application scenario, however, we would need to measure parser accuracy relative to the realizer’s input. We initially tried using OpenCCG’s 6 Related Work Approaches to surface realization have been developed for LFG, HPSG, and TAG, in addition to CCG, and recently statistical dependency-based approaches have been developed as well; see the report from the first surface realization shared 420 ciently as a combined process, rather than running independent parsers as a post-process following realization. task (Belz et al., 2010; Belz et al., 2011) for an overview. To our knowledge, however, a comprehensive investigation of avoiding vicious structural ambiguities with broad coverage statistical parsers has not been previously explored. As our SVM ranking model does not make use of CCG-specific features, we would expect our selfmonitoring method to be equally applicable to realizers using other frameworks. 7 Acknowledgments We thank Mark Johnson, Micha Elsner, the OSU Clippers Group and the anonymous reviewers for helpful comments and discussion. This work was supported in part by NSF grants IIS-1143635 and IIS-1319318. Conclusion In thi"
P14-1039,boxwell-white-2008-projecting,1,0.850886,"g the dependency to based on; at the same time, it avoids ambiguity in what next year is modifying. In wsj 0020.1 we see the reverse case: the dependency length model produces a nearly exact match with just an equally acceptable inversion of closely watching, keeping the direct object in its canonical position. By contrast, the DEPORD model mistakenly shifts the direct object South Korea, Taiwan and Saudia Arabia to the end of the sentence where it is difficult to understand following two very long intervening phrases. and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White & Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as"
P14-1039,P05-1022,0,0.207435,"Missing"
P14-1039,J07-4004,0,0.224738,"Missing"
P14-1039,W02-1001,0,0.00704324,"ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic models of syntax that have been developed for CCG parsing, Hockenmaier & Steedman’s (2002) generative model and Clark & Curran’s (2007) normal-form model. Using the averaged perceptron algorithm (Collins, 2002), White & Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the state-of-the-art in terms of automatic evaluation scores on heldout test data, but nevertheless an error analysis revealed a surprising number of word order, function word and inflection errors. For each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model. With function words, Rajkumar and White (2011) showed that they could improve"
P14-1039,C92-2105,0,0.822745,"Missing"
P14-1039,de-marneffe-etal-2006-generating,0,0.0226992,"Missing"
P14-1039,P08-1022,1,0.908705,"call for each type of dependency, together with the realizer’s normalized perceptron model score as a feature. With the SVM reranker, we obtain a significant improvement in BLEU scores over 2 Background We use the OpenCCG1 surface realizer for the experiments reported in this paper. The OpenCCG realizer generates surface strings for input semantic dependency graphs (or logical forms) using a chart-based algorithm (White, 2006) for Combinatory Categorial Grammar (Steedman, 2000) together with a “hypertagger” for probabilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-i"
P14-1039,J05-1004,0,0.012679,"time, it avoids ambiguity in what next year is modifying. In wsj 0020.1 we see the reverse case: the dependency length model produces a nearly exact match with just an equally acceptable inversion of closely watching, keeping the direct object in its canonical position. By contrast, the DEPORD model mistakenly shifts the direct object South Korea, Taiwan and Saudia Arabia to the end of the sentence where it is difficult to understand following two very long intervening phrases. and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). To select preferred outputs from the chart, we use White & Rajkumar’s (2009; 2012) realization ranking model, recently augmented with a largescale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000; Reiter, 2010): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two probabilistic m"
P14-1039,P10-1035,0,0.0848125,"und that it did not improve upon the averaged perceptron model, like the three parsers used subsequently. Given that with the more refined SVM ranker, the Berkeley parser worked nearly as well as all three parsers together using the complete feature set, the prospects for future work on a more realistic scenario using the OpenCCG parser in an SVM ranker for self-monitoring now appear much more promising, either using OpenCCG’s reimplementation of Hockenmaier & Steedman’s generative CCG model, or using the Berkeley parser trained on OpenCCG’s enhanced version of the CCGbank, along the lines of Fowler and Penn (2010). in the original, which the averaged perceptron model prefers. Finally, wsj 0044.111 is an example where a subject-inversion makes no difference to adequacy or fluency. 5.2 Discussion The BLEU evaluation and targeted manual analysis together show that the SVM ranker increases the similarity to the original corpus of realizations produced with self-monitoring, often in ways that are crucial for the intended meaning to be apparent to human readers. A limitation of the experiments reported in this paper is that OpenCCG’s input semantic dependency graphs are not the same as the Stanford dependenc"
P14-1039,P02-1040,0,0.0893552,"Missing"
P14-1039,P06-1055,0,0.0490557,"OS tags and supertags in the training sections of the CCGbank, and the large-scale 5-gram model from Gigaword. The second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2. In order to experiment with multiple parsers, we used the Stanford dependencies (de Marneffe et al., 2006), obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser (Petrov et al., 2006), Brown parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003) to parse the realizations generated by the (1) He said that/∅? for the second month in a row, food processors reported a shortage of nonfat dry milk. (PTB WSJ0036.61) Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and 2 Note that the features from the local classification model for that-complementizer choice have not yet b"
P14-1039,C10-2119,1,0.850949,"2006), obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser (Petrov et al., 2006), Brown parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003) to parse the realizations generated by the (1) He said that/∅? for the second month in a row, food processors reported a shortage of nonfat dry milk. (PTB WSJ0036.61) Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and 2 Note that the features from the local classification model for that-complementizer choice have not yet been incorporated into OpenCCG’s global realization ranking model, and thus do not inform the baseline realization choices in this work. 416 That’s Not What I Meant! Using Parsers to Avoid Structural blocked Ambiguities in Generated Text Berkeley Brown Stanford the gold realization from emerging on top. No reranking Labeled Unlabeled 87.93 87.93"
P14-1039,W11-2706,1,0.780404,"Using the averaged perceptron algorithm (Collins, 2002), White & Rajkumar (2009) trained a structured prediction ranking model to combine these existing syntactic models with several n-gram language models. This model improved upon the state-of-the-art in terms of automatic evaluation scores on heldout test data, but nevertheless an error analysis revealed a surprising number of word order, function word and inflection errors. For each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model. With function words, Rajkumar and White (2011) showed that they could improve upon the earlier model’s predictions for when to employ that-complementizers using features inspired by Jaeger’s (2010) work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. In news text, com415 wsj 0015.7 DEPLEN DEPORD wsj 0020.1 DEPLEN DEPORD the exact amount of the refund will be determined next year based on actual collections made until Dec. 31 of this year . [same] the exact amount of the refund will be determined b"
P14-1039,P02-1043,0,0.102754,"Missing"
P14-1039,N09-2041,1,0.848576,"nput (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1 414 http://openccg.sf.net To improve word ordering decisions, White & Rajkumar (2012) demonstrated that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation. Supporting Gibson’s theory, comprehension and corpus studies"
P14-1039,J07-3004,0,0.0176149,"algorithm (White, 2006) for Combinatory Categorial Grammar (Steedman, 2000) together with a “hypertagger” for probabilistically assigning lexical categories to lexical predicates in the input (Espinosa et al., 2008). An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization; the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; 1 414 http://openccg.sf.net To improve word ordering decisions, White & Rajkumar (2012) demonstrated that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce t"
P14-1039,C08-1055,0,0.0598682,"Missing"
P14-1039,P03-1054,0,0.021281,"gram model from Gigaword. The second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2. In order to experiment with multiple parsers, we used the Stanford dependencies (de Marneffe et al., 2006), obtaining gold dependencies from the gold-standard PTB parses and automatic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser (Petrov et al., 2006), Brown parser (Charniak and Johnson, 2005) and Stanford parser (Klein and Manning, 2003) to parse the realizations generated by the (1) He said that/∅? for the second month in a row, food processors reported a shortage of nonfat dry milk. (PTB WSJ0036.61) Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and 2 Note that the features from the local classification model for that-complementizer choice have not yet been incorporated into OpenCCG’s global realization ranking model, and thus do not inform"
P14-1039,W04-3250,0,0.0113029,"abeled precision and recall for each parser’s best parse per-label precision and recall (dep) precision and recall for each type of dependency obtained from each parser’s best parse (using zero if not defined for lack of predicted or gold dependencies with a given label) n-best precision and recall (nbest) labeled and unlabeled precision and recall for each parser’s top five parses, along with the same features for the most accurate of these parses 4.2 Results Table 3 shows the results of different SVM ranking models on the devset. We calculated significance using paired bootstrap resampling (Koehn, 2004).3 Both the per-label precision & recall feaIn training, we used the BLEU scores of each realization compared with its reference sentence to establish a preference order over pairs of candidate realizations, assuming that the original corpus sentences are generally better than related alternatives, and that BLEU can somewhat reliably predict human preference judgments. 3 Kudos to Kevin Gimpel for making his implementation available: http://www.ark.cs.cmu.edu/MT/ paired_bootstrap_v13a.tar.gz 418 perceptron baseline Berkeley Brown Stanford Bkl+Brw+St BBS+dep BBS+nbest BBS+dep+nbest Bkl+dep Bkl+n"
P14-1039,W08-1703,1,0.899993,"Missing"
P14-1039,D09-1043,1,0.934024,"Missing"
P14-1039,D12-1023,1,0.926213,"heory, comprehension and corpus studies have found that the tendency to minimize dependency length has a strong influence on constituent ordering choices; see Temperley (2007) and Gildea and Temperley (2010) for an overview. s[dcl]
p/np have.03 &lt;TENSE>pres &lt;Arg0> h1 &lt;Arg1> n he point h2 np &lt;NUM>sg p1 &lt;Arg1> &lt;GenRel> &lt;Det> s[dcl]
p/(s[to]
p) a1 a w1 np h3 &lt;TENSE>pres &lt;Arg1> &lt;Arg0> np/n want.01 he m1 make.03 s[b]
p/np &lt;Arg0> Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node Table 1 shows examples from White and Rajkumar (2012) of how the dependency length feature (DEPLEN) affects the OpenCCG realizer’s output even in comparison to a model (DEPORD) with a rich set of discriminative syntactic and dependency ordering features, but no features directly targeting relative weight. In wsj 0015.7, the dependency length model produces an exact match, while the DEPORD model fails to shift the short temporal adverbial next year next to the verb, leaving a confusingly repetitive this year next year at the end of the sentence. Note how shifting next year from its canonical VP-final position to appear next to the verb shortens i"
P19-1080,N16-1086,0,0.0497733,"domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset. 1 Introduction Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. With their end-to-end trainability, neural approaches to natural language generation (NNLG), particularly sequence-to-sequence (Seq2Seq) models, have been promoted with great fanfare in recent years (Wen et al., 2015, 2016; Mei et al., 2016; Kiddon et al., 2016; Duˇsek and Jurcicek, 2016), and avenues like the recent E2E NLG challenge (Duˇsek et al., 2018, 2019) have made available large datasets to promote the development of these models. Nevertheless, current NNLG models arguably remain inadequate for most real-world ∗ † 1 Also see https://ehudreiter.com/2018/11/ 12/hallucination-in-neural-nlg/. Alphabetical by first name Work done while on leave from Ohio State University 831 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 831–844 c Florence, Italy, July 28 - August 2, 2019. 2019"
P19-1080,N18-1014,0,0.230511,"Missing"
P19-1080,J93-4004,0,0.509572,"Missing"
P19-1080,N19-1236,0,0.0974969,"Missing"
P19-1080,W05-1510,0,0.157963,"1 5.35 6.18 5.86 5.85 5.86 Cond. entropy bigrams 1.32 1.13 2.09 1.71 1.65 1.71 Table 8: E2E dataset diversity metrics. Rows in gray correspond to metrics that we cite from Duˇsek et al. (2019). Our constrained decoding approach goes beyond covering a simple list by enforcing constraints on ordering and grouping of tree structures, but theirs takes coverage into account during model training. A more direct inspiration for our approach is the way coverage has been traditionally tracked in grammar-based surface realization (Shieber, 1988; Kay, 1996; Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005; White, 2006; White and Rajkumar, 2009). Compared to our approach, grammar-based realizers can prevent hallucination entirely, though at the expense of developing an explicit grammar. Constrained decoding in MT (Post and Vilar, 2018, i.a.) has been used to enforce the use of specific words in the output, rather than constraints on tree structures. Also related are neural generators that take Abstract Meaning Representations (AMRs) as input (Konstas et al., 2017, i.a.) rather than flat inputs; these approaches, however, do not generate trees or use constrained decoding. Figure 3: Performance o"
P19-1080,D17-1238,0,0.0204016,"use an LSTMbased encoder and decoder, with attention. S2S-F LAT The input is a flat MR (for the E2E dataset, this is equivalent to the original form of the data; for weather, we remove all discourse relations and treat all dialog acts as a single large MR). The output is the raw delexicalized response. 6 Reed et al. only report results on controlling C ONTRAST using an augmented training set, precluding direct comparison to their results. 836 • BLEU-4 (Papineni et al., 2002) is a wordoverlap metric commonly used for evaluating NLG systems. Due to the limitations of automatic metrics for NLG (Novikova et al., 2017; Reiter, 2018), we also performed human evaluation studies by asking annotators to evaluate the quality of responses produced by different models. Annotators provided binary ratings on the following dimensions: • Grammaticality: Measures fluency of the responses. Our evaluation guidelines included considerations for proper subject-verb agreement, word order, repetition, and grammatical completeness. • Correctness: Measures semantic correctness of the responses. Our guidelines included considerations for sentence structure, contrast, hallucinations (incorrectly included attributes), and missin"
P19-1080,P02-1040,0,0.104986,"Seq2Seq-based baselines in our experiments (we use the open fairseq implementation (Gehring et al., 2017) for all our experiments). All models use an LSTMbased encoder and decoder, with attention. S2S-F LAT The input is a flat MR (for the E2E dataset, this is equivalent to the original form of the data; for weather, we remove all discourse relations and treat all dialog acts as a single large MR). The output is the raw delexicalized response. 6 Reed et al. only report results on controlling C ONTRAST using an augmented training set, precluding direct comparison to their results. 836 • BLEU-4 (Papineni et al., 2002) is a wordoverlap metric commonly used for evaluating NLG systems. Due to the limitations of automatic metrics for NLG (Novikova et al., 2017; Reiter, 2018), we also performed human evaluation studies by asking annotators to evaluate the quality of responses produced by different models. Annotators provided binary ratings on the following dimensions: • Grammaticality: Measures fluency of the responses. Our evaluation guidelines included considerations for proper subject-verb agreement, word order, repetition, and grammatical completeness. • Correctness: Measures semantic correctness of the res"
P19-1080,D14-1162,0,0.080679,"Missing"
P19-1080,N16-1015,0,0.12239,"Missing"
P19-1080,N18-1119,0,0.0313712,"yond covering a simple list by enforcing constraints on ordering and grouping of tree structures, but theirs takes coverage into account during model training. A more direct inspiration for our approach is the way coverage has been traditionally tracked in grammar-based surface realization (Shieber, 1988; Kay, 1996; Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005; White, 2006; White and Rajkumar, 2009). Compared to our approach, grammar-based realizers can prevent hallucination entirely, though at the expense of developing an explicit grammar. Constrained decoding in MT (Post and Vilar, 2018, i.a.) has been used to enforce the use of specific words in the output, rather than constraints on tree structures. Also related are neural generators that take Abstract Meaning Representations (AMRs) as input (Konstas et al., 2017, i.a.) rather than flat inputs; these approaches, however, do not generate trees or use constrained decoding. Figure 3: Performance of S2S-T REE models trained on E2E flat data, and flat E2E + full weather dataset, with a fraction of composition E2E. with only compositional examples. Without any compositional E2E examples, both models fail to produce any valid seq"
P19-1080,D15-1199,0,0.172047,"Missing"
P19-1080,H01-1055,0,0.141892,"tokens consistently with the data distribution. 2 The datasets and implementations can be found at https://github.com/facebookresearch/ TreeNLG. 832 non-contrastive alternative such as JJ’s Pub is a highly-rated restaurant for adults near the Crowne Plaza Hotel. 2.2 like date time1 and date time2. On the other hand, our MRs ease discourse-level learning and encourage reuse of arguments across multiple dialog acts. Tree-Structured MRs 3 In order to overcome these challenges, we propose the use of structured meaning representations like those explored widely in (hybrid) rule-based NLG systems (Rambow et al., 2001; Reiter and Dale, 2000; Walker et al., 2007). Our representation consists of three parts: 1. Argument can be any entity or slot mentioned in a response, like the name of a restaurant or the date. Some arguments can be complex and contain sub-arguments (e.g. a date time argument has subfields like week day and month). 2. Dialog act is an atomic unit that could correspond linguistically to a single clause. A dialog act can contain one or more arguments that need to be expressed. Examples: I N FORM , Y ES , R ECOMMEND . 3. Discourse relation defines the relationships between dialog acts. A singl"
P19-1080,W18-6535,0,0.425227,"and (3) avoid generating texts with semantic errors including hallucinated content (Duˇsek et al., 2018, 2019).1 In this paper, we explore the extent to which these issues can be addressed by incorporating lessons from pre-neural NLG systems into a neural framework. We begin by arguing in favor of enriching the input to neural generators to include discourse relations — long taken to be central in traditional NLG — and underscore the importance of exerting control over these relations when generating text, particularly when using user models to structure responses. In a closely related work, Reed et al. (2018), the authors add control tokens (to indicate contrast and sentence structure) to a flat input MR, and show that these can be effectively used to control structure. However, their methods are only able to control the presence or absence of these relations, without more fine-grained control over their structure. We thus go beyond their approach and propose using full tree structures as inputs, and generating treestructured outputs as well. This allows us to define a novel method of constrained decoding for standard sequence-to-sequence models for generation, which helps ensure that the generate"
P19-1080,J10-2001,1,0.82579,"Missing"
P19-1080,J18-3002,0,0.0135002,"er and decoder, with attention. S2S-F LAT The input is a flat MR (for the E2E dataset, this is equivalent to the original form of the data; for weather, we remove all discourse relations and treat all dialog acts as a single large MR). The output is the raw delexicalized response. 6 Reed et al. only report results on controlling C ONTRAST using an augmented training set, precluding direct comparison to their results. 836 • BLEU-4 (Papineni et al., 2002) is a wordoverlap metric commonly used for evaluating NLG systems. Due to the limitations of automatic metrics for NLG (Novikova et al., 2017; Reiter, 2018), we also performed human evaluation studies by asking annotators to evaluate the quality of responses produced by different models. Annotators provided binary ratings on the following dimensions: • Grammaticality: Measures fluency of the responses. Our evaluation guidelines included considerations for proper subject-verb agreement, word order, repetition, and grammatical completeness. • Correctness: Measures semantic correctness of the responses. Our guidelines included considerations for sentence structure, contrast, hallucinations (incorrectly included attributes), and missing attributes. W"
P19-1080,D09-1043,1,0.761571,"opy bigrams 1.32 1.13 2.09 1.71 1.65 1.71 Table 8: E2E dataset diversity metrics. Rows in gray correspond to metrics that we cite from Duˇsek et al. (2019). Our constrained decoding approach goes beyond covering a simple list by enforcing constraints on ordering and grouping of tree structures, but theirs takes coverage into account during model training. A more direct inspiration for our approach is the way coverage has been traditionally tracked in grammar-based surface realization (Shieber, 1988; Kay, 1996; Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005; White, 2006; White and Rajkumar, 2009). Compared to our approach, grammar-based realizers can prevent hallucination entirely, though at the expense of developing an explicit grammar. Constrained decoding in MT (Post and Vilar, 2018, i.a.) has been used to enforce the use of specific words in the output, rather than constraints on tree structures. Also related are neural generators that take Abstract Meaning Representations (AMRs) as input (Konstas et al., 2017, i.a.) rather than flat inputs; these approaches, however, do not generate trees or use constrained decoding. Figure 3: Performance of S2S-T REE models trained on E2E flat d"
P19-1080,C88-2128,0,0.719213,"S-T REE S2S-C ONSTR 83 74 455 137 134 134 597 507 3567 1147 1030 1128 5.41 5.35 6.18 5.86 5.85 5.86 Cond. entropy bigrams 1.32 1.13 2.09 1.71 1.65 1.71 Table 8: E2E dataset diversity metrics. Rows in gray correspond to metrics that we cite from Duˇsek et al. (2019). Our constrained decoding approach goes beyond covering a simple list by enforcing constraints on ordering and grouping of tree structures, but theirs takes coverage into account during model training. A more direct inspiration for our approach is the way coverage has been traditionally tracked in grammar-based surface realization (Shieber, 1988; Kay, 1996; Carroll et al., 1999; Carroll and Oepen, 2005; Nakanishi et al., 2005; White, 2006; White and Rajkumar, 2009). Compared to our approach, grammar-based realizers can prevent hallucination entirely, though at the expense of developing an explicit grammar. Constrained decoding in MT (Post and Vilar, 2018, i.a.) has been used to enforce the use of specific words in the output, rather than constraints on tree structures. Also related are neural generators that take Abstract Meaning Representations (AMRs) as input (Konstas et al., 2017, i.a.) rather than flat inputs; these approaches, h"
P19-1080,W18-6558,0,0.0194728,", where each key is a slot name that needs to be mentioned, and the value is the value of that slot (see Table 1). In Wen et al. (2015), MRs have a similar structure, and additionally contain information about the dialog act that needs to be conveyed (R EQUEST, I NFORM, etc.). These MRs are sufficient to capture basic semantic information, but fail to capture rhetorical (or discourse) relations, like C ONTRAST, that have long been taken to be central to generating coherent discourse in tradi3 An additional 86 outputs contained these tokens, but were generated by the TR2 template-based system (Smiley et al., 2018). The expected number of contrastive system outputs would be 4,200 if each of the 14 participating systems produced contrastive tokens consistently with the data distribution. 2 The datasets and implementations can be found at https://github.com/facebookresearch/ TreeNLG. 832 non-contrastive alternative such as JJ’s Pub is a highly-rated restaurant for adults near the Crowne Plaza Hotel. 2.2 like date time1 and date time2. On the other hand, our MRs ease discourse-level learning and encourage reuse of arguments across multiple dialog acts. Tree-Structured MRs 3 In order to overcome these chall"
P93-1040,J88-2003,0,0.111036,"Missing"
P93-1040,E93-1048,1,0.868975,"Missing"
W00-0505,M92-1038,0,0.0242643,"ticipant slots plus optional date and location slots.2 We then gathered a small corpus of thirty articles by searching for articles containing ""North Korea"" and one or more of about 15 keywords. The first two sentences (with a few exceptions) were then annotated with the slots to be extracted, leading to a total of 51 sentences containing 47 scenario templates and 89 total 4.2 2 In the end, we did not use the &apos;issue&apos; slot shown in Figure 1, as it contained more complex Idlers than those that typically have been handled in IE systems. For our feasibility study, we chose to follow the AutoSlog (Lehnert et al., 1992; Riloff, 1993) approach to extraction pattern acquisition. In this approach, extraction patterns are acquired 34 Extraction Pattern Learning i. E: &lt;target-np>=&lt;subject> &lt;active voice verb> &lt;participant> MET K: &lt;target-np>=&lt;subject> &lt;active voice verb> &lt;John-i> MANNASSTA &lt;John-nom>&apos;MET 2. E: &lt;target-np>=&lt;subject> &lt;verb> &lt;infinitive> &lt;participant> agreed to MEET K: &lt;target-np>=&lt;subject> &lt;verbl-ki-lo> &lt;verb2> &lt;John-un> MANNA-ki-lo hapuyhayssta &lt;John-nom> MEET-ki-lo agreed (-ki: nominalization ending, -io: an adverbial postposition) Figure 3 via a one-shot general-to-specific learning algorithm d"
W00-0505,1981.tc-1.3,0,0.0738175,"Missing"
W00-0505,W98-1428,1,0.744077,"an Transfer Lexicon is used to map the English keywords to corresponding Korean ones. When the match falls below a user• configurable threshold, the extracted information is filtered out. • The MT component. The MT component (cf. Lavoie et al., 2000) translates the extracted Korean phrases or sentences into corresponding English ones. • The Presentation Generator component. This component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information. It uses existing NLG components, including the Exemplars text planning framework (White and Caldwell, 1998) and the RealPro syntactic realizer (Lavoie and Rainbow, 1997). In our feasibility study, the majority of the effort went towards developing the PIE component, described in the next section. This component was implemented in a general way, i.e. in a way that we would expect to work beyond the specific training/test corpus described below. In contrast, we only implemented initial versions of the User Interface, Ranker and Presentation Generator components, in order to demonstrate the system concept; that is, these initial versions were only intended.to work with our training/test corpus, and wi"
W00-0505,1997.iwpt-1.25,1,0.761584,"Missing"
W00-0505,palmer-etal-1998-rapid,1,\N,Missing
W00-0505,M91-1033,1,\N,Missing
W00-0505,A00-1009,1,\N,Missing
W00-0505,1997.mtsummit-workshop.12,1,\N,Missing
W00-0505,A97-1039,1,\N,Missing
W01-1403,1983.tc-1.13,0,0.0943767,"Missing"
W01-1403,P98-2139,0,0.296578,"fer rules, in this work we instead focus on automating the acquisition of such rules. Our approach can be considered a generalization of syntactic approaches to example-based machine translation (EBMT) such as (Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992). While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process. Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the resulting transfer rules; in (Meyers et al., 1998), parses contain only lexical labels and syntactic roles (as arc labels), while our approach uses parses containing lexical labels, syntactic roles, and any other syntactic information provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et a"
W01-1403,C90-3044,0,0.0602762,"didate rules by co- occurrence; and (iv) applying error-driven filtering to select the final set of rules. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. Our approach can be considered a generalization of syntactic approaches to example-based machine translation (EBMT) such as (Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992). While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process. Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the co"
W01-1403,P97-1003,0,0.0665609,"h and syntactic context. • For a subset of the translated strings, human judgments of accuracy and grammaticality are gathered, and the correlations between the manual and automatic scores are calculated, in order to assess the meaningfulness of the automatic measures. 3 3.1 Data Preparation Parsing the Bi-texts In our experiments to date, we have used a corpus consisting of a Korean dialog of 4183 sentences and their English human translations. We ran off-the-shelf parsers on each half of the corpus, namely the Korean parser developed by Yoon et al. (1997) and the English parser developed by Collins (1997). Neither parser was trained on our corpus. We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer, RealPro (Lavoie and Rambow, 1997). This representation is based on the deepsyntactic structures (DSyntS) of Meaning-Text Theory (Mel’ˇcuk, 1988). The important features of a DSyntS are as follows: • a DSyntS is an unordered tree with labeled nodes and labeled arcs; • a DSyntS is lexicalized, meaning that the nodes are labeled with lexemes (uninflected words) from the target language; • a DSyntS is a"
W01-1403,H01-1014,0,0.0775868,"Missing"
W01-1403,J94-4004,0,0.315112,"provided by parsers (tense, number, person, etc.). The second difference concerns the node alignment; in (Meyers et al., 1998), the alignment of source and target nodes is designed in a way that preserves node dominancy in the source and target parses, while our approach does not have such restriction. One of the reasons for this difference is due to the different language pairs under study; (Meyers et al., 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). The third difference is in the process of identification of transfer rules candidates; in (Meyers et al., 1998), the identification is done by using the exact tree fragments in the source and target parse that are delimited by the alignment, while we use all source and target tree sub-patterns matching a subset of the parse features that satisfy a customizable set of alignment • Transfer rule candidates are generated based on the sub-patterns that contain the corresponding aligned nodes in the source and target parses. constraints and attribute constraints. The fourth third difference is in"
W01-1403,1997.iwpt-1.25,0,0.237382,"ome heuristics based on the similarity of part-of-speech and syntactic context. • For a subset of the translated strings, human judgments of accuracy and grammaticality are gathered, and the correlations between the manual and automatic scores are calculated, in order to assess the meaningfulness of the automatic measures. 3 3.1 Data Preparation Parsing the Bi-texts In our experiments to date, we have used a corpus consisting of a Korean dialog of 4183 sentences and their English human translations. We ran off-the-shelf parsers on each half of the corpus, namely the Korean parser developed by Yoon et al. (1997) and the English parser developed by Collins (1997). Neither parser was trained on our corpus. We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer, RealPro (Lavoie and Rambow, 1997). This representation is based on the deepsyntactic structures (DSyntS) of Meaning-Text Theory (Mel’ˇcuk, 1988). The important features of a DSyntS are as follows: • a DSyntS is an unordered tree with labeled nodes and labeled arcs; • a DSyntS is lexicalized, meaning that the nodes are labeled with lexemes (uninflect"
W01-1403,han-etal-2000-handling,1,0.90659,"irections. 1 Introduction This paper describes a novel approach to inducing transfer rules from syntactic parses of bi-texts and available bilingual dictionaries. The approach consists of inducing transfer rules using the four major steps described in more detail below: (i) aligning the nodes of the parses; (ii) generating candidate rules from these alignments; (iii) ordering candidate rules by co- occurrence; and (iv) applying error-driven filtering to select the final set of rules. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. Our approach can be considered a generalization of syntactic approaches to example-based machine translation (EBMT) such as (Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992). While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rul"
W01-1403,1992.tmi-1.15,0,0.690225,"currence; and (iv) applying error-driven filtering to select the final set of rules. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domain-specific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. Our approach can be considered a generalization of syntactic approaches to example-based machine translation (EBMT) such as (Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992). While such approaches use syntactic transfer examples during the actual transfer of source parses, our approach instead uses syntactic transfer examples to induce general transfer rules that can be compiled into a transfer dictionary for use in the actual translation process. Our approach is similar to the recent work of (Meyers et al., 1998) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, it also differs from (Meyers et al., 1998) in several important points. The first difference concerns the content of parses and the result"
W01-1403,1997.mtsummit-workshop.12,0,\N,Missing
W01-1403,P93-1004,0,\N,Missing
W01-1403,C98-2134,0,\N,Missing
W01-1403,A97-1039,1,\N,Missing
W02-0402,H01-1065,0,0.159675,"Missing"
W02-0402,A97-1029,0,0.0166665,"Missing"
W02-0402,A97-1051,0,0.0232257,"Missing"
W02-0402,W00-0405,0,0.0704563,"Missing"
W02-0402,H01-1054,1,0.878326,"Missing"
W02-0402,A00-2018,0,\N,Missing
W02-1610,J94-4004,0,0.0205716,"alignment constraint. This constraint, which matches the structural patterns of the transfer rule illustrated in Figure 2, uses the aid alignment attribute to indicate that in a Korean and English parse pair, any source and target sub-trees matching this alignment constraint (where $X1 and $Y1 are aligned, i.e. have the same aid attribute values, and where $X2 and $Y3 are aligned) can be used as a point of departure for generating transfer rule candidates. We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994), and that only a handful of them are necessary for two given languages (we have identified 11 general alignment constraints Each node of a candidate transfer rule must have its relation attribute (relationship with its governor) specified if it is an internal node, otherwise this relation must not be specified: e.g. « $X1 ( $R $X2 ) Figure 5: Independent attribute constraint necessary for Korean to English transfer so far). 4.2.2 Attribute constraints Attribute constraints are used to limit the space of possible transfer rule candidates that can be generated from the sub-trees satisfying the"
W02-1610,P01-1030,0,0.036283,"have been expected given the small amount of training data. Table 6 shows the Bleu overall precision scores. Our system (Lex+Induced) improved substantially over both the Lex Only and Babelfish baseline systems. The score for the statistical baseline system (GIZA++/RW) is not meaningful, due to the absence of 3-gram and 4-gram matches. 2-g Prec 0.1207 0.0173 0.1252 0.1618 3-g Prec 0.0467 0.0 0.0450 0.0577 4-g Prec 0.0193 0.0 0.0145 0.0185 Table 5: Bleu N-gram precision scores GIZA++/RW As a second baseline, we used an off-the-shelf statistical MT system, consisting of the ISI ReWrite Decoder (Germann et al., 2001) together with a translation model produced by GIZA++ (Och and Ney, 2000) and a language model produced by the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). This system was trained on our corpus only. Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource. 1-g Prec 0.3814 0.1894 0.4234 0.4725 System Babelfish GIZA++/RW Lex Only Lex+Induced Bleu Score 0.0802 NA 0.0767 0.0950 Table 6: Bleu overall precision scores 5.3 Human Evaluation Results For the human evaluation, we asked two English native speakers to"
W02-1610,han-etal-2000-handling,1,0.829729,"luation results. The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and errordriven filtering. In an experiment with domainspecific Korean to English translation, the approach yielded substantial improvements over three baseline systems. 1 Introduction In this paper, we describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998)"
W02-1610,A00-1009,1,0.850795,"ny source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (Lavoie et. al., 2001). 2 Overall Runtime System Design Our Korean to English MT runtime system relies on the following off-the-shelf software components: Korean parser For parsing, we used the wide coverage syntactic dependency parser for Korean developed by (Yoon et al., 1997). The parser was not trained on our corpus. Transfer component For transfer of the Korean parses to English structures, we used the same lexico-structural transfer framework as (Lavoie et al., 2000). Realizer For surface realization of the transferred English syntactic structures, we used the RealPro English realizer (Lavoie and Rambow, 1997). The training of the system is described in the next two sections. 3 Data Preparation 3.1 Parses for the Bitexts In our experiments, we used a parallel corpus derived from bilingual training manuals provided by the U.S. Defense Language Institute. The corpus consists of a Korean dialog of 4,183 sentences about battle scenario message traffic and their English human translations. The parses for the Korean sentences were obtained using Yoon’s parser,"
W02-1610,W01-1403,1,0.499484,"Missing"
W02-1610,1992.tmi-1.15,0,0.070093,"n MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A mor"
W02-1610,P93-1004,0,0.0967967,"Missing"
W02-1610,P98-2139,0,0.0198454,"and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (L"
W02-1610,P00-1056,0,0.0437641,"e Bleu overall precision scores. Our system (Lex+Induced) improved substantially over both the Lex Only and Babelfish baseline systems. The score for the statistical baseline system (GIZA++/RW) is not meaningful, due to the absence of 3-gram and 4-gram matches. 2-g Prec 0.1207 0.0173 0.1252 0.1618 3-g Prec 0.0467 0.0 0.0450 0.0577 4-g Prec 0.0193 0.0 0.0145 0.0185 Table 5: Bleu N-gram precision scores GIZA++/RW As a second baseline, we used an off-the-shelf statistical MT system, consisting of the ISI ReWrite Decoder (Germann et al., 2001) together with a translation model produced by GIZA++ (Och and Ney, 2000) and a language model produced by the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). This system was trained on our corpus only. Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource. 1-g Prec 0.3814 0.1894 0.4234 0.4725 System Babelfish GIZA++/RW Lex Only Lex+Induced Bleu Score 0.0802 NA 0.0767 0.0950 Table 6: Bleu overall precision scores 5.3 Human Evaluation Results For the human evaluation, we asked two English native speakers to rank the quality of the translation results produced by the Babelfish, L"
W02-1610,2001.mtsummit-papers.68,0,0.0227998,"upporting Korean to English translation. This system was not trained on our corpus. System Babelfish GIZA++/RW Lex Only Lex+Induced Lex+Induced We compared the three baseline systems against our complete system, using the baseline transfer dictionary augmented with the induced transfer rules. We ran each of the four systems on the test set of 50 Korean sentences described in Section 3.2 and compared the resulting translations using the automatic evaluation and the human evaluation described below. 5.2 Automatic Evaluation Results For the automatic evaluation, we used the Bleu metric from IBM (Papineni et al., 2001). The Bleu metric combines several modified N-gram precision measures (N = 1 to 4), and uses brevity penalties to penalize translations that are shorter than the reference sentences. Table 5 shows the Bleu N-gram precision scores for each of the four systems. Our system (Lex+Induced) had better precision scores than each of the baseline systems, except in the case of 4grams, where it slightly trailed Babelfish. The statistical baseline system (GIZA++/RW) performed poorly, as might have been expected given the small amount of training data. Table 6 shows the Bleu overall precision scores. Our s"
W02-1610,W01-1402,0,0.0300245,"ults for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (Lavoie et. al., 2001). 2 Overal"
W02-1610,C90-3044,0,0.0470392,"scribe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subse"
W02-1610,H01-1014,0,0.0286953,"Missing"
W02-1610,1997.iwpt-1.25,0,0.0306606,"syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (Lavoie et. al., 2001). 2 Overall Runtime System Design Our Korean to English MT runtime system relies on the following off-the-shelf software components: Korean parser For parsing, we used the wide coverage syntactic dependency parser for Korean developed by (Yoon et al., 1997). The parser was not trained on our corpus. Transfer component For transfer of the Korean parses to English structures, we used the same lexico-structural transfer framework as (Lavoie et al., 2000). Realizer For surface realization of the transferred English syntactic structures, we used the RealPro English realizer (Lavoie and Rambow, 1997). The training of the system is described in the next two sections. 3 Data Preparation 3.1 Parses for the Bitexts In our experiments, we used a parallel corpus derived from bilingual training manuals provided by the U.S. Defense Language Institute. The cor"
W02-1610,C92-3145,0,\N,Missing
W02-1610,P02-1040,0,\N,Missing
W02-1610,1997.mtsummit-workshop.12,0,\N,Missing
W02-1610,C98-2134,0,\N,Missing
W02-1610,A97-1039,1,\N,Missing
W03-2316,E03-1036,1,\N,Missing
W03-2316,hockenmaier-steedman-2002-acquiring,0,\N,Missing
W03-2316,P02-1041,1,\N,Missing
W03-2316,J93-1008,0,\N,Missing
W03-2316,P99-1039,0,\N,Missing
W03-2316,W98-1415,0,\N,Missing
W03-2316,P89-1005,0,\N,Missing
W03-2316,P96-1027,0,\N,Missing
W03-2316,W02-2106,0,\N,Missing
W03-2316,P01-1019,0,\N,Missing
W04-0601,W02-2211,0,0.122892,"sl:variable name=&quot;num-remaining&quot; select=&quot;count(xalan:nodeset($pruned-alts)/*)&quot;/&gt; &lt;!-- Propagation step --&gt; &lt;xsl:choose&gt; &lt;!-- keep one-of when multiple alts succeed --&gt; &lt;xsl:when test=&quot;$num-remaining &gt; 1&quot;&gt; &lt;one-of&gt; &lt;xsl:copy-of select=&quot;$pruned-alts&quot;/&gt; &lt;/one-of&gt; &lt;/xsl:when&gt; &lt;!-- filter out one-of when just one choice remains --&gt; &lt;xsl:when test=&quot;$num-remaining = 1&quot;&gt; &lt;xsl:copy-of select=&quot;$pruned-alts&quot;/&gt; &lt;/xsl:when&gt; &lt;!-- fail if none remain --&gt; &lt;xsl:otherwise&gt; &lt;fail/&gt; &lt;/xsl:otherwise&gt; &lt;/xsl:choose&gt; &lt;/xsl:template&gt; Figure 10: Failure-pruning template (Wilcock, 2001; Wilcock, 2003), and SmartKom (Becker, 2002). The main novel contribution of the text-planning approach described here is in its use of an external realizer that processes logical forms with embedded alternatives. This eliminates the need to use a backtracking AI planner (Becker, 2002) or to make arbitrary choices when multiple alternatives are available (van Deemter et al., 1999). The realizer also uses a completely different algorithm than the XSLT template processing—bottom-up, chartbased search rather than top-down rule expansion— which allows it to deal with those aspects of NLG that are more easily addressed using this kind of pro"
W04-0601,W00-1411,0,0.0279399,"red by the other processing and communication tasks in COMIC. The entire COMIC demonstrator will shortly be evaluated. As part of this evaluation, we plan to measure users’ recall of the information that the system presents to them, where that information is generated at different levels of detail. At the moment, the logical form for each message is created in isolation. In future versions of COMIC, we plan to use ideas from centering theory to help ensure coherence by planning a coherent sequence of logical forms for a description. We will implement this in a way similar to that described by Kibble and Power (2000). We will also incorporate a model of the user’s preferences into a later version of COMIC. The model will be used both to rank the options to be presented to the user, and to generate user-tailored descriptions of those options, as in FLIGHTS (Moore et al., 2004). Finally, we plan to extend the use of data-driven techniques in the realizer, and to make use of these techniques to help in choosing among alternatives in the other COMIC output modalities. Acknowledgements Thanks to Jon Oberlander, Johanna Moore, and the anonymous reviewers for helpful comments and discussion. This work was suppor"
W04-0601,W03-2316,1,0.908884,"ns in natural language generation. 1 Introduction In the traditional pipeline view of natural language generation (Reiter and Dale, 2000), many steps involve converting between increasingly specific tree representations. As Wilcock (2001) points out, this sort of tree-to-tree transformation is a task to which XML—and particularly XSLT template processing—is particularly suited. In this paper, we describe how we plan text by treating the XSLT processor as a top-down ruleexpanding planner that translates dialogue-manager specifications into logical forms to be sent to the OpenCCG text realizer (White and Baldridge, 2003; White, 2004a; White, 2004b). XSLT is used to perform many text-planning tasks, including structuring and aggregating the content, performing lexical choice via the selection of logical-form templates, and generating multiple alternative realizations for messages where possible. Using an external realizer at the end of the planning process provides two advantages. First, we can use the realizer to deal with those aspects of surface realization that are difficult to implement in XSLT, but that the realizer is designed to handle (e.g., syntactic agreement via unification). Second, we take advan"
W04-0601,W98-1428,1,0.814548,"Figure 7 will fail if the canned-text commentary cannot be realized as a verb phrase. In such cases, the text planner prunes out the failing possibilities before sending the set of The work presented here continues in the tradition of several recent NLG systems that use what could be called generalized template-based processing. By generalized, we mean that, rather than manipulating flat strings with no underlying linguistic representation, these systems instead work with structured fragments, which are often processed recursively. Other systems that fall into this category include EXEMPLARS (White and Caldwell, 1998), D2S (van Deemter et al., 1999), Interact 5 Related Work &lt;xsl:template match=&quot;one-of&quot;&gt; &lt;!-- Recursive pruning step --&gt; &lt;xsl:variable name=&quot;pruned-alts&quot;&gt; &lt;xsl:for-each select=&quot;*&quot;&gt; &lt;xsl:variable name=&quot;pruned-alt&quot;&gt; &lt;xsl:apply-templates select=&quot;.&quot;/&gt; &lt;/xsl:variable&gt; &lt;xsl:if test=&quot;not(xalan:nodeset($pruned-alt)//fail)&quot;&gt; &lt;xsl:copy-of select=&quot;$pruned-alt&quot;/&gt; &lt;/xsl:if&gt; &lt;/xsl:for-each&gt; &lt;/xsl:variable&gt; &lt;xsl:variable name=&quot;num-remaining&quot; select=&quot;count(xalan:nodeset($pruned-alts)/*)&quot;/&gt; &lt;!-- Propagation step --&gt; &lt;xsl:choose&gt; &lt;!-- keep one-of when multiple alts succeed --&gt; &lt;xsl:when test=&quot;$num-remaining &gt;"
W04-0601,E03-2016,0,0.0324945,"Missing"
W05-1104,C00-1007,0,0.361608,". A distinguishing feature of OpenCCG is that it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar (thereby reducing the need for hand-crafted rules). In contrast, previous chart realizers (Kay, 1996; Shemtov, 1997; Carroll et al., 1999; Moore, 2002) have not included a statistical component, while previous statistical realizers (Knight and Hatzivassiloglou, 1995; Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Oh and Rudnicky, 2002; Ratnaparkhi, 2002) have employed less general approaches to semantic representation and composition, and have not typically made use of finegrained logical forms that include specifications of such information structural notions as theme, rheme and focus. In this paper, we present OpenCCG’s extensible API (application programming interface) for integrating language modeling and realization, describing its design and efficient implementation in Java. With OpenCCG, language models may be used to select realizations with preferred word orders (White"
W05-1104,2001.mtsummit-papers.68,0,0.033838,"y rich grammars: the English grammar for the COMIC2 dialogue system—the core of which is shared with the FLIGHTS system (Moore et al., 2004)—and the Worldcup grammar discussed in 2 http://www.hcrc.ed.ac.uk/comic/ (Baldridge, 2002). Table 1 gives the sizes of the test suites. Using these two test suites, we timed how long it took on a 2.2 GHz Linux PC to realize each logical form under each realizer configuration. To measure accuracy, we counted the number of times the best scoring realization exactly matched the target, and also computed a modified version of the Bleu n-gram precision metric (Papineni et al., 2001) employed in machine translation evaluation, using 1- to 4-grams, with the longer n-grams given more weight (cf. Section 3.4). To rank candidate realizations, we used standard ngram backoff models of orders 2 through 6, with semantic class replacement, as described in Section 3.1. For smoothing, we used Ristad’s natural discounting (Ristad, 1995), a parameter-free method that seems to work well with relatively small amounts of data. To gauge how the amount of training data affects performance, we ran cross-validation tests with increasing numbers of folds, with 25 as the maximum number of fold"
W05-1104,P99-1018,0,0.0429345,"Missing"
W05-1104,W04-0601,1,0.550324,"emantic classes. Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form, pitch accent, stem, part of speech, supertag, and semantic class. In future work, we plan to further explore how to best employ factored language models; in particular, inspired by (Bangalore and Rambow, 2000), we plan to examine whether factored language models using supertags can provide an effective way to combine syntactic and lexical probabilities. We also plan to implement the capability to use one-of alternations in the input logical forms (Foster and White, 2004), in order to more efficiently defer lexical choice decisions to the language models. Acknowledgements Thanks to Jason Baldridge, Carsten Brockmann, Mary Ellen Foster, Philipp Koehn, Geert-Jan Kruijff, Johanna Moore, Jon Oberlander, Miles Osborne, Mark Steedman, Sebastian Varges and the anonymous reviewers for helpful discussion. «interface» PruningStrategy Returns the edges pruned from the given ones, which always have equivalent categories and are sorted by score. +pruneEdges(edges: List&lt;Edge>): List&lt;Edge> NBestPruningStrategy Keeps only the n−best edges. #CAT_PRUNE_VAL: int DiversityPruning"
W05-1104,P96-1027,0,0.0907941,"realizer takes as input a logical form specifying the propositional meaning of a sentence, and returns one or more surface strings that express this meaning according to the lexicon and grammar. A distinguishing feature of OpenCCG is that it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar (thereby reducing the need for hand-crafted rules). In contrast, previous chart realizers (Kay, 1996; Shemtov, 1997; Carroll et al., 1999; Moore, 2002) have not included a statistical component, while previous statistical realizers (Knight and Hatzivassiloglou, 1995; Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Oh and Rudnicky, 2002; Ratnaparkhi, 2002) have employed less general approaches to semantic representation and composition, and have not typically made use of finegrained logical forms that include specifications of such information structural notions as theme, rheme and focus. In this paper, we present OpenCCG’s extensible API (application programming interface"
W05-1104,W03-2316,1,0.853712,", avoid repetitive language use, and increase the speed of the best-first anytime search. The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies. The n-gram models may be of any order, operate in reverse (“right-to-left”), and selectively replace certain words with their semantic classes. Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form, pitch accent, stem, part of speech, supertag, and semantic class. 1 Introduction The OpenCCG1 realizer (White and Baldridge, 2003; White, 2004a; White, 2004c) is an open source surface realizer for Steedman’s (2000a; 2000b) Combinatory Categorial Grammar (CCG). It is designed to be the first practical, reusable realizer for CCG, and includes implementations of 1 http://openccg.sourceforge.net CCG’s unique accounts of coordination and information structure–based prosody. Like other surface realizers, the OpenCCG realizer takes as input a logical form specifying the propositional meaning of a sentence, and returns one or more surface strings that express this meaning according to the lexicon and grammar. A distinguishing"
W05-1104,P95-1034,0,0.0443608,"ss this meaning according to the lexicon and grammar. A distinguishing feature of OpenCCG is that it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar (thereby reducing the need for hand-crafted rules). In contrast, previous chart realizers (Kay, 1996; Shemtov, 1997; Carroll et al., 1999; Moore, 2002) have not included a statistical component, while previous statistical realizers (Knight and Hatzivassiloglou, 1995; Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Oh and Rudnicky, 2002; Ratnaparkhi, 2002) have employed less general approaches to semantic representation and composition, and have not typically made use of finegrained logical forms that include specifications of such information structural notions as theme, rheme and focus. In this paper, we present OpenCCG’s extensible API (application programming interface) for integrating language modeling and realization, describing its design and efficient implementation in Java. With OpenCCG, language models may be used to select r"
W05-1104,W02-2103,0,0.0394376,"f OpenCCG is that it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar (thereby reducing the need for hand-crafted rules). In contrast, previous chart realizers (Kay, 1996; Shemtov, 1997; Carroll et al., 1999; Moore, 2002) have not included a statistical component, while previous statistical realizers (Knight and Hatzivassiloglou, 1995; Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Oh and Rudnicky, 2002; Ratnaparkhi, 2002) have employed less general approaches to semantic representation and composition, and have not typically made use of finegrained logical forms that include specifications of such information structural notions as theme, rheme and focus. In this paper, we present OpenCCG’s extensible API (application programming interface) for integrating language modeling and realization, describing its design and efficient implementation in Java. With OpenCCG, language models may be used to select realizations with preferred word orders (White, 2004c), promote align"
W05-1104,P00-1012,0,0.0293126,"Missing"
W05-1104,W02-2106,0,0.0157995,"ng the propositional meaning of a sentence, and returns one or more surface strings that express this meaning according to the lexicon and grammar. A distinguishing feature of OpenCCG is that it implements a hybrid symbolic-statistical chart realization algorithm that combines (1) a theoretically grounded approach to syntax and semantic composition, with (2) the use of integrated language models for making choices among the options left open by the grammar (thereby reducing the need for hand-crafted rules). In contrast, previous chart realizers (Kay, 1996; Shemtov, 1997; Carroll et al., 1999; Moore, 2002) have not included a statistical component, while previous statistical realizers (Knight and Hatzivassiloglou, 1995; Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Oh and Rudnicky, 2002; Ratnaparkhi, 2002) have employed less general approaches to semantic representation and composition, and have not typically made use of finegrained logical forms that include specifications of such information structural notions as theme, rheme and focus. In this paper, we present OpenCCG’s extensible API (application programming interface) for integrating language modeling and realization"
W05-1104,A00-2023,0,\N,Missing
W05-1104,N03-2002,0,\N,Missing
W05-1104,P02-1040,0,\N,Missing
W06-1403,P02-1041,0,0.221427,"-level disjunction between the use of a deictic NP this design |this one |this (with an accompanying pointing gesture) followed by the copula, or the use of the phrase here we have to introduce the design. While these alternatives can function as paraphrases in this context, it is difficult to see how one might specify them in a single underspecified (and applicationneutral) logical form. Graphs such as those in Figure 1 are represented internally using Hybrid Logic Dependency Semantics (HLDS), as in Figure 2. HLDS is a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff (2002). In HLDS, hybrid logic (Blackburn, 2000) terms3 are used to describe dependency 3 The Algorithm As with the other chart realizers cited in the introduction, the OpenCCG realizer makes use of a chart and an agenda to perform a bottom-up dynamic programming search for signs whose LFs language, and thus formulas can be formed using propositions, nominals, and standard boolean operators. They may also employ the satisfaction operator, @. A formula @i (p∧hFi(j ∧q)) indicates that the formulas p and hFi(j ∧q) hold at the state named by i, and that the state j, where q holds, is reachable via the mo"
W06-1403,C00-1007,0,0.0478361,"puts from this space. To specify the desired paraphrase space, one may either provide an input logical form that underspecifies certain realization choices, or include explicit disjunctions in the input LF (or both). Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003). Prominent examples of surface realizers in the generate-and-select paradigm include Nitrogen/Halogen (Langkilde, 2000; Langkilde-Geary, 2002) and Fergus (Bangalore and Rambow, 2000). More recently, generate-and-select realizers in the chart realization tradition (Kay, 1996) have appeared, including the OpenCCG (White, 2004) 12 Proceedings of the Fourth International Natural Language Generation Conference, pages 12–19, c Sydney, July 2006. 2006 Association for Computational Linguistics @e (be ∧ hTENSEipres ∧ hMOODidcl ∧ hA RGi(d ∧ design ∧ hDETithe ∧ hNUMisg) ∧ hP ROPi(p ∧ based on ∧ hA RTIFACTid ∧ e be<TENSE>pres,<MOOD>dcl <ARG> hS OURCEi(c ∧ collection ∧ hDETithe ∧ hNUMisg ∧ <PROP> hH AS P ROPi(f ∧ Funny Day) ∧ hC REATORi(v ∧ V&B)))) p based_on design d <DET>the,<NUM>sg"
W06-1403,N03-1003,0,0.06126,"the task of surface realization. In this paradigm, symbolic methods are used to generate a space of possible phrasings, and statistical methods are used to select one or more outputs from this space. To specify the desired paraphrase space, one may either provide an input logical form that underspecifies certain realization choices, or include explicit disjunctions in the input LF (or both). Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003). Prominent examples of surface realizers in the generate-and-select paradigm include Nitrogen/Halogen (Langkilde, 2000; Langkilde-Geary, 2002) and Fergus (Bangalore and Rambow, 2000). More recently, generate-and-select realizers in the chart realization tradition (Kay, 1996) have appeared, including the OpenCCG (White, 2004) 12 Proceedings of the Fourth International Natural Language Generation Conference, pages 12–19, c Sydney, July 2006. 2006 Association for Computational Linguistics @e (be ∧ hTENSEipres ∧ hMOODidcl ∧ hA RGi(d ∧ design ∧ hDETithe ∧ hNUMisg) ∧ hP ROPi(p ∧ based on ∧ hA RTIFA"
W06-1403,I05-1015,0,0.0668745,"Missing"
W06-1403,W04-0601,1,0.743236,"Case Study To examine the potential of the algorithm to efficiently generate paraphrases, this section presents a case study of its run times versus sequential realization of the equivalent top-level LF alternatives in disjunctive normal form. The study used the COMIC grammar, a small but not trivial grammar that suffices for the purposes of the system. In this grammar, there are relatively few categories per lexeme on average, but the boundary tone categories engender a great deal of non-determinism. With other grammars, run times can be expected to vary. In anticipation of the present work, Foster and White (2004) generated disjunctive logical forms during sentence planning, then (as a stopgap measure) multiplied out the disjunctions and sequentially realized the top-level alternatives until an overall time limit was reached. Taking the previous logical forms as a starting point, 104 sentences from the evaluation in (Foster and White, 2005) were selected, and their LFs were manually augmented to cover a greater range of paraphrases allowed by the grammar.9 To obtain the corresponding top-level LF alternatives, 100-best realization was performed, and the unique LFs appearing in the top 100 realizations"
W06-1403,P96-1027,0,0.643683,"at underspecifies certain realization choices, or include explicit disjunctions in the input LF (or both). Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003). Prominent examples of surface realizers in the generate-and-select paradigm include Nitrogen/Halogen (Langkilde, 2000; Langkilde-Geary, 2002) and Fergus (Bangalore and Rambow, 2000). More recently, generate-and-select realizers in the chart realization tradition (Kay, 1996) have appeared, including the OpenCCG (White, 2004) 12 Proceedings of the Fourth International Natural Language Generation Conference, pages 12–19, c Sydney, July 2006. 2006 Association for Computational Linguistics @e (be ∧ hTENSEipres ∧ hMOODidcl ∧ hA RGi(d ∧ design ∧ hDETithe ∧ hNUMisg) ∧ hP ROPi(p ∧ based on ∧ hA RTIFACTid ∧ e be<TENSE>pres,<MOOD>dcl <ARG> hS OURCEi(c ∧ collection ∧ hDETithe ∧ hNUMisg ∧ <PROP> hH AS P ROPi(f ∧ Funny Day) ∧ hC REATORi(v ∧ V&B)))) p based_on design d <DET>the,<NUM>sg (a) .. . <SOURCE> <ARTIFACT> c collection<DET>the,<NUM>sg @e (be ∧ hTENSEipres ∧ hMOODidcl ∧"
W06-1403,W02-2103,0,0.226333,"are used to select one or more outputs from this space. To specify the desired paraphrase space, one may either provide an input logical form that underspecifies certain realization choices, or include explicit disjunctions in the input LF (or both). Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003). Prominent examples of surface realizers in the generate-and-select paradigm include Nitrogen/Halogen (Langkilde, 2000; Langkilde-Geary, 2002) and Fergus (Bangalore and Rambow, 2000). More recently, generate-and-select realizers in the chart realization tradition (Kay, 1996) have appeared, including the OpenCCG (White, 2004) 12 Proceedings of the Fourth International Natural Language Generation Conference, pages 12–19, c Sydney, July 2006. 2006 Association for Computational Linguistics @e (be ∧ hTENSEipres ∧ hMOODidcl ∧ hA RGi(d ∧ design ∧ hDETithe ∧ hNUMisg) ∧ hP ROPi(p ∧ based on ∧ hA RTIFACTid ∧ e be<TENSE>pres,<MOOD>dcl <ARG> hS OURCEi(c ∧ collection ∧ hDETithe ∧ hNUMisg ∧ <PROP> hH AS P ROPi(f ∧ Funny Day) ∧ hC REATORi(v ∧ V&B)"
W06-1403,A00-2023,0,0.775089,"atistical methods are used to select one or more outputs from this space. To specify the desired paraphrase space, one may either provide an input logical form that underspecifies certain realization choices, or include explicit disjunctions in the input LF (or both). Our experience suggests that disjunctive LFs are an important capability, especially as one seeks to make grammars reusable across applications, and to employ domain-specific, sentence-level paraphrases (Barzilay and Lee, 2003). Prominent examples of surface realizers in the generate-and-select paradigm include Nitrogen/Halogen (Langkilde, 2000; Langkilde-Geary, 2002) and Fergus (Bangalore and Rambow, 2000). More recently, generate-and-select realizers in the chart realization tradition (Kay, 1996) have appeared, including the OpenCCG (White, 2004) 12 Proceedings of the Fourth International Natural Language Generation Conference, pages 12–19, c Sydney, July 2006. 2006 Association for Computational Linguistics @e (be ∧ hTENSEipres ∧ hMOODidcl ∧ hA RGi(d ∧ design ∧ hDETithe ∧ hNUMisg) ∧ hP ROPi(p ∧ based on ∧ hA RTIFACTid ∧ e be<TENSE>pres,<MOOD>dcl <ARG> hS OURCEi(c ∧ collection ∧ hDETithe ∧ hNUMisg ∧ <PROP> hH AS P ROPi(f ∧ Funny Da"
W06-1403,P06-1140,1,0.839299,"Missing"
W06-1403,W06-1405,0,\N,Missing
W07-2305,W06-1405,0,0.0167762,"cored nearly as high on an automated evaluation as those from the optimised system, and also made use of a wider range of corpus data. Belz and Reiter’s (2006) “greedy roulette” pCRU text-generation system selected among generation rules weighted by their corpus probabilities, while Foster and Oberlander (2006) used a similar technique to select facial displays for an animated talking head. Both of these systems scored higher on a human evaluation than at least one competing system that always chose the single highest-scoring option; see Section 7 for further discussion. The C R AG -2 system (Isard et al., 2006) generates dialogues between pairs of agents who are linguistically distinguishable but able to align with each other. It uses the OpenCCG surface realiser to select appropriate paraphrases for the desired personality of the simulated character and the stage of the dialogue, integrating cache models built from the preceding discourse with the primary n-gram models to attain lexico-syntactic alignment. The 34 method of anti-repetition scoring described in this paper is similar, but the goal is opposite: instead of increasing alignment with an interlocutor, here we modify the n-gram scores to av"
W07-2305,P96-1027,0,0.259341,"Oh and Rudnicky, 2002). Surface Realisation with OpenCCG The studies described in this paper use the OpenCCG open source surface realiser (White, 2006a,b), which is based on Steedman’s (2000) Combinatory Categorial Grammar (CCG). A distinguishing feature of OpenCCG is that it uses a hybrid symbolic-statistical chart realisation algorithm combining (1) a theoretically grounded approach to syntax and semantic composition with (2) integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the traditions of symbolic chart realisation (Kay, 1996; Carroll et al., 1999) and statistical realisation (Langkilde and Knight, 1998; Langkilde, 2000; Bangalore and Rambow, 2000; LangkildeGeary, 2002). Another recent approach to combining these traditions appears in (Carroll and Oepen, 2005), where parse selection techniques are incorporated into an HPSG realiser. In OpenCCG, the search for complete realisations makes use of n-gram language models and proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configurable time limit: the scores assigned by the n-gram model"
W07-2305,W02-2103,0,0.239921,"Missing"
W07-2305,C00-1007,0,0.0628971,"G open source surface realiser (White, 2006a,b), which is based on Steedman’s (2000) Combinatory Categorial Grammar (CCG). A distinguishing feature of OpenCCG is that it uses a hybrid symbolic-statistical chart realisation algorithm combining (1) a theoretically grounded approach to syntax and semantic composition with (2) integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the traditions of symbolic chart realisation (Kay, 1996; Carroll et al., 1999) and statistical realisation (Langkilde and Knight, 1998; Langkilde, 2000; Bangalore and Rambow, 2000; LangkildeGeary, 2002). Another recent approach to combining these traditions appears in (Carroll and Oepen, 2005), where parse selection techniques are incorporated into an HPSG realiser. In OpenCCG, the search for complete realisations makes use of n-gram language models and proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configurable time limit: the scores assigned by the n-gram model determine the order of the edges on the agenda, and thus have an impact on realisation speed. In the two-stage mode, a pack"
W07-2305,N03-1003,0,0.0185335,"hm. Section 5 next presents the result of the human evaluation study. In Section 6, we then explore the impact of the two anti-repetition methods on the variability and quality of the generated text, using a range of parameter settings. In Section 7, we discuss the results of both studies and compare them with related work. Finally, in Section 8, we give some conclusions and outline possible extensions to this work. 2 Previous Work The acquisition and generation of paraphrases has been studied for some time (cf. Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003). Much recent work in this area has focussed on the automated acquisition of paraphrases from corpora, along with the use of the resulting paraphrases in language-processing areas such as information extraction and retrieval, questionanswering, and machine translation. The main technique that has been used for adding variation to stochastically-generated output is to modify the system so that it does not always choose the same option in a given situation, normally by modifying either the weights or the selection strategy. When selecting a combination of speech and body-lang"
W07-2305,N03-1024,0,0.0370774,"nts the result of the human evaluation study. In Section 6, we then explore the impact of the two anti-repetition methods on the variability and quality of the generated text, using a range of parameter settings. In Section 7, we discuss the results of both studies and compare them with related work. Finally, in Section 8, we give some conclusions and outline possible extensions to this work. 2 Previous Work The acquisition and generation of paraphrases has been studied for some time (cf. Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003). Much recent work in this area has focussed on the automated acquisition of paraphrases from corpora, along with the use of the resulting paraphrases in language-processing areas such as information extraction and retrieval, questionanswering, and machine translation. The main technique that has been used for adding variation to stochastically-generated output is to modify the system so that it does not always choose the same option in a given situation, normally by modifying either the weights or the selection strategy. When selecting a combination of speech and body-language output for an a"
W07-2305,P01-1008,0,0.014744,"nto this realisation algorithm. Section 5 next presents the result of the human evaluation study. In Section 6, we then explore the impact of the two anti-repetition methods on the variability and quality of the generated text, using a range of parameter settings. In Section 7, we discuss the results of both studies and compare them with related work. Finally, in Section 8, we give some conclusions and outline possible extensions to this work. 2 Previous Work The acquisition and generation of paraphrases has been studied for some time (cf. Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003). Much recent work in this area has focussed on the automated acquisition of paraphrases from corpora, along with the use of the resulting paraphrases in language-processing areas such as information extraction and retrieval, questionanswering, and machine translation. The main technique that has been used for adding variation to stochastically-generated output is to modify the system so that it does not always choose the same option in a given situation, normally by modifying either the weights or the selection strategy. When selecting a combination"
W07-2305,E06-1040,0,0.252377,"hat it can be beneficial for a natural language generation system to strive to avoid repetition, we first conducted a human evaluation study in which subjects were asked to compare texts generated with and without the two variation-enhancing methods. Strikingly, subjects judged the versions generated using ε-best sampling and anti-repetition scoring to be both better written and less repetitive than the versions generated with optimal n-gram scoring. To our knowledge, this study is the first to show a clear benefit for enhancing variation; while other recent studies (e.g., Stent et al., 2005; Belz and Reiter, 2006) have shown that automatic evaluation metrics do not always correlate well with human judgments of high quality generated texts with periphrastic variations, these studies examined sentences out of context, and thus could not take into account the benefit of avoiding repetition as a discourse progresses. Following the human evaluation study, we varied the main parameters used in ε-best sampling and anti-repetition scoring and analysed the resulting impact on the amount of periphrastic variation and the number of dispreferred paraphrases in the generated outputs. The analysis revealed that the"
W07-2305,I05-1015,0,0.0439017,"CCG). A distinguishing feature of OpenCCG is that it uses a hybrid symbolic-statistical chart realisation algorithm combining (1) a theoretically grounded approach to syntax and semantic composition with (2) integrated language models for making choices among the options left open by the grammar. In so doing, it brings together the traditions of symbolic chart realisation (Kay, 1996; Carroll et al., 1999) and statistical realisation (Langkilde and Knight, 1998; Langkilde, 2000; Bangalore and Rambow, 2000; LangkildeGeary, 2002). Another recent approach to combining these traditions appears in (Carroll and Oepen, 2005), where parse selection techniques are incorporated into an HPSG realiser. In OpenCCG, the search for complete realisations makes use of n-gram language models and proceeds in one of two modes, anytime or two-stage (packing/unpacking). In the anytime mode, a best-first search is performed with a configurable time limit: the scores assigned by the n-gram model determine the order of the edges on the agenda, and thus have an impact on realisation speed. In the two-stage mode, a packed forest of all possible realisations is created in the first stage; in the second stage, the packed representatio"
W07-2305,J05-1002,0,0.104413,"Missing"
W07-2305,E06-1045,1,0.840934,"y. When selecting a combination of speech and body-language output for an animated character based on a corpus of recorded behaviour, for example, Stone et al. (2004) introduced variation by perturbing the scores slightly to choose from among low-cost utterances. The outputs from the system with perturbed weights scored nearly as high on an automated evaluation as those from the optimised system, and also made use of a wider range of corpus data. Belz and Reiter’s (2006) “greedy roulette” pCRU text-generation system selected among generation rules weighted by their corpus probabilities, while Foster and Oberlander (2006) used a similar technique to select facial displays for an animated talking head. Both of these systems scored higher on a human evaluation than at least one competing system that always chose the single highest-scoring option; see Section 7 for further discussion. The C R AG -2 system (Isard et al., 2006) generates dialogues between pairs of agents who are linguistically distinguishable but able to align with each other. It uses the OpenCCG surface realiser to select appropriate paraphrases for the desired personality of the simulated character and the stage of the dialogue, integrating cache"
W07-2305,W05-1104,1,0.826386,"eness may take several forms: using the same words or syntactic structures, repeatedly giving the same facts, or even repeating entire turns (for example, error-handling turns in dialogue systems). 1 http://www.hcrc.ed.ac.uk/comic/ 33 means of adding variation, anti-repetition scoring, is to store the words from recently generated sentences and to penalise a proposed realisation based on the number of words that it shares with these sentences. OpenCCG provides a built-in facility for implementing such anti-repetition scorers and integrating them with the normal n-gram–based scoring algorithm (White, 2005). To verify that it can be beneficial for a natural language generation system to strive to avoid repetition, we first conducted a human evaluation study in which subjects were asked to compare texts generated with and without the two variation-enhancing methods. Strikingly, subjects judged the versions generated using ε-best sampling and anti-repetition scoring to be both better written and less repetitive than the versions generated with optimal n-gram scoring. To our knowledge, this study is the first to show a clear benefit for enhancing variation; while other recent studies (e.g., Stent e"
W07-2305,W04-0601,1,0.808363,"it as the subject), or may involve entirely different structures (e.g., here we have a design in the classic style vs. this design is classic). The algorithm uses packed representations similar to those initially proposed by Shemtov (1997), enabling it to run many times faster than sequential realisation of an equivalent set of non-disjunctive LFs. 4 Anti-Repetition Methods For both studies in this paper, we used OpenCCG to realise a range of texts describing and comparing bathroom-tile designs. The starting point for this implementation was the XSLT-based text planner from the COMIC system (Foster and White, 2004), which transforms sets of facts about tile designs into OpenCCG logical forms. We enhanced this text planner to produce disjunctive logical forms covering the full range of paraphrases permitted by the most recent version of the COMIC grammar, and then used OpenCCG realise those forms as text. In the normal OpenCCG realisation process outlined above, corpus-based n-grams are used to select the single highest-scoring realisation for a given logical form. To allow the realiser to choose paraphrases other than the top-scoring one, we modified the realisation process in two ways: ε-best sampling"
W07-2305,W06-1403,1,0.918352,"varying output requires choosing less frequent options, which inevitably reduces scores on corpus similarity measures. To the extent that corpus-based measures (such as n-gram scores) are used to avoid overgeneration and select preferred paraphrases, it is not obvious how to enhance variation without reducing output quality. With this question in mind, we investigate in this paper the impact of two different methods for enhancing variation in the output generated by the COMIC multimodal dialogue system.1 Both methods take advantage of the periphrastic ability of the OpenCCG surface realiser (White, 2006a). In the usual OpenCCG realisation process, when a logical form is transformed into output text, n-gram models are used to steer the realiser towards the single highest-scoring option for the sentence. This process tends to select the same syntactic structure for every sentence describing the same feature: for example, in the COMIC domain (describing and comparing bathroom tiles), the structure The colours are [colours] would be used every time the colours of a tile design are to be presented, even though alternative paraphrases are available. The first (and simplest) means of avoiding such"
W07-2305,E06-1036,0,0.0406033,"Missing"
W07-2305,A00-2023,0,\N,Missing
W07-2305,P98-1116,0,\N,Missing
W07-2305,C98-1112,0,\N,Missing
W08-1703,P08-1022,1,0.870084,"Missing"
W08-1703,J07-3004,0,0.375399,"sis of Punctuation for Broad-Coverage Surface Realization with CCG Michael White and Rajakrishnan Rajkumar Department of Linguistics The Ohio State University Columbus, OH, USA {mwhite,raja}@ling.osu.edu Abstract computational systems, punctuation provides disambiguation cues which can help parsers arrive at the correct parse. From a natural language generation standpoint, text without punctuation can be difficult to comprehend, or even misleading. In this paper, we describe a more precise analysis of punctuation for a bi-directional, broad coverage English grammar extracted from the CCGbank (Hockenmaier and Steedman, 2007). In contrast to previous work, which has been primarily oriented towards parsing, our goal has been to develop an analysis of punctuation that is well suited for both parsing and surface realization. In addition, while Briscoe and Doran have simply included punctuation rules in their manually written grammars, our approach has been to revise the CCGbank itself with punctuation categories and more precise linguistic analyses, and then to extract a grammar from the enhanced corpus. In developing our analysis, we illustrate how aspects of Briscoe’s (1994) approach, which relies on syntactic feat"
W08-1703,D07-1028,0,0.136633,"Missing"
W08-1703,W02-2103,0,0.140253,"Missing"
W08-1703,W05-1510,0,0.228556,"Missing"
W08-1703,2007.mtsummit-ucnlg.4,1,0.860239,"Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [. . . ] are modeled as modal relations. 3 The need for an OpenCCG analysis of punctuation The linguistic analysis aims to make a broad coverage OpenCCG grammar extracted from the CCGbank (White et al., 2007) more precise by adding lexicalized punctuation categories to deal with constructions involving punctuation. The original CCGbank corpus does not have lexical categories for punctuation; instead, punctuation marks carry categories derived from their part of speech tags and form part of a binary rule. It is assumed that there are no dependencies between words and punctuation marks and that the result of punctuation rules is the same as the nonpunctuation category. OpenCCG does not support non-combinatory binary rules, as they can be replaced by equivalent lexicalized categories with application"
W08-1703,P02-1041,0,0.0830274,"r (White, 2006) which takes logical forms as input and produces sentences by using CCG combinators to combine signs. Alternative realizations are ranked using integrated n-gram scoring. To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1. In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. hNUMisg); nodes are connected via dependency relations (e.g. hA RG 0i). Internally, such graphs are represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). In HLDS, each semantic head (corresponding to a node in the graph) is associated with a nominal that identifies its discourse referent, and relations between heads and their dependents Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [. . . ] are modeled as modal relations. 3 The need for an OpenCCG analysis of punctuation The linguistic analysis aims to make a broad coverage OpenCCG grammar extracted from the CCGbank (White et al., 2007) more precise by adding lexicalized punctuation categories to deal with constructions involving punctuation. The ori"
W08-1703,boxwell-white-2008-projecting,1,0.625813,"Missing"
W08-1703,forst-kaplan-2006-importance,0,\N,Missing
W09-1508,boxwell-white-2008-projecting,1,0.852698,"ew Corpus conversion and grammar extraction have traditionally been portrayed as tasks that are performed once and never again revisited (Burke et al., 2004). We report the successful implementation of an approach to these tasks that facilitates the improvement of grammar engineering as an evolving process. Taking the standard version of the CCGbank (Hockenmaier and Steedman, 2007) as input, our system then introduces greater depth of linguistic insight by augmenting it with attributes the original corpus lacks: Propbank roles and head lexicalization for case-marking prepositions (Boxwell and White, 2008), derivational re-structuring for punctuation analysis (White and Rajkumar, 2008), named entity annotation and lemmatization. Our implementation applies successive XSLT transforms controlled by Apache Ant (http://ant.apache.org/) to an XML translation of this corpus, finally producing an OpenCCG grammar (http://openccg. sourceforge.net/). This design is beneficial to grammar engineering both because of XSLT’s unique suitability to performing arbitrary transformations of XML trees and the fine-grained control that Ant provides. The resulting system enables state-of-the-art BLEU scores for surfa"
W09-1508,P08-1022,1,0.897279,"Missing"
W09-1508,J07-3004,0,0.0861129,"Missing"
W09-1508,W08-1703,1,0.80166,"Missing"
W09-1508,2007.mtsummit-ucnlg.4,1,0.846222,"Missing"
W11-1609,P02-1041,0,0.0658445,"Missing"
W11-1609,W05-0909,0,0.0604575,"Missing"
W11-1609,P05-1074,0,0.102165,"Missing"
W11-1609,N03-1003,0,0.0510485,"ordnet into the original reference sentences can increase the number of exact word matches with an MT system’s output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. Madnani (2010) has also shown that statistical machine translation technique can be employed in a monolingual setting, together with paraphrases acquired using Bannard and CallisonBurch’s (2005) pivot method, in order to enhance the tuning phase of training an MT system by augmenting a reference translation with automatic paraphrases. Earlier, Barzilay and Lee (2003) and Pang et al. (2003) developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences. By starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives. However, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar. By contrast, grammar-based paraphrasing methods in the natural language generation tradition (Iordanskaja et a"
W11-1609,J05-3002,0,0.029328,"ively (though the certain seems to have been mistakenly substituted for a certain). However, their system is not capable of generating a paraphrase with in the long run at the end of the sentence, nor can it rephrase insurance company’s overall allocation as overall allocations for insurance companies, which would seem to require access to more general grammatical knowledge. To combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1 The task is not unrelated to sentence fusion in multidocument summarization (Barzilay and McKeown, 2005), except there the goal is to produce a single, shorter sentence from multiple related input sentences. 75 tage of the OpenCCG realizer’s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White, 2006a; White, 2006b; Nakatsu and White, 2006; Espinosa et al., 2008; White and Rajkumar, 2009). In principle, semantic dependency graphs offer a better starting point for paraphrasing than the syntax trees employed by Pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than"
W11-1609,boxwell-white-2008-projecting,1,0.862058,"te and Rajkumar, 2009), where the latter includes syntactic features from Clark and Curran’s (2007) normal form model as well as discriminative n-gram features (Roark et al., 2004). Hypertagging (Espinosa et al., 2008), or supertagging for surface realization, makes it practical to work with broad coverage grammars. For parsing, an implementation of Hockenmaier and Steedman’s (2002) generative model is used to select the best parse. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). A distinctive feature of OpenCCG is the ability to generate from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (Nakatsu and White, 2006), and avoiding repetition in the output of a dialogue system (Foster and White, 2007). Disjunctive inputs make it possible to exert fine-grained control over the specified paraphrase space. In the chart realization tradition, previous work has not generally supported disjunctive logical forms, with Shemtov’s (Shemtov, 1997) more complex appr"
W11-1609,J07-4004,0,0.0532788,"Missing"
W11-1609,J08-4005,0,0.0487996,"Missing"
W11-1609,J97-2001,0,0.0234445,"et al. (2003) developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences. By starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives. However, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar. By contrast, grammar-based paraphrasing methods in the natural language generation tradition (Iordanskaja et al., 1991; Elhadad et al., 1997; Langkilde and Knight, 1998; Stede, 1999; Langkilde-Geary, 2002; Velldal et al., 2004; Gardent and Kow, 2005; Hogan et al., 2008) have the potential to produce many such grammatical alternatives: in particular, by parsing a reference sentence to a representation that can be used as the input to a surface realizer, grammar-based paraphrases can be generated if the realizer supports n-best output. To our knowledge though, methods of using a grammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74 Workshop on Monolingual Text-To-Text Generation, p"
W11-1609,P08-1022,1,0.901801,"e access to more general grammatical knowledge. To combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1 The task is not unrelated to sentence fusion in multidocument summarization (Barzilay and McKeown, 2005), except there the goal is to produce a single, shorter sentence from multiple related input sentences. 75 tage of the OpenCCG realizer’s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White, 2006a; White, 2006b; Nakatsu and White, 2006; Espinosa et al., 2008; White and Rajkumar, 2009). In principle, semantic dependency graphs offer a better starting point for paraphrasing than the syntax trees employed by Pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees. Our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the MT (Och and Ney, 2003; Haghighi et al., 2009, for example) or textual inference (MacCartney et al., 2008, inter alia) literature in principle."
W11-1609,W07-2305,1,0.856953,"ation of Hockenmaier and Steedman’s (2002) generative model is used to select the best parse. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). A distinctive feature of OpenCCG is the ability to generate from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (Nakatsu and White, 2006), and avoiding repetition in the output of a dialogue system (Foster and White, 2007). Disjunctive inputs make it possible to exert fine-grained control over the specified paraphrase space. In the chart realization tradition, previous work has not generally supported disjunctive logical forms, with Shemtov’s (Shemtov, 1997) more complex approach as the only published exception. An example disjunctive input from the COMIC system appears in Figure 1(c).2 Semantic dependency graphs such as these—represented internally in Hybrid Logic Dependency Semantics 2 To simplify the exposition, the features specifying information structure and deictic gestures have been omitted, as have the"
W11-1609,W05-1605,0,0.026473,"es and generate new sentences. By starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives. However, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar. By contrast, grammar-based paraphrasing methods in the natural language generation tradition (Iordanskaja et al., 1991; Elhadad et al., 1997; Langkilde and Knight, 1998; Stede, 1999; Langkilde-Geary, 2002; Velldal et al., 2004; Gardent and Kow, 2005; Hogan et al., 2008) have the potential to produce many such grammatical alternatives: in particular, by parsing a reference sentence to a representation that can be used as the input to a surface realizer, grammar-based paraphrases can be generated if the realizer supports n-best output. To our knowledge though, methods of using a grammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74 Workshop on Monolingual Text-To-Text Generation, pages 74–83, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 74"
W11-1609,P09-1104,0,0.0183088,"cked semantic dependency graphs (White, 2004; White, 2006a; White, 2006b; Nakatsu and White, 2006; Espinosa et al., 2008; White and Rajkumar, 2009). In principle, semantic dependency graphs offer a better starting point for paraphrasing than the syntax trees employed by Pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees. Our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the MT (Och and Ney, 2003; Haghighi et al., 2009, for example) or textual inference (MacCartney et al., 2008, inter alia) literature in principle. The alignments are projected onto the logical forms that result from automatically parsing these sentences. The projected alignments are then converted into phrasal edits for producing DLFs in both directions, where the disjunctions represent alternative choices at the level of semantic dependencies. The resulting DLFs are fed into the OpenCCG realizer for n-best realization. In order to enhance the variety of word and phrase choices in the n-best lists, a pruning strategy is used that encourages"
W11-1609,P02-1043,0,0.0255814,"Missing"
W11-1609,J07-3004,0,0.0309157,"items. Alternative realizations are scored using integrated n-gram and perceptron models (White and Rajkumar, 2009), where the latter includes syntactic features from Clark and Curran’s (2007) normal form model as well as discriminative n-gram features (Roark et al., 2004). Hypertagging (Espinosa et al., 2008), or supertagging for surface realization, makes it practical to work with broad coverage grammars. For parsing, an implementation of Hockenmaier and Steedman’s (2002) generative model is used to select the best parse. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). A distinctive feature of OpenCCG is the ability to generate from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (Nakatsu and White, 2006), and avoiding repetition in the output of a dialogue system (Foster and White, 2007). Disjunctive inputs make it possible to exert fine-grained control over the specified paraphrase space. In the chart realization tradition, previous work has not generally"
W11-1609,W08-1122,0,0.0534758,"Missing"
W11-1609,N06-1058,0,0.023906,"ch original sentence, as well as paraphrases that mix and match content from the pair. A preliminary error analysis suggests that the approach could benefit from taking the word order in the original sentences into account. We conclude with a discussion of plans for future work, highlighting the method’s potential use in enhancing automatic MT evaluation. 1 Introduction In this paper, we present our initial steps towards merging the grammar-based and data-driven paraphrasing traditions, highlighting the potential of our approach to enhance the automatic evaluation of machine translation (MT). Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system’s output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. Madnani (2010) has also shown that statistical machine translation technique can be employed in a monolingual setting, together with paraphrases acquired using Bannard and CallisonBurch’s (2005) pivot method, in order to enhance the tuning phase of training a"
W11-1609,P96-1027,0,0.106588,"pair. The rest of the paper is organized as follows. Section 2 provides background on surface realization with OpenCCG and DLFs. Section 3 describes our @e (be ∧ hTENSEipres hA RG ∧ design @e (be ∧ hi(d TENSE ipres method of creating DLFs from aligned paraphrases. Finally, Section 4 characterizes the recurring errors and concludes with a discussion of related and future work. 2 Surface Realization with OpenCCG OpenCCG is an open source Java library for parsing and realization using Baldridge’s multimodal extensions to CCG (Steedman, 2000; Baldridge, 2002). In the chart realization tradition (Kay, 1996), the OpenCCG realizer takes logical forms as input and produces strings by combining signs for lexical items. Alternative realizations are scored using integrated n-gram and perceptron models (White and Rajkumar, 2009), where the latter includes syntactic features from Clark and Curran’s (2007) normal form model as well as discriminative n-gram features (Roark et al., 2004). Hypertagging (Espinosa et al., 2008), or supertagging for surface realization, makes it practical to work with broad coverage grammars. For parsing, an implementation of Hockenmaier and Steedman’s (2002) generative model"
W11-1609,W98-1426,0,0.0479646,"d approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences. By starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives. However, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar. By contrast, grammar-based paraphrasing methods in the natural language generation tradition (Iordanskaja et al., 1991; Elhadad et al., 1997; Langkilde and Knight, 1998; Stede, 1999; Langkilde-Geary, 2002; Velldal et al., 2004; Gardent and Kow, 2005; Hogan et al., 2008) have the potential to produce many such grammatical alternatives: in particular, by parsing a reference sentence to a representation that can be used as the input to a surface realizer, grammar-based paraphrases can be generated if the realizer supports n-best output. To our knowledge though, methods of using a grammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74 Workshop on Monolingual Text-To-Text Generation, pages 74–83, Proceedings of t"
W11-1609,W02-2103,0,0.0211779,"ce translations in order to extract paraphrases and generate new sentences. By starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives. However, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar. By contrast, grammar-based paraphrasing methods in the natural language generation tradition (Iordanskaja et al., 1991; Elhadad et al., 1997; Langkilde and Knight, 1998; Stede, 1999; Langkilde-Geary, 2002; Velldal et al., 2004; Gardent and Kow, 2005; Hogan et al., 2008) have the potential to produce many such grammatical alternatives: in particular, by parsing a reference sentence to a representation that can be used as the input to a surface realizer, grammar-based paraphrases can be generated if the realizer supports n-best output. To our knowledge though, methods of using a grammar-based surface realizer together with multiple aligned reference sentences to produce synthetic 74 Workshop on Monolingual Text-To-Text Generation, pages 74–83, Proceedings of the 49th Annual Meeting of the Associ"
W11-1609,D08-1084,0,0.05548,"Missing"
W11-1609,P06-1140,1,0.898353,"hich would seem to require access to more general grammatical knowledge. To combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1 The task is not unrelated to sentence fusion in multidocument summarization (Barzilay and McKeown, 2005), except there the goal is to produce a single, shorter sentence from multiple related input sentences. 75 tage of the OpenCCG realizer’s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White, 2006a; White, 2006b; Nakatsu and White, 2006; Espinosa et al., 2008; White and Rajkumar, 2009). In principle, semantic dependency graphs offer a better starting point for paraphrasing than the syntax trees employed by Pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees. Our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the MT (Och and Ney, 2003; Haghighi et al., 2009, for example) or textual inference (MacCartney et al., 2008, inter alia) li"
W11-1609,J03-1002,0,0.00248083,"rms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White, 2006a; White, 2006b; Nakatsu and White, 2006; Espinosa et al., 2008; White and Rajkumar, 2009). In principle, semantic dependency graphs offer a better starting point for paraphrasing than the syntax trees employed by Pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees. Our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the MT (Och and Ney, 2003; Haghighi et al., 2009, for example) or textual inference (MacCartney et al., 2008, inter alia) literature in principle. The alignments are projected onto the logical forms that result from automatically parsing these sentences. The projected alignments are then converted into phrasal edits for producing DLFs in both directions, where the disjunctions represent alternative choices at the level of semantic dependencies. The resulting DLFs are fed into the OpenCCG realizer for n-best realization. In order to enhance the variety of word and phrase choices in the n-best lists, a pruning strategy"
W11-1609,J05-1004,0,0.00854603,"g integrated n-gram and perceptron models (White and Rajkumar, 2009), where the latter includes syntactic features from Clark and Curran’s (2007) normal form model as well as discriminative n-gram features (Roark et al., 2004). Hypertagging (Espinosa et al., 2008), or supertagging for surface realization, makes it practical to work with broad coverage grammars. For parsing, an implementation of Hockenmaier and Steedman’s (2002) generative model is used to select the best parse. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). A distinctive feature of OpenCCG is the ability to generate from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enabling the selection of realizations according to predicted synthesis quality (Nakatsu and White, 2006), and avoiding repetition in the output of a dialogue system (Foster and White, 2007). Disjunctive inputs make it possible to exert fine-grained control over the specified paraphrase space. In the chart realization tradition, previous work has not generally supported disjunctive logical forms"
W11-1609,N03-1024,0,0.0352058,"erence sentences can increase the number of exact word matches with an MT system’s output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. Madnani (2010) has also shown that statistical machine translation technique can be employed in a monolingual setting, together with paraphrases acquired using Bannard and CallisonBurch’s (2005) pivot method, in order to enhance the tuning phase of training an MT system by augmenting a reference translation with automatic paraphrases. Earlier, Barzilay and Lee (2003) and Pang et al. (2003) developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences. By starting with reference sentences from multiple human translators, these data-driven methods are able to capture subtle, highly-context sensitive word and phrase alternatives. However, the methods are not particularly adept at capturing variation in word order or the use of function words that follow from general principles of grammar. By contrast, grammar-based paraphrasing methods in the natural language generation tradition (Iordanskaja et al., 1991; Elhadad et al"
W11-1609,P02-1040,0,0.0824897,"the method’s potential use in enhancing automatic MT evaluation. 1 Introduction In this paper, we present our initial steps towards merging the grammar-based and data-driven paraphrasing traditions, highlighting the potential of our approach to enhance the automatic evaluation of machine translation (MT). Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system’s output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. Madnani (2010) has also shown that statistical machine translation technique can be employed in a monolingual setting, together with paraphrases acquired using Bannard and CallisonBurch’s (2005) pivot method, in order to enhance the tuning phase of training an MT system by augmenting a reference translation with automatic paraphrases. Earlier, Barzilay and Lee (2003) and Pang et al. (2003) developed approaches to aligning multiple reference translations in order to extract paraphrases and generate new sentences. By starting with reference s"
W11-1609,P04-1007,0,0.0143409,"uture work. 2 Surface Realization with OpenCCG OpenCCG is an open source Java library for parsing and realization using Baldridge’s multimodal extensions to CCG (Steedman, 2000; Baldridge, 2002). In the chart realization tradition (Kay, 1996), the OpenCCG realizer takes logical forms as input and produces strings by combining signs for lexical items. Alternative realizations are scored using integrated n-gram and perceptron models (White and Rajkumar, 2009), where the latter includes syntactic features from Clark and Curran’s (2007) normal form model as well as discriminative n-gram features (Roark et al., 2004). Hypertagging (Espinosa et al., 2008), or supertagging for surface realization, makes it practical to work with broad coverage grammars. For parsing, an implementation of Hockenmaier and Steedman’s (2002) generative model is used to select the best parse. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al., 2005) roles projected onto it (Boxwell and White, 2008). A distinctive feature of OpenCCG is the ability to generate from disjunctive logical forms (White, 2006a). This capability has many benefits, such as enab"
W11-1609,2006.amta-papers.25,0,0.0175184,"ber of acceptable paraphrases in the n-best list. With the extrinsic evaluation, we plan to investigate whether n-best paraphrase generation using the methods described here can be used to augment a set of reference translations in such a way as to increase the correlation of automatic metrics with human judgments. As Madnani observes, generated paraphrases of reference translations may be either untargeted or targeted to specific MT hypotheses. In the case of targeted paraphrases, the generated paraphrases then approximate the process by which automatic translations are evaluated using HTER (Snover et al., 2006), with a human in the loop, as the closest acceptable paraphrase of a reference sentence should correspond to the version of the MT hypothesis with minimal changes to make it acceptable. While in principle we might similarly acquire paraphrase rules using the pivot method, as in Madnani’s approach, such rules would be quite noisy, as it is a difficult problem to characterize the contexts in which words or phrases can be acceptably substituted. Thus, our immediate focus will be on generating synthetic references with high precision, re81 lying on grammatical alternations plus contextually accep"
W11-1609,D09-1043,1,0.905721,"l grammatical knowledge. To combine grammar-based paraphrasing with lexical and phrasal alternatives gleaned from multiple reference sentences, our approach takes advan1 The task is not unrelated to sentence fusion in multidocument summarization (Barzilay and McKeown, 2005), except there the goal is to produce a single, shorter sentence from multiple related input sentences. 75 tage of the OpenCCG realizer’s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White, 2006a; White, 2006b; Nakatsu and White, 2006; Espinosa et al., 2008; White and Rajkumar, 2009). In principle, semantic dependency graphs offer a better starting point for paraphrasing than the syntax trees employed by Pang et. al, as paraphrases can generally be expected to be more similar at the level of unordered semantic dependencies than at the level of syntax trees. Our method starts with word-level alignments of two sentences that are paraphrases, since the approach can be used with any alignment method from the MT (Och and Ney, 2003; Haghighi et al., 2009, for example) or textual inference (MacCartney et al., 2008, inter alia) literature in principle. The alignments are projecte"
W11-1609,P09-1094,0,0.0184369,"n], in terms of capital allocation, overseas investment should occupy the certain ratio of an [insurance company’s overall allocation] Table 1: Zhao et al.’s (2009) similarity example, with italics added to show word-level substitutions, and square brackets added to show phrase location or construction mismatches. Here, the source sentence (itself a reference translation) has been paraphrased to be more like the reference sentence. references have not been investigated.1 As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al. (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. Zhao et al. make use of a large paraphrase table, similar to the phrase tables used in statistical MT, in order to construct paraphrase candidates. (Like thesauri or WordNet, such resources are complementary to the ones we make use of here.) To test their system’s ability to paraphrase reference sentences in service of MT evaluation, they attempt to paraphrase one reference translation to make it more similar to another reference translation; thus, in Table 1, the source sentence (itself"
W11-1609,W06-1403,1,\N,Missing
W11-2706,P02-1041,0,0.0390531,"ess in my heart. (6) ? I realized with sadness in my heart [he had done it]. Background CCG (Steedman, 2000) is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode subcategorization as well as syntactic features (e.g. number and agreement). OpenCCG is a parsing/generation library which includes a hybrid symbolic-statistical chart realizer (White, 2006). The chart realizer takes as input logical forms represented internally using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1. In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. s[dcl]
p/np have.03 <TENSE>pres <Arg0> h1 <Arg1> n he point h2 np p1 <NUM>sg <Arg1> <GenRel> <Det> s[dcl]
p/(s[to]
p) a1 a w1 np h3 <TENSE>pres <Arg1> <Arg0> np/n want.01 he m1 make.03 s[b]
p/np <Arg0> Figure 1: Semantic dependency graph from the CCGbank for He has a point he wants to make [. . . ], along with gold-standard supertags (category labels) hNUMisg); nodes are connected via dependency relat"
W11-2706,J07-4004,0,0.14953,"Missing"
W11-2706,W09-0103,0,0.0234494,"t-mentioning. Our experiments confirm the efficacy of the features based on Jaeger’s work, including information density–based features. The experiments also show that the improvements in prediction accuracy apply to cases in which the presence of a that-complementizer arguably makes a substantial difference to fluency or intelligiblity. Our ultimate goal is to improve the performance of a ranking model for surface realization, and to this end we conclude with a discussion of how we plan to combine the local complementizer-choice features with those in the global ranking model. 1 Introduction Johnson (2009) observes that in developing statistical parsing models, “shotgun” features — that is, myriad scattershot features that pay attention to superficial aspects of structure — tend to be remarkably useful, while features based on linguistic theory seem to be of more questionable utility, with the most basic linguistic insights tending to have the 1 The term “shotgun” feature appears in the slides for Johnson’s talk (http://www.cog.brown.edu/˜mj/ papers/johnson-eacl09-workshop.pdf), rather than in the paper itself. 2 For German surface realization, Cahill and Riester (2009) show that incorporating"
W11-2706,W05-1510,0,0.227628,"prepositions are adjusted to reflect their purely syntactic status. Alternative realizations are ranked using an averaged perceptron model described in the next section. 3 Feature Design White and Rajkumar’s (2009) realization ranking model serves as the baseline for this paper. It is a global, averaged perceptron ranking model using three kinds of features: (1) the log probability of the candidate realization’s word sequence according to three linearly interpolated language models (as well as a feature for each component model), much as in the log-linear models of Velldal & Oepen (2005) and Nakanishi et al. (2005); (2) integer-valued syntactic features, representing counts of occurrences in a derivation, from Clark & Curran’s (2007) normal form model; and (3) discriminative n-gram features 41 (Roark et al., 2004), which count the occurrences of each n-gram in the word sequence. Table 1 shows the new complementizer-choice features investigated in this paper. The example features mentioned in the table are taken from the two complement clause (CC) forms (with-that CC vs. that-less CC) of the sentence below: (7) The finding probably will support those who argue [ that/∅ the U.S. should regulate the class"
W11-2706,P04-1007,0,0.0363798,"s (2009) realization ranking model serves as the baseline for this paper. It is a global, averaged perceptron ranking model using three kinds of features: (1) the log probability of the candidate realization’s word sequence according to three linearly interpolated language models (as well as a feature for each component model), much as in the log-linear models of Velldal & Oepen (2005) and Nakanishi et al. (2005); (2) integer-valued syntactic features, representing counts of occurrences in a derivation, from Clark & Curran’s (2007) normal form model; and (3) discriminative n-gram features 41 (Roark et al., 2004), which count the occurrences of each n-gram in the word sequence. Table 1 shows the new complementizer-choice features investigated in this paper. The example features mentioned in the table are taken from the two complement clause (CC) forms (with-that CC vs. that-less CC) of the sentence below: (7) The finding probably will support those who argue [ that/∅ the U.S. should regulate the class of asbestos including crocidolite more stringently than the common kind of asbestos, chrysotile, found in most schools and other buildings], Dr. Talcott said. (WSJ0003.19) The first class of features, de"
W11-2706,2005.mtsummit-papers.15,0,0.0193973,"subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status. Alternative realizations are ranked using an averaged perceptron model described in the next section. 3 Feature Design White and Rajkumar’s (2009) realization ranking model serves as the baseline for this paper. It is a global, averaged perceptron ranking model using three kinds of features: (1) the log probability of the candidate realization’s word sequence according to three linearly interpolated language models (as well as a feature for each component model), much as in the log-linear models of Velldal & Oepen (2005) and Nakanishi et al. (2005); (2) integer-valued syntactic features, representing counts of occurrences in a derivation, from Clark & Curran’s (2007) normal form model; and (3) discriminative n-gram features 41 (Roark et al., 2004), which count the occurrences of each n-gram in the word sequence. Table 1 shows the new complementizer-choice features investigated in this paper. The example features mentioned in the table are taken from the two complement clause (CC) forms (with-that CC vs. that-less CC) of the sentence below: (7) The finding probably will support those who argue [ that/∅ the U.S"
W11-2706,D09-1043,1,0.924632,"Missing"
W11-2827,P02-1041,0,0.166878,"agment concatenations to be explored, and since these rules are integrated into the general chart realization framework, they remain compatible with returning n-best outputs and allowing disjunctively specified inputs, in contrast to the earlier greedy concatenation method.3 2 Background OpenCCG is a parsing/generation library for CCG which includes a hybrid symbolic-statistical chart realizer (White, 2006b). The chart realizer takes as input (quasi-) logical forms (LFs) represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002); see White (2006b) for discussion. Semantic dependency graphs are derived from the CCGbank (Hockenmaier and Steedman, 2007), modified to incorporate Propbank roles (Boxwell and White, 2008), where semantically empty function words such as complementizers, relativizers, infinitival-to, and expletive subjects are adjusted to reflect their purely syntactic status. Lexical category assignments are statistically filtered in a hypertagging step (Espinosa et 2 http://openccg.sf.net While the greedy approach to fragment assembly could conceivably be generalized to a beam search that respected disjunc"
W11-2827,boxwell-white-2008-projecting,1,0.855475,"tively specified inputs, in contrast to the earlier greedy concatenation method.3 2 Background OpenCCG is a parsing/generation library for CCG which includes a hybrid symbolic-statistical chart realizer (White, 2006b). The chart realizer takes as input (quasi-) logical forms (LFs) represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002); see White (2006b) for discussion. Semantic dependency graphs are derived from the CCGbank (Hockenmaier and Steedman, 2007), modified to incorporate Propbank roles (Boxwell and White, 2008), where semantically empty function words such as complementizers, relativizers, infinitival-to, and expletive subjects are adjusted to reflect their purely syntactic status. Lexical category assignments are statistically filtered in a hypertagging step (Espinosa et 2 http://openccg.sf.net While the greedy approach to fragment assembly could conceivably be generalized to a beam search that respected disjunctive constraints, doing so would introduce considerable redundancy with the core chart realization algorithm; indeed, generalizing the greedy approach by reusing the existing chart realizati"
W11-2827,P06-1130,0,0.417893,"Missing"
W11-2827,I05-1015,0,0.144554,"Missing"
W11-2827,J07-2003,0,0.161419,"Missing"
W11-2827,J07-4004,0,0.0673614,"Missing"
W11-2827,P08-1022,1,0.926756,"Missing"
W11-2827,N09-3004,0,0.0312616,"Missing"
W11-2827,C08-1038,0,0.0392624,"Missing"
W11-2827,J07-3004,0,0.0502725,"they remain compatible with returning n-best outputs and allowing disjunctively specified inputs, in contrast to the earlier greedy concatenation method.3 2 Background OpenCCG is a parsing/generation library for CCG which includes a hybrid symbolic-statistical chart realizer (White, 2006b). The chart realizer takes as input (quasi-) logical forms (LFs) represented using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002); see White (2006b) for discussion. Semantic dependency graphs are derived from the CCGbank (Hockenmaier and Steedman, 2007), modified to incorporate Propbank roles (Boxwell and White, 2008), where semantically empty function words such as complementizers, relativizers, infinitival-to, and expletive subjects are adjusted to reflect their purely syntactic status. Lexical category assignments are statistically filtered in a hypertagging step (Espinosa et 2 http://openccg.sf.net While the greedy approach to fragment assembly could conceivably be generalized to a beam search that respected disjunctive constraints, doing so would introduce considerable redundancy with the core chart realization algorithm; indeed, genera"
W11-2827,D07-1028,0,0.0273136,"Missing"
W11-2827,P96-1027,0,0.588848,"glue rules are only invoked after the chart has been completed with no grammatically complete derivation found to cover the input, and then only when the glue rule fills in an empty cell (i.e. set of covered elementary predications, or EPs). Additionally, to aid in the search for a fragment that covers the input completely, edges on the realizer’s agenda are sorted first by the number of covered EPs, and secondarily by their model score. The second twist concerns the LF chunking constraints in the realizer. In order to address the problem of proliferating semantically incomplete constituents (Kay, 1996), OpenCCG requires all the EPs in an LF chunk—by default, a non-trivial subtree in the input—to be covered by an edge before combination is allowed with another edge with EPs outside the chunk (White, 2006b). To effectively relax these constraints, if there are elementary predications within an LF chunk which are not covered by any lexical items or instantiated unary rules, those EPs are made optional; similarly, the EPs for instantiated unary rules are made optional, so that they can 196 Preds: ep[0]: ep[1]: ep[2]: ep[3]: ep[4]: ep[5]: ep[6]: ep[7]: ep[8]: ep[9]: ep[10]: ep[11]: @p(pro2) @c(c"
W11-2827,W05-1510,0,0.0456751,"Missing"
W11-2827,C10-2119,1,0.781466,"part of speech (POS) of the head and the relation between the head and the dependent, with different combinations of words and POS tags. The features at the bottom record the order of sibling dependent words appearing on the same side of the head word, similarly grouped by the broad POS of the head and at different granularities of word or POS tag, and additionally with relation-relation orderings. Table 2 shows the results of reverse realization with OpenCCG on the development section of the 7 Features incorporating named entity classes (Rajkumar et al., 2009) and targeting agreement errors (Rajkumar and White, 2010) were not used in the experiments reported here. Feature Type HeadBroadPos + Rel + Precedes + HeadWord + DepWord . . . + HeadWord + DepPOS . . . + HeadPOS + DepWord . . . + HeadWord + DepPOS HeadBroadPos + Side + DepWord1 + DepWord2 . . . + DepWord1 + DepPOS2 . . . + DepPOS1 + DepWord2 . . . + DepPOS1 + DepPOS2 . . . + Rel1 + Rel2 Example hVB, Arg0, dep, wants, hei hVB, Arg0, dep, wants, PRPi hVB, Arg0, dep, VBZ, hei hVB, Arg0, dep, VBZ, PRPi hNN, left, an, importanti hNN, left, an, JJi hNN, left, DT, importanti hNN, left, DT, JJi hNN, left, Det, Modi Table 1: Basic head-dependent and sibling"
W11-2827,N09-2041,1,0.850425,"edes the dependent or vice-versa, grouped by the broad part of speech (POS) of the head and the relation between the head and the dependent, with different combinations of words and POS tags. The features at the bottom record the order of sibling dependent words appearing on the same side of the head word, similarly grouped by the broad POS of the head and at different granularities of word or POS tag, and additionally with relation-relation orderings. Table 2 shows the results of reverse realization with OpenCCG on the development section of the 7 Features incorporating named entity classes (Rajkumar et al., 2009) and targeting agreement errors (Rajkumar and White, 2010) were not used in the experiments reported here. Feature Type HeadBroadPos + Rel + Precedes + HeadWord + DepWord . . . + HeadWord + DepPOS . . . + HeadPOS + DepWord . . . + HeadWord + DepPOS HeadBroadPos + Side + DepWord1 + DepWord2 . . . + DepWord1 + DepPOS2 . . . + DepPOS1 + DepWord2 . . . + DepPOS1 + DepPOS2 . . . + Rel1 + Rel2 Example hVB, Arg0, dep, wants, hei hVB, Arg0, dep, wants, PRPi hVB, Arg0, dep, VBZ, hei hVB, Arg0, dep, VBZ, PRPi hNN, left, an, importanti hNN, left, an, JJi hNN, left, DT, importanti hNN, left, DT, JJi hNN,"
W11-2827,P04-1007,0,0.0151895,"’s β setting is progressively relaxed until a complete realization is found or the space/time limits are exceeded. Alternative realizations are ranked using an averaged perceptron model (White and Rajkumar, 2009) that makes use of three kinds of features: (1) the log probability of the candidate realization’s word sequence according to a trigram word model and a factored language model over part-of-speech tags and supertags; (2) integervalued syntactic features, representing counts of occurrences in a derivation, from Clark & Curran’s normal form model; and (3) discriminative n-gram features (Roark et al., 2004), which count the occurrences of each n-gram in the word sequence. Section 4 of this paper also explores the use of a basic dependency model, with head-dependent and sibling dependent ordering features. 3 Glue Rules for Chart Realization As in Chiang’s (2007) approach to using glue rules in synchronous context-free grammars and the XLE approach to fragment rules in hand-crafted gramcontinue through four (traffic) lights sb 
p simp ss/np n/n ∅ n n frag G n fragc Input LF: @c(continue ˆ &lt;Actor>(p ˆ pro2) ˆ &lt;Path>(t1 ˆ through ˆ &lt;Ref>(l ˆ light ˆ &lt;num>pl ˆ &lt;Card>(f ˆ four) ˆ &lt;Mod>(t2 ˆ traffic)"
W11-2827,P98-2196,0,0.0575011,"Note that it is the recursive use of glue rules, along with the connection to dependency realization discussed next, that perhaps most distinguishes the present approach from the use of fragment rules in hand-crafted grammars with XLE. 4 Experiments with relaxed relation matching, which is similar to the use of relaxed unification constraints in grammarbased error detection (Schwind, 1988), have been inconclusive to date. In future work, it would be interesting to further explore the use of constraint relaxation and possibly other techniques from error detection, such as the use of mal-rules (Schneider and McCoy, 1998). As glue rules are applied, LF chunking constraints are applied as usual, and thus the fragment gluing phase becomes tantamount to exploring different permutations of heads and phrases headed by their dependents, much as in dependency-based realization approaches. That is, since fragment edges are constructed by assembling existing edges in either order, all permutations of edges whose EPs fall within an LF chunk will eventually be tried (subject so search constraints), with preference given to the orderings with the best model scores. Note that with glue rules, tracking of disjunctive altern"
W11-2827,C88-2127,0,0.325114,"lied recursively, fragments that complete an LF chunk or disjunction are marked as completed fragments (fragc ), so that they may be used with the glue rule as the right category (where fragments are normally disallowed). Note that it is the recursive use of glue rules, along with the connection to dependency realization discussed next, that perhaps most distinguishes the present approach from the use of fragment rules in hand-crafted grammars with XLE. 4 Experiments with relaxed relation matching, which is similar to the use of relaxed unification constraints in grammarbased error detection (Schwind, 1988), have been inconclusive to date. In future work, it would be interesting to further explore the use of constraint relaxation and possibly other techniques from error detection, such as the use of mal-rules (Schneider and McCoy, 1998). As glue rules are applied, LF chunking constraints are applied as usual, and thus the fragment gluing phase becomes tantamount to exploring different permutations of heads and phrases headed by their dependents, much as in dependency-based realization approaches. That is, since fragment edges are constructed by assembling existing edges in either order, all perm"
W11-2827,2005.mtsummit-papers.15,0,0.296114,"Missing"
W11-2827,D09-1043,1,0.864004,"pendency graph from the CCGbank for He has a point he wants to make [. . . ], along with gold-standard supertags (category labels) al., 2008); Figure 1 illustrates the desired output of the hypertagger. As in Clark & Curran’s (2007) approach to integrating supertagging and parsing, an adaptive strategy is employed, whereby a β-best list of supertags is returned for each lexical predication, and the hypertagger’s β setting is progressively relaxed until a complete realization is found or the space/time limits are exceeded. Alternative realizations are ranked using an averaged perceptron model (White and Rajkumar, 2009) that makes use of three kinds of features: (1) the log probability of the candidate realization’s word sequence according to a trigram word model and a factored language model over part-of-speech tags and supertags; (2) integervalued syntactic features, representing counts of occurrences in a derivation, from Clark & Curran’s normal form model; and (3) discriminative n-gram features (Roark et al., 2004), which count the occurrences of each n-gram in the word sequence. Section 4 of this paper also explores the use of a basic dependency model, with head-dependent and sibling dependent ordering"
W11-2827,W06-1403,1,\N,Missing
W11-2827,C98-2191,0,\N,Missing
W11-2832,W05-0909,0,0.0208607,"selected (and removed) from PTB Section 24. Note that a small number of sentences from the selected WSJ sections were not included in the CoNLL-08 data (and are thus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt"
W11-2832,P11-2040,1,0.695361,"ccessful in choosing a single-best output that is more similar to the reference sentence than the others in the top 5. In the absence of multiple reference sentences or human evaluation results for the n-best list though, it is unclear to what extent the outputs in the n-best list might represent valid paraphrases versus clearly less acceptable outputs. 4 Human Evaluations 4.1 Experimental Set-up We assessed three criteria in the human evaluations: Clarity, Readability and Meaning Similarity. We used continuous sliders as rating tools (see Figures 1 and 2), because raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). The instructions relating to Clarity and Readability read as follows:8 The first criterion you need to assess is Clarity. How clear (easy to understand) is the highlighted sentence within the context of the text extract? The second criterion to assess is Readability. This is sometimes called ’fluency’, and your task is to decide how well the highlighted sentence reads; is it good fluent English, or does it have grammatical errors, awkward constructions, etc. Note that you should assess Clarity separately from Readability: it is pos"
W11-2832,W10-4237,1,0.90252,"Missing"
W11-2832,C10-1012,0,0.117338,"ncies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is represented as a node in a syntactic dependency tree. Nodes: The node information consists of a word’s lemma, a coarse-grained POS-tag, and, where appropriate, number, tense and participle features and a sense tag id (as a suffix to the lemma). In addition, two punctuation features encode the quotation and bracketing information for the sentence. The POS-tag set is slightly less fine-grained than the Pen"
W11-2832,P06-1130,0,0.108586,"Missing"
W11-2832,W11-2107,0,0.00555759,"om PTB Section 24. Note that a small number of sentences from the selected WSJ sections were not included in the CoNLL-08 data (and are thus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http"
W11-2832,W07-2416,0,0.0146291,"h are ordered). The shallow input representation is intended to be a more ‘surfacey’, syntactic represention of the sentence. The deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntact"
W11-2832,W02-2103,0,0.249839,"aning Similarity. This report presents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 1 Introduction and Overview Many different surface realisers have been developed over the past three decades or so. While symbolic realisers dominated for much of this period, the past decade has seen the development of many different types of statistical surface realisers. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has produced results for regenerating the Penn Treebank (PTB) (Marcus et al., 1995). The basic approach in all this work was to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then 217 Dominic Espinosa2 Eric Kow1 Department of Linguistics Ohio State University Columbus, OH, 43210, US {espinosa,mwhite}@ling.osu.edu 2 Amanda Stent AT&T Labs Research Florham Park, NJ 07932, US stent@research.att.com c"
W11-2832,W04-2705,0,0.123598,"ic represention of the sentence. The deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is re"
W11-2832,W05-1510,0,0.0626047,"sents the evaluation results, along with descriptions of the SR Task Tracks and evaluation methods. For descriptions of the participating systems, see the separate system reports in this volume, immediately following this results report. 1 Introduction and Overview Many different surface realisers have been developed over the past three decades or so. While symbolic realisers dominated for much of this period, the past decade has seen the development of many different types of statistical surface realisers. A significant subset of statistical realisation work (Langkilde, 2002; Callaway, 2003; Nakanishi et al., 2005; Zhong and Stent, 2005; Cahill and van Genabith, 2006; White and Rajkumar, 2009) has produced results for regenerating the Penn Treebank (PTB) (Marcus et al., 1995). The basic approach in all this work was to remove information from the Penn Treebank parses (the word strings themselves as well as some of the parse information), and then 217 Dominic Espinosa2 Eric Kow1 Department of Linguistics Ohio State University Columbus, OH, 43210, US {espinosa,mwhite}@ling.osu.edu 2 Amanda Stent AT&T Labs Research Florham Park, NJ 07932, US stent@research.att.com convert and use these underspecified repr"
W11-2832,J05-1004,0,0.0968466,"e deep(er) input type is intended to be closer to a semantic, more abstract, representation of the meaning of the sentence. The input representations were created by postprocessing the CoNLL 2008 Shared Task data (Surdeanu et al., 2008). For the preparation of the CoNLL-08 Shared task data, selected sections of the Penn WSJ Treebank were converted to syntactic dependencies via the LTH Constituent-to-Dependency Conversion Tool for Penn-style Treebanks (Pennconverter) (Johansson and Nugues, 2007). The resulting dependency bank was then merged with the Nombank (Meyers et al., 2004) and Propbank (Palmer et al., 2005) corpora. Named entity information from the BBN Entity Type corpus was also integrated into the CoNLL-08 data. Our shallow representation is based on the Pennconverter dependencies. The deep representation is derived from the merged Nombank, Propbank and syntactic dependencies in a process similar to the graph completion 218 algorithm outlined in (Bohnet et al., 2010) (see Section 2.2 for differences). 2.1 Shallow representation The shallow data consists of unordered syntactic dependency trees. Each word and punctuation marker from the original sentence is represented as a node in a syntactic"
W11-2832,P02-1040,0,0.0951709,"sal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ 6 http://www.cs.cmu.edu/ alavie/METEOR/ 3 7 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ http://www.umiacs.umd.edu/ snover/terp/ 3 1. BLEU (Papineni et al., 2002): geometric mean of 1- to 4-gram precision with a brevity penalty; recent 220 N-best, ranked system outputs: Ranked 5-best outputs were scored using a weighted average of the sentence-level scores for each metric, with these sentence-level weighted sums averaged across all outputs. The weight wi assigned to the ith system output was in inverse proportion to its rank ri (K = 5): wi = PKK−ri +1 j=1 K−rj +1 Missing outputs: Missing outputs were scored as zero (one for TER); in the n-best evaluation, missing or duplicate outputs were scored as 0 (1 for TER). Since coverage was high for all systems"
W11-2832,2006.amta-papers.25,0,0.0358882,"hus not included in the SR Task data) due to difficulties in merging the various data sets (e.g. Section 23 has 17 fewer sentences). 3 Automatic Evaluations We computed scores using the following wellknown automatic evaluation metrics: implementations use smoothing to allow sentencelevel scores to be computed. 2. NIST:4,5 n-gram similarity weighted in favour of less frequent n-grams which are taken to be more informative. 3. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2011):6 lexical similarity based on exact, stem, synonym, and paraphrase matches between words and phrases. 4. TER (Snover et al., 2006):7 a length-normalized edit distance metric where phrasal shifts are counted as one edit. For each metric, we calculated system-level scores, the mean of the sentence-level scores and weighted n-best scores (described below). Text normalisation: Output texts were normalised by lower-casing all tokens, removing any extraneous white space characters and ensuring consistent treatment of ampersands. 4 http://www.itl.nist.gov/iad/mig/tests/mt/doc /ngram-study.pdf 5 http://www.itl.nist.gov/iad/mig/tests/mt/2009/ 6 http://www.cs.cmu.edu/ alavie/METEOR/ 3 7 http://www.itl.nist.gov/iad/mig/tests/mt/200"
W11-2832,W08-2121,0,0.0614246,"Missing"
W11-2832,D09-1043,1,\N,Missing
W11-2832,H94-1020,0,\N,Missing
W11-2836,P02-1041,0,0.0163662,"s a parsing/generation library for Combinatory Categorial Grammar (Steedman, 2000). CCG is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode sub-categorization as well as syntactic features. OpenCCG implements a grammarbased chart realization algorithm in the tradition of Kay’s (1996) approach to bidirectional processing with unification grammars. The chart realizer takes 236 as input logical forms represented internally using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguistic meaning (Baldridge and Kruijff, 2002). To illustrate the input to OpenCCG, consider the semantic dependency graph in Figure 1. In the graph, each node has a lexical predication (e.g. make.03) and a set of semantic features (e.g. hNUMisg); nodes are connected via dependency relations (e.g. hA RG 0i). Such graphs are broadly similar to the “deep” shared task inputs. Note, however, that they are quite different from the shallow input trees, where many of the expected dependencies from coordination, control and relatization are missing. For example, in the figure, both dependents of make.03 would be missing in the shallow tree, which"
W11-2836,P08-1022,1,0.829282,"ce several robustness measures into OpenCCG’s grammar-based chart realizer, the percentage of grammatically complete realizations still remained well below results using native OpenCCG inputs on the development set, with a corresponding drop in output quality. We discuss known conversion issues and possible ways to improve performance on shared task inputs. 1 Introduction Our Generation Challenges 2011 shared task system represents an initial attempt to develop a surface realizer for shared task inputs that takes advantage of prior work on broad coverage realization with OpenCCG (White, 2006; Espinosa et al., 2008; Rajkumar et al., 2009; White and Rajkumar, 2009; Rajkumar and White, 2010). OpenCCG is a parsing/generation library for Combinatory Categorial Grammar (Steedman, 2000). CCG is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode sub-categorization as well as syntactic features. OpenCCG implements a grammarbased chart realization algorithm in the tradition of Kay’s (1996) approach to bidirectional processing with unification grammars. The chart realizer takes 236 as input logical forms represented internally using Hybrid Logic Depend"
W11-2836,J07-3004,0,0.018502,"3 would be missing in the shallow tree, which involve control and relativization (with a null relativizer). As it would be difficult to hallucinate such dependencies, we have only attempted the deep task. Grammar-based chart realization in the tradition of Kay is capable of attaining high precision, but achieving broad coverage is a challenge, as is robustness to any deviations in the expected input. Previous work on chart realization has primarily used inputs derived from gold standard parses, and indeed, native OpenCCG inputs have been obtained from gold standard derivations in the CCGbank (Hockenmaier and Steedman, 2007). Given the available time, our strategy was to make minor adjustments to OpenCCG’s extracted grammars while devoting the bulk of our effort to converting the shared task inputs to be as similar as possible to the native inputs. Difficulties in conversion led us to employ machine learning for relation mapping and to introduce Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), pages 236–238, c Nancy, France, September 2011. 2011 Association for Computational Linguistics Construction Collapsed NEs Collapsed hyphenations Conjunctions Possessives Relative clauses Punc"
W11-2836,P96-1027,0,0.203094,"Missing"
W11-2836,C10-2119,1,0.815981,"er, the percentage of grammatically complete realizations still remained well below results using native OpenCCG inputs on the development set, with a corresponding drop in output quality. We discuss known conversion issues and possible ways to improve performance on shared task inputs. 1 Introduction Our Generation Challenges 2011 shared task system represents an initial attempt to develop a surface realizer for shared task inputs that takes advantage of prior work on broad coverage realization with OpenCCG (White, 2006; Espinosa et al., 2008; Rajkumar et al., 2009; White and Rajkumar, 2009; Rajkumar and White, 2010). OpenCCG is a parsing/generation library for Combinatory Categorial Grammar (Steedman, 2000). CCG is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode sub-categorization as well as syntactic features. OpenCCG implements a grammarbased chart realization algorithm in the tradition of Kay’s (1996) approach to bidirectional processing with unification grammars. The chart realizer takes 236 as input logical forms represented internally using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approach to representing linguisti"
W11-2836,N09-2041,1,0.843001,"easures into OpenCCG’s grammar-based chart realizer, the percentage of grammatically complete realizations still remained well below results using native OpenCCG inputs on the development set, with a corresponding drop in output quality. We discuss known conversion issues and possible ways to improve performance on shared task inputs. 1 Introduction Our Generation Challenges 2011 shared task system represents an initial attempt to develop a surface realizer for shared task inputs that takes advantage of prior work on broad coverage realization with OpenCCG (White, 2006; Espinosa et al., 2008; Rajkumar et al., 2009; White and Rajkumar, 2009; Rajkumar and White, 2010). OpenCCG is a parsing/generation library for Combinatory Categorial Grammar (Steedman, 2000). CCG is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode sub-categorization as well as syntactic features. OpenCCG implements a grammarbased chart realization algorithm in the tradition of Kay’s (1996) approach to bidirectional processing with unification grammars. The chart realizer takes 236 as input logical forms represented internally using Hybrid Logic Dependency Semantics (HLDS),"
W11-2836,D09-1043,1,0.853817,"grammar-based chart realizer, the percentage of grammatically complete realizations still remained well below results using native OpenCCG inputs on the development set, with a corresponding drop in output quality. We discuss known conversion issues and possible ways to improve performance on shared task inputs. 1 Introduction Our Generation Challenges 2011 shared task system represents an initial attempt to develop a surface realizer for shared task inputs that takes advantage of prior work on broad coverage realization with OpenCCG (White, 2006; Espinosa et al., 2008; Rajkumar et al., 2009; White and Rajkumar, 2009; Rajkumar and White, 2010). OpenCCG is a parsing/generation library for Combinatory Categorial Grammar (Steedman, 2000). CCG is a unification-based categorial grammar formalism defined almost entirely in terms of lexical entries that encode sub-categorization as well as syntactic features. OpenCCG implements a grammarbased chart realization algorithm in the tradition of Kay’s (1996) approach to bidirectional processing with unification grammars. The chart realizer takes 236 as input logical forms represented internally using Hybrid Logic Dependency Semantics (HLDS), a dependency-based approac"
W11-2836,W11-2827,1,0.800009,"Missing"
W12-1525,W04-2705,0,\N,Missing
W12-1525,C10-1012,1,\N,Missing
W12-1525,W08-2121,0,\N,Missing
W12-1525,W11-2832,1,\N,Missing
W12-1525,W07-2416,0,\N,Missing
W12-1525,J05-1004,0,\N,Missing
W12-1525,W12-1528,1,\N,Missing
W12-1525,W04-3250,0,\N,Missing
W12-1525,kow-belz-2012-lg,1,\N,Missing
W12-1525,W12-1527,1,\N,Missing
W12-1528,D10-1055,1,0.857985,"ect developers of automatic quality metrics in the MT community to be interested in the proposed task, which is anticipated to be both more focused (with lexical choice largely excluded) and more challenging than in the MT case, given the generally high level of quality in realization results: as realization quality increases, the metrics’ task becomes more difficult, since the paraphrases of a given sentence often involve subtle differences between acceptable and unacceptable variation. In an earlier study of the utility of automatic metrics with Penn Treebank (PTB) surface realization data (Espinosa et al., 2010), we observed moderate correlations between the most popular metrics and human judgments, though lower than the levels seen with MT data. promote reuse of human judgments The task is intended to test the effectiveness of realization ranking models in a way that reuses human judgments, making it possible to carry out re150 INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 150–153, c Utica, May 2012. 2012 Association for Computational Linguistics Track Realization Ranking Hybrid Metrics Meta-Eval Reference Sentence N Y Y PTB Gold Y Y N PTB Auto N N Y Ta"
W12-1528,2006.amta-papers.25,0,0.0339254,"ranking model. To do so, each system is allowed to use its own “native” inputs derived from the Penn Treebank and PTB-based resources. To the extent that a system’s statistical ranking model can be used to assign a score to any possible realization, the ranking task can be accomplished by simply ranking the realizations by model score. As such, following this strategy, the task is one of analysis by synthesis. 151 For non-statistical realizers, or ones that cannot assign a score to any possible realization, there is an alternative strategy available, namely to automatically approximate HTER. Snover et al. (2006) demonstrate that the human-targeted translation edit rate (HTER) represents a reliable and easily interpretable method of evaluating MT output. With this method, a human annotator produces a targeted reference sentence which is as close as possible to the MT hypothesis while being fully acceptable; from the targeted reference, the TER score then represents a normalized post-edit score, which has been shown to correlate with human ratings at least as well as more complex competing metrics. As Madnani (2010) points out, generated paraphrases of the reference sentence can be used to approximate"
W12-1528,D09-1043,1,0.841908,"ended meaning, while also arguably representing the most sensible way to automatically evaluate outputs in a data-to-text setting, where intended meanings can be reliably represented. 3 Pilot Experiments In this section, we present two pilot experiments intended to demonstrate the feasibility of the task. The experiments use the human judgments collected in Espinosa et al.’s (2010) study, which consist of adequacy and fluency ratings from two judges for a variety of realizations for PTB Section 00. The realizations in the corpus were generated using several OpenCCG realization ranking models (White and Rajkumar, 2009) and using the XLE symbolic realizer with subsequent n-gram ranking (paraphrases involving WordNet substitutions were excluded). For comparison purposes, three well-known metrics (BLEU, METEOR and TER) were tested, along with three OpenCCG ranking models: (I) a generative baseline model, incorporating three n-gram models as well as Hockenmaier’s (2003) generative model; (II) a model additionally incorporating a slew of discriminative features, extending White & Rajkumar’s model with dependency ordering features; and (III) a model adding one additional feature for minimizing dependency length."
W13-2104,D09-1043,1,0.90494,"Missing"
W13-2104,W98-1426,0,0.10689,"for naturalness ratings of SRC texts gathered using Amazon’s Mechanical Turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. We conclude with a discussion of possible ways of improving the ranker in future work. 1 Introduction To lessen the need for handcrafting in developing generation systems, Walker et al. (2007) extended the overgenerate-and-rank methodology (Langkilde and Knight, 1998; Mellish et al., 1998; Walker et al., 2002; Nakatsu and White, 2006) to complex information presentation tasks involving variation in rhetorical structure. They illustrated their approach by developing SPaRKy (Sentence Planning with Rhetorical Knowledge), a sentence planner for generating restaurant recommendations and comparisons in the context of the MATCH (Multimodal Access To City Help) system (Walker et al., 2004), and showed that SPaRKY can produce texts comparable to those of MATCH’s templatebased generator. Despite the evident importance of expressing contrast clearly in making compar"
W13-2104,J10-2001,1,0.919289,"Missing"
W13-2104,W98-1411,0,0.192408,"Missing"
W13-2104,P06-1140,1,0.838674,"l Turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. We conclude with a discussion of possible ways of improving the ranker in future work. 1 Introduction To lessen the need for handcrafting in developing generation systems, Walker et al. (2007) extended the overgenerate-and-rank methodology (Langkilde and Knight, 1998; Mellish et al., 1998; Walker et al., 2002; Nakatsu and White, 2006) to complex information presentation tasks involving variation in rhetorical structure. They illustrated their approach by developing SPaRKy (Sentence Planning with Rhetorical Knowledge), a sentence planner for generating restaurant recommendations and comparisons in the context of the MATCH (Multimodal Access To City Help) system (Walker et al., 2004), and showed that SPaRKY can produce texts comparable to those of MATCH’s templatebased generator. Despite the evident importance of expressing contrast clearly in making comparisons among In this paper, we show that Nakatsu & White’s (2010) prop"
W13-2104,W08-0111,1,0.861446,"T. In Section 4, we describe how we trained discriminative n-gram rankers using cross validation on the gathered ratings. In Section 5, we present the oracle and cross validation results in terms of mean scores of the top-ranked text. In Section 6, we analyze how the individual contrast enhancements affected the naturalness ratings and discuss issues that may be still hampering naturalness. Finally, in Section 7, we conclude with a summary and a discussion of possible ways of creating improved rankers in future work. 2 Enhancing Contrast with Discourse Combinatory Categorial Grammar Figure 1 (Nakatsu, 2008) shows examples from the SRC where some of the SPaRKy realizations are clearly more natural than others. In Nakatsu’s experiments, she found that the use of contrastive connectives was negatively correlated with human ratings, and that an n-gram ranker learned to disprefer texts containing these connectives. In analyzing these unexpected results, Nakatsu noted two factors that appeared to hamper the naturalness of the contrastive connective usage. First, consistent with Grote et al.’s (1995) observation that however and on the other hand (unlike but and while) signal that the clause they attac"
W13-2104,J03-4002,0,0.0450119,"er larger text spans; (ii) adverbial modifiers only, just and merely to express a lesser value of a given property than one mentioned earlier;2 (iii) the modifers also and too to signal the repetition of the same value for a given property (Striegnitz, 2004); and (iv) contrastive connectives for different properties of the same restaurant, exemplified here by the contrast between decent decor and mediocre food quality for Bienvenue. In the text plan in Figure 2, <1>–<4> correspond to the propositions in the original SRC text plan and (1’)–(2’) are the new summary-level propositions. Following Webber et al. (2003), Nakatsu and White (2010) take only, merely, just, also, and too to be discourse adverbials, whose discourse relations are allowed to cut across the primary tree structure established by the other relations in the figure. Note that in addition to going beyond RST’s limitation to tree-structured discourses, the example also contains clauses employing multiple discourse connectives, where one is a structural connective (such as however or while) and the other is a discourse adverbial. To realize such texts, Nakatsu & White introduce Discourse Combinatory Categorial Grammar (DCCG), an extension"
W14-4424,D12-1023,1,\N,Missing
W14-4424,P02-1043,0,\N,Missing
W14-4424,W11-2832,1,\N,Missing
W14-4424,P08-1022,1,\N,Missing
W14-4424,Q13-1005,0,\N,Missing
W14-4424,W04-3250,0,\N,Missing
W14-4424,P13-2107,0,\N,Missing
W14-4424,C12-1123,0,\N,Missing
W15-0611,Q13-1005,0,0.0143705,"l patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical students were asked to complete a focused history of pre"
W15-0611,P05-1074,0,0.115102,"Missing"
W15-0611,P14-1133,0,0.0129923,"The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical students were asked to complete a focused history of present illness of a patient"
W15-0611,P09-1053,0,0.0458946,"Missing"
W15-0611,D08-1069,0,0.0752837,"Missing"
W15-0611,W11-2107,0,0.0343506,"Missing"
W15-0611,C04-1051,0,0.0143944,"Missing"
W15-0611,E12-1048,0,0.0229144,"nce, allowing them to “suspend disbelief” and behave as if the virtual patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of t"
W15-0611,W13-1738,0,0.0236171,"uspend disbelief” and behave as if the virtual patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical student"
W15-0611,D13-1090,0,0.0291762,"Missing"
W15-0611,N12-1019,0,0.0727964,"Missing"
W15-0611,P02-1040,0,0.0915643,"Missing"
W15-0611,W03-1209,0,0.289668,"Missing"
W15-0611,U06-1019,0,0.0515201,"Missing"
W15-0611,Q14-1034,0,0.0577806,"Missing"
W15-4704,D10-1049,0,0.106038,"ditional architectures. 1 Introduction In a traditional natural language generation (NLG) system (Reiter and Dale, 2000), a pipeline of hand-crafted components is used to generate high quality text, albeit at considerable knowledgeengineering expense. While there has been progress on using machine learning to ameliorate this issue in content planning (Duboue and McKeown, 2001; Barzilay and Lapata, 2005) and broad coverage surface realization (Reiter, 2010; Rajkumar and White, 2014), the central stage of sentence planning (or microplanning) has proved more difficult to automate. More recently, Angeli et al. (2010) and Konstas and Lapata (2013), inter alia, have developed end-to-end learning methods for NLG systems; however, as discussed further in the next section, these systems assume quite limited discourse structures in comparison to those with more traditional architectures. 2 Related Work Angeli et al. (2010) present an end-to-end trainable NLG system that generates by selecting a 1 The sentence plan ranker uses machine learning to rank sentence plans based on features derived from the sentence plan and its realization, together with accompanying human ratings for the realizations in the training"
W15-4704,H05-1042,0,0.0141035,"nning rules for both aggregation and discourse connective insertion, an important step towards ameliorating the knowledge acquisition bottleneck for NLG systems that produce texts with rich discourse structures using traditional architectures. 1 Introduction In a traditional natural language generation (NLG) system (Reiter and Dale, 2000), a pipeline of hand-crafted components is used to generate high quality text, albeit at considerable knowledgeengineering expense. While there has been progress on using machine learning to ameliorate this issue in content planning (Duboue and McKeown, 2001; Barzilay and Lapata, 2005) and broad coverage surface realization (Reiter, 2010; Rajkumar and White, 2014), the central stage of sentence planning (or microplanning) has proved more difficult to automate. More recently, Angeli et al. (2010) and Konstas and Lapata (2013), inter alia, have developed end-to-end learning methods for NLG systems; however, as discussed further in the next section, these systems assume quite limited discourse structures in comparison to those with more traditional architectures. 2 Related Work Angeli et al. (2010) present an end-to-end trainable NLG system that generates by selecting a 1 The"
W15-4704,W02-1001,0,0.0372484,"t ruleset, rules are induced from an n-best list of partially completed outputs paired with the target LF. Figure 5. Another common cause of bad rules is errors in parsing the target sentences, especially longer ones. To lessen the prevalence of bad induced rules, we take inspiration from work on learning semantic parsers (Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013) and embed the process of inducing clause-combining rules within a process of learning a model of preferred derivations that use the induced rules. The model is learned using the structured averaged perceptron algorithm (Collins, 2002), with indicator features for each rule used in deriving an output LF from and input LF. With each input–output pair, the current model is used to generate the highest-scoring output LF using a bottom-up beam search. If the highest-scoring output LF is not equal to the target LF, then the search is run again to find the highest-scoring derivation of the target LF, using the distance to the target to help guide the search. When the target LF can be successfully derived, a perceptron update is performed, adjusting the weights of the current model by adding the features from the target LF derivat"
W15-4704,P13-1123,0,0.0386141,"Missing"
W15-4704,P01-1023,0,0.439916,"possible to learn microplanning rules for both aggregation and discourse connective insertion, an important step towards ameliorating the knowledge acquisition bottleneck for NLG systems that produce texts with rich discourse structures using traditional architectures. 1 Introduction In a traditional natural language generation (NLG) system (Reiter and Dale, 2000), a pipeline of hand-crafted components is used to generate high quality text, albeit at considerable knowledgeengineering expense. While there has been progress on using machine learning to ameliorate this issue in content planning (Duboue and McKeown, 2001; Barzilay and Lapata, 2005) and broad coverage surface realization (Reiter, 2010; Rajkumar and White, 2014), the central stage of sentence planning (or microplanning) has proved more difficult to automate. More recently, Angeli et al. (2010) and Konstas and Lapata (2013), inter alia, have developed end-to-end learning methods for NLG systems; however, as discussed further in the next section, these systems assume quite limited discourse structures in comparison to those with more traditional architectures. 2 Related Work Angeli et al. (2010) present an end-to-end trainable NLG system that gen"
W15-4704,P08-1022,1,0.810838,"CUE - WORD - CONJUNCTION combines clauses using the conjunctions and, but, and while, while CUE - WORD - INSERTION combines clauses by inserting however or on the other hand into the second clause. Table 2 also shows two operations, VP- COORDINATION and NP- APPOSITION, which go beyond those in SPaRKy; these are discussed further in Section 5.5. Finally, it’s also possible to leave sentences as they are, simply juxtaposing them in sequence. For the experiments reported in this paper, we have reimplemented SPaRKy to work with OpenCCG’s broad coverage English grammar for parsing and realization (Espinosa et al., 2008; White and Rajkumar, 2009; White and Rajkumar, Other notable recent approaches (Lu et al., 2009; Dethlefs et al., 2013; Mairesse and Young, 2014) are similar in that they learn to map semantic representations to texts using conditional random fields or factored language models with no explicit model of syntactic structure, but the content to be expressed is assumed to be preaggregated in the input. Kondadadi et al. (2013) develop a rather different approach where largescale templates are learned that can encapsulate typical aggregation patterns, but the templates cannot be dynamically combine"
W15-4704,W09-3941,0,0.0222388,"f possible sentence plans, from which one is selected for output by a sentence plan ranker.1 To demonstrate the viability of our method, we present an experiment demonstrating that rules corresponding to all of the hand-crafted operators for aggregation and discourse connective insertion used in the SPaRKy Restaurant Corpus can be effectively learned from examples of their use. To our knowledge, these induced rules for the first time incorporate the constraints necessary to be functionally equivalent to the hand-crafted clause-combining operators; in particular, our method goes beyond the one Stent and Molina (2009) develop for learning clause-combining rules, which focuses on learning domain-independent rules for discourse connective insertion, ignoring aggregation rules and any potentially domain-dependent aspects of the rules. As such, our approach promises to be of immediate benefit to NLG system developers, while also taking an important step towards reducing the knowledge acquisition bottleneck for developing NLG systems requiring rich discourse structures in their outputs. We describe an algorithm for inducing clause-combining rules for use in a traditional natural language generation architecture"
W15-4704,W13-2104,1,0.853284,"ed the SPaRKy Restaurant Corpus (SRC), a collection of content plans, text plans and the surface realizations of those plans evaluated by users.2 While the restaurant recommendation domain is fairly narrow in terms of the kinds of propositions represented, it requires careful application of aggregation operations to make concise, natural realizations. This is evident both in the care taken in incorporating clause-combining rules into SPaRKy and in subsequent work on the expression of contrast which used this domain to motivate extensions of CCG to the discourse level (Nakatsu and White, 2010; Howcroft et al., 2013). Five kinds of clause-combining operations are included in SPaRKy, most of which involve lexically specific constraints. These are illustrated in Table 2, using propositions corresponding to sentences (1)–(4) from Table 1 as input (combined with either a CONTRAST or INFER relation). MERGE combines two clauses if they have the same verb with the same arguments and adjuncts except for one. WITH - REDUCTION replaces an instance of have plus an object X with the phrase with X. REL - CLAUSE subordinates one clause to another when they have a common subject. CUE - WORD - CONJUNCTION combines clause"
W15-4704,P13-1138,0,0.051963,"g them in sequence. For the experiments reported in this paper, we have reimplemented SPaRKy to work with OpenCCG’s broad coverage English grammar for parsing and realization (Espinosa et al., 2008; White and Rajkumar, 2009; White and Rajkumar, Other notable recent approaches (Lu et al., 2009; Dethlefs et al., 2013; Mairesse and Young, 2014) are similar in that they learn to map semantic representations to texts using conditional random fields or factored language models with no explicit model of syntactic structure, but the content to be expressed is assumed to be preaggregated in the input. Kondadadi et al. (2013) develop a rather different approach where largescale templates are learned that can encapsulate typical aggregation patterns, but the templates cannot be dynamically combined in a way that is sensitive to discourse structure Previous work on aggregation in NLG, e.g. with SPaRKy itself or earlier work by Pan and Shaw (2004), focuses on learning when to apply aggregation rules, which are themselves hand-crafted rather than learned. The clause-combining rules our system learns—based on lexico-semantic dependency edits—are closely related to the lexicosyntactic rewrite rules learned by Angrosh an"
W15-4704,D09-1043,1,0.871654,"N combines clauses using the conjunctions and, but, and while, while CUE - WORD - INSERTION combines clauses by inserting however or on the other hand into the second clause. Table 2 also shows two operations, VP- COORDINATION and NP- APPOSITION, which go beyond those in SPaRKy; these are discussed further in Section 5.5. Finally, it’s also possible to leave sentences as they are, simply juxtaposing them in sequence. For the experiments reported in this paper, we have reimplemented SPaRKy to work with OpenCCG’s broad coverage English grammar for parsing and realization (Espinosa et al., 2008; White and Rajkumar, 2009; White and Rajkumar, Other notable recent approaches (Lu et al., 2009; Dethlefs et al., 2013; Mairesse and Young, 2014) are similar in that they learn to map semantic representations to texts using conditional random fields or factored language models with no explicit model of syntactic structure, but the content to be expressed is assumed to be preaggregated in the input. Kondadadi et al. (2013) develop a rather different approach where largescale templates are learned that can encapsulate typical aggregation patterns, but the templates cannot be dynamically combined in a way that is sensiti"
W15-4704,D12-1023,1,0.872955,"Missing"
W15-4704,D10-1119,0,0.0332136,"ure 5: An undesirable rule that mistakenly reduces the best overall quality among the selected restaurants to among the selected restaurants, induced from LF subgraphs for these phrases 5. Partial Epoch For any examples where the target LF cannot be generated with the current ruleset, rules are induced from an n-best list of partially completed outputs paired with the target LF. Figure 5. Another common cause of bad rules is errors in parsing the target sentences, especially longer ones. To lessen the prevalence of bad induced rules, we take inspiration from work on learning semantic parsers (Kwiatkowksi et al., 2010; Artzi and Zettlemoyer, 2013) and embed the process of inducing clause-combining rules within a process of learning a model of preferred derivations that use the induced rules. The model is learned using the structured averaged perceptron algorithm (Collins, 2002), with indicator features for each rule used in deriving an output LF from and input LF. With each input–output pair, the current model is used to generate the highest-scoring output LF using a bottom-up beam search. If the highest-scoring output LF is not equal to the target LF, then the search is run again to find the highest-scori"
W15-4704,J10-2001,1,0.835887,"proach, they acknowledge that handling discourse-level document structure remains for future work. Given this limitation, under their approach there is no need to explicitly perform aggregation: instead, it suffices to “pre-aggregate” propositions about the same entity onto the same record. However, in the general case aggregation should be subject to discourse structure; for example, when contrasting the positive and negative attributes of an entity according to a given user model, it makes sense to aggregate the positive and negative attributes separately, rather than lumping them together (White et al., 2010). Consequently, we aim to learn aggregation rules that are sensitive to discourse structure, as with the SPaRKy architecture. SPaRKy Restaurant Corpus Walker et al. (2007) developed SPaRKy (a Sentence Planner with Rhetorical Knowledge) to extend the MATCH system (Walker et al., 2004) for restaurant recommendations. In the course of their study they produced the SPaRKy Restaurant Corpus (SRC), a collection of content plans, text plans and the surface realizations of those plans evaluated by users.2 While the restaurant recommendation domain is fairly narrow in terms of the kinds of propositions"
W15-4704,D09-1042,0,0.0138297,"- INSERTION combines clauses by inserting however or on the other hand into the second clause. Table 2 also shows two operations, VP- COORDINATION and NP- APPOSITION, which go beyond those in SPaRKy; these are discussed further in Section 5.5. Finally, it’s also possible to leave sentences as they are, simply juxtaposing them in sequence. For the experiments reported in this paper, we have reimplemented SPaRKy to work with OpenCCG’s broad coverage English grammar for parsing and realization (Espinosa et al., 2008; White and Rajkumar, 2009; White and Rajkumar, Other notable recent approaches (Lu et al., 2009; Dethlefs et al., 2013; Mairesse and Young, 2014) are similar in that they learn to map semantic representations to texts using conditional random fields or factored language models with no explicit model of syntactic structure, but the content to be expressed is assumed to be preaggregated in the input. Kondadadi et al. (2013) develop a rather different approach where largescale templates are learned that can encapsulate typical aggregation patterns, but the templates cannot be dynamically combined in a way that is sensitive to discourse structure Previous work on aggregation in NLG, e.g. wi"
W15-4704,D11-1038,0,0.0221143,"ourse structure Previous work on aggregation in NLG, e.g. with SPaRKy itself or earlier work by Pan and Shaw (2004), focuses on learning when to apply aggregation rules, which are themselves hand-crafted rather than learned. The clause-combining rules our system learns—based on lexico-semantic dependency edits—are closely related to the lexicosyntactic rewrite rules learned by Angrosh and Siddharthan’s (2014) system for text simplification. However, our learned rules go beyond theirs in imposing (non-)equivalence constraints crucial for accurate aggregation. Finally, work on text compression (Woodsend and Lapata, 2011; Cohn and Lapata, 2013) is also related, but focuses on simple constituent deletion, and to our knowledge does not implement aggregation constraints such as those here. 2 Available from http://users.soe.ucsc.edu/ ˜maw/downloads.html under the textplans/utterances. To our knowledge, the SRC remains the only publicly available corpus of input–output pairs for an NLG system using discourse structures with rhetorical relations. 29 Operator MERGE WITH - REDUCTION REL - CLAUSE CUE - WORD - CONJUNCTION CUE - WORD - INSERTION VP- COORDINATION NP- APPOSITION Sents 1, 2 1, 2 1, 2 1, 3 1, 3 3, 4 3, 4 Re"
W15-4704,J14-4003,0,0.0121482,"ng however or on the other hand into the second clause. Table 2 also shows two operations, VP- COORDINATION and NP- APPOSITION, which go beyond those in SPaRKy; these are discussed further in Section 5.5. Finally, it’s also possible to leave sentences as they are, simply juxtaposing them in sequence. For the experiments reported in this paper, we have reimplemented SPaRKy to work with OpenCCG’s broad coverage English grammar for parsing and realization (Espinosa et al., 2008; White and Rajkumar, 2009; White and Rajkumar, Other notable recent approaches (Lu et al., 2009; Dethlefs et al., 2013; Mairesse and Young, 2014) are similar in that they learn to map semantic representations to texts using conditional random fields or factored language models with no explicit model of syntactic structure, but the content to be expressed is assumed to be preaggregated in the input. Kondadadi et al. (2013) develop a rather different approach where largescale templates are learned that can encapsulate typical aggregation patterns, but the templates cannot be dynamically combined in a way that is sensitive to discourse structure Previous work on aggregation in NLG, e.g. with SPaRKy itself or earlier work by Pan and Shaw ("
W15-4704,Q13-1005,0,\N,Missing
W15-4704,W14-4404,0,\N,Missing
W16-1718,boxwell-white-2008-projecting,1,0.935668,"Missing"
W16-1718,J07-4004,0,0.133105,"Missing"
W16-1718,P14-1039,1,0.895954,"Missing"
W16-1718,P08-1022,1,0.866235,"Missing"
W16-1718,P10-1035,0,0.055691,"Missing"
W16-1718,W13-3711,0,0.0508635,"Missing"
W16-1718,P02-1043,0,0.152479,"Missing"
W16-1718,J07-3004,0,0.294656,"Missing"
W16-1718,W09-3306,0,0.0554518,"Missing"
W16-1718,W10-0702,0,0.314378,"Missing"
W16-1718,P08-2026,0,0.0823214,"Missing"
W16-1718,C92-2105,0,0.547533,"Missing"
W16-1718,J05-1004,0,0.201539,"Missing"
W16-1718,P06-1055,0,0.124646,"Missing"
W16-1718,C10-2119,1,0.857669,"Missing"
W16-1718,W11-2802,0,0.077329,"Missing"
W16-1718,D09-1043,1,0.884577,"Missing"
W16-1718,D12-1023,1,0.903181,"Missing"
W16-6638,Q13-1005,0,0.0301634,"culties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. White (2014) then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. 232 Proceedings of The 9th International Natural Language Generation conference, pages 232–236, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics A common thread in work on reversible, constraint-based grammars is emphasis on properly representing unbounded dependencies and coordination. For parsing, this emphasis has been shown to pay"
W16-6638,W11-2832,1,0.784685,"ies shows that the enhanced dependencies have greatly enhanced recall with moderate precision. We conclude with a discussion of the implications of the work for a second realization shared task. 1 Introduction Surface realization systems employing reversible, broad coverage constraint-based grammars together with statistical ranking models have achieved impressive results in multiple languages, using a variety of formalisms (HPSG, TAG, LFG, CCG). However, these systems all require somewhat different inputs, making comparative evaluation difficult. In the first surface realization shared task (Belz et al., 2011, henceforth SR-11), which aimed to ameliorate these difficulties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. White (2014) then showed that even better results can be"
W16-6638,W13-3721,0,0.0235867,"Missing"
W16-6638,W13-2105,0,0.128617,"uages, using a variety of formalisms (HPSG, TAG, LFG, CCG). However, these systems all require somewhat different inputs, making comparative evaluation difficult. In the first surface realization shared task (Belz et al., 2011, henceforth SR-11), which aimed to ameliorate these difficulties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. White (2014) then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. 232 Proceedings of The 9th International Natural Language Generation conference, page"
W16-6638,J07-3004,0,0.0365219,"dency Converter (Schuster and Manning, 2016, SDC). The SDC transforms automatic or gold PTB233 style trees into UDs; unfortunately, however, it was not designed to take traces into account, and thus the treatment of unbounded dependencies and coordination is only heuristic. To address this impasse, in this paper we report on progress towards creating SR-11–style realizer inputs that are both based on enhanced UDs and which accurately represent unbounded dependencies and coordination. To do so, we augment the UDs that result from running the SDC on the PTB with the dependencies in the CCGbank (Hockenmaier and Steedman, 2007), since the latter includes lexicalized dependencies derived from gold PTB traces. 2 Background Figures 1–2 show an example where the CCGbank preserves the information provided by the trace in a free relative clause along with a crucial structurally encoded dependency. In Figure 1 (left), the unbounded dependency between what and achieve is annotated via a trace in the PTB. Figure 1 (right) shows the SDC output for the sentence. While the SDC manages to capture the unbounded dependency in this case, what is not recognized as the head of the free relative clause and there is no direct dependenc"
W16-6638,D11-1140,0,0.036424,"to ameliorate these difficulties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. White (2014) then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. 232 Proceedings of The 9th International Natural Language Generation conference, pages 232–236, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics A common thread in work on reversible, constraint-based grammars is emphasis on properly representing unbounded dependencies and coordination. For parsing, this"
W16-6638,J93-2004,0,0.0548939,"s can likewise yield an empirical payoff, given the continuing lack of a common input representation that adequately treats unbounded dependencies and coordination, as these grammars require. With this issue in mind, White (2014) experimented with a version of the shallow SR-11 inputs (created by Richard Johansson) which included extra dependencies for unbounded dependencies and coordination, yielding dependency graphs extending core dependency trees. Unlike the rewrite rules employed by Gardent and Narayan (2013), the extra dependencies were derived from the gold traces in the Penn Treebank (Marcus et al., 1993, PTB), which is necessary to adequately handle right node raising and relativization. However, this version was still found to be incomplete, in particular because it was missing cases where the extra dependencies are encoded structurally in the PTB. Since then, Universal Dependencies (Nivre et al., 2016, UDs), which aim to represent syntactic dependencies similarly across languages, have become increasingly prominent. Building on the enhanced Stanford dependencies for English (de Marneffe et al., 2013)— which were designed to properly represent unbounded dependencies in dependency graphs— en"
W16-6638,C12-1123,0,0.0175424,"ad coverage constraint-based grammars together with statistical ranking models have achieved impressive results in multiple languages, using a variety of formalisms (HPSG, TAG, LFG, CCG). However, these systems all require somewhat different inputs, making comparative evaluation difficult. In the first surface realization shared task (Belz et al., 2011, henceforth SR-11), which aimed to ameliorate these difficulties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. White (2014) then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which"
W16-6638,C12-1130,0,0.0335404,"Missing"
W16-6638,L16-1262,0,0.0492523,"Missing"
W16-6638,S14-2008,0,0.0279607,"ained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. 232 Proceedings of The 9th International Natural Language Generation conference, pages 232–236, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics A common thread in work on reversible, constraint-based grammars is emphasis on properly representing unbounded dependencies and coordination. For parsing, this emphasis has been shown to pay off in improved recall of unbounded dependencies (Rimell et al., 2009; Nguyen et al., 2012; Oepen et al., 2014). For realization, however, it remains an open question as to whether approaches based on constraintbased grammars can likewise yield an empirical payoff, given the continuing lack of a common input representation that adequately treats unbounded dependencies and coordination, as these grammars require. With this issue in mind, White (2014) experimented with a version of the shallow SR-11 inputs (created by Richard Johansson) which included extra dependencies for unbounded dependencies and coordination, yielding dependency graphs extending core dependency trees. Unlike the rewrite rules employ"
W16-6638,D09-1085,0,0.0292538,"ion of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. 232 Proceedings of The 9th International Natural Language Generation conference, pages 232–236, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics A common thread in work on reversible, constraint-based grammars is emphasis on properly representing unbounded dependencies and coordination. For parsing, this emphasis has been shown to pay off in improved recall of unbounded dependencies (Rimell et al., 2009; Nguyen et al., 2012; Oepen et al., 2014). For realization, however, it remains an open question as to whether approaches based on constraintbased grammars can likewise yield an empirical payoff, given the continuing lack of a common input representation that adequately treats unbounded dependencies and coordination, as these grammars require. With this issue in mind, White (2014) experimented with a version of the shallow SR-11 inputs (created by Richard Johansson) which included extra dependencies for unbounded dependencies and coordination, yielding dependency graphs extending core depende"
W16-6638,L16-1376,0,0.0156617,"r, this version was still found to be incomplete, in particular because it was missing cases where the extra dependencies are encoded structurally in the PTB. Since then, Universal Dependencies (Nivre et al., 2016, UDs), which aim to represent syntactic dependencies similarly across languages, have become increasingly prominent. Building on the enhanced Stanford dependencies for English (de Marneffe et al., 2013)— which were designed to properly represent unbounded dependencies in dependency graphs— enhanced UDs for English have been partially implemented in the Stanford Dependency Converter (Schuster and Manning, 2016, SDC). The SDC transforms automatic or gold PTB233 style trees into UDs; unfortunately, however, it was not designed to take traces into account, and thus the treatment of unbounded dependencies and coordination is only heuristic. To address this impasse, in this paper we report on progress towards creating SR-11–style realizer inputs that are both based on enhanced UDs and which accurately represent unbounded dependencies and coordination. To do so, we augment the UDs that result from running the SDC on the PTB with the dependencies in the CCGbank (Hockenmaier and Steedman, 2007), since the"
W16-6638,W14-4424,1,0.852768,"first surface realization shared task (Belz et al., 2011, henceforth SR-11), which aimed to ameliorate these difficulties, attempts to use grammar-based realizers were unsuccessful, as converting shared task inputs to systemnative inputs turned out to be more difficult than anticipated. Subsequently, Narayan & Gardent (2012) demonstrated that grammarbased systems can be substantially improved with error mining techniques, and Gardent and Narayan (2013) showed that augmenting the (shallow) SR-11 representation of coordination to include shared dependencies can benefit grammar-based realizers. White (2014) then showed that even better results can be achieved by inducing a grammar (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013) that is directly compatible with (an enhanced version of) the SR-11 inputs. However, as explained below, subsequent analysis revealed substantial remaining issues with the data, which this paper takes a step towards addressing. 232 Proceedings of The 9th International Natural Language Generation conference, pages 232–236, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics A common thread in work on reversible, constraint-based gram"
W17-3702,W13-2322,0,0.0328223,"former, ‘top’ parse (correct according to the English Web Treebank) appears in (1-a), and the one for the latter, ‘next’ parse appears in (1-b), with underlining to highlight the differences between them. To parse sentences, we use the Berkeley parser (Petrov et al., 2006) trained on OpenCCG3 derivations (White, 2006; White et al., 2007; Boxwell and White, 2008) extracted from the CCGbank (Hockenmaier and Steedman, 2007). Derivations yield semantic dependency graphs represented using Hybrid Logic Dependency Semantics; the dependency graphs for (1) appear in Figure 1 using AMR-style notation (Banarescu et al., 2013). As shown in the figure, the :First and :Next relations can be used to identify coordinated phrases, and word identifiers allow spans to be extracted from subtrees.4 (b) Semantic dependency graph of ‘next’ parse Figure 1: Most likely parses for (1) (a) copying the words up to and including the first conjunct, followed by the words following the coordinated phrase; (b) copying any sentence-final punctuation, then starting a new sentence by copying the conjunction; and (c) again copying the words up to the first conjunct, then copying the second conjunct, again followed by the words following t"
W17-3702,boxwell-white-2008-projecting,1,0.790407,"s] :Arg0 (w0 / they) :Arg1 (w2 / have :Arg0 w0 :Arg1 (w8 / and :First (w5 / selection [num=sg] :Det (w3 / a) :Mod (w4 / good) :Mod (w6 / of :Arg1 (w7 / fabric [det=nil num=sg]))) :Next (w9 / notion [det=nil num=pl])))) b. The disambiguating paraphrase for the former, ‘top’ parse (correct according to the English Web Treebank) appears in (1-a), and the one for the latter, ‘next’ parse appears in (1-b), with underlining to highlight the differences between them. To parse sentences, we use the Berkeley parser (Petrov et al., 2006) trained on OpenCCG3 derivations (White, 2006; White et al., 2007; Boxwell and White, 2008) extracted from the CCGbank (Hockenmaier and Steedman, 2007). Derivations yield semantic dependency graphs represented using Hybrid Logic Dependency Semantics; the dependency graphs for (1) appear in Figure 1 using AMR-style notation (Banarescu et al., 2013). As shown in the figure, the :First and :Next relations can be used to identify coordinated phrases, and word identifiers allow spans to be extracted from subtrees.4 (b) Semantic dependency graph of ‘next’ parse Figure 1: Most likely parses for (1) (a) copying the words up to and including the first conjunct, followed by the words followin"
W17-3702,W16-1718,1,0.711419,"Missing"
W17-3702,J07-3004,0,0.0581616,"(w8 / and :First (w5 / selection [num=sg] :Det (w3 / a) :Mod (w4 / good) :Mod (w6 / of :Arg1 (w7 / fabric [det=nil num=sg]))) :Next (w9 / notion [det=nil num=pl])))) b. The disambiguating paraphrase for the former, ‘top’ parse (correct according to the English Web Treebank) appears in (1-a), and the one for the latter, ‘next’ parse appears in (1-b), with underlining to highlight the differences between them. To parse sentences, we use the Berkeley parser (Petrov et al., 2006) trained on OpenCCG3 derivations (White, 2006; White et al., 2007; Boxwell and White, 2008) extracted from the CCGbank (Hockenmaier and Steedman, 2007). Derivations yield semantic dependency graphs represented using Hybrid Logic Dependency Semantics; the dependency graphs for (1) appear in Figure 1 using AMR-style notation (Banarescu et al., 2013). As shown in the figure, the :First and :Next relations can be used to identify coordinated phrases, and word identifiers allow spans to be extracted from subtrees.4 (b) Semantic dependency graph of ‘next’ parse Figure 1: Most likely parses for (1) (a) copying the words up to and including the first conjunct, followed by the words following the coordinated phrase; (b) copying any sentence-final pun"
W17-3702,N13-1132,0,0.0276773,"Missing"
W17-3702,D12-1096,0,0.19683,"ntences, little work has been done to date on the task of generating questions to clarify structural ambiguities in a broad coverage setting. Recently, Duan et al. (2016) have shown that generating unambiguous paraphrases from competing parses of structurally ambiguous sentences can serve as a useful method for asking to clarify their intended meaning; in particular, they showed that their method enables crowd-sourced meaning judgments to be collected in order to improve parser accuracy in new domains. Duan et al.’s study covered most of the major sources of common parser errors identified by Kummerfeld et al. (2012), with the exception of ambiguities involving the correct spans of conjuncts in coordinated phrases (unless they involve modifier attachment ambiguities). Also closely related is He et al.’s (2016) work on generating questions to identify semantic roles, though their work does not address coordination span ambiguities either. In this paper, we present a novel method for generating disambiguating paraphrases for sentences with ambiguities involving two coordinated elements where the sentence is split in two, with one conjunct appearing in each half, so that the span of each conjunct becomes cle"
W17-3702,P06-1055,0,\N,Missing
W17-5002,P16-1036,0,0.0301705,"ct for image identification (Krizhevsky et al., 2012) and are becoming common for natural language processing. In general, CNNs are used for convolution over input language sequences, where the input is often a matrix representing a sequence word embeddings (Kim, 2014). Intuitively, word embedding kernels are convolving n-grams, ultimately generating features that represent n-grams over word vectors of length equal to the kernel width. CNNs are very popular in systems for tasks like paraphrase detection (Yin and Sch¨utze, 2015; Yin et al., 2016; He et al., 2015), community question answering (Das et al., 2016; Barbosa et al., 2016) and even machine translation (Gehring et al., 2017). Characterbased models that embed individual characters as input units are also possible, and have been used for language modeling (Kim et al., 2016) to good effect. It is worth noting that character sequences are more robust to spelling errors and potentially have the same expressive capability as word sequences given long enough character sequences. tem alone. Frequency quantile analysis shows that the hybrid system is able to leverage the relatively higher performance of ChatScript on the infrequent label items, whi"
W17-5002,I11-1150,0,0.075103,"Missing"
W17-5002,D15-1181,0,0.012728,"ked questions. CNNs have been used to great effect for image identification (Krizhevsky et al., 2012) and are becoming common for natural language processing. In general, CNNs are used for convolution over input language sequences, where the input is often a matrix representing a sequence word embeddings (Kim, 2014). Intuitively, word embedding kernels are convolving n-grams, ultimately generating features that represent n-grams over word vectors of length equal to the kernel width. CNNs are very popular in systems for tasks like paraphrase detection (Yin and Sch¨utze, 2015; Yin et al., 2016; He et al., 2015), community question answering (Das et al., 2016; Barbosa et al., 2016) and even machine translation (Gehring et al., 2017). Characterbased models that embed individual characters as input units are also possible, and have been used for language modeling (Kim et al., 2016) to good effect. It is worth noting that character sequences are more robust to spelling errors and potentially have the same expressive capability as word sequences given long enough character sequences. tem alone. Frequency quantile analysis shows that the hybrid system is able to leverage the relatively higher performance"
W17-5002,P17-1044,0,0.123453,"ature map. Because the kernel is as wide as the embeddings, it will only produce one 1 https://github.com/harvardnlp/ sent-conv-torch 13 and refer to it as max-renorm in this work. where αr is the coefficient of the r-th ensemble. The coefficients need to be trained. 5 6 Ensemble methods In order to reduce variance of performance when training on different splits of data, models trained with different training datasets and models with different architecture are combined together. Previous research has shown that ensembling models improves performance (Sakaguchi et al., 2015b; Ju et al., 2017; He et al., 2017). We train different models with different splits of training and develop data, and ensemble them together. We use two methods to combine the submodels together: majority voting and stacking. The individual CNNs, or the submodels, first are ensembled together according to their input features into two ensembled models, and then the two ensembles are stacked together to form the final stacked model. We now explain preprocessing steps, the hyperparameters we used for training, model initialization as well as the training process. 6.1 The majority voting strategy is adopted by the submodels to re"
W17-5002,W15-0611,1,0.639682,"inct tasks. Multi-class logistic regression is a standard approach that can take advantage of class-specific features but requires a good amount of training data for each class. A pairwise setup involves a more general binary classification decision which is then made for each label, choosing the highest confidence match. Early work (Ravichandran et al., 2003) found that treating a question answering task as a maximum entropy re-ranking problem outperformed using the same system as a classifier. DeVault et al. (2011) observed maximum entropy systems performed well with simple n-gram features. Jaffe et al. (2015) explored a log-linear pairwise ranking model for question identification and found it to outperform a multiclass baseline along the lines of DeVault et al. However, Jaffe et al. (2015) used a much smaller dataset with only about 915 user turns, less than one-fourth as many as in the current dataset. For this larger dataset, multiclass logistic regression outperforms a pairwise ranking model. With no pairwise comparisons, a multiclass classifier is also much faster, lending itself to real-time use. It is probable that multiclass vs. pairwise approaches’ overall effectiveness depends on the amo"
W17-5002,N15-1111,0,0.406871,"5 user turns, less than one-fourth as many as in the current dataset. For this larger dataset, multiclass logistic regression outperforms a pairwise ranking model. With no pairwise comparisons, a multiclass classifier is also much faster, lending itself to real-time use. It is probable that multiclass vs. pairwise approaches’ overall effectiveness depends on the amount of training data; pairwise ranking methods have potential advantages for cross-domain and one-shot learning tasks (Vinyals et al., 2016) where data is sparse or non-existent. In the closely related task of short-answer scoring, Sakaguchi et al. (2015a) found that pairwise methods could be effectively combined with regression-based approaches to improve performance in sparse-data cases. Other work involving dialogue utterance classification has traditionally required a large amount of data. For example, Suendermann et al. (2009) acquired 500,000 dialogues with over 2 million utterances, observin that statistical systems outperform rule-based ones as the amount of data increases. Crowdsourcing for collecting additional 3 Dataset The dataset consists of 94 dialogues of medical students interacting with the ChatScript system. The ChatScript s"
W17-5002,D14-1181,0,0.220259,"t dialogues. Rossen and Lok (2012) have developed an approach for collecting dialogue data for virtual patient systems, but their approach does not directly address the issue that even as the number of dialogues collected increases, there can remain a long tail of relevant but infrequently asked questions. CNNs have been used to great effect for image identification (Krizhevsky et al., 2012) and are becoming common for natural language processing. In general, CNNs are used for convolution over input language sequences, where the input is often a matrix representing a sequence word embeddings (Kim, 2014). Intuitively, word embedding kernels are convolving n-grams, ultimately generating features that represent n-grams over word vectors of length equal to the kernel width. CNNs are very popular in systems for tasks like paraphrase detection (Yin and Sch¨utze, 2015; Yin et al., 2016; He et al., 2015), community question answering (Das et al., 2016; Barbosa et al., 2016) and even machine translation (Gehring et al., 2017). Characterbased models that embed individual characters as input units are also possible, and have been used for language modeling (Kim et al., 2016) to good effect. It is worth"
W17-5002,D14-1162,0,0.0809616,"ed separately and remain fixed when the stacked model is being trained. Majority voting ˆ t = softmax( y Model setup and training (4) r 14 6.3 Initialization ChatScript Baseline CharCNN WordCNN Stacked For the word CNNs, we follow Kim (2014) to initialize the parameters. We use pre-trained word2vec word embeddings (Mikolov et al., 2013) for words that are in the whole dataset, and initialize embeddings of the other out-of-vocabulary words with Unif (−0.25, 0.25). This keeps the variance of each randomly initialized embedding close to the word2vec embeddings. We also tried the GloVe embeddings Pennington et al. (2014) and found it to be slight worse in performance than word2vec embeddings. We initialize the convolutional kernels with Unif (−0.01, 0.01) and the linear layer N (0, 1e − 4). We initialize all bias terms to 0. Ensembled n/a n/a 78.20 77.67 79.02 Table 1: Mean 10-fold Accuracy by System Type. Numbers reported are on the test set. For the character CNNs, we initialize all √ √ weights to follow Unif (−1/ nin , 1/ nin ) (Glorot and Bengio, 2010) where nin is the length of the input vector. For the convolutional kernels, the length of the input vector is hk. Additionally, we randomly initialize the"
W17-5002,N15-1091,0,0.0646983,"Missing"
W17-5002,Q16-1019,0,0.0414685,"Missing"
W17-5002,P15-2114,0,\N,Missing
W17-5405,D12-1096,0,0.0134449,"ight the distance which even sophisticated, modern sentiment analysis systems have yet to cover, particularly in terms of semantic and pragmatic analysis. Moreover, changes that broke the systems were often comparatively slight; just as image classification systems can be vulnerable to adversarial examples that look very similar to the originals (Szegedy et al., 2014), sentiment analysis systems may be fooled by changes to single words or morphemes. In many cases, of course, our strategies for constructing these examples drew on previous knowledge about hard problems, for instance in parsing (Kummerfeld et al., 2012) and the detection of irony in text (Wallace et al., 2014). Nonetheless, a concrete set of examples of these problems may help developers to create more robust systems in the future. For sets of constructed examples like ours to be useful, they should contain enough instances of each construction to reliably indicate a system’s capabilities. Looking towards the future, we hope that the next iteration of the contest will use a larger test section so that more examples can be created. Many of our strategies targeted particular constructions or idioms (for instance, right-node raising or concrete"
W17-5405,P14-2084,0,0.022055,"analysis systems have yet to cover, particularly in terms of semantic and pragmatic analysis. Moreover, changes that broke the systems were often comparatively slight; just as image classification systems can be vulnerable to adversarial examples that look very similar to the originals (Szegedy et al., 2014), sentiment analysis systems may be fooled by changes to single words or morphemes. In many cases, of course, our strategies for constructing these examples drew on previous knowledge about hard problems, for instance in parsing (Kummerfeld et al., 2012) and the detection of irony in text (Wallace et al., 2014). Nonetheless, a concrete set of examples of these problems may help developers to create more robust systems in the future. For sets of constructed examples like ours to be useful, they should contain enough instances of each construction to reliably indicate a system’s capabilities. Looking towards the future, we hope that the next iteration of the contest will use a larger test section so that more examples can be created. Many of our strategies targeted particular constructions or idioms (for instance, right-node raising or concrete metaphors), and it was difficult to create many instances"
W17-6208,Q14-1026,0,0.0273353,"Y XY a : α f : Mα → β >η X f aη : β <η (a) Base CCG Combinators with Monadic Type Coercion (not exhaustive) 7 Lower Right Lower Left .. .. . . A B a : Mα → β b : γ .. .. . . B A b : γ a : Mα → β .. . C c:β ↓R .. . C c:β A normal form parse is the simplest parse in an equivalence class of parses yielding the same interpretation. Normal form constraints can play an important role in practical CCG parsing by eliminating derivations leading to spurious ambiguities without requiring expensive pairwise equivalence checks on λ-terms (Eisner, 1996; Clark and Curran, 2007; Hockenmaier and Bisk, 2010; Lewis and Steedman, 2014). Continuized CCG can employ existing CCG normal form constraints at the base level. The main additional source of spurious ambiguity is illustrated in Figure 7.13 In the figure, the two towers at the upper left are combined via ↑R and ↑L to yield a three-level tower, which potentially allows an operator to subsequently take scope between any scopal elements present in the left and right input signs. However, if this threelevel tower is subsequently lowered without any operator taking intermediate scope, the derivation will yield an interpretation that is equivalent to the one yielded by the s"
W17-6208,P96-1011,0,0.227479,"ncreased to 4.6s per item, nearly two orders of magnitude slower. Y XY a : α f : Mα → β >η X f aη : β <η (a) Base CCG Combinators with Monadic Type Coercion (not exhaustive) 7 Lower Right Lower Left .. .. . . A B a : Mα → β b : γ .. .. . . B A b : γ a : Mα → β .. . C c:β ↓R .. . C c:β A normal form parse is the simplest parse in an equivalence class of parses yielding the same interpretation. Normal form constraints can play an important role in practical CCG parsing by eliminating derivations leading to spurious ambiguities without requiring expensive pairwise equivalence checks on λ-terms (Eisner, 1996; Clark and Curran, 2007; Hockenmaier and Bisk, 2010; Lewis and Steedman, 2014). Continuized CCG can employ existing CCG normal form constraints at the base level. The main additional source of spurious ambiguity is illustrated in Figure 7.13 In the figure, the two towers at the upper left are combined via ↑R and ↑L to yield a three-level tower, which potentially allows an operator to subsequently take scope between any scopal elements present in the left and right input signs. However, if this threelevel tower is subsequently lowered without any operator taking intermediate scope, the derivat"
W17-6208,levy-andrew-2006-tregex,0,\N,Missing
W17-6208,C10-1053,0,\N,Missing
W17-6208,J03-2002,0,\N,Missing
W17-6208,Q13-1005,0,\N,Missing
W17-6208,D16-1262,0,\N,Missing
W18-0502,N16-1172,0,0.0250028,". In its simplest form, one must simply replace a given word with an appropriate paraphrase, i.e. one that retains most of the original sentence’s meaning. As an example, in the question have you ever been seriously ill?, seriously could be replaced with severely, and we would consider this to be an appropriate substitution. However, if we instead substituted solemnly for the same word, we would not accept this as the meaning would have deviated too far. For generating paraphrases, we employ three resources: WordNet (Miller, 1995), Word2Vec (Le and Mikolov, 2014), and paraphrase clusters from Cocos and Callison-Burch (2016). To evaluate these resources, we took the mean average precision (MAP) of a given resource’s ability to produce a lexical substitution which matched a word that already existed in another variant for the same label. That is, if the label how has the pain affected your work? had only two variants, has the injury made your job difficult? and is it hard for you to do your job?, and a resource successfully produces the swap of hard → difficult (producing the sentence is it difficult for you to do your job?), this would positively affect a resource’s MAP score. We only performed this evaluation on"
W18-0502,D14-1181,0,0.0108042,"uestion variants are the paraphrases of the label appearing in the training data); given some similarity metric σ, we seek to find the yˆ label yˆ with the most similar question variant qi in the set Lyˆ to the candidate question q: y yˆ = argmax max σ(q, qi ) y y∈Y qi ∈Ly Early work on question answering (Ravichandran et al., 2003) found that treating the task as 14 4 Because of the data sparsity issue, we cast the problem of sentence classification for infrequent labels as a problem of few-shot learning. In particular, we use Kaiser et al.’s (2017) memory module together with a CNN encoder (Kim, 2014; Jin et al., 2017) as our main model, the memoryaugmented CNN classifier (MA-CNN). Our aim is to take advantage of the MA-CNN’s one-shot learning capability to mitigate the issue of data sparsity and also to make better use of data augmentation to achieve better performance. Figure 2: Label frequency distribution is extremely long-tailed, with few frequent labels and many infrequent labels. Values are shown above quintile boundaries. 4.1 tail of relevant but infrequently asked questions. The CNN encoder The CNN encoder follows Kim (2014) and Jin et al. (2017). We briefly summarize the archite"
W18-0502,I11-1150,0,0.0482249,"Missing"
W18-0502,E17-1083,0,0.128412,"ling candidates from all three resources performed better than any given one alone did. We also found that in the case of multiple word senses (e.g. bug meaning an insect, an illness, or a flaw in a program), simply picking the first sense produced a higher MAP score than a variety of other selection algorithms. This is not surprising since, Data Augmentation Since previous work (Jin et al., 2017) showed that the majority of labels in our dataset have 11 variants or fewer, we explore using lexical substitution (McCarthy and Navigli, 2009) and neural machine translation (NMT) back-translation (Mallinson et al., 2017) for data augmentation. The main difference in our use of lexical substitution and previous works’ is that our setup is unsupervised, as we have no gold test set for determining acceptable paraphrases. Similarly for the NMT system, we do not know which outputs are 17 in the case of WordNet, the first synset is the most frequently used sense of a given word. For Cocos and Callison-Burch’s semantic clusters, these were ordered by a given cluster’s average mutual paraphrase score as annotated in the Paraphrase Database (Ganitkevitch et al., 2013). Although our domain is medical, the dialogues are"
W18-0502,W16-1718,1,0.93011,"ne must simply replace a given word with an appropriate paraphrase, i.e. one that retains most of the original sentence’s meaning. As an example, in the question have you ever been seriously ill?, seriously could be replaced with severely, and we would consider this to be an appropriate substitution. However, if we instead substituted solemnly for the same word, we would not accept this as the meaning would have deviated too far. For generating paraphrases, we employ three resources: WordNet (Miller, 1995), Word2Vec (Le and Mikolov, 2014), and paraphrase clusters from Cocos and Callison-Burch (2016). To evaluate these resources, we took the mean average precision (MAP) of a given resource’s ability to produce a lexical substitution which matched a word that already existed in another variant for the same label. That is, if the label how has the pain affected your work? had only two variants, has the injury made your job difficult? and is it hard for you to do your job?, and a resource successfully produces the swap of hard → difficult (producing the sentence is it difficult for you to do your job?), this would positively affect a resource’s MAP score. We only performed this evaluation on"
W18-0502,P13-1158,0,0.0376992,"ing, we pursue paraphrasing for data augmentation in this paper, focusing on the simplest methods to employ, namely lexical substitution and neural backtranslation (see Section 5). The idea is to augment the observed question instances for questions with infrequent labels in the dataset with automatically generated paraphrases, with the aim of making such questions easier to recognize using machinelearned models. In future work, we plan to explore more complex paraphrasing methods, including syntactic paraphrasing (Duan et al., 2016) and inducing paraphrase templates from aligned paraphrases (Fader et al., 2013). 3 Memory-Augmented CNN Classifier Data Imbalance Our dataset currently consists of 4330 questionanswer pairs from 94 dialogues between first year medical students and the virtual patient. After classifying an asked question as having a certain label, the virtual patient replies with the canned response for that label, as illustrated in Table 1. Unfortunately, the labels do not have a uniform distribution with regards to the number of variants each label has (that is, the number of question instances for that label in the dataset). In fact, most of the labels are underrepresented. e= o · Wl +"
W18-0502,N13-1092,0,0.0704486,"Missing"
W18-0502,W15-0611,1,0.662908,"ach dialogue turn. With sufficient pattern-writing skill and effort, pattern matching with ChatScript 13 Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 13–23 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics a maximum entropy re-ranking problem outperformed using the same system as a multiclass classifier. By contrast, DeVault et al. (2011) observed that maximum entropy multiclass classifiers performed well with simple n-gram features when each class had a sufficient number of training examples. Jaffe et al. (2015) explored a log-linear pairwise ranking model for question identification in a virtual patient dialogue system and found it outperformed a multiclass baseline along the lines of DeVault et al. (2011). However, Jaffe et al. used a much smaller dataset with only about 915 user turns, less than one-fourth as many as in the current dataset. For this larger dataset, a straightforward logistic regression multiclass classifier outperforms a pairwise ranking model. Figure 1: Virtual Patient Dialogue System In general it appears reasonable to expect that the comparative effectiveness of multiclass vs."
W18-0502,W03-1209,0,0.0419867,"up can be used. For example, for each class a binary classification decision can be made as to whether a given question represents a paraphrase of a member of the class, choosing the highest confidence match. More geny erally, let qi ∈ Ly be the i-th question variant for label y (where the question variants are the paraphrases of the label appearing in the training data); given some similarity metric σ, we seek to find the yˆ label yˆ with the most similar question variant qi in the set Lyˆ to the candidate question q: y yˆ = argmax max σ(q, qi ) y y∈Y qi ∈Ly Early work on question answering (Ravichandran et al., 2003) found that treating the task as 14 4 Because of the data sparsity issue, we cast the problem of sentence classification for infrequent labels as a problem of few-shot learning. In particular, we use Kaiser et al.’s (2017) memory module together with a CNN encoder (Kim, 2014; Jin et al., 2017) as our main model, the memoryaugmented CNN classifier (MA-CNN). Our aim is to take advantage of the MA-CNN’s one-shot learning capability to mitigate the issue of data sparsity and also to make better use of data augmentation to achieve better performance. Figure 2: Label frequency distribution is extrem"
W18-0502,N15-1111,0,0.0484112,"Missing"
W18-0502,W16-2323,0,0.0234032,"n translations it came from in the forward step and its ranking among the m translations in the back step. Any duplicates within this final set are collapsed and their weights are combined before the set is ranked according to weight. This method favors translations which come from high quality sources (highranking translations in the lists n and m) as well as translations which occur multiple times. In our work we translated each given source sentence into 10-best forward translations and 10best back translations before finally collapsing and ranking the 100 paraphrases. We used a model from Sennrich et al. (2016) and chose German as our pivot language given the quality of the translations and paraphrases we observed.2 1 We used a 5-gram language model with back off, trained on the Gigaword (Parker et al., 2011). 2 We found that the pretrained model for German produced the best back-translations when compared to other pretrained models. In future work, we plan to train our own models across various pivot languages to produce an increased variety of paraphrases. 18 CNN. We only use word-based features in the encoder. Following Jin et al. (2017), we set the number of kernels of the encoder of MA-CNN to b"
W18-3605,C00-1007,0,0.403658,"ty), we opted to allow non-projective outputs to be generated. To do so, we used a discontinuity feature to encourage the model to learn that most choices should yield continuous phrases, nsubj advmod Dit is mooi . . . Figure 2: An example (from the Dutch training set) of how child dependencies with argument relations help with inflection, while other modifier relations do not. The person features in DIT help to realize ZIJN as IS and not BENT or BEN. However, the features from the advmod relation are not helpful. 4 det root Linearization Previous work on dependency-based surface realization (Bangalore and Rambow, 2000; Filippova and Strube, 2007, 2009; Guo et al., 2008; Bohnet et al., 2010, 2011; Guo et al., 2011; Zhang and Clark, 2015) has emphasized bottom-up approaches that make relatively little use of dependency locality. For this task, we opted to follow Liu et al. (2015); Puduppully et al. (2016) in taking an incremental approach to linearization so as to be compatible with future work incorporating neural language models (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Konstas et al., 2017) while giving greater emphasis to locality considerations. 42 Events next word dependency ordering completed arc"
W18-3605,C10-1012,0,0.575986,"a discontinuity feature to encourage the model to learn that most choices should yield continuous phrases, nsubj advmod Dit is mooi . . . Figure 2: An example (from the Dutch training set) of how child dependencies with argument relations help with inflection, while other modifier relations do not. The person features in DIT help to realize ZIJN as IS and not BENT or BEN. However, the features from the advmod relation are not helpful. 4 det root Linearization Previous work on dependency-based surface realization (Bangalore and Rambow, 2000; Filippova and Strube, 2007, 2009; Guo et al., 2008; Bohnet et al., 2010, 2011; Guo et al., 2011; Zhang and Clark, 2015) has emphasized bottom-up approaches that make relatively little use of dependency locality. For this task, we opted to follow Liu et al. (2015); Puduppully et al. (2016) in taking an incremental approach to linearization so as to be compatible with future work incorporating neural language models (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Konstas et al., 2017) while giving greater emphasis to locality considerations. 42 Events next word dependency ordering completed arc discontinuity Base Predictors trigram word, stem and POS whether initial"
W18-3605,K17-2001,0,0.0699478,"Missing"
W18-3605,P17-1014,0,0.0445934,"d relation are not helpful. 4 det root Linearization Previous work on dependency-based surface realization (Bangalore and Rambow, 2000; Filippova and Strube, 2007, 2009; Guo et al., 2008; Bohnet et al., 2010, 2011; Guo et al., 2011; Zhang and Clark, 2015) has emphasized bottom-up approaches that make relatively little use of dependency locality. For this task, we opted to follow Liu et al. (2015); Puduppully et al. (2016) in taking an incremental approach to linearization so as to be compatible with future work incorporating neural language models (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Konstas et al., 2017) while giving greater emphasis to locality considerations. 42 Events next word dependency ordering completed arc discontinuity Base Predictors trigram word, stem and POS whether initial or final; parent and child stems, POSs, grammatical features and dep relation; sibling stem, POS and dep rel whether projective; parent and child stems, POSs and dep relation trigram POS; bigram dep relation; relation of extraposed dep Locality Predictors difference in log binned size of sibling subtree log binned dependency length log binned size of extraposed subtree Table 4: Linearization features, which com"
W18-3605,N13-1138,0,0.0668284,"data, we adopt their architecture hypothesizing that it will generalize to the SRST ’18 data. The architecture uses gated recurrent units (Chung et al., 2015, GRU), a kind of recurrent neural network, whose hidden state ht depends on the current input xt , the previous hidden state ht−1 , and nonlinear function f Plural wir trinken ihr trinkt sie trinken Table 2: The inferred paradigm for TRINKEN (‘to drink’) as learned by the partial paradigms for SIN ¨ GEN (‘to sing’) and H OREN (‘to hear’) from Table 1. There has been extensive work computationally to combat the PCFP (Nicolai et al., 2015; Durrett and DeNero, 2013) and multiple shared tasks (Cotterell et al., 2016, 2017). Recently, models utilizing recurrent neural networks have proven most effective at the PCFP and have produced state-of-the-art results in the last two SIGMORPHON shared tasks (Kann and Sch¨utze, 2016). Although we think this approach lends itself to our task, in that we need to produce fully inflected wordforms along with linearizing them, Kann and Sch¨utze’s system has only been tested on SIGMORPHON data and, to our knowledge, has never been used in a downstream task such as surface realization. Turning now to dependency locality, Raj"
W18-3605,P16-2008,0,0.0571985,"Missing"
W18-3605,N16-1077,0,0.161547,"done jointly. For English, a high resource language, morphological inflection is relatively simple to do with existing rule-based resources like MorphG.1 To predict fully inflected word forms in other languages as well, we exploit recent advances in neural machine translation (NMT) as implemented by Kann and Sch¨utze in the two most recent SIGMORPHON shared tasks. Their system is based on Bahdanau et al.’s (2014) attention-based NMT architecture and models the task of wordform prediction as a kind of translation of one sequence to another. Figure 1 shows the original architecture developed by Faruqui et al. (2016). Given Kann and Sch¨utze’s success in adapting this architecture to work with the SIGMORPHON data, we adopt their architecture hypothesizing that it will generalize to the SRST ’18 data. The architecture uses gated recurrent units (Chung et al., 2015, GRU), a kind of recurrent neural network, whose hidden state ht depends on the current input xt , the previous hidden state ht−1 , and nonlinear function f Plural wir trinken ihr trinkt sie trinken Table 2: The inferred paradigm for TRINKEN (‘to drink’) as learned by the partial paradigms for SIN ¨ GEN (‘to sing’) and H OREN (‘to hear’) from Tab"
W18-3605,N15-1012,0,0.485348,"king.2138@osu.edu Abstract PHON shared tasks on morphological reinflection (Faruqui et al., 2016; Kann and Sch¨utze, 2016) could be adapted to the more realistic setting for generation of SRST ’18. Second, we aimed to investigate the extent to which dependency locality (Gibson, 2000) features previously shown to be important for grammar-based generation in English (White and Rajkumar, 2012) and in corpus-based studies of syntactic choice (Temperley, 2007; Liu, 2008; Gildea and Temperley, 2010; Rajkumar et al., 2016) would also prove effective with incremental, dependency-based linearization (Liu et al., 2015; Puduppully et al., 2016) across languages. At an overview level, our system treats the task of surface realization as a simple two-stage process. First, we convert uninflected lexemes to fully inflected wordforms using the grammatical features supplied by the UD corpus; and second, we incrementally linearize the inflected words using the supplied syntactic dependencies, grammatical features and locality-based features that take dependency length and phrase size into account. A simple rule-based detokenizer attaches punctuation to adjacent words in a final step. The system was trained using o"
W18-3605,W18-3601,0,0.077911,"sequence-to-sequence model before incrementally linearizing them. For linearization, we use a global linear model trained using early update that makes use of features that take into account the dependency structure and dependency locality. Using this pipeline sufficed to produce surprisingly strong results in the shared task. In future work, we intend to pursue joint approaches to linearization and morphological inflection and incorporating a neural language model into the linearization choices. 1 Introduction We participated in the surface track of the 2018 Surface Realization Shared Task (Mille et al., 2018, SRST ’18). In the surface track, task inputs were created by extracting sentences in 10 languages from the Universal Dependency treebanks corpus, scrambling the words and converting them to their citation form. The task was then to generate a natural and semantically adequate sentence by inflecting and ordering the words. Our aims in participating in the shared task were twofold. First, we aimed to investigate the extent to which neural sequence-to-sequence models developed for the 2016 and 2017 SIGMOR2 Background The intuition behind using neural and statistical models for learning morpholo"
W18-3605,P07-1041,0,0.0491444,"rojective outputs to be generated. To do so, we used a discontinuity feature to encourage the model to learn that most choices should yield continuous phrases, nsubj advmod Dit is mooi . . . Figure 2: An example (from the Dutch training set) of how child dependencies with argument relations help with inflection, while other modifier relations do not. The person features in DIT help to realize ZIJN as IS and not BENT or BEN. However, the features from the advmod relation are not helpful. 4 det root Linearization Previous work on dependency-based surface realization (Bangalore and Rambow, 2000; Filippova and Strube, 2007, 2009; Guo et al., 2008; Bohnet et al., 2010, 2011; Guo et al., 2011; Zhang and Clark, 2015) has emphasized bottom-up approaches that make relatively little use of dependency locality. For this task, we opted to follow Liu et al. (2015); Puduppully et al. (2016) in taking an incremental approach to linearization so as to be compatible with future work incorporating neural language models (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Konstas et al., 2017) while giving greater emphasis to locality considerations. 42 Events next word dependency ordering completed arc discontinuity Base Predictor"
W18-3605,N09-2057,0,0.45783,"Missing"
W18-3605,N15-1093,0,0.0231182,"k with the SIGMORPHON data, we adopt their architecture hypothesizing that it will generalize to the SRST ’18 data. The architecture uses gated recurrent units (Chung et al., 2015, GRU), a kind of recurrent neural network, whose hidden state ht depends on the current input xt , the previous hidden state ht−1 , and nonlinear function f Plural wir trinken ihr trinkt sie trinken Table 2: The inferred paradigm for TRINKEN (‘to drink’) as learned by the partial paradigms for SIN ¨ GEN (‘to sing’) and H OREN (‘to hear’) from Table 1. There has been extensive work computationally to combat the PCFP (Nicolai et al., 2015; Durrett and DeNero, 2013) and multiple shared tasks (Cotterell et al., 2016, 2017). Recently, models utilizing recurrent neural networks have proven most effective at the PCFP and have produced state-of-the-art results in the last two SIGMORPHON shared tasks (Kann and Sch¨utze, 2016). Although we think this approach lends itself to our task, in that we need to produce fully inflected wordforms along with linearizing them, Kann and Sch¨utze’s system has only been tested on SIGMORPHON data and, to our knowledge, has never been used in a downstream task such as surface realization. Turning now"
W18-3605,N16-1058,0,0.429239,"Abstract PHON shared tasks on morphological reinflection (Faruqui et al., 2016; Kann and Sch¨utze, 2016) could be adapted to the more realistic setting for generation of SRST ’18. Second, we aimed to investigate the extent to which dependency locality (Gibson, 2000) features previously shown to be important for grammar-based generation in English (White and Rajkumar, 2012) and in corpus-based studies of syntactic choice (Temperley, 2007; Liu, 2008; Gildea and Temperley, 2010; Rajkumar et al., 2016) would also prove effective with incremental, dependency-based linearization (Liu et al., 2015; Puduppully et al., 2016) across languages. At an overview level, our system treats the task of surface realization as a simple two-stage process. First, we convert uninflected lexemes to fully inflected wordforms using the grammatical features supplied by the UD corpus; and second, we incrementally linearize the inflected words using the supplied syntactic dependencies, grammatical features and locality-based features that take dependency length and phrase size into account. A simple rule-based detokenizer attaches punctuation to adjacent words in a final step. The system was trained using only the supplied data. We"
W18-3605,C08-1038,0,0.269549,"Missing"
W18-3605,W11-2833,0,0.0639162,"Missing"
W18-3605,D15-1199,0,0.112705,"Missing"
W18-3605,D12-1023,1,0.960917,"tion and Incremental Locality-Based Linearization Michael White The Ohio State University Department of Linguistics Columbus, Ohio mwhite@ling.osu.edu David L. King The Ohio State University Department of Linguistics Columbus, Ohio king.2138@osu.edu Abstract PHON shared tasks on morphological reinflection (Faruqui et al., 2016; Kann and Sch¨utze, 2016) could be adapted to the more realistic setting for generation of SRST ’18. Second, we aimed to investigate the extent to which dependency locality (Gibson, 2000) features previously shown to be important for grammar-based generation in English (White and Rajkumar, 2012) and in corpus-based studies of syntactic choice (Temperley, 2007; Liu, 2008; Gildea and Temperley, 2010; Rajkumar et al., 2016) would also prove effective with incremental, dependency-based linearization (Liu et al., 2015; Puduppully et al., 2016) across languages. At an overview level, our system treats the task of surface realization as a simple two-stage process. First, we convert uninflected lexemes to fully inflected wordforms using the grammatical features supplied by the UD corpus; and second, we incrementally linearize the inflected words using the supplied syntactic dependencies, gra"
W18-6528,W11-2835,0,0.062773,"Missing"
W18-6528,C10-1012,0,0.0626774,"Missing"
W18-6528,boxwell-white-2008-projecting,1,0.744415,". Note that semantically empty function words such as infinitival-to are missing. Generally speaking, the semantic dependency graphs are more abstract than unordered dependency trees, but more detailed than AMRs. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization, where the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). As in other work with OpenCCG (e.g., Duan and White, 2014), we use OpenCCG’s realization ranking model off the shelf in order to select preferred outputs from the chart; in particular, we use White & Rajkumar’s (2009; 2012) averaged perceptron realization ranking model augmented with a large-scale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000): inflecting lemm"
W18-6528,P17-1112,0,0.05692,"Missing"
W18-6528,P06-1088,0,0.124024,"Missing"
W18-6528,W11-2832,1,0.861691,"Missing"
W18-6528,P08-1022,1,0.694564,"Missing"
W18-6528,W10-4237,0,0.0540705,"Missing"
W18-6528,J17-1001,0,0.017255,"e sentence arguably makes the best choices here). More generally, while the choice whether to include a that-complementizer occasionally made a crucial difference, they were a frequent source of insubstantial differences, along with contractions and adverbial placement. There were also cases where both realizations made distinct but important mistakes that yielded equally bad realizations. 5 Related Work Hypertagging can potentially benefit other grammar-based methods using lexicalized grammars, e.g. using HPSG (Velldal and Oepen, 2005; Carroll and Oepen, 2005; Nakanishi et al., 2005) or TAG (Gardent and Perez-Beltrachini, 2017). Much recent work in NLG (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Kiddon et al., 2016; Konstas et al., 2017; Wiseman et al., 2017) has made use of neural sequenceto-sequence methods for generation rather than grammar-based methods. The learning flexibility of neural methods make it possible to develop very knowledge lean systems, but they continue to suffer from a tendency to hallucinate content and have not been used with texts exhibiting 216 Set 1 (±complete) 2 (=complete) Better 84 31 Adequacy Same Worse 12 4 62 7 Better 88 37 Fluency Same Worse 7 5 52 11 Table 4: Re"
W18-6528,W05-1510,0,0.0716916,"or fluency (though the reference sentence arguably makes the best choices here). More generally, while the choice whether to include a that-complementizer occasionally made a crucial difference, they were a frequent source of insubstantial differences, along with contractions and adverbial placement. There were also cases where both realizations made distinct but important mistakes that yielded equally bad realizations. 5 Related Work Hypertagging can potentially benefit other grammar-based methods using lexicalized grammars, e.g. using HPSG (Velldal and Oepen, 2005; Carroll and Oepen, 2005; Nakanishi et al., 2005) or TAG (Gardent and Perez-Beltrachini, 2017). Much recent work in NLG (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Kiddon et al., 2016; Konstas et al., 2017; Wiseman et al., 2017) has made use of neural sequenceto-sequence methods for generation rather than grammar-based methods. The learning flexibility of neural methods make it possible to develop very knowledge lean systems, but they continue to suffer from a tendency to hallucinate content and have not been used with texts exhibiting 216 Set 1 (±complete) 2 (=complete) Better 84 31 Adequacy Same Worse 12 4 62 7 Better 8"
W18-6528,J05-1004,0,0.0380014,"rds such as infinitival-to are missing. Generally speaking, the semantic dependency graphs are more abstract than unordered dependency trees, but more detailed than AMRs. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization, where the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). As in other work with OpenCCG (e.g., Duan and White, 2014), we use OpenCCG’s realization ranking model off the shelf in order to select preferred outputs from the chart; in particular, we use White & Rajkumar’s (2009; 2012) averaged perceptron realization ranking model augmented with a large-scale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000): inflecting lemmas with grammatical word forms, insert"
W18-6528,C08-1038,0,0.0821377,"Missing"
W18-6528,N16-1058,0,0.0341997,"Missing"
W18-6528,J07-3004,0,0.0530952,"predicates in the input, as noted above. An example input appears in Figure 1. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure; gold standard CCG lexical categories (i.e, what the hypertagger learns to predict) are also shown. Note that semantically empty function words such as infinitival-to are missing. Generally speaking, the semantic dependency graphs are more abstract than unordered dependency trees, but more detailed than AMRs. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization, where the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). As in other work with OpenCCG (e.g., Duan and White, 2014), we use OpenCCG’s realization ranking model off the shelf in order to select preferred outputs from the chart; in particular, we use White & Rajkumar’s (2009; 2012) averaged per"
W18-6528,E17-1061,0,0.0404563,"Missing"
W18-6528,D16-1032,0,0.0191244,"casionally made a crucial difference, they were a frequent source of insubstantial differences, along with contractions and adverbial placement. There were also cases where both realizations made distinct but important mistakes that yielded equally bad realizations. 5 Related Work Hypertagging can potentially benefit other grammar-based methods using lexicalized grammars, e.g. using HPSG (Velldal and Oepen, 2005; Carroll and Oepen, 2005; Nakanishi et al., 2005) or TAG (Gardent and Perez-Beltrachini, 2017). Much recent work in NLG (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Kiddon et al., 2016; Konstas et al., 2017; Wiseman et al., 2017) has made use of neural sequenceto-sequence methods for generation rather than grammar-based methods. The learning flexibility of neural methods make it possible to develop very knowledge lean systems, but they continue to suffer from a tendency to hallucinate content and have not been used with texts exhibiting 216 Set 1 (±complete) 2 (=complete) Better 84 31 Adequacy Same Worse 12 4 62 7 Better 88 37 Fluency Same Worse 7 5 52 11 Table 4: Results (counts of judgments) of human evaluations of realizations, which indicate how often the new system pro"
W18-6528,C10-2119,1,0.769883,"ores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors. With function words, Rajkumar and White (2011) showed that they could improve upon the earlier model’s predictions for when to employ that-complementizers using features inspired by Jaeger’s (2010) work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and with expressions such as a lot of where the correct choice is not determined solely by the head noun. They also improved animacy agreement with relativizers, reducing the number of errors where that or which was chosen to modify an animate noun rather than who or whom (and vice-versa), while also allowing both choices where corpus evidence was mixed. 211 2.2 Original Hypertagger The original MaxEnt hypertagger uses three general types of features f"
W18-6528,W18-3605,1,0.831539,"Missing"
W18-6528,W11-2706,1,0.749391,"e m1 make.03 s[b]
p/np <Arg0&gt; Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node order. Notably, to improve word ordering decisions, White & Rajkumar (2012) demonstrated that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors. With function words, Rajkumar and White (2011) showed that they could improve upon the earlier model’s predictions for when to employ that-complementizers using features inspired by Jaeger’s (2010) work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) extended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and with expressio"
W18-6528,P17-1014,0,0.026067,"cial difference, they were a frequent source of insubstantial differences, along with contractions and adverbial placement. There were also cases where both realizations made distinct but important mistakes that yielded equally bad realizations. 5 Related Work Hypertagging can potentially benefit other grammar-based methods using lexicalized grammars, e.g. using HPSG (Velldal and Oepen, 2005; Carroll and Oepen, 2005; Nakanishi et al., 2005) or TAG (Gardent and Perez-Beltrachini, 2017). Much recent work in NLG (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Kiddon et al., 2016; Konstas et al., 2017; Wiseman et al., 2017) has made use of neural sequenceto-sequence methods for generation rather than grammar-based methods. The learning flexibility of neural methods make it possible to develop very knowledge lean systems, but they continue to suffer from a tendency to hallucinate content and have not been used with texts exhibiting 216 Set 1 (±complete) 2 (=complete) Better 84 31 Adequacy Same Worse 12 4 62 7 Better 88 37 Fluency Same Worse 7 5 52 11 Table 4: Results (counts of judgments) of human evaluations of realizations, which indicate how often the new system produced better, same, an"
W18-6528,N09-2041,1,0.791019,"etween nodes encode argument structure; gold standard CCG lexical categories (i.e, what the hypertagger learns to predict) are also shown. Note that semantically empty function words such as infinitival-to are missing. Generally speaking, the semantic dependency graphs are more abstract than unordered dependency trees, but more detailed than AMRs. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization, where the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). As in other work with OpenCCG (e.g., Duan and White, 2014), we use OpenCCG’s realization ranking model off the shelf in order to select preferred outputs from the chart; in particular, we use White & Rajkumar’s (2009; 2012) averaged perceptron realization ranking model augmented with a large-scale 5-gram model based on the Gigaword corpus. The ranking model makes choices addressing all three interrelated sub-tasks tra"
W18-6528,N16-1026,0,0.0564264,"Missing"
W18-6528,Q14-1026,0,0.0554604,"Missing"
W18-6528,N15-1012,0,0.0437551,"Missing"
W18-6528,N16-1086,0,0.0131075,"-complementizer occasionally made a crucial difference, they were a frequent source of insubstantial differences, along with contractions and adverbial placement. There were also cases where both realizations made distinct but important mistakes that yielded equally bad realizations. 5 Related Work Hypertagging can potentially benefit other grammar-based methods using lexicalized grammars, e.g. using HPSG (Velldal and Oepen, 2005; Carroll and Oepen, 2005; Nakanishi et al., 2005) or TAG (Gardent and Perez-Beltrachini, 2017). Much recent work in NLG (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Kiddon et al., 2016; Konstas et al., 2017; Wiseman et al., 2017) has made use of neural sequenceto-sequence methods for generation rather than grammar-based methods. The learning flexibility of neural methods make it possible to develop very knowledge lean systems, but they continue to suffer from a tendency to hallucinate content and have not been used with texts exhibiting 216 Set 1 (±complete) 2 (=complete) Better 84 31 Adequacy Same Worse 12 4 62 7 Better 88 37 Fluency Same Worse 7 5 52 11 Table 4: Results (counts of judgments) of human evaluations of realizations, which indicate how oft"
W18-6528,D15-1199,0,0.0605883,"Missing"
W18-6528,W08-1703,1,0.749937,"se referents labeled with lexical predicates, and dependency relations between nodes encode argument structure; gold standard CCG lexical categories (i.e, what the hypertagger learns to predict) are also shown. Note that semantically empty function words such as infinitival-to are missing. Generally speaking, the semantic dependency graphs are more abstract than unordered dependency trees, but more detailed than AMRs. The grammar is extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) enhanced for realization, where the enhancements include: better analyses of punctuation (White and Rajkumar, 2008); less error prone handling of named entities (Rajkumar et al., 2009); re-inserting quotes into the CCGbank; and assignment of consistent semantic roles across diathesis alternations (Boxwell and White, 2008), using PropBank (Palmer et al., 2005). As in other work with OpenCCG (e.g., Duan and White, 2014), we use OpenCCG’s realization ranking model off the shelf in order to select preferred outputs from the chart; in particular, we use White & Rajkumar’s (2009; 2012) averaged perceptron realization ranking model augmented with a large-scale 5-gram model based on the Gigaword corpus. The rankin"
W18-6528,D09-1043,1,0.810009,"Missing"
W18-6528,D12-1023,1,0.807404,"anking model makes choices addressing all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research (Reiter and Dale, 2000): inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural a w1 np h3 <TENSE&gt;pres <Arg1&gt; <Arg0&gt; np/n want.01 he m1 make.03 s[b]
p/np <Arg0&gt; Figure 1: Example OpenCCG semantic dependency input for he has a point he wants to make, with gold standard lexical categories for each node order. Notably, to improve word ordering decisions, White & Rajkumar (2012) demonstrated that incorporating a feature into the ranker inspired by Gibson’s (2000) dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors. With function words, Rajkumar and White (2011) showed that they could improve upon the earlier model’s predictions for when to employ that-complementizers using features inspired by Jaeger’s (2010) work on using the principle of uniform information density, which hold"
W18-6528,D17-1239,0,0.0233492,"Missing"
W19-0123,Q13-1005,0,0.0142398,"afforded by Continuized CCG (Barker & Shan, 2014) makes it possible to not only implement an improved analysis of negative polarity items in Dynamic Continuized CCG (White et al., 2017) but also to develop an accurate treatment of balanced punctuation. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) has been increasingly employed with success for a wide range of NLP problems. An important factor in its success is that its flexible treatment of word order and constituency enable it to employ a compact lexicon, making it easier to acquire lexicalized grammars automatically (Artzi and Zettlemoyer, 2013) and to train machinelearned models that generalize well (Clark and Curran, 2007; Lee et al., 2016). However, its word order flexibility can be problematic for linguistic phenomena where linear order plays a key role. In particular, linear order effects can be problematic for Steedman’s (2012) treatment of negative polarity items (NPIs) as well as for implementing a treatment of balanced punctuation that attempts to track the status of the right periphery, as White & Rajkumar (2008) have previously shown. In this paper, we show that the enhanced control over evaluation order afforded by Contin"
W19-0123,J07-4004,0,0.0467633,"ment an improved analysis of negative polarity items in Dynamic Continuized CCG (White et al., 2017) but also to develop an accurate treatment of balanced punctuation. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) has been increasingly employed with success for a wide range of NLP problems. An important factor in its success is that its flexible treatment of word order and constituency enable it to employ a compact lexicon, making it easier to acquire lexicalized grammars automatically (Artzi and Zettlemoyer, 2013) and to train machinelearned models that generalize well (Clark and Curran, 2007; Lee et al., 2016). However, its word order flexibility can be problematic for linguistic phenomena where linear order plays a key role. In particular, linear order effects can be problematic for Steedman’s (2012) treatment of negative polarity items (NPIs) as well as for implementing a treatment of balanced punctuation that attempts to track the status of the right periphery, as White & Rajkumar (2008) have previously shown. In this paper, we show that the enhanced control over evaluation order afforded by Continuized CCG (Barker & Shan, 2014) makes it possible to successfully address both o"
W19-0123,forst-kaplan-2006-importance,0,0.0897121,"Missing"
W19-0123,J07-3004,0,0.0394038,"Missing"
W19-0123,D16-1262,0,0.0444117,"Missing"
W19-0123,W17-6208,1,0.920632,"e@ling.osu.edu Abstract Combinatory Categorial Grammar’s (CCG; Steedman, 2000) flexible treatment of word order and constituency enable it to employ a compact lexicon, an important factor in its successful application to a range of NLP problems. However, its word order flexibility can be problematic for linguistic phenomena where linear order plays a key role. In this paper, we show that the enhanced control over evaluation order afforded by Continuized CCG (Barker & Shan, 2014) makes it possible to not only implement an improved analysis of negative polarity items in Dynamic Continuized CCG (White et al., 2017) but also to develop an accurate treatment of balanced punctuation. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) has been increasingly employed with success for a wide range of NLP problems. An important factor in its success is that its flexible treatment of word order and constituency enable it to employ a compact lexicon, making it easier to acquire lexicalized grammars automatically (Artzi and Zettlemoyer, 2013) and to train machinelearned models that generalize well (Clark and Curran, 2007; Lee et al., 2016). However, its word order flexibility can be problematic fo"
W19-0123,W08-1703,1,0.723347,"ency enable it to employ a compact lexicon, making it easier to acquire lexicalized grammars automatically (Artzi and Zettlemoyer, 2013) and to train machinelearned models that generalize well (Clark and Curran, 2007; Lee et al., 2016). However, its word order flexibility can be problematic for linguistic phenomena where linear order plays a key role. In particular, linear order effects can be problematic for Steedman’s (2012) treatment of negative polarity items (NPIs) as well as for implementing a treatment of balanced punctuation that attempts to track the status of the right periphery, as White & Rajkumar (2008) have previously shown. In this paper, we show that the enhanced control over evaluation order afforded by Continuized CCG (Barker & Shan, 2014) makes it possible to successfully address both of these problems. In particular, we show that Barker & Shan’s analysis of NPIs, which does not suffer from the linear order issues that we raise for Steedman’s analysis, can be straightforwardly implemented in Dynamic Continuized CCG (White et al., 2017), a system that combines Barker & Shan’s “tower” grammars with Charlow’s (2014) monadic approach to dynamic semantics, employing Steedman’s CCG on the to"
W19-5608,al-sabbagh-girju-2012-yadac,0,0.0247246,"of EA. 4 Case study: verb register variation To assure tractability for this study, two subcorpora were created from CALM: (1) the Movies subcorpus, consisting of transcriptions from movies (113,163 word tokens) and television shows (115,236 word tokens), for a total of 228,399 word tokens including 38,768 verbs; and (2) the Blogs subcorpus, containing 141,318 word tokens and 27,616 verbs. While not exactly balanced, they are of reasonably comparable size. For this study only the verbs were annotated, in part because they are slightly easier to identify and annotate than nouns and adjectives (Al-Sabbagh and Girju, 2012a), and because of their widespread use in determining register (Ferguson, 1983; Friginal, 2009; Staples, 2016). Table 1 gives sample annotations for several verbal features. In this section, then, we perform a register variation analysis on verb features to characterize the two dimensions of content in CALM: oral versus literate (or spoken versus written), as represented by the Movies and Blogs subcorpora, respectively. The ﬁrst step was to annotate each verb in the two subcorpora. Once each verb was assigned a part-of-speech tag, a verbal category, and a lemma, each of these features were co"
W19-5608,pasha-etal-2014-madamira,0,0.0561152,"Missing"
W19-5608,baroni-bernardini-2004-bootcat,0,0.1338,"e blog subcorpus. Although internet texts can be classiﬁed into many diﬀerent genres (Biber et al., 2015), in this paper they will be treated as a single register. We exclude internet texts that contain transcriptions of speeches, movies, television programs, and songs. Some of the blog texts were collected from the internet based upon seeded n-gram searches via Bing and Google, as discussed in the previous section, though this time relying on frequent dialectspeciﬁc words to decrease the chances of dialect mixing. We also used BootCat, a do-it-yourself web-to-corpus text conversion pipeline (Baroni and Bernardini, 2004), to ﬁnd, scrape, and convert other webpages written in Egyptian Arabic into text ﬁles. A cursory review of the ﬁles was completed to remove non-EA texts that were returned by the process. However, some MSA is contained in CALM because it is interwoven throughout posts written in Egyptian; posts completely written in MSA, though, were removed. EA exhibits numerous orthographic, lexical, morphological, and syntactic diﬀerences from MSA that will be familiar to many Arabists (ElTonsi, 1982; Hassan, 2000; Ryding, 2005; AbdelMassih et al., 2009). Even the representation of lemmas (base forms, or d"
W19-5608,R13-1026,0,0.0869912,"Missing"
W19-5608,N04-4038,0,0.114137,"Missing"
W19-5608,W05-0708,0,0.0856974,"Missing"
W19-5608,J93-1003,0,0.695353,"Missing"
W19-5608,maamouri-etal-2014-developing,0,0.0342354,"Missing"
W19-5608,W12-2301,0,0.0623002,"Missing"
W19-5608,W00-0901,0,0.233702,"Missing"
W19-5608,P06-1086,0,0.151445,"Missing"
W19-5608,I05-3005,0,0.177205,"Missing"
W19-8611,N19-1236,0,0.0289361,"ut following the input tree structures. We represent each token in the input MR as a tree node, using the tree structure to compute the hidden state of the k-th parent node hpk as a function of its child states {hck1 , ..., hckN }: Related Work Several previous works have focused on adding planning steps to neural NLG architectures or employed non-sequential encoders. Puduppully et al. (2019) add a content planning step where a set of input database records are mapped to an ordered list of selected records; however, their approach does not employ hierarchical content plans as in our approach. Moryossef et al. (2019) add a symbolic text planning step where facts are grouped and ordered in the input; in contrast to our work though, their approach uses standard Seq2Seq models for realization and leaves no ordering choices to the model. Previous work on AMR and WebNLG (Beck et al., 2018; Song et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) has demonstrated improvements over Seq2Seq models by using graph-to-sequence models; while similar in principle, these works do not explore the use of hierarchical content plans as intermediate structures and do not experiment with constrained decoding. hpk = ftree"
W19-8611,P19-1080,1,0.854136,"ed MR in the last row. The tree-structured MR provides much better controllability to a live task-oriented dialog system, where developers can easily inject external knowledge into a rule-based response planner to specify the relationship between multiple dialog acts (e.g., rainy is the opposite to sunny), and the grouping of arguments in a dialog act is possible. These consideration have been shown to be critical to user perceptions of quality and naturalness (Lemon et al., 2004; Carenini and Moore, 2006; Walker et al., 2007; White et al., 2010; Demberg et al., 2011). In their Seq2Seq model, Balakrishnan et al. (2019) treat the tree-structured MR as just a sequence of tokens, ignoring the inherent tree structure (though this structure is taken into account in constrained decoding). We aim to examine the hypothesis that a better representation of the input tree structures could lead to better generalizability of the model and enhance semantic correctness. Therefore, we propose a tree-to-sequence model that uses a tree-based encoder to better represent the tree-structured MRs, and a structure-enhanced decoder to further incorporate contextual information in decoding. Our contributions are summarized as follo"
W19-8611,P02-1040,0,0.103184,"Missing"
W19-8611,P18-1026,0,0.0294752,"ed on adding planning steps to neural NLG architectures or employed non-sequential encoders. Puduppully et al. (2019) add a content planning step where a set of input database records are mapped to an ordered list of selected records; however, their approach does not employ hierarchical content plans as in our approach. Moryossef et al. (2019) add a symbolic text planning step where facts are grouped and ordered in the input; in contrast to our work though, their approach uses standard Seq2Seq models for realization and leaves no ordering choices to the model. Previous work on AMR and WebNLG (Beck et al., 2018; Song et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) has demonstrated improvements over Seq2Seq models by using graph-to-sequence models; while similar in principle, these works do not explore the use of hierarchical content plans as intermediate structures and do not experiment with constrained decoding. hpk = ftree ({hck1 , ..., hckN }) where N is the number of children for k-th node and ftree is a non-linear function. We implemented a variant of the N-ary TreeLSTM by (Tai et al., 2015) as our tree encoder. Since trees can have completely different layouts, it’s hard to train and d"
W19-8611,W18-6535,0,0.0272349,"structured semantic representations is crucial to building engaging and effective task-oriented dialog systems. Neural approaches for natural language generation (NNLG), particularly sequenceto-sequence approaches, have achieved promising results and were dominant in the recent E2E Challenge. Most of these approaches are built on flat meaning representations (MR) that use keyvalue pairs to capture attributes to be conveyed in responses. However, coupled with such flat MRs, current NNLG methods still struggle with 1) reliably performing sentence-level planning and discourse-level structuring (Reed et al., 2018); 2) avoiding generating semantic errors like hallucinated content (Duˇsek et al., 2018, 2019); and 3) generalizing to hard inputs (Wiseman et al., 2017). 95 Proceedings of The 12th International Conference on Natural Language Generation, pages 95–100, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Reference Flat MR Our MR Annotated Reference It’ll be sunny throughout this weekend. The high will be in the 60s, but expect temperatures to drop as low as 43 degrees by Sunday evening. There’s also a chance of strong winds on Saturday morning. condition1[sunny]"
W19-8611,J11-3003,0,0.0228043,"dd a reference annotated with the tree-structured MR in the last row. The tree-structured MR provides much better controllability to a live task-oriented dialog system, where developers can easily inject external knowledge into a rule-based response planner to specify the relationship between multiple dialog acts (e.g., rainy is the opposite to sunny), and the grouping of arguments in a dialog act is possible. These consideration have been shown to be critical to user perceptions of quality and naturalness (Lemon et al., 2004; Carenini and Moore, 2006; Walker et al., 2007; White et al., 2010; Demberg et al., 2011). In their Seq2Seq model, Balakrishnan et al. (2019) treat the tree-structured MR as just a sequence of tokens, ignoring the inherent tree structure (though this structure is taken into account in constrained decoding). We aim to examine the hypothesis that a better representation of the input tree structures could lead to better generalizability of the model and enhance semantic correctness. Therefore, we propose a tree-to-sequence model that uses a tree-based encoder to better represent the tree-structured MRs, and a structure-enhanced decoder to further incorporate contextual information in"
W19-8611,P18-1150,0,0.0215391,"ng steps to neural NLG architectures or employed non-sequential encoders. Puduppully et al. (2019) add a content planning step where a set of input database records are mapped to an ordered list of selected records; however, their approach does not employ hierarchical content plans as in our approach. Moryossef et al. (2019) add a symbolic text planning step where facts are grouped and ordered in the input; in contrast to our work though, their approach uses standard Seq2Seq models for realization and leaves no ordering choices to the model. Previous work on AMR and WebNLG (Beck et al., 2018; Song et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) has demonstrated improvements over Seq2Seq models by using graph-to-sequence models; while similar in principle, these works do not explore the use of hierarchical content plans as intermediate structures and do not experiment with constrained decoding. hpk = ftree ({hck1 , ..., hckN }) where N is the number of children for k-th node and ftree is a non-linear function. We implemented a variant of the N-ary TreeLSTM by (Tai et al., 2015) as our tree encoder. Since trees can have completely different layouts, it’s hard to train and do inference with tr"
W19-8611,W18-6539,0,0.0581256,"Missing"
W19-8611,P15-1150,0,0.10851,"ls for realization and leaves no ordering choices to the model. Previous work on AMR and WebNLG (Beck et al., 2018; Song et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) has demonstrated improvements over Seq2Seq models by using graph-to-sequence models; while similar in principle, these works do not explore the use of hierarchical content plans as intermediate structures and do not experiment with constrained decoding. hpk = ftree ({hck1 , ..., hckN }) where N is the number of children for k-th node and ftree is a non-linear function. We implemented a variant of the N-ary TreeLSTM by (Tai et al., 2015) as our tree encoder. Since trees can have completely different layouts, it’s hard to train and do inference with tree inputs in parallel. We propose an iterative bottomup traversal algorithm to support batch forward and backward with tree inputs. Given a batch of trees, we first extract all the leaf nodes and update their states in a batch manner. Then we iteratively update the states of non-leaf nodes if all of their 96 children nodes have been processed. As nodes can have different number of children nodes, we padded non-leaf nodes to have the same number of children nodes (i.e., N ) for ba"
W19-8611,W19-2308,0,0.0263858,"ce of ] [wind summary strong winds ] on [date time [week day Saturday ] [colloquial morning ] ] . ] Table 1: Sample flat MR with reference compared against tree-structured MR. The last row shows an annotated reference with the tree-structured MR. Nodes in blue are all children of the root node of the tree. • We propose a tree-to-sequence (tree2seq) model to better leverage the inherent structures in the tree-based MRs. Coupled with the constrained decoding technique from (Balakrishnan et al., 2019), we explore whether combining better learning and decoding methods yields the best performance. Elder et al. (2019) propose using an intermediate representation motivated by a universal dependency tree, and find that this greatly improves performance. However, their approach is still Seq2Seq-based and can’t explicitly model the tree structures. Similar to our approach, Eriguchi et al. (2016) use a tree-to-sequence model for machine translation, but here we focus on NLG and use different tree encoder and constrained decoding techniques. • Extensive evaluations on conversational weather and E2E datasets (Duˇsek et al., 2019) show that the tree2seq model can significantly improve semantic correctness. Analysi"
W19-8611,J10-2001,1,0.848389,"Missing"
W19-8611,W18-6501,0,0.0215018,"NLG architectures or employed non-sequential encoders. Puduppully et al. (2019) add a content planning step where a set of input database records are mapped to an ordered list of selected records; however, their approach does not employ hierarchical content plans as in our approach. Moryossef et al. (2019) add a symbolic text planning step where facts are grouped and ordered in the input; in contrast to our work though, their approach uses standard Seq2Seq models for realization and leaves no ordering choices to the model. Previous work on AMR and WebNLG (Beck et al., 2018; Song et al., 2018; Marcheggiani and Perez-Beltrachini, 2018) has demonstrated improvements over Seq2Seq models by using graph-to-sequence models; while similar in principle, these works do not explore the use of hierarchical content plans as intermediate structures and do not experiment with constrained decoding. hpk = ftree ({hck1 , ..., hckN }) where N is the number of children for k-th node and ftree is a non-linear function. We implemented a variant of the N-ary TreeLSTM by (Tai et al., 2015) as our tree encoder. Since trees can have completely different layouts, it’s hard to train and do inference with tree inputs in parallel. We propose an iterat"
W98-1428,W96-0416,0,0.0610454,"Missing"
W98-1428,A97-1039,0,0.131348,"Missing"
W98-1428,J97-1004,0,0.0282869,"Missing"
W98-1428,P92-1034,0,0.201859,"Missing"
W98-1428,J95-1002,0,0.0605868,"Missing"
